Our capacity to collect and store data increases in an exponential manner but our capacity to analyze it has not followed the same trend. Despite the explosion of available data, the discovery of truly interesting patterns is a rare event. Outlier detection the discovery of observations that deviates from normal behavior has been widely studied in recent years [26,15,7], resulting in a set algorithms designed to detect these rare but potentially crucial events. In some specific contexts an outlier is a data point that can be considered either as an abnormality or noise,

The process of outlier detection represents a very specific classification scenario: first, the quantity of outliers is very small in proportion to the quantity of normal instances; and second, the use of labels (supervised approach) in outlier detec- tion is limited due to the fact that, by definition, the outliers that we are trying to detect represent a new or unseen behavior. Despite the fact that some algo- rithms (techniques) can operate using only labels for the normal class [23] (semi supervised approach) and use this information to increase the detection rate, un- supervised approaches have the undeniable advantage of operating over unlabeled data. Furthermore, unlabeled data are usually easier to obtain and represents the more common scenario in outlier detection [10].

Combining outputs of different classifiers is not a novel task; however, outlier detection has to face two additional problems [20]. First, an ensemble of classifiers works with discrete labels whereas outlier detection is mainly concerned with scores. Second, an ensemble of classifiers generally relies on the existence of training data (supervised approach), whereas outlier detection generally does not have access to labeled data (unsupervised approach).

Outlier detection is a very active research area where new approaches are proposed each year. Nevertheless, the detection of outliers was first contemplated in the statistical community in 1887 [9]. Since then, different techniques based on various approaches such as classification, clustering, density based and statistical inference have been proposed.

An important characteristic of an outlier algorithm is its output, which can be either a score or binary label [1]. The former type of output assigns a score to each observation and in general can be used to rank the observations depending on its level of outlierness. The latter assigns binary labels, commonly using 1 for outliers and 0 to designate normal observations (inliers).

The construction of an ensemble of outlier algorithms seems like a viable so- lution when the objective is to increase the detection rate of outliers (e.g. breast cancer detection) while diminishing the variance introduced by each outlier detec- tion algorithm. However, no gain will be obtained by using algorithms whose results are identical. Therefore, two important factors must be taken into account when constructing an ensemble: accuracy and diversity. Accuracy measures the output quality of each algorithm, while diversity endeavors to build an ensemble whose results are distinct and, in theory, complementary. Accuracy depends on the right association of technique and dataset; diversity can be established using variations of the search space (data and feature sampling) or by the use of different types of algo- rithms [28]. Combining different types of algorithms could yield better performance than simply using parametric variations of the same algorithm [27]. However, a balance between accuracy and diversity is needed in order to obtain an improved ensemble detection rate [32]; highly diverse, but inaccurate algorithms, results in an ensemble whose components are truly diverse, but without the accuracy component is unable to converge near the true classification output, resulting in an ensemble whose detection rate is below that of its individual members.

The process of building an ensemble involves three main considerations: the choice of the algorithms, the organization (modular or ensemble) and the combi- nation method [6]. A multiclassifier can be categorized as modular or ensemble. A multiclassifier is modular when each member is responsible for a specific part of the process and the algorithms are used in a series of steps, using the results of the previous algorithm. It is an ensemble when each single member works on the same search space and a combinatorial process joins the results to produce a unified output. In this paper we are focusing on the latter type. The most important com- ponent is the combinatorial approach chosen so that each single member (classifier) contributes to improve the overall performance.

One critical factor in the construction of an ensemble is to mix members (al- gorithms) whose errors are not identical; doing so assures us that these members complement each other, therefore producing potential improved results. However, the majority of such approaches assume that a measure of accuracy for each member is available, using class labels for each observation. Still, considering that outlier detection is mainly an unsupervised field, it is not practical to measure accuracy using output labels. In our proposed approach, we do not assume highly accurate classifiers trained with the use of labeled data; instead we estimate accuracy by considering only the output scores of each algorithm and attempting to achieve diversity using different types of outlier detection techniques.

The Breadth First method first sorts the outlier scores from all the iterations of feature bagging, next takes the index of the record with the highest score and then inserts its index in a vector, and so on. If an index is already in the vector, it is omitted. The final output is a vector of indices pointing to its corresponding scores. The second variant of feature bagging is Cumulative Sum. This method simply adds up the scores of each iteration of feature bagging, and the outliers are those

The Breadth First approach is exposed to a critical observation: it is highly sensitive to the order in which the outlier detection algorithms were applied. This means that the first technique in the ensemble has priority to decide about the outlierness of a given data record. Also, the methodology of this approach does not indicate how to establish the order of the algorithms.

The approaches use two different similarity/dissimilarity measures for numerical values: correlation and MAD; we use them to measure the similitude between the outputs of different classifiers. The former can be used to evaluate the statistical correlation between different outputs; also it is indifferent to the scale of the input values and will produce a result of 1 for perfectly correlated values, 0 for uncorrelated values and -1 for negatively correlated values. The latter is used to measure the absolute deviation between different outputs. MAD produces results relative to the scale of its components. Whereas MAD tends to assign low values to similar scores, correlation coefficient assigns high values to correlated scores.

As shown in Algorithm 1, a given dataset (DS ) of size m is first examined by applying each of the algorithms in a series of T rounds, where T represents the number of algorithms available in the ensemble. For testing purposes we are using T =4 . Nonetheless, T can take different values, meaning that our approach is not constrained either to the use of specific outlier algorithms or by the number of them. We expect that our approach can be applied using the majority of outlier detection

Using these standardized outputs (F ) from each algorithm, we then apply a modified boxplot technique to detect those outputs whose deviations are greater than the rest. In this way we produce a vector of votes (V ) of size m * T (number of observations multiplied by the number of algorithms) that contains the number of votes of each algorithm for each observation. An observation receives a vote if its score is greater that 1.5*IQR (where IQR is the inter quartile range). We determine the IQR in the conventional way [31] IQR=Q3-Q1, where Q3 and Q1 stand for third quartile and first quartile respectively. Accordingly, the output matrix V in this step has the same dimensions as the matrix containing the standardized scores F. Each score in F will have a corresponding number of votes in V ; for example Vij corresponds to the number of votes assigned to Fij.

(1) with dimensions m=size of T by n=size of T by calculating the correlation between the standardized scores F. For example, as represented in (1), Cmn stands for the correlation coefficient between scores Fm and Fn. Next, we divided the average of the correlations corresponding to each Fn by the size of T to obtain the matrix W ; given that the correlation of an algorithm with itself is meaningless as it corresponds invariably to a perfect correlation with value 1, then we subtracted 1 from both the numerator and denominator.  The resulting matrix of weights

The final process is displayed in Algorithm 3. First, we calculate the product of each of the standardized scores F and their corresponding votes in matrix V, then the resulting values are updated by applying the weights W obtained by either EDCV or EDVV. Finally, the updated scores from each algorithm are simply added together and divided by the size of T.

The goal in our experiments is to mimic a real estimation of the performance of the ensemble methods and not the performance of perfectly tuned outlier detection algorithms. Differently from the experiments performed by the authors of feature bagging who used the labels for the inliers (normal instances), we do not suppose the existence of labels, given that our experiments are based on a completely unsu- pervised approach. Despite this, we acknowledge that the inclusion of labels for the inliers will increase the performance of the algorithms and thus that of the ensemble. Our results are also compared to a simple average of the scores of each algorithm, detecting free electrons in the ionosphere; the majority class is composed of those measures representing some structure in the ionosphere, and the minority class by those cases where there is no evidence of structure formation in the ionosphere. For the satimage dataset, we use the smallest class as the outlier and merged the rest to be considered as the normal class. In this dataset, the classes represent multispectral values of pixels in a satellite image. When performing experiments on lymphography, we selected classes one and four (less than 5%) to be the outlier class and used classes two and three as the normal class.

To increase the number of available datasets, we used a procedure commonly used in similar studies [20,16], which consists in the adaptation of datasets not directly related with the problem of outlier detection. The procedure consists of transforming a multivariate problem into a two class problem in two steps: first, we identify the smallest class or a subset of the smallest classes, and consider them as the outlier class, then, the majority - or the rest of the classes - are merged and used as the normal class. Following this method, we formed 7 additional datasets based on ann thyroid and shuttle datasets. Accordingly, for the ann thyroid dataset, which contains three classes, the smallest two are related with hyperfunction and subnormal function (less than 10% of the dataset), and a third not hypothyroid class (normal condition); in this case, we produced 2 datasets by using each one of the minority classes in turns as the outlier class versus the normal condition.

As expected the worst performance for all algorithms was with the datasets adapted to a binary class problem. This is understandable since the union of differ- ent classes produced a single class with different distributions that are very difficult to detect by the individual algorithms of the ensembles. However, even on the ar- tificially generated datasets, EDCV and EDVV offered an improved performance compared with the rest of the approaches. The advantage of EDCV and EDVV is that they do not assume an exceptional and constant good performance of the algorithms over all the different types of datasets, but instead, assign weights to the algorithms based on their performance on each dataset in particular.

In this paper, two novel and completely unsupervised ensemble approaches for com- bining the output scores of different outlier detection algorithms were presented: en- semble of detectors with correlated votes (EDCV ) and ensemble of detectors with variability votes (EDVV ). Experiments on several popular real life datasets sug- gested that both approaches can achieve better performance than similar methods. Also, it is worth considering that our results were obtained using only 4 iterations of the ensemble, while for feature bagging we set the number of iterations to 50.

of comparing their outputs; thus the advantage is that both approaches are not expecting an exceptional and constant performance from all the algorithms on dif- ferent types of datasets. Moreover, not expecting a constant performance of the algorithms allows for the inclusion of different types of outlier detection algorithms. While similar approaches like feature bagging Cumulative Sum and feature bagging Breadth First introduce diversity through variation on the search space, EDCV and EDVV attempt to ensure diversity by using different types of algorithms, which re- sults in a more widely applicable approach.

