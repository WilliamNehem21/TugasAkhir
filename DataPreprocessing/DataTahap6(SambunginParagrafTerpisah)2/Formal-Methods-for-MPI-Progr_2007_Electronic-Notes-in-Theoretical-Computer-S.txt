High-end computing is universally recognized to be a strategic tool for leadership in science and technology. A significant portion of high-end computing is conducted on clusters running the Mes- sage Passing Interface (MPI) library. MPI has become the de facto standard in high performance computing (HPC). Our research addresses the need to avoid bugs in MPI programs through a combination of techniques ranging from the use of formal specifications to the use of in-situ model checking techniques. This paper details an assessment of the efficacy of these techniques, as well as our future work plans.

Just as is the case with parallel programs in general, MPI programs in particular can contain bugs. Specifically, the sources of MPI program bugs have been identified to include the following. First of all, the large number of functions in MPI libraries can overwhelm developers. Second, MPI is most commonly taught or learned at an informal level. As such, programmers writing advanced MPI applications may overlook corner cases. Finally, MPI programs are not static; they are sometimes manually re-tuned when ported to a new hardware platform.

This paper is about our ongoing research that emphasizes the use of for- mal methods for making the creation of bug-free MPI programs easier. Our approach consists of (i) developing a formal model of the MPI library, (ii) de- veloping in-situ (run-time) model checking tools, and (iii) developing static analysis support for enhancing the efficacy of model checking. This paper briefly describes our ongoing work in these areas, as well as our future plans. The authors wish to acknowledge the impact that Professor Gary Lindstrom had in the parallel computing research conducted at the University of Utah, and thank him for his feedback and encouragement of the research reported here.

The rest of the paper is organized as follows. In Section 2, we present an overview of our work underway in developing a formal specification for MPI. In Section 3, we describe our work on developing an in-situ model checker for MPI. In-situ model checking was introduced in VeriSoft [13] in the context of directly model checking C/C++ programs. Ours is believed to be the first realization of this idea for MPI programs. In Section 4, we present an assessment of our work so far, and draw conclusions for the future.

In our previous work [16], we captured around 10% of the MPI-1.0 primi- tives (mainly for point-to-point communication) in TLA+ [18]. TLA+ enjoys wide usage within (at least) Microsoft and Intel by practicing engineers. We built a C front-end in the Microsoft Visual Studio (VS) parallel debugger en- vironment through which users can submit short (but tricky) MPI programs, with embedded assertions. The embedding C statements as well as the MPI calls are turned into TLA+, run through the TLC model checker [18]. When the assertions fail, the error traces turn into the VS debugger stepping com- mands. This approach gives the practitioner a tool for understanding MPI based on a formal semantic description which may, as a one-time activity, be validated by experts. Our formal specification maintains cross-reference tags with the MPI reference document, citing line and page numbers of pertinent references. Another project with similar motivations (but not a C front-end) is [17], which formalizes the kernel threads procedures of the Win32 API.

Currently ISP checks for deadlocks and local assertion violations. For ev- ery MPI function (e.g. MPI send), ISP provides a replacement function (a modest, one-time effort). When invoked, these replacement functions first consults a central scheduler (an N+1st process) through TCP sockets. If the scheduler gives permission, the replacement function invokes PMPI send (that is provided with every MPI implementation, and has the same functionality as MPI send). This allows the scheduler to march the processes of a given MPI program according to one arbitrary interleaving, till all processes hit MPI finalize. ISP examines the resulting trace of actions, and records at each of its choice points, where a different process could have been selected. Such alternative choices are deemed necessary based on the dynamic dependence between actions in the current trace (see [14] for details). If, at choice point i,

ISP itself follows a highly parallelizable algorithm. The alternative interleav- ings may well be explored by other nodes of a large cluster. Our implemen- tation of an ISP algorithm for PThreads [10] (a different effort with no code in common) shows that with a suitably designed heuristic, linear speed-up on large clusters is a possibility. Further plans regarding ISP are presented in Section 4.

Our future work will consist of two distinct phases. First, we plan to more rigorously analyze our MPI formal specification and find ways in which to solid feedback. Our plans regarding ISP are to develop new combinations of static and dynamic analysis, obtain MPI programs that are from a class known to be difficult to debug, and measure the efficacy of our tools.

