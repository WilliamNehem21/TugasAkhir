The model and property being verified have significant impacts on the readability of counter-examples. The further a model is abstracted the more difficult it becomes to interpret. Abstraction often involves leaving some data out of the state space of the model. This makes it difficult for experts in the application domain to understand the counter-example when there is information missing that they would like to see. It is also difficult to interpret a counter-example when there is a lot of data that is not relevant to the error in the system.

dependent on the current value of the state variables. The rules for changing signal aspects in our abstracted model only depend on the values of other state variables and not the current aspect. We therefore exclude signals from the state space for efficiency, and they are modelled by checking the current values of the variables that the signal depends on. Trains are well-behaved in the model. They travel along an interlocking obeying the rules in the Signalling Principles. They stop at red signals and can also stop on any track segment. There is no speed associated with train movement.

Abstracting the model can make the counter-example more difficult to read. Our model abstracts by excluding signals. This reduces the number of state variables but also means that the user does not know what the aspects of any of the signals are. We have proven through comprehensive testing that the model is accurate. However, signalling engineers still find the model disconcerting.

Counter-examples typically include in each state the variables that have changed since the previous state. When verifying railway interlockings, to check just one route it is necessary to include all opposing routes. If an error is detected, the counter-example produced will contain variables for every route, track segment, point, train, the current route of each train and so on, even if it is only a single error that occurs in one route. This can be overwhelming given that a small verification area can contain as many as 30 variables.

The second approach involves the automatic interpretation of counter-examples to determine the actual error that caused the collision or derailment. The key concept is to design an algorithm to interpret the counter-example and produce a description of the error that caused the safety violation. The algorithm needs to be generic to handle all of our counter-examples despite the differences between them. Only the data relevant to the error should be gathered and presented to the user. To be of value, the output of the algorithm should provide as much detail as possible without misleading the user.

The data is interpreted separately for each type of counter-example. The counter- examples may have more than one format for the output depending on the value of some of the data. The format and an example of the interpretations for each type of counter-example are given below.

Counter-examples are known to be a usability problem not just for domain experts but also for engineers designing the models. Most research, for example Clarke et al. [3], Pasareanu et al. [10] and Dwyer et al. [5], focuses on eliminating false or spurious counter-examples rather than improving their readability. These papers all

Two approaches, animation and interpretation, were considered for presenting counter- examples to end users. Animation was investigated but is not appropriate for our application as our model is significantly abstracted from the real system. An inter- preter was designed and prototyped based on the Signalling Principles [11]. This creates a robust algorithm that can handle any counter-example from our models.

