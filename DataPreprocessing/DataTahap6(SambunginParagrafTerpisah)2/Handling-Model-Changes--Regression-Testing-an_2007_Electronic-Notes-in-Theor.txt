Several model-checker based methods to automated test-case generation have been proposed recently. The performance and applicability largely depends on the complexity of the model in use. For complex models, the costs of creating a full test-suite can be significant. If the model is changed, then in general the test- suite is completely regenerated. However, only a subset of a test-suite might be invalidated by a model change. Creating a full test-suite in such a case would therefore waste time by unnecessarily recreating valid test-cases. This paper investigates methods to reduce the effort of recreating test-suites after a model is changed. This is also related to regression testing, where the number of test-cases necessary after a change should be minimized. This paper presents and evaluates methods to identify obsolete test-cases, and to extend any given test-case generation approach based on model-checkers in order to create test-cases for test-suite update or regression testing.

The need for efficient methods to ensure software correctness has resulted in many different approaches to testing. Recently, model-checkers have been considered for test-case generation use in several works. In general, the counter example mecha- nism of model-checkers is exploited in order to create traces that can be used as test-cases.

We present different methods to create new test-cases after a model change. These test-cases can be used as regression tests if the number of test-cases executed after a model change should be minimized. They are also used in order to update test- suites created with older versions of the model.

This paper is organized as follows: Section 2 identifies the relevant types of changes to models and relates them to a concrete model-checker input language, and then presents our solutions to the tasks of identifying invalid test-cases and creating new ones. Section 3 describes the experiment setup and measurement methods applied to evaluate these approaches, and presents the results of these experiments. Finally, Section 4 discusses the results and concludes the paper.

In practice, the Kripke structure is described with the input language of the model-checker in use. Such input languages usually describe the transition relation by defining conditions on AP , and setting the values of variables according to these conditions. A transition condition C describes a set of states Si where C is fulfilled. In all successor states of these states the variable v has to have the next value n:

In order to use a model-checker to decide if a test-case is valid for a given model, the test-case is converted to a verifiable model. The transition relations of all variables are given such that they depend on a special state counting variable, as suggested by Ammann and Black [1]. An example transition relation is modeled in Listing 2, where State denotes the state counting variable. There are two methods to decide whether a test-case is still valid after a model change. One is based on an execution of the test-case on the model, and the other verifies change related properties on the test-case model.

Now the problem of checking the validity of a test-suite with regard to a changed model reduces to model-checking each of the test-cases combined with the new model. Each test-case that results in a counter example is obsolete. The test-cases that do not result in a counter example are still valid, and thus are not affected by the model change. A drawback of this approach is that the actual model is involved in model-checking. If the model is complex, this can have a severe impact on the performance.

Theoretically, the latter case can report false positives, if the removed transition is subsumed or replaced by another transition that behaves identically. This is conceivable as a result of manual model editing. Such false positives can be avoided by checking the new model against this change property. Only if this results in a counter example the removal has an effect and really needs to be checked on test- cases. Although verification using the full model is necessary, it only has to be done once in contrast to the symbolic execution method.

Test-cases that are invalidated by a model change can be useful when testing an implementation with regard to the model change. Obsolete positive test-cases can be used as negative regression test-cases. A negative test-case is such a test-case that may not be passed by a correct implementation. Therefore, an implementation that passes a negative regression test-case adheres to the behavior described by the old model.

Once the obsolete test-cases after a model change have been identified and discarded, the test-cases that remain are those that exercise only unchanged behavior. This means that any new behavior added through the change is not tested. Therefore, new test-cases have to be created.

Alternatively, when checking test-cases using the symbolic execution method, the counter examples in this process can directly be used as test-cases. In contrast to the method just described resulting test-cases can potentially be shorter, depending on the change. This can theoretically have a negative influence on the overall coverage of the new test-suite.

that make up a coverage criterion as properties that claim the items cannot be reached [9]. For example, a trap property might claim that a certain state or tran- sition is never reached. When checking a model against a trap property the model- checker returns a counter example that can be used as a test-case. We generalize the approach of Xu et al. in order to be applicable to a broader range of test-case generation techniques. The majority of approaches works by either creating a set of trap properties or by creating mutants of the model.

The second category of test-case generation approaches uses mutants of the model to create test-cases (e.g., [3, 2, 11, 8]). For example, state machine duplica- tion [11] combines original and mutant model so that they share the same input variables. The model-checker is then queried whether there exists a state where the output values of model and mutant differ. Here, the solution is to use only those mutants that are related to the model change. For this, the locations of the changes are determined (e.g., in the syntax tree created by parsing the models) and then the full set of mutants for the changed model is filtered such that only mutants of changed statements in the NuSMV source remain. Test-case generation is then performed only using the remaining mutants.

As a third method to create change related test-cases we propose a generic ex- tension applicable to any test-case generation method. It rewrites both the model (or mutants thereof) and properties involved in the test-case generation just before the model-checker is called. This rewriting is fully automated. The model is ex- tended by a new Boolean variable changed. If there is more than one change, then there is one variable for each change: changei. These variables are initialized with the value false. A change variable is set to true when a state is reached where a changed transition is taken. Once a change variable is true, it keeps that value. The transition relation of the change variable consists of the transition condition of the changed variable is shown in Listing 4.

The methods described in this paper have been implemented using the programming language Python and the model-checker NuSMV [6]. All experiments have been run on a PC with Intel Core Duo T2400 processor and 1GB RAM, running GNU/Linux. We automatically identify changes between two versions of a model by an analysis of the abstract syntax trees created from parsing the models. We use a simple example model of a cruise control application based on a version by Kirby et al. [10]. In order to evaluate the presented methods, the mutation score and creation time of new and updated test-suites were tracked over several changes. There is a threat to the validity of the experiments by choosing changes that are not representative of real changes. Therefore the experiments were run several times with different changes and the resulting values are averaged.

suite. This is because the test-suites created with the focus method are bigger than new test-suites for the transition coverage criterion. Adaptation generally achieves the lowest mutation scores. However, the mutation score is only slightly smaller than for the update method, so a significant performance gain could justify this degradation.

In this paper, we have shown how to decide whether a test-case is still valid after the model it was created from is changed. That way, it is possible to reuse some of the test-cases after a model change and reduce the test-suite generation effort. Different methods to create test-cases specific to the model change were presented. We used the model-checker NuSMV for our experiments and as an example model syntax. However, there is no reason why the approach should not be applicable to other model-checkers. Experiments have shown that the presented methods can be used to update test-suites after a model change, although there is a trade-off between performance improvement and quality loss.

The main problem of model-checker based approaches in general is the perfor- mance. If the model is too complex, then test-case generation will take very long or might even be impossible. Therefore, it is important to find ways of optimizing the approach. The potential savings when recreating test-suites after a model change are significant. Even for the small model used in our evaluation a large performance gain is observable when only selectively creating test-cases for the model changes. Although a model is usually more abstract than the program it represents, the model size can still be significant. For instance, automatic conversion (e.g., Matlab Stateflow to SMV) can result in complex models.

There are some approaches that explicitly use specification properties for test- case generation [3, 11, 2]. This paper did not explicitly cover the aspects of test- suite update with regard to specification properties. However, the idea of test-suite focus directly applies to such approaches, as well as the presented test-suite update techniques. The cruise control example is only a small model, and the changes involved in our experiments were generated automatically. This is sufficient to show the feasibility of the approach. However, actual performance measurements on more complex models and realistic changes would be desirable.

