as Support Vector Machines (SVM). The bag of features [2] used to represent objects in images is extended to spatiotempo- ral bag of features [3] to represent human activities in videos. Standard human activity recognition consists of four stages. First stage is the detection of important interest points. Second stage is to describe the detected interest points using one or more of the descriptors such as Histogram of Oriented Gradients (HOG), Histogram of Optical Flow (HOF), Scale- Invariant Feature Transform (SIFT), and cuboids. Third stage is the bag of features algorithm, which encodes all the descrip- tors extracted from each video into a single code [3,4]. This

Recently, many efforts have been done to improve bag of features algorithm. In this paper the bag of features represen- tation has been improved to increase the accuracy of the human activity recognition. The results of the used approach are validated on the standard KTH benchmark [3] and Weizmann dataset [5,6]. The results show that our new approach improves and outperforms the state-of-the-art.

The rest of the paper is organized as follows: Section 2 pre- sents the different video representations and encoding methods and their variations. In Section 3, the methods used for human activity recognition are presented. The framework used and the details of detector, and descriptors used are stated. The bag of features and the proposed enhancement are presented. Then the classification stage is stated. Section 4 presents and discusses the experimental results and the effect of different parameters of the proposed enhanced bag of features. Finally, Section 5 concludes the paper and suggests the future work.

Other alternative methods have been proposed to overcome this problem such as Soft quantization [10,11] and local linear encoding [12]. These methods capture more information from the features by representing them as a combination of visual words. Fisher encoding [13,14] and super-vector encoding [15], record the difference between the features and visual words.

Another limitation of bag of features is its inability to encode any spatial information about relationship between words. To overcome this limitation, spatial arrangement of words is added to improve the bag of features. Some methods capture spatial relationship information of visual words by encoding the spatial arrangement of every visual word in an image [19]. Temporal bag of words model (TBoW) divides each video into N temporal bins and constructs a histogram for each bin representing points belonging to that particular bin [20]. Another encoding schemes use n-grams to augment bag of words with the discovered temporal events in a way that preserves the local structural information (relative word posi- tions) in the activity [21].

Hierarchical two level clustering, clustering per video then for all the training videos, has been proposed before [22]. A new feature representation was proposed which captures the statistics of pairwise co-occurring local spatiotemporal fea- tures. The number of features produced for every video was too large, so it was constrained to a maximum limit FMAX ran- domly sampled from the video. Features from each video are separately clustered, then all the training videos are processed together and all the obtained groups of features are reclustered to form a final codebook.

Descriptors are feature vectors describing each interesting point and its surrounding volume. The descriptors can depict shape or gradient properties in the volume such as Histogram of Oriented Gradient (HOG) [1,25]. Some descriptors depict the motion around the point using the optical flow and its his- togram such as Histogram of Optical Flow (HOF) [1]. Two or more descriptors can be used together. Fusion between the fea- tures can be done at the descriptor level called early fusion or at the classifier level with a two channel classifier called late fusion. In this study, HOF and HOG descriptors were used with the KTH and Weizmann datasets, respectively. Local spa- tiotemporal features have demonstrated high recognition results for a number of action classes. Therefore, we use them as basic features for our approach. The selection of these speci- fic descriptors was based on their good recognition accuracy when used with the KTH [26] and Weizmann datasets.

In this study, we extend the k-means algorithm to be able to cluster all the training data and not just a small sample of it. Clustering all the training data results in enhancing the accu- racy of human activity recognition. To overcome the computa- tional complexity problem, the clustering is done in a multilevel methodology instead of clustering all input data at once. Clustering is performed for every video, for every action, and for all the actions. This can be achieved using two-level clustering or three-level clustering.

Four experiments have been conducted on two popular human action datasets, namely KTH and Weizmann datasets. In this section, a brief introduction for these datasets, and the exper- imental setup used is presented. The details and results of experiments 1 and 2 performed on KTH dataset are presented in Section 4.2. Results of experiments 3 and 4 performed on Weizmann dataset are presented in Section 4.3. Finally, the performance of the new methods has been compared with the state of the art on KTH and Weizmann datasets.

Weizmann dataset is introduced by Gorelick et al in 2005 [5]. It consists of 10 actions (bending, jumping, jumping jack, jumping in place, running, galloping sideways, skipping, walk- ing, one hand-waving and two hands waving). Each of these actions is performed by 9 actors resulting in 90 videos. Leave-one-person out experimental setup is used with the Weizmann dataset, where at each run 8 persons (80 videos) are used for clustering and training, and one person (10 videos) for testing. Then the average accuracy of the results is taken as the final recognition accuracy.

The first experiment is performed on KTH dataset using two levels of clustering. In the first level the descriptors from each video in the training set are clustered. In the second level all the clusters from each video in the training set are clustered again to make the dictionary. The main idea is to make two level clustering and take all the descriptors data from all the training set, so that all the descriptor features contribute to the final result.

Increasing k1 when k2 is constant usually increases the accuracy of classification too. The second column is represent- ing the accuracy for k1 changing from 500 to 2500 with 250 steps and k2 fixed at 1000. The accuracy starts at 93.5% increases to reach 94% at k1 = 1000. The highest accuracy is 96.3% at k1 = 1250. Finally the best accuracy (97.7%) has been observed at k1 = 750, and k2 = 4500.

A comparison between the time of one level standard k-means, two level and three level k-means on the KTH dataset is made. The input to the k-means algorithm is a subset of 30 K, 40 K, 50 K, 100 K features randomly sampled from the training videos. The 100 K is not drawn for one level since it will take very large time. Code book size k is changed. The clustering per video is done for 10 percent and clustering per action is

This was achieved through extracting holistic features from clouds of interest points. Feature selection was applied, and frame differencing was used to extract region of interest, with extra processing. Also they made a fusion between clouds of interest points, containing complementary interest point distri- bution, with the conventional bag of features representation, and used a feature fusion method based on Multiple Kernel Learning. However, the developed modification in the current study achieved better results than the ones obtained by their enhancements.

Bilinski et al. [22] used Harris 3D detector, HOF and HOG descriptors and proposed a novel feature representation which captures statistics of pairwise co-occurring local spa- tiotemporal features. However, this method produces large number of features limited to 100,000 feature for each video and takes a lot of computation time and it achieved 96.30% accuracy using easier evaluation scheme (Leave-One-Out Cross-Validation).

The accuracy of human activity recognition can be enhanced by the developed multilevel k-means. This modification was introduced in order to have a better code book by using all the descriptors from all training videos in the clustering stage. Achieving this in two level or three level clustering steps can be done in a reasonable time. Two-level clustering methodology was used to cluster descriptors data from each video separately and then cluster all the result clusters into a single code book. The two-level clustering enhances accuracy from 92.1% [26] to reach 96.28% using the same detector, descriptor and classifier.

A three-level clustering methodology was used to cluster descriptors data from each video separately and then cluster all the result clusters from each action into a k1 clusters. The cluster centers obtained from action classes were then clustered into a final single code book with k2 clusters. Three-level clus- tering methodology further enhanced the results to reach 97.7%.

