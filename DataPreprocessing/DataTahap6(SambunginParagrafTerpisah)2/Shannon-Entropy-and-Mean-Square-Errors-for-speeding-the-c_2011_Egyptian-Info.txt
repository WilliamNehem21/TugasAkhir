Abstract Improving the efficiency and convergence rate of the Multilayer Backpropagation Neu- ral Network Algorithms is an active area of research. The last years have witnessed an increasing attention to entropy based criteria in adaptive systems. Several principles were proposed based on the maximization or minimization of entropic cost functions. One way of entropy criteria in learning systems is to minimize the entropy of the error between two variables: typically one is the output of the learning system and the other is the target. In this paper, improving the efficiency and convergence rate of Multilayer Backpropagation (BP) Neural Networks was proposed. The usual Mean Square Error (MSE) minimization principle is substituted by the minimization of Shan- non Entropy (SE) of the differences between the multilayer perceptions output and the desired tar- get. These two cost functions are studied, analyzed and tested with two different activation functions namely, the Cauchy and the hyperbolic tangent activation functions. The comparative approach indicates that the Degree of convergence using Shannon Entropy cost function is higher than its counterpart using MSE and that MSE speeds the convergence than Shannon Entropy.

approach it can solve the large-scale problem efficiently and optimal solution can be obtained [10]. The advantage of neural networks lies in the following theoretical aspects. First, neural networks are data driven self-adaptive methods in that they can adjust themselves to the data without any explicit specifi- cation of functional or distributional form for the underlying model. Second, they are universal functional approximators in that neural networks can approximate any function with arbitrary accuracy. Third, neural networks are nonlinear mod- els, which makes them flexible in modeling real world complex relationships. Finally, neural networks are able to estimate the posterior probabilities, which provides the basis for establish- ing classification rule and performing statistical analysis.

Usually error backpropagation for neural network learning is made using MSE as the cost function [22]. During the learn- ing process, the ANN goes through stages in which the reduc- tion of the error can be extremely slow. These periods of stagnation can influence learning times. In order to resolve this problem, the MSE are replaced by entropy error function [23,8,24]. Simulation results using this error function shows a better network performance with a shorter stagnation period. Accordingly, our purpose is the use of the minimization of the error entropy instead of the MSE as a cost function for classification purposes. Let the error e(j)= T(j) Y(j) repre- sent the difference between the target T of the j output neuron and its output Y, at a given time t. The MSE of the variable e(j) can be replaced by its EEM counterpart.

The rest of the paper is organized as follows. Related work is outlined in Section 2. Section 3 introduces the Multilayer Backpropagation Neural Networks. Section 4 introduces the Mean Square Error. Shannon Entropy was discussed and ana- lyzed in Section 5. Simulated results were discussed in Section 6 for Shannon Entropy and in Section 7 for Mean Square Error. Section 8 compares Shannon Entropy and MSE. Final- ly Conclusions are outlined in Section 9.

are used for controlling the weight adjustment along the descent direction and for dampening oscillations. However, the convergence rate of the BP algorithm is relatively slow, especially for networks with more than one hidden layer. The reason for this is the saturation behavior of the activation function used for the hidden and output layers. Since the out- put of a unit exists in the saturation area, the corresponding descent gradient takes a very small value, even if the output er- ror is large, leading to very little progress in the weight adjust- ment. The Backpropagation algorithm may be described with the following three steps, which have to be applied several times in an iteration.

layer with output y and a target variable (class membership for each example in the dataset), t. for each example we mea- sure the error using e(n)= t(n)  y(n), n = 1, 2,..., N where N is the total number of examples. We only consider the two- class problem; thus we set t { 1, 1}. The proposed Back- propagation algorithm does not use expression (17) directly

