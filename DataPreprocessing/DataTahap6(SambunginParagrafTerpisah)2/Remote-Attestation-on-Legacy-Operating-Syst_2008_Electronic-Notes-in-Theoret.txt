1 This work was supported in part by the Concerted Research Action (GOA) Ambiorics 2005/11 of the Flemish Government, by the IAP Programme P6/26 BCRYPT of the Belgian State (Belgian Science Policy), in part by the European Commission through the IST Programme under Contract IST-027635 OPEN TC and IST-021186 RE-TRUST, and in part by a Ph.D. grant of the Institute for the Promotion of Innovation through Science and Technology in Flanders (IWT-Vlaanderen).

For all these applications, it is clear that only legitimate, untampered client applications should be granted access to a service. Hence an authorized entity wants to be able to verify if client software is running untampered on a remote untrusted platform. If tampering is detected, this verifier will want to disconnect the client from the network, stop the service to this particular client, or even force that client application to halt its execution.

So far, establishing a trusted execution environment on an untrusted platform has been an open research challenge. An adversary having complete control over an untrusted platform, implies that he also has control over its input and output traffic. This makes it difficult for a verifier to be assured of communicating with a particular environment on a untrusted platform. Even more: to be guaranteed software is actually running in that environment. For example, how can we detect if the software is running directly on the OS of the platform? Techniques like simulation, emulation, virtualization, or misdirection, are available to an adversary.

These issues were addressed by Kennell et al. [11] who developed so called genuinity tests, to verify if the hardware is real, and certain software is actually running. These tests leverage detailed knowledge about the processor of the untrusted platform, and are slow to execute on other processors or to simulate in virtual machines. In practice however, the proposed solution turns out to be not sufficient [20].

Eventually, when attestation systems are unable to guarantee reliable execution of software, one can move critical code away from untrusted platforms. Techniques such as program slicing split software into non-critical and critical code slices. Only the non-critical code is run on the untrusted platform, guaranteeing that the critical slices can not be tampered [4,5,25]. This is an example of server side execution.

In this paper, we address the problem of remote code integrity verification based on the latter two approaches. Since recent, a lot of TPM enabled computers are sold on the market. Therefore, we want to use them to address the problems of the pure software solutions, without the deployment of heavily adapted operating systems. In section 2, we focus on trusted computing platform based attestation. The opposite angle, purely software attestation techniques, is discussed in section 3. The new mixed attestation technique is presented in section 4. Section 5 concludes our results and presents future work.

The TCG sees itself as a standard body only. Neither does it provide any infras- tructure to fully utilize the technology, nor does it perform certification of any kind. The TCG specifications define three components that form a Trusted Platform 2 .

The TPM is the main component of a TCG platform and offers a physical true ran- dom number generator, cryptographic functions (i.e., SHA-1, HMAC, RSA encryp- tion/decryption, signatures and key generation), and tamper resistant non-volatile memory (mainly used for persistent key storage). Remark that no symmetric en- cryption algorithm is provided.

The initial platform state is measured by computing cryptographic hashes of all software components loaded during the boot process. The task of the CRTM is to measure (i.e., compute a hash of) the code and parameters of the BIOS and extend the first PCR register with this measurement. Next, the BIOS will measure the binary image of the bootloader before transferring control to the bootloader, which in its turn measures the operating system 4 . In this way a chain of trust is established from the CRTM to the operating system and potentially even to individual applications.

The operating system needs to measure the integrity of all privileged code it loads (i.e., kernel modules), because it can be used to subvert the integrity of the kernel; traditionally loadable kernel modules are used to inject kernel backdoors. How- ever, legacy operating system are monolithic, too big and too complex to provide a sufficiently small trusted computing base and hence they are prone to security vul- nerabilities. As legacy operating system can not guarantee a chain of trust beyond the bootloader, trusted computing initiatives opt for a microkernel or hypervisor in combination with virtualization to achieve both security and backward compatibil- ity.

Two techniques to detect memory copy attack have been proposed. A first approach is the measurement of the execution time of the verification function. Memory copy attacks introduce some levels of indirection, which imply extra com- putations that slow down the execution, and this behavior can be detected.

A second option is the usage of self modifying code to detect a memory copy at- tack [8]. If the verification function modifies itself, only the clean (i.e., untampered) memory copy, where memory reads/writes are pointed to, will be updated. Doing so, a verifier can notice that the execution, i.e., running the unmodified tampered copy, has not been changed, and thus detect the attack.

Secondly, an adversary could act as a proxy, and ask a faster computing device to compute the checksum on his behalf. We call these proxy attacks. To avoid this, in the Pioneer protocol, it is assumed that there is an authenticated communication channel between the verification entity and the untrusted execution platform.

This way, TEAS tries to address some of the shortcomings of Pioneer. Adver- saries are delayed due to difficulties in reverse engineering, and the unpredictability of the verification agent. The verification entity still keeps track of execution time to detect (hardware assisted) memory copy attacks.

In this section, we proposed a mixed solution, to obtain the advantages of both software based attestation and TCG attestation techniques. We present a practical solution to remotely verify software, based on software-only solutions like Pioneer and TEAS, combined with limited support of trusted platform modules. As such, we can invalidate the strong assumptions (which are unrealistic in some deployment scenarios), but avoid the need to deploy an adapted bootloader and secure operating system.

The solution can be further improved, if the TPM is used to report the processor specification. In this way some hardware attacks, where the processor or/and the memory get replaced by faster ones, can be detected during attestation. To achieve this extra feature, we propose to modify the bootloader. Bootloaders tend to be a lot smaller, and hence more trustworthy, than legacy operating systems: the OSLO bootloader [10] for instance is around 1000 lines of code, while a Linux 2.6 kernel contains more than 6 million lines of code. The integrity of the enhanced bootloader can be reported using standard TCG functionality. We still rely on timed execution to detect the compromise of legacy operating systems, given that the correct processor specification is known.

Although this protocol addresses a great deal of the issues raised in Pioneer, it still remains vulnerable to a proxy attack. A slow computer with TPM can send its timestamp TS1 to a fast computer that computes the checksum results. This result c is sent back to the slow machine that provides a signed attestation TS2 to the verifier. The network delay is captured by the computation profit. We provide two possible strategies to address this attack.

In the original protocol, a checksum is computed over the memory of the veri- fication function, which includes the send function. The verification agent can be modified to only accept messages from the verifier, based on the IP or MAC address. However, these addresses can be spoofed.

At the moment commercially available operating system only offer limited trusted computing support. At most they provide a TPM device driver, a TCG Software Stack and/or a TPM-aware bootloader. This however is insufficient to achieve remote attestation of individual applications. In the meantime, pure software based attestation schemes have been proposed for legacy platforms. They rely on the timed execution of a checksum function, that computes an application fingerprint. The execution time is measured remotely by the verifier, imposing heavy assumptions that are difficult to achieve in practice.

