The most fundamental communication mechanism for interaction is dialogues involving speech, gesture, semantic and pragmatic knowledge. Various researches on dialogue management have been conducted focusing on standardized model for goal oriented applications using machine learning and deep learning models. The paper presents the overview on existing methods for dialogue manager training; their advan- tages and limitations. Furthermore, a new image-based method is used in Facebook bAbI Task 1 dataset in Out Of Vocabulary setting. The results show that using dialogue as an image performs well and helps dialogue manager in expanding out of vocabulary dialogue tasks in comparison to Memory Networks.

The Information and Communication Technology (ICT) with which we interact in daily life is more distributed and embodied into the environment (the so called intelligent space) [1]. Especially, when designing ICT solutions for elderly peo- ple, who are very often critical towards new technology, distributed system can be even more challenging. To improve the Human Computer Interaction (HCI) with ICT solutions, a directed natural interaction and an emotional intelligence is very important [2].

This kind of face-to-face interaction can be provided utilizing avatars. Avatars have the potential to impersonate the used technology and thus increase the accep- tance of the software [4]. The interaction with avatars is able to provide multiple advantages. Avatars can, for instance, provide gestures, which in turn are able to increase the understanding of the presented information. Furthermore, the visual enrichment of verbal information i.e., adding a lip synched animated character to audio speech output, can increase the intelligence and enhance the robustness of the information transmission as known from natural speech. Therefore, a consistency between the visual and vocal output is of uttermost important [5].

(which is trained to output a response to user utterance) can generate textual reply to user. Later this textual reply is converted to speech by text-to-speech (TTS) synthesizer. Since ASR and TTS are not related to dialogue manager directly, they can be considered as complementary modules to complete dialogue solution [14].

The paper is structured into six sections. First section provides an introduc- tion of the Dialogue system followed by Embodied Interaction. The third section describes the existing rule based and machine learning based models in dialogue systems. Later, a new proposed methodology for the dialogue system is explained. The next section consists of the preliminary results and their discussions; and the last section concludes the findings of the work.

Several working groups and projects have been shown that embodied interactions can help to increase the interaction with ICT systems [1]. Bickmore et al. de- veloped a virtual laboratory to explore the longitudinal usage of a virtual exercise coach. Older adult participants interacted with an agent from their home once a day for up to 120 days. The results showed that users who interacted with an

The first chatbot developed by using rule-based system was ELIZA [18], which uses pattern matching based on user replies. In rule-based systems, human dialogues are modeled as set of states and dialogue manager has to choose replies for the conversation from the given set of rules [19]. This model has been used in many different applications such as restaurant booking or online psychological therapy chatbot [18]. In this method, a human, who is usually a domain expert analyzes the dialogue flow between human agents and tries to come up with predefined dialogue states and possible replies for each state based on patterns. The advantages of rule- based methods are that dialogue managers have control on selecting replies for the conversation and these selected replies from the full set of replies ensure that the user is not upset or offended, thus keeping the system consistent.

Reinforcement learning (RL) method is a machine learning method where agent learns to maximize notion of reward in given environment by learning which actions to take in each state [34]. Reinforcement learning methods gained popularity in recent years due to revisiting existing RL algorithms with new deep learning meth- ods. Deep reinforcement learning algorithms outperform humans in certain game tasks such as in Atari games, chess and GO [27].

a mapping between states and actions. Policy of an agent can either be stochastic or deterministic and actions policy maps to can be either continuous valued actions or discreet valued actions. RL agent dialogue manager is trained in order to learn a policy which maximizes the expected discounted cumulative return.

Dialogue modelling can be modeled as an Markov Decision Process (MDP), however, if dialogue task is complex and natural language is used, then state and action spaces are of very high dimension. Traditional RL suffers from curse of dimensionality and different solutions are investigated by researchers. One of solutions, is using hierarchical reinforcement learning, where agent learns to abstract problem. Agent in HRL setting learns to abstract state and/or action instead of learning primitive actions for each state. The agent usually has a different hierarchical levels of policies where high level policy controls which low level policy would be chosen; while each low level policy is optimized for a different and simpler task. Furthermore, the agent makes a decision on which policy it should choose from and then follow this sub-policy which is different from higher level policy until termination condition is satisfied [33].

We used Xception vision model on Facebook bAbI dataset Task 1 [21]. In this approach, we created a train set consisting of correct and wrong dialogue history and reply pairs. Later, we converted this textual dialogue to an image and processed it as a binary classification problem. For testing we rank all candidate answers for given dialogue history and choose the highest. This work is an extension of the work done [38] in Out Of Vocabulary (OOV) setting.

