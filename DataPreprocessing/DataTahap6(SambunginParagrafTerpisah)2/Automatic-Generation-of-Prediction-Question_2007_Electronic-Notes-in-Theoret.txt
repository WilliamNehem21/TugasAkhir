Based on previous research, it seems that the activities performed by and the engagement of the students matter more than the content of the visualization. One way to engage students to interact with a visualiza- tion is to present them with prediction questions. This has been shown to be beneficial for learning. Based on the engagement taxonomy and benefits of the question answering during the algorithm visualization, we propose to implement an automatic question generation into a program visualization tool, Jeliot 3. In this paper, we explain how the automatic question generation can be incorporated into the current design of Jeliot 3. In addition, we provide various example questions that could be automatically generated based on the data obtained during the visualization process.

According to Hundhausen et al. [6], the activities performed by and the engage- ment of the students matter more than the content of the visualization. Thus, a research program has been laid down in which the level of engagement (engagement taxonomy ) and its effects on learning with algorithm or program visualization are being studied [13]. One of the ways to engage students to interact with a visual- ization is to present them with questions, which ask the students to predict what happens next in the execution or visualization (level 3: responding) [12]. This has been shown to be beneficial for learning as well [3,11]. Furthermore, interaction and question answering during learning have been found to have a positive influ- ence on problem-solving ability in the given domain [5]. In addition to the benefits

Based on the found benefits of the question answering during visualization, we propose to implement an automatic question generation into a program visualization tool, Jeliot 3 [9]. In this paper, we explain how the automatic question generation can be incorporated into the current design of Jeliot 3. In addition, we provide various example questions that could be automatically generated based on the data obtained during the visualization process. Finally, conclusions and future directions are presented.

Jeliot 3 is a program visualization system that visualizes the execution of Java pro- grams [9]. It has been designed to support the teaching and leaning of introductory programming. Jeliot visualizes the data and control flow of the program. In a class- room study, it was found that especially the mid-performers benefited from the use of Jeliot while the performance of others was not harmed [1].

could allow the user to select the variables or expression types that should generate questions and thus focus the questions on the selected concepts or parts of the program. Similarly to related systems (e.g. Problets and WadeIn (see Section 4)), we could adapt the question generation, visualizations and explanations based on the performance data of the user. We have done preliminary work on this direction, and it is described in [7].

Kumar et al. [4] have developed a system, called Problets, that generates exercises related to programming concepts (e.g. loops, pointers etc.) from language indepen- dent templates, thus supporting multiple programming languages. These exercises present a program and ask the user to identify the lines that generate output and determine what is the output during the execution of the program. In exercises re- garding pointers, user needs to identify the code lines that are either syntactically or semantically erroneous. These exercises are delivered in the form of an applet that is connected to a server that handles the exercise generation and stores information related to the performance of the user. This is done in order to analyze what kind of exercises to present to the user.

When compared to the question generation in Jeliot 3, we can identify certain similarities and differences. Both ask questions related to the execution of a pro- gram. However, Problets are related to the program code, whereas questions in Jeliot can be related to the program code and visualization. This can give more variation in the question types as seen in Section 3. Jeliot supports dynamic as- pects of the program execution, for example, user can give input to the program and the questions are adjusted accordingly because they are based on the information acquired during the interpretation process. Currently, Jeliot supports only Java. However, if interpreters for other programming languages are integrated into it, the question generation is language independent. Problets support multiple program- ming languages because of the language independent templates that are translated to the programming language in question. Problets can be used for learning and testing similarly as the automatic question generation in Jeliot 3.

WadeIn II [2] visualizes the expression evaluation in C language. The sys- tem consist of two modes: exploration and knowledge evaluation. The question generation is related to the knowledge evaluation mode in which student needs to demonstrate the understanding of the expression evaluation by simulating it. The task is to simulate the evaluation of the expression, whereas in Jeliot, a user is asked to predict what will happen next in the given context of the program and its execution.

As future work, we implement the proposed question types and test their us- ability. We also plan to study the use of question answering both during individual as well as collaborative learning of programming concepts and programming. We will variate the level of engagement to analyze its effects to the learning and col- laboration. Furthermore, we can test how different types of questions support the understanding of programs and programming learning. For example, should the questions be related to data flow or control flow, or both. In addition to this, we are planning to use the feature in distance education as a part of the summative evaluation.

