Decision-Making (MCDM) problem, one for each ALM domain. Our domain-parametric recommender takes as inputs a domain, a process definition, and a set of tool evaluation criteria, and yields a ranked list of tools. The approach has been prototyped with the Testing domain and evaluated using a real process and project; the recommendations generated by our approach were quite similar to those of three Testing experts. Pending further evaluation, these results suggest that our approach can generate project-specific tool recommendations with results comparable to those of experts, but at a fraction of the cost.

The advantage of this approach is that any company that has formalized its software development processes can easily filter through a large amount of tools quite quickly (using a reduced set of criteria). Moreover, if a company evolves or tailors their development process [9], it is easy to check whether the same tools are recommended for the new process. Another advantage of this approach is that the tool and task taxonomies can be built incrementally.

In this article, we describe our framework as applied to the Testing domain. We have validated our prototype by using it to recommend tools for a real, previously documented process and project; the recommendations we obtained using our pro- totype were quite similar to those of three Testing experts. This article makes the following contributions: (1) We propose a domain-parametric, semi-automated tool recommendation framework that takes into account the project context and devel- opment process; (2) We have developed a testing tool catalog and its corresponding taxonomies; (3) We pose tool recommendation as a MCDM problem, which allows us to control the tool recommendation process through the specification of criteria preference levels.

The rest of this article is organized as follows. We give an overview MCDM and our approach in Sections 2 and 3. The Testing domain taxonomies are described in Section 4, and our expert study is presented in Section 5. After comparing our work with related approaches in Section 6, we conclude in Section 7 with a summary of the article and suggestions for future work.

At this point, we can use the tool catalog to determine which tools can be used to carry out the activities and tasks of the input Software Development Process. This is not a simple task, and if done manually, the team would have to install and evaluate each available tool individually. Instead, we rely on selection criteria to automate this step: the team indicates their preferences with respect to a limited set of selection criteria, and we use the process described in Section 2.3 to rank the tools.

Software testing is a complex process. There is a wide range of possible testing activities, and which activities are carried out (and how) depends on the testing approach and techniques used, as well as the type of software being developed. These factors make automated tool selection a difficult task; however, additional tool-specific factors like which testing activities are supported, usability, and com- patibility with other tools, makes the tool selection process even more difficult in practice. In this section, we describe the taxonomies used to define the Testing Tool Catalog.

We contacted three Chilean testing experts in order to validate our prototype. These experts all work in software development at SMBs, specifically in Software Quality Assurance. In this section, we first describe the case study that was presented to our three experts and we then discuss the results of this study.

Project 2: the SMB has been tasked with adding some new features (like processing credit card payments) to an existing web product catalog, which has also been developed in PHP and JavaScript. In this case, the new features have already been added and the development team is currently testing the GUI. However, the new features were directly added to a live system, so the development team wants to automate the testing process as much as possible in order to wrap-up the testing process as fast as possible.

This information was distributed to the experts via email. We also sent them the full list of the testing tools available in our testing tool catalog. The experts were then given a week to make their testing tool recommendations for these projects.

MENTOR in first place (Testlink). Expert 1 selected Tarantula as the best suited tool for this project, and listed Testlink in second place. Note however that this expert failed to read the handout carefully: Tarantula could not be used in this case because of the limited hardware available to the SMB. Furthermore, Xstudio appears in second or third position in most of the rankings.

With respect to Project 2, we see that there is an agreement with respect to the best suited tool (Selenium-IDE). Additionally, Apache JMeter appears second or third in most of the rankings. There is less agreement between the rankings as we analyze further recommendations; we believe that this is because the experts attempted to make their recommendations as complete as possible and included tools that can be used to somewhat carry out the tasks described in the handout, whereas MENTOR only included tools that were described as being able to carry out the tasks.

Recent years have seen a growing interest in the definition of knowledge domains as a way of sharing information and standardizing domains. These domains can be used to avoid differences in concept definitions, which facilitates tool integration, as well as the creation of new tools, so there has been a renewed interest in ontology definition [1]. An ontology defines the common vocabulary for a specific domain, providing a formal specifications of the domain concepts and the relationships be- tween them [15].

Several software engineering-specific ontologies have been defined in the litera- ture. Falbo et al. [4] presents a software process ontology that supports the acqui- sition and organization of software processes, as well as process knowledge sharing and reuse. Barbosa et al. [1] propose a testing ontology based on the ISO/IEC 12207 standard, which is used to aid in the definition of support tools, as well as enhance interoperability between these types of tools. As this ontology models tool

In the software testing domain, Nakagawa et al. [14] have focused on tool stan- darization by proposing a reference architecture for software testing tools. Zhu et al. [8, 31] have implemented a prototype multi-agent system that focuses on testing web-based applications. This system does not try to automate the testing process, nor does it recommend testing tools; however, we have used their testing concepts taxonomy as the basis of our testing taxonomy.

In others domains, Patel et al. [17] present a metodology that describes the is- sues and factors that should be taken into consideration for select Knowledge Man- agement tools. While Vafaie et al. [27] show a commercial-off-the-shelf (COTS) y government-off-the-shelf (GOTS) selection methodology, where vendors make a survey about tools characteristics. On the other hand, Maxville et al. [11] present a process for selecting COTS from repositories using evaluation techniques like goal/question/metric (GQM), Analytic Hierarchy Process (AHP) and Weighted Score Method (WSM).

Rivas et al. [21, 22] propose a selection model aimed at supporting software developing SMBs in the selection of project management tools based in ISO/IEC 14102 and Goal/Question/Metric (GQM) approach. Tran et al. [26] propose other tool selection process for SMBs, which consist in three phases: research, vendors communication and tools installation. Theses proposals need a lot manual work to implementation.

In order to validate our approach, we created tool and task taxonomies for the Testing domain, and we used these taxonomies to create a testing tool catalog. We asked three experts to recommend testing tools for a real, previously documented process and project. Our prototype was able to generate recommendations similar to those made by the experts, and in much less time that if we had manually researched each of the tools available in the tool catalog.

As a result of using this framework, we expect to see an improvement in software quality, since the testing activities of the process will be better supported. We also expect that the testing team will have more time to do actual testing, since they will no longer manually evaluate available support tools. Testing teams may even become aware of the existence of tool support for activities that they had never considered using tools for.

As future work, we will extend this proposal with additional taxonomies, so that MENTOR can make tool recommendations for additional ALM domains. We are also evaluating the use of other MCDM techniques, like Fuzzy [19], in order to improve the recommendation process, as well as improving the selection criteria evaluation method. Finally, we are populating the catalog with more tools, which will be made available on line.

