Task models are amongst the most commonly used models during interactive systems design. In this paper we address the abstraction level problem identified above by investigating the feasibility of using task models as oracles for model-based testing of user interfaces. We will be using ConcurTaskTrees (CTT) [14] as the task modelling notation. This has a number of potential advantages:

Task models are common for modelling GUIs. Task models may be used to describe typical usage scenarios in a higher level of abstraction. The goal of this research work is to make the GUI models used for model-based testing more abstract (when compared to, for example, [11]), and diminish the effort in their construction. We propose to achieve this by adopting task models as oracles.

Having decided to investigate how task models can be used as oracles in a model- based testing context, we must now choose a task modelling notation. We have chosen the ConcurTaskTrees (CTT) notation. This choice was based on a number of factors: CTT belongs to the family of hierarchical task analysis notations, the most common approach to task analysis; CTT itself is becoming a popular language for task modelling and analysis; and, finally, and most important of all, CTT has tool support. In fact, the TERESA tool[9] supports editing and analysis of CTT models, and a number of features relating to the animation of task models that will be useful in our work.

In order to automatically generate test oracles from CTT models we must further refine the type of tasks that are available. That is because the oracles will be expressed at a low level of abstraction and we want to include as much information as possible in the task model in a transparent manner (for example, regarding issues of window management). We decided to make use of the keywords from the Framework for Integrated Tests (FIT) [10], and structure the CTT models around them.

To use a task model as an oracle during model-based testing, we must be able to animate the model, in order to compare its behaviour with that of the actual application. From CTT task models we can generate task sets that represent which tasks are enabled at each particular point in the interaction. These are called Presentation Task Sets (PTS). PTS are automatically derived by TERESA through analysis of the formal semantics of the CTT temporal operators [9].

A simple window manager model is used to deal with the application windows being placed on and removed from the screen. Window manager behaviour, at the CTT level, is described by Show and Close actions, which are translated to the appropriate method calls in Spec# (e.g., WindowManager.AddWindow(...)). Additionally, for Press, Enter, and Display tasks, the pre-condition which checks if the current window is enabled is automatically inserted into the generated Spec# model.

An additional step that must be performed is the definition of the domains of the input parameters (the possible values each parameter might take). Spec Explorer does not provide support for the definition of these domains. At the moment this step has to be done manually. However, it would be possible to have that information incorporated in the CTT model, and have TOM extract it and process it automatically.

In this section we demonstrate, with an example, how the tool that is being de- veloped is able to generate Spec# oracles automatically from CTT models which respect the conventions defined in the previous section. The application being used to demonstrate the approach is the Windows Notepad editor. Due to space con- straints, we concentrate the analysis on the Find functionality of the Notepad.

Since we were dealing with a third party application, we started the process by developing a task model from scratch. This was done based on previous experience of using the tool, and on extended testing of all possible interactions with the tool. TERESA was used to write the model.

conditions are expressed as properties of the atomic tasks in the model. Two such tasks are Press Find and Press FindNext. In the first case, text in the Notepad must exist. Only if the pre-condition is true, is it possible to press the Find but-

This, again, is a fully automated step. In this case, performed by our tool (TOM). With the PTS generated by TERESA, and the original CTT model, TOM was able to create a richer version of the original FSM (expressed with the PTS) representing the intended behaviour of the model. This FSM is expressed in Spec#, and becomes the preliminary version of the oracle.

to refine manually the preliminary generated Spec# oracle. For example, the tester may opt to maintain the CTT model in a higher level of abstraction and refine later the generated Spec# model to describe, for instance, the behaviour of finding a word in a text. The above adjustments took only half a person hour.

At the start of the project, the vision was that of developing task models that would fully capture all possible interactions with the system. Several authors have looked into generating interactive systems from their task models (see, for example, TERESA [9] or Mobi-D [15]), so this would be the counterpart: testing the sys- tems against the task models. The analysis would be a complement to traditional analytic approaches to usability analysis, enabling an analysis of whether the final implementation obeys the envisaged (and analysed) design.

Task models capture idealised user behaviour. In practice, users may achieve goals in different ways and therefore oracles based on these idealised behaviours may fail to test the range of possibilities. Dialogue models capture dialogue con- trol within the application (i.e., all possible behaviour of its GUI). The original Spec# oracles in [11] are more akin to dialogue models than task models, since they captured all possible system behaviours.

becomes problematic since that information is not present in the task model. This is an issue that needs further analysis, but the envisaged solution is to annotate the task model with information about specific implementation patterns used. In fact, this has already been done in the case of modal windows.

Regarding the second, testing behaviours outside the task model will not be possible since the oracle being used will not be able to cope with them. However, because the testing process is mostly automated, once the testing set up has been created it is easy to envisage a scenario where different variations of the task model are used to test the system against. Hence, by applying appropriate transformations, we will be able to test the system against possible user erroneous behaviour. For example, we might alter the task model to allow situations were the user performs post-completion errors [7].

In a second phase, we will be interested in testing resilience (the capacity of the GUI to deal with user errors, and/or unexpected behaviours). For that, one possibility is to introduce changes into the task model to allow for those anoma- lous or boundary behaviours to be present in the task model, as discussed above. Alternatively a more dialogue control based oracle must be used.

In this paper we have shown that task models, despite limitations, can be used to generate oracles economically in an interactive systems model based testing context. Tool support is being developed that enables the automated generation of oracles from task models. An example of use has briefly been described, and the advantages and shortcomings of the approach have been discussed.

