One solution to reduce the mismatch between training data and unseen samples of a specific writer is to modify the model into a WD model. Writer adaptation is a process of converting the WI model to the WD model, given adaptation data from a specific wri- ter. The main challenge to writer adaptation is having limited data. A WI model with a large number of parameters can easily get over- fitted if adapted with limited adaptation data. Adaptation in deep learning is closely related to such techniques as domain adapta- tion, transfer learning and fine-tuning.

In this paper, we propose a novel writer model-based adapta- tion method that utilizes a CTC-based model to achieve optimal Arabic online handwriting recognition. Our method employs adversarial multi-task learning (MTL). Adversarial MTL techniques alleviate deep features mismatch between distributions of features generated by the WI model and the WD model. In adversarial learning, two tasks are involved: a primary task of label classifica- tion and a secondary task deep features discrimination. We evalu- ated the proposed method using two datasets; the CHAW and Online-KAHTT datasets. The proposed method has both consis- tently and significantly shown superior performance in compar- ison to the WI and fine-tuned WI models with the same data of target writers.

The paper is organized as follows: Section 2 explores previous work done in writer adaptation for online handwriting recognition. In Section 3, we explain the architecture of our proposed method. Section 4 details the results and provides a comparison of the pro- posed architecture against the original WI with both supervised and unsupervised adaptation settings. Lastly, Section 5 details our conclusions and recommendations for possible future work.

Several approaches to model adaptation have been proposed. In [26], Matic et al. designed a writer dependent system based on a pre-trained Time-Delay Neural Network (TDNN) character-based recognition system. Adaptation is conducted by altering the last layer of TDNN, which acts as an optimal hyperplane classifier. This last layer is retrained to a new writing style. In [28], authors placed an Output Adaptation Module (OAM) on top of WI neural net- works. OAM is a radial basis function (RBF) network that maps the output of the WI module into a writer-adapted output.

that adaptation could be performed deeply on multiple layers of the model, including the CNN and fully connected layers, rather than exclusively on the top layers. The authors referred to their method as Deep Transfer Mapping (DTM), entirely run in an unsu- pervised manner. Adaptation is achieved through the capturing of additional information from different abstraction levels. During the adaption procedure, adaptation layers serve as the linear transformation.

Deep learning methods were also utilized in features transfor- mation adaptation. In [9], researchers used deep learning features to perform writer adaptation for online Chinese handwriting recognition. In this method, DNNs and CNNs are utilized to extract these features (called tandems). Extracted features are then fed into the prototype-based classifier. A linear transformation of the deeply learned features is used to perform adaptation.

Designers of the aforementioned writer adaptation methods for online handwriting recognition systems primarily utilized tradi- tional techniques such as HMMs, SVM, TDNN and prototype classi- fiers. Despite that a small number of these methods did incorporate deep learning in their features representation, none of their creators tackled the use of E2E model-based writer adaptation. Languages addressed were primarily either Latin or Chinese languages.

is created and initialized by cloning WI, then divided into features extraction and label classification sub-networks. In the secondary task, a discriminative sub-network is attached on top of the fea- tures extraction sub-network in parallel with a label classification sub-network of the primary task. The two tasks are jointly opti- mized in a minimax manner with multi-task learning, where fea- tures extraction layers are shared among the tasks. The goal of a minimax game is to minimize label classification loss while jointly maximizing features discrimination loss. By the end of this game, the regularized WD model should reach a higher recognition rate against the test data of the target writer than that of the WI model. The WI is kept fixed during adaptation and discarded with the dis- criminative sub-network during the test.

Assuming that x is a sample instance that represents input data, then D(x) is the discriminator. D(x) is a binary classifier that pro- duces a probability with x as a real instance from the training data. D(x) should output a high probability with respect to real instances and a low probability with respect to synthesized instances. Simi- larly, assuming that z is a latent vector sampled from Gaussian dis- tribution, then G(z) is the generator that transforms z to the data space.

For the purposes of evaluating our method, we used standard metrics character error rate (CER) and word error rate (WER). We used the same handwriting samples from the Online-KHATT and CHAW datasets (see Section 4.1) that were used to train the base- line model. Samples used in the adaptation process were not used in the training of the WI model.

For evaluation, selected handwritings of 84 writers from the test sets of both datasets; 42 writers from each dataset. For each writer, we split his/her samples into training and test sets. From the CHAW dataset, we selected 40 samples (words) for training and 80 samples for testing (to be used in all experiments). The online-KHATT dataset is comprised of sentence-based samples. From that dataset, we choose four samples per writer for adapta- tion training and an average of 10 samples for testing.

to the improvement of results compared with the use of the basic characters only. In Arabic script, character shape not only differs according to position in a word, but is also influenced by both the preceding and succeeding letters. Character with position car- ries contextual information which helps in the decoding phase [4]. Adaptation training data would not however cover all 160 target units for each individual writer. The proposed adaptation method works on features representation and performs the distribution shift based on the features detected. Thus, our model is able to sat- isfactorily generalize, despite possibly not covering all target units of a specific writer.

In all our experiments, we used greedy decoding for the model outputs. It selects the most probable character over characters dis- tribution for each time step as described in Section 3.1. Using decoding techniques like beam search decoder with LM or dic- tionary integration could improve the recognition rate. In our case of data availability. Then, we can apply adversarial multi-task learning adaptation to these models in the same manner as was done in this work. In addition, data augmentation is used to increase the training data. Thus, synthetic data generation [13] for writers with style transfer would increase the adaptation sam- ples for writers, resulting in increased overall performance.

Our work is the first work that tackles writer adaptation for online handwriting recognition for E2E models. It will contribute to opening more doors for Arabic language applications which could be used in different domains. Also, The proposed method can be applied to other languages as well such as Latin languages. Adversarial multi-task learning adaptation is not limited to CTC- based models and it can be applied to other E2E models in the same manner such as attention and self-attention models. In the future, we will explore the possibility of applying adversarial multi-task learning on attention-based models. In addition, we will explore the effects of synthesized adaptation training data on the performance of adaptation.

