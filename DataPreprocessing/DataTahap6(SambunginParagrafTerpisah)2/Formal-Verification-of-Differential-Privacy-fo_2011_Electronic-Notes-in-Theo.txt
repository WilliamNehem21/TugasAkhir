Differential Privacy. Differential privacy is a promising approach to privacy- preserving data analysis (see [14,16] for surveys). This work is motivated by statis- tical data sets that contain personal information about a large number of individuals (e.g., census or health data). In such a scenario, a trusted party collects personal information from a representative sample with the goal of releasing statistics about

In a different direction, these sanitization algorithms are being implemented for inclusion in data management systems. For example, pinq resembles a sql database, but instead of providing the actual answer to sql queries, it provides the output of a differentially private sanitization function operating on the actual answer [29]. Another such system, airavat, manages distributed data and per- forms MapReduce computations in a cloud computing environment while using differential privacy as a basis for declassifying data in a mandatory access control framework [39]. Both of these are interactive systems that use sanitization func- tions as a component: they interact with both the providers of sensitive data and untrusted data examiners, store the data, and perform computations on the data some of which apply sanitization functions.

Formal Methods for Differential Privacy. We work toward reconciling formal analysis techniques with the growing body of work on abstract frameworks or implemented systems that use differential privacy as a building block. While prior work in the area has provided a type system for proving that a non-interactive program is a differentially private sanitization function [38], we know of no formal methods that can, in addition, prove that an interactive system using such functions has differential privacy. Applying formal methods to interactive systems ensures

As we are interested in formally proving that finite systems provide differential privacy, we limit ourselves to privacy mechanisms that operate over only a finite number of values. One such mechanism is the Truncated Geometric Mechanism of Ghosh et al. [20], which uses noise drawn from a bounded, discrete version of the Laplace distribution.

Actions. We model the input command in the source code with the input action set I of our automaton: for each possible value that input can return there is an input action in I corresponding to that value. Inputs in the code can be either queries or data points, which is modeled by the partition of the set I into the sets Q for queries and D for data points. We model the print command in the source code with the observable outputs R (responses) of our automaton. For each possible value that can be printed we have an output action in R. We model all other commands by internal (hidden) actions.

We desire a technique for drawing conclusions about the global behavior (execu- tions) of the system from local aspects (states, actions, and transitions) of the model. Faced with a similar situation, Goguen and Meseguer introduced unwinding relations to simplify proving that a system has noninterference [22]. We present a similar technique for proving that a system has differential noninterference. In particular we state what it means for a relation family to be an unwinding family and prove Theorem 4.5, which roughly states that the existence of an unwinding family for a given automaton implies that it satisfies differential noninterference. Our unwinding notion is probabilistic and approximate, which is in keeping with the notion of differential privacy. The novelty lies in the way we keep track of the privacy leakage bound, which evolves as the system evolves where the evolution is constrained by the differential privacy definition.

Differential privacy is a very active research field giving rise to new definitions and techniques at a fast pace [16,18]. For example, pan-privacy is a notion of differential privacy that gives differential privacy against adversaries that can ob- serve the internal state of a system, in addition to outputs [32]. Computational differential privacy gives certain differential privacy guarantees against computa- tionally bounded adversaries. Our definition of differential noninterference and the formal proof technique was developed from the definition of Dwork [13]. We think that our choice of probabilistic automata as a model would prove useful in extending the work of this paper to these new definitions as well. For example, algo- rithms such as stream-processing algorithms that have been subject to research from pan-privacy point of view can be naturally modeled using probabilistic automata. Similarly, probabilistic automata-based models have successfully been used in the formal analysis of cryptographic protocols against computationally bounded adver- saries [42,3,8].

The results of this paper represent progress towards developing a basis for the formal verification of differential privacy for systems, but leave open several inter- esting directions that we plan to explore in future work. While our related technical report [45] provides an algorithm for mechanically checking a restricted class of rela- tions from the proof technique, we hope, in addition, to create a decision procedure for our proof technique by extending prior work on decision procedures for proba- bilistic bisimulations [5,4,35,9] to make them produce a family of relations rather than a single one. We also plan to extend the theory to model and reason about higher level systems, such as computer systems of hospitals and other distributed systems [39] that allow interactions of the system with data providers and with data analysts, while protecting the privacy of the data stored and manipulated by the system. For example, airavat allows computations over data distributed in a cloud, and combines mandatory access control with differential privacy where dif- ferential privacy is used to facilitate declassification governed by the privacy error bound set by a data provider. Our techniques can currently apply to the verifica- tion of differential privacy property of the airavat system using a whole-system model. We are interested in exploring the computational model of airavat further to understand the interplay between the fine-grained access control mechanisms and the differential privacy mechanisms in stating the end-to-end information-flow guarantee of airavat.

