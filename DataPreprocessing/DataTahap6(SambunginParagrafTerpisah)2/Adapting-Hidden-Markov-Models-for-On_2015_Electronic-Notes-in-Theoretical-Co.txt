The capability for training models on multiple traces is growing in importance with the analysis of online interactions on social media and across shared systems and resources. For example, variations of HMMs have identified trends in group behaviour in Twitter data by adapting k-means clustering and the Baum-Welch al- gorithm to train on multiple users simultaneously [4]. This multi-input HMM (Mul- tiHMM) was an improvement (as we scaled up) on the computationally-expensive coupled HMM [19], which used a Markov chain to represent one user and the cou- pling of chains was the social interaction. Despite the computations saved by the MultiHMM, an improvement through incremental training of parameters would be advantageous for online learning.

ing jobs is useful for workload benchmarking and scheduling of MMPP-distributed tasks, but is only one part of the wider goal. By modelling servers as part of queueing systems, one can understand and (ideally) predict waiting times, variable load, sys- tem bottlenecks and resource allocation of modern servers in cloud and file storage applications. In section 4.4, we incorporate an MMPP into a first-come first-served (FCFS) queue and obtain mean waiting times for varying load. Fluid input mod- els, such as MMPP, can have discretised time variants, such as the HMM, which is simpler to model and offers similar powerful traffic analysis. In fact, an advantage of HMMs is the parsimonious nature which allows representation of time-varying, correlated traffic streams in workload models. In the next section, the fundamental HMM algorithms are introduced.

To understand the OnlineHMM, we must first analyse the processes that compose it. The first is an adaptation of the sliding HMM (SlidHMM) [9], which is based on a simple moving average and uses a sliding window technique for data measurement. The second is a multi-input HMM (MultiHMM) capable of training on multiple discrete traces simultaneously. The OnlineHMM attempts to merge both techniques and create a novel online learning workload benchmark.

The concept of a sliding window to update data sets on which HMM parameters are trained on-the-fly is appealing in terms of run-time performance and online workload characterization. By updating the data set with new arrivals and simultaneously discarding the oldest observations, one can measure time-variant processes parsimo- niously. The sliding window effect improves the incremental learning of IncHMM [11], which accumulates an increasingly large observation set as the outdated data points are included in iterative updates of HMM parameters.

The multi-input HMM (MultiHMM) [4] comprises of a k-means clustering algorithm and a weighted BWA, which trains on multiple discrete traces simultaneously and maintains accuracy w.r.t. comparisons of trace moments. We present the doubly clustered methodology and the full MultiHMM algorithm.

