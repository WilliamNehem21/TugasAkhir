With increasing speech technologies, as Apple Siri and Amazon Alexa, the speech separation problem has become more of an important and interest. Actually, speech separation is considered as a pre-processing phase that is used in many speech applications such as noise separation from speech signals, ASR and creation of speech databases. Actually, machines cannot recognize speech rel- atively well in noisy environments, there is a significant deteriora- tion in performance of recognition in crowding environment with sounds. Progress in the speech separation area relies heavily on the development of appropriate speech databases.

vary widely. In an acoustic environment like a cocktail party, it is difficult to follow one speaker in the presence of other speakers and background noises. Researchers have shown that the advan- tage of using visual streams beside audio signal as an assistant fac- tor to analyze mixed sounds in a noisy environment. As we have a limited resource, it is difficult to train the proposed model with datasets having millions of recorded videos so a subset samples were taken from TheOxford-BBC LRS2 Dataset.

The rest of this paper is organized as follows. We first review several related speech separation approaches in Section 2. Then, we elaborate our proposed model for sound separation in Section 3. In Section 4, we describe the experimental setup, report the exper- imental results, and discuss our findings. Finally, we conclude our work in Section 5.

There are many researchers who try to solve this problem using audio information only. Yannan et al. [1] demonstrated unsuper- vised co-channel speech separation. Their proposed DNN frame- work could well segregate speech from mixtures of two unseen speakers with different genders because the distance between speakers of the same gender is smaller than those between speak- ers of different genders. They used log power spectral an input fea- ture to DNN. They implemented the system under the MMSE criteria which minimize the mean squared error between the DNN outputs and the reference clean features of both female and male speakers. Their system achieved PESQ better than GMM (Gaussian mixture model).

There are some researchers have tried to separate sounds using reference audio of the target speaker. Quan Wang et al. [2] pre- sented a novel system that separates the voice of a target speaker from multi-speaker signals, by making use of a reference signal from the target speaker. They achieved this by training two sepa- rate neural networks. Their system consists of two separately

Ariel Ephrat et al. [3] designed and trained a dilated convolution neural network model which takes an audio of the mixed sounds, with all detected faces of each frame in the video as an input, and splits the mixed sounds into sets of separated audios for all detected speakers. The model used visual Embedding features to improve the source separation quality and also to support their model in keeping track with all speakers in the video. Their model takes from the user marked face of the target speaker which he/she wants to separate as a guide in separation process. They introduced a new dataset which consists of thousands of hours vides recorded from web. To generate the training data, they mixed clean audios of two different speakers. Their proposed network was imple- mented by using TensorFlow. It has high SDR compared with other previous visual methods when trained in their data sets. They showed that their audio system is robust to separate voices of the same gender. They presented the importance of visual informa- tion in increasing the system accuracy even if it is small.

The proposed model introduced architecture for speech separa- tion when a list of speaker is available where the number of speak- ers in this paper is two. It is considered as audio visual model that separates speech relative to face embedding features of each speaker. It could achieve better performance even using limited size dataset.

NN. U-net was used for medical image segmentation and its struc- ture is very shallow and simple as it repeat blocks of CNNs,Relu and maxPooling for decoding and encoding path so it is very easy to implement. It achieved high segmentation performance on var- ious benchmarks and also it can work well with little amount of data. It has better performance in case of using it in speech separa- tion and enhancement.

This module is used to guide the overview model in the process of separation as the model would retrieve the voice of the target speaker based on the his/her face features.Firstly a pre- processing step is applied on the scanned images of the recoded videos to prepare the input of this module. In this step, a pre- trained FaceNet network takes detected faces from input image as input and output is a vector of 1792 values which represent the most important features crossponding to each face. In machine learning, this vector is called embedding feature vector then the proposed module takes as an input the concatenation of face embeddings features which are 75x1792Xnumber of speakers, as the number of frames in the input video is 75 frames, and FACENET network obtains 1792 face embeddings features for each frame.

to cover a large area from the input and discover relations between elements of the input easily. The classical CNN consumes too much computation resources but the dilated CNN could reduce the time of training by 12.09 % [11] and also it has better performance in image classification and segmentation.

Quan Wang, Hannah Muckenhirn, , Prashant Sridhar, Zelin Wu, John Hershey, Rif A. Saurous, Ron J. Weiss, Ye Jia, Ignacio Lopez Moreno, VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking, Proceeding in International Speech Communication Association INTERSPEECH, Graz, Austria, 15- 19 September 2019.

