This short paper documents our initial investigation into the communication patterns present in the High Performance Conjugate Gradient (HPCG) benchmark. Through our analysis, we identify patterns and features which may warrant further investigation to improve the performance of CG algorithms and ap- plications which make extensive use of them. In this paper, we capture communication traces from runs of the HPCG benchmark at a variety of different processor counts and then examine this data to identify potential performance bottlenecks. Initial results show that there is a fall in the throughput of the network when more processes are communicating with each other, due to network contention.

Benchmarking systems allows comparisons to be viewed so that system designers can design and build the most suited system for their applications. Various bench- marks are available to ascertain the performance of a full system and its subsystems, and all of these tools have advantages and disadvantages. Benchmarking subsys- tems can be beneficial when system designers want to gain a greater understanding of said subsystem; for example MPI benchmarks include SKaMPI and Intel MPI benchmarks. SKaMPI is a benchmark for testing an inconnect using the Message Passing Interface (MPI) developed by the University of Karlsruhe [20]. The Intel MPI benchmarks provide a set of tests for point-to-point and global communication operations for variable message sizes [11].

0.26 Gbps; significantly under the 70.96 Gbps that can be achieved over the inter- connect. As Orac is a shared system, network traffic from other applications running on the system can cause external contention. However, this does not explain the large difference in performance we observe.

Rajovic, N., P. M. Carpenter, I. Gelado, N. Puzovic, A. Ramirez and M. Valero, Supercomputing with commodity CPUs: Are mobile SoCs ready for HPC?, in: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, ACM, 2013, p. 40.

