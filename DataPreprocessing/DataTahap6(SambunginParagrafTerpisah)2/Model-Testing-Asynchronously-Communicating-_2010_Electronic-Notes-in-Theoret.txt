Systematic testing is indispensable to assure reliability and quality of software and systems. Hosts of different testing approaches and frameworks have been proposed and put to (good) use over the years. Formal methods and program language theory have proven valuable to render testing practice a more formal, systematic discipline (cf. e.g. [16,2]). Formal approaches to testing have gained momentum in recent years, as for instance witnessed by the trend towards model-based testing [12,4]. In previous work [19] we presented a formal approach for black-box specification- based testing of asynchronously communicating components in open environments

We do this in the context of Creol [11,27], a high-level, object-oriented modelling language for distributed systems. Object-orientation is a natural choice, as object modelling is the fundamental approach to open distributed systems as recommended by RM-ODP [24]. For such systems an asynchronous communication model is advantageous as it decouples caller and callee thus avoiding unnecessary waiting for method returns. On the downside, asynchronicity makes verifying and testing models more challenging. In an asynchronous system, communication delays due to the network or to queuing may lead to message overtaking and the resulting non-determinism leads to a state space explosion.

This section describes two series of experiments, using the implementation sket- ched in the previous section. The experiments demonstrate the usefulness of the approach: using AC rewriting may considerably reduce the resource consumption, when testing asynchronously communicating objects. AC rewriting significantly pays off in terms of time and the number of rewrites. With regards to the state space, the effects are not so definite.

In the first example, the component under test consists of one object with n methods m1 through mn. The specification prescribes that all methods must have been called before any method may return. In Creol this is implemented by com- bining processor release points and await guards [27]. The behavioral specification for 3 methods reads:

Note that whereas the previous example illustrated generation of incoming calls to the component and testing of outgoing returns from the component, this example also includes testing of outgoing calls, and generation of incoming returns. For incoming returns, the test framework generates pseudo-random, type correct return values. For this specification a broker component would be non-conforming if it were to call the providers before receiving a call from the client and also if it were to return the initial call from the client before finishing its interaction with the providers.

We have presented a formalization of a concurrent object-oriented language and a behavioral specification language, for testing and validation of asynchronously communicating objects. Potential reorderings of communication events occur due to network properties. Our approach describes one way to deal with such situ- ations, namely by defining rewriting specifications modulo AC for output events. One advantage of this approach is that we can define precisely the scheduling of input, and test internal synchronization properties of the object. When evaluating our approach by experimental case studies we get evidence that using modulo AC rewriting enable us to cover more extensive test cases than we could do otherwise. Testing of Creol models is relevant also for testing of implementations in lan- guages like C or Java: First indirectly, since many forms of non-determinism inher- ent in distributed system can be formalized by means of associativity and commu- tativity, our results are relevant also for other languages with asynchronous com-

The paper [13] describes compositional analysis based on combining compo- nents with specifications. Also here VeriSoft is used for bounded model checking of assume/guarantee specifications, built-in partial order reduction contributes to effi- ciency of the analysis. However, both the object interaction model, shared variables, and the specifications, invariant based, using Hoare logic, differ from ours.

In [3] assumptions are used as environments to drive individual components for unit testing. LTSs are used to model the behavior of components. An interesting feature of this work, absent in ours, is techniques for automatic generation of exactly the assumptions that a component needs to make about the environment for some property to hold.

