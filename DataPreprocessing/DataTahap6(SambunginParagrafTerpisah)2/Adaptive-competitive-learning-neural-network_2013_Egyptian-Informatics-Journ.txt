Abstract In this paper, the adaptive competitive learning (ACL) neural network algorithm is pro- posed. This neural network not only groups similar input feature vectors together but also determines the appropriate number of groups of these vectors. This algorithm uses a new proposed criterion referred to as the ACL criterion. This criterion evaluates different clustering structures produced by the ACL neural network for an input data set. Then, it selects the best clustering structure and the cor- responding network architecture for this data set. The selected structure is composed of the minimum number of clusters that are compact and balanced in their sizes. The selected network architecture is efficient, in terms of its complexity, as it contains the minimum number of neurons. Synaptic weight vectors of these neurons represent well-separated, compact and balanced clusters in the input data set. The performance of the ACL algorithm is evaluated and compared with the performance of a recently proposed algorithm in the literature in clustering an input data set and determining its number of clus- ters. Results show that the ACL algorithm is more accurate and robust in both determining the number of clusters and allocating input feature vectors into these clusters than the other algorithm especially with data sets that are sparsely distributed.

The adaptive competitive learning (ACL) neural network algo- rithm uses a new proposed ACL criterion to determine the optimal number of output neurons in the CNN for clustering an input data set. This criterion is based on the theory that the best CNN architecture for clustering an input data set should produce a cluster structure that is composed of dense, well-separated and balanced clusters that have the minimum number of parameters to be estimated. To realize this theory, the ACL criterion selects the CNN that represents minimum within-cluster variations and minimum value for the product of the relative weights of clusters in the given data set. To introduce the notation, let D = {p1, p2, .. ., pn} be a given data set that consists of n feature vectors that are independently and identically distributed in R-feature space. The values on each feature are scaled such that they range from 0 to 1. This re- of clusters in it. The EMCE algorithm proves superiority to other algorithms in the literature in clustering an input data set and determining the number of clusters in it [12]. It uses mixture models and a criterion that is based on likelihood and mutual information theory for evaluating different mixture models with different numbers of clusters. The com- pared algorithms are implemented, and experiments are carried out using the MATLAB software. Different data sets with different cluster structures are used. These data sets are described in Section 4.1. The measure used to quantify how good the clustering results obtained from each algorithm is described in Section 4.2.

It consists of 178 feature vectors each of which is a vector in 13-feature space. These feature vectors represent three clusters whose sizes are 59, 71 and 48 feature vectors. The clusters are separated in the data space. The purpose of using this data set is to test the algorithms compared when data clusters are sep- arated and when the number of features is large compared to the number of feature vectors.

The Mutual Information is a symmetric measure to quantify the statistical information shared between two distributions [20]. Therefore, this measure is used to quantify how good the clustering results obtained for a certain data set is by com- paring it to the true classification of this data set [21]. Let X and Y be two random variables represent the true class labels [1..m] for a certain data set and the cluster labels [1..k] resulting from an algorithm for the same data set, respectively. The mu- tual  information  between  X  and  Y  is  defined  as

NMI has the value of 1 when there is a one to one mapping between the clusters obtained and the true classes (i.e., k = m) of a given data set. Since this measure is not biased to- ward large k, it is preferred to compare different data parti- tions [21,22].

Regarding the clustering results, the ACL algorithm produces the highest and the most robust results of the NMI measure with all data sets except the Iris and the Seeds data sets. With the Iris and the Seeds data sets, the EMCE algo- rithm produces the highest and the most robust results for the NMI measure as these data sets have overlapping among their clusters. In general, results of the ACL algorithm are accurate and robust in determining the number of clusters in all data sets used. With respect to clustering results, the ACL algorithm is accurate and robust with data sets that do not have overlapping among their clusters and are sparsely distrib- uted. These results show that the ACL algorithm is less sensi- tive to the curse of dimensionality than the EMCE algorithm. This is because the ACL algorithm depends on the Euclidean distance in the feature space, while the EMCE algorithm de- pends on the probability density function in clustering an input feature vector.

to as the adaptive competitive learning (ACL) neural network. It is based on a new proposed criterion for evaluating the clus- tering structure produced by the CNN, called the ACL crite- rion. This criterion evaluates the clustering structure by compromising the compactness of the clusters produced by the relative weights of these clusters. It prefers the cluster struc- ture that is composed of the minimum number of clusters that are compact, well-separated and balanced in their sizes. The ACL algorithm is compared with the recently proposed EMCE algorithm that proves superiority to other algorithm in the lit- erature in clustering an input data set and determining its num- ber of clusters. Different data sets with different cluster structures in terms of number of clusters, cluster separation and data sparsity in the data space are used. Results show that the ACL algorithm produces more accurate and robust results than the EMCE algorithm especially with data sets that are sparsely distributed. The ACL algorithm does not impose a certain cluster shape on the data set, while the EMCE imposes the Gaussian shape on data clusters. In addition, it overcomes the problem of the EMCE algorithm when the input data are sparsely distributed. This is because the ACL algorithm uses

