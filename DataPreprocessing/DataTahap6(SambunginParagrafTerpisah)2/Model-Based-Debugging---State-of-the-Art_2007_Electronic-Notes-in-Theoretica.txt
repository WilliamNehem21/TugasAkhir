A considerable body of work on model-based software debugging (MBSD) has been published in the past decade. We summarise the underlying ideas and present the different approaches as abstractions of the concrete semantics of the programming language. We compare the model-based framework with other well- known Automated Debugging approaches and present open issues, challenges and potential future directions of MBSD.

Translated to the software domain, substituting the program for the concrete system and observing its behaviour on a set of test cases seems possible. This turns out to be difficult in practice, as a formal description of the correct program is re- quired to detect discrepancies. Current practice in software engineering shows that formal models are rarely provided and if available, they often suffer from mainte- nance problems where changes in the desired functionality of the program are not reflected in the formal models. Further, formal models typically do not cover the complete behaviour of the system, but are restricted to a particular property of the program.

of the program. 3 The model components, a formal description of the semantics of the programming language and a set of test cases are submitted to the confor- mance testing module to determines if the program reflecting the fault assumptions is consistent with the test cases. A program is found consistent with a test case specification if the program possibly satisfies behaviour specified by the test case.

In case the program is found inconsistent, a set of components necessary to derive the inconsistency is computed and passed to the MBSD engine. The MBSD engine computes possible explanations in terms of mode assignments to components and invokes the conformance testing module to determine if the explanation is indeed valid. This process iterates until one (or all) possible explanations have been found.

In contrast to the ETDM, the Detailed Dependency Model (DDM) does not rely on program execution to build the model representation. Instead, static analysis techniques are applied to analyse the program with respect to control and data flow, including dynamically created data structures. All possible executions must be considered.

[39] present a dependency-based model designed to locate faults in a subset of the VHDL programming language. The semantics of VHDL are based on the notion concurrent processes which may be triggered by changing activations of signals and may in turn change the activation of other signals. Dependency models of a VHDL program express concurrent processes as components and the signals used and changed by each process as connections. Similar to the DDM and the SDM, cyclic dependency graphs are collapsed into a single node.

Kuper [26] introduces an interactive debugger for LISP programs, Debussi, which is also based on dependencies between expressions. The model constructs a simple plan (essentially a dependency graph), representing the expressions and subexpres- sions computed during program execution and their interdependencies. The reason- ing strategy used to identify potentially incorrect expressions is based on constraint suspension [14] to exonerate conditional expressions that cannot be responsible for a fault. Simple heuristics to exclude unlikely explanations are used to filter candidate explanations.

The models discussed previously are able to locate faults that involve incorrect ex- pressions, but do not provide the means necessary to locate faults involving missing statements or assignments to wrong variables. To locate such faults, complementary models not derived from the program are necessary. Ideas introduced in [36,33] pro- vide a first attempt at exploiting simple specifications [21] defining the relationships between input variables and result variables of a method.

A possible remedy is to strengthen the model representation such that conflicts can be derived in some of these cases, excluding spurious explanations.This section surveys some of the approaches to strengthen the model representations and conflict extraction procedures.For brevity, only differences to the dependency models are described. As before, our focus is on Java models, glossing over similar developments for VHDL programs [40].

ceived). The model essentially simulates the program in case all values necessary to compute a new value are known and does not predict any value otherwise. In- consistencies are derived when two differing values are derived for the same model variable.

The VBM can effectively handle a variety of programs for which the dependency- based models provide little advantage compared to slicing. For data structures where different instance variables are processed by different sections of the pro- gram, the heap-partitioned model provides much improved explanations. However, soundness of the results depend on the absence of variable faults on the left hand side of assignments. The key advantage of the VBM compared to dependency-based models is its ability to approximate the control flow of programs more precisely and to derive contradictions even for branch-free executions.

The conflict extractor can be described as follows: The model is represented as a flow-graph and is partitioned into regions with a single entry point. In case an inconsistency is detected in a region, the entire region is marked inconsistent and a different path must be followed. Once the region containing the entry point of the program is marked inconsistent, there is no consistent execution and the set of components associated with the outermost region is returned as conflict.

[24] introduce a synthesis between Predicate Abstraction [4] and the VBM. When- ever the plain VBM cannot derive a conflict, the PAM is applied to the regions of the model where no values could be predicted. A conflict is returned if a set of predicates can be derived that are sufficient to prove that the model is inconsistent. While the abstraction refinement approach has been shown to perform well for the purpose of verifying programs [9], the impact of under-specified program el- ements introduced by fault assumptions on the refinement process remains to be

The HOM encompasses a catalogue of abstract properties, each associated with a plain text description for interacting with the user. Properties represented by the HOM include among others: (i) variables and data structures should (not) be modified between two points in the execution, (ii) data structures should always be (a)cyclic, or (iii) a loop should iterate over all elements of a data structure. The benefit of such high-level specifications is twofold: (i) any fault candidate violating the property is eliminated, and (ii) the conformance tester can exploit those prop- erties to build more precise models and to better approximate the consistency test. The properties have been confirmed to exclude a number of spurious but difficult to eliminate explanations on a set of toy programs. Thorough evaluation on a larger set of programs remains future work.

Proof. Given that the models apply the same reasoning strategy and model rep- resentation, it is sufficient to examine the heap abstraction and approximation of dependencies applied by each model. While the ETDM represents only a single execution, the DDM safely approximates dependencies in all possible executions. All dependencies modelled in the ETDM must be contained in the DDM. It can be seen that the representation of dynamic data structures in the DDM (SDM ) is de- rived from the ETDM (DDM) by aggregating heap locations and adding additional dependencies for summary locations. It follows that the set of dependencies derived for the precise models are all included in the abstract models.The more abstract model is a safe approximation of the more precise model. It follows that whenever the precise model is consistent, so is the abstract model.

If only a single variable is observed to be incorrect, dependency-modes lead to the same results as slicing. When restricted to single fault explanations, the expla- nations are precisely the ones contained in the intersection of the individual slices for each incorrect variable [41]. Otherwise, the model-based approaches can improve the results compared to purely slicing-based strategies through the application of specific fault models for components.

Proof. Both the LFM and the PAM extend the VBM with additional constraints that restrict behavioural models of components.Extensions are applied when the VBM alone is consistent to refine the approximation of the consistency test. It follows that the specialised models derive a superset of all conflicts obtained from the plain VBM. Therefore, the VBM is consistent whenever the specialised models are consistent.

Proof. The AIM can be seen as a dynamic unfolding of the EM, specialising the control flow graph to a subset of the paths in case some paths cannot be realised in a program state. The interval lattice is strictly more expressive than the simple lattice used in the VBM. Therefore, the AIM has a potentially larger set of control locations, each more specialised than the EM and annotated with at least the amount of information as the EM provides. It follows that consistency of the AIM model implies consistency of the EM model.

Injecting different values in the programs state can be seen as a special case of fault models of a program statement. While DD uses values taken from a different program execution, MBSD does not specify exact values. Instead, generic place- holders ( ) are used, causing the program simulator to follow all possible paths that are consistent with the under-specified program state.

Well-known verification techniques have recently been applied to not only verify correctness, but also locate a fault [18]. The basic principle is to relate abstract execution traces leading to correct and erroneous program states. By focussing the search on traces that deviate only slightly from passing and failing test cases, likely causes for a misbehaviour can be identified. [25] compare the error trace-based strategy to MBSD and conclude that the former is sensitive to variations of the search depth limit used to restrict the search. Conversely, counterexamples may provide more information to the developer provided the trace is short.

Despite considerable progress in MBSD, many challenging issues remain to be solved. Some of the aspects presented below are particular to the MBSD approach, while others are instances of problems common to many different automatic debug- ging techniques. We do not claim that the list is complete; however, it reflects a number of issues we feel are important for the further development of MBSD and automated debugging in general.

How to select appropriate models and fault assumption given a program and test cases? Both the complexity of the debugging process and the quality of the result directly depend on the selected model. [42] provides a set of heuristics as a first step towards automatic model selection. It remains an open issue if techniques developed for semi-automatic verification of programs can be adapted and extended to suit the MBSD framework.

This work briefly introduces the idea of Model-based Software Debugging and com- pares individual models. Relations between different models have been studied, leading to a hierarchy of models with different diagnostic characteristics. The com- parison was focused on diagnostic strength of different models, a more in-depth analysis quantifying the differences between models remains for future work.

