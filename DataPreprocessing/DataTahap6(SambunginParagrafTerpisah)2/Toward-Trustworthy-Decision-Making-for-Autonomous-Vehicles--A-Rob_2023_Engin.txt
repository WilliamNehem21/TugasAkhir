Here, we provide a detailed introduction to the implementation specifications of the proposed technique. Algorithm 2 outlines the RRL-SG approach for trustworthy autonomous driving decision-making. The initial model parameters for the actor, adversary, and critic were set using a random distribution. In terms of interaction with the environment, our agent

Designing the state, action, and reward functions of the autonomous driving agent was essential to implement the proposed scheme. In this study, we consider the relevant states of the six nearest social vehicles in the ego vehicle lane and adjacent lanes as observations for the autonomous driving agent (i.e., the ego vehicle). The state space of the autonomous driving agent has 15 dimensions, including the relative distance and velocity of the surrounding social vehicles and the velocity, acceleration, and lane index of the ego vehicle. The lane index is the index of the lane where the ego vehicle is located.

The experimental space was free of static or dynamic obstacles, implying that Hunter should be able to maintain a straight run without attacks from an adversary model. We assessed each policy model during the period (150 time steps) in which Hunter drove from one side to the other. In the test case with adversarial attacks, the attacks started at the 75th time step. The hunter can execute five decision-making behaviors: turning right, turning left, maintaining the current state, accelerating, and decelerating.

