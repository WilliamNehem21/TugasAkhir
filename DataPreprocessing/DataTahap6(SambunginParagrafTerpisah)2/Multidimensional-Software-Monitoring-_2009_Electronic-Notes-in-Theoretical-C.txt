Measuring the quality of software is gaining more and more attention from the industry, since high quality systems generally have a lower total cost of ownership. In order to give both managers and software developers insight in the quality of the software system they develop, a software quality monitoring system is needed. The metrics calculated on the software system need to give a comprehensive, yet meaningful report. The Software Improvement Group has developed the Software Monitor, an analysis and on-line reporting tool for monitoring the quality of a software system during development.

At the Software Improvement Group (SIG), we specialize in reviewing and moni- toring the quality of software systems. In our experience, most development teams can give a rough estimate of which parts of their system are difficult to maintain, but these estimates are no more than educated guesses. On a management level we see that the demand for a quality-review is often available, but the knowledge to perform such a review is lacking.

R3 Furthermore, to easily navigate to problematic areas in the code, the reporting side must offer a drill-down functionality. To offer different views on a system this drill-down is preferably available on multiple dimensions, i.e. both over the file-system as well as over a logical (language-specific) hierarchy. For example, we imagine that a Java-project has a package-hierarchy linking levels of pack- age/file/class and methods.

OLAP-cubes are commonly used to store measurements, numeric facts, categorized by dimensions. The usual example is the tracking of sales of different types of products in stores across multiple countries. By keeping track of the sales and the properties of the products, the management of the company is able to get reports of the sales by product, by product-category or by store. Additionally, OLAP-cubes provide aggregation on each dimension, making it possible to get reports on sales by city, region and the entire country. Furthermore, dimensions and aggregation can be combined in order to generate a report on, for example, the number of items sold within a city grouped by product-color.

In our case the measurements are calculated metrics. The dimensions support the storing of metrics in, amongst others, a file-system dimension, a language di- mension and language specific dimensions such as the package-dimension for Java. By default, an OLAP-cube has a time-dimension, providing us with an easy way to store metrics of multiple versions in a single location.

By using an OLAP-cube as a data-store, the result of multiple calculations is stored in a single place. Because the cube supports aggregation over dimensions, the data can be shown both on the system-level as well as the code-unit level. Furthermore, the ability to combine multiple dimensions at will makes sure that the reporting side can be quite flexible.

ends with .java. The Java-source-context knows which parser to use to process the file-node and determines which package a node belongs to. When the node for this package is already in the graph, the node is reused, otherwise a new package node is created. The file-node is linked to this package-node by creating a new edge representing the package-to-file-relation. Furthermore, for each class within the file, a node is created and linked to the file-node. Note that this process is not limited to file-nodes. The Java-source-context also extracts method-nodes from a class-node and links these to the parent class-node.

. . . ) to use and the dimension (file-system, package, . . . ) to traverse. It is allowed to use the same technology in combination with several dimensions as well as to use the same dimension for several technologies.

Just like software written in any other language, custom ABAP reports and forms have to be maintained. All the pitfalls that apply to development in e.g. Java also apply to ABAP. Maintaining the custom forms in a SAP system should take as little effort as possible. To achieve this, one would need to write ABAP code that is as easy to maintain as possible.

can verify all programs, forms and reports written for a SAP system. The code inspector is a static analysis tool which can check for errors in the ABAP code. The code inspector has access to meta information about the ABAP programs, and can check whether the code adheres to naming standards, or whether programs are unicode enabled (this allows the programs to be deployed globally) as well as many other checks.

During the development of the custom SAP modules, the complexity of the system is changing constantly. The complexity, volume and amount of duplication in a system play important roles in influencing the maintainability of the software system. The Maintainability Model (MM) [2] gives us insight in which source code properties of a system attribute to the maintainability of the software system. In this model, the source code properties are all mapped onto the ISO 9126 standard [4].

are always calculated for the systems under analysis. Furthermore, the volume per unit, cyclomatic complexity and duplication are related to the total size of the sys- tem, and grouped in bins. This shows for example the percentage of the system that has a McCabe value between 1 and 20, which percentage has a McCabe between 20 and 50, and which percentage has a McCabe of 50 and higher. Similar grouping can be done for the size of the units in the system.

Besides hard-coding, the number of deprecated keywords and statements used in the system are counted. During the development of a SAP system, one expects the number of used deprecated keywords to decrease over time. An increase of this metric shows that developers are still actively using deprecated functionality. The SAT also calculates a number of metrics that count bad practices in

In this section, we discuss related solutions for measuring the quality of a software system in terms of metrics. These solutions can roughly be divided into two sep- arate groups. The first group contains the solutions that can serve as a complete replacement for our Software Monitor System. In Section 5.1, we discuss the char- acteristics of these tools and the reasons for not using them. The second group of tools contains existing front-ends for OLAP-cubes which in theory can be used with the cube that is generated by our SAT. The reasons for building our own front-end instead of using one of these existing solutions are given in Section 5.2.

There exists a large collection of stand-alone programs that is capable of calculating metrics on source-level. A simple search on sites that index open-source programs, for example sourceforge 1 , returns over 100 results. Unfortunately, most of these tools do not support more than three languages. Furthermore, the functionality of

A second set of solutions that is available is the editor-support for calculating metrics on source-files. This support can either be given by plug-ins or by the editor itself. The problem with this approach is again the lack of system-level overview. Also, the results are usually only available within the editor itself, making it harder to share the results with others.

Currently, a single interface for every user of the monitor is deployed. However, we imagine that different users are interested in different views on the system. Also, more experienced users could benefit from a more dynamic and interactive interface. Because of this, we are exploring the possibilities to differentiate the functionality of the monitor based on the role of the user.

Furthermore, for most of the calculated metrics, the semantics on different levels can be defined by a relation to the numbers on lower levels. For example, the number of lines of code for a module is simply the sum of the lines of code of each file in the module. However, there are some metrics, such as the fan-in, that must be calculated differently based on the level in the dimension. Currently, this type of metric is only available in a separate report instead of in the monitor.

Finally, the relation between a project and other similar projects can also help to better interpret the results of the monitor. In order to support this we are building a benchmark application capable of relating the characteristics of several similar projects. Finding out which metrics are relevant in this context is still work in progress.

