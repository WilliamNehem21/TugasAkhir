Feature selection plays a significant role in selecting informative, relevant, and non-redundant features from a large feature space during data preprocessing. After a comprehensive study of existing feature selection techniques, we observed that many wrapper-based FS methods ignore feature redundancy, and filter-based methods select features with some redundancy among the selected features. Moreover, the computational cost of wrapper techniques is high, and the selection of an optimal subset of features from a high-dimensional dataset is a major problem for ML researchers. Among existing FS methods, incorporating a single objective function into the standalone FS method does not yield satisfactory results for high-dimensional data. To overcome these problems, we developed a hybrid feature-selection method combining the concepts of PSO optimization as well as Mutual Information (MI). The PSO optimization is applied to a distributed dataset to select optimal features from each distribution and then combine the solutions of all distributions using MI, which yields the best subset of features. The proposed hybrid model integrates both the filter and wrapper methods. As the wrapper method consumes a significant amount of time, we applied it in a distributed manner. The wrapper methods are executed in parallel with multiple objective functions during the feature subset evaluation. This step significantly reduces the computational cost. Moreover, for each partition of the original data, the proposed method employs a filter FS using MI and combines the features from each partition to obtain the best set of optimal features. The main advantage of the proposed hybrid model is that it reduces the computational cost of feature selection by the parallel execution of the method on multiple cores.

The remainder of this paper is organized as follows. Related studies and existing FS methods are discussed in section 2. The PSO algorithm is described in section 3. The proposed HDFS(PSO-MI) method and the corresponding algorithm with a working example are described in Section 4. Experimental results and analysis are presented in Section

As previously mentioned, a single objective function may be biased in selecting the best subset of features from a distribution. Hence, we used multiple objective functions for an effective analysis of features in different data distributions. To define the objective function, we used the accuracy value obtained from the SVM classifier using the corresponding features the PSO swarm considers. This accuracy value plays a significant role in selecting the best subset of features, along with other parameters such as relevance (Rel), redundancy (red) of the features, and the ratio between the number of selected features and the total number of features. The major disadvantage of applying multiple objective functions in HDFS (PSO-MI) is that it takes an exponential time on large datasets to evaluate features with the three objective functions incorporated into the PSO.

We developed an efficient hybrid feature selection method, known as HDFS (PSO-MI) using PSO and MI. This method is highly effective in selecting an optimal subset of features that can yield high classification accuracy on different datasets. Although this method does not consider the class imbalance problem during feature selection, MCC values ensure that the selected features on various datasets provide a significantly high classification accuracy. The main advantage of the proposed hybrid method is that it can select optimal subset features from a high-dimensional dataset by evaluating possible subsets generated by PSO, where SVM is used as an evaluator of the subset. Hence,

PSO ensures that the best subset of features can be selected. In the next phase, the method evaluates each feature of the selected subset to compute their ranks in terms of their MI scores. Thus, the hybrid method always selects the best features from the entire feature subset. The experimental results demonstrate that the proposed method significantly reduces the dimensions and redundancies of high-dimensional datasets. In addition, the feature subset selected by the proposed method yields better results than the existing filter-based FS methods, that is, mutual information, gain ratio, Spearman correlation, ANOVA, Pearson correlation, and the ensemble feature selection with ranking method using five ML classifiers: LR, k-NN, SVM, DT, and RF. Although the proposed objective function enhances the PSO algorithm, the computational cost is slightly higher than those of the two existing objective functions used in our HDFS(PSO-MI) method.

