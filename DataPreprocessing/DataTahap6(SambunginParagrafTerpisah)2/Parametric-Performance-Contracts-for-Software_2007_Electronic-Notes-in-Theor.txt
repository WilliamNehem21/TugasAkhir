Many existing performance prediction approaches are based on analytical models such as queuing networks, stochatic Petri nets, or stochastic process algebras [2]. Although multicore systems can be modelled with these formalisms, reality is often inaccurately reprensented by them because of hard underlying assumptions. For example, most of these formalism assume exponential distributions for execution times or only compute response times as mean values [3]. Because of the many influencing factors on performance in large enterprise systems, mean response times are often not useful. Predicting response times as distribution functions can be more useful to support design decisions.

The contribution of this paper is an initial approach to include multithreaded behaviour in component-based performance predictions. We report on a first case study to validate our approach in specific situations. In the case study, we have compared predictions of our method with measurements of an implemented example system. Although simple, the case study was suited to reveal more challenges for the prediction of the performance in multicore systems, such as CPU hopping and cache thrashing. We have summed up lessons learned from measuring a dual-core system and provide directions for future research.

During early development stages of a component-based software system, reasoning about the quality attributes of the system has to be based on models, since an implementation is often not or only partially available. It is an established prac- tice in the software performance engineering community to use a design-oriented model for specification and transform it into an analysis-oriented model to predict performance attributes [2]. The design-oriented model is often based on UML as the de-facto standard modelling language and uses extensions like the UML SPT profile [5] and self-defined semantics to include information related to performance. We follow the approach outlined above and base our design model on UML keep- ing in mind that component-based development usually involves several developer

To predict the response time of a service, the design model is transformed into an analytical model. We use stochastic regular expressions (SRE) [4] for this pur- pose and extend them with an operator for parallelism. The transformation maps the structural elements of activity charts to regular expressions. Performance rel- evant information present in the design model, like branching probabilities, loop iteration functions, and random variables for time consumption, are passed to the corresponding elements of the resulting stochastic regular expression.

bility to each option. For each loop, a probability mass functions characterises its number of iterations. The new operator parallel extends the usual regular expres- sions to model the parallel execution of two independent tasks. It can be interpreted as the forking of two threads or processes and their joining. For sake of simplicity, we omit synchronisation mechanisms like semaphores and monitors.

The parallel operator combines the computation times of its child nodes to optimally use the available processing resources. Each child expression might itself contain concurrent parts (e.g. it can be a parallel operator itself) and can thus contain multiple computation times. The mapping of all incoming computation times to the available processing resources should be optimal meaning that the maximum of all time consumptions is minimal for all possible mappings. In other words, the execution time of the parallel expression is minimised by using as much parallelism as possible.

To sort a list of N computation times and determine the PMFs of the new random variables, we proceed as follows. For each incoming computation time Xn, the new PMF is set to Pn(X = t), which contains the probability that up to time t at least n tasks are computing. In the unsorted set, each random variable is specified by a PMF Qn(X = t). From this PMF, the probability that the task is still computing at time t Q(Xn > t) is determined as well as the probability that

For the behaviour of the system, we assume that a task cannot switch the CPU. Additionally, scheduling is assumed to be optimal, as tasks are immediately sched- uled to free CPUs. Furthermore, we neglect the overhead created by task switching. So far, we do not model locking or synchronisation mechanisms, since we are fo- cussing on the influence of the concurrent execution of independent tasks on perfor- mance. We consider only CPUs of the same type and do not include other resources, such as memory, disks, or networks.

The case study described here intends to analyse the validity of the new parallel operator, and thus does not model an industry size architecture. Instead, we use a rather simple architecture employing concurrency. We are planning larger case studies in the future.

by different algorithms. Two computationally complex, but less memory intensive algorithms were implemented. Of those algorithms, one calculated an array of prime numbers larger than a given integer (Primes). The other one calculated a fractal (Mandelbrot). Furthermore, two algorithms were using a large amount of memory. The first algorithm generated a large array of random numbers and sorted them (Sorting). The second algorithm performed a fast Fourier transform on a probability mass function (FFT).

We implemented the architecture and the four algorithms described in the previous section in Java. The measurements discussed in the following were performed on a dual-core Pentium D with 3 GHz and Windows Server 2003 as operating system. During measurement the provided service of the client component was called repeat- edly 500 times for all scenarios, and the response times were saved as probability mass functions.

We adjusted the parameters of the algorithms (e.g. number of generated random or prime numbers) so that their response time for a single execution with one active core was about 50ms (short) or 500ms (long).We chose the following independent variables for the experiments: (a) memory intensive vs. CPU intensive algorithms,

Interestingly, some of the measured execution times exactly match the predicted ones. These values are outliers and form a second peak in the distribution function of the measurements. This behaviour could be observed for completly different algorithms (Mandelbrot and Primes), and is thus not related to characteristics of the code. The scheduling algorithm of the operating system is a possible explanation for this behaviour. The outliers are measurements from tasks that have not been moved among the processors. In these cases, the actual execution time matches our prediction. In general, our prediction match the worst-case execution times in this scenario, and thus should nevertheless be suited to make rough estimations and support early design decisions.

Coming back to the initial questions of the case study, we can state for the first question, that for algorithms with a low memory footprint, we can make accurate predictions (even if they only reflect the worst case behaviour). However, for algo- rithms with a high memory footprint our approach would yield inaccurate results. First, we did not include memory access in our model. Thus, we would have predicted similar execution times for memory intensive algorithms as in the first scenario. As we have seen in the second scenario, the memory bus can become a bottleneck and, thus, has to be modelled for accurate predictions. Second, we did not include internal information about the CPU, like cache misses or pipeline invalidations. However, we can predict the response times for the parallel execution

On the analytical side, queueing network models, stochastic Petri nets , and stochastic process algebras like PEPA [14] are among the most established analyti- cal performance prediction approaches [11]. Even though these models provide com- pletely different formalisms for system specification, they are mostly transformed to continuous time Markov chains for analysis. This is followed by a lot of well known mathematical assumptions that limit the applicability of these models. Exponential distributions for time consumptions and the Markov property (transition probabili- ties/rates depend on the current state only and are independent of the history) are the most important ones.

It utilises design models in UML provided by component developers and system ar- chitects and transforms them into an analytical model based on stochastic regular expressions. We introduce a new parallel operator to analyse multithreaded be- haviour. A case study shows that the approach is able to predict accurate response times if threads do not change CPUs, the scheduling is optimal, no synchronisation is necessary, and the effect of cache thrashing can be neglected.

Our case study shows that the approach is still very limited in predicting mul- tithreaded behaviour. The effects of CPU hopping and cache thrashing vastly dis- tort the results and need to be included into our analytical model in the future. Moreover, synchronisation mechanisms such as semaphors or monitors do have an important influence on the performance of a component service and have been ne- glected so far. Larger case studies with industrial size component-based systems on multicore processors are needed to validate the approach further.

