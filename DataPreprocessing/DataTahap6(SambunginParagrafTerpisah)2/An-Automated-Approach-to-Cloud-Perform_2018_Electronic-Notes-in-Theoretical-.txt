Cloud infrastructure has been supplied for many years as an on-demand service across numerous different providers. The different instance types are provided with hardware specifications, but no concrete per- formance metrics. Benchmarking virtual machines is one method to contrast and compare between the different services offered across several providers. This paper presents a tool which permits the automated collection of cloud infrastructure performance information, to compare against the cost of these services.

The motivation for this work is to gain insights into how the performance of cloud infrastructure varies between instance types and providers. This offers an in- depth exploration into the real performance of cloud offerings, rather than making assumptions based on the listed hardware specifications and historical performance. With improved estimated execution times for jobs running on the cloud, there should be fewer delays waiting for an instance to become available. Jobs with deadlines can be better scheduled by knowing whether the execution time on a higher performance instance type is worth the increased cost. If a particular job is estimated to complete with sufficient available time remaining on a provisioned instance, better estimates could permit another job to be allocated to make the most of this remaining time. To fulfil this aim, a tool is created which can be used to autonomously collect computational performance results from cloud instances across different providers. By handling the entire testing process, from provisioning new cloud instances to extracting results without leaving resources running, the tool is designed with ease of use in mind. The initial implementation stresses the computational power of cloud infrastructure, focussing on CPU and memory performance, however the intent is for the tool to act as a harness for any benchmark suite. Whilst others have created similar systems, some are more focussed on data driven applications, and fewer are available for public use. For this reason, we present this tool as an open-source alternative, where it is available in its entirety for others to use. By using this system, performance information has been gathered for many different instance types across the platforms of both Microsoft Azure, and Amazon Web Services

The performance information collected shows a large distinction between the dif- ferent VM series from cloud providers. Some of the results display a large variance in performance from one instance to the next, whereas others perform very similarly from one run to the other. The different instance series have largely different per- formance, whilst the cost is not always representative, for example the multi-core instance types within the basic A-series offered by Azure in general perform worse than the other series whilst also costing more per hour.

There are several different criteria which are involved in the collection of perfor- mance results from a cloud VM. First, we must acquire a new VM to perform our benchmark on, to ensure that we are running with a clean state. The test suite can then be run on the instance, and the results collected. For the initial implementa- tion, SPECjvm2008 was selected as the benchmarking suite, however it is intended for future iterations to facilitate a selection of testing suites. Finally the cloud ser- vice must be removed entirely post extraction of result data, to prevent additional cost incursion from leaving any remaining components.

To run SPECjvm2008, a command must be run from within the installation folder. The python paramiko package was used to connect to the VM over a Secure Shell (SSH) connection, where the benchmark could then be executed from within the VM. To connect over SSH however, the VM must be setup correctly to accept an SSH key, and the VM must be accessible from the default SSH port (22). To ensure the benchmark runs until completion, a blocking call is made on the output stream from the execution, waiting for the command to exit or the channel to be closed. Once this returns the benchmark has completed, and the results file is copied across to the original machine using an SFTP session. The final step is to release the virtual machine, by deleting the hosted service and deployments, along with associated data disks.

The Basic A2 tier instance score increases by more than a factor of 2 from the Basic A1, when the number of cores, memory and price also double. This is un- usual, as it shows that doubling the number of threads resulted in an increase in performance by slightly more than double. Although the memory on the instance increases also, the amount of memory provided to the JRE under test remained at 1GB for both. This may be an erroneous result, and the instances that were provi- sioned for Basic A2 may have simply been slightly faster than those for Basic A1, however without running more iterations it would not be possible to substantiate this claim.

Each T2 VM receives CPU credits at the beginning of each hour, which allow them to burst above the baseline core usage [1]. The full benchmark run time is approximately 2.25 hours, and is likely to be using near 100% utilization of that which is available at all times. For the first two hours of running, the benchmark will use up the available credits, and run at baseline for the remaining time before more credits are provided at the hourly boundary. For the last hour slot where only 25% of the time is used, the benchmark may be able to burst with the available credits for the remaining duration of the benchmark. If when bursting, the performance of this t2.large instance was greater than that of an m4.large, but far lower when no credits were available and running at baseline, the expected performance would be within the region of that which is seen. The startup and compiler tests which run towards the beginning of the benchmark, while credits would be available to a T2 instance, have greater results for the T2 instance than the M4, whereas others such as scimark and serial, have superior performance on the M4 instance. If the benchmark ran over the entire VM hour slots provided with high utilization, the scores would likely be worse than at present due to running out of credits in the third hour and running at baseline for the remainder of the hour.

Some workloads may be single threaded, and therefore unable to benefit from the additional cores that a virtual machine offers. This limits the number of options for these sorts of tasks on current cloud offerings, as the price of multicore systems rep- resents the performance gained from additional cores. However, it may be possible to batch load a number of single-threaded tasks on to a multicore instance type. If the performance of a single thread on a multiple core instance is higher than that of a single core system, and the price per core is the same, it may be worth loading a single threaded job per core on the multicore system to take advantage of the higher performance. SPECjvm2008 supports running in single threaded mode [10], and so results have been collected of the performance across the multicore variants of both the D-series and Dv2-series. As the previously collected results were not greatly influenced by high memory instance types, the D11-13 VM types have been excluded from the testing process.

The output of the compression test is very similar across the board for this set of tests, with the results deviating by only 6% lower and 4% higher performance than the overall average. Some of the instances remained relatively consistent across all 6 iterations, such as instance numbers 3 and 7, with a maximum change in result between runs at 0.84 and 0.81 respectively. Others performed more erratically, such as instance 5, where the compression result increases by 2.60 from the first iteration to the second. Instance 4 performed particularly poorly when compared to the others, with the lowest score seen in the third iteration, and no scores above 48.81. Half of the instances did not produce results that low, and the lowest score of another three were not far below this value. The higher performing instances, 1, 2, and 7, all scored above 50.42, with an average score greater than 51.18.

The intention for this set of testing however is to evaluate how the first test iteration performs in relation to the rest of the results. In 40% of cases, the initial test iteration was the lowest score seen, with the performance increasing by more than 2.00 points in three of them. However, in two of the instance results, the initial test run result was the highest seen over the time period, with a decrease in performance from this result by as much as 1.52.

The difference from the initial result to the average score for the VM across the results ranged from 0.04, where the initial result gave a good indication to the performance of the instance, to 1.60 where the average was greater than the initial result would have indicated. These differences from the initial result show that it is hard to judge an instance from an initial reading; in some cases this can work well, but others it will not. The variability from one instance to the next would make this method impractical to measure a VMs performance before making use of it. To refine this approach, the initial test could be run for a longer duration in an attempt to get a better representation of the instances performance. Different VM types may also perform less or more variably over time, and so a higher instance tier may produce more useful results.

Throughout this research, the intent has been to create a benchmarking tool to collect performance information of cloud infrastructure, which can in turn be used to make superior scheduling decisions. The tool has been designed to permit the direct comparison of compute performance from one virtual machine to another, irrespective of the service provider. At the time of writing, the presented system may only support the SPECjvm2008 test suite, however the end goal is for the tool to act as a harness for other benchmarking software. Whilst others, [5], [3], perform similar experiments to those covered here, we aim to provide this tool as an available open-source system for others to use 3 . The tool has been used to measure a wide variety of instance type offerings from two different cloud service providers, where it is observed that an increase in cost does not always reflect in higher compute performance.

