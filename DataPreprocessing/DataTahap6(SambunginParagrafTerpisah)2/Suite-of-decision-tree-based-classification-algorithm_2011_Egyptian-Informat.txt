of microarray image processing technique, while n is always less than 200 samples according to the previously collected datasets [5]. Category column presents the actual class of the sample. For the shown example AML stands for acute myeloid leukemia disease and ALL represents acute lymphoblastic.

Our study provides a performance comparison of nine deci- sion tree methods. The rest of this paper is organized as the fol- lows. In Section 2, we present brief challenges that faced in cancer classification area. In Section 3, we provide problem definitions. In Section 4, we exploit decision tree and micro- array classification. In Section 5, we discuss related works in this domain. In Section 6, we explore the methodologies used in this work. In Section 7, we describe experimental setup. In Section 8, we present results and analysis. In Section 9, we con- clude the paper.

Gene classification as domain of research poses a new challenges due to its unique problem nature. First, challenge comes from the unique nature of the available gene expression dataset; where most of these datasets has sample size below 200, vs. thousands to hundred thousands of genes presented in each tuples. Second, only a few numbers of these (genes) presents relevant attributes to the investigated disease. Third, comes from the presence of noise (biological and technical) inherent in the dataset. Fourth challenge arises from the application area, for instance accuracy is an important criterion in cancer classification task, but it is not the only goal, in cancer domain we want to achieve, biological relevancy as well as classification accuracy.

ples to represent benchmarking datasets for evaluation of the most complex classifiers [12]. Most of the microarray data to- day is collected in centralized repositories containing large numbers of samples like Gene Expression Omnibus (GEO) by National Center for Biotechnology Information (NCBI) or ArrayExpress by European Bioinformatics Institute (EBI) [13]. Unfortunately such repositories are too large and contain data coming from various sources using different protocols to serve as a benchmarking collection of datasets. Our study takes advantage of one of the largest publicly available repos- itories of gene expression measurements that were collected by EBI. Which is currently one of the most appropriate collec- tions of gene expression samples for evaluation of classification methods [11].

One of the main advantages of decision trees is the ability to generate understandable knowledge structures, i.e., hierarchi- cal trees or sets of rules, a low computational cost when the model is being applied to predict or classify new cases, the abil- ity to handle symbolic and numeric input variables, provision of a clear indication of which attributes are most important for prediction or classification [10].

several attribute selection (chi-square, information gain, Re- lief-F and symmetric uncertainty) the average accuracy in their study was between 69.33% and 90.01%. Peter et al. [14] uses Partial Least-Squares (PLS) regression as a feature selection method, and compare performance of several ensemble models the predictive accuracy in their study was between 61.2 and

In this experimental study we focus on nine public decision tree methods, some of these methods build single decision tree such as C4.5, CART, REPTree, RandomTree, and Decision- Stump. The other are ensample decision tree such as ADTree, Random Forests, Bagging, and AdaBoost. These methods are described briefly as the following:

C4.5 algorithm top-down decision tree base proposed by Quinlan [16]. The algorithm is a successor of ID3, which deter- mines at each step the most predictive attribute, and splits a node based on this attribute. Every node represents a decision point over the value of some attribute.

ADTree: Applied Alternating decision trees, it is a general- ization of decision trees, voted decision trees and voted deci- sion stumps. The algorithm boosting procedures to decision tree algorithms to produce accurate classifiers. The classifiers are in the form of a majority vote over a number of decision trees but having a smaller and easier to understand classifica- tion rules [19].

Bagging: produced by Leo Breiman [20,21] it uses a boot- strap technique to resample the training data sets D. To form a resampled data set Di. Each sample in D has a probability of 1/n of being drawn in any trial. The most often predicted class label will be the final classification result.

All experiments described in this paper were performed using libraries from Weka 3.7.1 machine learning environment [28]. A lot of studies used Weka in classification task, for examples [26,29]. Nine selected decision tree classifiers are used to build the classification models, these classifier was briefly de- scribed above (Section 6.1), as well as two type of attribute selection (Chi-square, Gain Ratio) were used to reduce the ini- tial set of available genes, and evaluate the performance of these methods, on elected subset of attribute.

was performed before or during classification performance comparison. Each feature selection method was used in combi- nation with all nine classification models included in compar- ison. As well as we evaluate AUC of the classification method using, each test we used 10-fold cross-validation. For bagging and AdaBoost method we used (C4.5) and REPTree as a classifiers and applying 10 iterations each experiment.

This experimental study compares classification performance of different nine decision tree algorithms via using of eleven cancerous microarray datasets. These algorithms include five single decision tree (C4.5, REPTree, CART, DecisionStump, and RandomTree) as well as four ensample decision tree meth- ods AdaBoost (C4.5), AdaBoost (REPTree), Bagging (REP- Tree), Bagging (C4.5), ADTree. In addition, the effect of attribute selection on building decision tree based models is investigated.

From the obtained results we can highlights some interest- ing conclusions: The ensample method (AdaBoost, Bagging and Random Forest) significantly improve the classification accuracy of single decision tree, Due to building several classi- fier and voting techniques. Accuracy of AdaBoost (C4.5) out- performs other ensample methods on original dataset without using of attribute selection. AdaBoost (REPTree), AdaBoost (C4.5) outperform other method with Chi-square attribute evaluation, Bagging (REPTree) outperform other method with Gain Ratio attribute.

classifier accuracy such as C4.5, due to the elimination of some informative genes. By analyzing the results obtained with mul- ticlass microarray datasets (lung1. Multi-tissues dataset) we noticed that CART and AdaBoost (C4.5) outperform other methods on the original dataset. While AdaBoost (REPTree) outperforms with Chi-square attribute selection. Usually, Bag- ging (C4.5) and AdaBoost (C4.5) outperform with Gain ratio attribute selection.

