The contributions in this paper are: 1) a systematic architecture to manage the sharing of GPU resources in Cloud computing environments for general purpose use. The architecture will focus on the application deployment time and ensure the applications QoS is fulfilled in the operation time. Moreover, in this architecture, we will consider the performance and the energy consumption as key factors. 2) We perform a comparative experimental study that reveals the architectural impact on the performance, power and energy consumption on heterogeneous GPUs.

The remainder of this paper is structured as the follows. Section 2 introduces the related work. Section 3 presents the proposed architecture. Section 4 introduces the heterogeneous GPU Benchmarking and Analysis. Section 5 explains the GPU benchmarking experiments and the results. Finally, Section 6 concludes the paper and describes the future work.

Occupancy is an important metric to analyse the performance when dealing with GPUs for general purpose use. GPU Occupancy is defined as the ratio of the active number of threads to the maximum number of the threads in the SM. The value of the GPU Occupancy is between 0 and 1. To calculate the GPU Occupancy, we use the following formula:

We gradually increased the number of the threads per block up to the maximum number (1024 threads per block), and froze the number of blocks. The number of blocks was 80 x 80 to ensure that SMs were working simultaneously. By increasing the number of threads per block, we increased the size of the memory as well. Then, we ran each matrix multiplication size five times and calculated the average of the power consumption and the execution time. We profiled the GPU power consumption every 50 milliseconds.

Considering Fermi C2075 GPU, we found that there is a gradual increase in power consumption up to a certain level of the number of threads per block percentage, when the active threads per block percentage is 66%. After that, power consumption significantly decreases to 136 Watts. To explain the trend of the power consumption during an increase in the GPU workload, we need to compose a performance and architectural analysis for the applications running on the Fermi C2075 GPU at the runtime.

of threads per block, there was a 13.1% energy saving with the matrix that had 30 x 30 number of threads per block, which was similar to the previous experiment and had a lower execution time. In this case, by increasing the execution time, the performance loss decreased to 5.6% compared to the previous case.

In Kepler K40c, we found the opposite situation. We found that there was an energy consumption saving of 9.1% in the matrix that had a faster execution time and a larger number of blocks (100 x 100) since there was no substantial difference in power consumption between the first and the second workload allocation. The power consumption difference was merely 3.08 Watts between them. Then, the 1. This reduction produces inefficient parallelism behaviour to cover the instruction pipeline and the memory latency. Therefore, it leads to a performance decrease. This performance decrease affects power consumption. Similarly, GPU Occupancy deceases alongside with the resident blocks in the SM leading to a performance and power decrease. However, in Kepler K40c, although the resident blocks number in the SM is decreased from 16 to 2, the performance of the matrix with the size 2560 x 2560, which is the largest matrix size, not affected by this reduction, and its execution time is faster than the previous matrix, 2400 x 2400. The reason lies in the effectiveness of the GPU Occupancy on performance since the GPU Occupancy of 2560 x 2560 is greater than 2400 x 2400.

GPU Occupancy, GPU memory types and hardware block scheduling factors have a strong correlation with power consumption in Fermi C2075 GPU. However, the effectiveness of GPU Occupancy on power consumption in Kepler K40c GPU is not reliable. It has a clear effect on the performance. The remainding factors, some GPU memory types and block scheduling, can be considered in terms of effectiveness on the power consumption.

Moreover, blocks and the threads per block allocations affect energy consump- tion. The impact depends on the type of GPU architecture. In Fermi C2075 GPU, there is a trade-off between the performance and energy consumption. Increasing the number of blocks will increase the performance and also increase energy con- sumption. However, in Kepler K40c GPU, increasing the number of blocks will increase the performance and become more energy efficient.

In this paper, we have proposed an adaptive architecture in Cloud computing envi- ronments. The aim of this architecture is to manage heterogeneous GPUs resources for general purpose in Cloud computing environments. The architecture considers the deployment and the runtime by focusing on performance and energy consump- tion factors. The Heterogeneous GPUs Analyser has been introduced as the initial step to develop the aforementioned architecture. The Heterogeneous GPUs Anal- yser aims to analyse the architectural behaviour of heterogeneous GPUs in terms of performance, power and energy consumption. Additionally, Kepler architecture is 46.5% more energy efficient than the Fermi architecture.

power and energy consumption, a novel energy consumption prediction model will be developed to estimate the energy consumed by the GPU application. The energy consumption prediction model will be developed by selecting the highest influential factors on energy consumption for both GPU architectures. These influential factors will be set as the model inputs. Then, energy efficient scheduling policy will be developed to allocate the GPU applications to the most energy efficient VM. The decision made by the energy efficient scheduling policy will rely upon the energy consumption prediction model. Additionally, this scheduling policy will consider the execution time and the energy consumption as key factors. Finally, we will develop an adaptive management framework to automatically maintain the QoS of the allocated application during the operation time. To maintain the QoS of the GPU applications during the operation time, there should be a trade-off in terms of energy efficiency, performance and cost. Therefore, another research aim is to find the aforementioned trade-off.

