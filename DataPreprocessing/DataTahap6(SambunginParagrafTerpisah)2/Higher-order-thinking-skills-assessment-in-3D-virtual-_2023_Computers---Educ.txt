in the 1980s and 1990s that allowed the creation of learning systems that are recognizable today as widespread Internet-based educational media (Duncan et al., 2012). VLEs provide educational content, allow communication, and can facilitate skills development. They are available in many different formats: single or multi-user, gamified or not gamified, and 3D immersive or not immersive.

As Queiroz et al. (2019) identified, the existing assessment methods in VLEs mostly focus on more tangible skills such as biology, computer science, and medicine. Limited work has been done on the assessment of higher-order thinking skills, analysis of learning patterns, and investi- gating methods that allow the use of AI. At the same time, researchers (Spector & Ma, 2019) have warned about over-emphasis on the use of AI and the need to rely on human intelligence, and the development and assessment of higher-order thinking skills in learners through simulations and games (Ketelhut et al., 2010; Van Voorhis & Paris, 2019).

More studies are needed to help instructors in offering more specific and targeted assessments identifying weaknesses and strengths in stu- dents. This might be achieved by performing series-based assessments on smaller tasks or elements of activity (for example, a motif). Expert actions can then be used to help manage the data size. The review of literature on existing score-based and series-based approaches shows that there are research gaps and open questions on the effectiveness of series-based assessment using smaller groups of tasks, the use of expert data and similarity measures for such small groups to avoid the need for large data sets, and the suitability of different similarity measures. These gaps are the basis of our research questions and the motivation for our proposed solution and study.

The research and our questions are motivated by the need to find assessment methods that allow localized feedback and evaluation on specific (smaller) activities without the need to use a large amount of training data. Motifs and similarity with expert actions have shown po- tential in other cases, and as such, we proposed an assessment method based on these concepts. The proposed method allows a more direct involvement of instructors and course designers in the VE assessment process, as they can flexibly define motifs and expert actions.

Our study was performed in the context of a chemistry lab in a 3DVLE (both desktop and head-mounted display). Providing safety training before letting students into a physical chemistry lab is a mandatory step that all educational organizations need to facilitate. Traditionally, this step is done by text or video-based materials along with a question-and- answer assessment of readiness. Before entering the lab, it is important for students to learn about the dress code (e.g., safety goggles), the components of the lab (e.g., eyewash and shower and how to use them), and the correct process in case of an emergency. Due to safety concerns, creating situations such as a fire or chemical spill where students can apply their emergency reaction skills is impractical or impossible except in virtual reality.

The research team invited university students to participate and collect research data in the Winter 2021 semester. Invitations were done through dedicated social media groups the university has for research participants and also colleagues who could forward the invitation. Any university student could participate (no VR experience needed) but we particularly encouraged those from science programs and also empha- sized the desired gender and ethnic diversity. Unfortunately, due to COVID-19 restrictions, we could not invite our participants to our lab to participate, which would have allowed us to capture screen video.

Ultimately, we recruited 36 university participants, 20 male and 16 female. The average age was 25, with a standard deviation of 6.45. All participants were university students in Chemistry or another science/ engineering program. On average, they had taken 5.10 chemistry courses with a standard deviation of 3.70. Almost all participants (94%) had completed previous traditional lab training; yet, according to the partner instructor, even students who passed training tended to have problems in the lab. 66% of participants had prior experience in immersive VR with a variety of games.

granular analysis, large patterns of action were transformed into motifs, which then became the transformed units of analysis. In our previous study (Nowlan et al., 2018), we defined motifs as an overlapping com- bination of activities that build four separate skills. These skills existed in

6 chambers that students visited. For this study, the instructor was looking for three skills that could be identified within three separate tasks. There was no longer any overlap (shared activities), and the pattern and order of activities for each HOTS (and the related motif) were defined by the instructor. We hypothesized that by applying different similarity index measurements on all tasks and correlating the results

In the educational context, we believed that having the students perform a controlled action was important, i.e., they were not randomly interacting without an overall strategy. To understand the implication of the importance of the order, we created and used both uni-gram and bi- gram series in all our comparisons. Each activity in our research was captured as an object and an action performed on it, such as LabCoattorso-Hover. Below are examples of the unigram and bigram series used for this research:

In this research, we also used multiple similarity indexes to assess HOTS by comparing to an expert. Our goal was to find out which one would be the Maximum Similarity Index (MSI) and if MSI would be the same for all HOTS, or different HOTS could use different indexes. Considering the many different similarity calculation methods and their advantages (Loh & Sheng, 2014; Winkler, 1999), we decided to use the following similarity indexes in our research.

Cleaning the log and removing the entries that were not important: We instrumented the environment to capture everything, including looking at or hovering on the object or selecting the object. We collected between 400 and 500 data entry points for one student during a half-hour session. We decided that if a student looked or hovered on an object, these were unintentional activities, not necessarily performed to serve the task, so these were removed from the log.

It can be argued that the information collection activity was not an order-sensitive activity, and as such, the unigram series could be used instead of the bigram series. If that is the case, the Cosine similarity index provides the highest correlation between matrix-based assessment versus manual SME assessment. Cosine similarity checks the existence of the same data elements in both series, just like Jaccard similarity. Addi- tionally, cosine similarity also checks the number of occurrences of common elements.

In this study, we applied motif-based learner vs. expert series simi- larity analysis for HOTS assessment. The study was motivated by the shortcomings of existing methods, such as the lack of localized feedback and the need for a large amount of training data. Our study answered our research questions and found that.

and possible when using motifs. Last but not least, finding a some- what unexpected one as we were hoping to find one similarity mea- sure that outperforms others but found out that there could be different ones depending on the task or skill. The literature on simi- larity measures already suggests the possibility of combining different indices (Loh & Sheng, 2014). So, the idea of the relative suitability of indices for different HOTS or tasks is in line with the research di- rection and further suggests more research to explore this relative suitability.

This paper investigated the use of motifs and expert data in series- based HOTS assessment in 3DVLEs. These platforms, with their ability to offer remote virtual classrooms, play an important role when physical classrooms cannot be used. We showed that motifs as small meaningful elements of learning activity have a strong potential to be used for series- based assessment. We also demonstrated the value of using expert data and multiple similarity measures.

