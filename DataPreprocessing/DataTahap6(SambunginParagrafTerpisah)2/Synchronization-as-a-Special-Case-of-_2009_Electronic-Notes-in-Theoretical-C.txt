Methods declared as async are executed in new threads as in Polyphonic C# [5]. An invocation of produce_consume creates a buffer and two threads concurrently accessing the buffer. The compiler associates b first with empty:true specified in the constructor of SimpleBuffer and then replaces this token with the less informative tokens empty:true?false and empty:false?true needed in produce and consume (see Section 3). Arrows in the annotations of the formal parameters p and c indicate that the tokens move from the actual parameter b to p and c and never come back to

Programmers think in terms of accessibility at a high level instead of low-level syn- chronization. Compilers ensure synchronization with properties like race-freeness and (to some extent) continuity. Concurrency need not dominate the program structure, and we can concentrate on other important programming principles.

Interfaces specify accessibility. Clients need this information to avoid access and synchronization conflicts. Since we specify in interfaces only necessary informa- tion we keep the black-box view of objects and can take advantage of data hiding: Changes of implementation details do not affect clients.

Tokens move between references also on assignment. When executing x = y; the tokens associated with x before assignment get lost. The tokens associated with y are divided into two parts as needed in further computations; x becomes associated with one part, and y remains to be associated with the other part. The compiler can always determine at compile time (for each class separately) how to divide the tokens using techniques similar to those proposed in [29].

Clients invoking put in an instance of Buffer usually own only a non-exclusive token size:0..(max-1)?1..max, and those invoking get own size:1..max?0..(max-1). In this case clients have no static knowledge of size, and when acquiring locks the threads must wait for appropriate values of size.

Simple mutex synchronization on x occurs in all other cases of synchronization on x. The execution neither depends on variable assignments in other threads nor wakes up other threads because of variable assignments. For example, we have simple mutex synchronization on i when using an instance of Counter. Simple mutex synchronization is a property of the access constraints of all methods in a class and does not depend on the context of an invocation.

Sometimes b has to be annotated with size:0..(max-1), and sometimes with size:1..max (where max = 1) depending on the value of e. In larger examples there can be many more conditional annotations of the same variable, and explicit annotations with all possibilities would be a large burden for programmers.

5 In this article we take a rather conventional view of synchronization based on locking. When using techniques like memory transactions [17] we can relax this condition to get more concurrency. The use of such techniques in our approach especially for simple mutex synchronization is important future work. We do not discuss this topic here because the focus is on dependent synchronization.

Direct accesses literally occur in the code of the method. Accesses within other methods invoked by the method in question do not count. Formal parameters, local variables, and constants (these are variables declared as final) need not be considered because they are not shared or not modifiable. Instance variables in constructors are not considered because we preclude concurrency during object creation.

Programmers must provide access constraints on methods as well as annotations of formal parameters and results. This information must be considered in (behavioral) subtyping because it has major influence on object behavior. We consider annota- tions of formal parameters and method results to belong to corresponding types. On such annotated types we use the following subtyping rules:

Unfortunately, access constraints and synchronization in subtypes cannot be more restrictive than in supertypes (except for simple mutex synchronization) ac- cording to the principle of substitutability. Programmers usually want to have it the other way around. Therefore, the more flexible solution for simple mutex syn- chronization (which probably occurs more often than dependent synchronization) can be quite helpful in many situations. It allows programmers to introduce syn- chronization in derived classes even if there is no synchronization in base classes.

systems) it is actually possible to get static guarantees. However, they come at very high costs. We must accept restrictive design rules and inflexible tools, need experienced and expensive experts who put much effort into low-level programming and program analysis, and get long development times.

Method invocations using t1 and t2 (as above) do not block each other because of nonterminating computations. The execution of each method invoked using these tokens must terminate in finite (and for practical reasons short) time. Program- mers have to ensure that there are no infinite loops in such method executions.

concurrency. The invocations shall occur in different threads. The compiler shall warn if two different ?-tokens of the same variable name occur in the same method. Because of insufficient aliasing information there will be false positives, that is, the variables in the tokens can belonging to different objects. Furthermore, in some cases it is useful for t1 and t2 to temporarily occur in the same method, for example in an initialization phase while starting concurrent threads.

Without warning there can be no deadlock because all locks must be acquired in a specific order; there can be no cycles. However, the compiler will find false pos- itives (as above), and complete avoidance of cycles is a very restrictive property. Sometimes developers prefer possible (but unlikely) deadlocks over very expensive program refactoring to avoid cycles.

In the proposed approach, concurrency is based on rather conventional threads, locks, and values of shared variables. In this respect there are many similarities with the SCOOP model of concurrency [22]. Both, the SCOOP model and our model, disclose information on the variables used for synchronization. However, the way how and the time when such information becomes available to clients is different: Every client can get access to such variables at runtime in the SCOOP model while access control in our model causes much information to be available at compilation time and usually only few clients actually access the variables at runtime. The access control mechanism adds a further dimension to concurrent programming and reduces the importance of synchronization at the presence of static information.

Many proposals ensure race-free programs [4,8,15]. Some approaches depend on explicit type annotations [15] while others perform type inference [4]. Such techniques can lead to more locks because no approach accurately decides between necessary and unnecessary locks. Program optimization can remove some unnec- essary locks [9,34]. Unfortunately, we usually must analyze complete programs for good results.

There is also much work on deadlock prevention [1,15,19]. A major problem of all such proposals is that static deadlock prevention considerably affects the flexi- bility of the language. The approach proposed in the present work is no exception. Therefore, we argue that a potential deadlock found by a compiler is no more than just a hint for the programmer to have a closer look into the code.

concurrency to be independent of the object model. A concept similar to the one expressing synchronization in the Actor-like language became more useful as an access control mechanism, and a new concept for synchronization was needed [30]. There are several approaches related to process types. Especially linear types

Several programming languages [5,14,25] were developed based on the Join cal- culus [16]. For example, in Polyphonic C# [5] we combine methods that must be executed simultaneously to a chord which is executed as a single unit. Clients can see how methods in a chord are synchronized. Since only one method in a chord is executed synchronously and all other methods are asynchronous, only specific forms of synchronization are supported. Communication in Polyphonic C# and similar languages resembles that of the rendezvous concept in Ada [18]. There is no way to constrain method invocations as with our token concept, and there is no obvious way to use chords in controlling aliasing.

In previous work the author often used dependent types (these are types de- pending on values) to increase the flexibility of the system. In the present work we avoid dependent types as much as possible because they are difficult to deal with. We get the necessary flexibility by dynamically changing tokens based on results of dynamic type queries (instanceof).

The present work is in an early stage. There exist only fragments of a prototype implementation, and a rigorous practical evaluation of the proposed approach has not yet been carried out. An evaluation is important future work. In further future work we want to address (among others) the following topics:

Synchronization and access control ensuring exclusive access to shared variables fit together quite naturally. We explored an approach to integrate synchronization into a static access control mechanism based on tokens giving information about exclusive access to variables and supposed types of their values. It is possible and beneficial to annotate methods with access constraints not distinguishing between static access control and dynamic synchronization. Clients decide if they prefer access control or synchronization. Object interfaces provide all the information that clients need to avoid conflicts of dependent synchronization, and simple mutex synchronization can be regarded as an implementation detail. Static type checking guarantees exclusive write-access and consistent read-access to shared variables and thereby ensures race-free synchronization. We get flexibility by using dynamic type information in tokens. The approach supports subtyping and ensures to some extent continuous operation of the system.

