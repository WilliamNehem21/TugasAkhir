Plug-in Hybrid Electric Vehicles (PHEVs) showed significant improvement in fuel consumption and CO2 emissions over the last decade. However, the performance superiority depends on the quality of the on-board Energy Management Strategy (EMS). The emerging technologies such as connected vehicles and automated driving increased the complexity of the EMS control objectives. Traditional hand-crafted rule-based approaches are robust, reliable and computationally affordable. However, they do not guarantee optimality with several objectives in a multi-domain, nonlinear and time-varying systems. Accordingly, the need for more intelli- gent controllers becomes vital for future vehicles.

[9] saving 10% in the fuel consumption in comparison to the Charge-Depletion Charge-Sustaining (CDCS) strategy. The afore- mentioned approaches offered a feasible solution to real-time Hybrid Control Units (HCUs), however the trade-off between con- trol objectives, controller performance in real-time, tuning effort, and vehicle hardware capabilities are difficult to balance [10]. Reinforcement Learning (RL)-based energy management approaches in PHEVs took immense attention of the Artificial Intel- ligence (AI) community due to the ability to learn control policies through interaction without being explicitly programmed on a cer- tain strategy.

1) Conventional Drive (CD {0}) where ICE is the only propul- sion source and supplying the low voltage auxiliaries. 2) Opti- mum Generation (OG {3}) where the demanded torque is low and the ICE load point can be increased to a better fuel- economy location. The EM works as a generator for charging the HV battery with the leftover power from the ICE. 3) Electric Drive (ED {6}) which enables the EM to propel the vehicle.

The agent collected 50 k experience tubles, following a random policy, which were postprocessed to accumulate the episodic reward and calculate the Q-value for each tuble. Afterwards, sev- eral NN architectures were trained and tested on different datasets in a supervised learning fashion to calculate the Root Mean Square Error (RMSE) as training and validation errors. The aforementioned architecture was selected due to achieving the minimum RMSE among others which indicates the best ability for the NN to learn and approximate the Q-function efficiently.

The Double Deep Q-Network (DDQN) algorithm, introduced by Hasselt et al. [27], is used to reduce the observed Q-values overes- timation bias and stabilize the training by decoupling the action selection from the target Q-value estimation using two different NNs; policy and target networks. Lillicrap et al. proposed an will be selected according to the RL policy. Other approaches sug- gested including the agent only in the free modes and bypass it in the fixed modes segment. This approach was tested, and the agent faced difficulties in learning the correct value function that repre- sents the future cumulative rewards, as the agent is not aware of part of such rewards if bypassed, hence it never converged. The advantage of our approach is that the agent will have full-observa bility/controllability over the environment even with the action

Several scholars proved that advanced AI-based strategies such as reinforcement learning EMSs can significantly improve the fuel economy of PHEVs. This study introduced an adaptive online learn- ing RL agent into the existing HCU architecture. DP results are used to benchmark the developed RL algorithms which solved the EMS for near-optimal solutions. The development process began with formulating the control problem mathematically as an infinite- horizon optimal-control problem. The mathematical formulation is a function of the battery SoC, driver torque demand, vehicle speed, remaining trip distance, and the engine on/off status. The objective is to minimize the total fuel consumption and the fre- quent engine switch. Accordingly, the design process reasonably gration with the Intelligent Transportation Systems (ITSs) to con- struct a smart city or a smart grid is coming soon with more comprehensive and complicated optimization control objectives. In the future connected environment, distributed and multi-agent DRL systems are necessities for cooperative learning between vehi- cles on the road. The sooner the transition towards intelligent control systems by automotive OEMs, the better they are prepared and qual- ified for the upcoming challenges. The research presented an initial step overlooking the way towards realizing an intelligent adaptive energy management system for PHEVs. The upcoming research efforts shall investigate more the following spots:

The author would like to thank Prof. Thomas Schlechter and Prof. Stefan Winkler for their support during the master thesis research. Moreover, sincere gratitude to the AVL DSP team: Patrick Teufelberger, Huan Chen, Evgeny Korsunsky and Bhargav Adabala, for their collaboration and valuable feedback. Furthermore, deep thanks to the editors and anonymous reviewers for providing insightful suggestions and comments to improve the quality of research paper.

