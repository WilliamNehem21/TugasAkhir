Few studies correlate activities to specific workers because the majority of approaches only assess static video frame images and do not establish associations between workers with front and rear frames, thus further limiting the evaluation of the labor consumption of each worker [10]. This is a disadvantage of computer vision-based noncontact approaches, although some nonvision-based systems perform this task more effectively. In addition, the majority of worker action identification systems rely on raw images, which are computationally intensive and difficult to guarantee in real time compared to methods based on key point data. In addition, construction sites typically have several cameras, and the volume of processed video data is substantial. Compared to approaches based on key point data, the majority of existing vision-based systems rely on the original image to complete worker action detection, which is computationally demanding and difficult to perform in real time.

In this paper, a new vision-based activity recognition framework is introduced, where a video recorded by an ordinary camera is used as the input to automatically obtain the behavior of each worker. In this framework, the lightweight pose estimation network is used to obtain human key point information from the video taken by an ordinary camera. Then, a multiperson tracking algorithm is adopted, and the boundary frame of workers is extracted by using the key points to complete the extraction of motion and appearance information, thus effectively tracking workers and key points. Finally, multilayer fully connected (FC) neural networks and stacked long short-term memory (LSTM) are designed to classify the key point information of each worker, complete action recognition, and analyze the construction efficiency. The feasibility of the proposed framework is tested and verified using videos collected from actual construction sites. The contribution of the article is as follows:

Considering the presence of new workers, missed detection of algorithms, disappearance in the field of vision, and continuous movement and occlusion of workers, it is difficult to keep the value of n per worker unchanged and integrate continuous worker frame key point data. To solve the above problems, this study uses a multitarget tracking algorithm to realize the continuous frame association between workers and key point data. In the tracking module, a deep simple online and real-time (SORT) tracker is applied to associate the same workers detected in the previous step across all the frames in the video [46]. The method is used to track the prefabricated wall and collect information on the location and time of the prefabricated wall from the surveillance video [47].

The updated trajectory is predicted using a standard Kalman filter with constant velocity motion and a linear observation model [44]. Then, the currently detected value is matched according to the predicted value. Based on the dk prediction of the state at frame (k + 1), dk + 1 can be obtained.

In this study, the accuracy and confusion matrix are used as the performance indicators. The accuracy gives the probability of a correct prediction and has been a widely used measurement method in activity recognition. The confusion matrix can not only completely show the numbers of correct and incorrect predictions but also completely show the specific prediction categories of the results of the corresponding activity prediction errors. Additionally, the accuracy of the prediction results for each category can be calculated.

frames) to determine the construction efficiency of workers over a period. Similarly, the construction progress statistics of all workers within the monitoring range of a certain construction stage can be calculated by the number of construction workers (the number of people in state = 1 in each frame) and the construction efficiency of all workers in each frame. The above information can be recorded over time to analyze the construction efficiency of each worker and the construction efficiency of all workers over the same period.

A deep learning-based activity analysis framework is proposed to handle the very large amount of construction information involving multiple workers. The framework integrates key point extraction, tracking, activity recognition and efficiency analysis modules. The pose estimation detector processes 2D pose information obtained from RGB video feeds. The key point action dataset of a construction site is also proposed to complete the training of the activity classification network. The small amount of key point data can represent human actions and positions. The proposed framework can handle the identification of multiple worker activities recorded in videos and the statistics of construction efficiency. By validating the proposed procedure on construction monitoring videos collected from actual construction sites, the proposed framework is shown to be effective in monitoring the activities of construction workers. The experimental results show that the 2D pose information is useful for construction worker activity analysis and efficiency analysis.

