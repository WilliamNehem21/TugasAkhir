This paper establishes a link between Bayesian inference (learning) and predicate and state transformer operations from programming semantics and logic. Specifically, a very general definition of backward inference is given via first applying a predicate transformer and then conditioning. Analogously, forward inference involves first conditioning and then applying a state transformer. These definitions are illustrated in many examples in discrete and continuous probability theory and also in quantum theory.

The paper is organised as follows. We first introduce the notions of backward and forward inference in terms of predicate and state transformers and show some basic properties. Then, we concentrate on illustrating the impact and power of our definition in many situations. We show what our abstract setting translates to in discrete and continuous probability theory and also (briefly) in quantum theory. We elaborate many examples of computations of how inference works, and what it produces. Of special interest is the application of our definition of inference in Bayesian networks. It is shown that the forward/backward distinction can be used flexibly, and can describe what inference means at different points in the network.

an instance of backward inference, where an observation on the codomain (the test outcome) changes the state of knowledge about the domain (the disease occurrence). Of course, standard Bayesian methods will arrive at the same outcome. The point is that we can describe these methods here in a uniform, abstract manner via calculations in (Kleisli) categories.

The initial conditions of the example estimate whether there is enough time available to prepare the paper (the variable T ) and whether the scientist is sufficiently skilled to do the necessary research (S). The results that the scientist is able to obtain (R) depend both on the time and the skill, while how well the paper reads only depends on the time. Both results and readability have an influence on whether the reviews will be positive (P ), but results will be more relevant. Similarly, these two factors may lead a PC member to enthusiastically endorse the paper (M ), independently of what the reviewers say, although this possibility is quite rare. Finally, acceptance

Differently from A?, this E? is a fuzzy predicate: a mistake gets more likely the less time and skill are available to the scientist. If this situation occurs, the scientist may still be able to produce on time a paper that gets accepted, but chances are lower: they decrease from 48% to 43%. This is expressed by the following inference.

Our abstract description of inference allows us to transfer the definitions from the discrete to the continuous approach simply by switching from the Kleisli category Kl(D) of the distribution monad to the Kleisli category Kl(G) of the Giry monad [9] on measurable spaces. We shall sketch an example where the function f in the inference situation (3) is the identity, but where we have multiple predicates pi for successive learning. Hence there is no predicate/state transformation involved. We describe the essentials and refer to [5] for more information.

Our inference situations (3) and (4) can also be interpreted in the effectus of von Neumann algebras for quantum computation. Actually, one uses the opposite vNAop of the category vNA of von Neumann algebras, with normal completely positive unital maps between them (see [5] for details). We have to take the oppo- site category because maps between von Neumann algebras should be understood as predicate transformers. Typical examples are the von Neumann algebras B(H ) of bounded operators on a Hilbert space H . Below we use the matrix algebra M2 = B(C2) as special case.

The application to Bayesian networks also leaves room for interesting develop- ments. As sketched in Remark 3.1, the interpretation of networks as arrows of Kl(D) can be seen as part of a broader picture, that can be formulated in the language of PROPs and their models. We find particularly worthwhile trying to understand Bayesian inference, as introduced in the present paper, as a categorical transforma- tion on models of a PROP: it should map one network into another one with the same topology, but different probability distributions.

