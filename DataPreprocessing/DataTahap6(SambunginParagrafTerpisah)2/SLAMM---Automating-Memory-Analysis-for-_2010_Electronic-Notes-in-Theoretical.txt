Memory efficiency is overtaking the number of floating-point operations as a performance determinant for numerical algorithms. Integrating memory efficiency into an algorithm from the start is made easier by computational tools that can quantify its memory traffic. The Sparse Linear Algebra Memory Model (SLAMM) is implemented by a source-to-source translator that accepts a MATLAB specification of an algorithm and adds code to predict memory traffic.

Our tests on numerous small kernels and complete implementations of algorithms for solving sparse linear systems show that SLAMM accurately predicts the amount of data loaded from the memory hierarchy to the L1 cache to within 20% error on three different compute platforms. SLAMM allows us to evaluate the memory efficiency of particular choices rapidly during the design phase of an iterative algorithm, and it provides an automated mechanism for tuning exisiting implementations. It reduces the time to perform a priori memory analysis from as long as several days to 20 minutes.

Creating new algorithms that demonstrate both numerical and memory effi- ciency [35,7,2,5,6,37,14,20,17] is a difficult task that has not been addressed exten- sively or in a systematic fashion. The all-too-common approach is to ignore memory efficiency until after the implementation is complete. Unfortunately, retroactive re- tooling of code for memory efficiency can be a daunting task. It is better to integrate memory efficiency into the algorithm from the start.

Both static and dynamic analysis of the prototype are necessary in order to obtain this information. SLAMM performs the static analysis directly, and also produces a copy of the prototype that has been augmented with additional MAT- LAB code blocks. The MATLAB interpreter executes the transformed code to provide the dynamic analysis.

An occurrence of an identifier in the prototype does not necessarily indicate that the corresponding variable must be loaded from the memory hierarchy. Fortunately, it is possible to improve the accuracy of the base identifier counts through a series of corrections. These corrections capture properties of the translation from MATLAB to a compiled language.

This calculates the amount of data loaded from the memory hierarchy by the if- expression and the fetch of new, accumulating the result in field wsl of the structure slm foo. Similarly, the amount of data stored to the memory hierarchy (in the assignment to b at the beginning of the block) is recorded in field wss. Each of these assignments has two components: the total amount loaded or stored before this execution of the code in B1 (e.g., slm foo.wss) and the amount loaded or stored during this execution (e.g., slm new.bytes). The former is initialized to zero in Header, the latter is a constant determined by whos.

Note that this calculation accounts only for the accesses in the scope of the bindings of B1. Both new and b have defining occurrences in B2, and thus B2 constitutes a hole in the scope of the B1 bindings for those identifiers. Memory access information associated with the assignment in B2 is accumulated by code in the exclusive memory analysis code block B2.

Here the single original expression t = sin(r)+ cos(z) is broken into three separate statements. The temporary variables slm L1C5 and slm L1C14 are the original output arguments of the sin and cos functions respectively and are used to calculate the expected result t. The structures slm sinL1C5 and slm cosL1C14 contain the results of the memory analysis of the sin and cos functions respectively.

Automated memory analysis provides both the ability to evaluate the memory effi- ciency of a particular design choice rapidly during the design phase and the ability to improve the memory efficiency of a pre-existing solver. We illustrate both applica- tions by using SLAMM to reduce the execution time of the Parallel Ocean Program (POP) [32], a global ocean model developed at Los Alamos National Laboratory. POP, which is used extensively as the ocean component of the Community Cli- mate System Model [8], uses a preconditioned conjugate gradient solver to update surface pressure in the barotropic component. Parallelism on distributed memory computers is supported through the Message Passing Interface (MPI) [34] standard. We used POP version 2.0.1 [29] to examine data movement in the preconditioned

POP uses a three-dimensional computational mesh. The horizontal dimensions are decomposed into logically rectangular two-dimensional (2D) blocks [21]. The computational mesh is distributed across multiple processors by placing one or more 2D blocks on each processor. The primary advantage of the 2D data structure is that it provides a regular stride-one access for the matrix-vector multiply. The disadvantage of the 2D data structure within the conjugate gradient solver is that it includes a potentially large number of grid points that represent land. In effect, a number of explicitly stored zeros are added to the matrix. An alternative 1D data structure that uses compressed sparse row storage would avoid the inclusion of land points but would introduce indirect addressing. Additional details concerning the changes in data structures are provided in [11].

