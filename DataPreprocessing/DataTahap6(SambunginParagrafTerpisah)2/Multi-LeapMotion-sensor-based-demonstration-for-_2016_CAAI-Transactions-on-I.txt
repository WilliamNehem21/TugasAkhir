rather than at gesture recognition. Furthermore, this is a typical contact control method in which a human works within the same environment as the robot. Therefore, it is hardly used in human-unfriendly environments. For this reason, non- contact tele-control methods are more appropriate for these situation. For example, some mechanical based [7e9], optical tracking based or vision based master-slave-device and tele- operation system [10e12] are developed for robotic systems. Comparing with the mechanical devices, the optical and vision tracking systems are lower cost and easier to be mounted in difference environment.

For hand gesture recognition, a highly efficient way is using data glove that can record the motion of each finger [13,14]; some kinds of data glove can even measure the contact force of a grasping or pinching action [15]. However, beside the high cost of data glove, they lack the capability to track po- sition of the hand. Therefore, extra approaches are added to track hand positions [16,17], such as inferred optical tracking [18], which also increases the complexity of the system.

Some scholars only use the vision based method for both the hand tracking and gesture recognition. But the perfor- mance of the gesture recognition is much effected by the lighting and background conditions [19e21]. Thus, some aiding methods like skin color and pure color background are used to improve the recognition accuracy [22,23]. Some other scholars use RGB-D data from Kinect for gesture recognition [24]. However, the Kinect sensor is developed for body motion tracking, In the research of Kim et al., it has been proved that the accuracy of hand motion tracking using Kinect is much lower than LeapMotion sensor, which is particularly designed for hand motion tracking [25].

The LeapMotion1 sensor, developed by Leap Motion Inc., is a new non-contact finger/hand tracking sensor. It has a high tracking accuracy and provides plenty of software interface for pose and gesture recognition. Some preliminary studies have been carried out for robot manipulation. Zubrycki et al. use a LeapMotion sensor to control a 3-finger gripper [26], Guer- reroRincon et al. develop a interface to control a robotic arm [27], Marin et al. report the first attempt to detect gestures from the data combination of LeapMotion and Kinect [28,29]. These use single LeapMotion for hand tracking and gesture recognition, however, due to the occlusion problem between fingers, single sensor can perform well only when the palm is with a ideal orientation.

In this paper, a multi-LeapMotion hand tracking system is developed to overcome the limitation of the aforementioned drawback of single LeapMotion. The tracking space and working area are analyzed to gain an appropriate setup for two LeapMotion sensors. With self-registration, a coordinate sys- tem are established. Based on the definition of the element actions, an algorithm to calibrate the delay and combine the data from the two LeapMotion sensors is proposed to improve the stability for both the hand tracking and gesture recognition. To developed a tele-operative demonstration system, a Kinect sensor and a 7-DoFs (Degree of Freedoms) robotic arm with a 3-finger gripper are combined with the developed Multi- LeapMotion hand tracking system in ROS (Robot Operation System).2 Functional experiments are performed to indicate the results of combined hand tracking and gesture recognition. At the end, a scenario experiment is performed to show how this proposed system is used in a robotic system.

The position and orientation of the palm and the extension status of each finger can be directly provided by the APIs of LeapMotion sensor. However, this information and status are obtained by the build-in algorithms, which is only available for single LeapMotion sensor. Therefore, it is difficult to fuse the data in these status-level. We use the original position and direction data of each links and joints of the fingers from the two sensors, and fuse them in data-level to gain the correct information of the hand tracking.

between two gestures. Thus, if the similar Mult-LeapMotion device is used for real-time tele-operation in robotic sys- tems, the algorithm should be specifically revised. Moreover, the setup method of the two sensors is primarily for grasping from the side or top of the object. If the palm direct forward or backward, the occlusion problems will happen for both of the bottom or side mounted sensors. To solve these problems, more future works on the optimization of the sensor setup and the revision of the gesture recognition algorithm should be done in our further research.

Y. Wang, J. Cai, Y. Wang, Y. Hu, R. Xiong, Y. Liu, J. Zhang, L. Qi, Probabilistic graph based spatial assembly relation inference for pro- gramming of assembly task by demonstration, in: IEEERSJ International Conference on Intelligent Robots and Systems (IROS 2015), Hamburg, Germany, 2015, pp. 4402e4407.

