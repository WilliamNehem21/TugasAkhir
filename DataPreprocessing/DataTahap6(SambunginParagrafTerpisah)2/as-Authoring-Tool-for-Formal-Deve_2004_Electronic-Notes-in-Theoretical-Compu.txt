A first solution that comes to mind consists in writing a LATEX document. This is how one typically diffuses his results, inserting in places selected pieces of the formal development. This method has obvious drawbacks. The cut and paste process may introduce errors when the formal development had centric to be useful in our context. The users want to use LATEX for writing, documenting, and publishing their work since a formal development consists of definitions, theorems, and proofs rather than pieces of programs. Nevertheless, a starting point is simply putting TEX commands within comments as part of the scripts.

The tool must provide high-quality visualization, using notation that is familiar to the reader (e.g., using standard mathematical notation) as well as a user-friendly interface. It also must allow one to present the formal devel- opments using a natural syntax and must avoid whenever possible the use of

Finally, to avoid a common source of errors in the translation from the source code to the presentation (typos are always unpleasant but are par- ticularly annoying in the context of certified developments), the tool must allow the automatic importation of the source code (for example the theorems statements, the definitions, etc.) to preserve their correctness. This automatic translation should also make it possible to preserve links, connections, corre- spondences back and forth between the presentation and the scripts produced, and controlled by the prover.

The  target is important. First, for compatibility reasons, we must provide a stand-alone version of the script, where the documentation frag- ments are inserted in the appropriate comment regions. Second, and more importantly, this target is evidence that one can extract from the enriched document the parts which concern the user formal development, this new script must be accepted by the prover afterwards.

Our tool is not tightly coupled to a given theorem prover. In the future it should even be an independent tool. However, we begin with a case-study of our formal developments which are mainly done using the Coq theorem prover. Thus, the presented examples use Coq syntax.

The user expects other features as well. For instance, he might want to modify imported fragments to make them more readable or easier to under- stand. Thus, variable renaming, hypotheses reorganization, or other changes at the presentation level should be possible while keeping the formal develop- ment correct with respect to the underlying proof assistant. Such demands are reasonable since one cannot expect any automatic generation from a formal development to provide any article-ready text. As soon as interactive editing introduces changes with respect to the initial development, there is a possi- bility for introducing errors. It is these errors we would like to avoid with our tool.

We definitively want to offer the user a way to enrich their formal devel- opment with convenient documentation or informal explanations. Typically after the automated importation step of a Coq script the user will do some interactive editing to prepare for dissemination of results. LATEX exportation is satisfactory as a scientific document format. However, we must be able to recover the script to certify a posteriori the embedded code. Using LATEX as the generic format for the script leads to a substantial loss of structure information. The resulting document is (enriched) text, while on the Coq side, a script is a program, where documentation is in comments. Therefore,

the crucial distinction between the active parts of the script (the vernacu- lar commands mainly) and the inactive parts (the interleaved comments-as- documentation fragments) is lost in the conversion process if we use LATEX as the generic format. Although this could be handled to some extent with the help of tricky annotations within the LATEX end-of-line based comments, this is not satisfactory. The TEXmacs editor deals with structured documents, which is precisely what we want.

ures, and bibliographies. TEXmacs documents are saved as structured text files using a simple notation system which helps automatic (machine-driven) search. Therefore, as a whole, TEXmacs presents many attractive features which make it an interesting candidate in the context of authoring scientific documents.

The binder list, arrowc, and appc nodes should be obvious from the above tree. The N~_ and NEXT_:_|_ come from the extendible Notation command introduced in the last version of Coq that permits the kind of enhancement that formula (1) uses for the verbatim output. While this is not at all necessary in our tool (see RSA example below), at this stage of our development we decide to represent these nodes as special nodes in the generated structure.

Coq provides specific outputs in many contexts, not only for formulas. Let us have a look at the Inductive vernacular command. The Coq standard library provides many examples. From Datatypes unit, the sum of two sets A + B is inductively defined as

Following these brief explanations, the full Coq Standard Library is gen- erated automatically by our documentation tool as a bunch of TEXmacs files which can be browsed within the editor. The blue items Acc (in lemma 5) represent hyperlinks to the definition declaration Inductive 4.. Our contri- bution consists in a complete set of TEXmacs files accessible online from within the editor and browsable in the same way as the web documentation on the Coq site.

to be executed by the TEX engine. So the key idea is that the user is working in a dual world with respect to program source editing. Our tool transforms any program source file into a dual document source file, since TEXmacs is basically working in the same way as TEX.

be inserted as comments into the script, but comments are thrown away by the Coq lexical analyzer. In order to collect the documentation provided in the source script, we had to change the lexer behavior, so that not all the comments are ignored. However, doing this in every place where comments may occur would result in a parser with unreasonable complexity. We decided on a model where documentation and code alternate, but no documentation is looked for within the vernacular commands. Documentation fragments may

appear in many places within the TEXmacs representation of a given script. We ensure that they do not get lost through the exportation step by placing them in places where the (modified) Coq lexer is able to find them. However, we cannot guarantee that their exact position is preserved.

Therefore, we have split the exportation in two steps, where the first one profits from the fact that we are able to identify the fragments specific to Coq among the whole document. This first step is designed as a Guile script filter, which can be rewritten by the user at will. Selecting the exported material can be locally tuned inside the document (source) by assigning values to specific variables locally, following the same idea as marking a fragment as math, bold, or italic. Afterwards, everything is put in the hands of the common

Let us have a look at the coq-tex tool provided with Coq distribution. This is a stand-alone program which processes Coq phrases embedded in LATEX files. This tool greps the LATEX for Coq vernacular commands, identified by par- ticular LATEX environment tags (coq_examples, coq_examples*, coq_eval). The result is stored in a temporary script and sent to Coq toplevel (coqtop) for evaluation. Depending on the surrounding tag, the command itself or the result are then re-inserted in place in the LATEX file. As it stands, the environ- ment tags offered with coq-tex gives the user the different possibilities based on two criteria: i) is the vernacular command to be displayed and ii) is the result of its evaluation to be displayed? Whenever the answer to a question is positive, the appropriate text is printed verbatim at the appropriate place.

