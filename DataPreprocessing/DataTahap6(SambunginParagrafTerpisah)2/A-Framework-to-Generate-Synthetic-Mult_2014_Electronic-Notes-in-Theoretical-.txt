A controlled environment based on known properties of the dataset used by a learning algorithm is useful to empirically evaluate machine learning algorithms. Synthetic (artificial) datasets are used for this purpose. Although there are publicly available frameworks to generate synthetic single-label datasets, this is not the case for multi-label datasets, in which each instance is associated with a set of labels usually correlated. This work presents Mldatagen, a multi-label dataset generator framework we have implemented, which is publicly available to the community. Currently, two strategies have been implemented in Mldatagen: hypersphere and hypercube. For each label in the multi-label dataset, these strategies randomly gener- ate a geometric shape (hypersphere or hypercube), which is populated with points (instances) randomly generated. Afterwards, each instance is labeled according to the shapes it belongs to, which defines its multi-label. Experiments with a multi-label classification algorithm in six synthetic datasets illustrate the use of Mldatagen.

In practice, the effectiveness of machine learning algorithms depends on the quality of the generated classifiers. This makes fundamental research in machine learning inherently empirical [6]. To this end, the community carries out exten- sive experimental studies to evaluate the performance of learning algorithms [9]. Synthetic (artificial) datasets are useful in these empirical studies, as they offer a controlled environment based on known properties of the dataset used by the learn- ing algorithm to construct the classifier [2]. A good classifier is generally considered to be one that learns to correctly identify the label(s) of new examples with a high probability. Thus, synthetic datasets can be used instead of real world datasets to derive rigorous results for the average case performance of learning algorithms.

The remainder of this work is organized as follows: Section 2 briefly de- scribes multi-label learning concepts and strategies to generate synthetic multi-label datasets. The proposed framework is presented in Section 3 and illustrated in Sec- tion 4. Section 5 presents the conclusions and future work.

Multi-label learning methods can be organized into two main categories: algo- rithm adaptation and problem transformation [15]. The first one consists of methods which extend specific learning algorithms to handle multi-label data directly, such as the Multi-label Naive Bayes (MLNB) algorithm [18]. The second category is algorithm independent, allowing one to use any state of the art single-label learn- ing method. Methods which transform the multi-label classification problem into several single-label classification problems, such as the Binary Relevance (BR) ap- proach, fall within this category. Specifically, BR transforms a multi-label dataset into q single-label datasets, classifies each single-label problem separately and then combines the outputs.

The evaluation of single-label classifiers has only two possible outcomes, correct or incorrect. However, evaluating multi-label classification should also take into account partially correct classification. To this end, several multi-label evaluation measures have been proposed, as described in [15]. In what follows, we briefly describe the example-based and label-based evaluation measures used in this work. All these performance measures range in [0..1].

Thus, the binary evaluation measure used is computed on individual labels first and then averaged for all labels by the macro-averaging operation, while it is com- puted globally for all instances and all labels by the micro-averaging operation. This means that macro-averaging would be more affected by labels that partici- pate in fewer multi-labels, i.e., fewer examples, which is appropriate in the study of unbalanced datasets [5].

Synthetic datasets with different properties are generated in [11] to study multi- label decision trees. In this study, the power to identify good features, related to the feature selection task [10], was also verified. The strategy used to generate the datasets considers several functions to define feature values related to the labels.

To illustrate a new multi-label learning algorithm, a synthetic dataset is specified in [17]. Given three labels and a covariance matrix, instances are labeled according to seven Gaussian distributions, such that each distribution is related to one multi- label. The number of instances per multi-label is arbitrarily defined.

It should be emphasized that Mldatagen extends the strategy proposed in [18] by enabling the user to choose the number of different kinds of features, i.e., rel- evant, irrelevant and redundant, the maximum and minimum radius of the small hyperspheres, as well as the noise level of the datasets generated.

As the possible range to define each coordinate of Ci reduces whenever a new coordinate is set, it is necessary to avoid determinism during the generation of the Ci coordinates of the hypersphere hsi, i.e., to avoid always generating ci1 as the first coordinate and ciMrel as the last one. To this end, the index of the coordinates j to be set, j = 1..Mrel, is randomly defined.

To ensure that the multi-labels contain at least one of the q possible labels, the generation of the N instances is oriented, such that, for each instance Ei, the point with coordinates (xi1, xi2,..., xiMrel ) is at least inside one small hypersphere. By using this procedure, none of the instances will have an empty multi-label. All small hyperspheres hsi, i = 1..q, are populated using this criterion.

Algorithm 2 summarizes the generation of the instances xk, k = 1..N , in- side the small hyperspheres hsi = (ri, Ci), i = 1..q. Similar to Algorithm 1, the updateminX(x) and updatemaxX(x) functions refresh respectively the lower and upper bounds of the next xk coordinate to be defined inside the corresponding hy- persphere hsi. The already set coordinates are also taken into account to randomly generate the remaining coordinates.

After generating the N points related to Mrel, Mirr and Mred features are set by adding Mirr irrelevant features, with random values, and Mred redundant features. The features to be replicated as redundant are chosen randomly. In the end, the N points are in RM , M = Mrel + Mirr + Mred.

Different from HyperSpheres, the possible ranges to define each coordinate cij are the same for all coordinates. Thus, Algorithm 3 is simpler than Algorithm 1, as the functions updateminC(x) and updatemaxC(x) are not needed to generate the hypercubes hci = (ei, Ci), i = 1..q.

Algorithm 4 summarizes the generation of the instances xk, k = 1..N , inside a small hypercube hci = (ei, Ci), i = 1..q. This algorithm is simpler than Algo- rithm 2 by discarding the functions updateminX(x) and updatemaxX(x), as the coordinates range in the same domain.

As HyperSpheres does, after generating the N points related to Mrel, the Mirr and Mred features are set by adding Mirr irrelevant features, with random values, and Mred redundant features. The features to be replicated as redundant are chosen randomly. In the end, the N points are in RM , M = Mrel + Mirr + Mred.

Mldatagen was used to generate 6 synthetic multi-label datasets, 3 using the Hyper- Spheres strategy and the other 3 the HyperCubes strategy. To generate the datasets, different values of the Mrel, Mirr and Mred parameters were used. These values were chosen to analyze how the number of features (M = Mrel + Mirr + Mred) and the number of unimportant features (Mirr and Mred) influence the performance of the multi-label BRkNN-b learning algorithm [13], available in Mulan. In what follows, information about the synthetic datasets generated, BRkNN-b and the classification results are presented.

Lazy algorithms are useful in the evaluation of datasets with irrelevant features, as the classifiers built by these algorithms are usually susceptible to irrelevant features. The multi-label learning algorithm BRkNN is an adaptation of the single-label lazy k Nearest Neighbor (kNN ) algorithm to classify multi-label examples proposed in [13]. It is based on the well-known Binary Relevance approach, which transforms a multi- label dataset into q single-label datasets, one per label. After transforming the data, kNN classifies each single-label problem separately and then BRkNN aggregates the prediction of each one of the q single-label classifiers in the corresponding multi- label, which is the one predicted. Despite the similarities between the algorithms, BRkNN is much faster than kNN applied according to the BR approach, as BRkNN performs only one search for the k nearest neighbors.

To improve the predictive performance and to tackle directly the multi-label problem, the extensions BRkNN-a and BRkNN-b were also proposed in [13]. Both extensions are based on a label confidence score, which is estimated for each label from the percentage of the k nearest neighbors having this label. BRkNN-a classifies an unseen example E using the labels with a confidence score greater than 0.5, i.e., labels included in at least half of the k nearest neighbors of E. If no label satisfies this condition, it outputs the label with the greatest confidence score. On the other hand, BRkNN-b classifies E with the [s] (nearest integer of s) labels which have the greatest confidence score, where s is the average size of the label sets of the k nearest neighbors of E.

It can be observed that between the two datasets with M = 5 features, dataset F , which was created using the HyperCubes strategy, shows the best results. Among the four datasets with M = 20, dataset B obtained the best example-based mea- sure values for Hamming-Loss (HL) and Subset-Accuracy (SAcc), while dataset D obtained the best results for the remainder of the example-based measures and for all the label-based measures considered. Both datasets were also created using the HyperCubes strategy.

As stated before, for M = 20 the classifiers build with datasets B and D show better performance than the ones build with datasets A and C. Moreover, for M = 5 the classifier build with dataset F shows better results than the one build with dataset E. Observe that these datasets (B, D and F ) were generated using the HyperCubes strategy.

is reached for M = 5, decreasing afterwards. This problem is well described in [12]. On the other hand, in the HyperCubes strategy the volume of the main hypercube HC always increases with the dimensionality. Both cases are related to the curse of dimensionality.

To illustrate Mldatagen, multi-label classifiers were built from six synthetic datasets. The results suggest that the datasets generated by the HyperCubes strat- egy provide better classification results than the ones based on HyperSpheres. In fact, despite both strategies being sensitive to the curse of dimensionality, the vol- ume of a hypercube always increases with the dimensionality, showing better behav- ior than a hypersphere, whose volume decreases from one dimension upwards. As future work, we plan to implement more strategies into the Mldatagen framework and make them available to the community.

