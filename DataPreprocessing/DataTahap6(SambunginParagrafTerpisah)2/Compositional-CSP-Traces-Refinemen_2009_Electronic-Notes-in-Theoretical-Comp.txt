Even after 25 years of model checking [9], the complexity of state spaces is still the major issue in an automatic verification of formally specified models. One factor of state explosion in complex systems are parallel components, because - in principle - the size of the state space grows exponentially in the number of components. Techniques to specifically treat this problem have been and are still under development. Among these are for instance partial order reductions [19], symmetry reductions [6], specific abstraction techniques for parallel systems [16] or compositional verification, the technique we will be interested in here.

First of all imagine a set of jobs which are to be handled by a scheduler. Let n be the number of these jobs. A job can be started or finished. Therefore, when looking at a single job the events start and finish may occur. In order to identify

In between two start-calls of the job i, there must have occurred exactly one finish.i event. Thus a job may only be restarted if it was finished before. Furthermore, a finish.i event may only occur after a preceding execution of a start.i event.

After having an intuition on how the scheduler should work, the task is now to express this system using the language CSP. Before actually constructing a CSP process fulfilling the constraints, we first focus on describing the constraints them- selves as CSP processes. This will prove useful later.

Next, we design a scheduler obeying both constraints. To this end, we first introduce a new operator. The parallel composition operator is used to express that two CSP processes P1, P2 are executed concurrently, denoted by P1 X1 ||X2 P2. Although P1 and P2 are executed concurrently, they may have to synchronise on some events. This is why the alphabets X1 and X2 are needed. An alphabet is a set of events, c.i occurs before finish.i, the cell should just wait for finish.i and execute start.i afterwards immediately. These two possibilities can be expressed using the choice- operator in CSP, denoted by P  Q for some CSP process P and Q. Summing up we get the following CSP processes:

where Ci is defined as Ci = {start.i, finish.i, c.i, c.inc(i)}. Since the events c.i are just artificial events used by our implementation of the scheduler, we want to hide them in the traces of the process. CSP offers the hiding operator for this purpose,

Using this alphabet, the above given proof rule is sound and complete ([4]). Given the CSP processes S1, S2, and Prop the task is now to automatically find, if possible, an assumption A fulfilling the conditions of the proof rule. For this purpose we use a learning algorithm developed by Angluin ([2], improved by [17]) and proposed for assumption generation in [4]. To this end we split the task into two components: a learner and a teacher. Using the teacher, the learner can successively build assump- tions A which may be sufficient for the proof rule. In fact, the learner successively tries to learn the so called weakest assumption Aw. The weakest assumption acts as A in the proof rule if the refinement

Although the learner tries to learn the weakest assumption, other assumptions may be found as an intermediate step in the learning process, which are sufficient to show that the refinement holds or to show that it does not hold. In that case, of course, the learning algorithm terminates immediately instead of continuing learning the weakest assumption. In fact, this is the anticipated case.

To implement the learner we use the algorithm proposed in [2] and [17]. However, we do not go into detail here how to actually implement the learner since it is mostly independent of the specification language used. Instead we focus on the teacher, beginning with the membership-query.

As seen above, the weakest assumption satisfies this refinement. Thus, if the refinement does not hold when using A instead of Aw, then A cannot be the weakest assumption. In that case we just return the counterexample provided by the refinement check as a proof for the conclusion traces(A) /= traces(Aw). If the refinement holds, we continue with step (ii).

Although the membership-queries can be handled quite quickly using the Tcl interface to FDR, there is still some optimisation possible. Most obvious, we can save many FDR calls by simply caching the membership-queries. Thus, whenever the learner asks for the membership of a trace t more than once, we can just return the cached result. This is of course faster than the FDR call. But we can save even more FDR calls by using properties of CSP. The set of traces of a CSP process is prefix closed ([18]), thus the following holds for some CSP process P , a trace t and a prefix s of t:

The recursive call only makes sense if the direct call takes very long. Currently, we use the following heuristic: if the number of components is more than c, the direct call will take very long. This is a quite rough heuristic of course but due to a lack of other (better) heuristics we use this rough heuristic.

of the scheduler for different n. We use the default caching settings for the im- plementation (i.e. caching and prefix-caching). Furthermore, for the non-recursive implementation we split the process SCHED by putting n + 1 cells into the first component and the others to the second component. When using the recursive implementation we split by half and use c = 8.

As we see here, for one property compositional verification is much better, for the other, a plain use of FDR. The reason for this is the size of the assumption generated during learning. When the assumption is much smaller than S1 itself, compositional verification will outperform FDR, otherwise the overhead of learning will make the verification much slower. Thus, what we clearly need for a successful application of this technique is a means for determining beforehand, whether a compositional verification makes sense. This will be the topic of future work.

The main advantage of this parallel proof rule over the basic one is that it is sym- metric. Thus we can independently learn assumptions for the components of the system at the same time. More precisely, the CSP processes A1 and A2 describe assumptions under which S1 and S2 can guarantee Prop. The third condition of the proof rule simply ensures that both assumptions do not disallow common traces. This is needed for the proof rule to be sound, since, if we for example used the deadlock process STOP for both assumptions (i.e. disallow every nonempty trace),

Our implementation can make use of both rules during a refinement check. This is especially useful for implementing a recursive version of the parallel proof rule. In that case we actually call the algorithms for the two proof rules in alternation.

