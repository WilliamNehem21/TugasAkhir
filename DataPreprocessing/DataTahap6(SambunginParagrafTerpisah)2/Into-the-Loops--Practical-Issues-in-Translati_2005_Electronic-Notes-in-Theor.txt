Translation Validation is a technique for ensuring that the target code produced by a translator is a correct translation of the source code. Rather than verifying the translator itself, translation validation validates the correctness of each translation, generating a formal proof that it is indeed a correct. Recently, translation validation has been applied to prove the correctness of compilation in general, and optimizations in particular.

Translation Validation (TV) is a technique for ensuring that the target code emitted by a translator - such as a compiler - is a correct translation of the source code. Because of the (well-documented) difficulties of verifying an entire compiler, i.e. ensuring that it generates the correct target code for every possible valid source program, translation validation can be used to validate each run of the compiler, comparing the actual source and target codes.

There has been considerable work in this area, by these authors and oth- ers, to develop TV techniques for optimizing compilers that utilize structure preserving transformations, i.e. optimizations which do not greatly change the structure of the program (e.g. dead code elimination, loop-invariant code motion, copy propagation) [1,7,11] as well as structure modifying transforma- tions, such as loop reordering transformations (e.g. interchange, tiling), that do significantly change the structure of the program [2,7,12]. In previous publi- cations, the authors and their students have described a prototype tool, Tvoc, that was developed for performing translation validation on the Intel Open Research Compiler (ORC) which performs a large number of transformations of both categories [13,14].

Tvoc does not use a single unified proof rule for validating loop reordering transformations, but rather relies on several proof rules of different forms depending on the particular optimization being applied. Specifically, Tvoc uses different proof rules for interchange, tiling, and skewing than it does for fusion and distribution. From a scientific (and engineering) perspective, a single proof-rule to handle all loop reordering transformations would be more satisfying.

When presented with the target code T that reflects a series of transforma- tions of the source code S, such that no intermediate versions of the code (e.g. after individual transformations) are available, Tvoc will synthesize a series of intermediate versions of the code, based on what transformations it believes were performed. That is, it will generate synthetic intermediate versions I1, I2, ... In, which might possibly not have been created by the compiler at all. Tvoc will then validate that the translation from S to I1, the translation from Ij to Ij+1 for each j, and the translation from In to T are correct.

In order to avoid having Tvoc rely on information produced by the compiler to determine which optimizations were actually performed, we have devel- oped a set of heuristics that are used to generate this information given only the source and target code. Heuristics were previously used in this way by Necula [8] for the TV of structure preserving transformations in gcc. In this paper, we describe the heuristics we use for the TV of structure modifying transformations, specifically loop reordering transformations.

The paper is organized as follows. Section 2 provides the necessary back- ground for understanding our TV work in the validation of individual struc- ture modifying transformations. Section 3 describes how the proof rule that we have used for loop optimizations such as interchange and tiling can be gen- eralized to include a wider variety of loop transformations including fusion, alignment, peeling, and unrolling. Section 4 describes the kinds of combina- tions of optimizations that ORC performs, and our techniques for validating such combinations using the creation of synthetic intermediate versions of the code. Section 5 presents the heuristics that we have developed in order to determine, in the absence of any suggestions by the compiler, which optimiza- tions have been performed. Finally, Section 6 concludes.

A transition system T is called deterministic if the observable part of the initial condition uniquely determines the rest of the computation. We restrict our attention to deterministic transition systems and the programs which gen- erate such systems. Thus, to simplify the presentation, we do not consider here programs whose behavior may depend on additional inputs which the program reads throughout the computation. It is straightforward to extend the theory and methods to such intermediate input-driven programs.

Rule Val constructs a set of verification conditions, one for each simple target path, whose aggregate consists of an inductive proof of the correctness of the translation between source and target. Roughly speaking, each verification condition states that, if the target program can execute a simple path, starting with some conditions correlating the source and target programs, then at the end of the execution of the simple path, the conditions correlating the source and target programs still hold. The conditions consist of the control mapping, the data mapping, and, possibly, some invariant assertion holding at the target code.

Somewhat related to our approach is the work on comparison checking where executions of unoptimized and optimized versions of code are compared on particular inputs [4,5,6]. Comparison checking depends on finding data and control mappings between a source and a target on particular inputs, and mismatches are reported to detect optimization errors. Comparison checking has mainly been used for structure preserving optimizations.

Structure modifying transformations are those that admit no natural map- ping between the states of the source and target programs at each cutpoint. In particular, A reordering transformation is a structure modifying trans- formation that merely changes the order of execution of the code, without adding or deleting any executions of any statement [2]. It preserves a depen- dence if it preserves the relative execution order of the source and target of that dependence, and thus preserves the meaning of the program. Reordering transformations cover many of the loop transformations, including fusion, dis- tribution, interchange, tiling, unrolling, and reordering of statements within a loop body.

The range of the middle elements (i.e. the range of values of the loop index variables) in JJ is {0..100} while the range of the middle elements of ff is {0..99}. An increase in the upper bound for the loop index variables, or a decrease in the lower bound, indicates that alignment may have occurred (it could also indicate skewing, but the relationship between ff and JJ when skewing occurs is substantially different than what we see here).

