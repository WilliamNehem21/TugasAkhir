Electronic Notes in Theoretical Computer Science 176 (2007) 79–95	
www.elsevier.com/locate/entcs

Structuring Optimizing Transformations and Proving Them Sound
Aditya Kanade1	Amitabha Sanyal2	Uday Khedker3
Dept. of Computer Science and Engineering, IIT Bombay.

Abstract
A compiler optimization is sound if the optimized program that it produces is semantically equivalent to the input program. The proofs of semantic equivalence are usually tedious. To reduce the efforts required, we identify a set of common transformation primitives that can be composed sequentially to obtain spec- ifications of optimizing transformations. We also identify the conditions under which the transformation primitives preserve semantics and prove their sufficiency. Consequently, proving the soundness of an opti- mization reduces to showing that the soundness conditions of the underlying transformation primitives are satisfied.
The program analysis required for optimization is defined over the input program whereas the soundness conditions of a transformation primitive need to be shown on the version of the program on which it is applied. We express both in a temporal logic. We also develop a logic called temporal transformation logic to correlate temporal properties over a program (seen as a Kripke structure) and its transformation.
An interesting possibility created by this approach is a novel scheme for validating optimizer implemen- tations. An optimizer can be instrumented to generate a trace of its transformations in terms of the transformation primitives. Conformance of the trace with the optimizer can be checked through simulation. If soundness conditions of the underlying primitives are satisfied by the trace then it preserves semantics.
Keywords: Optimization specification, Formal verification, Translation validation


Introduction
Modern compilers are equipped with sophisticated optimizations. An optimization is sound if the optimized program that it produces is semantically equivalent to the input program. The issue of soundness of optimizers has been addressed as verifica- tion of specifications [9,10] and translation validation [11,18]. The former approach seeks to guarantee soundness of specifications but does not address soundness of their implementations, while the latter approach checks the soundness of an opti- mizer on a run-by-run basis. The latter approach requires some heuristics [11] or hints from the compiler [18].

1 Email: aditya@cse.iitb.ac.in
2 Email: as@cse.iitb.ac.in
3 Email: uday@cse.iitb.ac.in

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2006.06.018

The proofs of semantic equivalence are usually tedious. To reduce the efforts re- quired, we identify a set of common transformation primitives that can be composed sequentially to obtain specifications of optimizing transformations. We also identify the conditions under which the transformation primitives preserve semantics. For example, common subexpression elimination, partial redundancy elimination, and loop invariant code motion replace some occurrences of an expression by a vari- able. Although they may select different application points, the same soundness condition has to be satisfied at each of the application points: The variable must have the same value as that of the expression being replaced. Such a soundness condition guarantees semantics preservation under the respective transformation is a one time proof and is independent of any optimization. The primitives are small- step transformations as compared to the optimizations and hence the semantics preservation proofs are easier. This approach reduces proving the soundness of an optimization to merely showing that the soundness conditions of the underlying transformation primitives are satisfied. This is much simpler than directly proving semantics preservation for each optimization.
We specify the program analysis and the soundness conditions in first-order logic. They are interpreted over control flow representation of programs. The prop- erties which relate information along control flow paths are expressed in a temporal logic. The program analysis is defined over the input program. Depending on its position in the transformation sequence, the soundness conditions of a transforma- tion primitive need to be shown either on the input program or its appropriate transformation. We develop a logic called Temporal Transformation Logic (TTL) to correlate temporal properties over a program seen as a Kripke structure and its transformation.
Based on our approach of identifying transformation primitives and their sound- ness conditions, we suggest a novel validation scheme: An optimizer can be instru- mented to generate a trace of its execution as a sequence of appropriately instan- tiated primitives. An execution preserves semantics if (1) the optimized program matches the output obtained after simulating the trace on the input program and
(2) the soundness conditions of the transformation primitives used in the trace are satisfied.
The rest of the paper is organized as follows: Section 2 introduces the specifica- tion mechanism. Section 3 describes the verification technique and introduces TTL. Section 4 proposes a validation scheme. Section 5 describes how proof obligations can be automatically generated from the specifications of optimizations in PVS. Section 6 reviews related work. Section 7 concludes the paper and proposes future directions.

Specifying Optimizations
The optimizations are specified over an abstraction based on control flow graph representation of three-address code. We use PVS [14] as the specification and verification framework. PVS language is based on typed higher-order logic [13]. We

explain PVS language features wherever required.
Abstraction of Programs
A program is a directed graph with a single entry and a single exit. Each node denotes a control location called a program point and holds a statement . Each program point has at least one successor. 4 The entry point has no predecessors whereas the exit point has only a self-loop. Variables and constant s form data part of a program. At present, we do not consider arrays and pointers. The expression s are formed from operator s and variable or constant type operand s. The operators are uninterpreted functions. We consider four kinds of statements: (1) SKIP is a “no-operation”, (2) ASSIGN(Lhs: variable, Rhs: expression ) is an “assignment”,
(3) ITE(Condition: operand , Tb, Fb: point ) is an “if–then–else” statement where Tb and Fb are respectively targets of “if” and “else” branches, and (4) HALT halts the execution of a program. The “goto”s are modeled as directed edges.
The type program is defined as a record with four fields:
program : type =  # cfg: Graph point , entry, exit: (cfg ‘S ),
L:  (cfg ‘S ) → statement  # 
cfg is the control flow graph whose nodes belong to a set of program points S and edges are given by a relation τ : S × S. They are respectively referred to as cfg‘S and cfg‘τ . entry and exit are respectively entry and exit points. The function L maps a program point in cfg‘S to a statement.
The abstraction is well-defined if its control flow and contents are consistent with each other. For example, if the statement at program point p is ITE(c, p1, p2) then p1 and p2 should be the only successors of p. These constraints are satisfied by a predicate subtype (Program) of program where Program : program → bool (definition omitted). In the running text, a program which satisfies this predicate is simply referred to as “program”.
An aside on PVS typing. Given a type T and a predicate ϕ : T → bool , (ϕ) denotes a predicate subtype of T . It is the set of entities from T that satisfy the predicate ϕ. PVS allows dependent types where the types are defined in terms of the components declared earlier. (cfg ‘S ) is a dependent predicate subtype indicating the set of program points S of cfg. Though cfg and S are field names, they are italicized when used in a type declaration. As a convention, we always italicize the types.

Computational Tree Logic with Branching Past
We use computational tree logic with branching past (CTLbp) [8] for specifying global program properties. Kripke structures are used as models for CTLbp. A Kripke structure is a directed graph whose nodes are labeled with atomic proposi- tions. Formally, a Kripke structure M = (S, R, P, L), where S is a finite non empty set of states. R : S × S is a transition relation which is total in its first element.

4 This is required for modeling programs as Kripke structures and is explained in section 2.2.

Transp(prog, e)(p): bool	= Assign?(prog‘L(p)) ⇒ Lhs(prog‘L(p)) /∈ VOperands(e) Antloc(prog, e)(p): bool	= Assign?(prog‘L(p)) ∧ Rhs(prog‘L(p)) = e
Dom(prog, q)(p): bool	= AP(prog, (p))(q)
DomS(prog, xs)(p): bool	= ∀(q: (xs )) : Dom(prog, q)(p)
Scc(prog, xs): bool	= ∀(p, q: (xs )) : EU(prog, xs, (q))(p)
Loop(prog)(xs): bool	= Scc(prog, xs) ∧ ∃(p: (xs )) : DomS(prog, xs)(p)∧
∀(p: (xs )) : ¬DomS(prog, xs)(p) ⇒ AY(prog, xs)(p)
%Header(prog : (Program ), xs: (Loop(prog )))(p : (prog ‘cfg ‘S )) : bool
Header(prog, xs)(p): bool = xs(p) ∧ DomS(prog, xs)(p)
%LInv(prog : (Program ), xs: (Loop (prog )))(e : (expressions (prog ))) : bool
LInv(prog, xs)(e): bool	= ∀(p: (xs )) : Transp(prog, e)(p) ∧ Header(prog, xs)(p)
⇒ AU(prog, Transp(prog, e), Antloc(prog, e))(p)
%Invs(prog : (Program ), xs: (Loop(prog )), e: (LInv (prog , xs ))) : set [(xs )]
Invs(prog, xs, e): set (xs )  = {p: (xs ) | Antloc(prog, e)(p)}
Fig. 1. Specification of program analyses pertaining to Loop Invariant Code Motion

To guarantee totality, in the program abstraction, every program point is required to have a successor. P is a set of atomic propositions. L : S → 2P is a labeling function which associates states to propositions.
We view programs as Kripke structures. The program points cfg‘S form the set of states S. The set of edges cfg‘τ form the transition relation R. The propositions are generalized to predicates over program points.
CTLbp has propositional connectives and temporal operators. The future op- erators describe properties of descendants of a state. The past operators describe properties of ancestors of a state. Consider formulae ϕ and ψ. The future operators are X(ϕ) (“neXt time ϕ holds”), U(ϕ, ψ) (“ϕ holds Until ψ”), and F(ϕ) (“ϕ holds sometime in Future”). The past operators are Y(ϕ) (“ϕ holds Yesterday”), S(ϕ, ψ) (“ϕ holds Since ψ”), and P(ϕ) (“ϕ holds sometime in Past”). They are prefixed with path quantifiers E (“for some path”) or A (“for all paths”). For example, AU(ϕ, ψ) states that “along all (forward) paths ϕ holds until ψ” or EP(ϕ) states that “along some (backward) path ϕ holds sometime”. Since we transform a program step-by- step, we have different versions of it. To distinguish between the interpretations of CTLbp formulae over different programs, we parameterize them with respective programs.

Specifying Analyses
We use loop invariant code motion to explain the specification mechanism. We first specify analyses for identifying loops and determining whether an expression is invariant within a loop. The specifications are given in Fig. 1. For readability, we do not explicate the types. However, they are explained in the running text. Some interesting type signatures are inserted in the specification as comments, starting with %.






··· 


1
1
··· 
1
··· 
1
··· 








prog1
prog2
prog3
optimized program

Fig. 2. An example of Loop Invariant Code Motion

Consider a program prog and an expression e in it. The expression e is trans- parent at a program point p if none of its operands is assigned to at p. This is defined as the predicate Transp and is in curried form. Assign? is the recognizer for assignment statements. Lhs is the accessor for left-hand side variable of an assign- ment. VOperands returns the set of variable type operands of an expression. The expression e is locally anticipatable at a program point p if the statement at p is an assignment whose right-hand side (accessible by Rhs) is e. This is defined as the predicate Antloc.
We now specify control flow analyses for identifying loops. A program point p dominates a program point q if along all backward paths from q, p is reachable. This property is defined as the predicate Dom. Let xs be a set of program points of prog. A program point p dominates xs if it dominates each member of xs. This is defined as the predicate DomS. The set xs is a strongly connected component if for every pair of program points in it there is a (non-empty) directed path whose intermediate program points also belong to xs. This is defined as the predicate Scc. A set of program points xs is a Loop if it is a strongly connected component, there is a program point in xs which dominates it, and the predecessors of other program points of xs are also in xs. The program point in a loop which dominates it is called its Header.
Let xs be a loop in program prog. An expression e is invariant within xs if it is transparent at all program points of xs and is anticipatable at the Header of xs, that is along all forward paths from the header, e is transparent until it is locally anticipatable. This is given as the predicate LInv. The function Invs gives program points of xs that contain occurrences a loop invariant expression e. These occurrences evaluate to the same value in every iteration and hence can be moved out of the loop.

Specifying Transformations
Consider a program prog1 shown in Fig. 2. The set xs = {2, 3, 4} is a Loop with program point 2 as its Header. According to the analyses defined in Fig. 1, the

expression a ∗ b is invariant within xs and is locally anticipatable at program point
3. We want to hoist a ∗ b to the incoming edges of the loop header except along the looping edge. This is done by transforming the program step-by-step as follows:
Split the edge ⟨1, 2⟩ by adding a new program point, say 5, containing a SKIP statement. A new program point is distinct from program points of the subject program.
Let t be a new variable with respect to prog2. A new variable does not appear anywhere in the subject program. Insert an assignment statement t = a ∗ b (simplified notation for ASSIGN(t, a ∗ b)) at program point 5.
Replace the occurrence of a ∗ b at program point 3 by the variable t.
The transformation is formally specified in Fig. 3. Let prog1 be a program, xs be a loop in it, and e be an invariant expression within xs. The predicate hdr denotes the headers of xs. By definition, a loop has only one header. The set predsNotInLoop denotes predecessors of the header which are not in xs. The function SE is a transformation primitive. Given a program and two sets of program points, it splits the edges going from the program points in the first set to those in the second set. It inserts new program points containing SKIP statements. Here, it takes the program prog1 and splits the edges from predsNotInLoop to the loop header. The transformed program is prog2.
Let invoccurs be the program points in xs where the expression e occurs. They are identified over prog1. Let t be a new variable w.r.t. prog2. newpoints are the new program points inserted by the first transformation. In prog2, they are predecessors of the loop header but are not in the loop. The function IA is a transformation primitive which takes a variable a and an expression b and inserts an assignment statement ASSIGN(a, b) at the given program points in the subject program. Here, it inserts ASSIGN(t, e) at newpoints in prog2. The resulting program is prog3.
The function RE is a transformation primitive which takes a program and re- places expressions at the given program points by a given variable. Here, it trans- forms program prog3 by replacing invariant occurrences of e at invoccurs by the variable t. This accomplishes loop invariant code motion.

%LICM(prog1 : (Program ), xs: (Loop (prog1 )), e: (LInv (prog1 , xs ))) : (Program )
LICM(prog1, xs, e): (Program ) =
let hdr	= Header(prog1, xs), preds	= EX(prog1, hdr), predsNotInLoop = (preds \ xs),
prog2	= SE(prog1, predsNotInLoop, hdr),
invoccurs	= Invs(prog1, xs, e),
t	= NEWVAR(prog2), newpoints	= (prog2‘cfg‘S \ prog1‘cfg‘S), prog3	= IA(prog2, newpoints, t, e)
in	RE(prog3, invoccurs, t)
Fig. 3. Specification of Loop Invariant Code Motion transformation

RE(prog,points,v): (Program ) =
(# cfg := prog‘cfg, entry := prog‘entry, exit := prog‘exit, L	:= λ(p:(prog ‘cfg ‘S )):
if (p ∈ points) then ASSIGN(Lhs(prog‘L(p)),BASE(V(v)))
else prog‘L(p) endif #)
SoundRE(prog,points,v): bool =
∀(p:(points )):Assign?(prog‘L(p)) ∧
let e = Rhs(prog‘L(p)) in v /∈ VOperands(e) ∧
AY prog,AS(prog,TranspNDef(prog,e,v),AssignStmt(prog,v,e)) (p)
Fig. 4. Definition of transformation primitive RE and its soundness conditions
Deﬁning Transformation Primitives and their Soundness Conditions
The transformation primitives are usually easy to define and their soundness condi- tions simple to characterize. The transformation primitive RE is defined in Fig. 4. It replaces expressions at program points points in a program prog by a base ex- pression constructed from a variable v. The constructor BASE gives an expression which merely consists of an operand. The constructor V constructs a variable type operand. The transformation primitive RE modifies only labeling function L of the subject program. The last transformation depicted in Fig. 2 is an application of RE transformation.
The soundness conditions of RE are defined as the predicate SoundRE in Fig. 4. Let p ∈ points. The transformations represented by RE are sound if: (1) p con- tains an assignment statement. (2) If e is the expression computed at p then the given variable v is not its operand. (3) Along all backward paths starting with the predecessors of p, the expression e is transparent and either the variable v is not defined or the expression assigned to it is e (denoted by TranspNDef) until a state- ment assigning e to v (denoted by AssignStmt) is encountered. This ensures that along all paths reaching p, the variable v has the same value as the expression e. In Appendix A, we prove that given the soundness conditions soundRE, RE preserves semantics of the input program.

Verifying Soundness of the Specifications
In the previous section, we identified some primitive transformations and expressed the optimizing transformations by composing them sequentially. We discussed the conditions under which the primitives preserve semantics. In this section, we discuss the verification scheme, TTL, and argue about soundness of LICM specification given in Fig. 3.

Veriﬁcation Scheme
Consider an optimizing transformation T defined in terms of the transformation primitives T1, ··· , Tk:

T (M ) =Δ let M
= T (M ,π ), ··· ,M = T	(M
,π	) in T
(M ,π )

1	2	1	1	1
k	k—1
k—1
k—1
k	k	k

where M1 is the abstraction of the input program. A transformation Ti is applied to an abstraction Mi at program points πi. Other parameters of the transformation primitives are implicit.
Let ϕ1,... , ϕk be the soundness conditions of the primitives T1,... , Tk. If a transformation primitive Ti is applied to a subset of ϕi(Mi), then Ti preserves semantics of Mi. Therefore, if we show that πi ⊆ ϕi(Mi), 1 ≤ i ≤ k, then each of the constituent transformations preserves semantics. This implies that the overall transformation T also preserves semantics.
The program points πi are identified over some program Mj, j ≤ i, whereas the safe application points ϕi(Mi) are identified over Mi only. Thus, we have to correlate program properties which define these points across different versions of the input program. To correlate temporal properties in such a manner, we develop a logic called temporal transformation logic (TTL). It relates temporal formulae whose outermost operators are the same. To prove the non-temporal properties, we use properties of the preceding transformations. In section 3.4, we discuss two non-temporal proof obligations viz. (A) and (B) and argue as to how they can be discharged.

Temporal Transformation Logic
A K-transformation f : M× 2S →M where M is the set of Kripke structures. Let M' = (S', R',P ', L') = f (M, π) where π ⊆ S. The K-transformations are classified depending on how they change structure of the input Kripke structure. Below we discuss a transformation and an inference rule associated with it that is relevant to this paper.
Consider two states i and j of M such that i is a predecessor of j. We want to add a new state k as a predecessor of j and a successor of i. The new state k is distinct from the states of M. We add an edge from i to k and an edge from k to
j. The edge from i to j is deleted whereas other edges of M are preserved. We call this transformation node addition or edge splitting.
As stated in section 2.2, we parameterize CTL formulae with Kripke structures to distinguish between their interpretations on different Kripke structures. Let N be an atomic proposition that denotes the new states. Let Δ denote EX, AX, EY, and AY. Let ∇ denote EU, AU, ES, and AS. The transformed Kripke structure M' is obtained by adding some states to M. The rule NA gives inference rules for node addition transformation.


(NA)
▶	Δ(M, ϕ)  =⇒  Δ(M', ϕ')
▶ ∇(M, ϕ, ψ)  =⇒  ∇(M', ϕ', ψ')

Lemma 3.1 NA is sound.
Proof. Available in [7]	 

If there is no structural change involved in a transformation then also above rule can be used. The set of new states N is empty in that case and the implication N =⇒ ϕ' is vacuously true.
TTL has inference rules for other classes of transformations like node splitting, node merging, node deletion, edge addition, and edge deletion. These can be com- posed to express various kinds of transformations. More on TTL is available in [7].
Why does LICM Preserve Semantics?
The soundness of LICM specified in Fig. 3 can be informally justified as follows:
The first transformation SE does not add or delete any control flow paths and inserts just SKIP statements. Hence the transformed program prog2 is semantically equivalent to prog1.
Since t is a new variable w.r.t. prog2, it does not modify any reaching definitions in prog2. The expression e is anticipatable at the loop header. The first transformation inserts newpoints as predecessors of the loop header. Hence the second transformation which inserts an assignment ASSIGN(t, e) at newpoints does not give rise to computation of any new value along any path and thus preserves semantics of prog2.
In the second transformation, an assignment ASSIGN(t, e) is inserted along all incoming edges of the loop header except the looping edge. Within the loop, neither the variable t nor any of the variable operands of the expression e are assigned. Hence t has the same value as e along all incoming paths to a program point in invoccurs. Therefore, the occurrence of e at such a program point can be replaced by t while preserving semantics of prog3.
An Example Proof of Soundness
We now prove soundness of the last transformation RE in LICM which is applied on prog3. It replaces the occurrences of the loop invariant expression e at the program points invoccurs within the loop xs with the variable t. We have to show that SoundRE(prog3, invoccurs, t) holds.
Let p ∈ invoccurs and e' = Rhs(prog3‘L(p)). We have to show that p which is a program point in prog1 is also in prog3. The program points invoccurs are defined over prog1. The same program points are identified in the transformed programs prog2 and prog3 by invoccurs since the transformations do not delete any program points. Similar is the case for xs, hdr, and newpoints. PVS generates type correctness conditions (TCCs) for them. They are discharged by rewriting the definitions of the transformation primitives. From the definition of SoundRE in Fig. 4, we have the following proof obligations:
(A) Assign?(prog3‘L(p)))
(B) t /∈ VOperands(e')
(C) AY(prog3,AS(prog3,TranspNDef(prog3, e', t),AssignStmt(prog3, t, e')))(p)
Proofs of (A) and (B):

(A) is proved by rewriting the definitions of the first two transformations SE and IA. SE inserts skip statements at the new points newpoints and IA replaces these skips by assignments ASSIGN(t, e). From the definitions of Invs, LInv, and Antloc in Fig. 1, we deduce that in prog1, the statement at p is an assignment statement. Since the first two transformations do not change contents of p, in prog3 it holds the same assignment statement as in prog1.
Clearly, the expression e' is same as the expression e = Rhs(prog1‘L(p)). Since t is a new variable w.r.t. prog2, it cannot be an operand of any expression in prog2. prog2 is a transformation of prog1 and e is an expression in prog1, therefore, t /∈ VOperands(e). (B) follows because e = e'.
Proof of (C):
Since e = e', we replace e' by e in (C). We have to show that in prog3, for all predecessors of p along all backward paths, the expression e is transparent and either the variable t is not defined or the expression assigned to it is e until a statement assigning e to t is encountered.
Auxiliary Results:
The first transformation SE inserts newpoints as predecessors to the loop header by splitting its incoming edges except the looping edge. The second transfor- mation IA does not change control flow of prog2. Using NA,
hdr =⇒ AY(prog3, newpoints ∨ xs)	(1)
From the definition of LInv in Fig. 1, the expression e is transparent in the loop xs in prog1. The first transformation SE inserts only SKIP statements. The second transformation IA inserts statements assigning the expression e to the new variable t at newpoints only. Hence
xs ∨ newpoints =⇒ TranspNDef(prog3, e, t)	(2)
newpoints ⇐⇒ AssignStmt(prog3, t, e)	(3)
The loop header belongs to the loop: hdr =⇒ xs. From (2),
hdr =⇒ TranspNDef(prog3, e, t)	(4)
From (1) and (3),
hdr =⇒ AY(prog3, AssignStmt(prog3, t, e) ∨ xs)	(5)
Our program abstraction has a unique entry. It has no incoming edges and is reachable along all backward paths. Since in CTLbp the past is finite, we have the following (derived) proof rule:
(β =⇒ AS(M, ϕ, ψ)) ∧ (ψ =⇒ ϕ ∧ AY(M, α∨β)) ▶ (β =⇒ AS(M, ϕ, α))   (6) Main Derivation:
{Program points in the loop xs are dominated by the loop header hdr in prog1.}
xs =⇒ AS(prog1, xs, hdr)
{The first transformation SE inserts newpoints. Using NA,}
xs =⇒ AS(prog2, xs ∨ newpoints, hdr)
{The second transformation IA does not change control flow of prog2.}


(instrumented)
P	P '

α	α


M	M '

πi ⊆ ϕi(Mi)?
Fig. 5. Validate an optimization against a generated trace
xs =⇒ AS(prog3, xs ∨ newpoints, hdr)
{From (2), (3), (4), and (5), using (6)}
xs =⇒ AS(prog3, TranspNDef(prog3, e, t), AssignStmt(prog3, t, e))
{All predecessors of a program point in xs belong to xs or it is the loop header.}
xs =⇒ AY(prog3, AS(prog3,TranspNDef(prog3, e, t),AssignStmt(prog3, t, e))) By definition, invoccurs =⇒ xs. Since p ∈ invoccurs,
AY(prog3, AS(prog3,TranspNDef(prog3, e, t),AssignStmt(prog3, t, e)))(p)	(7)
Similarly, it can be shown that the soundness conditions of other transformations are satisfied by LICM specification.

A Possible Approach for Validating Optimizers
While implementations can be validated against their provenly sound specifica- tions, our approach of identifying transformation primitives and their soundness conditions suggests a novel validation scheme shown in Fig. 5: Although an opti- mizer may not have been implemented using the transformation primitives, it can be instrumented to generate a trace of its execution as a sequence of appropriately instantiated primitives. For example, the trace of the transformations in Fig. 2 is SE({1}, {2}); IA({5}, t, a ∗ b); RE({3}, t).
The validation scheme consists of two parts: (1) The input program is abstracted and its transformed version is derived by simulating the trace on it. If the abstrac- tion of the optimized program matches this abstraction then the trace is faithful to the optimization performed. Two abstractions match each other iff their control flow graphs are isomorphic and contents at corresponding program points are same, modulo a globally consistent renaming of variables. (2) It is checked whether the soundness conditions of the transformation primitives used in the trace are met on the respective abstractions. This establishes whether the trace preserves semantics. If both these checks succeed then the optimizer preserves semantics of the input program.

This approach does not require any knowledge of the analysis employed in the optimizer because it directly uses the application points provided by the optimizer. Since the program abstractions are finite, it is reasonable to assume existence of an automatic checker for the soundness conditions. Such a checker along with a simulator, an implementation of the abstraction function, and a procedure for matching abstractions constitute the trusted computing base.

Automatically Generating Proof Obligations
We use PVS for specifying optimizations and verifying them. Emacs provides a front-end for PVS. We have built an Emacs based utility for automatically gener- ating soundness proof obligations from the specifications.
PVS parses and typechecks the specifications. It annotates the parse tree with typing information and keeps it in Common Lisp Object System (CLOS) format in PVS ILisp. PVS ILisp process runs as a subprocess of Emacs Lisp interpreter [2]. We probe the CLOS objects through Emacs interpreter using pvs-send-and-wait com- mand. We identify the transformation primitives and the context in which they are used by walking the annotated parse tree of the specification. We then generate a PVS theory containing the soundness proof obligations for each of the transforma- tion primitives used with the appropriate context. These proof obligations need to be discharged in order to prove that the optimization specification is sound. We are trying to develop high-level proof strategies so that these proof obligations can be discharged easily.

Related Work
Lacey et al. [9] specify optimizations as conditional rewrites whose enabling condi- tions are expressed in a temporal logic. They manually show the semantic equiva- lence of input and optimized programs. The rewrites are composed simultaneously. Due to this, as the number of rewrites increase, the proofs of semantic equivalence would get more complicated. Although program analyses are specified as temporal formulae, they prove semantic equivalence and hence cannot use temporal logic in the proofs. We also specify program analyses using a temporal logic. However, our transformation primitives are not general rewrites. Hence, it is possible to de- fine their soundness conditions. In our case, the proofs are much simpler owing to the fact that common patterns of semantic equivalence proofs are discharged sepa- rately and only once for each primitive. The primitives are composed sequentially and hence increase in the number of transformations does not affect the provability adversely.
Lerner et al. [10] follow an approach similar to Lacey et al. [9]. They use a re- stricted temporal logic to express analyses and require a property called witness for correlating analysis with semantics of the program. They can then automatically derive and discharge the required proof obligations. We also generate proof obliga- tions automatically. Though our proofs are not automated, they are mechanizable

to a large extent.
The translation validation approaches check semantic equivalence of input and optimized programs. They either use heuristics to guess the optimizations per- formed [11] or expect program annotations from compilers [15,18,19,1]. We do not address validation in as broad a sense as them and hence alleviate the checking of semantic equivalences of input and optimized programs.
Goldberg et al. [5] present a proof rule for reasoning about loop optimizations. They develop heuristics to determine which optimizations occurred and synthesize intermediate versions of the input program which may not have been generated by the compiler. This is similar to the approach in Fig. 5. However, we require an optimizer to generate a trace and check soundness conditions of the primitives used in it instead of semantics preservation.
The Verifix project [4,6] addresses the issue of construction of correct compilers. They distinguish between correctness of specifications and their implementations. They also consider the correctness when the machine resources are finite. In [3], the concept of program checking with certificates is introduced and applied to opti- mizing compiler back-ends. The compiler generates a trace of its search for optimal target code as a certificate of correctness.
Certifying compilers [12] generate type specifications and code annotations in addition to assembly code. This additional information is used to prove type and memory safety of target code. The proof obligations are generated, discharged, and checked outside of the compiler. [17,16] propose how compilers themselves can generate correctness proof for each run. These approaches require extensive instrumentation of compiler.

Conclusions and Future Work
We address issues regarding soundness of optimizations in two steps. We first iden- tify transformation primitives common to several optimizations and define sufficient conditions for ensuring soundness of these primitives. We then specify an optimiz- ing transformation as sequential compositions of appropriately chosen transforma- tion primitives. Consequently, proving the soundness of an optimization reduces to showing that soundness conditions of the underlying transformation primitives are satisfied. This reduces the overall verification efforts: The proofs of semantic equivalence need to be done only once for each primitive and are independent of any particular optimization. The primitives are much simpler than the optimizing transformations and hence the semantics preservation proofs are easier.
Based on our approach of identifying transformation primitives and their sound- ness conditions, we suggest a novel validation scheme: An optimizer can be instru- mented to generate a trace of its transformations in terms of the transformation primitives. Conformance of the trace with the optimizer can be checked through simulation. If soundness conditions of the underlying primitives are satisfied by the trace then it preserves semantics.
At present, our method can handle optimizations based on bit vector analyses.

We are developing TTL inference rules for several kinds of transformations. As part of future work, we would like to apply them for proving soundness of control flow optimizations that may change program structures significantly like loop unrolling, loop fusion, etc. Our framework needs to be extended for handling optimizations like constant propagation which are based on non-bit vector analyses.

Acknowledgement
Authors wish to thank Supratik Chakraborty for many insightful discussions. Au- thors are thankful to C´esar Mun˜oz for clarifying PVS related doubts.

References
C. Barrett, Y. Fang, B. Goldberg, Y. Hu, A. Pnueli, and L. Zuck. TVOC: A translation validator for optimizing compilers. In Proceedings of CAV’05, volume 3576 of LNCS, pages 291–295. Springer-Verlag, July 2005.
J. Crow, S. Owre, J. Rushby, N. Shankar, and D. Stringer-Calvert. Evaluating, testing, and animating PVS specifications. Technical report, Computer Science Laboratory, SRI International, Menlo Park, CA, March 2001.
S. Glesner. Using program checking to ensure the correctness of compiler implementations. Journal of Universal Computer Science, 9(3):191–222, 2003.
W. Goerigk, A. Dold, T. Gaul, G. Goos, A. Heberle, F. von Henke, U. Hoffmann, H. Langmaack,
H. Pfeifer, H. Ruess, and W. Zimmermann. Compiler correctness and implementation verification: The Verifix approach. In poster session of CC’96. Technical Report LiTH-IDA-R-96-12, Linkping, Sweden, 1996.
B. Goldberg, L. Zuck, and C. Barrett. Into the loops: Practical issues in translation validation for optimizing compilers. In Proceedings of COCV’04, volume 132(1) of ENTCS, pages 53–71. Elsevier, May 2005.
G. Goos and W. Zimmermann. Verification of compilers. In Ernst-Ru¨diger Olderog and Bernhard Steffen, editors, Correct System Design, volume 1710 of LNCS, pages 201–230. Springer, 1999.
A. Kanade, A. Sanyal, and U. Khedker. Temporal transformation logic. Technical Report TR-CSE- 001-06,  CSE,  IIT  Bombay,  January  2006.  Available  at: http://www.cse.iitb.ac.in/∼ aditya/reports/TR-TTL.ps .
O. Kupferman and A. Pnueli. Once and for all. In Proceedings of LICS’95, pages 25–35. IEEE Computer Society, 1995.
D. Lacey, N. D. Jones, E. Wyk, and C. Frederiksen. Proving correctness of compiler optimizations by temporal logic. In Proceeding of POPL’02, pages 283–294. ACM Press, 2002.
S. Lerner, T. Millstein, and C. Chambers.	Automatically proving the correctness of compiler optimizations. In Proceedings of PLDI’03, pages 220–231. ACM Press, 2003.
G. Necula. Translation validation for an optimizing compiler. In Proceedings of PLDI’00, pages 83–94. ACM Press, 2000.
G. Necula and P. Lee. The design and implementation of a certifying compiler. In Proceedings of PLDI’98, pages 333–344. ACM Press, June 1998.
S. Owre, N. Shankar, J. M. Rushby, and D. W. J. Stringer-Calvert. PVS Language Reference. CSL, SRI International, Menlo Park, CA, September 1999.
S. Owre, N. Shankar, J. M. Rushby, and D. W. J. Stringer-Calvert. PVS System Guide. CSL, SRI International, Menlo Park, CA, September 1999.
A. Pnueli, M. Siegel, and O. Shtrichman. Translation validation: From SIGNAL to C. In E.-R. Olderog and B. Steffen, editors, Correct System Design, volume 1710 of LNCS State-of-art Survey, pages 231–
255. Springer Verliag, 1999.
A. Poetzsch-Heffter and M. Gawkowski. Towards proof generating compilers. In Proceedings of COCV’04, volume 132(1) of ENTCS, pages 37–51, 2005.


M. Rinard and D. Marinov. Credible compilation with pointers. In Proceedings of the FLoC Workshop on Run-Time Result Verification, July 1999.
L. Zuck, A. Pnueli, Y. Fang, and B. Goldberg. VOC: A translation validator for optimizing compilers. In Proceedings of COCV’02, volume 65(2) of ENTCS, 2002.

L. Zuck, A. Pnueli, B. Goldberg, C. Barrett, Y. Fang, and Y. Hu. Translation and run-time validation of loop transformations. Formal Methods in System Design (FMSD), November 2005.

A  Proof of Semantics Preservation for RE
Let V alue be the domain of values that variables in a program can take. It contains a special element true. A function op) : V aluen → V alue denotes an n-ary operator op. A constant is a 0-ary operator. Consider a domain Store = Variables(prog ) → V alue which denotes valuations of variables in a program prog. A function · ) : Expressions(prog ) × Store → V alue evaluates expressions in a program prog. Let σ ∈ Store.
 v)σ = σ(v) where v is a variable
 c)σ = c) where c is a constant
 op(o1,... , o2))σ = op)( o1)σ,... , on)σ) where o1, . .., on are operands
Definition A.1 (Statement Semantics.) Consider a program prog and a do- main State = (prog‘S ∪ {⊙}) × Store representing program states where ⊙ is a special program point indicating termination, i.e., it does not hold any statement. The state transition relation ~: State × State defines how statements affect the program state. Let p ∈ prog‘S and σ ∈ Store.
If prog‘L(p)= SKIP then (p, σ) ~ (p', σ) where (p, p') ∈ prog‘cfg‘τ .
If prog‘L(p) = ASSIGN(v, e) then (p, σ) ~ (p', σ[v '→ e)σ]) where (p, p') ∈ prog‘cfg‘τ and σ[v '→ z] updates σ by mapping v to z and keeping rest of the mappings the same.
If prog‘L(p) = ITE(C, p1, p2) then for C)σ = true, (p, σ) ~ (p1, σ); otherwise (p, σ) ~ (p2, σ).
If prog‘L(p)= HALT then (p, σ) ~ (⊙, σ).
Definition A.2 (Program Trace.) Consider a program prog. A program trace ρ is a possibly infinite sequence of states s1 ~ ··· ~ sn ~ ··· where s1 = (prog‘entry, σ1) is the initial state and σ1 is the initial store.
Definition A.3 (Semantic Equivalence.) Consider two programs prog1 and prog2 whose state transition relations are ~ and ~' respectively. They are seman- tically equivalent if for every finite trace ρ = s1 ~ ··· ~ (⊙, σ|ρ|) of prog1 there
exists a finite trace ρ' = s' ~' ··· ~' (⊙, σ' ' ) of prog2 such that the initial and
final stores are the same: σ1 = σ' and σ|ρ| = σ' ' .
Theorem A.4 If SoundRE(prog, points, v) deﬁned in Fig. 4 holds then the programs
prog and RE(prog, points, v) are semantically equivalent.

Proof. Let prog1 = RE(prog, points, v) and ~' be its state transition relation. Con- sider a finite trace ρ = s1 ~ ··· ~ (⊙, σ|ρ|) of prog. We show by induction that

for every prefix ρi of length i ≤ |ρ| of ρ there exists a prefix ρ'
of a finite trace

ρ' = s' ~' ··· ~' (⊙, σ' ' ) of prog1 such that si = s'.
1	|ρ |	i
Base Case. Consider the initial state s1 = (prog‘entry, σ1) of ρ. From the definition of RE in Fig. 4, prog‘entry = prog1‘entry.  Since both programs have the same
variables, we can define s' = (prog1‘entry, σ1).
Induction Hypothesis. Suppose for some k, 1 ≤ k < |ρ|, and ρk there exists ρ'
such that sk = s' where sk = (pk, σk) and s' = (p' , σ' ).
k	k	k	k
Induction Step. Since the language is deterministic, every state has exactly one

successor state.	Suppose sk ~ sk+1 and s'
~' s'
k+1
. We have to show that

sk+1 = (pk+1, σk+1)= s'
'
k+1
'
k+1
). We have two cases:

Suppose pk /∈ points. From the hypothesis, (pk, σk)= (p' , σ' ). From the defi-
k	k
nition of RE in Fig. 4, prog‘L(pk )= prog1‘L(p' ). Since RE does not change the flow
of control, for all pj, (pk, pj) ∈ prog‘cfg‘τ implies that (p' , pj) ∈ prog1‘cfg‘τ . Thus,

(pk+1, σk+1)= (p'
k+1).

Suppose pk ∈ points.	pk satisfies the conditions stated in SoundRE.	Let
prog‘L(pk) = ASSIGN(x, e). From the hypothesis, (pk, σk) = (p' , σ' ). From the
k	k
definition of RE in Fig. 4, prog1‘L(p' )= ASSIGN(x, BASE(V(v))). From the defini-
tion A.1, σk+1 = σk[x '→  e)σk] and σ'	= σ' [x '→ v)σ' ]. Therefore, we have to
k+1	k	k
show that  e)σk = v)σ' . Since σk = σ' , this reduces to  e)σk = v)σk.
k	k
From the definition of SoundRE in Fig. 4,
AY(prog, AS(prog, TranspNDef(prog, e, v), AssignStmt(prog, v, e)))(pk )	(A.1)
From the definition of statement semantics A.1, (p1,... , pk) is a maximal backward control flow path in prog. From (A.1),
∃j :1 ≤ j < k : AssignStmt(prog, v, e)(pj) ∧
(∀l : j < l < k : TranspNDef(prog, e, v)(pl))
Let us instantiate the existential quantifier by j and skolemize the universal quan- tifier by l. Following are the definitions of AssignStmt and TranspNDef:
Δ

AssignStmt(prog, v, e)(pj)
TranspNDef(prog, e, v)(pl)
= prog‘L(pj)= ASSIGN(v, e)
Δ
= Assign?(prog‘L(pl)) =⇒

(Lhs(prog‘L(pl)) /∈ VOperands(e) ∧
(Lhs(prog‘L(pl)) /= v ∨ Rhs(prog‘L(pl)) = e)) From the statement semantics A.1,
σj+1 = σj[v '→ e)σj] which implies that  v)σj+1 = e)σj	(A.2)
 e)σl+1 = e)σl and ( v)σl+1 = v)σl or v)σl+1 = e)σl)	(A.3) From the definition of SoundRE, v /∈ VOperands(e). Therefore, e)σj+1 = e)σj. From (A.2),  v)σj+1 =  e)σj+1. A program point pl is preceded either by a pro-
gram point pj or another program point pl. Therefore, v)σl = e)σl. From (A.3),

 v)σl+1 =  e)σl+1. Since 1 ≤ j < k and j < l < k, we have  v)σk =  e)σk.
Therefore, σk+1 = σ'	.
Since RE does not change the flow of control, for all pj, (pk, pj) ∈ prog‘cfg‘τ

implies that (p' , pj) ∈ prog1‘cfg‘τ . Thus, (pk+1, σk+1)= (p'	, σ'
).	 

k	k+1	k+1
