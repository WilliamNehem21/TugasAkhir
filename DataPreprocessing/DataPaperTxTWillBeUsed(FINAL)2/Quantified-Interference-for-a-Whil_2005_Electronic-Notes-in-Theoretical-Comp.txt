Electronic Notes in Theoretical Computer Science 112 (2005) 149–166 
www.elsevier.com/locate/entcs


Quantified Interference for a While Language
David Clark1
Department of Computer Science King’s College London
London, UK
Sebastian Hunt2
Department of Computing City University London, UK
Pasquale Malacaria3
Department of Computer Science Queen Mary College, University of London
London, UK

Abstract
We show how information theory can be used to give a quantitative definition of interference between variables in imperative programming languages. In this paper we focus on a particular case of this definition of interference: leakage of information from private variables to public ones in While language programs. The major result of the paper is a quantitative analysis for this language that employs a use-definition graph to calculate bounds on the leakage into each variable.
Keywords: security, program analysis, information theory, information flow, interference


Introduction
Quantifying interference is an important part of assessing covert channels in security devices, and doing this is less well-established for programmable

1 Email: david@dcs.kcl.ac.uk
2 Email: seb@soi.city.ac.uk
3 Email: pm@dcs.qmul.ac.uk



1571-0661 © 2004 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.01.018


components than for simple hardware components.
Take the notion of interference [5,10] between program variables, infor- mally the capability of variables to affect the value of other variables. Ab- sence of interference (non-interference) is often used in proving that a system is well-behaved, whereas interference can lead to mysterious (mis-)behaviours. However, significant misbehaviours caused by interference will generally hap-
pen only when there is enough interference. Concrete examples of this are provided by access control based software systems. To enter such a system
the user has to pass an identification stage; whatever the outcome of this stage (authorisation or failure) some information has been leaked (in the case of failure the search space for the right key has now become smaller). Hence these systems present interference [5] so they are not “secure” in a qualitative sense. However, common sense suggests to consider them secure if the inter- ference is very small. This paper uses Shannon’s information theory [13] to define a quantified notion of interference for a simple imperative language and derives a program analysis based on this notion.
We briefly recall Shannon’s key definitions and use them to define a quan- titative measure of the leakage into each variable at each program point for a While language. We then use these definitions as the basis for a program anal- ysis which derives bounds on the quantity of confidential information leaked by a program.
In a previous paper [1] we sketched an information theory based program analysis for a simple language without loops. The achievements presented in this paper are:
Analysis structure and loops: The analysis is now graph and not syntax based; this allows us to handle loops.
Equality tests: we analyse general equality tests (not just tests against a constant, as previously).
Arithmetic expressions: we present a significantly improved analysis of arithmetic operators which exploits their algebraic properties.

Related work
The work we describe in this paper is not the first attempt to apply informa- tion theory to the analysis of confidentiality properties. The earliest example of which we are aware is in Denning’s book [4] where she gives some examples of how information theory may be used to calculate the leakage of confidential data via some imperative language program constructs. However she does not develop a systematic, formal approach to the question as we do in this pa- per. Another early example is that of Millen [8] which points to the relevance


of Shannon’s use of finite state systems in the analysis of channel capacity. More recent is the work of Gray [14], which develops a quite sophisticated operational model of computation and relates non-interference properties to information-theoretic properties. However, neither of these deals with the analysis of programming language syntax, as we do here. Contemporary with our own work has been that of Di Pierro, Hankin and Wiklicky [9]. Their interest has been to measure interference in the context of a probabilistic con- current constraint setting where the interference comes via probabilistic op- erators. They derive a quantitative measure of the similarity between agents written in a probabilistic concurrent constraint language. This can be inter- preted as a measure of how difficult a spy (agent) would find it to distinguish between the two agents using probabilistic covert channels, with a measure of 0 meaning the two agents were indistinguishable. Their approach does not deal with information in an information-theoretic sense although the implicit assumption in example 4 in that paper is that the probability distribution of the value space is uniform.
By contrast, much more has been done with regard to syntax directed analysis of non-interference properties. See particularly the work of Sands and Sabelfeld [11,12]. However, we aren’t aware of any work which develops
an automatable analysis for the quantity of information (in Shannon’s sense)
which may be leaked by a program.
A paper on confidentiality properties which has recently come to the au- thors’ attention, and which does use information theory, is [7]. Their definition of ‘information escape’ is similar to our definition of leakage (sect 2.3), though it does not appear to be equivalent. The focus of [7] is on the relationship be- tween security properties and refinement, rather than analysing the quantity of information leaked by a program. The connections with the work of the current paper deserve further investigation.

Information theory and leakage analysis
The language
The language contains just the following control constructs: assignment, while- statements, if-statements, sequential composition. The left hand sides of as- signments are variable identifiers, the right hand sides are integer or boolean expressions; while loops and if-statements involve boolean expressions in the standard way. We do not fully specify the language of expressions but we make the assumption that all expressions define total functions on stores. The language is deterministic and so, for each program P , the semantics induces a partial function [P ]] : Σ → Σ, where Σ is the domain of stores. A store σ ∈ Σ


is just a finite map from variable names to k-bit integers (integers n in the range −2k−1 ≤ n < 2k−1) and booleans.
We note that the primary interest in dealing with loops is not the possibility of non-termination but the potential for an arbitrary quantity of information to be leaked via iterations containing implicit information flows (an implicit flow being one which is achieved via control flow rather than assignment from confidential sources [4]).
Given a program P , a program point is either the special node ω (the exit point ), or any occurrence in P of an assignment statement, if-statement or while-statement. We call the top-most program point ι (the entry point ). The operational semantics is standard and defines a transition relation → on configurations (n, σ), where n is a program point and σ is a store. A trace is a sequence of configurations (n1, σ1) ··· (nj, σj) such that (ni, σi) → (ni+1, σi+1)
for 1 ≤ i < j.

Interference and information theory
We suppose that the variables of a program are partitioned into two sets, H (high) and L (low ). High variables may contain confidential information when the program is run, but these variables cannot be examined by an attacker
at any point before, during or after the program’s execution. Low variables do not contain confidential information before the program is run and can be freely examined by an attacker before and after (but not during) the program’s execution. This raises the question of what an attacker may be able to learn about the confidential inputs by examining the low variable outputs. One
approach to confidentiality, quite extensively studied [5], is based on the notion of non-interference: whether or not a program leaks confidential information. Here by contrast we address the question of how much may be leaked.
We use Shannon’s information theory to quantify the amount of informa- tion a program may leak and the way in which this depends on the distribution of inputs. For a discussion of the information theoretic background to the cur- rent work see our earlier papers [1,2]. Recall that Shannon’s theory is based
on the fundamental quantity H, variously known as information or entropy,
defined thus:

(1)
H(X) d=ef	p(x) log	1
p(x)

x
where X is a random variable, p(x) is shorthand for P (X = x), the probability that random variable X = x, and the sum is over the range of X.  The
derived quantity of conditional entropy is H(X|Y ) d=ef Σ p(y)H(X|Y = y),
where H(X|Y = y) d=ef Σ p(x) log  1  and p(x|y) is the probability that the


random variable X = x given that random variable Y = y.

Random variables and program points
We define for each program point a random variable corresponding to observa- tions of the value of the variable at this point. In particular, we are interested in how much of the information carried by the high inputs to a program can be learnt by observation of the low outputs, assuming that the low inputs are known. Since our language is deterministic, any variation in the outputs is a result of variation in the inputs. Once we account for knowledge of the pro- gram’s low inputs, therefore, the only possible source of surprise in an output is interference from the high inputs. Given a program variable (or set of pro- gram variables) X, let Xι and Xω be, respectively, the corresponding random variables on entry to and exit from the program (assume termination for now; this is relaxed below). We take as a measure of the amount of leakage into X due to the program:
def
(2)	L(X) = H(Xω|Lι)
(where Lι is the random variable describing the distribution of the program’s non-confidential inputs). We note that non-interference in this setting is just the special case of 0 leakage. (We give a precise formal presentation of this result, and more general discussion of why the definition is appropriate, in [2].)
The random variable for X at a program point n is defined to be such that P (Xn = x) is the probability that X takes the value x given that control passes through program point n. However n may be unreachable or, for a given input
store σ, control may actually pass through n many times, with X taking differ- ent values at different times. For these reasons, Xn is only defined for a subset of the program points. Let t be a trace (n1, σ1) ··· (nj, σj) with n1 = ι. The trace t decides nj if, for all traces which extend t, ni = nj implies i ≤ j. Given
a program point n, let ∆(n) be the set {(σ, σ')|(ι, σ) ··· (n, σ') decides n}. We write p(n) for the sum of the probabilities of the domain of ∆(n):

p(n) =
Σ

(σ,σ' )∈∆(n)
p(σ)

∆(n) can be interpreted as a random variable on its domain but, in general, we are interested in particular projections of ∆(n). In particular, the random variable Xn has the same domain as ∆(n) and is defined just when p(n) > 0:


P (Xn
def Σ(σ,σ' )∈∆(n),σ'(X)=x p(σ)



p(n)


(X may be a vector of variables, in which case σ'(X) means the elementwise application of σ to its elements). Note that Xn describes the values taken by X immediately before execution of any instruction at n. The definition of random variables Xn automatically extends to an analogous definition of random variables En, where E is any expression in the language (En is the
random variable describing the values which E takes if evaluated at n).
We generalise the definition of leakage above (equation 2) as follows. Let n be any program point. Then the leakage into E at n is Ln(E) d=ef p(n)H(En|Lι), taking Ln(E) to be 0 when p(n) = 0. Note that, for programs which always terminate, p(ω) = 1, so this generalises the previous definition with L(X) = Lω(X). The remainder of this paper is devoted to showing how bounds on
Ln(E) may be calculated.

Analysing programs for leakage
This section presents the analysis, which has two main parts:
A qualitative one (3.1–3.2): where we associate to the syntax of a program a graph summarising its information flow connections.
A quantitative one (3.3–3.6): where we provide bounds on the amount (number of bits) of information leaked along this graph.
The section ends with the correctness result.
The graphs we use are a form of use-definition graph. We prefer these to the syntax because they expose the structure we use to deal with loops. It should be noted that loops are a significant hurdle in the analysis of leakage. Consider the program in sect 3.1. A little thought shows that, for in taking values in 0 ≤ i < 216 − 1, this program copies in to out via the variables high and low, even though there is no assignment to low from high.

Use Deﬁnition Graphs
Given a program, the use-deﬁnition graph (UDG) is a directed graph whose nodes are program points. If n is an occurrence of an assignment X = E we call n a deﬁnition node and say that n defines X. A node n is called a use for the variable Y if Y appears in the expression at n (that is, the boolean
expression of a control construct or the right hand side of an assignment). There are two types of edges:
data edges (n −→ p): there is a data edge from n to p iff there is a non-empty path in the flowchart for the program starting from n and reaching p without any definition of X intervening and n is a definition


of X or n = ι, and p is a use of X or p = ω;
control edges (n −−· p): there is a control edge from n to p iff n is either a while or an if-statement and p is an assignment which occurs inside that statement.
We write =⇒ for —→ ∪ −−· and =⇒∗ for its transitive-reflexive closure.
Consider the following example program:
int high = in;
int b = 15, low = 0; while (b >= 0) {
int m = (int)Math.pow(2,b); if (high >= m) {
low = low + m; high = high - m;
}
b = b - 1;
}
out = low;
Figure 1, shows the data edges (a) and control edges (b) for this program.


Source nodes
When calculating the quantity of information which has flowed into a variable at a particular program point, we use the UDG to identify the other parts of
the program which make an immediate contribution; we call these the source nodes for the given occurrence of the variable. In defining these source nodes,
we need to distinguish between those program points which lie inside a while- statement and those which don’t; we write n /∈ W to mean that n does not lie within any while-statement. We need to make this distinction because of the way our analysis abstracts away from the intricacies of possible cyclic flows of information within loops: the approach within a loop (roughly speaking) is to treat a UDG path in the same way as an edge is treated outside.
To understand the following definitions it will help to bear in mind that
a −−· b iff b lies within the control structure a (if or while).
Let n be a use of the variable X; there are two types of source node, the control source nodes for X at n, denoted conn(X), and the data source nodes for X at n, denoted datn(X). Wherever n occurs in the program, conn(X) is the set

 

m∈datn(X)
{m' /∈ W : m' −−· m and m'−−· n}


high=in



b=15	low=0



while (b>=0)

m=pow(2,b) if (high>=m)
low=low+m high=high−m
b = b−1


out=low
out=low


(a) data edges	(b) control edges

Fig. 1. UDG for a simple Java program

The definition of datn(X) varies according to whether or not n lies within a
while-statement:
If n lies within a while-statement, let w be the outermost while con- taining n, then datn(X) is the set: {m : w−−· m, ∃m'. w −−· m' ∧ m —→ m' =⇒∗ n}.
If n does not lie within a while-statement, then datn(X) is the set:
{m : m defines X, m —→ n}.
In the definition of conn(X), note that the restriction of m' to points not in any while-statement has the consequence that all control source nodes are if-statements and that no control source node lies within a while-statement. This is not because while conditions cannot influence control flow (they clearly do) but because our analysis of loops is more pessimistic than our analysis of if-statements.
Thus, the data source nodes for X at n are the assignments immediately prior to n (or to the outermost while containing n); the control source nodes are those if-statements which determine which (if any) of those assignments actually occur (assuming that control passes through n).
Where it is not necessary to distinguish between data and control source


nodes, we consider the union: srcn(X) = datn(X) ∪ conn(X).
Each internal node in the UDG (that is, every node except ι and ω) has an associated expression: the right hand side for an assignment, the boolean condition for a control statement; we call this the expression at n, written
E(n). It is often necessary to consider the set of all source nodes for all the variables occurring in the expression at a node or one of its sub-expressions; given any such expression E, we denote this set srcn(E):
srcn(E) =  {src (X) : X occurs in E}
def
Fig 2(a) Illustrates the idea that for a definition inside a while loop, all the source nodes lie outside the loop. It shows a typical set of source nodes for the rhs of an assignment X=E at a program point p inside a loop.

high=in



b=15	low=0









low=low+m high=high−m

b = b−1 out=low

(a)	(b)

Fig. 2. Calculating source nodes within loops

Figure 2(b) shows a possible set of paths (obeying the existence constraints of the definition above) to the assignment low=low+m from its data source nodes. From this we can see that the rhs of the corresponding assignment inside the loop in the example from sect 3.1 has associated the set of source nodes {high=in, b=15, low=0}.


Demonic attackers
Until now we have (implicitly) assumed a probability distribution on the space of initial stores which is independent of the choice of program. There are two potential problems with this assumption:
while it is reasonable to assume that some knowledge will be available as to the distribution of the high inputs, it is likely that little or no knowledge will be available about the low inputs;
the distribution for low inputs may actually be in the control of the attacker; in this case it would be conservative to assume that the attacker chooses Lι to maximise leakage.
We deal with both of these problems by constructing our analysis to give results which are safe for all possible distributions on the low inputs. The approach is, essentially, to suppose that the low inputs take some fixed (but unknown) value λ. The safety of this approach is verified by proposition 3.1.
Note that we assume that the high inputs are not in the control of the attacker.
Thus we are modelling a situation in which the environment delivering high inputs to the program is trusted, even though the program itself is not. This is appropriate for example in the analysis of untrusted code which is to be downloaded and run on a user’s computer, where the user is the owner of the confidential data.
For each possible choice Lι = λ, we define pλ(n) to be the probability that program point n is eventually decided (see sect 2.3) given that Lι = λ. Formally:

pλ(n) =
Σ

(σ,σ' )∈∆λ(n)
p(σ)/P (Lι = λ),

where ∆ (n) d=ef {(σ, σ') ∈ ∆(n) : σ(L) = λ}. Now we can define the random variables at a program point given that Lι = λ: just when pλ(n) > 0, we define Xn d=ef Xn|Lι = λ. Finally, we can define the leakage into X at n given
that Lι = λ: Ln(X) d=ef p (n)H(Xn), taking Ln(X) d=ef 0 when p (n) = 0.
λ	λ	λ	λ	λ
From here on, we assume that λ is fixed but make no assumption as to its identity. This is conservative with respect to Ln(X), as shown by the following:
Proposition 3.1 (∀λ.Ln(X) ≤ a) ⇒ Ln(X) ≤ a

Note that, for all X ∈ L and for all λ, Lι (X) = Lι(X) = 0. Furthermore, when the high-security and low-security inputs are independent, Lι (Y ) =
Lι(Y ) = H(Y ι), for all Y ∈ H.


Total versus partial random variables
The rules we present below are intended to derive bounds on the leakage into a variable at a program point, given only assumptions on the entropy of the confidential variables at the entry point. Such assumptions actually give very limited knowledge of the distribution of input values and this means that a direct calculation of the leakage at a program point is usually not possible. To illustrate, suppose program point n is the assignment L = H in the following program:
if (H < 0) then L = H else L = 1 fi
Now suppose that H and L are independent 32 bit variables and H(H) = 16. To calculate directly the leakage into L we need to calculate Ln(H), but this in turn requires us to calculate both p(n), which is just the probability that the condition (H < 0) evaluates to true, and the entropy of H given that (H < 0) evaluates to true. But knowing only H(H) = 16 we cannot calculate either quantity. In particular, H(H|H < 0) can take essentially any value between 0 and 31. (For the lower bound, let p(h) = 0 for the lowest (232 — 216) values of h, p(h) = 1/216 for the rest. For the upper bound, let b = 231−a and ab — (1 — b)log(1 — b) = 16; then let p(h) = 1/2a when h < 0, p(0) = 1 — b, p(h) = 0 when h > 0.)
We deal with this difficulty by calculating bounds on the entropy of a hypothetical random variable Xˆn which is defined everywhere. Let χn be the characteristic function of the domain of ∆(n): χ (σ) d=ef 1 if ∃σ'.(σ, σ') ∈ ∆(n);
χn(σ) = 0 otherwise.  Then ˆn is defined to be a total random variable
such that Xn = Xˆn|χn = 1. The Xˆn are defined in the proof of correctness
λ	λ	λ
(theorem 3.2).


Analysis rules
The basic analysis is given by the rules shown in table 1. For ease of exposition, we assume the following:
high variables (H1, H2,.. .) are only ever used as the rhs of assignments of the form X = Hi, and then at most once
no high variable is ever assigned
Note that these assumptions are of presentational significance only, since once
Hi has been copied into X, the copy can be used and assigned freely.
A judgement n ▶ [E] ∼ a (where ∼ is one of ≤, ≥, =) is to be read as asserting that H(Eˆn) ∼ a. The initial assumptions for an analysis will be



Table 1 Analysis rules


given as special initialisation axioms for the high variables, each of the form:

n ▶ [Hi] ∼ a
where n is the first and only use of Hi. In all remaining judgements, two things are implicit:
E is either E(n) or a sub-expression of E(n)
E is not a high variable Hi
In rule [Max], bits(E) means the number of bits of storage determined by the type of the expression E (for example, if E is boolean, bits(E) is 1; if E is of Java’s int type, bits(E) is 32).
Note that, unlike the other rules, the conclusion of rule [Low] applies only to variables, not arbitrary expressions, and applies only to program points which do not lie inside any while-statement. Rule [DP] is so named because it is essentially justified by the so-called Data Processing theorem of infor- mation theory [3]: the quantity of information output by a function cannot exceed the quantity input. Rule [Const] is really a special case of rule [DP] but is stated separately for emphasis: constant expressions transmit zero in- formation. As an example, consider how these rules can be applied at the
statement low=low+m inside the loop example from sect 3.1. If we assume that in is uniformly distributed over 0 ≤ i < 216, it is easy to see, using the rules, that we get a leakage of all 16 bits into low.


Correctness
Theorem 3.2 Suppose, for each initialisation axiom deriving n ▶ [H] ∼ a
and for all λ, that H(Hι ) ∼ a. Then, for each program point n, each sub-

expression E of E(n), and each λ, there exists a random variable that:
En = Eˆn|χn = 1 
ˆn  such

λ	λ
whenever the rules in table 1 derive n ▶ [E] ∼ a, then H(Eˆn) ∼ a
Corollary 3.3 n ▶ [E] ≤ b implies Ln(E) ≤ b.
The proof of part 2 of the theorem hinges on the following lemma:
Lemma 3.4 Let Eˆn be as given by part 1 of the theorem. Then, if srcn(E) = 

λ	n1
{n1,... , nk}, there exists a function f such that Eˆn = f (E^(n )
nk
,... , E(n )  ).

λ	1 λ	k λ
Correctness of the rules in table 1 follows because the entropy of f (X) cannot be greater than that of X (the so called Data Processing Theorem of information theory [3]).

Refining the analysis of expressions
The rules in table 1 allow us to measure flows of information through the program but in a crude way, analogous to plumbing pipes together. The utility of an implemented analysis based on our approach will depend on its sensitivity, i.e. on being able to establish tight as opposed to loose bounds on the leakage of information. Although we are limited to the basic analysis rules inside loops, outside loops these rules may be refined to exploit the properties of the particular operators used in boolean and arithmetic expressions. The discovery of these refined rules is an on-going project. In the remainder of this section we present and justify improvements to the analysis of equality tests
and some arithmetic operations. The resulting rules may be found in table 2. Note that these rules apply only outside loops.

Analysis of equality tests
In this section we develop a refined rule for analysis of tests of the form E1==E2. The development reveals that calculating good lower bounds on the entropy of expressions at intermediate points will sometimes enable the calculation of good upper bounds on leakage. Our development is motivated by a simple observation: when the distribution of values for E1 is close to


uniform (high entropy) and the distribution for E2 is concentrated on just a few values (low entropy), then most of the time, E1 and E2 will not be equal. Suppose that X is a k-bit random variable and suppose that P (X = x) = q,
for some q; what is the maximum possible value for H(X)? We call this maximum the upper entropy for q in k bits, denoted Uk(q). Since entropy is maximised by uniform distributions, the maximum value possible for H(V ) is
obtained in the case that P (X = x') is uniformly distributed for all x' /= x. There are 2k — 1 such x' and applying the definition of H (eqn. 1) gives:

def	1
2k — 1

(3)
Uk(q) = q log q + (1 — q) log
1 — q

As the following proposition shows, if P (X = Y ) = q, then Uk(q) is an upper bound for the difference between H(X) and H(Y ).
Proposition 4.1 Given a k-bit random variable X and any other random variable Y , let q = P (X = Y ). Then H(X|Y ) ≤ Uk(q).
As an immediate corollary we have H(X) — H(Y ) ≤ Uk(q). Now the quantity of interest (H(X==Y )) is just B(q), where

(4)
B(q) = q log 1
q
+ (1 — q) log
1


1 — q

It is easily seen that B(q) is an increasing function of q in the region 0 ≤ q ≤ 0.5 and this is sufficient to justify the refined rule [Eq] for equality tests (see table 2).
(We leave unstated the companion rule, justified by commutativity of =, which reverses the roles of E1 and E2.) We note that [Eq] will give useful results (that is, much less than 1) in the case that a is high and b is low, that is, when E1 is known to contain a large amount of confidential information and E2 is known to contain very little.
The way in which rule [Eq] can be applied is illustrated by the example shown in fig. 3. This plots Uk(q) and B(q) against q for k = 4 and shows that for a lower bound of (a — b) = 3.75, q is bounded by 0 ≤ q ≤ 0.25 (the precise upper bound is slightly lower than this). To find a maximum for q, we need to solve equations of the form Uk(q) — (a — b) = 0 and, for this, simple numerical techniques suffice [6]. (Note also that B(q) + (1 — q)k is an upper bound for Uk(q) and that this bound is very tight unless k is small.)
As an example, consider the program P :
Y = H; [p]if (Y == 0) then [n0]X = 0 else [n1]X = 1 fi; [n2]Z = X
Suppose that k = 32 and the input distribution for H is uniform. Thus we can analyse the program starting with the assumption ι ▶ [H] ≥ 32. The basic rules plus [Eq] are then easily seen to derive: p ▶ [Y == 0] ≤ ϵ where ϵ =



Fig. 3. the upper entropy for q in 4 bits


Table 2
Some Refined Analysis rules (outside loops only)
B(1/232) ≈ 7.8 × 10−9. Thus, using [Const] and [DP], we derive n2 ▶ [Z] ≤ ϵ.

Analysis of arithmetic expressions
We can improve the analysis of leakage via arithmetic expressions by exploit- ing algebraic knowledge of the operations together with information about


the operands acquired through supplementary analyses such as parity analy- sis, constant propagation analysis etc. We consider unary negation, addition and multiplication (—, +, ∗) on the twos-complement representations of k-bit integers with overflow.
Unary negation is just a permutation and therefore easily seen to be an identity with respect to entropy, hence [Neg].
We use ⊙ to mean any binary operator. For random variables X, Y, Z with Z = X ⊙ Y we know (by the Data Processing Theorem) that H(Z) ≤ H(X)+ H(Y ). This justifies [OpMax]. By itself, this rule gives no improvement over [DP]. However, we can achieve improved lower bounds (and hence, via [Eq], improved upper bounds) by exploiting the properties of + and ∗.
As is well known, + makes the set of k-bit numbers in twos-complement, a cyclic additive group with identity 0 and generator 1. The key group property which we exploit is that, whenever x = y + z, each of x, y, z is uniquely determined by the other two. This is sufficient to establish the following result, which in turn justifies [AddMin]:
Proposition 4.2 Let Z = X + Y . Then
H(Z) ≥ max(H(X), H(Y )) — min(H(X), H(Y ))
Multiplication is less straightforward to analyse than addition as the al- gebraic structure of the operation is more complex. However, when one of the operands is a constant, we can sometimes establish good bounds on the entropy of the output. For example, X ∗ 0 is always zero, thus H(X ∗ 0) = 0, hence [ZeroMult]. In fact, this can be seen as a consequence of a more general group property of ∗, namely that ·∗ n generates a subgroup whose order is determined by the order of n.
In particular, the order of n is just that of the whole group (2k) whenever n is odd. From this it follows that, for odd n, ·∗ n (like unary negation) is an identity with respect to entropy. We note that, in this case, it is not necessary to establish the value of n, only that n is constant (once the low inputs are known) and that n is odd. Constancy is already captured by our analysis (n ▶ [E] = 0) but a separate analysis would be required to establish parity. These facts are sufficient to justify [OddMult]. Establishing a more general formula, relating H(X), H(X ∗ n) and the order of n, remains a subject for future work.

Correctness
Lemma 3.4, and hence theorem 3.2, extend easily to accommodate the addi- tional rules in table 2. The key observation is that, in the cases where the


new rules apply, we know not just that the function f of the lemma exists, but actually which function it is. The results above then justify the new rules directly.

Further improvements to the analysis
Our analysis of if-statements is effectively distributed between the definition of the UDG structure and the rule [DP]. As shown by the proof of correct- ness (theorem 3.2), the essential principle is that, at a point n immediately
following an if-statement in which X may be defined, Xˆn can be viewed as
function of B, Y and Z, corresponding to the condition and the values of X
at the end of the true and false branches, respectively. The Data Processing theorem then implies that H(Xˆn) ≤ H(B) + H(Y ) + H(Z). The weakness
of this approach is that it takes no account of the relative probabilities of either branch being chosen. Bounds on the probabilities will in many cases be available (provided, for example, by the analysis of equality tests). As an example let P ' be the program
Y = H; if (Y==0) then X = Y else X = 1 fi; Z = X;
This is semantically equivalent to P in the example of sect. 4.1 but the best we can derive for P ' is the totally uninformative: n ▶ [Z] ≤ 32. The problem is caused by the statement X = Y. In isolation this would leak all the information from Y into X but, in the context of this if-statement, it actually leaks no information.
We can improve on such examples by using what we know about q, where q is the probability that the condition evaluates to true. For equality tests (see sect 4.1) we may have an explicit bound on q as a result of applying the [Eq] rule. More generally, given n ▶ [B] ≤ b for a boolean expression B, we can invert B(q) to find an upper bound on q or 1 — q (though in general, we don’t know which). This can be used to tighten the upper bound derived in some cases. In the example above, application of the [Eq] rule provides a bound on q of 1/232. Since we know that H(X|B = 1) ≤ k for any k-bit variable X, we are able to derive the much improved: n ▶ [Z] ≤ 32/232 + B(1/232) ≈ 1.5 × 10−8.

Conclusions and future work
The work presented in this paper is the first time Information Theory has been used in an automatable analysis measuring interference between variables in a simple imperative language with loops. An obvious and very desirable extension of the work would be to a language with probabilistic operators.


The authors would like to thank Peter O’Hearn and the anonymous re- viewers for helpful comments on this work.

References
Clark, D., S. Hunt and P. Malacaria, Quantitative analysis of the leakage of conﬁdential data, Electronic Notes in Theoretical Computer Science 59 (2002).
Clark, D., S. Hunt and P. Malacaria, Quantiﬁed interference for a while language, Technical report, King’s College London (2003).
Cover, T. M. and J. A. Thomas, “Elements of Information Theory,” Wiley Interscience, 1991.
Denning, D. E. R., “Cryptography and Data Security,” Addison-Wesley, 1982.
Goguen, J. and J. Meseguer, Security policies and security models, in: IEEE Symposium on Security and Privacy (1982), pp. 11–20.
L.Burden, R. and J. D. Faires, “Numerical Analysis,” PWS-KENT, 1989, iSBN 0-534-93219-3.
McIver, A. and C. Morgan, A probabilistic approach to information hiding, in: Programming methodology, Springer-Verlag New York, Inc., 2003 pp. 441–460.
Millen, J., Covert channel capacity, in: Proc. 1987 IEEE Symposium on Research in Security and Privacy (1987).
Pierro, A. D., C. Hankin and H. Wiklicky, Approximate non-interference, in: I. Cervesato, editor, CSFW’02 – 15th IEEE Computer Security Foundation Workshop (2002).
Reynolds, J. C., Syntactic control of interference, in: Conf. Record 5th ACM Symp. on Principles of Programming Languages, ACM, New York, Tucson, Arizona, 1978, pp. 39–46.
Sabelfeld, A. and D. Sands, A per model of secure information flow in sequential programs, in:
Proc. European Symposium on Programming (1999).
Sabelfeld, A. and D. Sands, Probabilistic noninterference for multi-threaded programs, in: Proc. 13th IEEE Computer Security Foundations Workshop (2000).
Shannon, C., A mathematical theory of communication, The Bell System Technical Journal 27 (1948), pp. 379–423 and 623–656, available on-line at http://cm.bell- labs.com/cm/ms/what/shannonday/paper.html.
W. Gray, J., III, Toward a mathematical foundation for information flow security, in: Proc. 1991 IEEE Symposium on Security and Privacy, Oakland, CA, 1991, pp. 21–34.
