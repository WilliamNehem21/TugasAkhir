EURO Journal on Computational Optimization 9 (2021) 100008

		




Pareto front approximation through a multi-objective augmented Lagrangian method
Guido Cocchi a, Matteo Lapucci b,âˆ—, Pierluigi Mansuetob
a Intuendi Srl, Via Vittorio Emanuele II 165, Firenze 50139, Italy
b DINFO, UniversitÃ  di Firenze, Via di Santa Marta 3, Firenze 50139, Italy


a r t i c l e	i n f o	a b s t r a c t

	

2020 MSC:
90C29
90C30

Keywords:
Multi-objective optimization Augmented Lagrangian method Pareto front approximation Pareto stationarity
Global convergence
In this manuscript, we consider smooth multi-objective optimization problems with convex constraints. We pro- pose an extension of a multi-objective augmented Lagrangian Method from recent literature. The new algorithm is specifically designed to handle sets of points and produce good approximations of the whole Pareto front, as opposed to the original one which converges to a single solution. We prove properties of global convergence to Pareto stationarity for the sequences of points generated by our procedure. We then compare the performance of the proposed method with those of the main state-of-the-art algorithms available for the considered class of problems. The results of our experiments show the effectiveness and general superiority w.r.t. competitors of our proposed approach.





Introduction

Multi-objective optimization is a mathematical tool which proved to be particularly suited to model and tackle real-world problems where many contrasting goals have to be reached. Successful applications of multi-objective optimization can be found, for example, in statistics (Carrizosa and Frenk, 1998), design (Fu and Diwekar, 2004; JÃ¼schke et al., 1997; Shan and Wang, 2005), engineering (Campana et al., 2018; Kasperska et al., 2004; Liuzzi et al., 2003; Pellegrini et al., 2014; Sun et al., 2016), environmental analysis (Fliege, 2001; Leschine et al., 1992), management science (Gravel et al., 1992; White, 1998) or space exploration (Palermo et al., 2003; Tavana, 2004).
Popular classes of algorithms to solve multi-objective problems are those of scalarization methods (Drummond et al., 2008; Eichfelder, 2009; Fliege, 2004; Gass and Saaty, 1955; Geoffrion, 1968; Pasco- letti and Serafini, 1984; Steuer and Choo, 1983; Zadeh, 1963) and of heuristic methods based on genetic and evolutionary strategies (Deb et al., 2002; Konak et al., 2006; Laumanns et al., 2002; Mostaghim et al., 2007). However, both these families of approaches come with shortcomings. Indeed, scalarization techniques require a detailed anal- ysis of the problem structure in order to identify the weights defin- ing a suitable scalarized objective. Moreover, an unfortunate choice of the weights may lead to unbounded scalar problems, even un- der strong regularity assumptions (Fliege et al., 2009, sec. 7). On the other hand, convergence properties cannot be stated for heuristic algorithms.
âˆ— Corresponding author.
E-mail address: matteo.lapucci@unifi.it (M. Lapucci).
In order to overcome these limitations, descent methods extend- ing classical scalar optimization techniques have been proposed to address constrained and unconstrained multi-objective problems (see, e.g., Drummond and Iusem, 2004; Fliege et al., 2009; Fliege and Svaiter, 2000). In this work we will bring particular attention to one of such algorithms, the extension of scalar augmented Lagrangian method (Birgin and Martinez, 2014) to the multi-objective case proposed by Cocchi and Lapucci (2020).
This group of algorithms typically produces, similarly to the scalar case, a sequence of points that is asymptotically driven to optimality. However, in the context of multi-objective applications, it is in practice crucial to generate a set of solutions constituting an approximation of the Pareto set, so that the user can choose, a posteriori, the solution providing the most appropriate trade-off among many.
Some recent works actually focused on strategies allowing to handle sequences of sets of points, instead of sequences of points, within multi- objective descent methods. This idea was first explored for derivative- free methods (CustÃ³dio et al., 2011; Liuzzi et al., 2016) and then consid- ered for derivative based methods, both in the constrained (Fliege and Vaz, 2016) and the unconstrained (Cocchi et al., 2020) case.
The contribution of this paper consists of the definition of
an extended version of the augmented Lagrangian algorithm for multi-objective optimization (ALAMO) proposed by Cocchi and La- pucci (2020), which deals with sets of points and effectively produces an
approximation of the Pareto front for constrained vector-valued prob- lems. The key elements that characterize the proposed algorithm are


https://doi.org/10.1016/j.ejco.2021.100008
Received 15 February 2021; Received in revised form 29 June 2021; Accepted 13 August 2021
2192-4406/Â© 2021 The Author(s). Published by Elsevier Ltd on behalf of Association of European Operational Research Societies (EURO). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)



the management of a set of points at each iteration, as proposed by CustÃ³dio et al. (2011), which are all mutually nondominated w.r.t. the current augmented Lagrangian;
the use of an Armijo-type line search, which possibly considers de- scent w.r.t. only a subset of objectives, in order to enrich the approx- imate front;
the use of a common penalty parameter and Lagrange multipliers for all points in the set of solutions;
the use of the multi-objective steepest descent algorithm from Fliege and Svaiter (2000) to make each point in the current set approximately Pareto-stationary w.r.t. the augmented Lagrangian, with increasing accuracy throughout the iterations.
For the proposed algorithm, we prove properties of convergence to Pareto-stationarity for the generated sequence of sets of points, with- out the need to recur to the concept of linked sequence introduced by Liuzzi et al. (2016). In fact, the convergence along linked sequences is implied by our result.
To the best of our knowledge, the SQP procedure by Fliege and
Vaz (2016) is the only other derivative based method for constructing an approximated Pareto front of constrained multi-objective problems
Definition 2.1. A point ğ‘¥Ì„ âˆˆ Î© is Pareto optimal for problem (1) if there does not exist ğ‘¦ âˆˆ Î© such that ğ¹ (ğ‘¦) â‰¨ ğ¹ (ğ‘¥Ì„). If there exists a neighborhood
îˆº (ğ‘¥Ì„) such that the previous property holds in Î©âˆ© îˆº (ğ‘¥Ì„), then ğ‘¥Ì„ is locally
Pareto optimal.
Pareto optimality is a strong property which is hard to attain in prac- tice. A slightly weaker, but certainly more viable to obtain condition is weak Pareto optimality.
Definition 2.2. A point ğ‘¥Ì„ âˆˆ Î© is weakly Pareto optimal for problem (1) if there does not exist ğ‘¦ âˆˆ Î© such that ğ¹ (ğ‘¦) < ğ¹ (ğ‘¥Ì„). If there exists a neigh- borhood îˆº (ğ‘¥Ì„) such that the previous property holds in Î©âˆ© îˆº (ğ‘¥Ì„), then
ğ‘¥Ì„ is locally weakly Pareto optimal.
the problem. The image of the Pareto set through ğ¹ is referred to as the The set of all Pareto optimal solutions constitutes the Pareto set of
Pareto front. We can now turn to the first order necessary conditions for Pareto optimality.
Definition 2.3. A point ğ‘¥Ì„ âˆˆ Î© is Pareto-stationary for problem (1) if, for all feasible directions ğ‘‘ âˆˆ Q(ğ‘¥Ì„) = {ğ‘£ âˆˆ â„ğ‘› âˆ£ âˆƒğ‘¡Ì„ > 0 âˆ¶ ğ‘¥Ì„ + ğ‘¡ğ‘£ âˆˆ Î© âˆ€ ğ‘¡ âˆˆ [0, ğ‘¡Ì„]}, it holds

that can be found in the literature. It is worth remarking that, in contrast with the SQP method, convergence of our algorithm does not depend
max
ğ‘—=1,â€¦,ğ‘š
âˆ‡ğ‘“ğ‘— (ğ‘¥Ì„)ğ‘‡ ğ‘‘ â‰¥ 0.

on a final refinement step that follows a finite exploration phase. As also noted by its authors, SQP can indeed be seen as a single point pro- cedure run in a multi-start fashion. On the contrary, in our procedure
Under differentiability assumptions, Pareto-stationarity is a neces- sary condition for all kinds of Pareto optimality; note that the Pareto- stationarity condition can be compactly written as

convergence and exploration advance alongside, with both asymptoti-
min  max âˆ‡ğ‘“ (ğ‘¥)ğ‘‡ ğ‘‘ = 0.

introduce basic concepts and notation that will be used in the presen- tation of the proposed procedure; in Section 3 we describe in detail our approach; we then provide the convergence analysis in Section 4. In



Now, let us address well known results for unconstrained problems of the form
min ğ¹ (ğ‘¥) = (ğ‘“1 (ğ‘¥), â€¦ , ğ‘“ (ğ‘¥))ğ‘‡ .	(2)

ing the good performance of the proposed procedure compared to a set of different state-of-the-art approaches. We finally give some concluding remarks in Section 6.

Preliminaries

In this paper, we consider optimization problems of the form
Pareto optimality notions match those of the constrained case. Even
such case Q(ğ‘¥) = â„ğ‘› for all ğ‘¥ âˆˆ â„ğ‘›.           Pareto-stationarity can be defined as in Definition 2.3, recalling that in
If a point ğ‘¥Ì„ is not a Pareto-stationary point for problem (2), then
there exists a direction which is a descent direction w.r.t. all objective
functions. Hence (according to Fliege and Svaiter, 2000, sec. 3.1) we can define the steepest common descent direction as the solution of problem

min
ğ‘¥âˆˆâ„ğ‘›
ğ¹ (ğ‘¥) = (ğ‘“1 (ğ‘¥), â€¦ , ğ‘“ (ğ‘¥))ğ‘‡
(1)
min
max
âˆ‡ğ‘“ğ‘— (ğ‘¥Ì„)ğ‘‡ ğ‘‘,	(3)

s.t.	ğ‘”(ğ‘¥) â‰¤ 0,
where ğ¹ âˆ¶ â„ğ‘› â†’ â„ğ‘š is a continuously differentiable function and ğ‘” âˆ¶ â„ğ‘› â†’ â„ğ‘ is a continuously differentiable, component-wise convex func- tion, so that the feasible set Î© = {ğ‘¥ âˆˆ â„ğ‘› âˆ£ ğ‘”(ğ‘¥) â‰¤ 0} is a closed convex set, which we assume to be nonempty. We denote by ğ½ğ¹ and ğ½ğ‘” the Ja- cobian matrices associated respectively with ğ¹ and ğ‘”. In the following, we will also denote by ğ‘’ the vector of all ones. Note that equality con-
straints can be equivalently expressed as couples of opposite inequality constraints, so this formulation is in fact general. Actually, specific man- agement of equality constraints can often be convenient from a compu- tational perspective; the following discussion could easily be extended to address the presence of explicit equality constraints, but we prefer not to take them into account for the sake of simplicity.
In the following we will make use of a partial ordering of points in
â„ğ‘š. Given two vectors ğ‘¢, ğ‘£ âˆˆ â„ğ‘š, we have
ğ‘¢ < ğ‘£  â‡”  ğ‘¢ğ‘– < ğ‘£ğ‘–  âˆ€ ğ‘– = 1, â€¦ , ğ‘š,
ğ‘¢ â‰¤ ğ‘£  â‡”  ğ‘¢ğ‘– â‰¤ ğ‘£ğ‘–  âˆ€ ğ‘– = 1, â€¦ , ğ‘š.
We also say that ğ‘¢ dominates ğ‘£, and denote it by ğ‘¢ â‰¨ ğ‘£, if ğ‘¢ â‰¤ ğ‘£ and ğ‘¢ â‰  ğ‘£. Finally, we say that ğ‘¥ âˆˆ â„ğ‘› dominates ğ‘¦ âˆˆ â„ğ‘› w.r.t. ğ¹ if ğ¹ (ğ‘¥) â‰¨ ğ¹ (ğ‘¦).
the objectives ğ‘“1, â€¦ , ğ‘“ğ‘š ; however, such a solution is very unlikely to Ideally, we would like to find a point simultaneously minimizing all
exist; instead, we rely on the concept of Pareto optimality.
ğ‘‘âˆˆâ„ ğ‘—=1,â€¦,ğ‘š
1
which, if ğ“âˆ norm is employed, can be reformulated as the LP problem
min	ğ›½
ğ›½âˆˆâ„, ğ‘‘âˆˆâ„ğ‘›
s.t.	âˆ’1 â‰¤ ğ‘‘ğ‘– â‰¤ 1  âˆ€ ğ‘– = 1, â€¦ , ğ‘›,
âˆ‡ğ‘“ğ‘— (ğ‘¥Ì„)ğ‘‡ ğ‘‘ â‰¤ ğ›½  âˆ€ ğ‘— = 1, â€¦ , ğ‘š.
scent directions, based on an ğ“2-regularized formulation of problem Note that a slightly different characterization of steepest common de-
(3) and again proposed by Fliege and Svaiter (2000), could be employed. Here we preferred to use formulation (3) because of the simplicity of the LP problem.
a real technical issue; we can define function ğœƒ âˆ¶ â„ğ‘› â†’ â„ such that ğœƒ(ğ‘¥Ì„) The solution of problem (3) may in fact be not unique, but this is not indicates the optimal value of problem (3) at ğ‘¥Ì„; function ğœƒ is continuous. We also denote by ğ‘£(ğ‘¥Ì„) the set of optimal solutions of (3), which is cer- tainly nonempty. As previously stated, if ğ‘¥Ì„ is Pareto-stationary, ğœƒ(ğ‘¥Ì„) = 0, if it is not, ğœƒ(ğ‘¥Ì„) < 0.
Now, based on the concept of steepest common descent, the stan- dard (single-point) multi-objective steepest descent (MOSD) algorithm
was proposed by Fliege and Svaiter (2000). We report the algorithm in Algorithm 1 .
The algorithm makes use of a backtracking Armijo-type line search, which is described in Algorithm 2. The idea of the latter procedure is




Algorithm 1: MultiObjectiveSteepestDescent.
1 Input: ğ¹ âˆ¶ â„ğ‘› â†’ â„ğ‘š, ğ‘¥0 âˆˆ â„ğ‘›
2 ğ‘˜ = 0
3 while ğ‘¥ğ‘˜ is not Pareto stationary do
4	Compute
ğ‘‘ğ‘˜ âˆˆ arg min  max âˆ‡ğ‘“ğ‘— (ğ‘¥ğ‘˜)ğ‘‡ ğ‘‘
Finally, we recall, from Cocchi and Lapucci (2020), the definition of multi-objective augmented Lagrangian for problems with inequality constraints.

penalty parameter ğœ associated with problem (1) is given by Definition 2.5. The multi-objective augmented Lagrangian function of
(âˆ‘ğ‘ (	{	ğœ‡ })2)
	 		

5	ğ›¼ğ‘˜ = ArmijoTypeLineSearch(ğ¹ (â‹…), ğ‘¥ğ‘˜, ğ‘‘ğ‘˜)
6	ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜ + ğ›¼ğ‘˜ ğ‘‘ğ‘˜
7	ğ‘˜ = ğ‘˜ + 1
8 return ğ‘¥ğ‘˜
Algorithm 2: ArmijoTypeLineSearch.
1 Input: ğ¹ âˆ¶ â„ğ‘› â†’ â„ğ‘š, ğ‘¥ âˆˆ â„ğ‘›, ğ‘‘ âˆˆ â„ğ‘›, ğ›¼0 > 0, ğ›¿ âˆˆ (0, 1), ğ›½ âˆˆ (0, 1)
2 ğ›¼ = ğ›¼0
3 while ğ¹ (ğ‘¥ + ğ›¼ğ‘‘) â‰° ğ¹ (ğ‘¥) + ğ›½ğ›¼ğ½ğ¹ (ğ‘¥)ğ‘‘ do
where ğœ‡ â‰¥ 0 âˆˆ â„ğ‘ is the vector of Lagrange multipliers.
The algorithm

In this section, we describe the multiple-points multi-objective
augmented Lagrangian method proposed in this paper, which we call FRONT-ALAMO, to solve problem (1). The algorithmic scheme is  reported in Algorithm  3. Note  that we have  de-
noted by MultiobjectiveSteepestDescent(â‹…, â‹…, ğœ€ğ‘˜ ) the procedure in Algorithm 1 run until the solution is ğœ€ğ‘˜ -Pareto-stationary. We also de-
note by Gğ¼ (ğ‘¥, ğœ‡) the components of Gğœ (ğ‘¥, ğœ‡) indexed by ğ¼ , and by ğœƒğ‘˜ and

4	ğ›¼ = ğ›¿ğ›¼	ğœ	ğ‘˜

5 return ğ›¼
ğ‘£ğ‘˜ maps ğœƒ and ğ‘£ associated with Gğœ (ğ‘¥, ğœ‡ ).





that of reducing the step size as long as a suï¬ƒcient decrease has not been reached for all the objective functions.
We now recall the main theoretical property characterizing the line


Algorithm 3: FRONT-ALAMO.

1 Input: ğœ‡0 âˆˆ â„ğ‘ , ğœ‡Ì„ â‰¥ 0, ğœŒ > 1, ğœ âˆˆ (0, 1), ğœ0 > 0, ğ‘‹0 a list of feasible non-dominated points for the original problem, {ğœ€ğ‘˜ } âŠ‚ â„ a
decreasing sequence
2 for ğ‘˜ = 0, 1, â€¦ do

search (Fliege and Svaiter, 2000, Lemma 4).
Lemma 2.1. If ğ¹ is continuously diï¬€erentiable and ğ½ğ¹ (ğ‘¥)ğ‘‘ < 0 (i.e., ğœƒ(ğ‘¥) <
3	Let Gğœğ‘˜
as:
the current Augmented Lagrangian function defined

0), then there exists some ğœ€ > 0, which may depend on ğ‘¥, ğ‘‘ and ğ›½, such that
ğœ â›âˆ‘ğ‘ (	{	ğœ‡ğ‘˜ })2â

ğ¹ (ğ‘¥ + ğ‘¡ğ‘‘) < ğ¹ (ğ‘¥) + ğ›½ğ‘¡ğ½ğ¹ (ğ‘¥)ğ‘‘

Gğœ (ğ‘¥, ğœ‡ğ‘˜) = ğ¹ (ğ‘¥) +
ğ‘˜
2  ğ‘–=1
max
0, ğ‘”ğ‘– (ğ‘¥) +
ğ‘–
ğœğ‘˜
âŸâŸğ‘’

The above lemma guarantees finite termination of the line search procedure along a common descent direction. The following conver- gence properties (Fliege and Svaiter, 2000, Theorem 1 and Section 9.1)
hold instead for the MOSD procedure.
Lemma 2.2. Every accumulation point of the sequence {ğ‘¥ğ‘˜} produced by
5	set ğ‘‹tmp = ğ‘‹Ì‚ ğ‘˜
6	for ğ‘¥ğ‘ âˆˆ ğ‘‹Ì‚ ğ‘˜ do
7	for ğ¼ âˆˆ 2{1,â€¦,ğ‘š} do
8	if ğœƒğ¼ (ğ‘¥ğ‘ ) < 0 then
9	set ğ‘‘ âˆˆ ğ‘£ğ¼ (ğ‘¥ğ‘ )
ğ‘˜	ğ‘˜


ğ¼	ğ‘˜

10
Algorithm 1 is a Pareto stationary point. If the function ğ¹ has bounded level
sets, in the sense that {ğ‘¥ âˆˆ â„ğ‘› âˆ£ ğ¹ (ğ‘¥) â‰¤ ğ¹ (ğ‘¥0)} is bounded, then the sequence	11
{ğ‘¥ğ‘˜} stays bounded and has at least one accumulation point.
Now, we need to introduce relaxations to the concepts of Pareto-	12
set ğ›¼ = ArmijoTypeLineSearch(Gğœ (â‹…, ğœ‡ ), ğ‘¥ğ‘ , ğ‘‘)
set ğ‘§ =
MultiObjectiveSteepestDescent(Gğœ (â‹…, ğœ‡ğ‘˜), ğ‘¥ğ‘ +
ğ›¼ğ‘‘, ğœ€ğ‘˜ )
if âˆ„ ğ‘¦ âˆˆ ğ‘‹tmp âˆ¶ Gğœ (ğ‘¦, ğœ‡ğ‘˜) â‰¨ Gğœ (ğ‘§, ğœ‡ğ‘˜) then

stationary and common descent directions. First, we recall the concept	13
set ğ‘‹tmp
(
= ğ‘‹
ğ‘˜
tmp
â§µ {ğ‘¥ âˆˆ ğ‘‹
ğ‘˜
tmp |
Gğœğ‘˜
(ğ‘§, ğœ‡ğ‘˜) â‰¨

of ğœ€-Pareto-stationarity introduced by Cocchi and Lapucci (2020).
Definition 2.4. Let ğœ€ â‰¥ 0. A point ğ‘¥Ì„ âˆˆ â„ğ‘› is ğœ€-Pareto-stationary for prob-
lem (2) if
min  max âˆ‡ğ‘“ğ‘— (ğ‘¥Ì„)ğ‘‡ ğ‘‘ â‰¥ âˆ’ğœ€.

14
15



Gğœğ‘˜
set ğ‘‹ğ‘˜+1 = ğ‘‹tmp
for ğ‘– = 1, â€¦ , ğ‘ do

 

ğ‘¥, ğœ‡ğ‘˜)} âˆª {ğ‘§}
{	ğœ‡ğ‘˜ }


Next, inspired by Cocchi et al. (2020), we can introduce the concept
17
set ğœ‡ğ‘˜+1 = max {0, min{ğœ‡ğ‘˜ + ğœ
max {ğ‘” (ğ‘¥)}, ğœ‡}}

{1, â€¦ , ğ‘š}. Given problem
18	if (
ğ‘˜+1
ğ‘˜ ) or (âˆƒ ğ‘¥ğ‘˜+1 âˆˆ ğ‘‹ğ‘˜+1 s.t. ğ‘” (ğ‘¥ğ‘˜+1) < 0 and

||ğ‘‰	|| > ğœ||ğ‘‰ ||	ğ‘–

ğ‘‘âˆˆâ„ğ‘› ğ‘—âˆˆğ¼	ğ‘—
1
ğ‘–
19	set ğœğ‘˜+1 = ğœŒğœğ‘˜

ğ‘‘ â‰¤
we denote by ğœƒğ¼ (ğ‘¥Ì„) its optimal value and by ğ‘£ğ¼ (ğ‘¥Ì„) the set of optimal
solutions, which we refer to as steepest partial descent directions w.r.t.

20	else
21	set ğœğ‘˜+1 = ğœğ‘˜

ğ¼ . Partial descent directions, if used appropriately, can be useful in al-		
gorithms to perform exploration steps to enrich the current Pareto set

2020, Proposition 3) that, if ğ‘¥Ì„ is not a Pareto-stationary point for (2), approximation. It is easy to see (by analogous reasonings as Cocchi et al., then ğœƒğ¼ (ğ‘¥Ì„) < 0 for any ğ¼ âŠ† {1, â€¦ , ğ‘š}.
points {ğ‘‹ğ‘˜}, which approximate the Pareto set of the original problem Through the iterations, the algorithm produces a sequence of sets of
with increasing accuracy. At each iteration, an augmented Lagrangian




eter ğœğ‘˜ and multipliers ğœ‡ğ‘˜. At the beginning of the generic iteration ğ‘˜, all function defined as in Definition 2.5 is considered, with penalty param-
Proposition 4.2. Let {ğ‘‹ğ‘˜+1} be the sequence of sets generated by Algorithm
Then, for each ğ‘˜ and for each ğ‘¥ğ‘˜+1 âˆˆ ğ‘‹ğ‘˜+1, we have:

points that are dominated w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜) are filtered out of the set; we
(a)
ğ‘¥ğ‘˜+1
is not dominated by any other point in ğ‘‹
ğ‘˜+1
w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜), i.e.,

denote such filtered set by ğ‘‹Ì‚ ğ‘˜ . Now, the following iterate is initialized
there does not exist
ğ‘˜+1
ğ‘˜
ğ‘˜	ğ‘˜

ğ‘˜	ğ‘˜
ğ‘¦ âˆˆ ğ‘‹	such that Gğœ (ğ‘¦, ğœ‡ ) â‰¨ Gğœ (ğ‘¥ğ‘˜+1, ğœ‡ );

as ğ‘‹Ì‚ ; then, each point ğ‘¥ğ‘ âˆˆ ğ‘‹Ì‚  is used as a starting point for explo-
ration; in particular, for any possible subset ğ¼ âŠ† {1, â€¦ , ğ‘š} the steepest
(b)
ğ‘¥ğ‘˜+1 is ğœ€ğ‘˜
ğ‘˜	ğ‘˜
-Pareto-stationary w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜).

partial descent direction is explored by an ArmijoTypeLineSearch
restricted to the components of Gğ¼ (ğ‘¥, ğœ‡ğ‘˜), provided that a partial de-
Proof. We prove the two statements one at a time:
ğ‘˜+1

ğœğ‘˜
ğ‘‹	is equal to ğ‘‹tmp at the end of the main loop of each iteration,

scent direction actually exists. After the line search step, the obtained
point is refined by steepest descent on all the objectives up to ğœ€ğ‘˜ -Pareto-
at step 14. ğ‘‹
tmp
is initialized with ğ‘‹Ì‚ ğ‘˜ , which contains mutually non-
ğ‘˜

stationarity. If it is then not dominated w.r.t. G
ğœğ‘˜
(ğ‘¥, ğœ‡ğ‘˜) by any other
dominated points w.r.t. Gğœ (ğ‘¥, ğœ‡ ) by its definition at line 4. Then,
ğ‘‹tmp can be modified only at step 13, where a point is added only

point currently in the new iterate set, it is consequently added to such set, while all points that are dominated by it are removed.
Once all points in ğ‘‹Ì‚ ğ‘˜ are tested, the constructed set will constitute
the next iterate ğ‘‹ğ‘˜+1. The multipliers and the penalty parameter are
updated similarly as in the scalar ALM with multipliers safeguarding
(Kanzow and Steck, 2017), with one key adjustment: to evaluate how much a constraint is violated, the worst violation attained on that con-
straint by any point in ğ‘‹ğ‘˜+1 is considered. In addition, the second clause
where a point which is strictly feasible w.r.t. some constraint ğ‘”ğ‘– is un- of the conditional statement at line 18 allows to avoid unfortunate cases
necessarily pushed to satisfy it with a larger margin.
if it is nondominated, from the condition at line 12, and all points dominated by it are removed.
We have two possible cases: ğ‘¥ğ‘˜+1 âˆˆ ğ‘‹Ì‚ ğ‘˜ or ğ‘¥ğ‘˜+1 âˆ‰ ğ‘‹Ì‚ ğ‘˜ . In the latter
case, ğ‘¥ğ‘˜+1 has necessarily been added to ğ‘‹tmp through instructions 9â€“13; in particular, ğ‘¥ğ‘˜+1 was produced by instruction 11 and is thus
ğœ€ğ‘˜ -Pareto-stationary.
So, let us assume that ğ‘¥ğ‘˜+1 âˆˆ ğ‘‹Ì‚ ğ‘˜ and, by contradiction, that
ğœƒğ‘˜ (ğ‘¥ğ‘˜+1) < âˆ’ğœ€ğ‘˜ . In this case, ğ‘¥ğ‘ = ğ‘¥ğ‘˜+1 would satisfy the conditions at step 8 for ğ¼ = {1, â€¦ , ğ‘š}, as ğœƒğ¼ (ğ‘¥ğ‘˜+1) = ğœƒğ‘˜ (ğ‘¥ğ‘˜+1) < âˆ’ğœ€ğ‘˜ < 0. The line search hence is guaranteed, by Lemma 2.1, to find a step ğ›¼ such that
Gğœ (ğ‘¥ğ‘ + ğ›¼ğ‘‘, ğœ‡ğ‘˜) < Gğœ (ğ‘¥ğ‘ , ğœ‡ğ‘˜), and by the properties of the MOSD pro-

ğ‘˜	ğ‘˜

Remark 3.1. At each iteration ğ‘˜, the set ğ‘‹ğ‘˜+1 is a list of mutually non-
cedure we have Gğœ (ğ‘§, ğœ‡ğ‘˜) â‰¤ Gğœ (ğ‘¥ğ‘ + ğ›¼ğ‘‘, ğœ‡ğ‘˜). Hence, this new point ğ‘§

ğ‘˜	ğ‘˜

dominated points w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜). As we will shortly see, maintaining a
set of mutually nondominated points with respect to the augmented La-
grangian does not provide theoretical asymptotic properties. However,
(strictly) dominates ğ‘¥ğ‘˜+1 w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜). Now, from the instructions
of the algorithm, either ğ‘§ belongs to ğ‘‹ğ‘˜+1 or there exists ğ‘¦ âˆˆ ğ‘‹ğ‘˜+1
such that Gğœ (ğ‘¦, ğœ‡ğ‘˜) â‰¨ Gğœ (ğ‘§, ğœ‡ğ‘˜) < Gğœ (ğ‘¥ğ‘˜+1, ğœ‡ğ‘˜). However, this is ab-

ğ‘˜	ğ‘˜	ğ‘˜

this has a remarkable impact from a computational point of view: it al- lows, especially at late iterations, to remove solutions that are too far from feasibility or that have bad values for all the objectives; in addi- tion, in practice the algorithm will be run for a large enough number of iterations and then stopped; the solutions in the returned set are mu- tually nondominated w.r.t. the final augmented Lagrangian; because of this property, most of the points in the returned set that are â€œsuï¬ƒciently feasibleâ€ are nondominated also w.r.t. the original problem.
surd, since ğ‘¥ğ‘˜+1 âˆˆ ğ‘‹ğ‘˜+1 and from statement (a) ğ‘‹ğ‘˜+1 contains mutu- ally nondominated points. Hence, ğœƒğ‘˜ (ğ‘¥ğ‘˜+1) â‰¥ âˆ’ğœ€ğ‘˜ .
â–¡
Let {ğ‘‹ğ‘˜} be the sequence of (finite) sets produced by the algorithm.
we need to consider sequences of points {ğ‘¥ğ‘˜} such that ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ for all In order to assess the asymptotic convergence properties of Algorithm 3,
ğ‘˜.

In the next section, we will show in detail that Algorithm 3 is well defined and we will carefully address its convergence properties.

Convergence analysis

In this section, we provide a rigorous formal analysis of Algorithm 3 from a theoretical perspective. We first show that the pro- cedure is actually well defined and then we state its asymptotic con- vergence properties. Before proceeding, we need to make a reasonable assumption.
Assumption 4.1. The objective function ğ¹ has bounded level sets in the multi-objective sense, i.e., the set {ğ‘¥ âˆˆ â„ğ‘› âˆ£ ğ¹ (ğ‘¥) â‰¤ ğ‘§} is bounded for any ğ‘§ âˆˆ â„ğ‘š.

Concerning algorithm well-definiteness, we begin by noting that the
We are now able to begin the convergence analysis with a technical Lemma.
Lemma 4.3. Let {ğ‘‹ğ‘˜} be the sequence of sets generated by Algorithm 3, and let {ğ‘¥ğ‘˜} be any sequence of points such that ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ for all ğ‘˜. Let ğ‘¥Ì„ be a limit point of {ğ‘¥ğ‘˜}, i.e., there exists an infinite subset ğ¾ âŠ† {0, 1, â€¦} such
that
lim ğ‘¥ğ‘˜ = ğ‘¥Ì„,
ğ‘˜â†’âˆ
ğ‘˜âˆˆğ¾
and suppose that ğ‘”(ğ‘¥Ì„) â‰¤ 0, i.e., ğ‘¥Ì„ âˆˆ Î©. Then, for all ğ‘– = 1, â€¦ , ğ‘ such that
ğ‘”ğ‘– (ğ‘¥Ì„) < 0 we have
max{0, ğœ‡ğ‘˜ + ğœğ‘˜ ğ‘”ğ‘– (ğ‘¥ğ‘˜+1)} = 0
for all ğ‘˜ âˆˆ ğ¾ sufficiently large.
Proof. Let ğ‘”ğ‘– (ğ‘¥Ì„) < 0 and ğ‘˜1 âˆˆ ğ¾ be such that ğ‘”ğ‘– (ğ‘¥ğ‘˜+1) < ğ‘ < 0 for all ğ‘˜ â‰¥
ğ‘˜1, ğ‘˜ âˆˆ ğ¾. From the instructions of the algorithm we know that ğœ‡ğ‘˜ â‰¥ 0

line search procedure at line 10 of Algorithm 3 stops in a finite time, producing a valid stepsize. Indeed, this result holds straightforwardly from Lemma 2.1 and the fact that the procedure is performed consider-
ing Gğ¼ (ğ‘¥, ğœ‡ğ‘˜), starting at a point ğ‘¥ğ‘ such that ğœƒğ¼ (ğ‘¥ğ‘ ) < 0.
for all ğ‘˜. There are two possible cases:
(a) ğœğ‘˜ â†’ âˆ
The sequence {ğœ‡ğ‘˜} is bounded by definition, hence there exists ğ‘˜2 â‰¥
ğ‘˜	ğ‘˜+1

ğœğ‘˜
ğ‘˜	ğ‘˜1 , ğ‘˜2 âˆˆ ğ¾, such that for all ğ‘˜ âˆˆ ğ¾, ğ‘˜ â‰¥ ğ‘˜2 we have ğœ‡ğ‘– + ğœğ‘˜ ğ‘”ğ‘– (ğ‘¥	) <

The other nontrivial instruction of the FRONT-ALAMO procedure is
0 and thus max{0, ğœ‡ğ‘˜ + ğœğ‘˜ ğ‘”ğ‘– (ğ‘¥ğ‘˜+1)} = 0.

step 11, that we address in the following proposition.
(b) {
ğ‘–
ğœğ‘˜ } is bounded.

Proposition 4.1. The MultiObjectiveSteepestDescent pro- cedure at line 11 of Algorithm 3 stops in a finite number of iterations.
Proof. Finite termination can be proved as in Proposition 4 from Cocchi and Lapucci (2020), recalling that Assumption 4.1 holds.	â–¡

set                             ğ‘‹ğ‘˜. Now, we are able to characterize the points belonging to each iterate
From instruction 18 of the algorithm, there must exist ğ‘˜2 â‰¥ ğ‘˜1 such
that, for all ğ‘˜ â‰¥ ğ‘˜2, condition
âˆ€ ğ‘¥ âˆˆ ğ‘‹ğ‘˜+1  ğœ‡ğ‘˜ + ğœğ‘˜ ğ‘”ğ‘— (ğ‘¥) â‰¤ 0  âˆ€ğ‘— âˆˆ {1, â€¦ , ğ‘} s.t. ğ‘”ğ‘— (ğ‘¥) < 0
holds. Hence, for ğ‘˜ â‰¥ ğ‘˜2 , ğ‘˜ âˆˆ ğ¾, we have ğœ‡ğ‘˜ + ğœğ‘˜ ğ‘”ğ‘– (ğ‘¥ğ‘˜+1) â‰¤ 0. Thus, we have max{0, ğœ‡ğ‘˜ + ğœğ‘˜ ğ‘”ğ‘– (ğ‘¥ğ‘˜+1)} = 0 for ğ‘˜ âˆˆ ğ¾ suï¬ƒciently large.
â–¡



quences   {ğ‘¥ğ‘˜}   produced   by   the   algorithm. Next, we prove feasibility of limit points of all possible points se-
Proposition 4.4. Let {ğ‘‹ğ‘˜} be the sequence of sets generated by Algorithm 3, with ğœ€ğ‘˜ â†’ 0, and let {ğ‘¥ğ‘˜} be any sequence of points such that ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ for all ğ‘˜. Let ğ‘¥Ì„ be a limit point of {ğ‘¥ğ‘˜}. Then, ğ‘¥Ì„ is feasible for problem (1), i.e., ğ‘”(ğ‘¥Ì„) â‰¤ 0.
Proof. Let ğ¾ âŠ† {0, 1, â€¦} be an infinite subset such that
lim ğ‘¥ğ‘˜+1 = ğ‘¥Ì„.
ğ‘˜â†’âˆ
ğ‘˜âˆˆğ¾
If the sequence {ğœğ‘˜ } is bounded, from the instructions of the algorithm
Remark 4.1. Pareto-stationarity, which we are able to prove for limit points of FRONT-ALAMO, is the same property that holds for limit points of the sequence produced by the single point ALAMO and anal-
ogous, in the scalar context, to stationarity attained by limit points of scalar ALM. Therefore, it is reasonable to assume that stronger properties are unlikely to be obtained by an ALM-like algorithm.
Remark 4.2. In the literature of Pareto front constructing descent meth- ods (Cocchi et al., 2020; Liuzzi et al., 2016), convergence analysis is
based on the concept of linked sequence. A sequence {ğ‘¥ğ‘˜} is a linked
sequence if, for all ğ‘˜, ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ and ğ‘¥ğ‘˜ is generated at iteration ğ‘˜ âˆ’ 1
starting the search procedure from ğ‘¥ğ‘˜âˆ’1. It is easy to see that linked se-
quences are a particular instance of the sequences of points considered

there must exist ğ‘˜
such that, for all ğ‘˜ > ğ‘˜ , we have
ğ‘‰ ğ‘˜+1
â‰¤ ğœ ğ‘‰ ğ‘˜ .

1
Since	1
1	â€–	â€–	â€–	â€–
in Propositions 4.4 and 4.5, hence the convergence result obtained for




lim
ğ‘‰ ğ‘˜
= 0,

ğ‘˜â†’âˆ
i.e., for all ğ‘– âˆˆ {1, â€¦ , ğ‘},
lim ğ‘‰ ğ‘˜+1 = lim min {



ğœ‡ğ‘˜
min {âˆ’ğ‘”ğ‘– (ğ‘¥)}, ğ‘–

= 0.
a limit point ğ‘¥Ì„. As commonly done in the literature of augmented La- Remark 4.3. In our theoretical analysis we assumed the existence of
grangian methods (Birgin and Martinez, 2014; Cocchi and Lapucci, 2020), we do not directly address properties of existence of limit points,

ğ‘˜â†’âˆ ğ‘–
ğ‘˜â†’âˆ
ğ‘¥âˆˆğ‘‹ğ‘˜+1
ğœğ‘˜
leaving it to boundedness arguments on the sequences, level sets, lower- level feasible sets or restart strategies.

Since by definition ğœ‡ğ‘˜ â‰¥ 0 for all ğ‘– and ğ‘˜, it has to be
lim min  âˆ’ğ‘” (ğ‘¥) â‰¥ 0.
ğ‘˜â†’âˆğ‘¥âˆˆğ‘‹ğ‘˜+1
But minğ‘¥âˆˆğ‘‹ğ‘˜+1 {âˆ’ğ‘”ğ‘–(ğ‘¥)} â‰¤ âˆ’ğ‘”ğ‘–(ğ‘¥ğ‘˜+1). Hence
Remark 4.4. The SQP algorithm from Fliege and Vaz (2016) which is, to the best of our knowledge, the only other derivative-based method in the literature to generate an approximation of the Pareto front, has
similar convergence properties as Algorithm 3, in the sense that limit
points of sequences of solutions are Pareto-stationary. However, the set-

ğ‘”ğ‘– (ğ‘¥) = lim ğ‘”ğ‘– (ğ‘¥ğ‘˜+1) â‰¤ lim  max {âˆ’ğ‘”ğ‘– (ğ‘¥)} â‰¤ 0.
ting is basically different, as the exploration phase of the SQP method



Now, assume ğœğ‘˜ â†’ âˆ. From Proposition 4.2, we know that each point
ğ‘¥ âˆˆ ğ‘‹ğ‘˜+1 is ğœ€ğ‘˜ -Pareto-stationary w.r.t. Gğœ (ğ‘¥, ğœ‡ğ‘˜). Hence
â§âª(	âˆ‘ğ‘	{	ğœ‡ğ‘˜ }	)ğ‘‡ â«âª
dently driven to Pareto-stationarity by an iterative method. Conver- gence hence follows from a single-point mechanism. On the other hand, in Algorithm 3 exploration and convergence are performed somewhat
in parallel, in an effectively multiple-points fashion.

max
ğ‘—=1,â€¦,ğ‘š
â©
âˆ‡ğ‘“ğ‘— (ğ‘¥ğ‘˜+1 ) + ğœğ‘˜
ğ‘–=1
max
0, ğ‘”ğ‘– (ğ‘¥ğ‘˜+1 ) +
ğ‘–
ğœğ‘˜
âˆ‡ğ‘”ğ‘– (ğ‘¥ğ‘˜+1 )
ğ‘‘  â‰¥ âˆ’ğœ€ğ‘˜
âªâ­
âˆ€ ğ‘‘ âˆˆ â„ğ‘› âˆ¶ ğ‘‘
â‰¤ 1.
Computational experiments

Dividing both sides of the inequality by ğœğ‘˜ and taking the limits for
ğ‘˜ â†’ âˆ, ğ‘˜ âˆˆ ğ¾, recalling the continuity of ğ½ğ¹ and ğ½ğ‘” , the boundedness of ğ‘‘ and {ğœ‡ğ‘˜} and that ğœğ‘˜ â†’ âˆ, we get

In this Section, we show the results of thorough computational ex- periments, focusing on the comparisons between FRONT-ALAMO and
some state-of-the-art methods in the multi-objective constrained opti-

max
â§âª(âˆ‘ğ‘
max{0, ğ‘” (ğ‘¥Ì„)}âˆ‡ğ‘” (ğ‘¥Ì„)
)ğ‘‡
ğ‘‘â«âª
â‰¥ 0  âˆ€ ğ‘‘ âˆˆ â„ğ‘› âˆ¶ ğ‘‘
â‰¤ 1,
mization context. All the tests were run on a computer with the follow-

ğ‘—=1,â€¦,ğ‘š â¨âª
ğ‘–=1
ğ‘–	ğ‘–
âªâ¬â­
ing characteristics: Intel Xeon Processor E5-2430 v2 6 cores 2.50 GHz, 16 GB RAM. The code for all the algorithms considered in the experi- ments was written in Python3.

of       ğ‘—,       is       equal       to which, since the arguments of the outer max operator are independent
1 âˆ‡  max{0  ( )} 2 ğ‘‡	0  âˆ€  âˆˆ  ğ‘› âˆ¶	1
2
where the max operator is intended component-wise. Thus, ğ‘¥Ì„ is a critical
point for problem

Experiment Settings

Before commenting the results, we list the state-of-the-art methods used in the comparisons with FRONT-ALAMO. In addition, we describe the tested problems and the metrics used in the comparisons.

min 1
ğ‘¥âˆˆâ„ 2
max{0, ğ‘”(ğ‘¥)} 2 .
Metrics
In order to evaluate the performance of the algorithms, we employed

Since Î© â‰  0 and the above problem is convex, ğ‘¥Ì„ is a global minimum point with max{0, ğ‘”(ğ‘¥Ì„)} = 0, i.e., ğ‘”(ğ‘¥Ì„) â‰¤ 0.	â–¡
Finally, we show that limit points are Pareto-stationary for the orig- inal problem.
Proposition 4.5. Let {ğ‘‹ğ‘˜} be the sequence of sets generated by Algorithm 3, with ğœ€ğ‘˜ â†’ 0, and let {ğ‘¥ğ‘˜} be any sequence of points such that ğ‘¥ğ‘˜ âˆˆ ğ‘‹ğ‘˜ for all ğ‘˜. Let ğ‘¥Ì„ be a limit point of {ğ‘¥ğ‘˜}. Then, ğ‘¥Ì„ is Pareto-stationary for problem
(1).
Proof. Recalling that, from Proposition 4.2, ğ‘¥ğ‘˜+1 is ğœ€ğ‘˜ -Pareto-stationary for Gğœ (ğ‘¥, ğœ‡ğ‘˜), the result follows as in Proposition 6 from Cocchi and
Lapucci (2020), where Lemma 4.3 can be used in place of Lemma 9 from the referenced paper.	â–¡
the three metrics defined by CustÃ³dio et al. (2011), which are very pop- ular and used by the multi-objective optimization community: purity,
Î“â€“spread and Î”â€“spread. We recall that the purity metric measures the
quality of the generated front, that is, how good the non-dominated
points computed by a solver are with respect to those obtained by the other ones. Clearly, a higher value is associated with a better perfor- mance. In order to calculate this metric, we need a reference front to which compare the generated front of an algorithm. In our experiments, the reference front was the one obtained by combining the fronts of all the considered algorithms and by discarding the dominated points. The spread metrics are equally essential because they measure the uniformity
of the generated front in the objectives space. In particular, the Î“â€“spread
is defined as the maximum ğ“âˆ distance in the objectives space between adjacent points of the Pareto front, and the Î”â€“spread is quite similar



to the standard deviation of the ğ“âˆ distances between adjacent Pareto front points. In these metrics, good performance is associated with a low
value.
In addition to the three previous metrics, we evaluated the number of non-dominated points obtained by a method with respect to the ref- erence front (NDâ€“points). We supposed that this metric was as important as the purity one. Indeed, NDâ€“points metric allowed us to see how many non-dominated points an algorithm was capable to obtain with respect to the whole reference front.
Lastly, we employed the popular performance profiles introduced by Dolan and MorÃ© (2002), that are an useful tool to better appreciate the relative performance and robustness of the algorithms. The performance profile for a solver is the (cumulative) distribution function for the ratio of the value of the performance measure obtained by the solver to the best one of all of the solvers. In particular, it is the probability for solver
ğ‘  that one of its performance measure values achieved in a problem is
within a factor ğœ âˆˆ â„ of the best possible value obtained by all of the
solvers in that problem. For a more detailed explanation about perfor-
mance profiles, we refer to Dolan and MorÃ© (2002). Note that perfor- mance profiles w.r.t. purity and NDâ€“points were generated considering the inverse of the obtained values, since the metrics have increasing values for better solutions.

Algorithms and hyper-parameters
The choices about the FRONT-ALAMO hyper-parameters values
were made based on some preliminary results on a subset of the tested problems, which we do not report here for the sake of brevity. The val- ues are the following:
ğœ0 = 1;
if the problem only has bound constraints ğœŒ = 10, otherwise ğœŒ = 2;
ğœ = 0.9;
ğœ‡ = 104;
ğœ‡0 = 0 âˆˆ â„ğ‘;
in the ArmijoTypeLineSearch ğ›½ = 10âˆ’4 and ğ›¿ = 0.5.
The first algorithm we chose to use in the comparison with FRONT-ALAMO is MOSQP (Fliege and Vaz, 2016), which is a gradient- based method for constrained and unconstrained nonlinear multi- objective optimization problems that implements an SQPâ€“type ap- proach. Since it is the only gradient-based algorithm from the literature
designed to produce Pareto front approximations, we consider it our most important competitor. The chosen hyper-parameters for MOSQP
approximations, we used ğ»ğ‘– = ğ¼ğ‘š (ğ¼ğ‘š being the identity matrix) in the were the best ones according to Fliege and Vaz (2016). For the quadratic second stage and ğ»ğ‘– = âˆ‡2ğ‘“ğ‘–(ğ‘¥ğ‘˜) + ğ¸ğ‘– (ğ¸ğ‘– being obtained by a modified
Cholesky algorithm) in the third stage, as Fliege and Vaz (2016) state that it is the most robust and eï¬ƒcient way to use MOSQP. For a more de- tailed explanation about the various MOSQP stages and versions, we re- fer to Fliege and Vaz (2016). Lastly, we used the Ipopt software pack-
age (WÃ¤chter and Biegler, 2006) (https://github.com/coin-or/Ipopt) in order to solve the SQP problems.
DMS (CustÃ³dio et al., 2011) is the second algorithm used in
the comparisons. It is a multi-objective derivative-free methodology, which is inspired by the search/poll paradigm of direct-search meth- ods of directional type and uses the concept of Pareto dominance to maintain a list of non-dominated points, from which the new iter-
ates or poll centers are chosen. The hyper-parameters for DMS were
set according to CustÃ³dio et al. (2011) and to the authors code (http://www.mat.uc.pt/dms).
The third and last algorithm is NSGA-II (Deb et al., 2002), which is
a non-dominated sorting-based multi-objective evolutionary algorithm. In particular, NSGA-II is a genetic algorithm that is mainly composed
by a fast non-dominated sorting approach and a selection operator that creates a mating pool by combining the parent and offspring populations
and selecting the best ğ‘ solutions. In contrast to the other algorithms,
NSGA-II considers a fixed number of solutions in the pool, which was
Table 1
Problems used in the computational experiments. #L.C. indicates the num-
ber of linear constraints (in this column the boundary constraints are not con- sidered). #N.L.C. indicates the number of non linear constraints. B.C. indicates the type(s) of the boundary conditions.



set to 100 in our experiments. The hyper-parameters of the algorithm were the ones chosen by Deb et al. (2002).
For each algorithm and problem, we decided to execute the test for up to 2 min. A termination criterion based on a time limit is the fairest way to evaluate the behavior of such diverse algorithms on the tested problems. Clearly, specific stopping criteria indicating that a certain al- gorithm cannot improve the solutions anymore were also taken into ac-
count. Since NSGA-II is the only non-deterministic algorithm used in
the computational experiments, we decided to run it with 10 different seeds for the pseudo-random number generator. Every execution had the same time limit used for the other algorithms (2 min). After the ex- ecution of the 10 runs, we compared the fronts based on the purity met- ric and we chose the best one among them. In this case, the reference front for the comparison was obtained by combining the fronts of the
10 executions. We considered the resulting best front as NSGA-II out- put. Executing 10 runs lets NSGA-II reduce its sensibility with respect
to the seed used for its random operations. Note however that, since we consider a best case scenario for NSGA-II, the overall comparison should be considered at least partially biased in favor of this algorithm. The other methods (FRONT-ALAMO, DMS, MOSQP) are deterministic. Therefore, they were executed once.

Problems
The tested problems are described in Table 1. In this benchmark, we considered problems whose objective functions are at least continuously differentiable almost everywhere. Since some problems present singu- larities, we counted them as Pareto-stationary points. All the constraints are defined by continuously differentiable convex functions.
We included in our benchmark the slightly modified versions of the BNH problems and the LAP problems from Cocchi and Lapucci (2020). We also included a modification of the OSY problem (Osyczka and Kundu, 1995). The modified form of this problem can be found in Appendix A.
Furthermore, we included into the test set problems characterized only by boundary constraints: the CEC problems (Zhang et al., 2008), the ZDT problems (Zitzler et al., 2000) and the MOP problems (Huband et al., 2006). It is worth remarking that the CEC and ZDT problems have particularly diï¬ƒcult objective functions, so they are interesting to study the effectiveness of the algorithms when solving hard problems.
For each problem with general constraints, we started the algorithms from one feasible point (Table 2). In this way, we intended to study the exploration capabilities of the algorithms. Indeed, algorithms with great exploration abilities should create a spread and solid Pareto front on these problems. For the bound constrained problems (MOP1 is the only

ering the M-BNH problems (For interpretation of the references to color in text, the reader is referred to the web version of the article).














Table 2
Initial points for the tested problems.

PROBLEM(S)	INITIAL POINT(S)
M-BNH1, LAP1,	0 âˆˆ â„ğ‘›
LAP2, MOP1
M-BNH2	[8, âˆ’3]
M-OSY	[2, 0, 1, 0, 1, 8]
CEC, ZDT,	Points from the MOP2, MOP3	hyper-diagonal





exception since it is too small in terms of number of dimensions), the ini- tial points were uniformly selected from the hyper-diagonal defined by the lower and upper bounds, as done by CustÃ³dio et al. (2011). In these cases, the number of initial points is equal to the number of dimensions of the considered problem.
M-BNH, LAP1 and M-OSY problems

We begin by studying the performance of the considered algorithms on the M-BNH1, M-BNH2, LAP1 and M-OSY problems.
The results on the M-BNH problems (Fig. 1) show the great perfor- mance of FRONT-ALAMO with respect to the competitors: indeed, our method obtained the best purity value in the M-BNH1 problem and a
purity value very close to the best one in the M-BNH2. In this latter problem, the differences with respect to our gradient-based competitor
and DMS are even clearer, as FRONT-ALAMO obtained a purity value
very close to 1. Here, the MOSQP method did not manage to obtain a single non-dominated point w.r.t. the competitors. Considering the Î”â€“
spread, FRONT-ALAMO was the second best method in both problems. As for the Î“â€“spread, FRONT-ALAMO appears to have a decent behavior,
being the second best algorithm in the M-BNH1 problem and outper- forming all the competitors in the M-BNH2 problem. This behavior can

ering the LAP1 and M-OSY problems (For interpretation of the ref- erences to color in text, the reader is referred to the web version of the article).















also be observed on the LAP1 and M-OSY problems (Fig. 2). The worst algorithm turned out to be MOSQP, which seems to lack of search ca- pabilities in the objectives space on the M-BNH problems. This fact can also be noted for the DMS and NSGA-II algorithms on the M-BNH2
problem, which turned out to be diï¬ƒcult to solve. DMS outperformed the NSGA-II method on the M-BNH1 problem in terms of NDâ€“points
ing the spread metrics, DMS performed better in terms of Î“â€“spread, while and purity, while on the M-BNH2 one it is the opposite. Lastly, consider- NSGA-II outperformed DMS on the Î”â€“spread.
Considering the LAP1 problem (Fig. 2), FRONT-ALAMO performed very well and its scores are close to those of DMS, which was the over- all best algorithm on this problem. This fact can be also seen in the
front plots: the Pareto fronts obtained by the two algorithms are very similar, while the other two methods (NSGA-II and MOSQP) did not
have the same performance. Among these two latter algorithms, the
MOSQP method managed to outperform FRONT-ALAMO on one met-
ric, that is the purity, while the NSGA-II algorithm performed bet- ter on the Î”â€“spread. In the M-OSY problem (Fig. 2) NSGA-II was the
most effective obtaining an uniform and spread Pareto front. In this case, FRONT-ALAMO achieved some interesting results. First of all, it outper- formed the DMS algorithm: this achievement is remarkable since DMS is
gradient-free and can escape non optimal Pareto-stationary points, while our method is gradient-based. In addition, our algorithm obtained more
non-dominated points and a better Î“â€“spread than its gradient-based com-
petitor (MOSQP).
The last FRONT-ALAMO peculiarity that we can see from these plots is the number of non-dominated points it achieved. Only the DMS algo- rithm managed to obtain a much greater value of NDâ€“points in some


Fig. 3. Performance profiles for the four algorithms on the LAP2 problems. Each sub-figure represents the performance profiles con- sidering as a performance measure one of the metrics explained in Section 5.1.1 (For interpretation of the references to color in text, the reader is referred to the web version of the article).










Fig. 4. Pareto front approximation for the four algorithms consider- ing LAP2 problems at different dimensionalities (For interpretation of the references to color in text, the reader is referred to the web version of the article).









problems. However, in these problems FRONT-ALAMO was equally competitive in terms of this metric and the purity one.

LAP2 problems

The LAP2 problems represent another useful class of problems: in- deed, they allow to discuss about the sensibility of the algorithms with
regard to ğ‘›, that is, how well they scale. Indeed, many algorithms have
great performance considering small values of ğ‘›. However, when a prob-
lem size grows, they lose their abilities to retrieve good Pareto front ap-
proximations. Before seeing some plots and metric values, we show the performance profiles considering all the LAP2 problems (Fig. 3).
The performance profiles highlight that FRONT-ALAMO strongly outperformed the other competitors with respect to NDâ€“points, pu-
rity and Î“â€“spread. The second most robust algorithm is the DMS
one. As for the Î”â€“spread, the methods performed similarly: here, it
is very diï¬ƒcult to indicate the best algorithm. This fact can be a
On the contrary, in terms of the Î“â€“spread there is a clear winner: proof that no algorithm suffered from a non-uniformity in their fronts.
FRONT-ALAMO.
The motivation of these different results on the two spread metrics can be explained through the Fig. 4, where we show the Pareto fronts in four different LAP2 problems. Here, we show the fronts all together in order to provide a more direct impression of the results. Indeed, in


Table 3
Metrics values obtained by the four algorithms in the LAP2 problems with ğ‘› = 2, 10, 50, 100. The values marked
in bold are the best values (each of which is related to a specific score) obtained in a specific problem.



Fig. 5. Performance profiles for the four algorithms on the CEC, ZDT and MOP problems. Each sub-figure represents the perfor- mance profiles considering as a performance measure one of the metrics explained in Section 5.1.1 (For interpretation of the refer- ences to color in text, the reader is referred to the web version of the article).









the LAP2 problems, FRONT-ALAMO results show the superiority of our
form Pareto front. The competitors obtained large Î“â€“spread values with method at exploring the objectives space and creating a spread and uni-
respect to those of FRONT-ALAMO, since they struggle to explore the
extreme regions of the front.
When ğ‘› = 2, all the methods managed to obtain the same Pareto front. However, increasing ğ‘›, the differences between them become
more and more clear. For instance, NSGA-II performance got worse with ğ‘› â‰¥ 10. It seems to be unable to spread the search in the objectives
space and, also, to create a good, although small, Pareto front. The other gradient-free method (DMS) performed better but still it hardly reached the extremes of the objectives space. MOSQP seems not to have this last
negative feature but it generally retrieved very few points and, in ad-
dition, most of them are dominated. However, the MOSQP performance was good with ğ‘› â‰¤ 10.
The above comments on the algorithm behaviors on the LAP2 prob- lems are also supported by the numbers in Table 3. Observe that

FRONT-ALAMO, whose performance was quite good on problems with small dimension, outperformed the competitors as the value of ğ‘› in-
creased. The superiority of our method when the dimension of a prob- lem is high is very remarkable, especially considering NDâ€“points (we obtained the best value, by far, for this metric in all the four problems),
purity and Î“â€“spread. As we just highlighted commenting the performance
profiles, all the algorithms performed well regarding the Î”â€“spread met-
ric.

CEC, ZDT and MOP problems

In this last section of computational experiments, we comment the results on the problems characterized only by boundary constraints. Some of these are very diï¬ƒcult to solve and, in detail, the hardest ones are the CEC and ZDT problems. The CEC problems have non- continuously differentiable objective functions. The same feature is present in the ZDT problems, where the objective functions are also



composite. For the sake of brevity, we preferred to show the perfor- mance profiles related to all these problems. The performance profiles are shown in Fig. 5.
Regarding the NDâ€“points metric, FRONT-ALAMO managed to out-
perform the competitors once again. Indeed, we can conclude that this is one of the most important peculiarities of our method: its capabilities
Appendix A. Modified form of the OSY problem

In this Appendix, we introduce the modified version of the OSY prob- lem (Osyczka and Kundu, 1995) used in the computational experiments. The modification is carried out in order to make the feasible set and the objective functions convex.

allow it to expand the search towards a great portion of the objectives
space and to retrieve many Pareto points. Considering this metric, the other two best competitors were NSGA-II and DMS: because of their
min
ğ‘¥âˆˆâ„6
ğ‘“1 (ğ‘¥) = 25(ğ‘¥1 âˆ’ 2)2 + (ğ‘¥2 âˆ’ 2)2 + (ğ‘¥3 âˆ’ 1)2 + (ğ‘¥4 âˆ’ 4)2 + (ğ‘¥5 âˆ’ 1)2
ğ‘“2 (ğ‘¥) = âˆ‘6	2

gradient-free nature, allowing them to potentially escape from non opti- mal Pareto-stationary points, they managed to obtain good results in the
most complex functions. On the contrary, the performance difference be- tween FRONT-ALAMO and the other gradient-based method (MOSQP) is very sharp.
The performance profiles on the purity metric highlight the effec- tiveness of our method: it was not obvious, a priori, to obtain such great results with such complex functions, especially when some of our com- petitors are derivative-free.
with respect to the other competitors. In particular, in the Î“â€“spread Lastly, considering the spread metrics, our results are competitive
performance profiles FRONT-ALAMO was the third best algorithm, while the gradient-free methods (NSGA-II and DMS) managed to have
slightly better performance. Regarding the Î”â€“spread, NSGA-II turned
out to be the most robust algorithm. However, the performance profiles
on this metric are another proof of the effectiveness of the four algo- rithms to retrieve an uniform Pareto front.

Conclusions

In this paper, we considered smooth multi-objective optimization problems subject to convex constraints. We focused on the task of gener- ating good Pareto front approximations for this class of problems. After a brief review of the existing literature, we proposed an Augmented La- grangian Method specifically designed for this task.
The method represents an extension of the ALAMO procedure from
Cocchi and Lapucci (2020), which is designed to produce a single Pareto- stationary solution. The proposed algorithm handles, at each iteration, a list of points that are mutually non-dominated and Pareto-stationary with respect to the current multi-objective augmented Lagrangian. Line searches along steepest common and partial descent directions are em- ployed to carry out an exploration of the objectives space. The penalty parameter and the Lagrange multipliers are updated taking into account constraints violations committed by all the points in the current list.
For this algorithm, we proved global convergence to Pareto- stationarity of the sequences of points in the iterates lists. This type of convergence is more general than that based on linked sequences.
With respect to the only other derivative-based method for this kind of problems, the SQP from (Fliege and Vaz, 2016), we obtain similar asymptotic properties for the limit points, but our method does not stop
the exploration phase after a finite number of iterations.
Moreover, thorough computational experiments show that our method outperforms the SQP algorithm in terms of popular metrics for multi-objective optimization. We also compared the proposed procedure with the state-of-the-art derivative-free (DMS) and genetic (NSGA-II) approaches. Our procedure proved to obtain better results even w.r.t.
the two mentioned ones.

Declaration of Competing Interest

The authors declare that they have no conflict of interest.

Acknowledgment

The authors would like to thank professor Marco Sciandrone for the precious discussions.
s.t.	ğ‘¥1 + ğ‘¥2 âˆ’ 2 â‰¥ 0, 6 âˆ’ ğ‘¥1 âˆ’ ğ‘¥2 â‰¥ 0,
2 âˆ’ ğ‘¥2 + ğ‘¥1 â‰¥ 0,
2 âˆ’ ğ‘¥1 + 3ğ‘¥2 â‰¥ 0,
4 âˆ’ (ğ‘¥3 âˆ’ 3)2 âˆ’ ğ‘¥4 â‰¥ 0,
âˆ’(ğ‘¥5 âˆ’ 3)2 + ğ‘¥6 âˆ’ 4 â‰¥ 0,
0 â‰¤ ğ‘¥1 , ğ‘¥2 , ğ‘¥6 â‰¤ 10,
1 â‰¤ ğ‘¥3, ğ‘¥5 â‰¤ 5,
0 â‰¤ ğ‘¥4 â‰¤ 6.
References

Birgin, E.G., Martinez, J.M., 2014. Practical Augmented Lagrangian Methods for Con- strained Optimization, 10. SIAM.
Campana, E.F., Diez, M., Liuzzi, G., Lucidi, S., Pellegrini, R., Piccialli, V., Rinaldi, F., Serani, A., 2018. A multi-objective DIRECT algorithm for ship hull optimization. Com- put. Optim. Appl. 71 (1), 53â€“72.
Carrizosa, E., Frenk, J.B.G., 1998. Dominating sets for convex functions with some appli- cations. J. Optim. Theory Appl. 96 (2), 281â€“295.
Cocchi, G., Lapucci, M., 2020. An augmented Lagrangian algorithm for multi-objective optimization. Comput. Optim. Appl. 77 (1), 29â€“56.
Cocchi, G., Liuzzi, G., Lucidi, S., Sciandrone, M., 2020. On the convergence of steepest descent methods for multiobjective optimization. Comput. Optim. Appl. 1â€“27.
CustÃ³dio, A.L., Madeira, J.F.A., Vaz, A.I.F., Vicente, L.N., 2011. Direct multisearch for multiobjective optimization. SIAM J. Optim. 21 (3), 1109â€“1140.
Deb, K., Pratap, A., Agarwal, S., Meyarivan, T., 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans. Evol. Comput. 6 (2), 182â€“197.
Dolan, E.D., MorÃ©, J.J., 2002. Benchmarking optimization software with performance pro- files. Math. Program. 91 (2), 201â€“213.
Drummond, L.M.G., Iusem, A.N., 2004. A projected gradient method for vector optimiza- tion problems. Comput. Optim. Appl. 28 (1), 5â€“29.
Drummond, L.M.G., Maculan, N., Svaiter, B.F., 2008. On the choice of parameters for the weighting method in vector optimization. Math. Program. 111 (1-2), 201â€“216.
Eichfelder, G., 2009. An adaptive scalarization method in multiobjective optimization.
SIAM J. Optim. 19 (4), 1694â€“1718.
Fliege, J., 2001. OLAFâ€”A general modeling system to evaluate and optimize the location of an air polluting facility. OR-Spektrum 23 (1), 117â€“136.
Fliege, J., 2004. Gap-free computation of Pareto-points by quadratic scalarizations. Math.
Methods Oper. Res. 59 (1), 69â€“89.
Fliege, J., Drummond, L.M.G., Svaiter, B.F., 2009. Newtonâ€™s method for multiobjective optimization. SIAM J. Optim. 20 (2), 602â€“626.
Fliege, J., Svaiter, B.F., 2000. Steepest descent methods for multicriteria optimization.
Math. Methods Oper. Res. 51 (3), 479â€“494.
Fliege, J., Vaz, A.I.F., 2016. A method for constrained multiobjective optimization based on SQP techniques. SIAM J. Optim. 26 (4), 2091â€“2119.
Fu, Y., Diwekar, U.M., 2004. An eï¬ƒcient sampling approach to multiobjective optimiza- tion. Ann. Oper. Res. 132 (1-4), 109â€“134.
Gass, S., Saaty, T., 1955. The computational algorithm for the parametric objective func- tion. Naval Res. Logist. Q. 2 (1-2), 39â€“45.
Geoffrion, A.M., 1968. Proper eï¬ƒciency and the theory of vector maximization. J. Math.
Anal. Appl. 22 (3), 618â€“630.
Gravel, M., Martel, J.M., Nadeau, R., Price, W., Tremblay, R., 1992. A multicriterion view of optimal resource allocation in job-shop production. Eur. J. Oper. Res. 61 (1-2), 230â€“244.
Huband, S., Hingston, P., Barone, L., While, L., 2006. A review of multiobjective test problems and a scalable test problem toolkit. IEEE Trans. Evol. Comput. 10 (5), 477â€“
506. doi:10.1109/TEVC.2005.861417.
JÃ¼schke, A., Jahn, J., Kirsch, A., 1997. A bicriterial optimization problem of antenna de- sign. Comput. Optim. Appl. 7 (3), 261â€“276.
Kanzow, C., Steck, D., 2017. An example comparing the standard and safeguarded aug- mented Lagrangian methods. Oper. Res. Lett. 45 (6), 598â€“603.
Kasperska, R., Ostwald, M., Rodak, M., 2004. Bi-criteria optimization of open cross sec- tion of the thin-walled beams with flat flanges. In: PAMM: Proceedings in Applied Mathematics and Mechanics, 4. Wiley Online Library, pp. 614â€“615.
Konak, A., Coit, D.W., Smith, A.E., 2006. Multi-objective optimization using genetic algo- rithms: a tutorial. Reliab. Eng. Syst. Saf. 91 (9), 992â€“1007.
Laumanns, M., Thiele, L., Deb, K., Zitzler, E., 2002. Combining convergence and diversity in evolutionary multiobjective optimization. Evol. Comput. 10 (3), 263â€“282.
Leschine, T.M., Wallenius, H., Verdini, W.A., 1992. Interactive multiobjective analysis and assimilative capacity-based ocean disposal decisions. Eur. J. Oper. Res. 56 (2), 278â€“289.



Liuzzi, G., Lucidi, S., Parasiliti, F., Villani, M., 2003. Multiobjective optimization tech- niques for the design of induction motors. IEEE Trans. Magn. 39 (3), 1261â€“ 1264.
Liuzzi, G., Lucidi, S., Rinaldi, F., 2016. A derivative-free approach to constrained multi- objective nonsmooth optimization. SIAM J. Optim. 26 (4), 2744â€“2774.
Mostaghim, S., Branke, J., Schmeck, H., 2007. Multi-objective particle swarm optimization on computer grids. In: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation. ACM, pp. 869â€“875.
Osyczka, A., Kundu, S., 1995. A new method to solve generalized multicriteria opti- mization problems using the simple genetic algorithm. Struct. Optim. 10 (2), 94â€“99. doi:10.1007/BF01743536.
Palermo, G., Silvano, C., Valsecchi, S., Zaccaria, V., 2003. A system-level methodology for fast multi-objective design space exploration. In: Proceedings of the 13th ACM Great Lakes Symposium on VLSI. ACM, pp. 92â€“95.
Pascoletti, A., Serafini, P., 1984. Scalarizing vector optimization problems. J. Optim. The- ory Appl. 42 (4), 499â€“524.
Pellegrini, R., Campana, E.F., Diez, M., Serani, A., Rinaldi, F., Fasano, G., Iemma, U., Li- uzzi, G., Lucidi, S., Stern, F., et al., 2014. Application of derivative-free multi-objective algorithms to reliability-based robust design optimization of a high-speed catamaran in real ocean environment1. In: Engineering Optimization IV-Rodrigues et al.(Eds.),
p. 15.
Shan, S., Wang, G.G., 2005. An eï¬ƒcient Pareto set identification approach for multiobjec- tive optimization on black-box functions. J. Mech. Des. 127 (5), 866â€“874.
Steuer, R.E., Choo, E.-U., 1983. An interactive weighted Tchebycheff procedure for mul- tiple objective programming. Math. Program. 26 (3), 326â€“344.
Sun, Y., Ng, D.W.K., Zhu, J., Schober, R., 2016. Multi-objective optimization for robust power eï¬ƒcient and secure full-duplex wireless communication systems. IEEE Trans. Wirel. Commun. 15 (8), 5511â€“5526.
Tavana, M., 2004. A subjective assessment of alternative mission architectures for the human exploration of Mars at NASA using multicriteria decision making. Comput. Oper. Res. 31 (7), 1147â€“1164.
WÃ¤chter, A., Biegler, L.T., 2006. On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Math. Program. 106 (1), 25â€“57.
White, D.J., 1998. Epsilon-dominating solutions in mean-variance portfolio analysis. Eur.
J. Oper. Res. 105 (3), 457â€“466.
Zadeh, L., 1963. Optimality and non-scalar-valued performance criteria. IEEE Trans. Au- tom. Control 8 (1), 59â€“60.
Zhang, Q., Zhou, A., Zhao, S., Suganthan, P.N., Liu, W., Tiwari, S., 2008. Multiobjective optimization test instances for the CEC 2009 special session and competition. Univer- sity of Essex, Colchester, UK and Nanyang technological University, Singapore, special session on performance assessment of multi-objective optimization algorithms, tech- nical report 264, 1â€“30.
Zitzler, E., Deb, K., Thiele, L., 2000. Comparison of multiobjective evo- lutionary algorithms: empirical results. Evol. Comput. 8 (2), 173â€“195. doi:10.1162/106365600568202.
