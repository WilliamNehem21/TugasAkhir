
ORIGINAL ARTICLE

Support vector machine for diagnosis cancer disease: A comparative study
Nasser H. Sweilam a,*, A.A. Tharwat b, N.K. Abdel Moniem c

a Department of Mathematics, Cairo University, Faculty of Science, Giza, Egypt
b Department of Operation Research and Decision Support, Cairo University, Faculty of Computer & Information, Giza, Egypt
c Department of Statistics, Cairo University, National Cancer Institute, Giza, Egypt

Received 5 January 2010; accepted 29 September 2010
Available online 4 November 2010

Abstract Support vector machine has become an increasingly popular tool for machine learning tasks involving classification, regression or novelty detection. Training a support vector machine requires the solution of a very large quadratic programming problem. Traditional optimization methods cannot be directly applied due to memory restrictions. Up to now, several approaches exist for circumventing the above shortcomings and work well. Another learning algorithm, particle swarm optimization, Quantum-behave Particle Swarm for training SVM is introduced. Another approach named least square support vector machine (LSSVM) and active set strategy are intro- duced. The obtained results by these methods are tested on a breast cancer dataset and compared with the exact solution model problem.
© 2010 Faculty of Computers and Information, Cairo University.
Production and hosting by Elsevier B.V. All rights reserved.





* Corresponding author.
E-mail addresses: n_sweilam@yahoo.com (N.H. Sweilam), assemthar- wat@hotmail.com (A.A. Tharwat), nermeen2000@hotmail.com, ner- meenka2000@yahoo.com (N.K. Abdel Moniem).
1110-8665 © 2010 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.

Peer review under responsibility of Faculty of Computers and Information, Cairo University.
doi:10.1016/j.eij.2010.10.005

Introduction

Cancer is a group of diseases in which cells in the body grow, change, and multiply out of control [1]. Usually, cancer is named after the body part in which it originated; thus, breast cancer refers to the erratic growth of cells that originate in the breast tissue. A group of rapidly dividing cells may form a lump or mass of extra tissue. These masses are called cancer [2].
Cancer can either be cancerous (malignant) or non-cancer- ous (benign). Malignant tumours penetrate and destroy healthy body tissues, for more details see [3]. Cancer detection has become a significant area of research in pattern recognition community.
This paper intends to exhibit an integrated view of imple- menting automated diagnostic systems for breast cancer detection, and to classify cancer patients by constructing a

82	N.H. Sweilam et al.


non-linear optimal classifier using support vector machine. Because of the importance of making a right decision, The clas- sification accuracies of different training method for SVM, namely particle swarm optimization (PSO) method, quantum particle swarm optimization (QPSO) method, quadratic pro- gramming (QP) method, and the modifying learning problem of SVM namely least square SVM (LSSVM) are calculated.
The use of the classifier systems in the medical diagnosis’ area is increasing gradually, and there is no doubt that evalu- ation of data taken from patients and decisions of the experts are the most important factors in diagnosis. However, expert systems and deferent artificial intelligence techniques for clas- sification minimizing possible errors that could be occured be- cause of inexperienced experts, and also provide medical data to be examined in shorter time and more detailed.
Fig. 1 shows the various stages followed for the design of a clas- sification system. As it is apparent from the feedback arrows, these stages are dependent. On the contrary, they are interrelated and, depending on the results, one may go back to redesign earlier stages in order to improve the overall performance.

CAD system

The main purpose of pattern recognition in the field of cancer
computer aided diagnosis (CAD) of breast cancer from FNA depending on computational intelligence.

Support vector machine (SVM): an overview

The SVM proposed by Vapnik [6] has been studied extensively for classification, regression and density estimation.
SVMs attempt to find a hyperplane w · x + b = 0; xi ∈ Rn that separates the data points xi (meaning that all xi in a given class are on the same side of the plane), that corresponding to a given decision rule: g x  sign w x  b .
The question here is how this plane is determined? SVMs choose the separating hyperplane w · x + b = 0 that is furthest away from the data points xi, that is, that has maximal margin (Fig. 2). The underlying idea is that a hyperplane far from any observed data points should minimize the risk of making wrong decisions when classifying new data. To be precise, in SVMs the distance to the closest data points is maximized.
Suppose l patterns are given and each pattern consists of a pair {xi; y }N : a vector xi ∈ Rn and the associated label yi	1; 1 .  Let  X  R  is  the  space  of  patterns, Y    1; 1  is the space of labels. The SVM approach aims to find a classifier of the following form:

diagnosis is to solve the pattern classification dilemma where a pre-described set of input features is used to determine if a pa- tient has a particular disorder or not. This can help in the pro-

y(x)= sign

N


i=1
aiyi K(xi; x)+ b#

(1)

cess of diagnosis, namely Computer Aided Diagnostic (CAD) systems that used in the classification task where certain fea- tures (clinical findings) are used to assign a case to a particular pattern (malignant or benign) which represents a diagnosis.
Therefore, the CAD systems can improve the performance of the physicians in terms of (1) reducing the number of misdi- agnosis and (2) reducing the time taken to reach a diagnosis, these are the most two important criteria in the developing process of a CAD system. Other performance measures, such as computational complexity and operational load can be overlooked, if kept in an acceptable level.
where ai are positive real constants and b is a real constant, in
general, K(xi; x)= ⟨/(xi); /(x)⟩, ⟨·; ·⟩ represents the inner product operation, and / x is a nonlinear map from the ori- ginal space to the high dimensional space.
Assume the data set can be separated by a linear hyperplane in the high dimensional space, and that will cause:
T
i
In case of such separating hyperplane does not exist, a slack variable n is introduced, namely
y [wT/(x )+ b]≥ 1 — n ;	i = 1; .. . ; N

Breast cancer may be detected via a careful study of clinical	i	i	i
(3)

history, physical examination, and imaging with either mam- mography or ultrasound. However, definitive diagnosis of a breast mass can only be established through fine needle aspira- tion (FNA) biopsy, core needle biopsy, or excisional biopsy.
ni ≥ 0;	i = 1; ... ; N
According to the structural risk minimization principle, the risk bound is minimized by the following minimization problem:

Among these methods, FNA is the easiest and fastest method of obtaining a breast biopsy, and is effective for women who have fluid-filled cysts. Research works on the Wisconsin Diag-
min J1 w; n
w;n
1
wTw	c
2
N


i=1
ni	(4)

nosis Breast Cancer (WDBC) data grew out of the desire to diagnose breast masses accurately based solely on FNA. To
Subject to (3), one constructs the Lagrangian function as
follows:

improve the accuracy and efficiency of the detection of breast cancer, a number of research projects are focusing on develop- ing methods for computer aided diagnosis (CAD) of breast


L1(w; b; n; a; b) = J1(w; n)— 
XN

N


i=1
ai y [wT/(xi)+ b]— 1 + ni}








Figure 1	The basic stages involved in the design of a classification system.

Support vector machine for diagnosis cancer disease: A comparative study	83
























Figure 2	A separating hyperplane (w, b) for a two dimensional training set [5].


where ai > 0; bi > 0; i  1; .. . ; N are the Lagrangian multi- pliers of (3). The optimal point will in the saddle point of the Lagrangian function, i.e.
max min L1 w; b; n; a; b	6
w;b  w;b;n
By equating the partial differentiation with zeros, the following equalities will obtained:
However, these algorithms are no longer suitable when the ker- nel matrix (or original data matrix for linear SVM) does not fit in main memory. In order to solve larger problems, special- purpose algorithms have been created that take advantage of unique aspects of the SVM problem. These can be divided into three categories.
Subset selection algorithms

@L1
@w = 0;	w =
N
i=1
aiyi/(xi)
Subset selection methods sacrifice some precision in the solu-

@L1
@b = 0;
N
i=1
aiyi = 0
(7)
tion (in terms of the Lagrange multipliers ai) in order to break the optimization problem up into manageable pieces. One
optimization approach for SVM, called Chunking [9], relies

@L1
@n = 0;	0 ≤ a ≤ c;	i = 1; 2; ... ; N
Substituting (7) in (5), the following quadratic programming (QP) problem will arise:
on the observation that only the support vectors contribute to the final model and other data points are inconsequential to the solution. So in Chunking, an arbitrary subset of the data is first used to generate an SVM solution with a general-
purpose QP package. Then only the support vectors are re-

N
min Q1 a
a	i=1
ai —
N
i;j=1
aiajyiyjK(xi; xj)	(8)
tained and the rest of the data are discarded. Additional data are then added to complete the subset and a new QP solution is determined. This is repeated until the Kuhn–Tucker conditions

where K xi; xj   / xi ; / xj  is called the kernel function
(Fig. 3 shows the architecture of SVM with kernel function). By solving the above QP problem Eq. (8) subject to the gi-
ven constraints in (7), a hyperplane in the high dimensional space and the classifier in the original space as in (1) are ob- tained .

Survey of training algorithms of SVM

Implementing an SVM learning algorithm requires solving a QP problem. Initially, existing general-purpose quadratic opti- mization algorithms were applied to solve the SVM problem [6]. For example, quasi-Newton methods such as MINOS [7] or primal–dual interior point methods such as LOQO [8] are applicable for small data sets (1000 s of points). Their advan- tage is that they are off the shelf and so can be immediately exploited, and they also provide high numerical precision.
are met for each data sample. The Chunking approach works as long as the kernel matrix for the support vectors can be stored in main memory. If this is not the case, then alternative methods are required, such as decomposition.
In decomposition approaches, the data (and correspond- ingly the parameters) are split into a number of fixed-size sets, each called a working set. Optimization occurs on each work- ing set while holding the other parameters fixed. This effec- tively performs coordinate descent on subsets of the parameters. The popular software implementation SVMLight
[10] and SVMTorch [11] uses the decomposition strategies. The sequential minimal optimization (SMO) algorithm [12] is an extreme form of decomposition using working sets of two data points. The smallest working set that can be optimized is 2 if the constraints (3) for SVM classification are to hold. SMO takes advantage of the fact that under this condition the optimization sub_problem for standard SVM can be solved




84	N.H. Sweilam et al.






















Figure 3	Architecture of SVM.

analytically. SMO exhibits better scaling properties and has re- duced demand on main memory than Chunking. The popular software implementation LIBSVM [13] implements a variant of SMO for classification, regression, and single class learning settings.

Iterative algorithms

Gradient descent can be applied to the primal SVM optimiza- tion problem resulting in an iterative algorithm. The main advantage of iterative methods is that they result in algorithms with few steps and so are simple to implement. The disadvan- tage is that in general they exhibit linear convergence and so are slower than standard QP solvers.

Exploiting alternative SVM formulations

By modifying Eq. (4) subject to constraint in Eq. (3), it is pos- sible to simplify the resulting optimization problem. This may involve simplifying or reducing the number of constraints by modifying the error functional or penalization. For example, an approach called Lagrangian SVM (LSVM) [14] uses a learning formulation, which results in an optimization prob- lem that depends on solving systems of linear inequalities.
LSVM has formulation:
a = Q—1(e + ((Qa — e)— ba) )	(11)
This leads to the following iterative scheme,
ai+1 = Q—1(e + ((Qai — e)— bai) )	(12)
Mangasarian established the global linear convergence from any starting point under condition 0 < b < 2/C. LSVM re- quires nothing more complex than the inversion Q—1 computed once at the beginning of the algorithm, and Sherman– Morrison–Woodbury identity will be used. For linear decision boundaries, this algorithm can solve problems with millions of samples in minutes on a desktop computer. The drawback is that the learning problem is modified to minimize the square of the original SVM loss function and regularization is also ap- plied to the constant offset b. It is still an open question how these modifications affect generalization performance.

Training of SVM methods

Different methods for training SVM will be examined. Where training of SVM consists of determining the optimal value of non-negative multipliers a in Eq. (6).These methods are:

Particle swarm optimization (categorizing in iterative methods).
Quantum-behaved particle swarm optimization (subset

min
w,b,n
1	2
2 ( w
+ b )+ 
l
2
2 i=1

(9)
selection methods).
Quadratic program using active set strategy (subset selection methods).

s.t. yi(⟨w, xi⟩ + b) ≥ 1 — ni, ∀i
The benefit is that objective function is strongly convex and equality constrain disappears in its dual:
max eTa — 1 aTQa
Least square version of SVM (alternative SVM formulations).
For comparison we try to pick one method from different category of training algorithms mentioned in the survey above.

a	2
s.t. a ≥ 0
(10)
Particle swarm optimization

LSVM algorithm is based directly on Karush–Kuhn–Tucker necessary and sufficient optimality conditions for the dual problem, it can be written in the following equivalent form. For any positive b,
The particle swarm optimization (PSO) method has been intro- duced by Kennedy and Eberhart [15] and is inspired by the emergent motion of a flock of birds searching for food. As a

stochastic search scheme, PSO has characters of simple com-
Randomly initialize particle velocities 0 6 (0) 6 max for

putation and rapid convergence capability.
PSO is a population-based heuristic search technique; each
i = 1, 2, .. . , p, d	1, 2, ... , l.
Evaluate cost function values (Eq. (8))
i,d	d

particle represents a potential solution within the search space. Each particle has a position vector Xi, a velocity vector Vi, the



l	l	N
min / a(0)  =	a a y y K(x , x )—	a

particle so far, and the best position of all particles gbest in
Using design space coordinates a(0) for i = 1, 2, ... , p.

current generation. The updating equations of PSO are as
pbest
i
(0)
(0)

follows:
Vi(t + 1)= wVi(t + 1)+ c1r1(Xpbesti(t)— Xi(t))
Set	/ ai	/ ai	and	pbesti	ai	for
i = 1, 2, .. . , p.
Set (ugbest) to minimal (upbest) and gbest to correspond-

+ c2r2(Xgbest(t)— Xi(t))
Xi(t + 1)= Xi(t)+ Vi(t + 1)
(13)
i	i
ing a(0).
Optimize
w	w 


where the parameters c1

and c2

are set to constant value, which
Update inertia weight w using: w =t  wmax —
Update particle velocity vectors Vi+1 .
max— min
itermax

are normally taken as 2, r1
and r2
are two random values, uni-
Update	particle	position	vectors	a(t+1).	Using

formly distributed in [0, 1], w is inertia weight which controls
ai+1 = ai
+ t+1 .

the influence of previous velocity on the new velocity. For	(d) Evaluate	cost	function
the second part in Eq. (13) is called ‘‘cognition’’ character	min /(a)= Pl  Pl  aiajy y K(xi, xj)— PN ai. Using
1
			 		
If	/(a(t+1)) < /pbest	then	(/pbest)= /(a(t+1)),

Particle swarm for training SVM [16]
i	i	i	i
pbest = a(t+1).

If /(a(t+1)) < /gbest then (/gbest)= /(a(t+1)). For i = 1,

Because the Lagrange multipliers a, constitute a vector
a	a1, a2, .. . , al in one-dimensional space, the optimization
(8) can be solved by PSO. Differing from the general PSO, all particles of the PSO training SVM must satisfy both constraints
PN a y = 0 and C P a P 0, ∀i. Thus, the PSO algorithm
i	i	i	i
2, ... , p.
If all members of gbest fulfil the Karush–Kuhn–Tucker (KKT) optimality conditions of the QP problem, or the number of iterations, t, is up to itermax, then go to Report

must be improved according to constraint C P ai P 0, i, Eq.
(9) can be written in the following form [16]:
Increment t.
Go to 2(a).

temp — (t+1) = w · (t) + c rand ( )(pbest
a(t))
Report Results

+ c rand ( )(gbest
a(t))
Terminate


C	a(t),	a(t)	temp	(t+1) > C
(t) = < (t+1),	a(t) + temp — (t+1) < C


i,d
According to linear equality constraint	N
i=1





aiyi = 0
(14)
Complexity of the algorithm. The complexity of the above algo- rithm depends on evaluation of fitness function in each step, the evaluation of fitness function is merely evaluation of the kernel used K(xi, xj), which requires O(dL) operations, where dL is the size of training data.

l	l
a(t+1)y =
l
a(t)y +
l
(t+1) y =


(t+1)y
Quantum-behaved particle swarm optimization [17]

i,d	d	i,d d	i,d	d
d=1	d=1	d=1	d=1
i,d	d

= X (t+1)y — X

	 


 —(t+1) y 
In the quantum physics, the motion state of particles having the





= sum V+ — sum V—	(15)
Thus, the Lagrange multipliers a(t+1) satisfies both constraints
N aiy = 0, C P ai P 0, ∀i.

Algorithm to train SVM by PSO

Initialize
Set constants wmin, wmax is inertia weights equation (13), p: no. of iterations. C: constant defined by user such that C P ai P 0, ∀i, parameters c1 and c2 are set to constant value as in Eq. (13), itermax: maximum no. of iterations, Vmax maximum velocity, d: no. of particles.
Set constants t = 0. Set random number seed.
Randomly  initialize  particle  positions  a(0) ∈ Rl, 0 6
cles can be expressed by wave function instead of denoting method of the velocity and position for the particles. At the same time, based on the Heisenberg uncertainty principle, the velocity and position of particles cannot be accurately measured simultaneously. The probability of occurrence for the particle in a case of position and time can be denoted by probability den- sity function of the corresponding wave function w.
In order to facilitate the analysis, particles are considered to move in the one dimensional space. Assume p is the canter of Delta potential field, so the potential energy of particles in the Delta potential trough is:
V(x)= —c&d(x — p)	(16)
The wave function of particle can be got by the above equation:

ai 6 C.
For i = 1, 2, .. . , p, d = 1, 2, ... , l where l is equal to
1
w x)= 

exp(—p — x  /L)	(17)

no. of samples.
(	,ﬃLﬃﬃﬃ *




In Eq. (17), the parameter L relying on the width of poten- tial trough is used to determine the search domain. The parti- cles move according to the following iterative equation:
zation problem remains essentially the same. Also, since H is symmetric, the problem is to solve:
1

min
T	T
Q (aB)= a HBBaB — a (e — HBNaN)

	!	aB

		 	 	


1	2 B	B





i=1
i=1
i=1
i=1
(18)
aB ≥ 0
C1 — aB ≥ 0

PPid = / × Pid + (1 — /)× Pgd,  / = rand	(19)

xid = PPid  h × |mbestd — xid|
× ln(1/u),	u is a random variable	(20)
where mbest is the middle position of the particle swarm (pbest); PPid is the random point between Pid and Pgd, h is the only parameter of the QPSO algorithm. Commonly let h  1.0  0.5   MAXITER  T /MAXITER  0.5, where
T is the current number of iterations, MAXITER is the max- imum number of iterations.

Training algorithm for training SVM by QPSO
Using QPSO to solve the SVM equation (8) requires

Criteria for optimality.
A way to decompose the problem
A way to extend QPSO to optimize SVM sub problem.

Criteria for optimality. The Karush–Kuhn–Tucker (KKT) con- ditions are necessary and sufficient for optimality. Since H is a positive semi-define matrix [18] (the kernel function used is po- sitive semi-define).
Decompose the problem. The decomposition method presented here is due to [19], and works on the method of feasible direc- tions. The idea of the method is to find the steepest feasible direction d of ascent on the objective function W (as defined in Eq. (3)), under the requirement that only the q components is a nonzero value. The ai corresponding to the q components will be included in the working set. Finding an approximation
An algorithm to train SVM with QPSO

Initialize
A feasible solution that satisfies the linear constraint
aT y = 0, with constraints 0 6 ai 6 C also met, is needed.
Construction of initial solution:
Let c ∈ [0, C] C R, and c (some positive integer) 6 min(#
+ve examples, i.e. (yi = +1), # of —ve examples, i.e. (yi = —1)) in the training set.
Randomly pick a total of c positive examples, c negative
examples, and initialize their corresponding ai to c.
By setting all other ai to zero, the initial solution will be feasible.
The value 2c gives the total number of initial support vectors, and since these initial support vectors are a ra- ndomly chosen, it is suggested that the value of c be kept small.
Repeat
Decompose the problem
Sorting the training vectors in increasing order according to yi W a i.
Select q variables for the working set B.
The remaining l q variables (set N) are fixed at their current value.
Assuming q to be even, a ‘‘forward pass’’ select q/2 examples from the front of the sorted list, and a ‘‘backward pass’’ selects q/2 examples from the back of the sorted list.
Initialize
All particles be initialized such that aT yB	aT yN	0 is met. This is done as follows:

to d is equivalent to solve the following problem:

Maximise ∇W(a)Td
Subject to yTd = 0,	di ≥ 0,	if ai = 0
di ≤ 0,	if ai = C di ∈ {—1, 0, 1}
|{di : di–0}| = q

T




(21)
Set each particle in the swarm to the q-dimensional vector aB.
Add a random q-dimensional vector d satisfying to each particle, under the condition that the particle will still lie in the hypercube 0, C q. Initializing the swarm in this way ensures that the initial swarm lies in the set of feasible solutions P P AP  aT yN allowing the flight of the swarm to be defined by fea- sible directions.

for y d to be equal to zero, the number of elements with sign
matches between di and yi must be equal to the number of ele- ments with sign mismatches between di & yi . Also, d should be chosen to maximize the direction of ascent W a Td. It is necessary to rewrite the objective function Eq. (8) as a function that is only dependent on the working set. Split a into two sets aBandaN. If a, y and H are appropriately rearranged, we have
a =  aB  ,  Y =  YB  ,  H =  HBB  HBN	(22)
Set the iteration number to zero
Use QPSO to optimize W on B.
Repeat
Evaluate the performance W a of each particle.
Evaluate new P id of each particle.
Evaluate new Pgd
Evaluate mbest by Eq. (18).
Evaluate the random point PP id of each particle by Eq. (19).

aN	YN
HNB  HNN
Move each particle to its new position, according to Eq. (20).

Since only aB is going to be optimized. Q1 is rewritten in terms
of aB. If terms that do not contain aB are dropped, the optimi-
Make T	T	1 go to step (a) until terminal condi- tion satisfied (set by user).



Until the KKT condition are met
Return the optimized ai from B to the original set of variables.
Terminate and return a.
Complexity of the algorithm. The complexity of the above algo- rithm depends on evaluation of the fitness function in each step that is merely evaluation of the used kernel K xi, xj, which re-
quires O(d ) operations, where d is the size of training data.
system of linear equations which yields the solution of the QP problem. Because the set A is unknown in the beginning, it is constructed iteratively by adding and removing constraints and testing if the solution remains feasible.
Algorithm
The construction of the set A starts with an initial active set
A0 containing the indices of the bounded variables (lying

L	L

So one can deduce that the decomposition will be of order O(N log(N)) and the evaluation of fitness function will be of order O(N) so the total time will be of order O(N)+ O(N log(N)).

Quadratic program using active set strategy

The medium-scale algorithm is an active-set strategy (also known as a projection method) similar to that of Gill et al de- scribed in [20]. It has been modified for both linear program- ming (LP) and quadratic programming (QP) problems.
The basic idea of the algorithm is to find the active set A, i.e., those inequality constraints that are fulfilled with equality. If the set A is known, the KKT conditions reduce to a simple













Figure 4	Affine scaling of the non-feasible solution.
on the boundary of the feasible region) whereas those in
F 0	1, ... , N A0 are free (lying in the interior of the fea- sible region) (Fig. 4).
Then the following steps are performed repeatedly for
k = 1, 2, .. .:
Solve the KKT system for all variables in Fk.
If the solution is feasible, find the variable in Ak that violates most of the KKT conditions, move it to Fk then go to 1.
Otherwise find an intermediate value between old and new solution lying on the border of the feasible region, move one bounded variable from Fk to Ak then go to 1. The intermediate solution that is obtained in step 3 is computed as follows: ak ga—k  1 g ak—1 with maxi- mal g [0, 1] (affine scaling), where a—k is the solution of the linear system in step 1, i.e. the new iterate ak lies on the connecting line of ak—1 and ak, see Fig. 4.
While the optimum is found if during step 2 no violat- ing variable is left in Ak.

Complexity of algorithm
Active-set algorithm needs O N2	N memory, where N is the number of free unbounded variables.

Least square support vector machine (LSSVM)

The least squares version of the SVM classifier is formulated by the classification problem as [21]:




Figure 5	Methodology of cancer diagnosis model.

1		C Xn

@	*  *  *  *	X *

	

Subject to yi (⟨w.xi⟩ + b)= 1 — fi,	i = 1, 2, ... , n
 @ L (w*, b*, a*, f*)= — X a*y = 0

According to Eq. (19), their dual problems are built as follows:
@f P	i i
(26)


Xn	Xn

	

@ L (w*, b*, a*, f*)→ a* = Cf

i=1
j=1
@f LP(w*, b*, a*, f*)→ yi (⟨w · xi⟩ + b)— 1 + ni = 0

where ai are Lagrange multipliers (which can be either positive
or negative) now due to the equality constraints as follows from the Kuhn–Tucker conditions [18],
can be written immediately as the solution to the following set of linear equations [18]

(27)



where	z = [xTy1, xTy2,, ... , xTyn], y = [y1, ... , yn ],~1 = [1,

... , 1], e = [e1, ... , eN], a = [a1, ... , aN]The solution is also gi-





Figure 6	Correction rate value on testing data, 40–60% training test partition. The SVM-PSO shows the highest accuracy.
(28)

Mercer’s condition can be applied again to the matrix
X = ZZT,	where

X = y y x x = y y K(x , x )	(29)
LSSVMs use a set of linear equations for training while SVMs use a quadratic optimization problem, then X  A B is the solution to the equation AX B computed by Gaussian elimination with partial pivoting [22] and this is the technique we use with LSSVM.







Figure 7	Specificity value on testing data, 40–60% training test partition. The SVM-PSO shows the highest accuracy.




Figure 8	Sensitivity value on testing data, 40–60% training test partition. The SVM-QPSO shows the highest accuracy.
Figure 10	Negative predictive value on testing data, 40–60% training test partition. The SVM-QPSO shows the highest accuracy.

Support vector machine for diagnosis cancer disease: A comparative study	89

Complexity of algorithm
Gaussian elimination solves a system of n equations for n un- knowns in ‘‘n(n + 1)/2’’ divisions, ‘‘(2n3 + 3n2 5n)/6’’ multi- plications, and ‘‘(2n3 + 3n2 5n)/6’’ subtractions, for a total of approximately ‘‘2n3/3’’ operations. So it has a complexity of order O(N3).

Methodology





Figure 11	Error rate on testing data, 40–60% training test partition. The SVM-PSO shows the lowest error.
Fig. 5 depicts the proposed methodology for Cancer Diagnosis Model by pre-processing the data using scaling (we scale line- ally each attribute to the range of [0, 1]), pre-processed data are split into training and testing (independent) datasets. The training dataset is used to build SVM classifier. The validity

























Figure 12	Receiver operating characteristic (ROC) curves for classifier resulted from training of SVM with PSO. The area under curve is 0.96281.
























Figure 13	Receiver operating characteristic (ROC) curves for classifier resulted from training of SVM with QPSO. The area under curve is 0.95983.



of each classifier created with the classifier is evaluated using the sensitivity and specificity of the SVM classifier in distin- guishing cancer patients from non-cancer controls. SVM clas- sifiers are built for various combinations of features until the classification accuracy of the SVM classifier reaches its maxi- mum value. Estimates of classification accuracy are calculated by using the cross validation method where a validation data- set is used to evaluate the generalization error.
Four different methods to construct classifier (i.e. training SVM) are:
Particle swarm,
Quantum behaved particle swarm,
Quadratic program using active set strategy,
Gaussian elimination with partial pivoting of set of linear equations of LSSVM.
The experiments are done on the Wisconsin Database of Breast Cancer (WDBC) from the UCl [23]. The data taken from fine needle aspirates from human breast tissue were analyzed. They have been collected by Wolberg and Mangasarian [24], at the University of Wisconsin-Madison Hospital. The data consists of 683 records of virtually as- sessed nuclear features of fine needle aspirates taken from patient’s breasts. The results of the four methods were compared.

Experimental results and discussion

The effectiveness of the four different methods for training support vector machine will be evaluated and compared.


























Figure 14	Receiver operating characteristic (ROC) curves for classifier resulted from training of SVM with QP. The area under curve is 0.95442.























Figure 15	Receiver operating characteristic (ROC) curves for classifier resulted from training of LSSVM. The area under curve is 0.93788.

Support vector machine for diagnosis cancer disease: A comparative study	91
swarm into perspective, so two more machine learning tech- niques were evaluated alongside it.
Measures for performance evaluation
























Figure 16	A basic ROC graph showing four discrete classifiers.



Particle swarm optimization: A MATLAB code was writ- ten to train SVM by PSO. The KKT conditions needed to be satisfied within an error threshold of 0.005 in order to find an optimal solution quickly. The upper bound C was kept at 100.0.
Quantum behaved particle swarm: A MATLAB code was written to train SVM by QPSO. The KKT conditions needed to be satisfied within an error threshold of 0.02.Optimization of the working set terminated when the KKT conditions on the working set were met within an error of 0.001, or when the swarm has optimized for five hundred iterations. The following parameters defined for the experimental QPSO: By letting c = 10, a total of 20 ini- tial support vectors were chosen to start the algorithm. The value of Contraction–Expansion Coefficient h is set to be 0.7, along with iteration increases, the value of a linearly reduces to 0.3, so h = (0.7 — 0.3)* (MAXITER— T )/MAXITER + 0.3.

For each experiment the upper bound C was kept at 100.0.

Quadratic program using active set strategy: the BIO- LEARNING toolbox under the BIOINFO of the MAT- LAB toolbox is used.
Least square support vector machine: The BIOLEARN- ING toolbox under the BIOINFO of the MATLAB toolbox is used.

Training was done with the kernel function: k(x, xi)
x xi
1.02
The aim of the above comparison is to place the SVM’s per- formance trained by the quantum particle swarm and particle

Several measures were used in order to evaluate the effective- ness of the given methods. These measures are classification accuracy (Fig. 6), analysis of specificity (Fig. 7), sensitivity (Fig. 8), positive predictive value (Fig. 9), negative predictive value (Fig. 10), error rate (Fig. 11), receiver operating charac- teristic (ROC) curves Figs. 12–15 and confusion matrix [25].
Discrete classifier: is one that outputs only a class label. Each discrete classifier produces an (fp rate, tp rate) pair cor- responding to a single point in ROC space.
The classifier results from training SVM with PSO shows best area under curve which means that this classifier have bet- ter average performance.
Discrete Roc Curve (Fig. 16).
Several points in ROC space are important to note. The lower left point (0, 0) represents the strategy of never issuing a positive classification; such a classifier commits no false positive errors but also gains no true positives. The opposite strategy, of unconditionally issuing positive classifications, is represented by the upper right point (1, 1). The point (0, 1) represents perfect classification. B, C’s performance is perfect as shown.
Informally, one point in ROC space is better than another if it is to the northwest (tp rate is higher, fp rate is lower, or both) of the first. Classifiers appearing on the left hand-side of an ROC graph, near the X axis, may be thought of as con- servative: they make positive classifications only with strong evidence so they make few false positive errors, but they often have low true positive rates as well.
Classifiers appearing on the left hand-side of an ROC graph, near the X axis, may be thought of as conservative: they make positive classifications only with strong evidence so they make few false positive errors, but they often have low true positive rates as well. Classifiers on the upper right-hand side of an ROC graph may be thought of as liberal: they make positive classifications with weak evidence so they classify nearly all pos- itives correctly, but they often have high false positive rates.

Conclusion

To place the SVM’s performance trained by swarm intelligence into perspective, four more machine learning techniques were evaluated alongside it. The techniques selected were PSO, QPSO, active set strategy and LSSVM, PSO and QPSO re- cords slightly higher overall accuracy (0.9352%–0.9306%) than the other techniques (0.8773%–0.9091%) on set. Consid- ering the abnormality assessment rank feature in the proposed comparative study is beyond our plan in this work, so it will be considered in the extension of this work to re-compare the de- scribed techniques.
When using the SVM, three obstacles are confronted: how to choose the kernel function and optimal input feature subset for SVM, and how to set the best kernel parameters. These obstacles are crucial because the feature subset choice influ- ences the appropriate kernel parameters and vice versa.
Feature selection is an important issue in building classifica- tion systems. It is advantageous to limit the number of input fea- tures in a classifier to in order to have a good predictive and less



computationally intensive model. Building a model that can handle the three obstacles at the same time is a very important issue and needs further research and work in the future.

References

West D, Mangiameli P, Rampal R, West V. Ensemble strategies for a medical diagnosis decision support system: a breast cancer diagnosis application. Eur J Operat Res 2005;162:532–51.
<http://www.imaginis.com/breasthealth/breast_cancer.asp>
[accessed May 2010].
Kordylewski H, Graupe D, Liu K. A novel large-memory neural network as an aid in medical diagnosis applications. IEEE Trans Inform Technol Biomed 2001;5(3):202–9.
Wolberg WH, Street WN, Mangasarian OL. Breast cytology diagnosis via digital image analysis. Anal Quant Cytol Histol 1993;15(6):396–404.
Mu T, Nandi AK. Breast cancer detection from FNA using SVM with different parameter tuning systems and SOM–RBF classifier. J Franklin Inst 2007;344(3–4):285–311.
Vapnik VN. The Nature of Statistical Learning Theory. New York: Springer-Verlag; 1995.
Murtaghand BA, Saunders MA. Large-scale linearly constrained optimization. Math. Programming 1978;14:41–72.
Vanderbei RJ. LOQO: an interior point code for quadratic programming. Optimizat Methods Software 1999;12:451–84.
Osuna E, Freund R, Girosi F. Improved training algorithm for support vector machines. In: NNSP; 1997.
Joachims T. Web page on SVMLight: <http://svmlight.joachims. org>.
Collobert R, Bengio S. SVMTorch: <http://www.idiap.ch/ machine_learning>.
Platt J, Scho¨lkopf B, Burges CJC, Smola AJ. Fast training of support vector machines using sequential minimal optimization. In: Advances in Kernel methods support vector learning. Cam- bridge, MA: MIT Press; 1999. p. 185–208.
Chang CC, Lin CJ. LIBSVM: a library for support vector machines; 2001. <http://www.csie.ntu.edu.tw/_cjlin/libsvm>.
Mangasarian OL, Musicant DR. Lagrangian support vector machines. J Mach Learning Res 2001;1:161–77.
Kennedy J, Eberhart R. Particle swarm optimization. In: Pro- ceedings of the IEEE international conference on neural network, IV[C], vol. 4(2). Piscataway, NJ: IEEE Service Center; 1995. p. 1942–8.
Tang F, Chenn M, Wang Z. New approach to training support vector machine. J Syst Eng Electron 2006;17(1):200–5.
Sun J, Feng B, Xu W. Particle swarm optimization with particles having quantum behavior. In: Congress on evolutionary compu- tation. IEEE, Piscataway, NJ, ETATS-UNIS (Monographie); 2004. p. 325–31.
Fletcher R. Practical methods of optimization. Wiley; 1988.
Joachims T, Scolkopf B, Rurges CJC, Smola AJ. Making large- scale SVM learning practical. In: Advances in Kernel methods support vector learning. Cambridge, MA: MIT Press; 1999. p. 169–84.
Gill PE, Murray W, Saunders MA, Wright MH. Procedures for optimization problems with a mixture of bounds and general linear constraints. ACM Trans Math Software 1984;10: 282–98.
Suykens J, Vandewalle J. Least squares support vector machine classifiers. Neural Process Lett 1999:293–300.
David G. Linear and non linear programming. Luenberger Stanford University; 2002.
Blake CL, Merz CJ. UCI repository of machine learning data base. Irvine, CA: University of California; 1998. <http:// www.ics.uci.edu/mlearn/MLRepository.html>.
Wolberg WH, Mangasarian OL. Multisurface method of pattern separation for medical diagnosis applied to breast cytology. PNAS 1990;87:9193–6.
Hand J, Till RJ. A simple generalization of the area under the ROC curve for multiple class classification problems. Mach Learning 2001;45:171–86.
