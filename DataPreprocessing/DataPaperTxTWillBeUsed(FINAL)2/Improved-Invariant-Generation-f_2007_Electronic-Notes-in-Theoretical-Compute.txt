Electronic Notes in Theoretical Computer Science 176 (2007) 21–35	
www.elsevier.com/locate/entcs


Improved Invariant Generation for
Tvoc

Yi Fang1
Microsoft Corp. Redmond WA, U.S.A

Lenore D. Zuck 2 ,3
Computer Science Department University of Illinois at Chicago Chicago MI, U.S.A

Abstract
The NYU Tvoc project applies the method of translation validation to verify that optimized code is semantically equivalent to the unoptimized code, by establishing, for each run of the optimizing compiler, a set of verification conditions (VCs) whose validity implies the correctness of the optimized run. The core of Tvoc is Tvoc-sp, that handles structure preserving optimizations, i.e., optimizations that do not alter the inner loop structures. The underlying proof rule, Val, on whose soundness Tvoc-sp is based, requires, among other things, to generating invariants at each “cutpoint” of the control graph of both source and target codes. The current implementation of Tvoc-sp employs somewhat na¨ıve fix-point computations to obtain the invariants. In this paper, we propose an alternative method to compute invartiants which is based on simple data-flow analysis techniques.
Keywords: Translation validation, invariant generation, data abstraction, data-flow analysis.


Introduction
There is a growing awareness, both in industry and academia, of the crucial role of formally proving the correctness of safety-critical portions of systems. Most verifi- cation methods focus on verification of specifications with respect to requirements, and high-level code with respect to specifications. However, if one is to prove that the high-level specification is correctly implemented in low-level code, one needs to verify the compiler which performs the translations. Verifying the correctness

1 Email: yfang@microsoft.com
2 Email: lenore@cs.uic.edu
3 This research was supported in part by NSF grant CCR-0205571 and SRC grant 2004-TJ-1256.

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2006.06.016

of modern optimizing compilers is challenging because of the complexity and re- configurability of the target architectures, as well as the sophisticated analysis and optimization algorithms used in the compilers.
Formally verifying a full-fledged optimizing compiler, as one would verify any other large program, is not feasible, due to its size, evolution over time, and, pos- sibly, proprietary considerations. The Translation Validation approach, which was introduced in [9], offers an alternative to the verification of translators in general and of compilers in particular: Rather than verify the compiler itself one constructs a validating tool which, after every run of the compiler, formally confirms that the target code produced is a correct translation of the source program.
In the past five years we have been working towards developing a methodol- ogy for fully automatic translation validation of optimizing compilers (see, e.g., [14,15,3]). The methodology consists of a theory of correct translation, and a tool suite, Tvoc, that performs translation validation for Intel’s ORC compiler. The theory distinguishes between structure preserving optimizations, that admit a clear mapping of control and data values in the target program to corresponding con- trol and data values in the source program, and structure modifying optimizations that admit no such clear mapping. Most high-level optimizations are structure pre-

serving.
Tvoc consists of two main parts:
Tvoc-sp that handles the structure

preserving transformations, and Tvoc-loop that handles loop reordering transfor- mations, which are reduced to equivalence checking handled by Tvoc-sp.
Given source (pre-optimization) and target (post-optimization) codes, Tvoc-sp constructs verification conditions (VCs) whose validity implies the semantic equiv- alence between the two, and passes the VCs to a theorem prover. The theory underlying Tvoc-sp is the Floyd-style proof rule Val. The proof rule Val differs from similar proof rules by requiring, in addition to the usual data and control mappings, the construction, at selected control points, of invariants that carry in- formation among basic blocks. The strength of the invariants generated determines the power of the tool.
In this paper we study the invariant generation that Val calls for, and propose a methodology to obtain stronger invariants than obtained in the current implemen- tation of Tvoc-sp. Currently (see [3]), Tvoc-sp performs rather na¨ıve fixed-point computations to obtain the data mappings and the invariants. The gist of our idea is to translate the input to Tvoc, which is textual Whirl ([12]), into Static Single Assignment (SSA) form, and perform simple data-analysis to obtain both the data mapping and the invariants required to apply Val. We also present a version of Val that is more suitable for our needs. The techniques described here were im- plemented (see [7]) and produce superior results to those of Tvoc in almost all cases.
The paper is organized as follows: In Section 2 we present the new version of rule Val and discuss how it differs from its predecessors. In Section 3, which is the heart of paper, we describe the new method of invariant generation. In Section 4 we describe a new computation of data mapping that the new invariants generation entails. We conclude in Section 5.

Related Work
The work in [9] developed a tool for translation validation, cvt, that suc- ceeded in automatically verifying translations involving approximately 10,000 lines of source code in about 10 minutes. The success of cvt critically depends on some simplifying assumptions that restrict the source and target to programs with a single external loop, and assume a very limited set of optimizations.
Other approaches [8,11] considered translation validation of less restricted lan- guages than considered in [9], allowing, for example, nested loops. They also consid- ered a more extensive set of optimizations. However, the methods proposed there were restricted to structure preserving optimizations, and could not directly deal with more aggressive optimizations that involve code motion or loop reordering transformations.
The theory of correct translation validation on which the current paper is based appears in, e.g., [14,15], and [3] presents the Tvoc tool suite.

The General Framework
The compiler receives a source program written in some high-level language, trans- lates it into an Intermediate Representation (IR), and then applies a series of opti- mizations to the program – starting with classical architecture-independent global optimizations, and then architecture-dependent ones such as register allocation and instruction scheduling.
The intermediate code is a three-address code. It is described by a flow graph, which is a graph representation of the three-address code. Each node in the flow graph represents a basic block, that is, a sequence of statements that is executed in its entirety and contains no branches. The edges of the graph represent the flow of control.
In order to present the formal semantics of source and intermediate code we introduce transition systems, Ts’s, a variant of the transition systems of [10]. A Transition System S = ⟨V, O, Θ, ρ⟩ is a state machine consisting of:
V a set of state variables,
O ⊆ V a set of observable variables,
Θ an initial condition characterizing the initial states of the system, and
ρ a transition relation, relating a state to its possible successors.
The variables are typed, and a state of a Ts is a type-consistent interpretation of the variables. For a state s and a variable x ∈ V , we denote by s[x] the value that s assigns to x. The transition relation refers to both unprimed and primed versions of the variables, where the primed versions refer to the values of the variables in the successor states, while unprimed versions of variables refer to their value in the pre-transition state. Thus, e.g., the transition relation may include “y' = y + 1” to denote that the value of the variable y in the successor state is greater by one than its value in the old (pre-transition) state.

The observable variables are the variables we care about, where we treat each I/O device as a variable, and each I/O operation removes/appends elements to the corresponding variable. If desired, we can also include among the observables the history of external procedure calls for a selected set of procedures. When comparing two systems, we will require that the observable variables in the two systems match. A computation of a Ts is a maximal finite or infinite sequence of states σ :
s0, s1,... , starting with a state that satisfies the initial condition such that every two consecutive states are related by the transition relation. I.e., s0 |= Θ and
⟨si, si+1⟩ |= ρ for every i, 0 ≤ i +1 < |σ| 4 .
A transition system S is called deterministic if the observable part of the initial condition uniquely determines the rest of the computation. That is, if S has two computations s0, s1,... and t0, t1,... such that the observable part (values of the observable variables) of s0 agrees with the observable part of t0, then the two com- putations are identical. We restrict our attention to deterministic transition systems and the programs which generate such systems. Thus, to simplify the presentation, we do not consider here programs whose behavior may depend on additional in- puts which the program reads throughout the computation. It is straightforward to extend the theory and methods to such intermediate input-driven programs.
Let P	= ⟨V , O , Θ ,ρ ⟩ and P	= ⟨V , O , Θ ,ρ ⟩ be two Ts’s, to which
S	S	S	S	S	T	T	T	T	T
we refer as the source and target Ts’s, respectively. Such two systems are called comparable if there exists a one-to-one correspondence between the observables of PS and those of PT . To simplify the notation, we denote by X ∈ OS and x ∈ OT the corresponding observables in the two systems. A source state s is defined to be compatible with the target state t, if s and t agree on their observable parts. That is, s[X]= t[x] for every x ∈ OT . We say that PT is a correct translation (reﬁnement) of PS if they are comparable and, for every σT : t0, t1,... a computation of PT and every σS : s0, s1,... a computation of PS such that s0 is compatible with t0, σT is terminating (finite) iff σS is and, in the case of termination, their final states are compatible.
Our goal is to provide an automated method that will establish (or refute) that a given target code correctly implements a given source code, where both are expressed as Ts’s.
Assume PS and PT are source and target programs given in some intermediate language, that we wish to show are equivalent. Furthermore, we assume that both PS and PT are in SSA-form ([6]), which allow for more powerful data mappings and invariant generation than those allowed by the non-SSA-formed programs ([5,4] 5 ). The translation of the codes into Ts’s is straightforward. We therefore assume that both programs are Ts’s. Hence we refer to both IR programs and their Ts’s by the same name. For each of the programs, we compute a cutpoint set, i.e., a set of control points that includes the entry point, exit point, and at least one control
point of each loop. The source cutpoint set is denoted by CP , and the target’s is
denoted by CP . A simple path is a path between control cutpoints that has no

4  |σ|, the length of σ, is the number of states in σ. When σ is infinite, its length is ω.
5 See also ipf-orc.sourceforge.net/ORC-documentation.htm .



Fig. 1. The Proof Rule Val
intermediate cutpoints.
The proof rule Val, presented in Fig. 1, describes how to establish that PT is a correct translation of PS . The rule calls for computing a control mapping from target cutpoints into source cutpoints, and a data mapping that maps (some) source variables into expressions over the target variables. In addition, invariants are computed at each control points of both PS and PT . The invariants allows to carry information between basic blocks and give Val extra edge, missing from other validation techniques. The proof rule presented here is a variant of some predecessors (see, e.g., [15]).
In Fig. 1, upper cases letters are used to denote variables in the PS , and lower case letters are used to denote variables in PT .
For each cutpoint i and cutpoint j, ρij describes the generalized transition rela- tion between i and j, i.e., an assertion of the type τ (V, V ') where V is the variables before the transition and V ' is the variables after the transition. For example, con- sider the example on the left hand-side of Fig. 2. There, for the generalized transi- tion from location 1 to location 2 we have: ρ12 : (PC = 1) ∧ (PC' = 2) ∧ ((B1 =

1 ∧ N ' = 500) ∨ (B1 /=1 ∧ N ' = 0)) ∧ pres (V
— {N1, N2, PC}). The first two

1	2	S
conjuncts refer to the value of the program counter, that is 1 before, and 2 after,
the transition. The second conjunct describes the effects of the two paths that can occur. The third conjunct specifies the transition preserves the values of all source variables but for PC, N1 and N2.

The invariants ϕ(i) are “program annotations” that are computed by the trans- lation validator. Their role is to carry information in between cutpoints. Their invariance is proved in step (5).
The main differences between this version of Val and previous versions of it are:
In previous versions of Val, only target invariants are mentioned. However, in the tool Tvoc, both source and target invariants are used. From a theoretical point of view, nothing is gained by adding source invariants since target in- variants together with the data mapping α can capture the source invariants. From a practical point of view, however, it is simpler to compute the source invariants directly, which is explicit here.
The data abstraction α in previous versions of the rule includes guards that re- fer to target variables. Here we choose a simpler form that does not include the guards, but that establishes a separate data mapping for each target cutpoint (to accommodate code motion), each may refer only to a subset of the source variables. The form here is weaker, since it (implicitly) allows only for target control variable to be the guard, and not any predicate over target variables. However, the implementation of Tvoc computes only data abstraction with the the target control variable as the guard, thus we choose here a version that corresponds to the tool.
The existential quantification in the right-hand-side of the verification condi- tion Cij is discussed in [14]. As pointed out in [15], this existential conjunct can be eliminated by including, in the left-hand-side of the implication, the
conjunct
	ρS
π∈{simple paths from κ(i)}
S
where ρπ is the transition relation corresponding to the path π, thus replacing
the existential quantifier with a finite (and small) conjunction.

Applying Val: An Example
In order to apply Val, the translation validation tool should compute the cutpoint sets, control mapping, simple paths, generalized transition relations (for the simple paths), invariants, and data abstraction. Armed with the above, the tool can then construct the VCs (in step (5) of Val) and send them to a theorem prover. We keep the computation of the cutpoint sets, control abstraction, simple paths, as in the current Tvoc-sp (see [14,3]). There, we identify each cutpoint with the first location of the basic block that is included in the cutpoint set. To accommodate the SSA form, we identify each cutpoint with the first location after the φ-assignments (of the SSA form) of the basic block that is included in the cutpoint set. In Section 3 we describe the new computation of invariants, and in Section 4 we describe the new computation of the data abstraction mapping (which uses the computed invariants). In this section we demonstrate, by a simple example, how Tvoc-sp computes the other ingredients of Val, and why it would fail to compute good invariants.

Consider the example in Fig. 2 which describes a sparse conditional constant propagation.


5:	W2 ← W3 +2 ∗ Y3 + 3;
Y2 ← Y3 + 1;
6:	goto 3;
7:	return Y3;
8:

14:
return y1;

Target

Source
Fig. 2. An example of source and target

With the above modification to accommodate the SSA from, Tvoc-sp computes:

CP
T
, the target cutpoint set, consists of locations: 9, which is the first location

in the initial block, 14, which is the terminal location, and 11, which is the first
non-φ location in the basic block of the loop’s body (that consists of locations 10, 11 and 12). Similarly, CP , the target cutpoint set, consists of locations {0, 5, 8}.
The control mapping κ is {9 '→ 0, 11 '→ 5, 14 '→ 8};
The simple paths in the source are:
{0 → 5, 5 → 5, 5 → 8, 0 → 8}
and the simple paths in the target are:
{9 → 11, 11 → 11, 11 → 14}
The transition relation for the source path [5 → 5] in PS is:

(PC = 5) ∧ (PC' = 5) ∧ (W ' = W3 + Y2 · 2+ 3) ∧ (Y ' = Y ' + 1)
2	2	3
∧ (W ' = W ') ∧ (Y ' = Y ') ∧ (W ' ≤ N3)
3	2	3	2	3
∧ pres (VS − {PC, W2, W3, Y2, Y3})

where pres(V ) is an abbreviation of   (v' = v)
v∈V
Tvoc-sp computes invariants by fixed-point computations depending on reach- ing definitions. For this code, Tvoc-sp cannot establish valid verification conditions since it cannot generate the invariant N3 = 500 at location 5. The invariant com- putation presented in the next section can trivially detect this invariant.

Generating Invariants
Invariant generation is the most challenging task in the application of Val. In this section we describe a new invariant generation method, that operates on IR programs in SSA form. Since the data-flow based method currently implemented in Tvoc is purely syntactic, and the new method is also semantic, we believe it to be both more efficient and to generate more information than the data-flow based method.
Intuitively, the role of invariants in Val is to carry information between basic blocks. For a program in SSA form, such a task can be performed by collecting the definitions, as well as the branching conditions, that reach certain program points, which can later be fed into a theorem prover with some arithmatics capabilities to reveal “hidden” information.
Consider the SSA program in Fig. 2 (a). Since the branch condition of the if- statement at L1 always evaluates to true, N3 evaluates to a constant value 500 at L3. Rather than attempting to directly detect (N3 = 500) as an invariant at L3, we backtrack from L3 to collect data flow information that the definition of N3 depends on. N3 is either assigned to the value of N1 or the value of N2 depending on the branch condition (B1 = 1), thus we obtain
(N3 = N1 ∧ N1 = 500 ∧ B1 = 1) ∨ (N3 = N2 ∧ N2 =0 ∧ ¬(B1 = 1))	(1)
as an invariant at L3. Besides, since L0 dominates L3 and the program is in SSA form, the definition at L0 holds at L3. Thus, we obtain
(B1 = 1) ∧ (W1 = 1) ∧ (Y1 = 0)	(2)
as invariant at L3. The conjunction of Eq. 1 and Eq. 2 implies the desired (N3 = 500).

Computing Invariants for programs in SSA Form
Assume a control flow graph (cfg) G of an SSA-formed program whose loops are all natural – strongly connected components with single entry locations. (If some of G’s loops are not natural, node splitting can be used to transform the offensive loops.) We denote the entry node of a loop as a loop header. We assume that each loop header has a single incoming edge from outside of the loop. (If this is not the case, we introduce preheaders – new (initially empty) blocks placed just before the

header of a loop, such that all the edges that had previously led into the header from outside the loop lead into the preheader, and there is a single new edge from the preheader into the header.)
Thus, we assume that G is a cfg with each node being a basic block. Since all of G’s loops are natural, G’s edges can be partitioned into backward edges and forward edges. The nodes of G and the forward edges define a directed acyclic graph (dag) that induces a partial order among the nodes.
Following the standard notation, given two nodes x, y ∈ G, we say that x dom- inates y if every path from the entry of G into y passes through x. We say that x is an immediate dominator of y if every dominator of y is either x or else is strictly dominated by x. The immediate dominator of a node y is denoted by idom(y). For a node y ∈ G, we denote by Gidom(y) the graph obtained from G by removing all edges that lead into idom(y), and then removing all the nodes and edges that do no reach y.
For a node x ∈ G, let assign(x) be the set of non φ-assignments in x. If x is not a loop header, we define:
gen(x) =		(v = exp).
(v:=exp)∈assign(x)
The expression gen(x) describes the invariants generated by x regardless of its environment.
For a node x with an immediate successor y, we denote by cond(x, y) the con- dition under which the control transfers from x into y.
Let x ∈ G be a node that is not a loop header. Assume that x has mx prede-
cessors x' ,... , x'	. Let Vx denote the set of variables defined by φ-functions in x.
1	mx
Assume that for every v ∈ Vx, the definition of v in x by the φ-function is


vtx (v) ← φ(vtx (v),... , vtx
(v)).

0	1	mx
Let x ∈ G be now a node which is a loop header. Obviously, if x is reached through a back edge, one must consider, in addition to the definitions of the in- duction variables as expressed after the φ-functions, the information about their

updates in the loop’s body. If vx,... , vx
are the basic induction variables of the

i	K
loop whose header is x, where each induction variable vx is initialized to bi before
entering the loop, and is incremented by ci at each iteration, we define:

induc(x) = ∃vˆx ≥ 0.  (vx = bi + ci × vˆx)	(3)
i=1
where vˆx is a new variable. I.e., vˆx is a loop iteration count, and induc(x) captures the values of the induction variables at some (possibly the 0th) iteration of the loop. We shall return to the issue of dealing with (i.e., eliminating) the existential variables in Section 3.2.
In Fig. 3 we describe data flow equations to compute the assertions in(x) and

out(x) for every node x ∈ G. The former is an invariant at the beginning of x, after all the φ-functions, and the latter is an invariant at the end of x. The invariants in(x) and out(x) can be computed for every x ∈ G simultaneously by forward traversal of the dag induced by the forward edges of G.
⎧⎪ out(idom (x), G) ∧
cond(idom(x), x) ∧ induc(x)	if x is a loop header,
⎪


in(x, G)	=
⎪⎪⎨ out(idom (x), G) ∧

⎪	 m ⎛
out(x' , Gidom(x) ∧ cond(x' , x) ⎞

x
i=1
⎪	∧
i
(v x
= v x	)	⎠

⎧⎪⎩
v∈Vx
t0 (v)
ti (v)
otherwise

out(x, G) =
⎨ in(x, G) ∧ gen(x) if x ∈ G,
⎩ true	otherwise.

Fig. 3. Data-flow equations for in(x, G) and out(x, G)

Theorem 3.1 The data-flow equations computed in Fig. 3 are sound, i.e., during an execution of a program, for every basic block B represented by node x in the cfg G, when the program reaches B (after the φ-functions), in(x, G) holds, and whenever the program exits B, out(x, G) holds.
Proof Outline:   The proof is by induction on the breadth first search of the
G. The base case is for the entry node(s). Then, we have the data-flow equation for out, which is trivially true. For the inductive step, we distinguish between loop headers and non-loop headers. Suppose that x is a loop header. Since we assume that all loops are natural, the control reaches a loop header either from its immediate dominator or from a back edge. Since we assume SSA form, we have that all invariants at the end of the immediate dominator, as well as the condition leading to the loop, hold. If this is the first entry to the loop, then induct(x) holds with the trivial vˆx = 0. If the loop header is reached by a back edge, then induct(x) holds with the trivial vˆx > 0. By the induction hypothesis, the out(idom(x), G) is sound. We can therefore conclude that in(x, G) is sound.
If x is not a loop header, then x is reached through one of its predecessors, x', with cond(x' , x) holding. Thus, the soundness of in(x, G) follows immediately from the induction hypothesis.
Finally, whether x is or is not a loop header, gen(x) holds after block x is executed. Therefore, out(x, G) is a conjunction of in(x, G) with gen(x). Since we assume that in(x, G) is sound, the soundness of out(x, G) follows.	 It thus follows from Theorem 3.1 that for every initial location i of the cfg
corresponding to G, we can take ϕ(i): in(x, G) as the invariant at i.

Note that in(x, G) and out(x, G) are mutually recursive functions over the struc- ture of the dag induced by G. Hence, no fix-point computation is necessary to solve the equations. As a matter of fact, each node and edge of x’s ancestor in G is visited only once in the computation of in(x, G) and out(x, G), thus the complexity of the computation is linear to the size of G.

Quantiﬁer Elimination
Since the VCs generated by Val have invariants on both sides of implications, and since most theorem provers do not accept existential formulae as consequents, we need to eliminate such quantification in invariants. We accomplish this by instan- tiating the offensive existential quantifiers. In this discussion, we consider only the case of target invariants. The case of source invariants is similar.
Suppose a VC Cij that has a ∃vˆx on the right-hand-side. From the invariant generation algorithm, it follows that x is a loop header. Let “the loop” mean “the loop whose header is x” for this discussion. Assume that on the left-hand-side of Cij has the invariant ϕT (i). We distinguish between the following cases:
j is the loop’s header, and i is outside the loop. Thus, the simple path be- tween i and j is one that corresponds to the first entry into the loop. In this case, we can instantiate vˆx to 0, and replace the existential part of ϕ' (j) with

  (vi = bi).
i=1
j is the loop header and i is in the loop. Thus, the simple path between i and j corresponds to a back edge of G. We can then “reuse” the value of vˆx from the antecedent and replace the existential part of ϕ' (j) with

  (vi = bi + ci × (vˆx + 1))
i=1
Neither of the previous cases. Thus, the simple path between i and j does not alter the values of the induction variables, and we can “reuse” the value of vˆx from the antecedent, thus replacing the existential part of ϕ' (j) with

  (vi = bi + ci × vˆx)
i=1

Example: Invariant Generation
To apply the method described above to the target program in our running example, we define basic blocks B1 = {L9}, B2 = {L10, L11, L12}, B3 = {L13}, for which we

have:
gen(B1)	: (t1 = 0) ∧ (w1 = 1)
gen(B2)	: (w2 = w3 + t3 + 3) ∧ (t2 = t3 + 2)
cond(B1, B2) : true
cond(B2, B3) : ¬(w2 ≤ 500)
The program has a loop {B2} in which there is one induction variable, t3, which is initialized to t1 before the loop and is incremented by 2 at each iteration.
We therefore have:
induct(B2) = ∃vˆT .(t3 = t1 + 2vˆT ) Solving the equations of Fig. 3, we obtain:
in(B1) : true
out(B1) : (t1 = 0) ∧ (w1 = 1)
in(B2) : (t1 = 0) ∧ (w1 = 1) ∧ ∃vˆT : (t3 = t1 + 2vˆT )
out(B2) : (t1 = 0) ∧ (w1 = 1) ∧ ∃vˆT : (t3 = t1 + 2vˆT )
∧ (w2 = w3 + t3 + 3) ∧ (t2 = t3 + 2)
in(B3) : (t1 = 0) ∧ (w1 = 1) ∧ ∃vˆT : (t3 = t1 + 2vˆT )
∧ (w2 = w3 + t3 + 3) ∧ (t2 = t3 + 2) ∧ ¬(w2 ≤ 500) We can therefore conclude that:
ϕT (9)  : true
ϕT (11) : (t1 = 0) ∧ (w1 = 1) ∧ ∃vˆT : (t3 = t1 + 2vˆT )
Similarly, for the source program in our running example, we can compute that:
ϕS (0) : true
ϕS (5) : (N3 = 500) ∧ (Y1 = 0) ∧ (W1 = 1)
∧ ∃vˆS : (Y3 = Y1 + vˆS ) ∧ (W3 ≤ N3)
Since vˆS is the iteration counter of the source loop, and vˆT is its counterpart in the target, we can safely include (vˆS = vˆT ) in the antecedent of both verification conditions C11,11 and C11,14, which allows us to relate loop induction variables in source and target. Together with ϕS (5) and ϕT (11), we obtain that t3 = 2 · Y3 holds at target cutpoint L11 (and its corresponding source cutpoint L5), implying the equality between observable variables Y3 and y1 upon termination.

Computing Data Abstraction
The data abstraction α maps, for each target cutpoint, a conjunction of equalities. Each equality is of the form U = EU , where U is a source variables and EU is an expression over target variables. The meaning of including the conjunct U = EU in α(i) is that when the control of the target is in location i, and, consequently, the control of the source is in κ(i), the value of U in k(i) is the value of EU computed at the target. The more precise the invariants are, the more precise α can be. In this section we describe how to compute α given the computation of the ϕ’s described above.
The algorithm described here assumes that some initial set of equalities of the type U = EU are given. In practice, such an initial set can be obtained from the symbol table of the compiler if one can access it (which one can, in the case of ORC), or by “reasonable” guesses, .e.g, by assuming that variables with the same name in source and target are equal. The correctness of the algorithm does not depend on the choice of the initial set, in particular, the initial set may include wrong equalities. Its ability to generate precise data abstraction, however, may be impacted by the initial set. We denote this initial set by Γ.
The algorithm is presented in Fig. 4. For each cutpoint i, the algorithm employs an auxiliary γ(i) which is the set of equalities (of the from U = EU ). At each step, γ(i) is the set of equalities that are assumed to hold whenever target is at location
i. Initially, γ(i) is Γ itself. At each iteration, until γ(i) is stabilizes, equalities that violate the invariants and transition relations are removed. At each step, α(i) is the conjunction of the equalities in γ(i). An equality U = EU is removed from γ(i) if, for some simple path leading from j into i,

T	S	'	'
ϕT (j) ∧ ϕS (κ(j)) ∧ α(j) ∧ ρπ  ∧ ρκ(j)κ(i)	/→	U = (EU )
where (EU )' is the evaluation of EU after the transition, i.e., EU where every variable is replaced by its primed version.
Note that while we choose the target transition function that corresponds to the target path π, we take the matching source path to be any path in between the two matching π-endpoints of the source. Thus, if there are several matching source paths, we take the disjunction of their respective transition relations.
The procedure can be viewed as an iterative forward data-flow analysis, oper- ating on the lattice (2Γ, ⊆). The flow function of γ can be solved by starting with Γ and descending values in the lattice at each iteration, until a fixpoint is reached. The validity of the logical formula in the flow equation of γ is decided by using a theorem prover. Unlike iterative data flow analyses used in compiler optimizations, this procedure applies a joint analysis on source and target programs.
In our running example, we obtain in the data abstraction, e.g.,
α(9) : (W0 = w0) ∧ (Y0 = y0)
α(11) : (W3 = w3)
α(14) : (W3 = w2) ∧ (Y3 = y1)


for each target cutpoint i
γ (i) := Γ
α(i)=  E∈γ(i){E}
repeat
for each target cutpoint i
for every simple path π leading into i j := start point of π
γ (i) :=
T	S	'
{E ∈ γ(i): ϕT (j) ∧ ϕS (κ(j)) ∧ α(j) ∧ ρπ ∧ ρκ(j)κ(i) → E }
α(i)=  E∈γ(i) E
until sets stabilize

Fig. 4. Computation of Data Abstraction
Note that had we started the algorithm with Γ containing only simple equalities of the form (U = u), more complex data mappings would be captured with the aid of the source invariants. E.g., to express α(i): (X = x) ∧ (Y = y) ∧ (Z = x + y), it suffices to obtain α(i): (X = x) ∧ (Y = y) and ϕS (κ(i)) : (Z = X + Y ).

Discussion and Conclusion
In this paper we propose a method based on SSA-from to compute invariants and data mappings. The method allows for translation validation of programs that are

beyond the current power of
Tvoc. The translation to SSA-form and the new

algorithms proposed here were implemented and tested.
B0:	a1 ← 1

B0:	a1 ← 3
b1 ← 1
B1:	a3 ← φ(a1, a2)

d1 ← 2
B1:	d3 ← φ(d1, d2)
b3 ← φ(b1, b2)

a3 ← φ(a1, a2)
if (a3 mod 2= 0) goto B3
B2:	a4 ← a3 +1 

f1 ← a3 + d3
g1 ← 5
b4 ← b3 +1 




B2:
a2 ← g1 − d3 d2 ← 2
if (f1 ≤ g1) goto B1
goto B4 B3:	a5 ← a3 +3 
b5 ← b3 +3 
B4:	a2 ← φ(a4, a5)

(a)
b2 ← φ(b4, b5)
if (b2 < n0) goto B1
(b)
Fig. 5. Examples where our invariant generation fails

Yet there are some optimizations for which we still fail to generate good invari- ants. Notable examples are optimizations that include sparse conditional constant folding and global value numbering: Consider the programs in Fig. 5. Clearly, in the code in (a), ai = 3 and di = 2 are invariant in all cutpoints where ai and di are defined. While the sparse conditional constant propagation algorithm of [13]

succeeds in determining this, our method, that does not propagate information it- eratively, cannot. Similarly, in the code in (b), ai = bi is invariant in all cutpoints where ai and bi are defined. While the value numbering algorithm of [1] succeeds in determining this, our method, being unable to proceed around loops, cannot.
Our technique may also be useful in proving properties of microcode. E.g., [2] describes a tool for proving backward compatibility of microcode. The microcode there is assumed not to contain loops. Our technique may allow for extending the work to microcode programs with loops. (In fact, our technique orignated with the work on the project reported on in [2].) In addition, we expect the techniques to be applicable to property verification of microcode programs.

References
B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality of variables in programs. In POPL ’88: Proceedings of the 15th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 1–11, New York, NY, USA, 1988. ACM Press.
T. Arons, E. .Elster, L. Fix, S. Mador-Haim, M. Mishaeli, J. Shalev, E. Singerman, A. Tiemeyer, M. Y. Vardi, and L. D. Zuck. Formal verification of backward compatibility of microcode. In Proceedings of 17th International Conference on Computer Aided Verification (CAV 2005), number 3576 in Lncs, pages 185–200. Springer, 2005.
C. W. Barrett, Y. Fang, B. Goldberg, Y. Hu, A. Pnueli, and L. D. Zuck. Tvoc: A translation validator for optimizing compilers. In Proceedings of 17th International Conference on Computer Aided Verification (CAV 2005), Lncs, pages 291–295. Springer, 2005.
S. Chan, R. Ju, and C. Wu. Tutorial: Open research compiler (orc) for the itanium processor family. In Micro 34, 2001.
R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. An efficient method of computing static single assignment form. In POPL ’89: Proceedings of the 16th ACM SIGPLAN- SIGACT symposium on Principles of programming languages, pages 25–35, New York, NY, USA, 1989. ACM Press.
R. Cytron, Ronald, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Efficiently computing static single assignment form and the program dependence graph. In ACM TOPLAS, volume 13, pages 451–490, New York, NY, USA, 1991. ACM Press.
Y.  Fang.   Translation  Validation  of  Optimizing  Compilers.   PhD  thesis,  NYU,
http://cs.nyu.edu/web/Research/theses.html , 2005.
G. Necula. Translation validation of an optimizing compiler. In Proceedings of the ACM SIGPLAN Conference on Principles of Programming Languages Design and Implementation (PLDI) 2000, pages 83–95, 2000.
A. Pnueli, M. Siegel, and O. Shtrichman. The code validation tool (CVT) - automatic verification of a compilation process. Software Tools for Technology Transfer, 2, 1998.
A. Pnueli, M. Siegel, and E. Singerman. Translation validation. In Proc. 4th Intl. Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS’98), pages 151–166, 1998.
M. Rinard and D. Marinov. Credible compilation with pointers. In Proceedings of the Run-Time Result Verification Workshop, Trento, July 2000.
Silicon   Graphics.	Whirl   Intermediate   Language   Specification.
www.cs.ualberta.ca/∼amaral/courses/680/orc/whirl.pdf  .
M. N. Wegman and F. K. Zadeck. Constant propagation with conditional branches. ACM Trans. Program. Lang. Syst., 13(2):181–210, 1991.
L. Zuck, A. Pnueli, Y. Fang, and B. Goldberg. Voc: A methodology for the translation validation of optimizing compilers. Jounal of Universal Computer Science, 9, 2003.
L. Zuck, A. Pnueli, B. Goldberg, C. Barrett, Y. Fang, and Y. Hu. Translation and run-time validation of loop transformations. Journal on Formal Methods in System Design, 27(3), 11 2005. Preliminary version appeared in Proceedings of the Run-Time Result Verification Workshop, Electronic Notes in Theoretical Computer Science (ENTCS), 2002, 70(4).
