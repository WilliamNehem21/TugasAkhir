	Electronic Notes in Theoretical Computer Science 177 (2007) 123–136	
www.elsevier.com/locate/entcs

Static Slicing of Rewrite Systems1
Diego Cheda2	Josep Silva2	Germa´n Vidal2
DSIC, Technical University of Valencia Camino de Vera S/N, 46022 Valencia, Spain

Abstract
Program slicing is a method for decomposing programs by analyzing their data and control flow. Slicing- based techniques have many applications in the field of software engineering (like program debugging, testing, code reuse, maintenance, etc). Slicing has been widely studied within the imperative programming paradigm, where it is often based on the so called program dependence graph, a data structure that makes explicit both the data and control dependences for each operation in a program. Unfortunately, the notion of “dependence” cannot be easily adapted to a functional context. In this work, we define a novel approach to static slicing (i.e., independent of a particular input data) for first-order functional programs which
are represented by means of rewrite systems. For this purpose, we introduce an appropriate notion of dependence that can be used for computing program slices. Also, since the notion of static slice is generally undecidable, we introduce a complete approximation for computing static slices which is based on the construction of a term dependence graph, the counterpart of program dependence graphs.
Keywords: Program slicing, rewrite systems


Introduction
Program slicing [13] is a method for decomposing programs by analyzing their data and control flow. Roughly speaking, a program slice consists of those program state- ments which are (potentially) related with the values computed at some program point and/or variable, referred to as a slicing criterion. In imperative programming, slicing criteria are usually given by a pair (program line, variable).
Example 1.1 Consider the program in Figure 1 to compute the number of charac- ters and lines of a text. A slice of this program w.r.t. the slicing criterion (12, chars ) would contain the black sentences (while the gray sentences are discarded). This slice contains all those parts of the program which are necessary to compute the value of variable chars at line 12.

1 This work has been partially supported by the EU (FEDER) and the Spanish MEC under grant TIN2005- 09207-C03-02, by the ICT for EU-India Cross-Cultural Dissemination Project ALA/95/23/2003/077-054, by LERNet AML/19.0902/97/0666/II-0472-FA and by the Vicerrectorado de Innovacio´n y Desarrollo de la UPV under project TAMAT ref. 5771.
2 Email: {dcheda,jsilva,gvidal}@dsic.upv.es

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.01.010


then lines := lines + 1;
chars := chars + 1;
else chars := chars + 1;
i = i + 1;
return lines;
return chars;
Fig. 1. Example program lineCharCount

Fig. 2. Control flow graph of lineCharCount

In order to extract a slice from a program, the dependences between its statements must be computed first. The control flow graph (CFG) is a data structure which makes the control dependences for each operation in a program explicit. For in- stance, the CFG of the program in Fig. 1 is depicted in Fig. 2.
However, the CFG does not generally suffice for computing program slices be- cause it only stores control dependences and, for many applications (such as debug- ging), data dependences are also necessary. For this reason, in imperative program- ming, program slices are usually computed from a program dependence graph (PDG) [4,6] that makes explicit both the data and control dependences for each operation in a program. A PDG is an oriented graph where the nodes represent statements in the source code and the edges represent data and control dependences. As an example, the PDG of the program in Fig. 1 is depicted in Fig. 3 where solid arrows represent control dependences and dotted arrows represent data dependences.
Program dependences can be traversed backwards or forwards (from the slicing criterion), which is known as backward or forward slicing, respectively. Additionally, slices can be dynamic or static, depending on whether a concrete program’s input is provided or not. A complete survey on slicing can be found, e.g., in [12].
While PDGs are good to represent the data and control flow behavior of imper- ative programs, their level of granularity (i.e., considering all function bodies as a whole) is not appropriate for representing dependences in functional programming. In this work, we present a new notion of dependence in term rewriting that can be used to give an appropriate definition of static slicing. Unfortunately, the




Fig. 3. Program dependence graph of lineCharCount

computation of static slices is generally undecidable—since one should consider all possible computations for a given program—and, thus, we also introduce a complete algorithm to compute static slices which is based on the construction of term depen- dence graphs, a new formalism to represent both data and control flow dependences of first-order functional programs denoted by term rewriting systems.
The rest of the paper is organized as follows. In the next section, we recall some notions on term rewriting that will be used throughout the paper. Then, in Section 3, we present our approach to static slicing within a functional context. Section 4 introduces a new data structure, called term dependence graph, than can be used to compute static slices. Finally, Section 5 discusses some related work and concludes.

Preliminaries
For completeness, here we recall some basic notions of term rewriting. We refer the reader to [3] for details.
A set of rewrite rules (or oriented equations) l → r such that l is a nonvariable term and r is a term whose variables appear in l is called a term rewriting system (TRS for short); terms l and r are called the left-hand side and the right-hand side of the rule, respectively. We assume in the following that all rules are numbered uniquely. Given a TRS R over a signature F, the deﬁned symbols D are the root symbols of the left-hand sides of the rules and the constructors are C = F \ D. We restrict ourselves to finite signatures and TRSs. We denote the domain of terms and constructor terms by T (F, V) and T (C, V), respectively, where V is a set of variables with F ∩ V = ∅.
A TRS R is constructor-based if the left-hand side of its rules have the form f (s1,... , sn) where si are constructor terms, i.e., si ∈ T (C, V), for all i = 1,... , n. The root symbol of a term t is denoted by root (t). A term t is operation-rooted (resp. constructor-rooted ) if root (t) ∈ D (resp. root (t) ∈ C). The set of variables appearing in a term t is denoted by Var(t). A term t is linear if every variable of V occurs at most once in t. R is left-linear (resp. right-linear) if l (resp. r) is linear for all rules l → r ∈ R. In this paper, we restrict ourselves to left-linear constructor-based TRSs, which we often call programs.

As it is common practice, a position p in a term t is represented by a sequence of natural numbers, where Λ denotes the root position. Positions are used to address the nodes of a term viewed as a tree: t|p denotes the subterm of t at position p and t[s]p denotes the result of replacing the subterm t|p by the term s. A term t is ground if Var(t)= ∅. A substitution σ is a mapping from variables to terms such that its domain Dom(σ)= {x ∈ V | x /= σ(x)} is finite. The identity substitution is denoted by id. Term t' is an instance of term t if there is a substitution σ with t' = σ(t). A uniﬁer of two terms s and t is a substitution σ with σ(s) = σ(t). In the following, we write on for the sequence of objects o1,... , on.
A rewrite step is an application of a rewrite rule to a term, i.e., t →p,R s if there exists a position p in t, a rewrite rule R = (l → r) and a substitution σ with t|p = σ(l), s = t[σ(r)]p (p and R will often be omitted in the notation of a reduction step). The instantiated left-hand side σ(l) is called a redex. A term t is called irreducible or in normal form if there is no term s with t → s. We denote by →+ the transitive closure of → and by →∗ its reflexive and transitive closure. Given a TRS R and a term t, we say that t evaluates to s iff t →∗ s and s is in normal form.

Static Slicing of Rewrite Systems
In this section, we introduce our notion of dependence in term rewriting, which is then used to give an appropriate definition of static slicing. First, we define the program position of a term, which uniquely determines its location in the program.
Definition 3.1 (position, program position) Positions are represented by a se- quence of natural numbers, where Λ denotes the empty sequence (i.e., the root po- sition). They are used to address subterms of a term viewed as a tree:
t|Λ = t for all term t ∈ T (F, V)	d(tn)|i.w = ti|w  if i ∈ {1,... , n}, d/n ∈ F 

A program position is a pair (k, w) that addresses the (possibly variable) subterm r|w in the right-hand side r of the k-th rule l → r of P. Given a program P, Pos(P) denotes the set of all program positions of the terms in the right-hand sides of P.
Definition 3.2 (labeled term) Given a program P, a labeled term is a term in which each (constructor or deﬁned) function or variable symbol is labeled with a set of program positions from Pos(P). The domain of labeled terms can be inductively deﬁned as follows:
aP is a labeled term, with a ∈ F ∪ V and P ⊆ Pos(P);
if d/n ∈ F, P ⊆ Pos(P) and t1,... , tn are labeled terms, then dP (t1,... , tn) is also a labeled term.
In the remainder of this paper, we assume the following considerations:
The right-hand sides of program rules are labeled.
Labels do not interfere with the standard definitions of pattern matching, in- stance, substitution, rewrite step, derivation, etc (i.e., labels are ignored).

The application of a (labeled) substitution σ to a term t is redefined so that, for each binding x '→ dP0 (tP1 ,... , tPn ) of σ, if variable xP occurs in t, then it is
1	n
replaced by dP ∪P0 (tP1 ,... , tPn ) in σ(t).
1	n
The next example shows why we need to associate a set of program positions with every term and not just a single position:
Example 3.3 Consider the following labeled program: 3
(R1)	main → g{(R1,Λ)}(f{(R1,1)} (Z{(R1,1.1)} )) (R2)	g(x) → x{(R2,Λ)}
(R3)	f(x) → x{(R3,Λ)}
together with the derivation:
main{} →Λ,R1 g{(R1,Λ)}(f{(R1,1)} (Z{(R1,1.1)} ))

→1,R3  g{(R1,Λ)}(Z{(R3,Λ),(R1,1.1)} )
→Λ,R2 Z{(R2,Λ),(R3,Λ),(R1,1.1)}

Here, one may argue that the list of program positions of Z in the ﬁnal term of the derivation should only contain the pair (R1, 1.1) (i.e., the only occurrence of Z in the right-hand side of the ﬁrst rule). However, for program slicing, it is also relevant to know that Z has been propagated through the variable x that appears in the right-hand sides of both the second and third rules.
Observe that main is labeled with an empty set of program positions since it does not appear in the right-hand side of any program rule.
Before introducing our notion of “dependence”, we need the following definition. 4 Here, we let “•” be a fresh constructor symbol not occurring in F (which is not labeled). We use this symbol to denote a missing subterm.
Definition 3.4 (subreduction) Let D : t0 →p1,R1 ... →pn,Rn tn be a derivation

for t0 in a program P, with tn ∈ T (C, V). We say that D' : t'
→p' ,R'
... →p' ,R'
t' ,

0	1  1
m ≤ n, is a subreduction of D if the following conditions hold:
m  m  m

t'
t'
= t0[•]p for some position p, is a normal form, and

all the elements in the sequence (p' , R' ),... , (p' , R'
) also appear in the se-

1	1	m	m
quence (p1, R1),... , (pn, Rn) and in the same order.
Roughly, we say that a derivation is a subreduction of another derivation if
the initial terms are equal except possibly for some missing subterm,

3 In the examples, data constructor symbols start with uppercase letters while defined functions and vari- ables start with lowercase letters. Furthermore, we underline the selected redex at each reduction step.
4 A similar, though more general, notion of subreduction can be found in [5].

both derivations end with a normal form (i.e., an irreducible term), and
the same steps, and in the same order, are performed in both derivations except for some steps that cannot be performed because of the missing subterm in the initial term (and its descendants).
Example 3.5 Consider the following labeled program:
(R1)	main → C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), g{(R1,2)} (B{(R1,2.1)} )) (R2)	f(A) → D{(R2,Λ)}
(R3)	g(x) → x{(R3,Λ)}
together with the derivation
D : C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), g{(R1,2)} (B{(R1,2.1)} ))
→1,R2	C{(R1,Λ)} (D{(R2,Λ)}, g{(R1,2)} (B{(R1,2.1)} ))

→2,R3	C{(R1,Λ)} (D{(R2,Λ)}, B{(R3,Λ),(R1,2.1)} )

Then, for instance, the following derivations:
D1 : C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), g{(R1,2)} (•))

→1,R2	C{(R1,Λ)} (D{(R2,Λ)}, g{(R1,2)} (•))
→2,R3	C{(R1,Λ)} (D{(R2,Λ)}, •)

D2 : C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), •)

→1,R2	C{(R1,Λ)} (D{(R2,Λ)}, •)

D3 : C{(R1,Λ)} (f{(R1,1)} (•), g{(R1,2)} (B{(R1,2.1)} ))

→2,R3	C{(R1,Λ)} (f{(R1,1)} (•), B{(R3,Λ),(R1,2.1)} )

are subreductions of D.
Now, we can introduce our notion of dependence in term rewriting. Informally speaking, given a function call f (vn) that evaluates to a constructor term v, we say that a subterm of v, say s2, depends on a term s1 (that appears in the program) if
there exists a derivation of the form f (vn) →∗ t →∗ v where s1 is a subterm of t
and
if s1 is replaced in t by the fresh symbol •, then the root symbol of s2 is not computed anymore in the considered value v.
Definition 3.6 (dependence) Given a program P, a constructor term s2 depends on the term s1 w.r.t. function f of P, in symbols s1 ~f s2, iff there exists a

derivation of the form f (vn) →∗ t →∗ v, where vn and v are constructor terms, t|p = s1 and v|q = s2 for some positions p and q, and there is a subreduction t' →∗ v' of a suffix t →∗ v of the derivation f (vn) →∗ t →∗ v with t' = t[•]p such that root (v'|q) /= root (s2).
Example 3.7 Consider again the program of Example 3.5. Here, A ~main D because we have a derivation
D
main → C¸(f(A), g(B)) → Cx(`D, g(B)) → C(D, B˛)
with C(D, B)|1 = D, C(f(A), g(B))|1.1 = A, and in the following subreduction of Ð:
C(f(•), g(B)) → C(f(•), B)
we have root (C(f(•), B)|1)= f /= D.
Note that we are not fixing any particular strategy in the definition of dependence. Our aim is to produce static slices which are independent of any evaluation strategy. Now, we introduce the basic concepts of our approach to static slicing.
For the definition of slicing criterion, we recall the notion of slicing patterns:
Definition 3.8 (slicing pattern [8]) The domain Pat of slicing patterns is de- ﬁned as follows:
π ∈ Pat ::= ⊥ | T | c(πk)
where c/k ∈ C is a constructor symbol of arity k ≥ 0, ⊥ denotes a subexpression of the value whose computation is not relevant and T a subexpression which is relevant.
Slicing patterns are similar to the liveness patterns which are used to perform dead code elimination in [7]. Basically, they are abstract terms that can be used to denote the shape of a constructor term by ignoring part of its term structure. For instance, given the constructor term C(A, B), we can use (among others) the following slicing patterns T, ⊥, C(T, T), C(T, ⊥), C(⊥, T), C(⊥, ⊥), C(A, T), C(A, ⊥), C(T, B), C(⊥, B),
C(A, B), depending on the available information and the relevant fragments of C(A, B). Given a slicing pattern π, the concretization of an abstract term is formal- ized by means of function γ, so that γ(π) returns the set of terms that can be obtained from π by replacing all occurrences of both T and ⊥ by any construc- tor term.  This usually leads to an infinite set, e.g., γ(C(A, T)) = γ(C(A, ⊥)) =
{C(A, A), C(A, B), C(A, D), C(A, C(A, A)), C(A, C(A, B)),.. .}.
Definition 3.9 (slicing criterion) Given a program У, a slicing criterion for У
is a pair (f, π) where f is a function symbol and π is a slicing pattern.
Now, we introduce our notion of static slice. Basically, given a slicing criterion (f, π), a program slice is given by the set of program positions of those terms in the program that affect the computation of the relevant parts—according to π—of the possible values of function f . Formally,
Definition 3.10 (slice) Let У be a program and (f, π) a slicing criterion for У. Let Pπ be the set of positions of π which do not address a symbol ⊥. Then, the slice

of У w.r.t. (f, π) is given by the following set of program positions:




  {(k, w) | (k, w) ∈ P, tP ~f v|q, v ∈ γ(π) and q ∈ Pπ}
Observe that a slice is a subset of the program positions of the original program that uniquely identifies the program (sub)terms that belong to the slice.
Example 3.11 Consider again the program of Example 3.5 and the slicing pattern
(main, C(T, ⊥)).
The concretizations of C(T, ⊥) are γ(C(T, ⊥)) = {C(A, A), C(A, B), C(A, D), C(B, A),
C(B, B), C(B, D), C(D, A), C(D, B), C(D, D), .. .}.
The set of positions of C(T, ⊥) which do not address a symbol ⊥ are Pπ = {Λ, 1}.
Clearly, only the value C(D, B) ∈ γ(C(T, ⊥)) is computable from main.
Therefore, we are interested in the program positions of those terms such that either C(D, B) (i.e., C(D, B)|Λ) or D (i.e., C(D, B)|1) depend on them.
The only computations from main to a concretization of C(T, ⊥) (i.e., to a term from γ(C(T, ⊥))) are thus the following:





Ð1 : main{} →Λ,R1 C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), g{(R1,2)} (B{(R1,2.1)} ))

→1,R2 C{(R1,Λ)} (D{(R2,Λ)} , g{(R1,2)} (B{(R1,2.1)} ))
→2,R3  C{(R1,Λ)} (D{(R2,Λ)} , B{(R3,Λ),(R1,2.1)} )

Ð2 : main{} →Λ,R1 C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), g{(R1,2)} (B{(R1,2.1)} ))

→2,R3  C{(R1,Λ)} (f{(R1,1)} (A{(R1,1.1)} ), B{(R3,Λ),(R1,2.1)} )
→1,R2  C{(R1,Λ)} (D{(R2,Λ)} , B{(R3,Λ),(R1,2.1)} )







In this example, it suffices to consider only one of them, e.g., the ﬁrst one. Now, in order to compute the existing dependences, we show the possible suffixes of Ð1 together with their associated subreductions (here, for clarity, we ignore the

program positions):

Suffix : C(f(A), g(B)) →1,R2 C(D, g(B)) →2,R3 C(D, B)
Subreductions : •
C(f(•), g(B)) →2,R3 C(f(•), B) C(•, g(B))	→2,R3 C(•, B)
C(f(A), g(•)) →1,R2 C(D, g(•)) →2,R3 C(D, •)
C(f(A), •)	→1,R2 C(D, •)

Suffix : C(D, g(B))	→2,R3 C(D, B)
Subreductions : •
C(•, g(B))	→2,R3 C(•, B)
C(D, g(•))	→2,R3 C(D, •)

C(D, •)
Suffix : C(D, B)
Subreductions : •
C(•, B)
C(D, •)

Therefore, we have the following dependences (we only show the program posi- tions of the root symbols, since these are the only relevant program positions for computing the slice):
From the ﬁrst suffix and its subreductions:

C{(R1,Λ)}(f(A), g(B)) ~main C(D, B) A{(R1,1.1)}	~main D
f{(R1,1)} (A)	~main  D


From the second suffix and its subreductions:

C{(R1,Λ)}(D, g(B)) ~main C(D, B)
D{(R2,Λ)}	~main D


	

Fig. 4. Tree terms of f(C(A, B)) and f(C(A, B), g(B, B))


· From the third suffix and its subreductions:

C{(R1,Λ)}(D, B) ~main C(D, B)
D{(R2,Λ)}	~main D


Therefore, the slice of the program w.r.t. (main, C(T, ⊥)) returns the following set of program positions {(R1, Λ), (R1, 1), (R1, 1.1), (R2, Λ)}.
Clearly, the computation of all terms that depend on a given constructor term is undecidable. In the next section, we present a decidable approximation based on the construction of a graph that approximates the computations of a program.


Term Dependence Graphs
In this section, we sketch a new method for approximating the dependences of a pro- gram which is based on the construction of a data structure called term dependence graph. We first introduce some auxiliary definitions.
Definition 4.1 (Tree term) We consider that terms are represented by trees in the usual way. Formally, the tree term T of a term t is a tree with nodes labeled with the symbols of t and directed edges from each symbol to the root symbols of its arguments (if any).
For instance, the tree terms of the terms f(C(A, B)) and f(C(A, B), g(B, B)) are de- picted in Fig. 4.
We introduce two useful functions that manipulate tree terms. First, function Term from nodes to terms is used to extract the term associated to the subtree whose root is the given node of a tree term:
Term(T, n) = ⎧⎨ n 		if n has no children in T
⎩ n(Term(T, nk)) if n has k children nk in T


(R1,/\)	(R2,/\)	(R3,/\)

Fig. 5. Term dependence graph of the program in Example 3.5
Now, function Termabs is analogous to function Term but replaces inner operation- rooted subterms by fresh variables:


Term


abs
(T, n) = ⎧⎨ n 		if n has no children in T

⎩ n(Term'  (T, nk)) if n has k children nk in T


Term'abs(T, n) =  ⎨
where x is a fresh variable

⎪ Termabs(T, n) otherwise
Now, we can finally introduce the main definition of this section.
Definition 4.2 (Term dependence graph) Let У be a program. A term depen- dence graph for У is built as follows:
the tree terms of all left- and right-hand sides of У belong to the term depen- dence graph, where edges in these trees are labeled with S (for Structural);
we add an edge, labeled with C (for Control), from the root symbol of every left-hand side to the root symbol of the corresponding right-hand side;
ﬁnally, we add an edge, labeled with C, from every node n of the tree term Tr of the right-hand side of a rule to the topmost node m of the tree term Tl of a left-hand side of a rule whenever Termabs(Tr, n) and Term(Tl, m) unify.
Intuitively speaking, the term dependence graph stores a path for each possible computation in the program. A similar data structure is introduced in [1], where it is called graph of functional dependencies and is used to detect unsatisfiable computations by narrowing [11].
Example 4.3 The term dependence graph of the program of Example 3.5 is shown in Fig. 5. 5 Here, we depict C arrows as solid arrows and S arrows as dotted arrows. Observe that only the symbols in the right-hand sides of the rules are labeled with program positions.
Clearly, the interest in term dependence graphs is that we can compute a complete
program slice from the term dependence graph of the program. Usually, the slice will

5 For simplicity, we make no distinction between a node and the label of this node.

not be correct since the graph is an approximation of the program computations and, thus, some paths in the graph would not have a counterpart in the actual computations of the program.
Algorithm 1 Given a program У and a slicing criterion (f, π), a slice of У w.r.t.
(f, π) is computed as follows:
First, the term dependence graph of У is computed according to Def. 4.2.
For instance, we start with the term dependence graph of Fig. 5 for the program of Example 3.5.
Then, we identify in the graph the nodes U that correspond to the program po- sitions Pπ of π which do not address the symbol ⊥. For this purpose, we start from a node in the left-hand side of a rule labeled with symbol f and follow its C-path (i.e., a path made of C arrows). If the last node of this path is a constructor constant c and π = c or π = T, then this node belongs to U . Oth- erwise (i.e., it is a constructor-rooted term), we collect all topmost constructor symbols that agree with π and follow the C-paths that start from every maximal operation-rooted subterm in order to continue inspecting the associated values.
For instance, given the slicing criterion (main, C(T, ⊥)) and the term depen- dence graph of Fig. 5, the nodes U that correspond to the program positions of C(T, ⊥) which do not address the symbol ⊥ are shown with a bold box in Fig. 6.
Finally, we collect
the program positions of the nodes (associated with the right-hand side of a program rule) in every C-path that ends in a node of U and either starts in f or there is no node labeled with f in the path,
the program positions of the descendants ł of the above nodes (i.e., all nodes which are reachable following the S arrows), and
the program positions of the nodes which are reachable from ł following the
C arrows, its descendants, and so on, until no new nodes are collected.

Therefore, in the example above, the slice will contain the following program positions:
the program positions (R1, Λ), (R1, 1), (R2, Λ) associated with the paths that end in a node with a bold box;
the program positions of their descendants, i.e., (R1, 1.1);
and no more program positions, since there is no node reachable from the node labeled with A.
Trivially, this algorithm always terminates. The completeness of the algorithm (i.e., that all the program positions of the slice according to Definition 3.10 are collected) can be proved by showing that all possible computations can be traced using the term dependence graph.
However, the above algorithm for computing static slices is not correct since there may be C-paths in the term dependence graph that have no counterpart in

(R1,/\)	(R2,/\)	(R3,/\)

Fig. 6. Slice of the program in Example 3.5


Fig. 7. Term dependence graph of the program in Example 4.4
the computations of the original program. The following example illustrates this point.
Example 4.4 Consider the following program:
(R1)	main → g(f(A)) (R2)	g(B) → B
(R3)	f(A) → A

The associated term dependence graph is shown in Fig. 7. From this term depen- dence graph, we would infer that there is a computation from main to B while this is not true.

Related Work and Discussion
The first attempt to adapt PDGs to the functional paradigm has been recently introduced by Rodrigues and Barbosa [10]. They have defined the functional de- pendence graphs (FDG), which represent control relations in functional programs. However, the original aim of FDGs was the component identification in functional programs and thus they only consider high level functional program entities (i.e., the lowest level of granularity they consider are functions).
In a FDG, a single node often represents a complex term (indeed a complete function definition) and, hence, the information about control dependences of its subterms is not stored in the graph. Our definition of term dependence graph solves this problem by representing terms as trees and thus considering a lower level of

granularity for control dependences between subterms.
As mentioned before, our term dependence graph shares many similarities with the loop checks of [1]. Roughly speaking, [1] defines a directed graph of functional dependencies as follows: for every rule l → r, there is an R-arrow from l to every subterm of r (where inner arguments are replaced by fresh variables); also, u-arrows are added from every term in the right-hand side of an R-arrow to every term in the left-hand side of an R-arrow with which it unifies. In this way, every possible computation path can be followed in the directed graph of functional dependencies. Later, [2] introduced the computation of similar relations (the so called dependency pairs) to analyze the termination of term rewriting systems.
As for future work, we plan to formally prove the completeness of the slices computed by Algorithm 1. We also want to identify and define more dependence relations in the term dependence graph in order to augment its precision w.r.t. Def- inition 3.6. Then, we want to extend the framework to cover higher-order features. Finally, we plan to implement the slicing algorithm and integrate it in a Curry slicer
[9] to perform static slicing of functional and functional logic programs.

References
M. Alpuente, M. Falaschi, M.J. Ramis, and G. Vidal. Narrowing Approximations as an Optimization for Equational Logic Programs. In J. Penjam and M. Bruynooghe, editors, Proc. of PLILP’93, Tallinn (Estonia), pages 391–409. Springer LNCS 714, 1993.
T. Arts and J. Giesl. Termination of term rewriting using dependency pairs. Theoretical Computer Science, 236(1-2):133–178, 2000.
F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge University Press, 1998.
J. Ferrante, K.J. Ottenstein, and J.D. Warren. The Program Dependence Graph and Its Use in Optimization. ACM Transactions on Programming Languages and Systems, 9(3):319–349, 1987.
J. Field and F. Tip. Dynamic Dependence in Term Rewriting Systems and its Application to Program Slicing. Information and Software Technology, 40(11-12):609–634, 1998.
D.J. Kuck, R.H. Kuhn, D.A. Padua, B. Leasure, and M. Wolfe. Dependence Graphs and Compiler Optimization. In Proc. of the 8th Symp. on the Principles of Programming Languages (POPL’81), SIGPLAN Notices, pages 207–218, 1981.
Y.A. Liu and S.D. Stoller. Eliminating Dead Code on Recursive Data. Science of Computer Programming, 47:221–242, 2003.
C. Ochoa, J. Silva, and G. Vidal. Dynamic Slicing Based on Redex Trails. In Proc. of the ACM SIGPLAN 2004 Symposium on Partial Evaluation and Program Manipulation (PEPM’04), pages 123–
134. ACM Press, 2004.
C. Ochoa, J. Silva, and G. Vidal. Lighweight Program Specialization via Dynamic Slicing. In Workshop on Curry and Functional Logic Programming (WCFLP 2005), pages 1–7. ACM Press, 2005.
N. Rodrigues and L.S. Barbosa. Component Identification Through Program Slicing. In Proc. of Formal Aspects of Component Software (FACS 2005). Elsevier ENTCS, 2005.
J.R. Slagle. Automated Theorem-Proving for Theories with Simplifiers, Commutativity and Associativity. Journal of the ACM, 21(4):622–642, 1974.
F. Tip. A Survey of Program Slicing Techniques. Journal of Programming Languages, 3:121–189, 1995.
M.D. Weiser. Program Slicing. IEEE Transactions on Software Engineering, 10(4):352–357, 1984.
