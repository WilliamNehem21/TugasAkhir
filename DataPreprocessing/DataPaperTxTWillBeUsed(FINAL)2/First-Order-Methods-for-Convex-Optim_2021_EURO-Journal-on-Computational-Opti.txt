EURO Journal on Computational Optimization 9 (2021) 100015

		




First-Order Methods for Convex Optimization
Pavel Dvurechenskya,b,c, Shimrit Shternd, Mathias Staudigle,âˆ—
a Weierstrass Institute for Applied Analysis and Stochastics, Mohrenstr. 39, Berlin 10117, Germany
b Institute for Information Transmission Problems RAS, Bolshoy Karetny per. 19, build.1, Moscow, 127051, Russia
c Moscow Institute of Physics and Technology, 9 Institutskiy per., Dolgoprudny, Moscow Region, 141701, Russia
d Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa, Israel
e Maastricht University, Department of Data Science and Knowledge Engineering (DKE) and Mathematics Centre Maastricht (MCM), Paul-Henri Spaaklaan 1, Maastricht 6229 EN, The Netherlands


a r t i c l e	i n f o	a b s t r a c t

	

2010 MSC:
90C25
90C30
90C06
68Q25
65Y20
68W40

Keywords:
Convex Optimization Composite Optimization First-Order Methods Numerical Algorithms Convergence Rate Proximal Mapping Proximity Operator Bregman Divergence
First-order methods for solving convex optimization problems have been at the forefront of mathematical opti- mization in the last 20 years. The rapid development of this important class of algorithms is motivated by the success stories reported in various applications, including most importantly machine learning, signal process- ing, imaging and control theory. First-order methods have the potential to provide low accuracy solutions at low computational complexity which makes them an attractive set of tools in large-scale optimization problems. In this survey, we cover a number of key developments in gradient-based optimization methods. This includes non-Euclidean extensions of the classical proximal gradient method, and its accelerated versions. Additionally we survey recent developments within the class of projection-free methods, and proximal versions of primal- dual schemes. We give complete proofs for various key results, and highlight the unifying aspects of several optimization algorithms.





Introduction

The traditional standard in convex optimization was to trans- late a problem into a conic program and solve it using a primal- dual interior point method (IPM). The monograph Nesterov and Ne- mirovski (1994) was instrumental in setting this standard. The primal- dual formulation is a mathematically elegant and powerful approach as these conic problems can then be solved to high accuracy when the di- mension of the problem is of moderate size. This philosophy culminated into the development of a robust technology for solving convex opti- mization problems which is nowadays the computational backbone of many specialized solution packages like MOSEK (Andersen and Ander- sen, 2000), or SeDuMi (Sturm, 1999). However, in general, the iteration
mension. As a result, as the dimension ğ‘› of optimization problems grows, costs of interior point methods grow non-linearly with the problemâ€™s di-
off-the shelve interior point methods eventually become impractical. As an illustration, the computational complexity of a single step of many
standardized IPMs scales like ğ‘›3, corresponding roughly to the complex-
ity of inverting an ğ‘› Ã— ğ‘› matrix. This means that for already quite small problems of size like ğ‘› = 102, we would need roughly 106 arithmetic op-
erations just to compute a single iterate. From a practical viewpoint, such a scaling is not acceptable. An alternative solution approach, par- ticularly attractive for such â€large-scaleâ€ problems, are first-order meth- ods (FOMs). These are iterative schemes with computationally cheap iterations usually known to yield low-precision solutions within reason- able computation time. The success-story of FOMs went hand-in-hand with the fast progresses made in data science, analytics and machine learning. In such data-driven optimization problems, the trade-off be- tween fast iterations and low accuracy is particularly pronounced, as these problems usually feature high-dimensional decision variables. In these application domains precision is usually considered to be a sub- ordinate goal because of the inherent randomness of the problem data, which makes it unreasonable to minimize with accuracy below the sta- tistical error.
The development of first-order methods for convex optimization problems is still a very vibrant field, with a lot of stimulus from the


âˆ— Corresponding author.
E-mail addresses: pavel.dvurechensky@wias-berlin.de (P. Dvurechensky), shimrits@technion.ac.il (S. Shtern), m.staudigl@maastrichtuniversity.nl (M. Staudigl).

https://doi.org/10.1016/j.ejco.2021.100015
Received 30 December 2020; Received in revised form 19 August 2021; Accepted 11 October 2021
2192-4406/Â© 2021 The Author(s). Published by Elsevier Ltd on behalf of Association of European Operational Research Societies (EURO). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)



already mentioned applications in machine learning, statistics, optimal control, signal processing, imaging, and many more, see e.g. the recent review papers on optimization for machine learning (Bottou et al., 2018; Jain and Kar, 2017; Wright, 2018). Naturally, any attempt to try to sur- vey this lively scientific field is already doomed from the beginning to be a failure, if one is not willing to make restrictions on the topics cov- ered. Hence, in this survey we tried to give a largely self-contained and concise summary of some important families of FOMs, which we believe have had an ever-lasting impact on the modern perspective of continu- ous optimization. Before we give an outline of what is covered in this survey, it is therefore fair to mention explicitly, what is NOT covered in the pages to come. One major restriction we imposed on ourselves is the concentration on deterministic optimization algorithms. This is in- deed a significant cut in terms of topics, since the field of stochastic op- timization and randomized algorithms has particularly been at the fore- front of recent progresses made. Nonetheless, we intentionally made this cut, since most of the developments within stochastic optimization al- gorithms are based on deterministic counterparts, and actually in many cases one can think of deterministic algorithms as the mean-field equiv- alent of a stochastic optimization technique. As a well-known example, we can mention the celebrated stochastic approximation theory initi- ated by Robbins and Monro (1951), with its close connection to deter- ministic gradient descent. See Benveniste et al. (1990); Kushner (1984); Ljung et al. (2012), for classical references from the point of view of systems theory and optimization, and BenaÃ¯m, 1998 for its deep con- nection with deterministic dynamical systems. This link has gained sig- nificant relevance in various stochastic optimization models recently (Davis et al., 2020; Duchi and Ruan, 2018; Mertikopoulos and Staudigl,
timization, mainly because of its good scalability properties and small iteration costs. Conceptually, it is an interesting optimization method, as it allows us to solve convex programming problems with complicated geometry on which proximal operators are not easy to evaluate. This, in fact, applies to many important domains, like the Spectrahedron, or domains defined via intersections of several half spaces. CG is also rele- vant when the iterates should preserve structural features of the desired solution, like sparsity. Section 5 gives a comprehensive account of this versatile method.
All the methods we discussed so far generally provide sublinear con-
ity of ğ‘‚(1âˆ•ğœ€). In his influential paper (Nesterov, 1983), Nesterov pub- vergence guarantees in terms of function values with iteration complex- lished an optimal method with iteration complexity of ğ‘‚(1âˆ• ğœ€) to reach an ğœ€-optimal solution. This was the starting point for the development
of acceleration techniques for given FOMs. Section 6 summarizes the recent developments in this field.
With writing this survey, we tried to give a holistic presentation of the main methods in use. At various stages in the survey, we establish connections, if not equivalences, between various methods. For many of the key results we provide self-contained proofs.
Notation We use standard notation and concepts from convex and variational analysis, which, unless otherwise specified, can all be found in the monographs (Bauschke and Combettes, 2016; Hiriart-Urrut and
we let ğ–µ represent a finite-dimensional vector space of dimension ğ‘› with LemarÃ©chal, 2001; Rockafellar and Wets, 1998). Throughout this article, norm  â‹… . We will write ğ–µâˆ— for the (algebraic) dual space of ğ–µ with
is ğ‘¦  = sup{ ğ‘¦, ğ‘¥   ğ‘¥ â‰¤ 1}. The set of proper lower semi-continuous


important classes of alternating minimization methods, such as block- coordinate descent, and variations thereof. Section 14 in the beautiful book (Beck, 2017) gives a thorough account of these methods, and we urge the interested reader to start reading there.


0
given continuously differentiable function ğ‘“
gradient vector
(  ğœ•ğ‘“	 ğœ•ğ‘“ )âŠ¤
âˆ¶ ğ–·
âŠ† ğ–µ â†’|


â„ we denote its

ferent optimization algorithms are surveyed, all of which belong now to the standard toolkit of mathematical programmers. After introduc- ing the (standard) notation that will be used in this survey, we give a precise formulation of the model problem for which modern convex optimization algorithms are developed. In particular, we focus on the general composite convex optimization model, including smooth and non-smooth terms. This model is rich enough to capture a significant class of convex optimization problems. Non-smoothness is an impor- tant feature of the model, as it allows us to incorporate constraints via penalty and barrier functions. Non-smooth optimization methods also gained a lot of attention in statistical and machine learning where regu- larization functions are usually included in the estimation part in order to promote sparsity or other a-priori relevant information about the es- timator to be obtained. An eï¬ƒcient way to deal with non-smoothness is provided by the use of proximal operators, a key methodological contri- bution born within convex analysis (see Rockafellar and Wets (1998) for an historical overview). Section 3 introduces the general non-Euclidean proximal setup, which describes the mathematical framework within which the celebrated Mirror Descent and Bregman proximal gradient meth- ods are analyzed nowadays. These tools achieved extreme popularity in online learning and convex optimization (Bubeck, 2015; Juditsky and Nemirovski, 2011a; 2011b). The main idea behind this technology is to exploit favorable structure in the problemâ€™s geometry to boost the prac- tical performance of gradient-based methods. The proximal revolution has also influenced the further development of primal-dual optimization methods based on augmented Lagrangians. We review proximal variants of the celebrated Alternating Direction Method of Multipliers (ADMM) in Section 4. We then move on to give an in-depth presentation of projection-free optimization methods based on linear minimization ora-
cles, the classical Conditional Gradient (CG) (a.k.a Frank-Wolfe) method
The subdifferential at a point ğ‘¥ âˆˆ ğ–· âŠ† ğ–µ of a convex function ğ‘“ âˆ¶ ğ–µ â†’
â„ âˆª {+âˆ} is denoted as
ğœ•ğ‘“ (ğ‘¥) = {ğ‘ âˆˆ ğ–µâˆ— ğ‘“ (ğ‘¦) â‰¥ ğ‘“ (ğ‘¥) + ğ‘, ğ‘¦ âˆ’ ğ‘¥  âˆ€ğ‘¦ âˆˆ ğ–µ}.	(1.1)
The elements of ğœ•ğ‘“ (ğ‘¥) are called subgradients.
Given some set ğ–· âŠ† ğ–µ, denote its relative interior as relint(ğ–·). Recall As a notational convention, we write matrices in bold capital fonts.
that, if the dimension of the set ğ–· agrees with the dimension of the
interior, which we denote as int(ğ–·). Hence, the two notions differ only ground space ğ–µ, then the relative interior coincides with the topological
We denote the closure as cl(ğ–·). The boundary of ğ–· is defined in the usual in situations where ğ–· is contained in a lower-dimensional submanifold. way bd(ğ–·) = cl(ğ–·) â§µ int(ğ–·).

Composite convex optimization

In this survey we focus on the generic optimization problem
min{Î¨(ğ‘¥) âˆ¶= ğ‘“ (ğ‘¥) + ğ‘Ÿ(ğ‘¥)}.	(P)
ğ‘¥âˆˆğ–·
At many stages of this survey, the following properties are imposed on the data of the minimization problem (P):
Assumption 1.
ğ–· âŠ† ğ–µ is a nonempty closed convex set embedded in a finite- dimensional real vector space ğ–µ;
ğ‘“ âˆ¶ ğ–µ â†’ â„ is convex and continuously differentiable on a neighbor- hood of ğ–·. Furthermore, it possesses a ğ¿ğ‘“ -Lipschitz continuous gra-
dient on ğ–·:

and its recent variants. CG gained extreme popularity in large-scale op-
(âˆ€ğ‘¥, ğ‘¥â€² âˆˆ ğ–·) âˆ¶
âˆ‡ (ğ‘¥) âˆ’ âˆ‡ğ‘“ (ğ‘¥â€²)
â‰¤ ğ¿
ğ‘“ â€–ğ‘¥  ğ‘¥ â€–
(2.1)

ğ‘Ÿ âˆˆ Î“0(ğ–µ) and ğœ‡-strongly convex on ğ–µ for some ğœ‡ â‰¥ 0 with respect to a norm â‹… on ğ–µ. This means that for all ğ‘¥, ğ‘¦ âˆˆ dom ğ‘Ÿ, and any selection ğ‘Ÿ (ğ‘¥) âˆˆ ğœ•ğ‘Ÿ(ğ‘¥), we have
ğ‘Ÿ(ğ‘¦) â‰¥ ğ‘Ÿ(ğ‘¥) + ğ‘Ÿâ€²(ğ‘¥), ğ‘¦ âˆ’ ğ‘¥ + ğœ‡ ğ‘¥ âˆ’ ğ‘¦ 2.
2
If ğœ‡ = 0 then the function ğ‘Ÿ is called convex.
We are interested in problems which are feasible.
Assumption 2. dom ğ‘Ÿ âˆ© ğ–· â‰  âˆ….
In many recent applications, the smooth function ğ‘“ represents a data fidelity term and the non-smooth part ğ‘Ÿ takes the role of a penalty func- tion or regularizer. The most important examples of function ğ‘Ÿ are as
follows:

Wajs (2005). This led to a rich interplay between convex programming on the one hand and machine learning and signal/image processing on the other hand. Indeed, several work-horse models in these application domains are of the composite type
Î¨(ğ‘¥) = ğ‘”(ğ€ğ‘¥) + ğ‘Ÿ(ğ‘¥)	(2.7)
where ğ‘” âˆ¶ ğ–¤ â†’ â„ is a smooth function defined on a finite-dimensional set ğ–¤ (usually of lower dimension than ğ–µ), and ğ€ âˆˆ Bğ–«(ğ–µ, ğ–¤) is bounded linear operator mapping points ğ‘¥ âˆˆ ğ–µ to elements ğ€ğ‘¥ âˆˆ ğ–¤. Convexity al-
lows us to switch between primal and dual formulations freely, so that the above problem can be equivalently considered as a convex-concave minimax problem
min max{ğ‘Ÿ(ğ‘¥) + ğ€ğ‘¥, ğ‘¦ âˆ’ ğ‘”âˆ—(ğ‘¦)}	(2.8)

ğ‘Ÿ is an indicator function of a closed convex set ğ–¢ âŠ‚ ğ–µ with ğ–¢ âˆ© ğ–· â‰ 
ğ‘¥âˆˆğ–· ğ‘¦âˆˆğ–¤	âŸ¨	âŸ©

âˆ…:
ğ‘Ÿ(ğ‘¥) = ğ›¿ğ–¢(ğ‘¥) âˆ¶=
0	if ğ‘¥ âˆˆ ğ–¢,
+âˆ	if ğ‘¥ âˆ‰ ğ–¢.
Such minimax formulations have been of key importance in signal pro- cessing and machine learning (Juditsky et al., 2013; Juditsky and Ne- mirovski, 2011b), game theory (Sorin, 2000), decomposition methods (Tseng, 1991) and its very recent innovation around generative adver-

ğ‘Ÿ is a self-concordant barrier (Nesterov, 2018b; Nesterov and Ne-
mirovski, 1994) for a closed convex set ğ–¢ âŠ‚ ğ–µ with ğ–¢ âˆ© ğ–·  âˆ….
ğ‘Ÿ is a nonsmooth convex function with relatively simple structure.
For example, it could be a norm regularization like the celebrated
ğ“ -regularizer ğ‘Ÿ(ğ‘¥) = ğ‘¥  . This regularizer plays a fundamental role
sarial networks (Goodfellow et al., 2014).
Another canonical class of optimization problems in machine learn- ing is the finite-sum model
Î¨(ğ‘¥) =  1 âˆ‘ ğ‘“ (ğ‘¥) + ğ‘Ÿ(ğ‘¥),	(2.9)




cone associated with the closed convex set ğ–· âŠ† ğ–µ as For characterizing solutions to our problem (P), define the tangent
ğ–³ğ–¢ (ğ‘¥) âˆ¶= {{ğ‘£ = ğ‘¡(ğ‘¥â€² âˆ’ ğ‘¥)|ğ‘¥â€² âˆˆ ğ–·, ğ‘¡ â‰¥ 0} âŠ† ğ–µ	if ğ‘¥ âˆˆ ğ–·,
which comes from supervised learning, where ğ‘“ğ‘– (ğ‘¥) corresponds to the loss incurred on the ğ‘–-th data sample using a hypothesis parameter- ized by the decision variable ğ‘¥. Typically, ğ‘ is an extremely large
number as it corresponds to the size of the data set. The recent liter-




and the normal cone
{{ğ‘ âˆˆ ğ–µâˆ—| sup
â‰¤ 0}	if ğ‘¥ âˆˆ ğ–·


very active in making such large scale optimization problems tractable. Surveys on the latest developments in these fields can be found in



We remark that ğœ•ğ›¿ğ–·(ğ‘¥) = ğ–­ğ–¢ğ–·(ğ‘¥) for all ğ‘¥ âˆˆ ğ–·.
Given the feasible set ğ–· âŠ† ğ–µ, we denote the value function
Î¨  (ğ–·) âˆ¶= inf Î¨(ğ‘¥).	(2.3)
ğ‘¥âˆˆğ–·
We are focusing in this survey on problems which are solvable.
The Proximal Gradient Method

Motivation

In the context of the composite optimization problem (P), a classical

Assumption 3. ğ–·âˆ— âˆ¶= {ğ‘¥ âˆˆ ğ–· Î¨(ğ‘¥) = Î¨
min
(ğ–·)} â‰  âˆ….
and very powerful idea is to construct numerical optimization meth- ods by exploiting problem structure. Following this philosophy, we de-

Given the standing hypothesis on the functions ğ‘“ and ğ‘Ÿ, it is easy
to see that ğ–·âˆ— is always a closed convex set. Moreover, if ğœ‡ > 0, then problem (P) is strongly convex, and so ğ–·âˆ— is a singleton.
Optimality conditions for problem (P) can be formulated using dif- ferential calculus tools from convex analysis. (P) can be treated as an
linearization of the smooth part, the non-smooth part ğ‘Ÿ âˆˆ Î“0(ğ–µ), and a termine the position of the next iterate by minimizing the sum of the quadratic regularization term with weight ğ›¾ > 0:
ğ‘¥+(ğ›¾) = argmin{ğ‘“ (ğ‘¥) + âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥ + ğ‘Ÿ(ğ‘¢) +  1  ğ‘¢ âˆ’ ğ‘¥ 2}.	(3.1)

unconstrained problem by augmenting the objective function Î¨ = ğ‘Ÿ + ğ‘“
ğ‘¢âˆˆğ–·	âŸ¨
âŸ©	2ğ›¾ â€–	â€–2

by the non-smooth penalty ğ›¿ğ–·. Our main problem becomes then
min(ğ‘“ (ğ‘¥) + ğ‘Ÿ(ğ‘¥) + ğ›¿ (ğ‘¥)).	(2.4)
ğ‘¥âˆˆğ–µ
Fermatâ€™s rule says that ğ‘¥âˆ— âˆˆ ğ–µ is a solution to (2.4) if and only if 0 âˆˆ
ğœ•Î¨(ğ‘¥âˆ—) + ğ–­ğ–¢ğ–·(ğ‘¥âˆ—) (Rockafellar and Wets (1998, Theorem 8.15)). In gen-
eral, the subdifferential operator is not linear, and we only have a â€fuzzy


Disregarding terms which do not influence the computation of the so-
set constraint into the non-smooth part by defining ğœ™(ğ‘¥) = ğ‘Ÿ(ğ‘¥) + ğ›¿ğ–·(ğ‘¥), lution of this strongly convex minimization problem, and absorbing the
we see that (3.1) can be equivalently written as
ğ‘¥+ (ğ›¾) = argmin {ğ›¾ğœ™(ğ‘¢) + 1 â€–ğ‘¢ âˆ’ (ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥))â€–2 }.	(3.2)

and ğ‘“ is smooth on a neighborhood containing ğ–· (Assumptions 1(b)). Hence, by Rockafellar and Wets (1998, Exercise 8.8c), we have ğœ•Î¨(ğ‘¥âˆ—) =
âˆ‡ğ‘“ (ğ‘¥âˆ—) + ğœ•ğ‘Ÿ(ğ‘¥âˆ—). Therefore, Fermatâ€™s optimality condition becomes
0 âˆˆ âˆ‡ğ‘“ (ğ‘¥âˆ—) + ğœ•ğ‘Ÿ(ğ‘¥âˆ—) + ğ–­ğ–¢ğ–·(ğ‘¥âˆ—).	(2.5)
This means that there exists ğœ‰ âˆˆ ğœ•ğ‘Ÿ(ğ‘¥âˆ—) such that
interesting geometric principles acting here. Indeed, if ğ‘Ÿ would be a This way of writing the updating scheme immediately reveals some
finite constant on ğ–· (say 0 for concreteness), then the rule (3.2) is nothing else than the Euclidean projection of the directional vector
ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥) onto the set ğ–·. In this case, the minimization routine re-
turns the classical projected gradient step ğ‘¥+(ğ›¾) = ğ‘ƒğ–·(ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥)), where

âˆ‡ (ğ‘¥âˆ—) + ğœ‰, ğ‘£ â‰¥ 0	âˆ€ğ‘£ âˆˆ ğ–³ğ–¢ (ğ‘¥âˆ—).	(2.6)	ğ–·
ğ‘¦âˆˆğ–· 2 â€–	â€–2	ğ–·


The structured composite optimization problem (P) has attracted a lot of interest in convex programming over the last 20 years moti- vated by a number of important applications, see, e.g. Combettes and
cases where the non-smooth function ğ‘Ÿ is non-trivial over the domain erates the classical gradient projection method. A new obstacle arises in
ğ–·. A fundamental idea, going back to Moreau (1965), is to define the
proximity operator Proxğœ™ âˆ¶ ğ–µ â†’ ğ–µ associated with a function ğœ™ âˆˆ Î“0(ğ–µ)



as1
Prox (ğ‘¥) âˆ¶= argmin {ğœ™(ğ‘¢) + 1 â€–ğ‘¢ âˆ’ ğ‘¥â€–2}.	(3.3)
of the domain ğ–·, see e.g. Auslender and Teboulle (2009). This can not only positively affect the per-iteration complexity, but also will have a

Remark 3.1. The classical Moreau proximity operator of ğœ™ is, in gen- eral, explicitly computable when ğœ™ is norm like, or when ğœ™ is the in-
dicator function of sets whose geometry is favorable to Euclidean pro- jections. Although quite frequent in applications (orthant, second-order
cone, ğ“1 norm), these prox-friendly functions are very scarce, see, e.g.,
Combettes and Wajs (2005, Section 2.6). A significant improvement will
be made in Section 3.2, where a general Bregman proximal framework will be introduced.
The value function
 1
algorithms is to introduce a distance generating function â„ âˆ¶ ğ–µ â†’ (âˆ’âˆ, âˆ], which is a barrier-type mapping suitably chosen to capture geometric
features of the set ğ–·.

Definition 3.1. Let ğ–· be a closed convex subset of ğ–µ. We say that â„ âˆˆ Î“0(ğ–µ) is a distance generating function (DGF) with modulus ğ›¼ > 0 with
respect to  â‹…  on ğ–· if
â„ is closed, convex and proper;
ğ–· âŠ† dom â„;
the set ğ–·â—¦ = {ğ‘¥ âˆˆ ğ–· ğœ•â„(ğ‘¥)  âˆ…} is nonempty and convex;

ğœ™ (ğ‘¥) = inf{ğœ™(ğ‘¢) +
ğ‘¢
2ğ›¾
â€–ğ‘¢  ğ‘¥â€–
â„ restricted to ğ–·â—¦
is|continuously differentiable and strongly convex

is called the Moreau envelope of the function ğœ™, and is an important
smoothing and regularization tool, frequently employed in numerical
analysis. Indeed, for a function ğœ™ âˆˆ Î“ (ğ–µ) and ğ›¾ > 0, its Moreau enve-
under the norm  â‹…  with parameter ğ›¼:
(âˆ€ğ‘¥, ğ‘¥â€² âˆˆ ğ–·â—¦) âˆ¶	âˆ‡ (ğ‘¥) âˆ’ âˆ‡â„(ğ‘¥â€²), ğ‘¥ âˆ’ ğ‘¥â€² â‰¥
ğ›¼â€–ğ‘¥
â€“ ğ‘¥â€² 2.

0
lope is finite everywhere, convex and has ğ›¾âˆ’1-Lipschitz continuous gra- dient on ğ–µ given by âˆ‡ğœ™ğ›¾ (ğ‘¥) = 1 (ğ‘¥ âˆ’ Proxğ›¾ğœ™(ğ‘¥)) Bauschke and Combettes
(2016, Prop. 12.30).
In the context of minimization the composite model Î¨ = ğ‘“ + ğ‘Ÿ, the
gradient-based methods. Choosing ğœ™ = ğ‘Ÿ + ğ›¿F in (3.3), and replacing the proximity operator is the key actor in generating a large family of generic input with the specific input ğ‘¥ âˆ’ ğ›¾âˆ‡ğ‘“ (ğ‘¥), we are in the frame-
work of the Proximal Gradient Method (PGM). PGM is a very powerful method which received enormous interest in optimization and its ap- plications. For a survey in the context of signal processing we refer the reader to Combettes and Pesquet (2011). A general survey on proximal operators can be found in Beck (2017); Parikh and Boyd (2014).
We denote by îˆ´ (ğ–·) the set of DGFs on ğ–·.

(1970, Section 23-25), we know that dom(ğœ•â„) âŠ‚ dom â„. Hence, ğ–·â—¦ = From classical differential theory of convex functions Rockafellar dom(ğœ•â„) âˆ© ğ–· is contained in the set dom â„ âˆ© ğ–·, which in turn agrees with
ğ–· thanks to property (b). Restricted to ğ–·â—¦ the DGF â„ is continuously
differentiable.
In many proximal settings we are interested in DGFs which act as
function â„ are captured by its scaling near bd(ğ–·), usually encoded in barriers on the feasible set ğ–·. Naturally, the barrier properties of the
terms of the notion of essential smoothness (Rockafellar (1970, Section 26).)
Definition 3.2 (Essential smoothness). â„ âˆˆ îˆ´ (ğ–·) is essentially smooth


Input:    ğ‘¥0     âˆˆ    ğ–·. The Proximal Gradient Method (PGM)
General step: For ğ‘˜ = 0, 1, â€¦ do: Choose ğ›¾ğ‘˜ > 0.
if it satisfies the following three conditions:
int(dom â„) â‰  âˆ…;
â„ is differentiable throughout int(dom â„);
limğ‘–â†’âˆ â€–âˆ‡â„(ğ‘¥ğ‘–)â€– = +âˆ whenever ğ‘¥1, ğ‘¥2, â€¦ is a sequence in int(dom â„)

Given â„ âˆˆ îˆ´ (ğ–·), its Bregman divergence ğ· âˆ¶ dom â„ Ã— dom(ğœ•â„) â†’ â„

Remark 3.2. In the fully non-smooth case, i.e. when ğ‘“ = 0 in our model
is defined as
ğ· (ğ‘¢, ğ‘¥) âˆ¶= â„(ğ‘¢) âˆ’ â„(ğ‘¥) âˆ’ âˆ‡â„(ğ‘¥), ğ‘¢ âˆ’ ğ‘¥ .	(3.4)

problem, PGM reduces to a classical recursion known as the proximal	â„	âŸ¨	âŸ©



ğ‘¥ğ‘˜+1 = Prox	(ğ‘¥ğ‘˜) = argmin{ğœ™(ğ‘¢) +  1
ğ‘¢ âˆ’ ğ‘¥ğ‘˜ 2}.
(âˆ€ğ‘¥ âˆˆ dom(ğœ•â„), âˆ€ğ‘¢ âˆˆ dom â„) âˆ¶	ğ· (ğ‘¢, ğ‘¥) â‰¥ ğ›¼
ğ‘¢ âˆ’ ğ‘¥ 2.	(3.5)

ğ‘˜	ğ‘¢âˆˆğ–µ
2ğ›¾ğ‘˜ â€–	â€–
â„	2 â€–	â€–

This scheme has been first proposed by Martinet (1970) and Rockafellar (1976b).

Bregman Proximal Setup
Hence, ğ·â„(ğ‘¥, ğ‘¥) = 0 for ğ‘¥ âˆˆ dom(ğœ•â„), but in general it is not a symmetric
function and it does not satisfy a triangle inequality. This disqualifies
ğ·â„ from carrying the label of a metric, but it can still be interpreted as
a distance measure.
The convex conjugate â„ (ğ‘¦) = supğ‘¥âˆˆğ–µ{ ğ‘¥, ğ‘¦ âˆ’ â„(ğ‘¥)} for a function â„ âˆˆ ( )	âˆ—	1

the ğ“ -norm 1 ğ‘¢ âˆ’	2
gradient (Rockafellar and Wets (1998, Proposition 12.60)):

2	2 â€–	ğ‘¥â€– by a different distance-like function which is tai-

distance-like functions that will be used are Bregman divergences. The
2	1	âŸ¨
1	2	1âŸ©
2ğ›¼ â€– 2
1â€–âˆ—

transition from Euclidean to non-Euclidean distance measures is mo- tivated by the usefulness and flexibility of the latter in computational perspectives and potentials for improving convergence properties for specific application domains. In particular, the move from Euclidean to non-Euclidean distance measures allows to adapt the algorithm to the underlying geometry, typically explicitly embodied in the description
for all ğ‘¦1, ğ‘¦2 âˆˆ ğ–µâˆ—.
It will be instructive to go over some standard examples of dis-
tance generating functions. See also Combettes and Wajs (2005), Bauschke and Combettes (2016), and Ben-Tal and Nemirovski (2020).
Example 3.1 (Euclidean Projection). We begin by revisiting the ğ“2- projection on some closed convex subset ğ–· âŠ‚ ğ–µ = â„ğ‘›. Letting â„(ğ‘¥) =

1	2
1 The repository http://proximity-operator.net/index.html provides codes
â—¦ = ğ–·. Moreover, for ğ‘¥ âˆˆ ğ–·â—¦,

2 â€–ğ‘¥â€–2 for ğ‘¥
ğ–µ, we readily see that ğ–·



Guttmann-Beck (2019).
ğ· (ğ‘¢, ğ‘¥) = 1 ğ‘¢ âˆ’ ğ‘¥ 2 for all ğ‘¢, ğ‘¥ âˆˆ ğ–·.

â„	2 â€–	â€–2

Example 3.2 (Entâˆ‘ropic Regularization). Let ğ–· = {ğ‘¥ âˆˆ â„ğ‘› | âˆ‘ğ‘›
ğ‘¥ğ‘– =
(Bauschke et al., 2003; Censor and Zenios, 1992) onto the set ğ–·. It should

â„+	ğ‘¥
â„ |  ğ‘–=1 ğ‘¥ğ‘–
ğ–µ  â„ğ‘›. De-

fine the function ğœ“ âˆ¶ â„ â†’ [0, âˆ] as
â§âªğ‘¡ ln(ğ‘¡) âˆ’ ğ‘¡	if ğ‘¡ > 0,
Example 3.1 the Bregman proximal operator boils down to the Moreau proximal operator (3.3). As we already alluded to, the main rationale
for the introduction of Bregman proximal operators is that it allows us

ğœ“ (ğ‘¡) =
0	if ğ‘¡ = 0,
âªâ©+âˆ	else.
of ğ–·. Below, we give examples for which Proxâ„ (ğ‘¥) is easy to compute in to define a projection framework which can be adapted to the geometry
âˆ‘

dowing the ground space ğ–µ with the ğ“1 norm â€– â‹… â€–  â€– â‹… â€–1, iâˆ‘t can be

nested scheme if used in an algorithm).

1}	1	+	++ |
ğ‘–=1 ğ‘–
=

. Indeed, on ğ–·â—¦ the function â„ is continuously differentiable with
âˆ‡â„(ğ‘¥) = [ln(ğ‘¥1 ), â€¦ , ln(ğ‘¥ğ‘› )]âŠ¤. Furthermore, ğœ•â„(0) = âˆ…, so that dom(ğœ•â„) =
ğ‘› . The resulting Bregman divergence is the Kullback-Leibler diver-
gence
ğ‘›	( ğ‘¢ )	ğ‘›
Example 3.6. In the following examples we assume that ğ–µ  â„ for sim- plicity.
1. Let ğœ™(ğ‘¥) = ğ›¾ ğ‘¥ âˆ’ ğœ‰ where ğ›¾, ğœ‰ > 0. Take â„(ğ‘¥) = ğ‘¥ ln(ğ‘¥), dom â„ = [0, âˆ).
Then

ğ·â„ (ğ‘¢, ğ‘¥) =
ğ‘–=1
ğ‘¢ğ‘– ln
ğ‘–
ğ‘¥ğ‘–
+
ğ‘–=1
(ğ‘¥ğ‘– âˆ’ ğ‘¢ğ‘– ).
âˆ
Proxâ„ (ğ‘¥) =
exp(ğ›¾)ğ‘¥	if ğ‘¥ < exp(âˆ’ğ›¾)ğœ‰,
ğœ‰	if ğ‘¥ âˆˆ [exp(âˆ’ğ›¾)ğœ‰, exp(ğ›¾)ğœ‰],
âª

0 â‰¤	â‰¤
0 â‰¤  â‰¤
ğ‘–=1  ğ‘–  ğ‘–	â©



( âˆ’ ) ln( âˆ’
) + (
â€“  ) ln(   âˆ’  )
âˆˆ (	)
2 (âˆš	)




ğœ“ğ‘,ğ‘
(ğ‘¡) âˆ¶=
0	if ğ‘¡ âˆˆ {ğ‘, ğ‘},
âª+âˆ

Note that fâ©or ğ‘¡ âˆˆ (ğ‘, ğ‘), we have ğœ“ â€²
else
(ğ‘¡) = ln ( ğ‘¡âˆ’ğ‘ ), and ğœ•ğœ“ğ‘,ğ‘ (ğ‘¡) = âˆ… for ğ‘¡ âˆˆ
Bregman proximal gradient method

For solving our main problem (P), a special selection of the function

â„ â§µ (ğ‘, ğ‘). Accordinâˆgly, the function â„(ğ‘¥) = âˆ‘ğ‘›
ğœ“ğ‘ ,ğ‘ (ğ‘¥ğ‘– ) is a DGF on ğ–· =
ğœ™ âˆˆ Î“ (ğ–µ) in (3.7) is ğœ™(ğ‘¢) = ğ›¾(ğ‘Ÿ + ğ›¿ )(ğ‘¢) + ğ›¾âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥ . Replacing the

ğœ“ â€²
ğ‘1 ,ğ‘1
ğ‘¥ ,	, ğœ“ â€²
ğ‘ğ‘› ,ğ‘ğ‘›
(ğ‘¥ğ‘› )]âŠ¤.
mapping
P â„

(ğ‘¥, ğ‘¦) âˆ¶= argmin{ğ›¾ğ‘Ÿ(ğ‘¢) + ğ‘¦, ğ‘¢ âˆ’ ğ‘¥ + ğ· (ğ‘¢, ğ‘¥)}.	(3.8)

Example 3.4 (Semidefinite Constraints). Let ğ–µ = Sğ‘› be the set of real
symmetric matrices and ğ–· = Sğ‘› be the cone of real symmetric positive
ğ›¾ğ‘Ÿ
ğ‘¢âˆˆğ–·
â„
âˆ—

+
ğ€ ğ
= tr(ğ€ğ).
The prox-mapping takes as inputs a â€primal-dualâ€ pair (ğ‘¥, ğ‘¦) âˆˆ ğ–·
Ã— ğ–µ

semi-definite matrices equipped with the inner product	,
Define the negative von Neumann entropy â„(ğ—) = tr[ğ— log(ğ—)], which
entropy. It can be verified that dom â„ = ğ–· and âˆ‡â„(ğ—) = log(ğ—) + ğˆ for can be seen as the matrix-equivalent of the negative Boltzmann-Shannon
ğ— âˆˆ Sğ‘› . Hence, dom â„ = ğ–·, and ğ–·â—¦ = Sğ‘› , the cone of positive definite matrices. For ğ— âˆˆ S++, the corresponding Bregman divergence is given
by
ğ·â„ (ğ—â€², ğ—) = tr[ğ—â€² log(ğ—â€²) âˆ’ ğ—â€² log(ğ—) + ğ—â€² âˆ’ ğ—]
where ğ‘¥ is the current iterate, and ğ‘¦ is a dual variable representing a
â€œgradient signalâ€ we obtain on the smooth part of the minimization
problem (P) (usually obtained after consulting a black-box oracle). Var- ious conditions on the well-posedness of the prox-mapping have been stated in the literature. We will not repeat them here, but rather refer to the recent survey (Teboulle, 2018). Below we give some examples.
Example 3.7 (Moreau Proximal Operator). Let ğ–µ = â„ğ‘› and ğ–· a
nonempty, closed and convex set in ğ–µ. Let  â‹…  =  â‹…  , and â„(ğ‘¥) =

â€–	â€– â€–  â€– â€–2

is 1 -strongly convex with respect to the nuclear norm â€–ğ—â€–1 = âˆ‘ |ğœ† (ğ—)|
ğ›¾ğ‘Ÿ(ğ‘¥, ğ‘¦) = Proxğ›¾(ğ‘Ÿ+ğ›¿ğ–· )(ğ‘¥ âˆ’ ğ‘¦).

â€– â‹… â€– . Consider the set ğ–·
âˆˆ  ğ‘› | âˆ‘ğ‘›
1
= 1}

ğ· (ğ— , ğ—) â‰¥	ğ— âˆ’ ğ—  .
2
with the Boltzmann-Shannon entropy â„(ğ‘¥) = âˆ‘ğ‘›
ğ‘¥ğ‘– ln(ğ‘¥ğ‘– ). For ğ‘Ÿ(ğ‘¥) = 0,

{ğ‘¥ âˆˆ ğ–µ ğ‘¥ > (ğ‘¥2 + â€¦+ ğ‘¥2
)1âˆ•2 }
++	ğ‘¥ ğ‘’ğ‘¦ğ‘–

| ğ‘›	1


ğ‘›âˆ’1
the interior of the second-order cone.
[P â„ (ğ‘¥, ğ‘¦)] =	ğ‘–
1 â‰¤ ğ‘– â‰¤ ğ‘›, ğ‘¥ âˆˆ ğ–·, ğ‘¦ âˆˆ ğ–µâˆ—.	(3.9)

â€“ ln( ğ‰ ğ‘¥, ğ‘¥ ) + ğ›¼ ğ‘¥ 2. Then â„ âˆˆ îˆ´ (ğ–·) with dom â„ = ğ–·â—¦ = ğ¿ğ‘›
âŠ‚ ğ–·. The

ğ‘›	2	2	ğ›¼
associated Bregman divergence is
ğ· (ğ‘¥, ğ‘¢) = âˆ’ ln ( âŸ¨ğ‰ğ‘› ğ‘¥, ğ‘¥âŸ© ) + 2 âŸ¨ğ‰ğ‘› ğ‘¥, ğ‘¢âŸ© âˆ’ 2 + ğ›¼ â€–ğ‘¥ âˆ’ ğ‘¢â€–2 .
++	ponentiated gradient descent (Beck and Teboulle, 2003; Juditsky et al.,
2005).
Example 3.9 (Box Constraints). Consider the setting introduced in

â„	ğ‰ ğ‘¢, ğ‘¢
ğ‰ ğ‘¢, ğ‘¢	2	2
Example 3.3. For ğ‘Ÿ(ğ‘¥) = 0, one can compute

The proximal framework for general conic constraints has been devel-
oped in Auslender and Teboulle (2006b).
[P â„ (ğ‘¥, ğ‘¦)] = ğ‘ +
	ğ‘ğ‘– âˆ’ ğ‘ğ‘–	
1 â‰¤ ğ‘– â‰¤ ğ‘›, ğ‘¥ âˆˆ ğ–·, ğ‘¦ âˆˆ ğ–µâˆ—.

ğ›¿ğ–·
ğ‘–	ğ‘–
1 + ğ‘ğ‘– âˆ’ğ‘¥ğ‘– exp(ğ‘¦ )

Once we endow our set ğ–· with a DGF, the technology generating a gradient method in this non-Euclidean setting is the Bregman proximal
operator (Teboulle, 1992) applied to the function ğœ™ âˆˆ Î“ (ğ–µ):
ğ‘¥ğ‘– âˆ’ğ‘ğ‘–	ğ‘–
If ğ›¾ > 0 is a step-size parameter and ğ‘¦ = ğ›¾âˆ‡ğ‘“ (ğ‘¥), then we obtain the
Bregman proximal map ğ‘‡ â„(ğ‘¥) âˆ¶=  â„ (ğ‘¥, ğ›¾âˆ‡ğ‘“ (ğ‘¥)) for all ğ‘¥ âˆˆ ğ–·. Iterating

0	ğ›¾
ğ›¾ğ‘Ÿ

Proxâ„ (ğ‘¥) âˆ¶= argmin{ğœ™(ğ‘¢) + ğ·â„ (ğ‘¢, ğ‘¥)}.	(3.7)
ğœ™	ğ‘¢âˆˆğ–µ
If ğœ™(ğ‘¥) = ğ›¿ğ–·(ğ‘¥) is the indicator function of the closed convex set ğ–· âŠ‚ ğ–µ,
then the Bregman proximal operator defines the Bregman projection
this map generates a discrete-time dynamical system known as the Breg- man proximal gradient method (BPGM).

assumption that the prox-mapping â„(ğ‘¥, ğ‘¦) can be evaluated eï¬ƒciently Remark 3.3. For simple implementation, BPGM relies on the structural

(3â‰¤.5) ğ‘“ (ğ‘¢) âˆ’ ğ‘“ (ğ‘¥+) + ğ¿ğ‘“ ğ· (ğ‘¥+, ğ‘¥).

The Bregman Proximal Gradient Method (BPGM)
Input: â„ âˆˆ	(ğ–·). Pick ğ‘¥0 âˆˆ dom(ğ‘Ÿ) âˆ© ğ–·â—¦.
ğ›¼	â„

ğ›¼
General step: For ğ‘˜ = 0, 1, â€¦ do: choose ğ›¾ > 0.
Using this estimate in relation (3.12), we obtain, for all ğ‘¢ âˆˆ dom â„ âˆ© ğ–·,
ğ›¾(Î¨(ğ‘¥+) âˆ’ Î¨(ğ‘¢)) â‰¤ ğ· (ğ‘¢, ğ‘¥) âˆ’ ğ· (ğ‘¢, ğ‘¥+) âˆ’ (1 âˆ’ ğ›¾ ğ¿ğ‘“ )ğ· (ğ‘¥+, ğ‘¥).	(3.17)

ğ›¾ğ‘˜ ğ‘Ÿ	ğ‘˜
If ğ›¾ âˆˆ (0,  ğ›¼ ], then the above yields
ğ‘“
ğ›¾(Î¨(ğ‘¥+) âˆ’ Î¨(ğ‘¢))  ğ· (ğ‘¢, ğ‘¥) âˆ’ ğ· (ğ‘¢, ğ‘¥+),  ğ‘¢ âˆˆ dom â„ âˆ© ğ–·.

on the trajectory {(ğ‘¥ğ‘˜, ğ›¾ âˆ‡ğ‘“ (ğ‘¥ğ‘˜)) 0 â‰¤ ğ‘˜ â‰¤ ğ¾ âˆˆ â„•âˆ—}. This, often somewhat	â„	â„

ğ‘˜	|
Setting  = ğ‘˜  + = ğ‘˜+1 and  =  , one can reformulate the previous



Basic Complexity Properties
To assess the iteration complexity of BPGM, let us start with some preparatory estimates. The first-order optimality condition for the point
+	â„
ğ›¾ğ‘Ÿ
ğ›¾  Î¨(ğ‘¥ğ‘˜+1) âˆ’ Î¨(ğ‘¢) â‰¤ ğ· (ğ‘¢, ğ‘¥ğ‘˜) âˆ’ ğ· (ğ‘¢, ğ‘¥ğ‘˜+1),  ğ‘¢ âˆˆ dom â„ âˆ© ğ–·.
Choosing ğ‘¢ = ğ‘¥ğ‘˜, we readily see ğ›¾ (Î¨(ğ‘¥ğ‘˜+1) âˆ’ Î¨(ğ‘¥ğ‘˜)) â‰¤ âˆ’ğ· (ğ‘¥ğ‘˜, ğ‘¥ğ‘˜+1) â‰¤ 0,

i.e. the sequence of function values {Î¨(ğ‘¥ğ‘˜)}ğ‘˜âˆˆâ„• is non-increasing. On the other hand, for a general reference point ğ‘¢ âˆˆ dom â„ âˆ© ğ–·, we also see that

ğ‘ âˆ’1	ğ‘ âˆ’1

0 âˆˆ ğ›¾ğœ•ğ‘Ÿ(ğ‘¥+) + ğ›¾âˆ‡ğ‘“ (ğ‘¥) + âˆ‡â„(ğ‘¥+) âˆ’ âˆ‡â„(ğ‘¥) + ğ–­ğ–¢ğ–·(ğ‘¥+)
âˆ‘ (Î¨(ğ‘¥ğ‘˜+1 ) âˆ’ Î¨(ğ‘¢)) â‰¤ âˆ‘  1 [ğ·â„ (ğ‘¢, ğ‘¥ğ‘˜ ) âˆ’ ğ·â„ (ğ‘¢, ğ‘¥ğ‘˜+1 )]

=	+	+
ğ‘˜=0
ğ‘˜=0 ğ›¾ğ‘˜

ğ›¾(ğœ•ğ‘Ÿ + ğ–­ğ–¢ğ–·)(ğ‘¥ ) + ğ›¾âˆ‡ğ‘“ (ğ‘¥) + âˆ‡â„(ğ‘¥ ) âˆ’ âˆ‡â„(ğ‘¥).
+
=  1
ğ·â„ (ğ‘¢, ğ‘¥0 ) âˆ’
 1  ğ·â„ (ğ‘¢, ğ‘¥ğ‘ ) +
ğ‘âˆ‘âˆ’2 (  1 
1 )ğ·â„ (ğ‘¢, ğ‘¥ğ‘˜+1 ).

Whence, there exists ğœ‰ âˆˆ ğœ•ğ‘Ÿ(ğ‘¥ ) such that, for all ğ‘¢ âˆˆ ğ–·,
ğ›¾0
ğ›¾ğ‘ âˆ’1
ğ‘˜=0
ğ›¾ğ‘˜+1
ğ›¾ğ‘˜

âŸ¨ğ›¾ğœ‰
+ ğ›¾âˆ‡ğ‘“ (ğ‘¥) + âˆ‡â„(ğ‘¥+) âˆ’ âˆ‡â„(ğ‘¥), ğ‘¥+ âˆ’ ğ‘¢
â‰¤ 0.	(3.10)
Assuming a constant step size policy ğ›¾ğ‘˜ = ğ›¾, this gives us




tain for all ğ‘¢ âˆˆ ğ–·:
ğ‘Ÿ(ğ‘¥+) âˆ’ ğ‘Ÿ(ğ‘¢) â‰¤ âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥+
+ 1 âˆ‡â„(ğ‘¥+) âˆ’ âˆ‡â„(ğ‘¥), ğ‘¢ âˆ’ ğ‘¥+
.	(3.11)
ğ‘˜=0
ğ›¾  â„
ğ‘˜	ğ‘˜
ğ‘˜+1	ğ‘˜
ğ‘˜+1

âŸ¨	âŸ©	ğ›¾ âŸ¨
Define the function gap ğ‘ 
Î¨(ğ‘¥ğ‘˜)  0, and therefore
âˆ¶= Î¨(ğ‘¥ ) âˆ’ Î¨(ğ‘¢), then ğ‘ 
â€“ ğ‘ 
= Î¨(ğ‘¥	) âˆ’

For further analysis, we need the celebrated three-point identity, due to Chen and Teboulle (1993), whose simple proof we omit.
ğ‘ ğ‘ â‰¤  1
ğ‘ âˆ’1
ğ‘ ğ‘˜+1 =  
ğ‘ âˆ’1
ğ·â„ (ğ‘¢, ğ‘¥ )

Lemma 3.3 (3-point lemma). For all ğ‘¥, ğ‘¦ âˆˆ ğ–·â—¦ and ğ‘§ âˆˆ dom â„ we have
ğ‘ ğ‘˜=0
ğ‘ ğ‘˜=0
ğ‘ğ›¾

ğ· (ğ‘§, ğ‘¥) âˆ’ ğ· (ğ‘§, ğ‘¦) âˆ’ ğ· (ğ‘¦, ğ‘¥) = âˆ‡â„(ğ‘¥) âˆ’ âˆ‡â„(ğ‘¦), ğ‘¦ âˆ’ ğ‘§ .
for all ğ‘¢ âˆˆ dom â„ âˆ© ğ–·. As an attractive step size choice, we may take the

â„	â„	â„
â–¡
greedy choice ğ›¾	 ğ›¼ . However, we need to know the Lipschitz constant
ğ¿ğ‘“

Thanks to Lemma 3.3, relation (3.11) reads as
ğ›¾(ğ‘Ÿ(ğ‘¥+) âˆ’ ğ‘Ÿ(ğ‘¢)) â‰¤ ğ›¾ âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥+ + ğ· (ğ‘¢, ğ‘¥) âˆ’ ğ· (ğ‘¢, ğ‘¥+) âˆ’ ğ· (ğ‘¥+, ğ‘¥)
of the gradient map of the smooth part ğ‘“ of the minimization problem
(P) to make this an implementable solution strategy.
Proposition 3.4. Consider problem (P) with Assumptions 1-3 in place. Let

for all ğ‘¢ âˆˆ dom â„ âˆ© ğ–·.
ğ›¼
ğ‘¥âˆ— âˆˆ ğ–·âˆ—, we have
ğ‘˜	ğ¿ğ‘“

Remark 3.4. Note that if ğ‘¥+ is calculated inexactly in the sense that instead of (3.10), for some ğœ‰ âˆˆ ğœ•ğ‘Ÿ(ğ‘¥+), it holds that
Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨min
(ğ–·) â‰¤ ğ¿ğ‘“ ğ· (ğ‘¥âˆ—, ğ‘¥0).	(3.18)
ğ›¼ğ‘˜

âˆ‡ğ‘“ (ğ‘¥) + âˆ‡â„(ğ‘¥+) âˆ’ âˆ‡â„(ğ‘¥), ğ‘¥+ âˆ’ ğ‘¢
âŸ¨ğ›¾ğœ‰  ğ›¾
â‰¤ Î”	(3.13)
This global sublinear rate of convergence for the Euclidean setting is due to Beck and Teboulle (2009b); Nesterov (2013).



ğ›¾(ğ‘Ÿ(ğ‘¥+) âˆ’ ğ‘Ÿ(ğ‘¢)) â‰¤ ğ›¾ âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥+ + ğ· (ğ‘¢, ğ‘¥) âˆ’ ğ· (ğ‘¢, ğ‘¥+) âˆ’ ğ· (ğ‘¥+, ğ‘¥) + Î”.
strongly-convex with ğœ‡ > 0 it is possible to obtain linear convergence
rate of BPGM, i.e. Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨  (ğ–·)  2ğ¿ exp(âˆ’ğ‘˜ğœ‡âˆ•ğ¿ )ğ· (ğ‘¥âˆ—, ğ‘¥0).

(3.14)
See Auslender and Teboulle (2006b) for an explicit analysis of the error-
min	ğ‘“
Subgradient and Mirror Descent
ğ‘“	â„

prone implementation.
Assumption 1(a) gives rise to the the classical â€descent Lemmaâ€ (Nesterov, 2018b):
ğ‘“ (ğ‘¥+) â‰¤ ğ‘“ (ğ‘¥) + âˆ‡ğ‘“ (ğ‘¥), ğ‘¥+ âˆ’ ğ‘¥ + ğ¿ğ‘“  ğ‘¥+ âˆ’ ğ‘¥ 2.	(3.15)
2
Additionally, for all ğ‘¢ âˆˆ ğ–·, differential convexity of ğ‘“ on ğ–µ implies (cf.
(1.1))
ğ‘“ (ğ‘¢) â‰¥ ğ‘“ (ğ‘¥) + âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥ .	(3.16)
This allows us to bound
ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥+ = âˆ‡ğ‘“ (ğ‘¥), ğ‘¥ âˆ’ ğ‘¥+ + âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥
In the previous subsections we focused on the setting of problem
(P) with smooth part ğ‘“ and obtained for BPGM a convergence rate
ğ‘‚(1âˆ•ğ‘˜). The same method actually works for non-smooth convex op- timization problems when ğ‘“ has bounded subgradients In this setting BPGM with a different choice of the step-size (ğ›¾ğ‘˜ )ğ‘˜ is known as the Mir-
ror Descent (MD) method (Nemirovski and Yudin, 1983). A version of this method for convex composite non-smooth optimization was pro- posed in Duchi et al. (2010), and an overview of Subgradient/Mirror Descent type of methods for non-smooth problems can be found in Beck (2017); Dvurechensky et al. (2020b); Lan (2020). The main dif- ference between BPGM and MD is that one replaces the assumption that
âˆ‡ğ‘“ is Lipschitz continuous with the assumption that ğ‘“ is subdifferen-
tiable with bounded subgradients, i.e. ğ‘“ â€²(ğ‘¥)   ğ‘€  for all ğ‘¥ âˆˆ ğ–· and

2 â€–	â€–	âŸ¨
âŸ©	next test point as

(3â‰¤.16) ğ‘“ (ğ‘¢) âˆ’ ğ‘“ (ğ‘¥+) + ğ¿ğ‘“ ğ‘¥+ âˆ’ ğ‘¥ 2
ğ‘¥ğ‘˜+1 = argmin {âŸ¨ğ›¾ ğ‘“ â€²(ğ‘¥ğ‘˜), ğ‘¢ âˆ’ ğ‘¥ğ‘˜âŸ© + ğ›¾ ğ‘Ÿ(ğ‘¢) + ğ· (ğ‘¢, ğ‘¥ğ‘˜)} = P â„ (ğ‘¥ğ‘˜, ğ›¾ ğ‘“ â€²(ğ‘¥ğ‘˜)).



policy like ğ›¾ğ‘˜ âˆ¼ ğ‘˜âˆ’1âˆ•2. Under such a specification, the MD sequence (ğ‘¥ğ‘˜)ğ‘˜ A typical choice for the step size sequence is a monotonically decreasing can be shown to converge with rate ğ‘‚(1âˆ• ğ‘˜) to the solution, which
is optimal in this setting. A proof of this result can be patterned via a suitable adaption of the arguments employed in our analysis of the Dual Averaging Method in Section 3.4.

Potential Improvements due to relative smoothness
A key pillar of the complexity analysis of BPGM was the descent inequality (3.15), which is available thanks to the as-
sumed Lipschitz continuity of the gradient âˆ‡ğ‘“ . The influential work
Bauschke et al. (2016) introduced a very clever construction which al-
lows one to relax this restrictive assumption.2 The elegant observation
The first important observation is an extended version of the fundamen- tal inequality (3.17), which reads as
ğ›¾(Î¨(ğ‘¥+) âˆ’ Î¨(ğ‘¢)) â‰¤ ğ· (ğ‘¢, ğ‘¥) âˆ’ ğ· (ğ‘¢, ğ‘¥+) âˆ’ (1 âˆ’ ğ›¾ğ¿â„ )ğ· (ğ‘¥+, ğ‘¥) âˆ€ğ‘¢ âˆˆ dom â„ âˆ© ğ–·.	(3.21)
The derivation of this inequality is analogous to inequality (3.17), re- placing the Lipschitz-gradient-based descent inequality (3.15) by the
relative smoothness inequality (3.19) with parameter ğ¿â„ . The contin-
introduction of the symmetry coefficient of the DGF â„ as uation of the proof differs then in one important aspect. It relies on the
( ) âˆ¶= inf	ğ·â„(ğ‘¥, ğ‘¢) (	) âˆˆ  â—¦ Ã—  â—¦	â‰ 
ğ·â„(ğ‘¢, ğ‘¥)
The symmetry coeï¬ƒcient ğœˆ(â„) is confined to the interval [0,1], and
ğœˆ(â„) = 1 applies essentially only to the energy function â„(ğ‘¥) = 1 ğ‘¥ 2.

made in Bauschke et al. (2016) is that the Lipschitz-gradient-based de- scent lemma has the equivalent, but insightful, expression
( ğ¿ğ‘“ â€–ğ‘¥â€–2 âˆ’ ğ‘“ (ğ‘¥)) âˆ’ ( ğ¿ğ‘“ â€–ğ‘¢â€–2 âˆ’ ğ‘“ (ğ‘¢)) â‰¥ âŸ¨ğ¿ ğ‘¢ âˆ’ âˆ‡ğ‘“ (ğ‘¢), ğ‘¥ âˆ’ ğ‘¢âŸ©	âˆ€ğ‘¥, ğ‘¢ âˆˆ ğ–µ.
Choosing ğ›¾
ğ‘˜+1
= 1+ğœˆ(â„)
ğ‘“
, ğ‘¥+ =


ğ‘¥ğ‘˜+1
ğ‘˜
, ğ‘¥
= ğ‘¥


ğ‘˜ gives
ğ‘˜+1
1 âˆ’ ğœˆ(â„)

ğ‘˜+1
2 â€– â€–

ğ‘˜

This is just the gradient inequality for the convex function ğ‘¥ â†¦ ğ¿ğ‘“	2 âˆ’
Setting ğ‘¢ = ğ‘¥ğ‘˜ gives descent of the function value sequence (Î¨(ğ‘¥ğ‘˜)) â‰¥ .

ğ‘“ (ğ‘¥).
2 â€–ğ‘¥â€–
Moreover, it immediately follows that
ğ‘˜ 0

Dîˆ´efinition 3.5. The family of functions Â£(ğ–·) is the class of DGFs â„ âˆˆ
Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨(ğ‘¢) â‰¤ 	ğ‘“   (ğ· (ğ‘¢, ğ‘¥ğ‘˜âˆ’1) âˆ’ ğ· (ğ‘¢, ğ‘¥ğ‘˜))

0(ğ–·) which are of Legendre type: â„ essentially smooth and strictly con-	1 + ğœˆ(â„)	â„	â„

vex on int dom â„ with cl(dom â„) = ğ–·.
In this section we work in a Bregman proximal setting with Legendre type distance generating function.
Summing from ğ‘˜ = 1, 2, â€¦ , ğ‘ , the same argument as for the BPGM give
sublinear convergence of NoLips
2ğ¿â„

Assumption 4. ğ–· has nonempty interior and â„ âˆˆ Â£(ğ–·).
Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨(ğ‘¢) â‰¤
ğ‘“
ğ‘ (1 + ğœˆ(â„))
ğ·â„(ğ‘¢, ğ‘¥0).	(3.23)

Remark 3.6. For â„ âˆˆ Â£(ğ–·), it is true that ğ–·â—¦ âŠ† int(dom â„) âˆ© ğ–·.
Based on the general intuition we have gained while working with a general proximal setup, a very tempting and natural generalization is the following.
Definition  3.6  (Relative  Smoothness,  (Bauschke  et  al.,  2016;
Comparing the constants in the complexity estimates of NoLips and BPGM we see that the relative eï¬ƒciency of the two methods depends on
2ğ¿â„ âˆ•(1+ğœˆ(â„))
the relative condition number	ğ‘“	 . Hence, even if the objective
ğ¿ğ‘“ âˆ•ğ›¼
function is globally Lipschitz smooth (i.e. admits a Lipschitz continu-
ous gradient), exploiting the idea of relative smoothness might lead to superior performance of NoLips.

Van Nguyen, 2017)). The function ğ‘“ is smooth relative to â„ âˆˆ Â£(ğ–·), if
To establish global convergence of the trajectory (ğ‘¥ğ‘˜)
ğ‘˜âˆˆâ„•
, additional

there exists ğ¿â„ > 0 such that for any ğ‘¥, ğ‘¢ âˆˆ ğ–·â—¦
ğ‘“ (ğ‘¢) â‰¤ ğ‘“ (ğ‘¥) + âˆ‡ğ‘“ (ğ‘¥), ğ‘¢ âˆ’ ğ‘¥ + ğ¿â„ ğ· (ğ‘¢, ğ‘¥).	(3.19)
â€reciprocityâ€ conditions on the Bregman divergence must be imposed.
Assumption 6. The DGF â„ âˆˆ Â£(ğ–·) satisfies the Bregman reciprocity con-

ğ‘“  â„
Rearranging terms, a very concise and elegant way of writing the
dition: The level sets {ğ‘¢ âˆˆ ğ–·â—¦ ğ· (ğ‘¢, ğ‘¥) â‰¤ ğ›½} are bounded for all ğ›½ âˆˆ â„,
and ğ‘¥ğ‘˜ â†’ ğ‘¥ âˆˆ ğ–·â—¦ if and only if limğ‘˜â†’âˆ ğ·â„(ğ‘¥, ğ‘¥ğ‘˜) = 0.

relative smoothness condition is ğ·ğ¿â„ â„ ğ‘“
ğ‘“
(ğ‘¢, ğ‘¥) â‰¥ 0 on ğ–·â—¦. This amounts

This assumption is necessary, as in some settings Bregman reci-

to saying that ğ¿â„ â„ âˆ’ ğ‘“ is convex on ğ–·â—¦. Clearly, if ğ‘“ and â„ are twice
ğ‘“	â—¦	procity is violated. See Example 4.1 in Doljansky and Teboulle (1998) as

continuously differentiable on ğ–· , the relative smoothness condition can be stated in terms of a positive semi-definitness condition on the set ğ–·â—¦ as
ğ¿â„ âˆ‡2â„(ğ‘¥) âˆ’ âˆ‡2ğ‘“ (ğ‘¥) âª° 0   âˆ€ğ‘¥ âˆˆ ğ–·â—¦.	(3.20)
Beside providing a non-Euclidean version of the descent lemma, the no- tion of relative smoothness allows us to rigorously apply gradient meth- ods to problems whose smooth part admits no global Lipschitz continu- ous gradient. This gains relevance in solving various classes of inverse problems under Poisson noise (see Section 5.2 in Bauschke et al. (2016)), and optimal experimental design (Lu et al., 2018), a class of problems structurally equivalent to finding the minimum volume ellipsoid con- taining a list of vectors (Boyd and Vandenberghe, 2004; Todd, 2016).
Assumption 5. There exists a DGF â„ âˆˆ Â£(ğ–·) for which (ğ‘“, â„) is a rela-
a simple illustration. Under Bregman reciprocity, one can prove global convergence in the spirit of Opialâ€™s lemma (Opial, 1967):

1-6 hold. Let (ğ‘¥ğ‘˜)ğ‘˜âˆˆâ„• be the sequence generated by BPGM with the relatively Theorem 3.7 (Bauschke et al. (2016), Theorem 2). Suppose Assumptions smooth pair ğ‘“, â„ with ğ›¾  , 1+ğœˆ(â„) and â„  ğ–· . Then, the sequence
ğ¿â„
ğ‘“
(ğ‘¥ğ‘˜)ğ‘˜âˆˆâ„• converges to some solution ğ‘¥âˆ— âˆˆ ğ–·âˆ—.
Under additional assumption that ğ‘“ is ğœ‡â„ -relatively strongly convex (Lu et al., 2018) with ğœ‡â„ > 0, i.e. (cf. (3.20))
âˆ‡2ğ‘“ (ğ‘¥) âˆ’ ğœ‡â„ âˆ‡2â„(ğ‘¥) âª° 0   âˆ€ğ‘¥ âˆˆ ğ–·â—¦,	(3.24)
it is possible to obtain linear convergence rate of BPGM, i.e.
Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨min(ğ–·)  2ğ¿â„ exp(âˆ’ğ‘˜ğœ‡â„ âˆ•ğ¿â„ )ğ·â„(ğ‘¥âˆ—, ğ‘¥0). Stonyakin et al. (2020,

ğ‘“	ğ‘“	ğ‘“

tively smooth pair.

assumption on the pair (ğ‘“, â„) (the so called NoLips algorithm of The complexity analysis of BPGM under a relative smoothness
Bauschke et al. (2016)), proceeds analogous to the previous analysis.

2 Variations on the same theme can be found in Lu et al. (2018) and further de- velopments can be found in BÃ¹i and Combettes (2021); Stonyakin et al. (2020).
2019) show how to adapt the method to cope with inexact oracles and inexact Bregman proximal steps.

Dual Averaging

An alternative method called Dual Averaging (DA) was proposed in Nesterov (2009) and, on the contrary, is a primal-dual method making alternating updates in the space of gradients and in the space of iterates.



Below we give a self-contained complexity analysis of this scheme for
Rearranging and summing over ğ‘˜ = 1, 2, â€¦ , ğ‘ , we get

non-smooth optimization. The following assumptions shall be in place:
Assumption 7. ğ–· is a nonempty convex compact set.
Assumption 8. ğ‘Ÿ = 0 and ğ‘“ âˆˆ Î“0(ğ–µ) with ğ–· âŠ‚ dom(ğ‘“ ). Furthermore,
ğ‘
ğ‘˜	ğ»
ğ‘˜=1
Observe
ğ›½1
(ğ‘¦1) âˆ’ ğ»
ğ›½ğ‘ +1
ğ‘
(ğ‘¦ğ‘˜+1) +
ğ‘˜=1
2
ğ‘˜
2ğ›¼ğ›½ğ‘˜
â€–ğ‘“ ğ‘¥ â€–âˆ—

sup
ğ‘¥âˆˆğ–· â€–ğ‘“
â‰¤ ğ‘€
for all ğ‘“ â€²(ğ‘¥) âˆˆ ğœ•ğ‘“ (ğ‘¥) and some ğ‘€ğ‘“
âˆˆ (0, +âˆ).
ğ» (ğ‘¦1) = ğ» (âˆ’ğœ† ğ‘“ â€²(ğ‘¥0)) â‰¤ ğ» (0) + âˆ‡ğ» (0), âˆ’ğœ† ğ‘“ â€²(ğ‘¥0) +  0  ğ‘“ â€²(ğ‘¥0) 2
2

0  ğ›¼	= ğœ†0
â€²(ğ‘¥0) 2 â‰¤  ğœ†0
â€²(ğ‘¥0) 2 .

â„-center ğ‘¥ = argminğ‘¥âˆˆğ–· â„(ğ‘¥). Without loss of generality with assume
2ğ›¼ğ›½ â€–ğ‘“
â€–âˆ—  2ğ›¼ğ›½ â€–ğ‘“	â€–âˆ—

â„(ğ‘¥0) = 0. Denote by
Î˜â„(ğ–·) âˆ¶= max â„(ğ‘).	(3.25)
1
Therefore,
ğ‘
0
ğ‘	2

ğ‘âˆˆğ–·
âˆ‘ ğœ† âŸ¨ğ‘“ â€²(ğ‘¥ğ‘˜), ğ‘¥ğ‘˜ âˆ’ ğ‘¥0âŸ© â‰¤  1 âˆ‘ ğœ†ğ‘˜ â€–ğ‘“ â€²(ğ‘¥ğ‘˜)â€–2 âˆ’ ğ»
(ğ‘¦ğ‘+1).	(3.30)

function
(âˆ€ğ‘¦ âˆˆ ğ–µâˆ—) âˆ¶ ğ‘”(ğ‘¦) = max ğ‘¦, ğ‘¥ âˆ’ ğ‘¥0 ,	(3.26)
Note that
âˆ‘ğ‘
âˆ‘ğ‘
âˆ‘ğ‘


and
ğ‘˜=0
ğ‘˜âŸ¨	ğ‘¥
ğ‘˜=0
âˆ‘ğ‘
ğ‘˜âŸ¨
âŸ©  ğ‘˜=0
ğ‘˜âŸ¨	âŸ©

ğ›½	ğ‘¥âˆˆğ–· âŸ¨	âŸ©
ğ‘˜âŸ¨
âŸ©  âŸ¨	âŸ©

Also, observe that for ğ›½2
â‰¥ ğ›½1
> 0, it holds ğ»ğ›½
(ğ‘¦) â‰¤ ğ»
(ğ‘¦) for all ğ‘¦ âˆˆ ğ–µâˆ—.	ğ‘
ğ‘  2

2	1	âˆ‘ ğœ† âŸ¨ğ‘“ â€²(ğ‘¥ğ‘˜), ğ‘¥ğ‘˜ âˆ’ ğ‘¥âŸ© â‰¤  1 âˆ‘ ğœ†ğ‘˜ â€–ğ‘“ â€²(ğ‘¥ğ‘˜)â€–2 âˆ’ ğ»

(ğ‘¦ğ‘+1) + ğ‘¦ğ‘+1, ğ‘¥ âˆ’ ğ‘¥0	(3.31)

ğ›¼
ğ» (ğ‘¦) = max{ ğ‘¦, ğ‘¥ âˆ’ (â„ + ğ›¿ )(ğ‘¥)} âˆ’ ğ‘¦, ğ‘¥0
for all ğ‘¥ âˆˆ ğ–·. Defining ğœƒğ‘˜
âˆ¶= max
ğ‘¥âˆˆğ–·
ğ‘˜
ğ‘–=0
ğœ†	â€²( ğ‘– ), ğ‘¥ğ‘– âˆ’ ğ‘¥ , (3.31) gives

ğ›½	ğ‘¥âˆˆğ–µ âŸ¨	âŸ©
ğ–·	âŸ¨	âŸ©
the estimate

= ğ›½(â„ + ğ›¿ ) (ğ‘¦âˆ•ğ›½) âˆ’ ğ‘¦, ğ‘¥


ğœƒ  â‰¤  1 âˆ‘ ğœ†ğ‘˜ â€–ğ‘“ â€²(ğ‘¥ğ‘˜)â€–2 âˆ’ ğ»	(ğ‘¦ğ‘+1) + ğ‘”(ğ‘¦ğ‘+1).

ğ‘„ğ›½ (ğ‘¦) âˆ¶= argmax { ğ‘¦, ğ‘¥ âˆ’ ğ›½â„(ğ‘¥)}.	(3.28)
ğ‘¥âˆˆğ–·
A simple application of the min-max inequality shows
ğ‘”(ğ‘¦) = max ğ‘¦, ğ‘¥ âˆ’ ğ‘¥0

(âˆ€ğ‘¦ âˆˆ ğ–µâˆ—) âˆ¶ âˆ‡ğ»ğ›½ (ğ‘¦) = ğ‘„ğ›½ (ğ‘¦) âˆ’ ğ‘¥0.
ğ‘¥âˆˆğ–· ğ›½â‰¥0 âŸ¨	âŸ©	â„

1	â‰¤ min [(max{âŸ¨ğ‘¦, ğ‘¥ âˆ’ ğ‘¥0âŸ© âˆ’ ğ›½â„(ğ‘¥)) + ğ›½Î˜â„(ğ–·)]



DA performs a gradient step in the dual space ğ–µâˆ— to produce a new gradi- ent feedback point ğ‘¦+ = ğ‘¦ âˆ’ ğœ†ğ‘“ â€²(ğ‘¥), where ğœ† > 0 is a step size parameter.
for all ğ‘¦ âˆˆ ğ–µâˆ— and ğ›½ > 0. Hence, (3.31) gives

Taking this as a new signal, we update the primal state by applying the


ğœƒ  â‰¤
 1 âˆ‘ ğœ†ğ‘– â€– â€²
ğ‘˜  2

		Define Î›ğ‘ âˆ¶= âˆ‘ğ‘
ğœ†ğ‘˜ , and the ergodic average ğ‘¥Ì„ğ‘ âˆ¶= 1
ğ‘
ğ‘
ğ‘˜=0
ğœ†ğ‘˜ ğ‘¥ğ‘˜.

The Dual Averaging method (DA)
The subgradient inequality (1.1) applied to ğ‘“ gives

Input: pick ğ‘¦0 = 0, ğ‘¥0 = ğ‘„ğ›½ (0), nondecreasing learning sequence	ğ‘	ğ‘

(ğ›½ )
0
(ğœ† )
âˆ‘ ğœ† âŸ¨ğ‘“ â€²(ğ‘¥ğ‘˜), ğ‘¥ğ‘˜ âˆ’ ğ‘¥âŸ© â‰¥ âˆ‘ ğœ† ğ‘“ (ğ‘¥ğ‘˜) âˆ’ Î›


ğ‘“ (ğ‘¥) â‰¥ Î›
(ğ‘“ (ğ‘¥Ì„
) âˆ’ ğ‘“ (ğ‘¥)).

set ğ‘¥ğ‘˜+1 = ğ‘„ğ›½  (ğ‘¦ğ‘˜+1).
ğ‘	2

	ğ‘˜+1	
Î¨(ğ‘¥Ì„
) âˆ’ Î¨
(ğ–·) â‰¤ ğ›½ğ‘ +1 Î˜ (ğ–·) +   1   âˆ‘ ğœ†ğ‘˜ â€–
â€²( ğ‘˜ ) 2

The function ğ‘¦ â†¦ ğ» (ğ‘¦) is convex and âˆšcontinuously differentiable
Let us now make the concrete choice of parameters ğ›½ğ‘˜ = ğ›½ > 0 and ğœ†ğ‘˜ =
âˆšğ‘˜+1 for all ğ‘˜  0. Then,

with
ğ» (ğ‘¦ + ğ‘¤) â‰¤ ğ» (ğ‘¦) + âˆ‡ğ» (ğ‘¦), ğ‘¤ +  1
ğ‘¤ 2
âˆ€ğ‘¦, ğ‘¤ âˆˆ ğ–µâˆ—.	(3.29)
ğ‘
ğ‘	âˆš
â‰¥	ğ‘ +1	1
âˆš
ğ‘‘ğ‘¥ â‰¥ âˆšğ‘ + 1,

ğ›½	ğ›½
âŸ¨	ğ›½
âŸ©	2ğ›¼ğ›½ â€– â€–âˆ—
ğ‘˜=0
ğ‘˜ + 1	0
ğ‘¥ + 1

Thanks to the monotonicity in the parameters, we get through some elementary manipulations the relation


as well as
âˆ‘ ğœ†ğ‘˜ = 1 âˆ‘  1  â‰¤ 1 + ln(ğ‘ + 1) .

â‰¤	(	âˆ‡	(  ) âˆ’	(  ) + ğœ†2	(  )
Theorem 3.8. Suppose Assumptions 3, 7 and Assumption 8 hold true. Let

ğ›½ğ‘˜
âŸ¨ ğ»ğ›½ğ‘˜ ğ‘¦ ,
ğœ†ğ‘˜ ğ‘“ ğ‘¥ âŸ©
2ğ›¼ğ›½ â€–ğ‘“ ğ‘¥ â€–âˆ—
(ğ‘¥ )ğ‘˜ be generated by DA with parameters ğ›½ğ‘˜ = ğ›½ > 0 and ğœ†ğ‘˜ = âˆšğ‘˜+1 . Then,

= ğ»
(ğ‘¦ğ‘˜) âˆ’ ğœ†
ğœ†2
ğ‘¥ğ‘˜ âˆ’ ğ‘¥0, ğ‘“ â€²(ğ‘¥ğ‘˜) +	ğ‘˜ 
ğ‘“ â€²(ğ‘¥ğ‘˜) 2 .
â‰¤  ğ›½Î˜â„(ğ–·)
ğ‘€ 2 (1 + ln(ğ‘ + 1))

ğ›½ğ‘˜
ğ‘˜âŸ¨
âŸ©	2ğ›¼ğ›½ğ‘˜ â€–	â€–âˆ—
Î¨(ğ‘¥Ì„ğ‘ ) âˆ’ Î¨min(ğ–·)
âˆšğ‘ + 1
2ğ›¼ğ›½
âˆšğ‘ + 1
(3.32)



fixed time window ğ‘˜ âˆˆ {0, 1, â€¦ , ğ‘ }. Committing over this time interval A slightly better bound can be obtained if we decide to run DA over a on the constant parameter sequences ğ›½ğ‘˜ = ğ›½ > 0 and ğœ†ğ‘˜ = ğœ† > 0 yields
 Î˜ ( )		
Assuming that the penalty function â„ is of Legendre type, the primal
projection step is seen to be the regularized maximization step
ğ‘¥ğ‘˜ = argmax{ ğ‘¦ğ‘˜, ğ‘¢ âˆ’ ğ›½ â„(ğ‘¢)} â‡” ğ‘¦ğ‘˜ = ğ›½ âˆ‡â„(ğ‘¥ğ‘˜).

ğ‘	min
(ğ‘ + 1)ğœ†
2ğ›¼ ğ›½ ğ‘€ğ‘“ .
ğ›½ âˆš 2ğ›¼Î˜ (ğ–·)
Using the definition of the dual trajectory, we see that for all ğ‘˜ â‰¥ 0 the
primal-dual relation obeys:



the complexity upper bound
âˆš  2Î˜ ( )
Assuming that ğ›½ğ‘˜
â‰¡ 1, this implies

Î¨(ğ‘¥Ì„ğ‘ ) âˆ’ Î¨min(ğ–·)
ğ›¼(ğ‘ + 1) ğ‘€ğ‘“ .
ğ‘¢âˆˆğ–·
âŸ¨ ğ‘˜
âŸ©	â„	ğ›¿ğ–·	ğ‘˜

The eï¬€ectiveness of non-Euclidean setups With the help of the explicit rate estimate (3.32) we are now in the position to evaluate the potential
We will do so by focusing on the geometry ğ–· = {ğ‘¥ âˆˆ â„ğ‘›	ğ‘¥ğ‘– = 1}. eï¬ƒciency gains we can make by adopting the non-Euclidean framework.


We have thus shown that DA and BPGM/MD agree if all parameters and initial conditions are chosen in the same way.

3.4.2. Links to continuous-time dynamical systems




Consider the ğ“ setup in which	â‹…  =  â‹…	and â„(ğ‘¥) = 1 ğ‘¥ 2 âˆ’
in the literature (see e.g. Helmke and Moore (1996) for a textbook

2
1	0
â€– 0â€–  â€– â€–2	2 â€– â€–2
reference). Here we describe an interesting link between dual av-

Î˜â„(ğ–·) = ğ‘›âˆ’1 . We denote by ğ‘€ğ‘“, â‹…
the bound on the subgradients

2ğ‘›
â€– â€–2
duced in Alvarez et al. (2004); Attouch et al. (2004); Attouch and

of the function ğ‘“ under the ğ“2-norm. The corresponding complexity
estimate is
Teboulle (2004) and further studied in Bolte and Teboulle (2003). A complexity analysis of discretized versions of these gradient flows

Complexity(ğ–·, â€– â‹… â€–2) = âˆš (ğ‘› âˆ’ 1)âˆ•ğ‘› ğ‘€
ğ‘“,â€–â‹…â€–2
has recently been obtained in Bomze et al. (2019). Our point of departure is the following continuous-time dynamical system based on dual averaging, which has been introduced in Mertikopoulos and

A different sensible projection framework is obtained by consider the
ğ“1 -norm  â‹…	1	ğ‘–	ğ‘–
diameter is Î˜â„(ğ–·) = ln(ğ‘›). We let ğ‘€ğ‘“, â‹…	denote the bound on the
Staudigl (2018a) in the context of convex programming and in Mertikopoulos and Staudigl (2018b) for general monotone variational inequality problems. The main ingredient of this dynamical system is a

subgradients of the function ğ‘“
âˆ
under the dual norm
ğ“âˆ
. The corre-
pair of primal-dual trajectories (ğ‘¥(ğ‘¡), ğ‘¦(ğ‘¡))ğ‘¡â‰¥0 evolving in continuous time

sponding complexity estimate is
âˆš 2 ln(ğ‘›)
according to the differential-projection system
{ğ‘¦â€²(ğ‘¡) âˆ¶= ğ‘‘ğ‘¦(ğ‘¡) = âˆ’ğœ†(ğ‘¡)âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)),

Complexity(ğ–·,
â‹… â€–1
ğ›¼(ğ‘ + 1)
ğ‘“,â€–â‹…â€–âˆ
ğ‘‘ğ‘¡
ğ‘¥(ğ‘¡) = ğ‘„1(ğœ‚(ğ‘¡)ğ‘¦(ğ‘¡)) =âˆ¶ ğ‘„(ğœ‚(ğ‘¡)ğ‘¦(ğ‘¡)).
(3.34)

To compare the complexity estimates implied by the two different Bregman setups, we compute the eï¬ƒciency ratio
Complexity(ğ–·, â‹… )	âˆš	â€– â€–
In this formulation, Assumption 1(a) is in place, in order to ensure that the dynamical system is well-posed, thanks to the Picard-LindelÃ¶f theorem. To relate this scheme formally to its discrete-time counter-

Complexity(ğ–·,
â‹… â€–1
2ğ‘› ln(ğ‘›) ğ‘€
ğ‘“,â€–â‹…â€–âˆ
by ğ‘¦ğ‘˜
â€“ ğ‘¦
ğ‘˜âˆ’1
= âˆ’ğœ†ğ‘˜ âˆ‡ğ‘“ (ğ‘¥ğ‘˜), and project the resulting point to the pri-

ğ‘ âˆ	2
This implies
â‰¤ âˆšğ‘›â€–ğ‘â€–âˆ
ğ‘€ğ‘“, â‹…
ğ‘  â„
ğ‘€ğ‘“, â‹…
â‰¤ âˆšğ‘›.
discrete-time learning rate appropriately sampled from the function ğœ‚(ğ‘¡).
Legendre   function   â„   âˆˆ        (ğ–·),   so   that As in Section 3.4.1, let us assume that the mirror map is generated by a

ğ‘› âˆ’ 1 2ğ‘› ln(ğ‘›)
â‰¤	â‰¤		ğ‘› âˆ’ 1 2 ln(ğ‘›)
ğ‘¥(ğ‘¡) = âˆ‡â„âˆ—(ğœ‚(ğ‘¡)ğ‘¦(ğ‘¡)).
Let us further assume that â„ is twice continuously differentiable and

The closer the eï¬ƒciency ratio ğ‘… gets to the upper bound, the more fa- vorable the ğ“1-setup would be compared to the standard ğ“2-setup. This
imentally verify the significant advantages of the ğ“1 setup for large- is not an unrealistic situation in practice. Ben-Tal et al. (2001) exper-
dimensional simplex domains.

On the connection between Dual Averaging and Mirror Descent
A deep and important connection between the Dual Averaging and Mirror Descent algorithms for convex non-smooth optimization has been observed in Beck and Teboulle (2003). To relate these iterates to BPGM,
we assume that â„ âˆˆ  (ğ–·), in the sense of Definition 3.5. Let us recall
that â„ is essentially smooth if and only if its Fenchel conjugate â„âˆ— is es- sentially smooth. Moreover, âˆ‡â„ âˆ¶ int(dom â„) â†’ int(dom â„âˆ—) is a bijection
with
(âˆ‡â„)âˆ’1 = âˆ‡â„âˆ— and âˆ‡â„âˆ—(âˆ‡â„(ğ‘¥)) = ğ‘¥, âˆ‡â„(ğ‘¥) âˆ’ â„(ğ‘¥)	(3.33)
Since ğ–· = cl(dom â„), it follows
dom ğœ•â„ = int(dom â„) = int(ğ–·) with ğœ•â„(ğ‘¥) = {âˆ‡â„(ğ‘¥)}  âˆ€ğ‘¥ âˆˆ int(ğ–·).
ğœ‚(ğ‘¡) â‰¡ 1. Differentiating the previous equation with respect to time ğ‘¡
gives
ğ‘¥â€²(ğ‘¡) = âˆ‡2 â„âˆ—(ğ‘¦(ğ‘¡))ğ‘¦â€²(ğ‘¡) = âˆ’ğœ†(ğ‘¡)âˆ‡2 â„âˆ—(ğ‘¦(ğ‘¡))âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)).
ing âˆ‡â„âˆ—(âˆ‡â„(ğ‘¥)) = ğ‘¥ for all ğ‘¥ âˆˆ int dom â„ (cf. (3.33)). Differentiating im- To make headway, recall the basic properties of Legendre function say- plicitly this identity, we obtain âˆ‡2 â„âˆ—(âˆ‡â„(ğ‘¥))  Id, or
âˆ‡2 â„âˆ—(âˆ‡â„(ğ‘¥)) = [âˆ‡2 â„(ğ‘¥)]âˆ’1 =âˆ¶ ğ» (ğ‘¥)âˆ’1 .	(3.35)
As in Section 3.4.1, it holds true that ğ‘¦(ğ‘¡) = âˆ‡â„(ğ‘¥(ğ‘¡)) for all ğ‘¡ â‰¥ 0, we
therefore obtain the interesting characterization of the primal trajectory
as
ğ‘¥â€²(ğ‘¡) = âˆ’ğœ†(ğ‘¡)ğ» (ğ‘¥(ğ‘¡))âˆ’1 âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)).
If ğ–· is a smooth manifold, we can define a Riemannian metric
ğ‘” (ğ‘¢, ğ‘£) âˆ¶= ğ» (ğ‘¥)ğ‘¢, ğ‘£	âˆ€(ğ‘¥, ğ‘¢, ğ‘£) âˆˆ ğ–·â—¦ Ã— ğ–µ Ã— ğ–µ.
The gradient of a smooth function ğœ™ with respect to the metric ğ‘” is then given by âˆ‡ğ‘” ğœ™(ğ‘¥) = ğ» (ğ‘¥)âˆ’1 âˆ‡ğœ™(ğ‘¥). Hence, the continuous-time version of



the dual averaging method gives rise the class of primal Riemannian- Hessian gradient flows
ğ‘¥â€²(ğ‘¡) + ğœ†(ğ‘¡)âˆ‡ğ‘” ğ‘“ (ğ‘¥(ğ‘¡)) = 0,  ğ‘¥(0) âˆˆ ğ–·â—¦.	(3.36)
for a given bounded linear operator ğ€. To streamline the presentation, we directly assume in this section that ğ–µ = ğ–µâˆ— = â„ğ‘›, and the underly- ing metric structure is generated by the Euclidean norm ğ‘   ğ‘  =


literature in connection with Nesterovâ€™s optimal method, which we will thoroughly discuss in Section 6. As an appetizer, consider the system of differential equations
ğ‘¦â€²(ğ‘¡) = âˆ’ğœ†(ğ‘¡)âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)),  ğ‘¥â€²(ğ‘¡) = ğ›¾(ğ‘¡)[ğ‘„(ğœ‚(ğ‘¡)ğ‘¦(ğ‘¡)) âˆ’ ğ‘¥(ğ‘¡)].	(3.37)
problem can be equivalently written as
inf {Î¦(ğ‘¥, ğ‘§) = ğ‘”(ğ‘§) + ğ‘Ÿ(ğ‘¥) ğ€ğ‘¥ âˆ’ ğ‘§ = 0, ğ‘¥ âˆˆ ğ–·, ğ‘§ âˆˆ ğ–¹},	(4.1)
where ğ–· = â„ğ‘› and ğ–¹ = â„ğ‘š. We will call this the primal problem. The
Lagrangian associated to (4.1) is
ğ¿(ğ‘¥, ğ‘§, ğ‘¦) = ğ‘”(ğ‘§) + ğ‘Ÿ(ğ‘¥) + ğ‘¦, ğ€ğ‘¥ âˆ’ ğ‘§ ,

Suppose that in (3.37) we take ğ‘„(ğ‘¦) = ğ‘¦, ğœ‚(ğ‘¡) = 1. This corresponds to the	âŸ¨	âŸ©

2 â€– â€–2	ğ–·
where ğ‘¦ âˆˆ â„ is the Lagrange multiplier associated with the linear con-

Under this specification, the dynamical system (3.37) becomes
ğ‘¦â€²(ğ‘¡) = âˆ’ğœ†(ğ‘¡)âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)),  ğ‘¥â€²(ğ‘¡) = ğ›¾(ğ‘¡)[ğ‘¦(ğ‘¡) âˆ’ ğ‘¥(ğ‘¡)].
straint. The dual function is accordingly defined as
ğ‘(ğ‘¦) = inf ğ¿(ğ‘¥, ğ‘§, ğ‘¦) = inf {ğ‘”(ğ‘§) âˆ’ ğ‘¦, ğ‘§ } + inf {ğ‘Ÿ(ğ‘¥) + ğ€ğ‘¥, ğ‘¦ }

Combining the primal and the dual trajectory, we easily derive a purely primal second-order in time dynamical system given by
ğ‘¥,ğ‘§
ğ‘§  âŸ¨	âŸ©
ğ‘§	âŸ¨	âŸ©	ğ‘¥	âŸ¨	âŸ©
ğ‘¥  âŸ¨	âŸ©

â€²â€²	â€²
( ğ›¾(ğ‘¡)2 âˆ’ ğ›¾â€²(ğ‘¡) )
= âˆ’ğ‘”âˆ—(ğ‘¦) âˆ’ ğ‘Ÿâˆ—(âˆ’ğ€âŠ¤ğ‘¦).

Setting ğ›¾(ğ‘¡) = ğ›½âˆ•ğ‘¡ and ğœ†(ğ‘¡) = 1âˆ•ğ›¾(ğ‘¡) and rearranging gives

ğ‘¥â€²â€²(ğ‘¡) +
ğ›½ + 1
ğ‘¡
ğ‘¥â€²(ğ‘¡) + âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)) = 0,
min{Î¨Ìƒ (ğ‘¦) = ğ‘”âˆ—(ğ‘¦) + ğ‘Ÿâˆ—(âˆ’ğ€âŠ¤ğ‘¦)}	(4.2)
ğ‘¦

method of Polyak Polyak (1964). For ğ›½ = 2 this gives the continuous- which corresponds to the continuous-time version of the Heavy-ball
time formulation of Nesterovâ€™s accelerated scheme, as shown by Su et al. (2016).
More generally, suppose that â„ is a twice continuously differentiable
Legendre function and ğœ‚(ğ‘¡)  1. Then a direct calculation shows that
ğ‘¥â€²â€²(ğ‘¡) + (ğ›¾(ğ‘¡) âˆ’ ğ›¾â€²(ğ‘¡) )ğ‘¥â€²(ğ‘¡) + ğ›¾(ğ‘¡)ğœ†(ğ‘¡)(âˆ‡2 â„)âˆ’1 (ğ‘„(ğ‘¦(ğ‘¡)))âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)) = 0.
Using the identity (3.35), as well as ğ‘¥â€²(ğ‘¡) + ğ‘¥(ğ‘¡) = âˆ‡â„âˆ—(ğ‘¦(ğ‘¡)), it follows
that
A classical implicit method for solving this minimization problem is the
proximal point method:
ğ‘¦ğ‘˜+1 âˆˆ argmin{Î¨Ìƒ (ğ‘¦) +  1 ğ‘¦ âˆ’ ğ‘¦ğ‘˜ 2}	(4.3)
ğ‘¦	2ğ‘
where ğ‘ > 0 is a regularization parameter controlling the effects of the quadratic penalty term. By Fermatâ€™s optimality condition, the point ğ‘¦ğ‘˜+1
satisfied the monotone inclusion
0 âˆˆ ğœ•ğ‘”âˆ—(ğ‘¦ğ‘˜+1) âˆ’ ğ€ğœ•ğ‘Ÿâˆ—(âˆ’ğ€âŠ¤ğ‘¦ğ‘˜+1) + 1 (ğ‘¦ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜).
ğ‘
This means that there exists ğ‘¥ğ‘˜+1 âˆˆ ğœ•ğ‘Ÿâˆ—(âˆ’ğ€âŠ¤ğ‘¦ğ‘˜+1) and ğ‘§ğ‘˜+1 âˆˆ ğœ•ğ‘”âˆ—(ğ‘¦ğ‘˜+1)
such that

âˆ‡2 â„(ğ‘¥(ğ‘¡) + ğ‘¥â€²(ğ‘¡) )( ğ‘¥â€²â€²(ğ‘¡) + (1 âˆ’ ğ›¾â€²(ğ‘¡) )ğ‘¥â€²(ğ‘¡)) = âˆ’ğœ†(ğ‘¡)âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)) â‡” ğ‘‘ âˆ‡â„(ğ‘¥(ğ‘¡) + ğ‘¥â€²(ğ‘¡) )
+1	+1	1	+1

ğ›¾(ğ‘¡)
ğ›¾(ğ‘¡)
ğ›¾(ğ‘¡)2
ğ‘‘ğ‘¡
ğ›¾(ğ‘¡)
0 = ğ‘§ğ‘˜
â€“ ğ€ğ‘¥ğ‘˜
+  (ğ‘¦ğ‘˜
ğ‘
â€“ ğ‘¦ğ‘˜).

= âˆ’ğœ†(ğ‘¡)âˆ‡ğ‘“ (ğ‘¥(ğ‘¡)).
This shows that for ğœ‚ â‰¡ 1, the dynamic coincides with the Lagrangian
family of second-order systems constructed in Wibisono et al. (2016).
This means that the proximal point method can be implemented for the given instance as the implicit method
ğ‘¥ğ‘˜+1 âˆˆ argmin{ğ‘Ÿ(ğ‘¥) + ğ€ğ‘¥, ğ‘¦ğ‘˜+1 },

These ideas are now investigated heavily when combined with numer- ical discretization schemes for dynamical system with the hope to get insights how to construct new and more eï¬ƒcient algorithmic formula-
ğ‘¥
ğ‘§ğ‘˜+1 âˆˆ argmin{ğ‘”(ğ‘§) +
ğ‘§
â€“ ğ‘§, ğ‘¦ğ‘˜+1âŸ© }
âŸ©

tion of gradient-methods. This literature grew quite fastly over the last years, and we mention (Attouch et al., 2020; 2018; Bah et al., 2019; Shi et al., 2019).
The Proximal Method of Multipliers and ADMM

In this section we turn our attention to a classical method for solv- ing linearly constrained optimization problems building on the classi- cal idea of the celebrated method of multipliers. An extremely powerful proponent of this class of algorithms is the Alternating Direction Method of Multipliers (ADMM), which has received enormous interest from dif- ferent directions, including PDEs (Attouch et al., 2011; 2007), mixed- integer programming (Feizollahi et al., 2017), optimal control (Lin et al., 2012) and signal processing (Yang and Zhang, 2011; Yuan, 2012). The very influential monograph (Boyd et al., 2011) contains over 180 refer- ences, reflecting the deep impact of alternating methods on optimization theory and its applications. Following the general spirit of this survey, we introduce alternating direction methods in a proximal framework,
ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1).
erates ğ‘¥ğ‘˜+1, ğ‘§ğ‘˜+1, ğ‘¦ğ‘˜+1 simultaneously. Of course, this does not give rise This defines a fully implicit iteration, which requires to compute the it-
to a practical algorithm. The main idea behind alternating methods is to organize the computations in a Gauss-Seidel kind of iterations in which the sequences are updated sequentially using the most recent informa-
tion available. To set the stage, observe that (ğ‘¥ğ‘˜+1, ğ‘§ğ‘˜+1) defined above
is the coordinate-wise minimum of the function
ğ¹ (ğ‘¥, ğ‘§, ğ‘¦ğ‘˜) = ğ‘”(ğ‘§) + ğ‘Ÿ(ğ‘¥) + ğ‘ ğ€ğ‘¥ âˆ’ ğ‘§ + 1 ğ‘¦ğ‘˜ 2.
2	ğ‘
In that sense, the proximal point method applied to the dual can be represented more compactly as
(ğ‘¥ğ‘˜+1, ğ‘§ğ‘˜+1) âˆˆ argmin ğ¹ (ğ‘¥, ğ‘§, ğ‘¦ğ‘˜), ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1).
(ğ‘¥,ğ‘§)
This scheme is known as the augmented Lagrangian method.
Observe that minimizers of the function ğ¹ (ğ‘¥, ğ‘§, ğ‘¦ğ‘˜) with respect to
(ğ‘¥, ğ‘§) agree with the minimizers of the function

as pioneered by Rockafellar Rockafellar (1976a,b), and due to Shefi and
ğ¿ (ğ‘¥, ğ‘§, ğ‘¦ğ‘˜) = ğ‘”(ğ‘§) + ğ‘Ÿ(ğ‘¥) + ğ‘
ğ€ğ‘¥ âˆ’ ğ‘§ + 1 ğ‘¦ğ‘˜ 2,

Teboulle (2014). See also Banert et al. (2021) for some further important	ğ‘
2 â€–	ğ‘  â€–

elaborations.
To set the stage, consider the composite convex optimization prob- lem (P), in its special form (2.7):
Î¨(ğ‘¥) = ğ‘”(ğ€ğ‘¥) + ğ‘Ÿ(ğ‘¥),
which is known as the augmented Lagrangian of problem (4.1). Using the augmented Lagrangian, an alternating minimization procedure build- ing on the proximal point idea gives rise to the celebrated Alternating Direction of Method of Multipliers (ADMM).




Input: pick (ğ‘§0, ğ‘¦0) âˆˆ ğ–¹ Ã— â„ğ‘š and penalty parameter ğ‘ > 0; The Alternating Direction of Method of Multipliers (ADMM) General step: For ğ‘˜ = 0, 1, â€¦ do:
The coupling between primal and dual variables reads as
ğ‘¦ğ‘˜+1 = ğ‘¢ğ‘˜+1 âˆ’ ğ‘ğ‘§ğ‘˜+1 .
Combining all these relations, we can write the dual minimization prob-

ğ‘¥ğ‘˜+1
= argminğ‘¥âˆˆğ–·{ğ‘Ÿ(ğ‘¥) +
ğ€ğ‘¥ âˆ’ ğ‘§ğ‘˜ +
1 ğ‘¦
ğ‘˜ 2
}	(4.4)
lem as
ğ‘˜+1
ğ‘	ğ‘˜
1 ğ‘˜ 2

ğ‘§ğ‘˜+1
= argmin
{ğ‘”(ğ‘§) +
2
ğ‘ ğ€ğ‘¥
ğ‘˜+1
ğ‘
â€“ ğ‘§ +
2
1 ğ‘¦ğ‘˜ 2
}	(4.5)
ğ‘¥	= argmin{ğ‘Ÿ(ğ‘¥) +	ğ€ğ‘¥ âˆ’ ğ‘§
ğ‘¥	2
+  ğ‘¦	},
ğ‘

ğ‘§âˆˆğ–¹	2 â€–
ğ‘  â€–2
ğ‘§ğ‘˜+1 = argmin{ğ‘”(ğ‘§) + ğ‘ ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ + 1 ğ‘¦ğ‘˜ 2},



Remark 4.1.  ADMM updates the decision variables in a sequential man-
trix ğ€ is of special structure. In the context of the AC optimal power ner, and thus is not capable of featuring parallel updates unless the ma-
flow problem in electric power grid optimization (Sun et al., 2013) pro- vide such a modification of ADMM. Furthermore, the ADMM can be
form ğ€1ğ‘¥ + ğ€2ğ‘§ = ğ‘. For ease of exposition we stick to the simplified extended to consider formulations with general linear constraints of the
problem formulation above.

The Douglas-Rachford algorithm and ADMM

The Douglas-Rachford (DR) algorithm is a fundamental method to solve general monotone inclusion problems where the task is to find ze- ros of the sum of two maximally monotone operators (see Bauschke and Combettes (2016) and Auslender and Teboulle (2006a)). To keep the fo- cus on convex programming, we introduce this method for solving the
dual problem (4.2). To that end, let us define the matrix ğŠ = âˆ’ğ€âŠ¤, so
that our aim is to solve the convex programming problem
ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1)
which is just the standard ADMM. By this we have recovered a classi- cal result on connection between the DR and ADMM algorithms due to Gabay (1983) and Eckstein and Bertsekas (1992).
Proximal Variant of ADMM

the term ğ€ğ‘¥ in the update of ğ‘¥ğ‘˜+1. The presence of this factor makes One of the limitations of the ADMM comes from the presence of
it impossible to implement the algorithm in parallel, which makes it slightly unattractive for large-scale problems in distributed optimiza- tion. Moreover, due to the result of Chen et al. (2016) the conver- gence of ADMM for general linear constraints does not generalize to more than two blocks. Leaving parallelization issues aside, Shefi and Teboulle Shefi and Teboulle (2014) proposed an interesting extension of the ADMM by adding further quadratic penalty terms, which allows much flexibility by suitably choosing the norms employed in the algo-
rithm. Given some point (ğ‘¥ğ‘˜, ğ‘§ğ‘˜, ğ‘¦ğ‘˜) âˆˆ ğ–· Ã— ğ–¹ Ã— â„ğ‘š and two positive def-
inite matrices ğŒ1, ğŒ2, we define the proximal augmented Lagrangian of
(4.1) as
ğ‘ƒ (ğ‘¥, ğ‘§, ğ‘¦) = ğ¿ (ğ‘¥, ğ‘§, ğ‘¦) + 1 ğ‘¥ âˆ’ ğ‘¥ğ‘˜ 2  + 1 ğ‘§ âˆ’ ğ‘§ğ‘˜ 2 .	(4.12)

min ğ‘”âˆ—(ğ‘§) + ğ‘Ÿâˆ—(ğŠğ‘§).	(4.7)	ğ‘˜
ğ‘§
ğ‘	2 â€–
â€–ğŒ1	2 â€–
â€–ğŒ2

Any solution ğ‘§Ì„ âˆˆ dom(ğ‘Ÿâˆ—) satisfies the monotone inclusion
0 âˆˆ ğŠâŠ¤ğœ•ğ‘Ÿâˆ—(ğŠğ‘§Ì„) + ğœ•ğ‘”âˆ—(ğ‘§Ì„).	(4.8)
The DR algorithm aims to determine such a point ğ‘§Ì„ by iteratively con- structing a sequence {(ğ‘¢ğ‘˜, ğ‘£ğ‘˜, ğ‘¦ğ‘˜), ğ‘˜  0} determined by
ğ‘£ğ‘˜+1 = (Id +ğ‘ğŠâŠ¤â—¦ğœ•ğ‘Ÿâˆ—â—¦ğŠ)âˆ’1 (2ğ‘¦ğ‘˜ âˆ’ ğ‘¢ğ‘˜),
ğ‘¢ğ‘˜+1 = ğ‘£ğ‘˜+1 + ğ‘¢ğ‘˜ âˆ’ ğ‘¦ğ‘˜ ,
ğŒ, which is a norm
if ğŒ is positive definite.


The Alternating Direction proximal Method of Multipliers (AD-PMM)
Input: pick (ğ‘¥0, ğ‘§0, ğ‘¦0) âˆˆ ğ–· Ã— ğ–¹ Ã— â„ğ‘š and penalty parameter ğ‘ > 0;
General step: For ğ‘˜ = 0, 1, â€¦ do:

ğ‘¦ğ‘˜+1 = (Id +ğ‘ğœ•ğ‘”âˆ—)âˆ’1 (ğ‘¢ğ‘˜+1).
ğ‘¥ğ‘˜+1 = argmin
{ğ‘Ÿ(ğ‘¥) + ğ‘
ğ€ğ‘¥ âˆ’ ğ‘§ğ‘˜ + 1 ğ‘¦ğ‘˜ 2 + 1
ğ‘¥ âˆ’ ğ‘¥ğ‘˜ 2
} (4.13)

ğ‘¥âˆˆğ–·	2 â€–
ğ‘  â€–2	2 â€–
â€–ğŒ1

the ğ‘¦ğ‘˜+1 update, which reads as the inclusion
ğ‘§âˆˆğ–¹	2 â€–
ğ‘  â€–2	2 â€–
â€–ğŒ2

0 âˆˆ 1 (ğ‘¦ğ‘˜+1
â€“ ğ‘¢
ğ‘˜+1
) + ğœ•ğ‘”âˆ—(ğ‘¦
ğ‘˜+1 ).
ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1)  (4.15).

ğ‘
This is clearly recognizable as the first-order optimality condition of
the min {ğ‘”âˆ—(ğ‘¦) +  1 ğ‘¦ âˆ’ ğ‘¢ğ‘˜+1 2}. Therefore, we can rewrite the above

ğ‘¦	2ğ‘	2
iteration in terms of convex optimization subroutines as:
ğ‘£ğ‘˜+1 = argmin{ğ‘Ÿâˆ—(ğŠğ‘£) + 1  ğ‘£ âˆ’ (2ğ‘¦ğ‘˜ âˆ’ ğ‘¢ğ‘˜) 2 },	(4.9)
ğ‘£	2ğ‘
ğ‘¢ğ‘˜+1 = ğ‘£ğ‘˜+1 + ğ‘¢ğ‘˜ âˆ’ ğ‘¤ğ‘˜,	(4.10)
ğ‘¦ğ‘˜+1 = argmin{ğ‘”âˆ—(ğ‘¦) +  1 ğ‘¦ âˆ’ ğ‘¢ğ‘˜+1 2}.	(4.11)
ğ‘¦	2ğ‘

AD-PMM allows for various choices of the matrices ğŒ1, ğŒ2.
With ğŒ1 = ğŒ2 = 0, we recover the classical ADMM. For any ğ‘ > 0,
it is known (Gabay, 1983; Glowinski and Tallec, 1989) that conver-
gence in function values as well as global convergence to to dual multiplier are warranted. To ensure convergence of the primal se-
quence (ğ‘¥ğ‘˜)ğ‘˜, one needs to assume that ğ€ has full column rank.
With the choice ğŒ1 = ğœ‡1 ğˆğ‘›, ğŒ2 = ğœ‡2 ğˆğ‘š with ğœ‡1 , ğœ‡2 > 0, the AD-PMM
of (Eckstein, 1994) is recovered.
We give a brief analysis of the complexity of AD-PMM in the special




ğ‘¥ğ‘˜+1
= argmin{ğ‘Ÿ(ğ‘¥) + ğ‘
ğ€ğ‘¥ +
1 (2ğ‘¦ğ‘˜
â€“ ğ‘¢ğ‘˜)
2 },
is that the smooth part ğ‘“ of the composite convex programming prob-

ğ‘¥	2	ğ‘	2
where the coupling between the primal and the dual variables is
ğ‘¢ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1.
The dual to step (4.11) reads as
lem (P) admits a Lipschitz continuous gradient. Since ğ‘“ (ğ‘¥) = ğ‘”(ğ€ğ‘¥), the Lipschitz constant of âˆ‡ğ‘“ is determined by a corresponding Lipschitz as- sumption on âˆ‡ğ‘”, with the constant henceforth denoted as ğ¿ğ‘” , and a bound on spectrum of the matrix ğ€. To highlight the primal-dual na-
ture of the algorithm, a key element in the complexity analysis is the
bifunction

ğ‘§ğ‘˜+1 = argmin{ğ‘”(ğ‘§) + ğ‘ ğ‘§ âˆ’ 1 ğ‘¢ğ‘˜+1 2}.	âˆ—

ğ‘§	2 â€–	ğ‘	â€–2
ğ‘†(ğ‘¥, ğ‘¦) = ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘” (ğ‘¦) + ğ‘¦, ğ€ğ‘¥ = ğ¿(ğ‘¥, 0, ğ‘¦).



Our derivation of an iteration complexity estimate of AD-PMM proceeds in two steps. First, we present an interesting â€œMeta-Theoremâ€, due to Shefi and Teboulle (2014), and stated here as Proposition 4.2. It for-


Dividing both sides by ğ‘ and using the convexity of the Lagrangian with respect to (ğ‘¥, ğ‘§) and the linearity in ğ‘¦, we easily get
ğ¿(ğ‘¥Ì„ , ğ‘§Ì„ , ğ‘¦) âˆ’ ğ¿(ğ‘¥, ğ‘§, ğ‘¦Ì„ ) â‰¤  1  (ğ¶(ğ‘¥, ğ‘§) + 1 â€–ğ‘¦ âˆ’ ğ‘¦0â€–2 )

satisfying a specific per-iteration bound. We then apply this general re-
sult to AD-PMM, by verifying that this scheme actually satisfies these
in terms of the ergodic average

mentioned per-iteration bounds.
We start with an auxiliary technical fact.
ğ‘¥Ì„ğ‘
=  1
ğ‘
ğ‘ âˆ’1
ğ‘¥ğ‘˜, ğ‘¦Ì„ğ‘
ğ‘˜=0
=  1
ğ‘
ğ‘ âˆ’1
ğ‘¦ğ‘˜, ğ‘§Ì„ğ‘
ğ‘˜=0
=  1
ğ‘
ğ‘ âˆ’1
ğ‘§ğ‘˜,
ğ‘˜=0

Lemma 4.1. Let â„ âˆ¶ â„ğ‘› â†’ â„ be a proper convex and ğ¿â„ -Lipschitz contin-
and  the  constant  ğ¶(ğ‘¥, ğ‘§) = ğ‘ ğ€ğ‘¥ âˆ’ ğ‘§0 2 +
ğ‘¥ âˆ’ ğ‘¥0 2  + ğ‘§ âˆ’ ğ‘§0 2 .

uous. Then, for any ğœ‰ âˆˆ â„ğ‘› we have
â€–	â€–	â€–	â€–ğŒ1  â€–	â€–ğŒ2

â„(ğœ‰) â‰¤ max{ ğœ‰, ğ‘¢ âˆ’ â„âˆ—(ğ‘¢) âˆ¶ ğ‘¢  â‰¤ ğ¿ }.	(4.16)
Proof. Since â„ is convex and continuous, it agrees with its biconjugate:
â„âˆ—âˆ— = â„. By Corollary 13.3.3 in Rockafellar (1970), dom â„âˆ— is bounded
averages (ğ‘¥Ì„ğ‘˜ , ğ‘§Ì„ğ‘˜ , ğ‘¦Ì„ğ‘˜ ) generated by AD-PMM, and derive a ğ‘‚(1âˆ•ğ‘ ) Therefore, we can apply Proposition 4.2 to the sequence of ergodic
convergence rate in terms of the function value.

Relation to the Chambolle-Pock primal-dual splitting

with dom â„âˆ— âŠ† {ğ‘¢ âˆ¶
gives
â‰¤ ğ¿ }. Hence, the definition of the conjugate
â€–ğ‘¢â€–2
In this subsection we discuss the relation between ADMM and the

â„(ğœ‰) =  sup  { ğ‘¢, ğœ‰
ğ‘¢âˆˆdom â„âˆ—
â–¡
â€“ â„âˆ—(ğ‘¢)} â‰¤	max { ğœ‰, ğ‘¢ âˆ’ â„âˆ—(ğ‘¢)}.
ğ‘¢âˆ¶ ğ‘¢	ğ¿
celebrated Chambolle-Pock (a.k.a Primal-Dual Hybrid Gradient) method (Chambolle and Pock, 2011), designed for problems in the form (2.8).




Proposition  4.2. Let  (ğ‘¥âˆ—, ğ‘¦âˆ—, ğ‘§âˆ—)  be  a  saddle  point  for  ğ¿.  Let
{(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜, ğ‘§ğ‘˜); ğ‘˜ 0} be a sequence generated by some algorithm for which the following estimate holds for any ğ‘¦ âˆˆ â„ğ‘š:

The Chambolle-Pock primal-dual algorithm (CP)
Input: pick (ğ‘¥0, ğ‘¦0, ğ‘0) âˆˆ â„ğ‘› Ã— â„ğ‘š Ã— â„ğ‘š and ğ‘, ğœ > 0, ğœƒ âˆˆ [0, 1];
General step: For ğ‘˜ = 0, 1, â€¦ do:
 1

( ğ‘˜  ğ‘˜  ) âˆ’ Î¨( âˆ—) â‰¤  1 [ ( âˆ—  âˆ—) + 1
0 â€–2 ]
ğ‘¥ğ‘˜+1 = argminğ‘¥{ğ‘Ÿ(ğ‘¥) + 2	âˆ’ ( ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜) 2
(4.19)

for some constant ğ¶(ğ‘¥âˆ—, ğ‘§âˆ—) > 0. Then
ğ‘¦	2ğ‘ â€–	â€–2

Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨(ğ‘¥âˆ—) â‰¤
ğ¶1 (ğ‘¥âˆ—, ğ‘§âˆ—, ğ¿ğ‘” )
2ğ‘˜	.
ğ‘ğ‘˜+1 = ğ‘¦ğ‘˜+1 + ğœƒ(ğ‘¦ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜)	(4.21).



where ğ¶ (ğ‘¥âˆ—, ğ‘§âˆ—, ğ¿ ) = ğ¶(ğ‘¥âˆ—, ğ‘§âˆ—) + 2 (ğ¿2 + ğ‘¦0 2).

1	ğ‘”
ğ‘  ğ‘”
â€– â€–2
For later references it is instructive to write this algorithm slightly

Proof. Thanks to the Fenchel inequality
ğ¿(ğ‘¥, ğ‘§, ğ‘¦) âˆ’ ğ‘†(ğ‘¥, ğ‘¦) = ğ‘”(ğ‘§) + ğ‘”âˆ—(ğ‘¦) âˆ’ ğ‘¦, ğ‘§ â‰¥ 0.
of     the     step     ğ‘¥ğ‘˜+1,     we     see differently in operator-theoretic notation. From the optimality condition
0 âˆˆ ğœ•ğ‘Ÿ(ğ‘¥ğ‘˜+1) + 1 (ğ‘¥ğ‘˜+1 âˆ’ ğ‘¤ğ‘˜) â‡” 0 âˆˆ (Id +ğœğœ•ğ‘Ÿ)(ğ‘¥ğ‘˜+1) âˆ’ ğ‘¤ğ‘˜

By the definition of the convex conjugate
Î¨(	âˆ—
ğ‘¦	ğ‘¦
Now, since ğ‘” is convex and continuous on â„ğ‘š, we know ğ‘” = ğ‘”âˆ—âˆ—, and we
can apply Lemma 4.1 to obtain the string of inequalities:
ğœ
where ğ‘¤ğ‘˜ = ğ‘¥ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜. Hence, we can give an explicit expression of
the update as
ğ‘¥ğ‘˜+1 = (Id +ğœğœ•ğ‘Ÿ)âˆ’1 (ğ‘¤ğ‘˜) = (Id +ğœğœ•ğ‘Ÿ)âˆ’1 (ğ‘¥ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜).
Similarly, we can write the update ğ‘¦ğ‘˜+1 explicitly as

Î¨(ğ‘¥ğ‘˜ ) âˆ’ Î¨(ğ‘¥âˆ— ) = sup{ğ‘†(ğ‘¥ğ‘˜ , ğ‘¦) âˆ’ Î¨(ğ‘¥âˆ— )}â‰¤	sup
{ğ‘†(ğ‘¥ğ‘˜ , ğ‘¦) âˆ’ Î¨(ğ‘¥âˆ— )} â‰¤	sup  {ğ¿(ğ‘¥ğ‘˜ , ğ‘§ğ‘˜ , ğ‘¦)âˆ’Î¨(ğ‘¥âˆ— )} ğ‘¦ğ‘˜+1 = (Id +ğ‘ğœ•ğ‘”âˆ—)âˆ’1 (ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1 ).

ğ‘¦	ğ‘¦âˆ¶ ğ‘¦ â‰¤ğ¿ğ‘”
{  (
  

ğ‘¦âˆ¶ ğ‘¦ â‰¤ğ¿ğ‘”
)}	[	]

	 	 

â–¡
To apply this Meta-Theorem, we need to verify that AD-PMM satis- fies the condition (4.17). To make progress towards that end, Lemma
4.2 in Shefi and Teboulle (2014) proves that
ğ¿(ğ‘¥ğ‘˜+1, ğ‘§ğ‘˜+1, ğ‘¦) âˆ’ ğ¿(ğ‘¥, ğ‘§, ğ‘¦ğ‘˜+1) â‰¤ ğ‘‡ (ğ‘¥, ğ‘§, ğ‘¥ğ‘˜+1) + ğ‘… (ğ‘¥, ğ‘¦, ğ‘§)	(4.18)
for all (ğ‘¥, ğ‘§, ğ‘¦) âˆˆ ğ–· Ã— ğ–¹ Ã— â„ğ‘š and some explicitly given functions ğ‘‡ğ‘˜ and
ğ‘…ğ‘˜ . Furthermore, it is shown that
ğ‘ğ‘˜+1 = 2ğ‘¦ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜, which corresponds to a simple linear extrapolation
Pock (2011) provide a ğ‘‚(1âˆ•ğ‘ ) non-asymptotic convergence guarantees based on the current and previous iterates. In this case, Chambolle and
in terms of the primal-dual gap function of the corresponding saddle- point problem. The CP primal-dual splitting method has been of im- mense importance in imaging and signal processing and constitutes nowadays a standard method for tackling large-scale instances in these
application domains. Interestingly, if ğœƒ = 1, CP is a special case of the
proximal version of ADMM (AD-PMM). To establish this connection, let

1	âŠ¤

ğ‘‡ (ğ‘¥, ğ‘§, ğ‘¥ğ‘˜+1) â‰¤ ğ‘ (â€–ğ€ğ‘¥ âˆ’ ğ‘§ğ‘˜â€–2 âˆ’ â€–ğ€ğ‘¥ âˆ’ ğ‘§ğ‘˜+1â€–2 + ğ‘ â€–ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1â€–2 ), and
us set ğŒ1 = ğ‘ Id âˆ’ğ‘ğ€ ğ€ and ğŒ2 = 0. After some elementary manipula-

ğ‘… (ğ‘¥, ğ‘§, ğ‘¦) â‰¤ 1 (Î” (ğ‘¥, ğŒ ) + Î” (ğ‘§, ğŒ ) + 1 Î” (ğ‘¦, Id)) âˆ’ ğ‘ â€–ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1â€–2 ,


ğ‘˜+1


 1	ğ‘˜
âŠ¤  ğ‘˜
ğ‘˜	ğ‘˜
2 }.

Î” (ğ‘§, ğŒ) = 1
ğ‘§ âˆ’ ğ‘§ğ‘˜ 2 âˆ’ 1
ğ‘§ âˆ’ ğ‘§ğ‘˜+1 2 .
lently as

ğ‘˜	2 â€–	â€–ğŒ	2 â€–	â€–ğŒ
ğ‘¥ğ‘˜+1 = argmin{ğ‘Ÿ(ğ‘¥) +  1
ğ‘¥ âˆ’ (ğ‘¥ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜) 2 } = Prox
(ğ‘¥ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜).

Using these bounds and summing inequality (4.18) over ğ‘˜ = 0, 1, â€¦ , ğ‘ âˆ’
ğ‘¥	2ğœ â€–
â€–2	ğœğ‘Ÿ

ğ‘ âˆ’1
[ğ¿(ğ‘¥
ğ‘˜+1
, ğ‘§ğ‘˜+1
, ğ‘¦) âˆ’ ğ¿(ğ‘¥, ğ‘§, ğ‘¦
ğ‘˜+1
)] â‰¤

1 (ğ‘ ğ€ğ‘¥ âˆ’ ğ‘§0 2
+ ğ‘¥ âˆ’ ğ‘¥0 2
+ ğ‘§ âˆ’ ğ‘§0 2
+ 1 ğ‘¦ âˆ’ ğ‘¦
0 2 )
ğ‘§ğ‘˜+1
= (Id +
1 ğœ•ğ‘”)
âˆ’1 (
ğ€ğ‘¥
ğ‘˜+1 +
1 ğ‘¦
ğ‘˜)
= Prox 1
( 1 (ğ‘ğ€ğ‘¥
ğ‘˜+1
+ ğ‘¦ğ‘˜)).

ğ‘˜=0

2  â€–	â€–2  â€–
â€–ğŒ1  â€–
â€–ğŒ2

ğ‘ â€–	â€–2
ğ‘	ğ‘
ğ‘ ğ‘”  ğ‘



Moreauâ€™s identity Bauschke and Combettes (2016, Proposition 23.18) states that
ğ‘ Prox 1 ğ‘” (ğ‘¢âˆ•ğ‘) + Proxğ‘ğ‘”âˆ— (ğ‘¢) = ğ‘¢  âˆ€ğ‘¢ âˆˆ ğ–µ.	(4.22)
ğ‘
The practical application of an LO requires to make a selection from the set of solutions of the defining linear minimization problem. The precise definition of such a selection mechanism is not of any impor-
tance, and thus we are just concerned with any answer	(ğ‘¦) revealed

Applying this fundamental identity, we see	by the oracle.

ğ‘ğ‘§ğ‘˜+1 + Proxğ‘ğ‘”âˆ— (ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1) = ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1.
The second summand is just the ğ‘¦ğ‘˜+1-update in the CP algorithm, so that
we deduce
ğ‘ğ‘§ğ‘˜+1 + ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1 â‡” ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1).
Consequently,
ğ‘ğ‘˜+1 = ğ‘¦ğ‘˜+1 + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1) = 2ğ‘¦ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜,
and hence we recover the three-step iteration defining CP:
The information-theoretic assumption that the optimizer can only query a linear minimization oracle is clearly the main difference be- tween CG and other gradient-based methods discussed in Section 3. For instance, the dual averaging algorithm solves at each iteration a strongly convex subproblem of the form
min{ ğ‘¦, ğ‘¢ + â„(ğ‘¢)},	(5.3)
where â„ âˆˆ îˆ´ (ğ–·), whereas CG solves a single linear minimization prob-
lem at each iteration. This difference in the updating mechanism yields

ğ‘¥ğ‘˜+1
= argmin{ğ‘Ÿ(ğ‘¥) +  1  ğ‘¥ âˆ’ (ğ‘¥ğ‘˜ âˆ’ ğœğ€âŠ¤ğ‘ğ‘˜) 2 }
ğ‘¥	2ğœ
the following potential advantages of the CG method.
Low iteration costs: In many cases it is much easier to construct an

ğ‘¦ğ‘˜+1 = argmin{ğ‘”âˆ—(ğ‘¦) +  1
ğ‘¦ âˆ’ (ğ‘¦ğ‘˜ + ğ‘ğ€ğ‘¥ğ‘˜+1) 2 }
LO rather than solving the non-linear subproblem (5.3). We empha-

ğ‘¦
ğ‘ğ‘˜+1 = 2ğ‘¦ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜ .
2ğ‘ â€–	â€–2
ture of the objective function ğ‘“ , but rather on the geometry of the size that this potential benefit of CG does not depend on the struc-

Given the above derivations, we can summarize this subsection by the following interesting observation.
Proposition 4.3 (Proposition 3.1, Shefi and Teboulle (2014)). Let
(ğ‘¥ğ‘˜, ğ‘¦ğ‘˜, ğ‘ğ‘˜) be a sequence generated by CP with ğœƒ = 1. Then, the ğ‘¦ğ‘˜+1-update
(4.20) is equivalent to
feasible set ğ–·. To illustrate this point, consider the spectrahedron
ğ–· = {ğ— âˆˆ â„ğ‘›Ã—ğ‘› ğ— âª° 0, tr(ğ—)  1}. Computing the orthogonal projec- tion of some symmetric matrix ğ˜ onto the spectrahedron requires first to compute the full spectral decomposition ğ˜ = ğ”ğƒğ”âŠ¤ , and then for the diagonal matrix ğƒ computing the projection of its diagonal el-
ements onto the simplex. The resulting projection is therefore given

ğ‘§ğ‘˜+1 = argmin{ğ‘”(ğ‘§) + ğ‘
ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ + 1 ğ‘¦ğ‘˜ 2},	by

ğ‘§	2 â€–
ğ‘  â€–2
ğ‘ƒğ–·(ğ˜) = ğ” Diag(ğ‘ƒ
Î”ğ‘›
(diag(ğƒ)))ğ”âŠ¤.

ğ‘¦ğ‘˜+1 = ğ‘¦ğ‘˜ + ğ‘(ğ€ğ‘¥ğ‘˜+1 âˆ’ ğ‘§ğ‘˜+1)
which corresponds to the primal ğ‘§ğ‘˜+1-minimization step (4.14) with ğŒ2 = 0,
In contrast, computing a linear oracle over ğ–· for the symmetric
matrix ğ˜ involves finding the eigenvector of ğ˜ corresponding to

and to the dual multiplier update for ğ‘¦ğ‘˜+1 (4.15) of AD-PMM, respectively.
the minimal eigenvalue, that is G (ğ˜) = ğ‘¢ğ‘¢âŠ¤, where ğ‘¢âŠ¤ğ˜ğ‘¢ = ğœ†
min
(ğ˜).

Moreover, the minimization step with respect to ğ‘¥ in the CP algorithm given
in (4.19) together with (4.15) reduces to (4.13) of AD-PMM with ğŒ1 =
ğœ Id âˆ’ğ‘ğ€âŠ¤ğ€.
The Conditional Gradient Method

The eï¬ƒciency of the Bregman proximal gradient method stands and falls with the relative ease of evaluating the Bregman proximal operator (3.7). In this section, we present a class of first-order methods which gain relevance in large-scale problems for which the computation of the projection-like operators is a significant computational bottleneck. We describe conditional gradient (CG) methods, a family of methods which, originating in the 1960â€™s, have received much attention in both machine learning and optimization in the last 20 years. CG is designed to solve convex programming problems over compact convex sets. Therefore, we assume in this section that the feasible set ğ–· is a compact convex set.
Assumption 9. The set ğ–· is a compact convex subset in a finite- dimensional real vector space ğ–µ.
Classical Conditional gradient

CG, also known as the Frank-Wolfe method, was independently de- veloped by Frank and Wolfe Frank and Wolfe (1956) for linearly con- strained quadratic problems, and by Levitin and Polyak Levitin and Polyak (1966) for general smooth convex optimization problems over compact domains:
This operation can be typically done using numerical linear algebra techniques such as Power, Lanczos or Kaczmarz, and randomized versions thereof (see KuczyÅ„ski and WoÅºniakowski (1992) for gen- eral complexity results). For large-scale problems, computing such a leading eigenvector to a predefined accuracy is much more eï¬ƒcient than a full spectral decomposition.
Simplicity: The definition of an LO does not rely on a specific DGF
â„ âˆˆ  (ğ–·) and makes the update aï¬ƒne invariant.
Structural properties of the updates: When the feasible set ğ–· can be


represented as the convex hull of a countable set of atoms (â€gener- atorsâ€), then CG often leads to simple updates, activating only few atoms at each iteration. In particular, in the case of the spectrahe- dron, the LO returns a matrix of rank one, which allows for sparsity preserving iterates.
the LO at a given gradient feedback ğ‘¦ = âˆ‡ğ‘“ (ğ‘¥), and returns the target The classical form of CG takes the answer obtained from querying
vector
ğ‘(ğ‘¥) = G (âˆ‡ğ‘“ (ğ‘¥))	âˆ€ğ‘¥ âˆˆ ğ–·.	(5.4)
It proposes then to move in the direction ğ‘(ğ‘¥) âˆ’ ğ‘¥. As in every opti-
rules to guarantee reasonable numerical performance. Letting ğ‘¥ğ‘˜âˆ’1 and mization routine, a key question is how to design eï¬ƒcient step-size
ğ‘ğ‘˜ = ğ‘(ğ‘¥ğ‘˜âˆ’1) be a current position of the method together with its im-
plied target vector, the following policies are standard choices:
Standard:  ğ›¾ =   1   ,	(5.5)

Î¨min
(ğ–·) âˆ¶= min{ğ‘“ (ğ‘¥) ğ‘¥ âˆˆ ğ–·}.	(5.1)
2 + ğ‘˜

CG attempts to solve problem (5.1) by sequentially calling a linear oracle
(LO), a fundamental notion we introduce next.
Exact line search:  ğ›¾ğ‘˜ âˆˆ argmin ğ‘“ (ğ‘¥ğ‘˜âˆ’1 + ğ‘¡(ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1)),	(5.6)
ğ‘¡âˆˆ(0,1]

Definition 5.1. The Operator G âˆ¶ ğ–µâˆ— â†’ ğ–· is a linear oracle (LO) over

set
ğ–· if for any vector
ğ‘¦ âˆˆ
ğ–µâˆ—
ğ–·
we have that
Adaptive:  ğ›¾
= min { âŸ¨
ğ‘“ (ğ‘¥ğ‘˜âˆ’1), ğ‘¥ğ‘˜âˆ’1 âˆ’ ğ‘ğ‘˜
	 
âŸ© , 1
}.	(5.7)



Exact line search is conceptually attractive, but can be costly in large- scale applications when computing the function value is computation-
Note that when choosing â„ to be the squared Euclidean norm â„(ğ‘¥) =
1 ğ‘¥ 2 and ğ¿â„ = ğ¿ , then Assumption 10 is equivalent to the Lipschitz

2 â€– â€–
ğ‘“	ğ‘“

ally expensive. To understand the construction of the adaptive step-size scheme, it is instructive to introduce a primal gap (merit) function to the problem defined as
ğš(ğ‘¥) âˆ¶= sup âˆ‡ğ‘“ (ğ‘¥), ğ‘¥ âˆ’ ğ‘¢ .	(5.8)
ğ‘¢âˆˆğ–·
This merit function is just the gap program (see e.g. Facchinei and Pang (2003)) associated to the monotone variational inequality (2.6) in which the non-smooth part is trivial. In terms of this merit function, the descent lemma (3.15) yields immediately
gradient assumption, where Î©â„(ğ–·) is the diameter of set ğ–·. On the other
hand, choosing â„(ğ‘¥) = ğ‘“ (ğ‘¥) and ğ¿â„ = ğ¿ğ‘“ , we essentially retrieve the fi-
nite curvature assumption used by Jaggi Jaggi (2013).
Remark 5.1. It is clear that the finite curvature assumption (5.10) is not compatible with the DGF to be essentially smooth on ğ–·. We are therefore forced to work with non-steep distance-generating functions.
The analysis of CG under a relative smoothness condition and Assumption 10 runs in the same way as for the classical CG. However, the adaptive step-size is reformulated as

ğ‘“ (ğ‘¥ + ğ‘¡(ğ‘(ğ‘¥) âˆ’ ğ‘¥)) â‰¤
( ) +
âˆ‡ ( )
( ) âˆ’
+ ğ¿ğ‘“ ğ‘¡2
( ) âˆ’	2
{	ğ‘˜âˆ’1
ğ‘˜âˆ’1
ğ‘˜	}

ğ‘“ ğ‘¥	ğ‘¡âŸ¨ ğ‘“ ğ‘¥ , ğ‘ ğ‘¥
ğ‘¥âŸ©	2  â€–ğ‘ ğ‘¥	ğ‘¥â€–
ğ›¾ = min
âˆ‡ğ‘“ (ğ‘¥
), ğ‘¥
â€“ ğ‘
, 1  .

= ğ‘“ (ğ‘¥) âˆ’ ğ‘¡ğš(ğ‘¥) + ğ¿ğ‘“ ğ‘¡
ğ‘(ğ‘¥) âˆ’ ğ‘¥ 2 = ğ‘“ (ğ‘¥) âˆ’ ğœ‚ (ğ‘¡),
ğ‘“  â„

ğ¿ ğ‘¡2
2 â€–	â€–	ğ‘¥
This can be easily seen by replacing the upper model function ğ‘“ (ğ‘¥) âˆ’
ğ‘¡ğš(ğ‘¥) + ğ¿â„ ğ·â„ (ğ‘¥ + ğ‘¡(ğ‘ âˆ’ ğ‘¥), ğ‘¥), with its more conservative bound ğ‘“ (ğ‘¥) âˆ’

where ğœ‚ (ğ‘¡) âˆ¶= ğ‘¡ğš(ğ‘¥) âˆ’ ğ‘“	ğ‘(ğ‘¥) âˆ’ ğ‘¥ 2. Optimizing this function with re-	ğ‘“

ğ‘¥	2 â€–	â€–
ğ¿â„ ğ‘¡2  2

returns the adaptive step-size rule in (5.7). Once the optimizer decided upon the specific step-size policy, the classical CG picks one of the step sizes (5.5), (5.6), or (5.7), and performs the update
ğ‘¥ğ‘˜ = ğ‘¥ğ‘˜âˆ’1 + ğ›¾ğ‘˜ (ğ‘(ğ‘¥ğ‘˜) âˆ’ ğ‘¥ğ‘˜âˆ’1).


The classical conditional gradient (CG)
Input: A linear oracle , a starting point ğ‘¥0 âˆˆ ğ–·. Output: A solution ğ‘¥ such that Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·) < ğœ€. General step: For ğ‘˜ = 1, 2, â€¦
Compute ğ‘ğ‘˜ =	(âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1));
sults in a smaller step-size than the adaptive step, which hints towards a deterioration of performance. Nevertheless, this trick allows us to han- dle convex programming problems outside the Lipschitz smooth case, which is not uncommon in various applications (Bian and Chen, 2015; Bian et al., 2015; Haeser et al., 2018).

Generalized Conditional Gradient

Introduced by Bach Bach (2015) and Nesterov (2018a), the general- ized conditional gradient (GCG) method, is targeted to solve our master problem (P) over a compact set ğ–·. To handle the composite case, we need to modify our definition of a linear oracle accordingly.

Choose a step-size ğ›¾ğ‘˜ either by (5.5), (5.6), (5.7);
Update ğ‘¥ğ‘˜ = ğ‘¥ğ‘˜âˆ’1 + ğ›¾ğ‘˜ (ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1);
Definition 5.2. Operator G
ğ–·,ğ‘Ÿ
âˆ¶ ğ–µâˆ— â†’ ğ–· is a generalized linear oracle
âˆ—

Compute ğšğ‘˜ = ğš(ğ‘¥ğ‘˜âˆ’1).
If ğšğ‘˜ < ğœ€ return ğ‘¥ğ‘˜.
(GLO) over set ğ–· with respect to function ğ‘Ÿ if for any vector ğ‘¦ âˆˆ ğ–µ we
have that

G  (ğ‘¦) âˆˆ argmin ğ‘¦, ğ‘¥ + ğ‘Ÿ(ğ‘¥).

ğ–·,ğ‘Ÿ
ğ‘¥âˆˆğ–·  âŸ¨	âŸ©

The convergence properties of classical CG under either of the step- size variants above is well documented in the literature (see e.g. the recent text by Lan (2020), or Jaggi (2013)). We will obtain a full con-
vergence and complexity theory under our more general analysis of the
Besides this more demanding oracle assumption, the resulting gener- alized conditional gradient method is formally identical to the classical CG. In particular, we can consider the target vector


generalized CG scheme.
ğ‘(ğ‘¥) = G
ğ–·,ğ‘Ÿ
(âˆ‡ğ‘“ (ğ‘¥))	âˆ€ğ‘¥ âˆˆ ğ–·	(5.11)

Relative smoothness
The basic ingredient in proving convergence and complexity results on the classical CG is the fundamental inequality
ğ‘“ (ğ‘¥ + ğ‘¡(ğ‘(ğ‘¥) âˆ’ ğ‘¥)) â‰¤ (ğ‘¥) âˆ’ ğ‘¡ğš(ğ‘¥) + ğ¿ğ‘“ ğ‘¡  ğ‘(ğ‘¥) âˆ’ ğ‘¥ 2.
2
and the same three step size policies as in the classical CG, with the standard step size remaining the same and the obvious modifications for the two other step size policies:
Exact line search:  ğ›¾ğ‘˜ âˆˆ argmin Î¨(ğ‘¥ğ‘˜âˆ’1 + ğ‘¡(ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1)),	(5.12)
ğ‘¡âˆˆ[0,1]
{ ğ‘Ÿ(ğ‘¥ğ‘˜âˆ’1) âˆ’ ğ‘Ÿ(ğ‘ğ‘˜) + âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1), ğ‘¥ğ‘˜âˆ’1 âˆ’ ğ‘ğ‘˜	}

Based on the relative smoothness analysis in Section 3.3.3, it seems to be intuitively clear that we could easily prove also convergence of CG when instead of the restrictive Lipschitz gradient assumption we make
Adaptive:	ğ›¾ğ‘˜ = min
ğ¿ğ‘“ â€–ğ‘¥ğ‘˜âŸ¨âˆ’1 âˆ’ ğ‘ğ‘˜ â€–2
âŸ© , 1
.
(5.13)

a relative smoothness assumption in terms of the pair (ğ‘“, â„) for some DGF â„ âˆˆ ğ›¼(ğ–·). Indeed, if we are able to estimate a scalar ğ¿â„ > 0 such
that ğ¿â„ â„(ğ‘¥) âˆ’ ğ‘“ (ğ‘¥) is convex on ğ–·, then the modified Descent Lemma
(3.19) yields the overestimation
ğ‘“ (ğ‘¥ + ğ‘¡(ğ‘ âˆ’ ğ‘¥)) â‰¤ ğ‘“ (ğ‘¥) âˆ’ ğ‘¡ğš(ğ‘¥) + ğ¿â„ ğ· (ğ‘¥ + ğ‘¡(ğ‘ âˆ’ ğ‘¥), ğ‘¥).	(5.9) Instead of requiring that ğ‘“ has a Lipschitz continuous gradient over the convex compact set ğ–·, let us alternatively require the following:
The adaptive step size variant is derived from an augmented merit func- tion, taking into consideration the non-smooth composite nature of the underlying optimization problem. Indeed, as again can be learned from the basic theory of variational inequalities (see Nesterov (2007)), the natural merit function for the composite model problem (P) is the non- smooth function
ğš(ğ‘¥) = sup Î“(ğ‘¥, ğ‘¢), where Î“(ğ‘¥, ğ‘¢) âˆ¶= ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘Ÿ(ğ‘¢) + âˆ‡ğ‘“ (ğ‘¥), ğ‘¥ âˆ’ ğ‘¢ .   (5.14)
ğ‘¢âˆˆğ–·

ğ›¼	ğ‘“	âˆ—

such that ğ¿â„ â„ âˆ’ ğ‘“ is convex on ğ–·, and â„ has a finite curvature on ğ–·,
only if ğ‘¥ âˆˆ ğ–· . These basic properties justify our terminology, calling

ğ‘“
that is,
2ğ· (ğ‘¡ğ‘¢ + (1 âˆ’ ğ‘¡)ğ‘¥, ğ‘¥)
ğš(ğ‘¥) a merit function. Of course, ğš(â‹…) is also easily seen to be convex. Furthermore, using the convexity of ğ‘“ , one first sees that

Î©2 (ğ–·) âˆ¶=	sup	â„
< âˆ.	(5.10)
âˆ‡ ( )  âˆ’
â‰¥  ( ) âˆ’  ( )

â„	ğ‘¥,ğ‘¢âˆˆğ–·,ğ‘¡âˆˆ[0,1]
ğ‘¡2
âŸ¨ ğ‘“ ğ‘¥ , ğ‘¥
ğ‘¢âŸ©
ğ‘“ ğ‘¥	ğ‘“ ğ‘¢ ,



so that for all ğ‘¥, ğ‘¢ âˆˆ dom(ğ‘Ÿ),
Î“(ğ‘¥, ğ‘¢) â‰¥ ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘Ÿ(ğ‘¢) + ğ‘“ (ğ‘¥) âˆ’ ğ‘“ (ğ‘¢) = Î¨(ğ‘¥) âˆ’ Î¨(ğ‘¢).
From here, one immediately arrives at the relation
where the second inequality follows from ğ‘ ğ¾ < min{ğ¿ğ‘“ Î©2 , ğ‘ 0}, the third inequality follows from  ğ‘	 being a monotonic function in ğ‘ 0 for
any ğ‘˜ â‰¥ ğ¾ + 1, and the last inequality follows from ğ¾ â‰¤ max {2,  ğ‘ 0  }.

ğš(ğ‘¥) â‰¥ Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·).	(5.15)	Combining these two results, we have that
ğ‘ ğ‘˜ â‰¤
Clearly, with ğ‘Ÿ = 0, the above specification yields the classical CG.	2 max{ğ‘ 0 , ğ¿ğ‘“ Î©2 }
.
Basic Complexity Properties of GCG	ğ‘˜
We now turn to prove that the GCG method with one of the above	â–¡
mentioned step-sizes converges at a rate of ğ‘‚( 1 ). We will derive this rate

ğ‘˜
under the standard Lipschitz smoothness assumption on ğ‘“ . This gives us
assumed convexity of the non-smooth function ğ‘Ÿ(â‹…), we readily obtain access to the classical descent lemma (3.15). Combining this with the
ğ‘¡2 ğ¿
Alternative assumptions and step-sizes
A key takeaway from the analysis of the generalized conditional gra- dient is that one needs to have a bound on the quadratic term of the upper model

Î¨(ğ‘¥ğ‘˜âˆ’1 + ğ‘¡(ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1)) â‰¤ ğ‘“ (ğ‘¥ğ‘˜âˆ’1) âˆ’ ğ‘¡ âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1), ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 +
ğ‘“ ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 2

ğ‘˜âˆ’âŸ¨1	ğ‘˜
âŸ©	2 â€–	â€–
ğ‘¡ â†¦ ğ‘„(ğ‘¥, ğ‘(ğ‘¥), ğ‘¡, ğ¿ ) âˆ¶= Î¨(ğ‘¥) âˆ’ ğ‘¡ğš(ğ‘¥) +
ğ¿ğ‘“ ğ‘¡2
ğ‘(ğ‘¥) âˆ’ ğ‘¥ 2.

+ (1 âˆ’ ğ‘¡)ğ‘Ÿ(ğ‘¥  ) + ğ‘¡ğ‘Ÿ(ğ‘ )
ğ‘¡2 ğ¿ğ‘“
ğ‘“	2 â€–	â€–

= Î¨(ğ‘¥ğ‘˜âˆ’1) âˆ’ ğ‘¡ğš(ğ‘¥ğ‘˜âˆ’1) +
ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 2 .
Such a bound was given to us essentially for free under the compactness

2
Based on this fundamental inequality of the per-iteration decrease, we
can deduce the iteration complexity via an induction argument. First, one observes that for each of the three introduced step-size rules (stan- dard, line search and adaptive), one obtains a recursion of the form
Î¨(ğ‘¥ğ‘˜âˆ’1 + ğ›¾ (ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1)) â‰¤ Î¨(ğ‘¥ğ‘˜âˆ’1) âˆ’ ğ›¾ ğš(ğ‘¥ğ‘˜âˆ’1) +   ğ‘˜ ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜ 2.
on the smooth part ğ‘“ . The resulting complexity constant is then de- assumption of the domain ğ–·, and the Lipschitz-smoothness assumption termined by ğ¿ğ‘“ Î©2 . Moreover, this constant will be involved in lower
bounds of the adaptive step-size rule (5.13). However, such a constant may not be known, or may be expensive to compute. Moreover, a global
estimate of this constant is not actually needed for obtaining an upper

ğ‘˜	ğ‘˜
2  â€–	â€–
bound. To see this, we proceed formally as follows. Consider an alter-



Î©2	(ğ–·) = max	ğ‘¥ âˆ’ ğ‘¢ 2, this gives us	2

2 â€–â‹…â€–
ğ‘¥,ğ‘¢âˆˆğ–· â€–	â€–
ğ‘„(ğ‘¥, ğ‘, ğ‘¡, ğ‘€ ) âˆ¶= Î¨(ğ‘¥) âˆ’ ğ‘¡ğš(ğ‘¥) + ğ‘¡ ğ‘€ ğ‘(ğ‘, ğ‘¥),
2

ğ¿ ğ›¾2
ğ‘ ğ‘˜	ğ‘ ğ‘˜âˆ’1 âˆ’ ğ›¾ ğšğ‘˜ +	ğ‘˜ Î©2 .
where ğ‘(ğ‘, ğ‘¥) is a positive function bounded by some constant ğ¶, and

ğ‘˜	2
choose ğ›¾(ğ‘¥, ğ‘€ ) âˆ¶= min{1,   ğš(ğ‘¥)   }, for ğ‘(ğ‘¥) = G  (âˆ‡ğ‘“ (ğ‘¥)). Let ğ‘€ > 0

Applying to this recursion Lemma 13.13 in Beck (2017), we deduce the
ğ‘€ğ‘(ğ‘(ğ‘¥),ğ‘¥)
ğ–·,ğ‘Ÿ

next iteration complexity result for GCG.
Theorem 5.3. Consider algorithm GCG with one of the step size rules: stan- dard (5.5), line search (5.12), or adaptive (5.13). Then
be a constant such that the point obtained by using this step-size is upper bounded by the corresponding quadratic function, i.e.,
Î¨((1 âˆ’ ğ›¾(ğ‘¥, ğ‘€ ))ğ‘¥ + ğ›¾(ğ‘¥, ğ‘€ )ğ‘(ğ‘¥)) â‰¤ ğ‘„(ğ‘¥, ğ‘(ğ‘¥), ğ›¾(ğ‘¥, ğ‘€ ), ğ‘€ ) < Î¨(ğ‘¥).	(5.16)
Thus applying the update ğ‘¥+ âˆ¶= (1 âˆ’ ğ›¾(ğ‘¥, ğ‘€ ))ğ‘¥ + ğ›¾(ğ‘¥, ğ‘€ )ğ‘(ğ‘¥), we obtain

Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨min
(ğ–·) â‰¤ 2 max{Î¨(ğ‘¥0) âˆ’ Î¨min(ğ–·), ğ¿ğ‘“ Î©2 }
ğ‘˜
âˆ€ğ‘˜
â‰¥ 1.
Î¨(ğ‘¥+) âˆ’ Î¨min(ğ–·) â‰¤ Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·) âˆ’ 1 ğš(ğ‘¥) â‰¤ 1 (Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·))

Proof. We give a self-contained proof of this result for the adaptive step-size policy (5.13).
If ğ›¾ğ‘˜ = 1, the per-iteration progress is easily seen that ğšğ‘˜	ğ¿ğ‘“ â€–ğ‘ğ‘˜
  
2	2
if ğ›¾(ğ‘¥, ğ‘€ ) = 1, and
Î¨(ğ‘¥+) âˆ’ Î¨min(ğ–·) â‰¤ Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·) âˆ’ 	1	 ğš(ğ‘¥)2 â‰¤ Î¨(ğ‘¥) âˆ’ Î¨min(ğ–·)



ğ‘ ğ‘˜ â‰¤ ğ‘ ğ‘˜âˆ’1 âˆ’ ğšğ‘˜ + ğ¿ğ‘“
ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 2 â‰¤ ğ‘ ğ‘˜âˆ’1 âˆ’ 1 ğšğ‘˜ â‰¤ ğ‘ ğ‘˜âˆ’1 âˆ’ 1 ğ‘ ğ‘˜âˆ’1 = 1 ğ‘ ğ‘˜âˆ’1
ğš(ğ‘¥)	ğ‘˜

2 â€–	â€–	2
2	2	if ğ›¾(ğ‘¥, ğ‘€ ) = ğ‘€ğ‘(ğ‘(ğ‘¥),ğ‘¥) . If (ğ‘¥ )ğ‘˜â‰¥0 is the trajectory defined in this specific


âŸ¨ ğ‘˜âˆ’1 âˆ’ğ‘ğ‘˜ â€–2
âŸ© =	ğ‘˜ âˆ’ğ‘¥ğ‘˜âˆ’1 2 , a simple computation re-
ğ‘ ğ‘˜ â‰¤ min{ 1 ğ‘ ğ‘˜âˆ’1, ğ‘ ğ‘˜âˆ’1 âˆ’	1
(ğ‘ ğ‘˜âˆ’1)2 }

ğ‘ ğ‘˜ â‰¤ ğ‘˜âˆ’1	(ğšğ‘˜ )2
â‰¤ ğ‘˜âˆ’1	(ğšğ‘˜)2
â‰¤ ğ‘˜âˆ’1 âˆ’ (ğ‘ ğ‘˜âˆ’1)2
in terms of the approximation error ğ‘ ğ‘˜ âˆ¶= Î¨(ğ‘¥ğ‘˜) âˆ’ Î¨min(ğ–·), and the local

2ğ¿
ğ‘“ â€–
ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 2
ğ‘ 
2ğ¿ğ‘“ Î©2
2ğ¿ Î©2 .
estimates (ğ‘€ğ‘˜ )ğ‘˜â‰¥0. Thus, as we are able to bound ğ‘€ğ‘˜ from above for all
iterations of the algorithm, the same convergence as for GCG can be

ğ‘˜	{ 1 ğ‘˜âˆ’1
ğ‘˜âˆ’1
(ğ‘ ğ‘˜âˆ’1 )2 }
Based on this observation, and knowing that ğ‘€ğ‘˜ must be bounded for



Thus, the convergence is split into two periods, which are split by ğ¾ âˆ¶= log	âŒˆ 	ğ‘ 0	 âŒ‰ . If ğ‘˜ â‰¤ ğ¾ then ğ‘ ğ‘˜âˆ’1 â‰¥ ğ¿ Î©2 and thus ğ‘ ğ‘˜ â‰¤ 1 ğ‘ ğ‘˜âˆ’1,
struction, the resulting iterates ğ‘¥ğ‘˜ will induce monotonically decreasing backtracking procedure, as suggested in Pedregosa et al. (2020). By con- function values so that the whole trajectory ğ‘¥ğ‘˜ will be contained in the

which implies
ğ‘ ğ‘˜ â‰¤ 2âˆ’ğ‘˜ğ‘ 0, ğ‘˜ âˆˆ {0, 1, â€¦ , ğ¾}.
However,  if  ğ‘˜ > ğ¾  then  ğ‘ ğ‘˜âˆ’1 < min{ğ¿ Î©2 , ğ‘ 0}  and  ğ‘ ğ‘˜ â‰¤ ğ‘ ğ‘˜âˆ’1 âˆ’
be an upper bou|nd on Î¨(ğ‘¥ğ‘¡ ) for any point ğ‘¥ğ‘¡ = (1 âˆ’ ğ‘¡)ğ‘¥ğ‘˜âˆ’1 + ğ‘¡ğ‘ğ‘˜ such that sumed only on the appropriate level set and there is no need to insist on

  1  	ğ‘“	global Lipschitz smoothness on the entire set ğ–·. This insight enabled, for

ğ‘ ğ‘˜âˆ’1 2, which by induction (see for example Dunn (1979,
2ğ¿ğ‘“ Î©
Lemma 5.1)) implies that
ğ‘ ğ‘˜ â‰¤ 	ğ‘ ğ¾	 â‰¤  2ğ¿ğ‘“ Î©2  â‰¤ max{ğ¾, 2}ğ¿ğ‘“ Î© â‰¤ 2 max{ğ‘  , ğ¿ğ‘“ Î© }	â‰¥  + 1
example, proving the ğ‘‚(1âˆ•ğ‘˜) convergence rate of CG with adaptive and
exact step-size rules when applied to self-concordant functions, which
are not necessarily Lipschitz smooth on the predefined set ğ–· (Carderera

 ğ‘ ğ¾  ğ‘˜  ğ¾
2ğ¿ğ‘“ Î©2
2 + (ğ‘˜ âˆ’ ğ¾)	ğ‘˜	ğ‘˜
et al., 2021; Dvurechensky et al., 2020a; 2020; Zhao and Freund, 2020).


However, this observation need not apply to the standard step size rule
AW-CG produces ğ‘ğ‘˜ âˆˆ A and ğ‘¢ğ‘˜ âˆˆ ğ‘† ğ‘˜, and the away step maximal step

(5.5), since the standard step-size choice does not guarantee that all the
iterates remain in the appropriate level set.
size is respecified as ğ›¾max
ğœ†ğ‘¢ğ‘˜
1âˆ’ğœ†ğ‘¢ğ‘˜
. This implies, that using the maximal

To conclude, we reiterate that the step-size choices analyzed here are the most common, but there may be many more choices of step- size which provide similar guarantees. For example, Freund and Gri- gas (2016) suggests new step-size rules based on an alternative analysis of the CG method that utilizes an updated duality gap. (Nesterov, 2018a) discusses recursive step-size rules, and in Dvurechensky et al. (2020a); Odor et al. (2016) new step-size rules are suggested based on additional assumptions on the problem structure.

Variants of CG

One of the main drawbacks of CG method is that, in general, it comes with worse complexity bounds than BPGM for strongly convex functions. Indeed, it was shown as early as in 1968 by Canon and Cul-
lum (1968) (see also Lan (2013, 2020)) that the rate of ğ‘‚( 1 ) is in fact

ary of ğ–·. Thus, when ğ‘“ is strongly convex, Jaggi and Lacoste-Julian away-step step-size will not necessarily result on a point on the bound-
Lacoste-Julien and Jaggi (2015) show a linear convergence of AW-CG with a rate that only depends on the geometry of set ğ–·, which is cap- tured by the pyramidal width parameter. The Pairwise variant of AW-CG, which is also presented and analyzed in Lacoste-Julien and Jaggi (2015),
takes ğ‘‘ğ‘˜ = ğ‘¢ğ‘˜ âˆ’ ğ‘ğ‘˜ and ğ›¾max = ğœ†ğ‘¢ğ‘˜ , and has similar analysis.
vergence results of AS-CG to functions of the form ğ‘“ (ğ‘¥) = ğ‘”(ğ€ğ‘¥) + ğ‘, ğ‘¥ In Beck and Shtern (2017), Beck and Shtern extend the linear con- where ğ‘” is a strongly convex function. The linear rate depends on a
the geometry of ğ–· as well as matrix ğ€. It is also worth mentioning, a parameter based on the Hoffman constant, which captures both on
stream of work which shows linear convergence of AS-CG where the strong convexity assumption is replaced by the assumption that suï¬ƒ- cient second order optimality conditions, known as Robinson conditions

tight, even when the function ğ‘“
ğ‘˜
is strongly convex. This slow conver-
(Robinson, 1982), are satisfied (see for example Damla et al. (2008)).

ferent extreme points in ğ–·. In the smooth case, where ğ‘Ÿ = 0, and the gence is due to the well-documented zig-zagging effect between dif- objective function ğ‘“ and the feasible set ğ–· are both strongly convex, only a rate of ğ‘‚(  1 ) can be shown (Garber and Hazan, 2015), whereas
 1
5.3.2. Fully-corrective CG
The Fully-corrective variant of CG (FC-CG) also involves polyhedral
ğ–·, and aims to reduce the number of calls to the linear oracle, by re- placing them with a more accurate minimization over a convex-hull of

(Nesterov, 2018a) showed an accelerated ğ‘‚( ğ‘˜2 ) rate of convergence
for GCG with strongly convex ğ‘Ÿ (ğœ‡ > 0). Linear convergence of the CG
method can only be proved under additional assumptions regarding the
problem structure or location of the optimal solution (see e.g. Beck and Teboulle (2004); Dunn (1979); Epelman and Freund (2000); GuÃ©lat and Marcotte (1986); Levitin and Polyak (1966)).
Departing from these somewhat negative results, variants of the clas- sical CG were suggested in order to obtain the desired linear conver-
gence in the case of strongly convex function ğ‘“ . We will discuss four of
these variants: Away-step CG, Fully-corrective CG, CG based on a local
linear optimization oracle (LLOO), and CG with sliding.

Away-step CG
The away-step variation of CG (AW-CG), first suggested by
two calls of the LO at each iteration. The first call generates ğ‘ğ‘˜ = Wolfe (1970), treats the case where ğ–· is a polyhedron. It requires
ğ–·(âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1)), defined in the original CG algorithm, while the second
call generates an additional vector ğ‘¢ğ‘˜ =  (âˆ’âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1)). The two vec-
some subset Ağ‘˜ âŠ† A. The heart of the method is a correction routine, which updates the correction atoms Ağ‘˜ and iterate ğ‘¥ğ‘˜, and satisfy the
following:
ğ‘† ğ‘˜ âŠ† Ağ‘˜
ğ‘“ (ğ‘¥ğ‘˜) â‰¤ min ğ‘“ ((1 âˆ’ ğ‘¡)ğ‘¥ğ‘˜âˆ’1 + ğ‘¡ğ‘ğ‘˜)
ğ‘¡âˆˆ[0,1]
ğœ–	max âˆ‡ğ‘“ (ğ‘¥ğ‘˜), ğ‘  âˆ’ ğ‘¥ğ‘˜
ğ‘ âˆˆğ‘† ğ‘˜
where ğ‘ğ‘˜ = G (âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1)), and ğœ– is a given accuracy parameter. The FC-
CG was known by various names depending on the updating scheme of
ğ‘˜ and ğ‘¥ğ‘˜ (Holloway, 1974; Von Hohenbalken, 1977), and was uni-
fied and analyzed to show linear convergence in Lacoste-Julien and
Jaggi (2015). The convergence analysis of FC-CG is similar to that of AW-CG, and is based on the correction routine guaranteeing that the forward step is larger than the away-step computed in the previous it- eration.
In order to apply FC-CG one must choose a correction routine, and

ğ–·
tors ğ‘ğ‘˜ and ğ‘¢ğ‘˜ define the forward direction ğ‘‘ğ‘˜
= ğ‘ğ‘˜ âˆ’ ğ‘¥ğ‘˜âˆ’1 and the away
the linear convergence analysis does not take into account the computa-

direction ğ‘‘ğ‘˜ = ğ‘¥ğ‘˜âˆ’1 âˆ’ ğ‘¢ğ‘˜, respectively. By construction, both of this di-
iteration      ğ‘˜      is      obtained      by rections are non-ascent directions. The effectively chosen direction at
AS-CG on the subset ğ‘˜ = ğ‘† ğ‘˜âˆ’1 âˆª {ğ‘ğ‘˜} until the conditions are satisfied. tional cost of this routine. One choice of a correction routine is to apply This correction routine is wise only if eï¬ƒcient linear oracles  Ağ‘˜ can

ğ‘‘ğ‘˜ =  argmax
â€“ âˆ‡ğ‘“ (ğ‘¥ğ‘˜âˆ’1), ğ‘‘ ,
be constructed for all ğ‘˜ such that their low computational cost balances

ğ‘‘  ğ‘‘ğ‘˜
ğ¹ğ‘Š
,ğ‘‘ğ‘˜
ğ´
the routineâ€™s iteration complexity.

ensuring the chosen direction is a descent direction for non optimal ğ‘¥ğ‘˜âˆ’1,
with a corresponding updating step
ğ‘¥ğ‘˜ = ğ‘¥ğ‘˜âˆ’1 + ğ›¾ğ‘˜ ğ‘‘ğ‘˜.
Here, the choice of the step-size ğ›¾ğ‘˜ will also depend on the direc-
5.3.3. Enhanced LO based CG
A variant of CG which is based on an enhanced linear minimization oracle, was suggested by Garber and Hazan Garber and Hazan (2016).
GIn this variant, the linear oracle	(ğ‘) is replaced by a local oracle

ğ–·,ğœŒ
(ğ‘, ğ‘¥, ğ›¿) with some constant ğœŒ â‰¥
ğ–·
1, which takes an additional radius

tion chosen. The first analysis of this algorithm by GuÃ©lat and Mar-
input ğ›¿ and returns a point ğ‘ âˆˆ ğ–· satisfying

over ğ›¾ âˆˆ [0, ğ›¾	], where ğ›¾	âˆ¶= max{ğ‘¡ 0 âˆ¶ ğ‘¥ğ‘˜âˆ’1 + ğ‘¡ğ‘‘ğ‘˜ âˆˆ ğ–·}. Under cotte (1986) assumes that the step-size is chosen using exact line search
â€–ğ‘
â€“ ğ‘¥
â‰¤ ğœŒğ›¿
â‰¤
min
ğ‘¢, ğ‘ .

convex ğ‘“ . However, this rate estimate depends on the distance between this step-size choice, they prove linear convergence of CG for strongly
âŸ¨ğ‘, ğ‘âŸ©
ğ‘¢âˆˆğ–·âˆ¶â€–ğ‘¢âˆ’ğ‘¥â€–â‰¤ğ›¿ âŸ¨	âŸ©

the optimal solution and the boundary of set ğ‘‡ âŠ‚ ğ–·, which is the minimal	Thus, the only deviation from the CG algorithm is that ğ‘ğ‘˜ is obtained by
face of ğ–· containing the optimal solution. This result was later extended	applying G	(âˆ‡ğ‘“ (ğ‘¥ğ‘˜), ğ‘¥ğ‘˜, ğ›¿ ) for a suitably chosen sequence (ğ›¿ ) . The
ğ‘‹,ğœŒ	ğ‘˜	ğ‘˜ ğ‘˜

in Lacoste-Julien and Jaggi (2015), with a slight variation on the original algorithm. In this variation, the set ğ–· is represented as the convex hull of a finite set of atoms  (not necessarily containing only its vertices), and a representation of the current iterate as a convex combination of these
ğ‘† ğ‘˜ = {ğ‘ âˆˆ A âˆ¶ ğœ†ğ‘˜ > 0} is defined as the set of active atoms. Thus, the
linear convergence for the case where the smooth part ğ‘“ is strongly con-
vex, is obtained by a specific update of ğ›¿ğ‘˜ at each step of the algorithm. This update depends on the Lipschitz constant ğ¿ğ‘“ , the strong convex- ity constant of ğ‘“ , and the parameter ğœŒ. Moreover, despite the fact that
LLOO-CG can theoretically be applied to any set ğ–·, constructing a gen-
eral LLOO is challenging. In Garber and Hazan (2016), the authors sug-

ğ‘






geometric properties the polytope which may generally not tractably computed. Thus, while the strong convexity and geometric properties
Input: ğ‘¢1 = ğ‘¢, ğ‘¡ = 1.
Output: point ğ‘¢+ = CndG(ğ‘”, ğ‘¢, ğ›½, ğœ‚).
General step: Let ğ‘£ = argmax	ğ‘” + ğ›½(ğ‘¢ âˆ’ ğ‘¢), ğ‘¢ âˆ’ ğ‘¥

of the problem are only used for the analysis of the AW-CG and FC-CG,



âŸ¨ ğ‘¡	ğ‘¥âˆˆğ–· âŸ¨	âŸ©  ğ‘¡
ğ‘¡ = âŸ©

geometric parameters renders the LLOO-CG less applicable in practice.
{  âŸ¨ğ›½(ğ‘¢ âˆ’ ğ‘¢ ) âˆ’ ğ‘”, ğ‘£ âˆ’ ğ‘¢ âŸ©}

â€“ ğ‘¢
ğ›½â€–ğ‘£ğ‘¡	ğ‘¡ â€–
Each iteration of CG requires one call to the linear minimization or-		 acle and one gradient evaluation. Coupled with our knowledge about

the iteration complexity of CG, this fact implies that CG requires ğ‘‚(1âˆ•ğœ€)
where Î© â‰¡ Î© 1
2 (ğ–·). The number of calls of the linear minimization oracle

gradient evaluations of the objective function. This is suboptimal, when



2 â€–â‹…â€–


linear minimization oracle, the order estimate ğ‘‚(1âˆ•ğœ€) for the number timization, as we will see in Section 6. While it is known that within the
. In particular, if the parameter sequences in S-CG are
ğœ‚ğ‘˜
chosen as
3ğ¿	ğ¿ Î©2

of calls of the LO is unimprovable, in this section we review a method based on the linear minimization oracle which can skip the computa- tion of gradients from time to time. This improves the complexity of
ğ›½ğ‘˜ = ğ‘˜ + 1 , ğ›¾ğ‘˜ =
then
, ğœ‚ =	ğ‘“	,
ğ‘˜ + 2  ğ‘˜	ğ‘˜(ğ‘˜ + 1)

LO-based methods and leads us to the conditional gradient sliding (S-CG) algorithm introduced by Lan and Zhou Lan and Zhou (2016). S-CG is a numerical optimization method which runs in epochs and overall con- tains some similarities with accelerated methods, to be thoroughly sur-
ğ‘“ (ğ‘¦ğ‘˜) âˆ’ ğ‘“ (ğ‘¢) â‰¤	15ğ¿ğ‘“ Î©2
2(ğ‘˜ + 1)(ğ‘˜ + 2)
As a consequence, the total number of calls of the function gradients and the
ğ¿ Î©2	2

convex  programming  problem  for  which  ğ‘Ÿ  =  0. veyed in Section 6. S-CG has been described in the context of the smooth
LO oracle is bounded by ğ‘‚
Accelerated Methods
 ğ‘“ 
ğœ€
, and ğ‘‚(ğ¿ğ‘“ Î© âˆ•ğœ€), respectively.


Input: A linear oracle	a starting point ğ‘¥0 âˆˆ ğ–·. The conditional gradient sliding methods (S-CG) (ğ›½ğ‘˜ )ğ‘˜, (ğ›¾ğ‘˜ )ğ‘˜ parameter sequence such that
ğ›¾1 = 1, ğ¿ğ‘“ ğ›¾ğ‘˜ â‰¤ ğ›½ğ‘˜ ,
ğ›½ğ‘˜ ğ›¾ğ‘˜ â‰¥ ğ›½ğ‘˜âˆ’1 ğ›¾ğ‘˜âˆ’1
Î“ğ‘˜	Î“ğ‘˜âˆ’1
where
In previous sections we focused on simple first-order methods with sublinear convergence guarantees in the convex case, and linear con- vergence in the strongly convex case. Towards the end of the discussion in Section 3, we pointed out the possibility to accelerate simple itera- tive schemes via suitably defined extrapolation steps. In this last section of the survey, we are focusing on such accelerated methods. The idea of acceleration dates back to 1980â€™s. The rationale for this research direc- tion is the desire to understand the computational boundaries of solv-

Î“ = {1	if ğ‘˜ = 1,
(5.17)
ing optimization problems. Of particular interest has been the uncon-

ğ‘˜	Î“
ğ‘˜âˆ’1
(1 âˆ’ ğ›¾ )	if ğ‘˜ â‰¥ 2.
strained smooth, and strongly convex optimization problem. This would
be covered by our generic model (P) by setting ğ‘Ÿ = 0, ğ–· = ğ–µ = â„ğ‘› and ğ‘“

General step: For ğ‘˜ = 1, 2, â€¦
Compute
ğ‘§ğ‘˜ = (1 âˆ’ ğ›¾ğ‘˜ )ğ‘¦ğ‘˜âˆ’1 + ğ›¾ğ‘˜ ğ‘¥ğ‘˜âˆ’1,
ğ‘¥ğ‘˜ = CndG(âˆ‡ğ‘“ (ğ‘§ğ‘˜), ğ‘¥ğ‘˜âˆ’1, ğ›½ğ‘˜ , ğœ‚ğ‘˜ ),
ğ‘¦ğ‘˜ = (1 âˆ’ ğ›¾ğ‘˜ )ğ‘¦ğ‘˜âˆ’1 + ğ›¾ğ‘˜ ğ‘¥ğ‘˜.

tially updated sequences. The update of the sequence (ğ‘¥ğ‘˜) is stated in Similarly to accelerated methods, S-CG keeps track of three sequen-
terms of a procedure CndG, which describes an inner loop of condi- tional gradient steps. This subroutine aims at approximately solving for
strongly convex with parameter ğœ‡ğ‘“ > 0 and ğ¿ğ‘“ -smooth. The standard
approach to quantify the computational hardness of optimization prob-
point ğ‘¥, the oracle reports the corresponding function value ğ‘“ (ğ‘¥), and in lems is through the oracle model of optimization; Upon receiving a query first-order models, the function gradient âˆ‡ğ‘“ (ğ‘¥) as well. In their seminal
mization algorithm, there exists an ğ¿ğ‘“ -smooth (with some ğ¿ğ‘“ > 0) and work, Nemirovski and Yudin (1983) showed that for any first-oder opti- convex function ğ‘“ âˆ¶ â„ğ‘› â†’ â„ such that the number of queries required to obtain an ğœ€-optimal solution ğ‘¥âˆ— which satisfies
ğ‘“ (ğ‘¥âˆ—) < min ğ‘“ (ğ‘¥) + ğœ€,
ğ‘¥
is  at  least  of  the  order  of  min{ğ‘›, âˆšğ¿ğ‘“ âˆ•ğœ‡ğ‘“ } ln(1âˆ•ğœ€)  if  ğœ‡ğ‘“ > 0

min ğ‘“ (ğ‘§ğ‘˜) + âˆ‡ğ‘“ (ğ‘§ğ‘˜), ğ‘¥ âˆ’ ğ‘§ğ‘˜
+ ğ›½ğ‘˜
ğ‘¥ âˆ’ ğ‘¥ğ‘˜âˆ’1 2
information-theoretical arguments, turned out to be tight. Nemirovski Nemirovski (1982) proposed a method achieving the optimal rate

ğ‘¥âˆˆğ‘‹	âŸ¨
âŸ©	2 â€–	â€–
ğ‘‚(1âˆ•ğ‘˜2) via a combination of standard gradient steps with the

up to an accuracy of ğœ‚ğ‘˜ . As will become clear later, the S-CG can thus
be thought of as an approximate version of the accelerated scheme pre-
sented in Section 6.1.
The main performance guarantee of the algorithm S-CG is summa- rized in the following theorem:
Theorem 5.4. For all ğ‘˜ â‰¥ 1 and ğ‘¢ âˆˆ ğ–·, we have
classical center of gravity method, which required additional small- dimensional minimization, see also a recent paper (Nesterov et al., 2020). Nesterov (1983) proposed an optimal method with explicit step-sizes, which is nowadays known as Nesterovâ€™s accelerated gradient method.
Accelerated Gradient Method

ğ‘“ (ğ‘¦ğ‘˜) âˆ’ ğ‘“ (ğ‘¢) â‰¤ ğ›½ğ›¾ğ‘˜ Î© + Î“ âˆ‘ ğœ‚ğ‘– ğ›¾ğ‘– ,	(5.18)
In this section we consider one of the multiple variants of an Acceler-

ğ‘˜
ğ‘–=1
Î“ğ‘–
ated Gradient Method. This variant is close to the accelerated proximal



method in Tseng (2008), which has been very influential to the field. An- other very influential version of the accelerated method, especially in applications, is the FISTA algorithm (Beck and Teboulle, 2009a), which is excellently described in Beck (2017). A recent review on accelerated


where in the first inequality used the convexity of ğ‘Ÿ, and in the second inequality we used the convexity of ğ‘“ . Now we plug (6.2) and (6.3) into
(6.1) to obtain
Î¨(ğ‘¥ğ‘˜+1) â‰¤  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) + ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¢ğ‘˜+1))

spired by the Method of Similar Triangles (Gasnikov and Nesterov, 2018; Nesterov, 2018b). For an illustration see Figure 1
Accelerated schemes generically produce three sequences (ğ‘¢ğ‘˜, ğ‘¥ğ‘˜, ğ‘¦ğ‘˜),
+  1  ğ· (ğ‘¢ğ‘˜+1, ğ‘¢ğ‘˜)
ğ´ğ‘˜+1  â„

=  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) +  1  [ğ›¼  (ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¢ğ‘˜+1))

ations are governed by control sequences (ğ´ğ‘˜ )ğ‘˜ and (ğ›¼ğ‘˜ )ğ‘˜. laxation and gradient steps. The relative magnitude of inertia and relax-
The version we present below, is very flexible and allows one to obtain accelerated methods for many settings. As a particular example, below in Section 6.4, we show how a slight modification of this method
+ğ·â„ (ğ‘¢ğ‘˜+1, ğ‘¢ğ‘˜) .	(6.4)
Given the definition of ğ‘¢ğ‘˜+1 as a Prox-Mapping, we can apply (3.12) by substituting ğ‘¥+ = ğ‘¢ğ‘˜+1, ğ‘¥ = ğ‘¢ğ‘˜, ğ›¾ = ğ›¼ğ‘˜+1 . In this way, we obtain, for any
ğ‘¢ âˆˆ ğ–·,

allows one to obtain universal accelerated gradient method.
Î¨(ğ‘¥ğ‘˜+1) â‰¤  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) +  1  (ğ›¼
(ğ‘“ (ğ‘¦ğ‘˜+1) + âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1 + ğ‘Ÿ(ğ‘¢ğ‘˜+1))

Our aim is to solve the composite model problem (P) within a general Bregman proximal setup, formulated in Section 3.2. We are given a DGF
â„ âˆˆ  1(ğ–·). The scaling of the strong convexity parameter to the value
ğ´ğ‘˜+1
+ğ·â„ (ğ‘¢ğ‘˜+1, ğ‘¢ğ‘˜))
ğ´ğ‘˜+1
ğ‘˜+1

actually is without loss of generality, modulo a constant rescaling of
(3â‰¤.12)  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) +  1  (ğ›¼  (ğ‘“ (ğ‘¦ğ‘˜+1) + âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ âˆ’ ğ‘¦ğ‘˜+1 + ğ‘Ÿ(ğ‘¢))

+ğ·â„ (ğ‘¢, ğ‘¢ğ‘˜) âˆ’ ğ·â„ (ğ‘¢, ğ‘¢ğ‘˜+1)
â‰¤  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) + ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¢) + ğ‘Ÿ(ğ‘¢)) +  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜) âˆ’  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜+1)

The Accelerated Bregman Proximal Gradient Method
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1  â„
ğ´ğ‘˜+1  â„

(A-BPGM)
0	0	0
=  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) + ğ›¼ğ‘˜+1 Î¨(ğ‘¢) +  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜) âˆ’  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜+1),	(6.5)

Input: pick ğ‘¥
= ğ‘¢
= ğ‘¦
âˆˆ dom(ğ‘Ÿ) âˆ© ğ–·â—¦, set ğ´0 = 0
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1  â„
ğ´ğ‘˜+1  â„

General step: For ğ‘˜ = 0, 1, â€¦ do:
2	where we also used convexity of ğ‘“ . Multiplying both sides of the last

Find ğ›¼ğ‘˜+1 from quadratic equation ğ´ğ‘˜ + ğ›¼ğ‘˜+1 = ğ¿ğ‘“ ğ›¼ğ‘˜+1 . Set
inequality by ğ´
ğ‘˜+1
, summing these inequalities from ğ‘˜ = 0 to ğ‘˜ = ğ‘ âˆ’ 1,

ğ´ğ‘˜+1 = ğ´ğ‘˜ + ğ›¼ğ‘˜+1 .
and using that ğ´
â€“ ğ´
= âˆ‘ğ‘ âˆ’1 ğ›¼
, we obtain

Set ğ‘¦ğ‘˜+1 = ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜ +  ğ´ğ‘˜  ğ‘¥ğ‘˜.
ğ´ Î¨(ğ‘¥ğ‘ ) â‰¤ ğ´ Î¨(ğ‘¥0) + (ğ´
â€“ ğ´ )Î¨(ğ‘¢) + ğ· (ğ‘¢, ğ‘¢0) âˆ’ ğ· (ğ‘¢, ğ‘¢ğ‘ ).

ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ‘	0
ğ‘	0	â„	â„
(6.6)

Set
Since ğ´ = 0, we can choose ğ‘¢ = ğ‘¥âˆ— âˆˆ argmin{ğ· (ğ‘¢, ğ‘¢0) ğ‘¢ âˆˆ ğ–·âˆ—} âŠ† ğ–·âˆ— and

ğ‘¢ğ‘˜+1 = P â„
(ğ‘¢ğ‘˜, ğ›¼
âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1))
( âˆ— 0 ) â‰¥ 0
â‰¥ 1	â„	|

= argminğ‘¥âˆˆğ–·{ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¦ğ‘˜+1)+âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¥ âˆ’ ğ‘¦ğ‘˜+1âŸ©+ğ‘Ÿ(ğ‘¥))+ğ·â„ (ğ‘¥, ğ‘¢ğ‘˜)}.
Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨min(ğ–·) â‰¤
ğ·â„ (ğ‘¥âˆ—, ğ‘¢0)
,  ğ· (ğ‘¥âˆ—, ğ‘¢ğ‘ ) â‰¤ ğ· (ğ‘¥âˆ—, ğ‘¢0).	(6.7)

Set
ğ‘¥ğ‘˜+1
ğ›¼ğ‘˜+1
ğ´
ğ‘¢ğ‘˜+1
 ğ´ğ‘˜ 
ğ´
ğ‘¥ğ‘˜.

ğ´ğ‘	â„	â„

	ğ‘˜+1	ğ‘˜+1	

We start the analysis A-BPGM applying the descent Lemma property
tween the iterates {ğ‘¢ğ‘ }ğ‘â‰¥0 and the solution ğ‘¥âˆ— is bounded by the Breg- So, we see from the second inequality that the Bregman distance be-
man distance between the starting point and the solution ğ‘¥âˆ—. Then,

from the inequality ğ· (ğ‘¥âˆ—, ğ‘¢ğ‘ ) â‰¥ 1 ğ‘¥âˆ— âˆ’ ğ‘¢ğ‘ 2 it follows that ğ‘¥âˆ— âˆ’ ğ‘¢ğ‘

(3.15) which holds for any two points due to ğ¿ğ‘“ -smoothness:
â„	2 â€–	â€–	â€–	â€–

âŸ¨	âŸ©	2 â€–
â€– (6.1)

in terms of the objective residual it remains to estimate the sequence

Let us next consider the squared norm term. Using the definition of
ğ´ğ‘ from below.
2

ğ‘¥ğ‘˜+1
, ğ‘¦
ğ‘˜+1
and the quadratic equation for ğ›¼ğ‘˜+1 given in the listing of A-
We prove by induction that ğ´ â‰¥ (ğ‘˜+1)  . For ğ‘˜ = 1 this inequality
ğ‘“

BPGM, as well as the 1-strong convexity of the Bregman divergence, i.e. (3.5), we obtain
ğ¿ğ‘“ â€–ğ‘¥ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1 â€–2 = ğ¿ğ‘“ â€– ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜+1 +  ğ´ğ‘˜ ğ‘¥ğ‘˜ âˆ’ ( ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜ +  ğ´ğ‘˜ ğ‘¥ğ‘˜ )â€–2
holds as equality since ğ´0 = 0, and, hence, ğ´1 = ğ›¼1 =  1 . Let us prove
the induction step. From the quadratic equation ğ´ğ‘˜ + ğ›¼ğ‘˜+1 = ğ¿ğ‘“ ğ›¼2  , we
have

2	2  ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1	âˆš

2
ğ‘˜+1
â€–ğ‘¢
â€–	2ğ´ğ‘˜+1 â€–
â€–	ğ´ğ‘˜+1 â„
ğ‘˜+1
2ğ¿ğ‘“
4ğ¿2
ğ¿ğ‘“
2ğ¿
ğ¿ğ‘“
2ğ¿ğ‘“
2ğ¿ğ‘“
2ğ¿ğ‘“

Next, we consider the remaining terms in the r.h.s. of (6.1). Substituting
ğ‘¥ğ‘˜+1 and using ğ´ğ‘˜+1 = ğ´ğ‘˜ + ğ›¼ğ‘˜+1 , we obtain
(6.8)

ğ‘“ (ğ‘¦
ğ‘˜+1
) + âˆ‡ğ‘“ (ğ‘¦
(
ğ‘˜+1
), ğ‘¥
ğ‘˜+1
â€“ ğ‘¦
)
ğ‘˜+1
+ ğ‘Ÿ(ğ‘¥
ğ‘˜+1 )
ğ´ğ‘˜+1
(	)
= ğ´ğ‘˜
+ ğ›¼ğ‘˜+1 â‰¥
(ğ‘˜ + 1)2
4ğ¿ğ‘“
+ ğ‘˜ + 2 = 2ğ¿ğ‘“
ğ‘˜2 + 2ğ‘˜ + 1 + 2ğ‘˜ + 4 â‰¥
4ğ¿ğ‘“
(ğ‘˜ + 2)2
4ğ¿ğ‘“

ğ´ğ‘˜+1	ğ´ğ‘˜+1
+ ğ‘Ÿ( ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜+1 +  ğ´ğ‘˜  ğ‘¥ğ‘˜ )
âŸ¨	ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1	âŸ©
(6.9)
Thus, combining (6.9) with (6.7), we obtain that the A-BPGM has opti-

â‰¤  ğ´ğ‘˜  ğ‘“ (ğ‘¦ğ‘˜+1) + âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¥ğ‘˜ âˆ’ ğ‘¦ğ‘˜+1 + ğ‘Ÿ(ğ‘¥ğ‘˜)
ğ´ğ‘˜+1
+ ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¢ğ‘˜+1))
Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨
min
(ğ–·) â‰¤
4ğ¿ğ‘“ ğ·â„ (ğ‘¥âˆ—, ğ‘¢0) (ğ‘ + 1)2
.	(6.10)

ğ´ğ‘˜+1
â‰¤  ğ´ğ‘˜  (ğ‘“ (ğ‘¥ğ‘˜) + ğ‘Ÿ(ğ‘¥ğ‘˜)) + ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¢ğ‘˜+1))
Closing Remarks Mainly driven by applications in imaging and ma- chine learning, the research on acceleration techniques has been very

=  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) + ğ›¼ğ‘˜+1 (ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¢ğ‘˜+1)),	(6.3)


2013), general proximal setups (Nesterov, 2005b; 2018b), stochastic optimization problems (Dvurechensky et al., 2018a; Dvurechensky and


some fixed point and ğ‘¥ is such that ğ‘¥ âˆ’ ğ‘¥âˆ— 2 â‰¤ ğ‘…2, then
( ğ‘¥ âˆ’ ğ‘¥âˆ— )	Î©

tion with inexact oracle (Cohen et al., 2018; dâ€™Aspremont, 2008; De- volder et al., 2014; Dvurechensky and Gasnikov, 2016; Stonyakin et al., 2020), variance reduction methods (Allen-Zhu, 2017; Frostig et al., 2015; Lan and Zhou, 2017; Lin et al., 2015; Zhang and Xiao, 2015),
where Î© is some known number. For example, in the Euclidean setup Î© = 1, and other examples are given in Juditsky and Nesterov (2014, Section 2.3), where typically Î© = ğ‘‚(ln ğ‘›).

random coordinate descent (Fercoq and RichtÃ¡rik, 2015; Lee and Sid-		

ford, 2013; Lin et al., 2014; Nesterov, 2012; Nesterov and Stich, 2017; Shalev-Shwartz and Zhang, 2014) and other randomized methods such as randomized derivative-free methods (Dvurechensky et al., 2017; Gor-


The Restarted Accelerated Bregman Proximal Gradient Method (R-A-BPGM)
Input: ğ‘§0 âˆˆ dom(ğ‘Ÿ) âˆ© ğ–·â—¦ such that ğ‘§0 âˆ’ ğ‘¥âˆ— 2	2 Î©, ğ¿ , ğœ‡.

bunov et al., 2018; Nesterov and Spokoiny, 2017; Vorontsova et al., 2019b) and randomized directional search (Dvurechensky et al., 2017;
General step: For ğ‘ = 0, 1, â€¦ do:
2	ğ‘¥âˆ’ğ‘§ğ‘
â€–	ğ‘…0 ,	ğ‘“
âˆ’ğ‘

Svaiter, 2013; Nesterov, 2008), and even high-order methods (Baes,

âŒˆ âˆš Î©ğ¿ âŒ‰


Bregman proximal gradient method in the setting of relative smooth- ness (Hanzely et al., 2021) and relative strong convexity (Dvurechensky et al., 2021; Hendrikx et al., 2020) (see Section 3.3.3 for the definition of relative smoothness). Yet, the negative result of (Dragomir et al., 2021) suggest that, in general, the acceleration in the relative smoothness set- ting is not possible.
As it was mentioned above, accelerated gradient method in the form of A-BPGM can serve as a template for many acceleration techniques.
The examples of accelerated methods which have a close form include
point ğ‘¥0 = ğ‘§ğ‘ and proximal setup given by DFG â„ğ‘ (ğ‘¥)
Set ğ‘§ğ‘+1 = ğ‘¥ğ‘ .

We next use the above assumptions to show the accelerated loga-
imal steps to find a point ğ‘¥Ì‚ such that ğ‘“ (ğ‘¥Ì‚) âˆ’ ğ‘“ (ğ‘¥âˆ—)  ğœ€ is proportional to rithmic complexity of R-A-BPGM, i.e. that the number of Bregman prox-


ğ¿ğ‘“ âˆ•ğœ‡ log2(1âˆ•ğœ€) instead of (ğ¿ğ‘“ âˆ•ğœ‡) log2(1âˆ•ğœ€) for the BPGM under the er-
ror bound condition. The idea of the proof is to show by induction that,
for all ğ‘  0, ğ‘§ğ‘ âˆ’ ğ‘¥âˆ— 2	ğ‘…2. For ğ‘ = 0 this holds by the assumption on

primal-dual accelerated methods (Dvurechensky et al., 2018b; Lin et al.,	0
â€–	â€–	ğ‘

ized algorithms (Diakonikolas and Orecchia, 2018; Dvurechensky et al., 2017; Fercoq and RichtÃ¡rik, 2015), methods for stochastic optimization
definition of â„ğ‘âˆ’1 , assumptions about â„, and the inductive assumption,
we have

(Dvurechensky et al., 2018a; Lan, 2012), methods with inexact oracle (Cohen et al., 2018) and inexact model of the objective (Gasnikov and Tyurin, 2019; Stonyakin et al., 2020). Moreover, only using this one-
ğ·	ğ‘¥âˆ—, ğ‘§
ğ‘âˆ’1
ğ‘âˆ’1
) â‰¤ â„
ğ‘âˆ’1
(ğ‘¥âˆ—) = ğ‘…2
ğ‘§ğ‘âˆ’1 âˆ’ ğ‘¥âˆ— (6â‰¤.12) Î©ğ‘…ğ‘âˆ’1
ğ‘…ğ‘âˆ’1	2
.	(6.13)

projection version it was possible to obtain accelerated gradient meth- ods with inexact model of the objective (Gasnikov and Tyurin, 2019),
our  choice  of  the  number  of  steps  ğ‘ ,  we  obtain Thus, applying the error bound condition (6.11), the bound (6.10) and

accelerated decentralized distributed algorithms for stochastic convex	ğœ‡
(6.11)
(6.10) ğ¿ğ‘“ ğ·â„
(ğ‘¥âˆ— , ğ‘§ğ‘âˆ’1 ) (6.13) ğ¿ğ‘“ Î©ğ‘…2

ğ‘ âˆ’ âˆ— 2
â‰¤ Î¨(ğ‘§ğ‘) âˆ’ Î¨
(ğ–·) = Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨
(ğ–·) â‰¤
	ğ‘âˆ’1	
â‰¤ 	ğ‘âˆ’1

optimization (Dvinskikh et al., 2019; Gorbunov et al., 2019; Rogozin et al., 2021), and accelerated method for stochastic optimization with heavy-tailed noise (Gorbunov et al., 2020; 2021). The key to the last two
â€–ğ‘§
ğ‘¥		
2
ğ‘âˆ’1 =
8
 min
ğœ‡ğ‘…2
2 .
min
(ğ‘ + 1)2
2(ğ‘ + 1)2

results is the proof that the sequence generated by the one-projection ac-
So, we obtain that
ğ‘ âˆ’ âˆ—  â‰¤
= ğ‘…
â‹… 2âˆ’ğ‘ and Î¨(ğ‘§ğ‘) âˆ’ Î¨
(ğ–·) â‰¤

celerated gradient method is bounded with large probability, which, to	ğœ‡ ğ‘…2 â‹…2âˆ’2ğ‘
â€–ğ‘§
ğ‘¥ â€–
ğ‘…ğ‘	0
min

our knowledge, is not possible to prove for other types of accelerated methods applied to stochastic optimization problems.
02	. To estimate the total number of basic steps of A-BPGM to
achieve Î¨(ğ‘§ğ‘) âˆ’ Î¨min(ğ–·)  ğœ€, we need to multiply the suï¬ƒcient num-

ber of restarts ğ‘Ì‚ = âŒˆ 1 log2	âŒ‰ by the number of A-BPGM steps ğ‘ in
ğœ‡ğ‘…2


Linear Convergence

Under additional assumptions, we can use the scheme A-BPGM to obtain a linear convergence rate, or, in other words, logarithmic in the
that Î¨(ğ‘¥) satisfies a quadratic error bound condition for some ğœ‡ > 0: desired accuracy complexity bound. One such possible assumption is
each restart. This leads to the complexity estimate ğ‘‚(âˆš Î©ğ¿ğ‘“ log2	0 which is optimal (Nemirovski and Yudin, 1983; Nesterov, 2018b) for first-order methods applied to smooth strongly convex optimization problems.
Closing Remarks The restart technique which we used above was

Î¨(ğ‘¥) âˆ’ Î¨
min
(ğ–·) â‰¥ ğœ‡ ğ‘¥
2
â€“ ğ‘¥âˆ— 2.	(6.11)
extended in the past 20 years to many settings including problems with non-quadratic error bound condition (Juditsky and Nesterov,

This is a weaker assumption than the assumption that Î¨(ğ‘¥) is ğœ‡-strongly convex with ğœ‡ > 0. For a review of different additional conditions
which allow to obtain linear convergence rate we refer the reader to Bolte et al. (2017); Necoara et al. (2019). The linear convergence rate can be obtained under quadratic error bound condition by a widely used restart technique, which dates back to Nemirovskii and Nesterov (1985); Nesterov (1983).
sumptions. First, without loss of generality, we assume that 0 âˆˆ ğ–·, To apply the restart technique, we make several additional as- 0 = arg minğ‘¥âˆˆğ–· â„(ğ‘¥) and â„(0) = 0. Second, we assume that we are given
a starting point ğ‘¥0 âˆˆ ğ–· and a number ğ‘… > 0 such that ğ‘¥0 âˆ’ ğ‘¥âˆ— 2  ğ‘…2.
2014; Roulet and dâ€™Aspremont, 2017), stochastic optimization prob- lems (Bayandina et al., 2018; Dvurechensky and Gasnikov, 2016; Gas- nikov and Dvurechensky, 2016; Ghadimi and Lan, 2013; Juditsky and Nesterov, 2014), methods with inexact oracle (Dvurechensky and Gas- nikov, 2016; Gasnikov and Dvurechensky, 2016), randomized methods (Allen-Zhu and Hazan, 2016; Fercoq and Qu, 2020), conditional gra- dient (Kerdreux et al., 2019; Lan, 2013), variational inequalities and saddle-point problems (Stonyakin et al., 2018; 2020), methods for con- strained optimization problems (Bayandina et al., 2018). There are ver- sions of this technique even for discrete and submodular optimization
problems (Pokutta, 2020).

0
Finally, we make the assumption that
â€–	â€–	0
A possible drawback of the restart scheme is that one has to

0	â€–	â€–



directly incorporating the parameter ğœ‡ into the steps of A-BPGM,
see e.g. dâ€™Aspremont et al. (2021); Devolder (2013); Lan (2020);
Nesterov (2018b); Stonyakin et al. (2020). Yet, in this case, a stronger
Here the adjoint operator ğ€âˆ— is defined by equality  ğ€ğ‘¥, ğ‘¤ ğ–¤ =
ğ€âˆ— ğ‘¤, ğ‘¥ ğ–µ and the norm of the operator	ğ–µ,ğ–¤ is defined by ğ€	= max { ğ€ğ‘¥, ğ‘¤ âˆ¶ ğ‘¥	= 1, ğ‘¤	= 1}. Since ğ–¶ is bounded, Î¨ (ğ‘¥) is a


vex (Lu et al., 2018) is used. The second drawback of the restart
Î¨ (ğ‘¥) â‰¤ Î¨(ğ‘¥) â‰¤ Î¨ (ğ‘¥) + ğœÎ˜  (ğ–¶),	(6.17)

technique and direct incorporation of ğœ‡ into the steps, is that both
require to know the value of the parameter ğœ‡. This is in contrast
to non-accelerated BPGM, which using the same step-size as in the
ğœ
where
ğ‘¤
ğœ
(ğ–¶) âˆ¶= max{â„ğ‘¤
â„ğ‘¤
(ğ‘§) ğ‘§ âˆˆ ğ–¶}, assumed to be a finite number.

(	2 )


ğœ	ğ‘¥	1

	
only rough estimates of the parameter ğœ‡ are proposed in Fercoq and 2019). Several recipes on how to restart accelerated methods with


0 â‰¤ Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨
â„ğ‘¥
(ğ–·) â‰¤ Î¨ (ğ‘¥ğ‘ ) + ğœÎ˜
ğ‘¥
(ğ–¶) âˆ’ Î¨ (ğ‘¥âˆ—) â‰¤ Î¨ (ğ‘¥ğ‘ ) + ğœÎ˜
(ğ–¶) âˆ’ Î¨ (ğ‘¥âˆ—) â‰¤ ğœÎ˜
(ğ–¶)

Qu (2020) and a parameter-free accelerated method is proposed in Carderera et al. (2021); Nesterov (2013).
min
ğœ
+ 4ğ¿ğœ Î˜â„ğ‘¥ (ğ–·) (ğ‘ + 1)2
â„ğ‘¤

4 ğ€ 2 Î˜
ğœ
(ğ–·)
ğœ
4ğ¿ Î˜
(ğ–·)
â„ğ‘¤
ğœ  ğœ
â„ğ‘¤

= ğœÎ˜ (ğ–¶) +
â€– â€–ğ–µ,ğ–¤
â„ğ‘¥
+	ğ‘“
â„ğ‘¥	.

An important observation made during the last 20 years of devel-


Choosing ğœ to minimize the r.h.s., i.e. ğœ = 2â€–ğ€â€–ğ–µ,ğ–¤ âˆš Î˜â„ğ‘¥ (ğ–·) , we obtain

a large gap between the optimal convergence rate for black-box non-

 
â‰¤
â‰¤  â€– â€–
âˆšÎ˜  (ğ–·)Î˜
(ğ–¶)

Certainly, there arises the need to understand how this significant gap can be reduced. An important step towards that direction is Nesterovâ€™s smoothing technique (Nesterov, 2005b). To motivate this approach, let us make the following thought experiment. Assume that we mini-
mize a smooth function by ğ‘ steps of A-BPGM, i.e. solve problem
(6.18)
A more careful analysis in the proof of Nesterov (2005b, Theorem 3), allows also to obtain an approximate solution to the conjugate problem
max{ğœ“ (ğ‘¤) âˆ¶= âˆ’ğœ…(ğ‘¤) + min ( ğ€ğ‘¥, ğ‘¤ + ğ‘“ (ğ‘¥))}.	(6.19)

(P) with ğ‘Ÿ = 0. Then at each iteration we observe first-order information
(ğ‘“ (ğ‘¦ğ‘˜+1), âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1)) and can construct a non-smooth piecewise linear ap-
ğ‘¤âˆˆğ–¶
ğ‘¥âˆˆğ–· âŸ¨	âŸ©

proximation of ğ‘“ as ğ‘”(ğ‘¥) = max
{ğ‘“ (ğ‘¦ğ‘˜+1) + âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¥ âˆ’ ğ‘¦ğ‘˜+1 }. If
In each iteration of A-BPGM, the optimizer needs to calculate âˆ‡Î¨ğœ (ğ‘¦ğ‘˜+1),

ğ‘˜=1,...,ğ‘	âŸ¨
âŸ©	which requires to calculate ğ‘¤Ì‚ (ğ‘¦ğ‘˜+1). This information is aggregated to

we now make ğ‘ steps of A-BPGM with the same starting point to min-
imize ğ‘”(ğ‘¥), and choose the appropriate subgradients of ğ‘”(â‹…), the steps
ğœ
obtain the vector ğ‘¤Ì‚ğ‘ =  ğ‘âˆ’1 ğ›¼ğ‘˜+1 ğ‘¤Ì‚ (ğ‘¦ğ‘˜+1) and is used to obtain the

will be absolutely the same as when we minimized ğ‘“ (ğ‘¥), and we will be able to minimize a non-smooth function ğ‘” with much faster rate 1âˆ•ğ‘ 2 than the lower bound 1âˆ• ğ‘ . This leads to a way of trying to find a
ğ‘˜=0
following primal-dual result
0 â‰¤ Î¨(  ) âˆ’ Î¨  ( ) â‰¤ Î¨(  ) âˆ’ (
ğ´ğ‘˜+1  ğœ
) â‰¤ 4â€–ğ€â€–ğ–µ,ğ–¤ âˆšÎ˜â„ğ‘¥ (ğ–·)Î˜â„ğ‘¤ (ğ–¶) + 4ğ¿ğ‘“ Î˜â„ (ğ–·)

suï¬ƒciently wide class of non-smooth functions which can be eï¬ƒciently minimized by A-BPGM. To do so in a systematic way, we have to leave the pure black-box model of convex programming.
ğ‘¥ğ‘
min ğ–·
ğ‘¥ğ‘
ğœ“ ğ‘¤Ì‚ğ‘		
ğ‘ + 1
ğ‘¥
(ğ‘ + 1)2
.
(6.20)

Consider the model problem (P), with the added assumption that the non-smooth part admits a Fenchel representation of the form
ğ‘Ÿ(ğ‘¥) = max{ ğ€ğ‘¥, ğ‘¤ âˆ’ ğœ…(ğ‘¤)}.	(6.14)
ğ‘¤âˆˆğ–¶

Here, ğ–¶ âŠ† ğ–¤ is a compact convex subset of a finite-dimensional real vec- tor space ğ–¤, and ğœ… âˆ¶ ğ–¶ â†’ â„ is a continuous convex function on ğ–¶. ğ€ is a linear operator from ğ–µ to ğ–¤âˆ—. This additional structure of the problem
gives rise to a min-max formulation of (P), given by
min max{ğ‘“ (ğ‘¥) + ğ€ğ‘¥, ğ‘¤ âˆ’ ğœ…(ğ‘¤)}.	(6.15)
to obtain convergence rate ğ‘‚(1âˆ•ğ‘ ) for non-smooth optimization, which In both cases using the special structure of the problem it is possible is better than the lower bound ğ‘‚(1âˆ• ğ‘ ) for general non-smooth opti-
mization problems.
We illustrate the smoothing technique by two examples of piecewise- linear minimization; see also Figure 2

fit of some signal ğ‘ âˆˆ ğ–¤, given linear observations ğ€ğ‘¥. where ğ€ âˆ¶ ğ–µ â†’ ğ–¤ Example 6.1 (Uniform fit). Consider the problem of finding a uniform
is a bounded linear operator. This problem amount to minimize the non-

ğ‘¥âˆˆğ–· ğ‘¤âˆˆğ–¶

tion ğ‘Ÿ can be well approximated by a class of smooth convex functions, The main idea of Nesterov is based on the observation that the func- defined as follows. Let â„ğ‘¤ âˆˆ 1(ğ–¶) with a nonrestrictive assumption
ğ€ğ‘¥ âˆ’ ğ‘	. Of course, this problem can be equivalently
parameter vector ğ‘¥ is large, such a direct approach could turn out to be formulated as an LP, however in case where the dimensionality of the
not very practical. Adopting the just introduced smoothing technology, the representation (6.15) can be obtained using the definition of the dual

that minğ‘§âˆˆğ–¶ â„ğ‘¤ (ğ‘§) = 0. For given ğœ > 0, define the function
norm  â‹…
, i.e.
ğ€ğ‘¥ âˆ’ ğ‘
= max
ğ€ğ‘¥ âˆ’ ğ‘, ğ‘¤ . Yet, a better rep-

â€– â€–1	â€–	â€–âˆ
ğ‘¤âˆ¶â€–ğ‘¤â€–1 â‰¤1 âŸ¨
âŸ©	âˆ‘2

Î¨ (ğ‘¥) âˆ¶= ğ‘“ (ğ‘¥) + max{ ğ€ğ‘¥, ğ‘¤ âˆ’ ğœ…(ğ‘¤) âˆ’ ğœâ„ (ğ‘¤)}.	(6.16)
Since â„ğ‘¤ is 1-strongly convex on the compact convex set ğ–¶, the inner
Bregman setup is the noâˆ‘rm â€–ğ‘¤â€–ğ–¤
â€–ğ‘¤â€–1
+	ğ‘–=1  ğ‘–
and the Boltzmann-Shannon


main technical lemma needed for the analysis is the following.
Î¨ (ğ‘¥) = max{ ğ€Ì‚ ğ‘¥ âˆ’ ğ‘Ì‚, ğ‘¤ âˆ’ ğœâ„ (ğ‘¤)} = ğœ ln
(  1
ğ‘š
exp
(  âŸ¨ğ‘ğ‘– , ğ‘¥âŸ© âˆ’ ğ‘ğ‘– )


Proposition 6.1 (Nesterov (2005b)). The function Î¨ğœ (ğ‘¥) is well defined, convex and continuously diï¬€erentiable at any ğ‘¥ âˆˆ ğ–· with âˆ‡Î¨ğœ (ğ‘¥) = âˆ‡ğ‘“ (ğ‘¥) +
ğ€âˆ— ğ‘¤Ì‚ğœ (ğ‘¥). Moreover, âˆ‡Î¨ğœ (ğ‘¥) is Lipschitz continuous with constant ğ¿ğœ = ğ¿ğ‘“ +
ğ€ 2
.
ğœ
ğœ	ğ‘¤âˆˆğ–¶	ğ‘¤
+ exp (âˆ’ âŸ¨ğ‘ğ‘– , ğ‘¥âŸ© âˆ’ ğ‘ğ‘– )),
which is recognized as a softmax function.
2ğ‘š
ğ‘–=1	ğœ

Fig. 1. Illustration of the three sequences of the A-BPGM in the uncon-
strained case ğ–· = â„ğ‘›, ğ‘Ÿ = 0, â„ = 1 ğ‘¥ 2. In this simple case it is easy to


see that ğ‘¢ğ‘˜+1 = ğ‘¢ğ‘˜ âˆ’ ğ›¼
ğ‘˜+1
2	2
, and the sequence
ğ‘¢ğ‘˜ accumulates the

previous gradient, while helping to keep momentum. Also by the similar-
ity of the triangles, ğ‘¥ğ‘˜+1 = ğ‘¦ğ‘˜+1 âˆ’ ğ›¼  âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1) â‹… ğ›¼ğ‘˜+1 = ğ‘¦ğ‘˜+1 âˆ’ 1 âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1),

ğ‘˜+1
ğ´ğ‘˜+1
ğ¿ğ‘“

i.e. ğ‘¦ğ‘˜ is the sequence obtained by gradient descent steps. Finally, the se-
quence ğ‘¥ğ‘˜ is a convex combination of the momentum step and the gra-
dient step. The illustration is inspired by personal communication with Yu. Nesterov on the Method of Similar Triangles (Gasnikov and Nesterov, 2018; Nesterov, 2018b).










Fig. 2. Absolute value function ğ‘¥ , its softmax smoothing and Huber smooth- ing, both with ğœ = 1.
Example 6.2 (. ğ“1-fit) In compressed sensing Candes and Tao (2007);
minimize the ğ“1 norm of the residual vector ğ€ğ‘¥ âˆ’ ğ‘ over a given closed Candes et al. (2006); Donoho (2006) one encounters the problem to
convex set ğ–·. While it is well-known that this problem can in principle again be reformulated as an LP, the typical high-dimensionality of such problems makes this direct approach often not practicable. Adopting the
smoothing technology, it is natural to choose ğ–¶ = {ğ‘¤ âˆˆ â„ğ‘š ğ‘¤   1}
the above rate estimate with the one reported in Nemirovski (2004), one observes that the bound in Nemirovski (2004) has a similar to (6.20) structure, yet with the second term being non-accelerated,
i.e. proportional to 1âˆ•ğ‘ . This approach was generalized to obtain
an accelerated method for a special class of variational inequali-
ties in Chen et al. (2017), where an optimal iteration complexity
ğ‘‚(ğ¿âˆ•  ğœ€) to reach an ğœ€-close solution is reported. In the original
paper (Nesterov, 2005b), the smoothing parameter is fixed and re-
quires to know the parameters of the problem in advance. This has been improved in Nesterov (2005a), where an adaptive version of the smoothing techniques is proposed. This framework was extended in Alacaoglu et al. (2017); Tran-Dinh et al. (2020); Tran-Dinh and Cevher (2014); Tran-Dinh et al. (2018) for structured composite opti- mization problems in the form (2.7) and a related primal-dual represen- tation (2.8). A related line of works studies minimization of strongly con- vex functions under linear constraints. Similarly to (6.16) the objective in the Lagrange dual problem has Lipschitz gradient, yet the challenge is that the feasible set in the dual problem is not bounded. Despite that it is possible to obtain accelerated primal-dual methods (Anikin et al., 2017; Chernov et al., 2016; Dvurechensky et al., 2016; 2018b; Guminov et al., 2021; 2019; Ivanova et al., 2020; Kroshnin et al., 2019; Nesterov et al., 2020; Tran-Dinh and Cevher, 2014; Tran-Dinh et al., 2018). In particular, this allows to obtain improved complexity bounds for differ- ent types of optimal transport problems (Dvurechensky et al., 2018a; 2018b; Guminov et al., 2021; Kroshnin et al., 2019; Lin et al., 2020; 2019; 2019; Tupitsa et al., 2020; Uribe et al., 2018).

Universal Accelerated Method

1 âˆ‘ğ‘š  â€– â€–	2
|â€– â€–âˆ
As it was discussed in the previous subsection, there is a gap in the

and â„ğ‘¤ (ğ‘¤) = 2



ğ‘–=1
ğ‘ğ‘– ğ–¤,âˆ— ğ‘¤ğ‘– , which gives
ğ‘š
	

( |âŸ¨ğ‘ , ğ‘¥âŸ© âˆ’ ğ‘ |)
convergence rate between the class of non-smooth convex optimization problems and the class of smooth convex optimization problems. In this subsection, we present a unifying framework (Nesterov, 2015) for these

where ğœ“ (ğ‘¡) is the Huber function equal to ğ‘¡2âˆ•(2ğœ) for 0 â‰¤ ğ‘¡ â‰¤ ğœ and ğ‘¡ âˆ’
ğœâˆ•2 if ğ‘¡ â‰¥ ğœ.
For the particular case of smoothing the absolute value function |ğ‘¥|,
for both classes by a single method without the need to know whether the objective is smooth or non-smooth.
Consider the Problem (P) with ğ‘“ which belongs to the class of func-
tions with HÃ¶lder-continuous subgradients, i.e. for some ğ¿ > 0 and

ing and Huber smoothing, both with ğœ = 1. Potentially, other ways Figure 2 gives the plot of the original function, its softmax smooth-
of smoothing a non-smooth function can be applied, see Beck and Teboulle (2012) for a general framework.
Closing Remarks Let us make several remarks on the related literature. A close approach is proposed in Nemirovski (2004), where the problem (6.15) is considered directly as a min-max saddle-point problem. These classes of equilibrium problems are typically solved via tools from mono- tone variational inequalities, whose performance is typically worse than the performance of optimization algorithms. In particular, contrasting
ğœˆ
ğœˆ
âˆ—	ğœˆ
and all ğ‘“ (ğ‘¥) âˆˆ ğœ•ğ‘“ (ğ‘¥) and ğ‘“ (ğ‘¦) âˆˆ ğœ•ğ‘“ (ğ‘¦). If ğœˆ = 1, we recover the ğ¿ğ‘“ - smoothness condition (2.1). If ğœˆ = 0 we have that ğ‘“ has bounded varia-
tion of the subgradient, which is essentially equivalent to the bounded subgradient Assumption 8. The main observation (Devolder et al., 2014; Nesterov, 2015) is that this HÃ¶lder condition allows to prove an inex- act version of the â€descent Lemmaâ€ inequality (3.15). More precisely
Nesterov (2015, Lemma 2), for any ğ‘¥, ğ‘¦ âˆˆ dom ğ‘“ and any ğ›¿ > 0,
ğ‘“ (ğ‘¦) â‰¤ ğ‘“ (ğ‘¥) + ğ‘“ â€²(ğ‘¥), ğ‘¦ âˆ’ ğ‘¥ +  ğ¿ğœˆ  ğ‘¦ âˆ’ ğ‘¥ 1+ğœˆ â‰¤ ğ‘“ (ğ‘¥) + ğ‘“ â€²(ğ‘¥), ğ‘¦ âˆ’ ğ‘¥ + ğ¿ ğ‘¦ âˆ’ ğ‘¥ 2 + ğ›¿,

âŸ¨	âŸ©  1 + ğœˆ â€–	â€–	âŸ¨	âŸ©  2 â€–	â€–


the same steps as the derivation of the convergence rate for A-BPGM in Section 6.1. The first thing which is changed is equation (6.1), where now the inexact descent Lemma is used instead of the exact one. The
only difference is that ğ¿ğ‘“ is changed to its local approximation ğ¿ğ‘˜+1 and
add the error term ğœ€ğ›¼ğ‘˜+1 appears in the r.h.s. In (6.2) the new quadratic
2ğ´ğ‘˜+1
equation with ğ¿ğ‘˜+1 is used and the inequality remains the same. This
term  ğœ€ğ›¼ğ‘˜+1  in the r.h.s. Finally, this leads to the bound eventually leads to (6.5) with the only change being an additive error
2ğ´ğ‘˜+1

Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨min
(ğ–·) â‰¤ ğ·â„ (ğ‘¢âˆ—, ğ‘¢0) + ğœ€
ğ´ğ‘	2

After some algebraic manipulation, Nesterov (2015, p.397) obtains an
inequality ğ´  â‰¥ ğ‘ 1+ğœˆ ğœ€ 1+ğœˆ . Substituting, we obtain
1+3ğœˆ 1âˆ’ğœˆ
ğ‘	2+4ğœˆ  2 
2 1+ğœˆ ğ¿ 1+ğœˆ
2+4ğœˆ	 2 

2 1+ğœˆ ğ· (ğ‘¢âˆ—, ğ‘¢0)ğ¿ 1+ğœˆ
Î¨(ğ‘¥ ) âˆ’ Î¨min(ğ–·)	+
ğœ€ .

1+3ğœˆ
1âˆ’ğœˆ	2

Fig. 3. Non-smooth function ğ‘“ (ğ‘¥) = max{ğ‘¥ âˆ’ 1, ğ‘¥âˆ•2}, a quadratic function con-
ğ‘ 1+ğœˆ ğœ€ 1+ğœˆ
Since the method does not require to know ğœˆ and ğ¿ğœˆ , the iteration com- plexity to achieve accuracy ğœ€ is

structed using the first-order information at the point ğ‘¥ = 2, and a shifted
quadratic function constructed using the first-order information at the point
ğ‘¥ = 2. As one can see, adding a shift allows to obtain an upper quadratic bound
=	inf ( ğ¿ğœˆ
âœğœˆâˆˆ[0,1]	ğœ€
 2  1+3ğœˆ (
 1+ğœˆ
ğ·â„ (ğ‘¢âˆ— , ğ‘¢0 ) 1+3ğœˆ
âŸ

for the objective, which is then minimized to obtain a new test point.




where


(6.21)
It is easy to see that the oracle complexity, i.e. the number of proximal
calls for each ğ‘˜ is 2(ğ‘–ğ‘˜ + 1). Further, ğ¿ğ‘˜+1 = 2ğ‘–ğ‘˜ âˆ’1 ğ¿ğ‘˜ , which means that operations, is approximately the same. Indeed, the number of oracle the total number of the oracle calls up to iteration ğ‘ is  ğ‘âˆ’1 2(ğ‘– + 1) = âˆ‘ğ‘ âˆ’1 2(2 log2 ğ¿ğ‘˜+1 ) = 4ğ‘ + 2 log2 ğ¿ğ‘ , i.e. is, up to a logarithmic

ğ¿ â‰¥
( ) âˆ¶=  1 âˆ’ ğœˆ 1 1+ğœˆ
1 + ğœˆ ğ›¿
2
1+ğœˆ
ğœˆ
(6.22)
term, four times larger than ğ‘ . The obtained oracle complexity coin-
cides up to a constant factor with the lower bound (Nemirovski and

with the convention that 00 = 1. We illustrate this by Figure 3 where we plot a quadratic bound in the r.h.s. of (6.21) with ğ›¿ = 0 and a
tions with HÃ¶lder-continuous gradients. In the particular case ğœˆ = 0, we Yudin, 1983) for first-order methods applied to minimization of func-
ğ¿2 ğ·â„ (ğ‘¢âˆ— ,ğ‘¢0 )

quadratic bound can not be an upper bound for ğ‘“ (ğ‘¦) for any ğ¿ > 0, and
the positive shift allows to construct an upper bound. Thus, it is suf-
ficient to equip the A-BPGM with a backtracking line-search to obtain

gence rate 1âˆ• ğ‘˜, which is typical for general non-smooth minimization. In the opposite case of smooth minimization corresponding to ğœˆ = 1, we
ğ¿ ğ· (ğ‘¢âˆ— ,ğ‘¢0 )

sal Accelerated Bregman Proximal Gradient Method (U-A-BPGM). A key step is the potentially non-monotone adjustment of the local Lipschitz
gradient estimate ğ¿ğ‘˜ .
timal convergence rate 1âˆ•ğ‘˜2. The same idea can be used to obtain uni-
versal version of BPGM (Nesterov, 2015). One can also use the strong
convexity assumption to obtain faster convergence rate of U-A-BPGM

		either by restarts (Kamzolov et al., 2020; Roulet and dâ€™Aspremont,

The Universal Accelerated Bregman Proximal Gradient Method (U-A-BPGM)
Input: Pick ğ‘¥0 = ğ‘¢0 = ğ‘¦0 âˆˆ dom(ğ‘Ÿ) âˆ© ğ–·â—¦, ğœ€ > 0, 0 < ğ¿0 < ğ¿(ğœ€âˆ•2), set
ğ´0 = 0
General step: For ğ‘˜ = 0, 1, â€¦ do:
Find the smallest integer ğ‘–	0 such that if one defines ğ›¼
2017), or by incorporating the strong convexity parameter in the steps (Stonyakin et al., 2020). The same backtracking line-search can be ap-
plied in a much simpler way if one knows that ğ‘“ is ğ¿ğ‘“ -smooth with
tice caused by a pessimistic estimate for ğ¿ğ‘“ (Dvinskikh et al., 2020; some unknown Lipschitz constant or to achieve acceleration in prac-
Dvurechensky et al., 2016; 2018b; Malitsky and Pock, 2018; Nesterov,

ğ‘˜
ğ‘– âˆ’1	2
ğ‘˜+1
2013; Tran-Dinh et al., 2018). The idea is to use the standard exact â€de-

from the quadratic equation ğ´ğ‘˜ + ğ›¼ğ‘˜+1 = 2 ğ‘˜
ğ´ğ‘˜+1 = ğ´ğ‘˜ + ğ›¼ğ‘˜+1 ,
sets ğ‘¦ğ‘˜+1 = ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜ +  ğ´ğ‘˜  ğ‘¥ğ‘˜,
ğ¿ğ‘˜ğ›¼ğ‘˜+1 , sets
scent Lemmaâ€ inequality in each step of the accelerated method.
The idea of universal methods turned out to be very productive and

sets ğ‘¢ğ‘˜+1 =
ğ´ğ‘˜+1
ğ´ğ‘˜+1
several extensions have been proposed in the literature, including uni- versal primal-dual methods (Baimurzina et al., 2019), universal method

argminğ‘¥âˆˆğ–· ğ›¼ğ‘˜+1 ğ‘“ (ğ‘¦ğ‘˜+1) + ğ‘“ â€²(ğ‘¦ğ‘˜+1), ğ‘¥ âˆ’ ğ‘¦ğ‘˜+1 + ğ‘Ÿ(ğ‘¥) + ğ·â„ (ğ‘¥, ğ‘¢ğ‘˜) ,
sets ğ‘¥ğ‘˜+1 = ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜+1 +  ğ´ğ‘˜  ğ‘¥ğ‘˜ , then it holds that ğ‘“ (ğ‘¥ğ‘˜+1) â‰¤
for convex and non-convex optimization (Ghadimi et al., 2019), and a universal primal-dual hybrid of accelerated gradient method with con-



ğ‘“ (ğ‘¦ğ‘˜+1) + ğ‘“ â€²(ğ‘¦ğ‘˜+1), ğ‘¥ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1
+ 2ğ‘–ğ‘˜ âˆ’1 ğ¿ğ‘˜
ğ‘¥ğ‘˜+1 âˆ’ ğ‘¦ğ‘˜+1 2 + ğœ€ğ›¼ğ‘˜+1 .
(Nesterov et al., 2020). The above-described method is not the only way

2
Set ğ¿ğ‘˜+1 = 2ğ‘–ğ‘˜ âˆ’1 ğ¿ğ‘˜ and go to the next iterate ğ‘˜.
We first observe that for suï¬ƒciently large ğ‘– , 2ğ‘– âˆ’1 ğ¿
â€–	2ğ´ğ‘˜+1

â‰¥ ğ¿( ğœ€ğ›¼ğ‘˜+1 ), see
to obtain adaptive and universal methods for smooth and non-smooth optimization problems. An alternative way which uses the norm of the current (sub)gradient to define the step-size was initiated probably by Polyak (1987) and became very popular in stochastic optimization for


Nesterov (2015, p.396). This means that the process of finding ğ‘–ğ‘˜ is finite
since the condition which is checked for each ğ‘–ğ‘˜ is essentially (6.21) with
ğ›¿	ğœ€ğ›¼ğ‘˜+1 . The proof of convergence of (U-A-BPGM) follows essentially
2ğ´ğ‘˜+1
it was possible to obtain for ğœˆ âˆˆ {0, 1} universal accelerated optimiza-
tion method (Levy et al., 2018) and universal methods for variational
inequalities and saddle-point problems (Bach and Levy, 2019).



Connection between Accelerated method and Conditional Gradient

In this subsection we describe how a variant of conditional gradient method can be obtained as a particular case of A-BPGM with inexact
means that the change of (3.12) to (3.14) with Î” = Î©2 leads to an addi- tive term  Î©2  in the r.h.s. of (6.5):
ğ´ğ‘˜+1
Î¨(ğ‘¥ğ‘˜+1) â‰¤  ğ´ğ‘˜  Î¨(ğ‘¥ğ‘˜) + ğ›¼ğ‘˜+1 Î¨(ğ‘¢) +  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜) âˆ’  1  ğ· (ğ‘¢, ğ‘¢ğ‘˜+1) +  Î©2  ,  ğ‘¢ âˆˆ ğ–·.

Bregman Proximal step. We assume that ğ‘“ is ğ¿ğ‘“ -smooth and for sim-
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1
ğ´ğ‘˜+1
(6.24)

plicity choose â„ to be the squared Euclidean norm â„(ğ‘¥) = 1 ğ‘¥ 2. Since

2
we consider a conditional gradient method, it is natural to assume that
Multiplying both sides of the last inequality by ğ´ğ‘˜+1 , summing these in-

the set ğ–· is bounded with maxğ‘¥,ğ‘¢âˆˆğ–·
ğ· (ğ‘¥, ğ‘¢) â‰¤ Î©2	1  2
2 (ğ–·). We fol-
equalities from ğ‘˜ = 0 to ğ‘˜ = ğ‘ âˆ’ 1, and using that ğ´ğ‘ âˆ’ ğ´0 =
ğ‘ âˆ’1 ğ›¼ğ‘˜+1 ,

2 â€–â‹…â€–
we obtain

vation of is that the Prox-Mapping in A-BPGM can be calculated inex-	ğ‘
actly by applying the generalized linear oracle given in Definition 5.2.
0	ğ‘	0	â„
â„
(6.25)

The idea is very similar to the conditional gradient sliding described in Section 5.3.4 with the difference that here we implement an approx- imate Bregman Proximal step using only one step of the generalized
Since ğ´0 = 0, we can choose ğ‘¢ = ğ‘¥âˆ— âˆˆ argmin{ğ·â„ (ğ‘¢, ğ‘¢0) ğ‘¢ âˆˆ ğ–·âˆ—}, so that, for all ğ‘	1,

conditional gradient method. The resulting algorithm is listed below
Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨
(ğ–·) â‰¤ ğ·â„ (ğ‘¥âˆ—, ğ‘¢0) + ğ‘ Î©2 â‰¤ Î©2
+ ğ‘ Î©2

with the only difference with A-BPGM being the change of the Breg-
man Proximal step ğ‘¢ğ‘˜+1 =	( ğ‘˜	âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1)) to the step ğ‘¢ğ‘˜+1 =
min
ğ´ğ‘	ğ´ğ‘
2ğ´	ğ´	,

ğ–·,ğ›¼
ğ‘˜+1
ğ›¼ğ‘˜+1 ğ‘Ÿ ğ‘¢ , ğ›¼ğ‘˜+1
ğ‘Ÿ(ğ›¼ğ‘˜+1 âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1)) given by generalized linear oracle.
which, given the lower bound ğ´ğ‘
(ğ‘ +1)2 leads to the final result for
4ğ¿ğ‘“

the convergence rate of this inexact A-BPGM implemented via general- ized linear oracle:

Conditional Gradient Method by A-BPGM with Approximate Bregman Proximal Step
Input: pick ğ‘¥0 = ğ‘¢0 = ğ‘¦0 âˆˆ dom(ğ‘Ÿ) âˆ© ğ–·â—¦, set ğ´0 = 0
Î¨(ğ‘¥ğ‘ ) âˆ’ Î¨min
(ğ–·) â‰¤ 2ğ¿ğ‘“ Î©2
(ğ‘ + 1)2
+ 4ğ¿ğ‘“ Î©2
ğ‘ + 1

General step: For ğ‘˜ = 0, 1, â€¦ do:
Find ğ›¼ğ‘˜+1 from quadratic equation ğ´ğ‘˜ + ğ›¼ğ‘˜+1 = ğ¿ğ‘“ ğ›¼2
. Set
Thus, we obtain a variant of conditional gradient method with the
same convergence rate 1âˆ•ğ‘ as for the standard conditional gradient

ğ´ğ‘˜+1
= ğ´ğ‘˜
+ ğ›¼
ğ‘˜+1 .
ğ‘˜+1
method. Using the same approach, but with U-A-BPGM as the basis

Set ğ‘¦ğ‘˜+1 = ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜ +  ğ´ğ‘˜  ğ‘¥ğ‘˜.
method, one can obtain a universal version of conditional gradient

ğ´ğ‘˜+1
ğ´ğ‘˜+1
method (Stonyakin et al., 2020) for minimizing objectives with HÃ¶lder-

Set (Approximate Bregman proximal step by generalized linear oracle)
ğ‘¢Gğ‘˜+1 = argminğ‘¥âˆˆğ–· ğ›¼ğ‘˜+1 ğ‘“ (ğ‘¦ğ‘˜+1) + âŸ¨âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¥ âˆ’ ğ‘¦ğ‘˜+1âŸ© + ğ‘Ÿ(ğ‘¥)	=
continuous gradient. The bounds in this case a similar to the ones ob- tained in a more direct universal method in Nesterov (2018a). Similar


Set ğ‘¥ğ‘˜+1 = ğ›¼ğ‘˜+1 ğ‘¢ğ‘˜+1 +  ğ´ğ‘˜  ğ‘¥ğ‘˜ .

ğ´ğ‘˜+1
ğ´ğ‘˜+1
Conclusion





A-BPGM is in one simple change of the step for ğ‘¢ğ‘˜+1, to obtain the con- Since the difference between such conditional gradient method and
vergence rate of the former, it is suï¬ƒcient to track, what changes such approximate Bregman Proximal step entails in the convergence rate proof for A-BPGM. In other words, we need to understand what hap- pens with the proof for A-BPGM if the Bregman Proximal step is made inexactly by applying the generalized linear oracle. The first important difference is that we need an inexact version of inequality (3.12), which was used in the convergence proof of A-BPGM and which the result of the exact Bregman Proximal step. To obtain its inexact version, let us denote
ğœ‘(ğ‘¥) = ğ›¼ğ‘˜+1 ğ‘“ (ğ‘¦ğ‘˜+1) + âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1), ğ‘¥ âˆ’ ğ‘¦ğ‘˜+1 + ğ‘Ÿ(ğ‘¥) .
set ğ–· to obtain ğ‘¢ğ‘˜+1. Thus, by the optimality condition, we have that Then generalized linear oracle actually minimizes this function on the there exists ğœ‰ âˆˆ ğœ•ğœ‘(ğ‘¢ğ‘˜+1) such that ğœ‰, ğ‘¢ğ‘˜+1 âˆ’ ğ‘¥  0 for all ğ‘¥ âˆˆ ğ–·. Now
we remind that the Bregman Proximal step in A-BPGM minimizes
ğœ‘(ğ‘¥) + ğ·â„ (ğ‘¥, ğ‘¢ğ‘˜). These observations allow to estimate the inexactness
of the Bregman Proximal step implemented via generalized linear ora-
We close this survey, with a very important fact which Nesterov writes in the introduction of his important textbook (Nesterov, 2018b): in general, optimization problems are unsolvable. Convex programming stands out from this general fact, since it describes a significantly large class of model problems, with important practical applications, for which general solution techniques have been developed within the mathematical framework of interior-point techniques. However, mod- ern optimization problems are large-scale in nature, which renders these polynomial time methods impractical. First-order methods have become the gold standard in balancing cheap iterations with low solution accu- racy, and many theoretical and practical advances having been made in the last 20 years.
Despite the fact that convex optimization is approaching the state
of being a primitive similar to linear algebra techniques, we foresee that the development of first-order methods has not come to a halt yet. In connection with stochastic inputs, the combination of acceler- ation techniques with other performance boosting tricks, like variance reduction, incremental techniques, as well as distributed optimization, still promises to produce some new innovations. On the other hand,

cle. Indeed, for ğ‘¢ğ‘˜+1 = G
ğ–·,ğ›¼ğ‘˜+1 ğ‘Ÿ
(ğ›¼ğ‘˜+1
âˆ‡ğ‘“ (ğ‘¦ğ‘˜+1))
there is also still much room for improvement of algorithms for opti- mization problems which do not admit a prox-friendly geometry. Dis-

+ âˆ‡â„(ğ‘¢ğ‘˜+1) âˆ’ âˆ‡â„(ğ‘¢ğ‘˜), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¥ â‰¤ âˆ‡ (ğ‘¢ğ‘˜+1) âˆ’ âˆ‡â„(ğ‘¢ğ‘˜), ğ‘¢ğ‘˜+1 âˆ’ ğ‘¥
âŸ¨ğœ‰
tributed optimization, in particular in the context of federated learning

â„	â„	â„
max  ğ· (ğ‘¥, ğ‘¢)  Î©2 . This inequality provides inexact version of the where we used three-point identity in Lemma 3.3 and that optimality condition (3.10) in the problem minğ‘¥âˆˆğ–·{ğœ‘(ğ‘¥) + ğ·â„ (ğ‘¥, ğ‘¢ğ‘˜)}, i.e. (3.13) with Î” = Î©2 . This in order leads to (3.14) with Î” = Î©2 , which is
the desired inexact version of (3.12).
Let us now see, how this affects the convergence rate proof of A- BPGM. Inequality (3.12) was used in the analysis only in (6.5). This

recent review of federated learning and (Gorbunov et al., d) for a re- cent review of distributed optimization. Another important focus in the research in optimization methods is now on numerical methods for non- convex optimization motivated by training of deep neural networks, see Danilova et al. (2020); Sun (2019) for a recent review. A number of open questions remain in the theory of first-order methods for variational in- equalities and saddle-point problems, mainly in the case of variational inequalities with non-monotone operators. In particular, recently the



authors of (Cohen et al., 2021) observed a connection between extra- gradient methods for monotone variational inequalities and accelerated first-order methods. Thus, as we emphasize in this survey, new con- nections, that are still continuously being discovered between different methods and different formulations, can lead to new understanding and developments in this lively field of first-order methods.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgment

The authors are grateful to Yurii Nesterov and Alexander Gas- nikov for fruitful discussions. Mathias Staudigl thanks the COST Action CA16228 (European Network for Game Theory), the FMJH Program PGMO and from the support of EDF (Project â€Privacy preserving algo- rithms for distributed control of energy marketsâ€) for its support. The research of Pavel Dvurechensky is supported by the Ministry of Science and Higher Education of the Russian Federation (Goszadaniye) No.075- 00337-20-03, project No. 0714-2020-0005.

References

Alacaoglu, A., Tran Dinh, Q., Fercoq, O., Cevher, V., 2017. Smooth primal-dual co- ordinate descent algorithms for nonsmooth convex optimization. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems, Vol. 30. Curran Asso- ciates, Inc., pp. 5852â€“5861.
Allen-Zhu, Z., 2017. Katyusha: The first direct acceleration of stochastic gradient methods. In: Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017. ACM, New York, NY, USA, pp. 1200â€“1205.
Allen-Zhu, Z., Hazan, E., 2016. Optimal black-box reductions between optimization ob- jectives. In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (Eds.), Ad- vances in Neural Information Processing Systems, Vol. 29. Curran Associates, Inc.,
pp. 1614â€“1622.
Alvarez, F., Bolte, J., Brahic, O., 2004. Hessian Riemannian gradient flows in convex pro- gramming. SIAM Journal on Control and Optimization 43 (2), 477â€“501.
Andersen, E.D., Andersen, K.D., 2000. The Mosek Interior Point Optimizer for Linear Pro- gramming: An Implementation of the Homogeneous Algorithm. Springer US, Boston, MA, pp. 197â€“232.
Anikin, A.S., Gasnikov, A.V., Dvurechensky, P.E., Tyurin, A.I., Chernov, A.V., 2017. Dual approaches to the minimization of strongly convex functionals with a simple structure under aï¬ƒne constraints. Computational Mathematics and Mathematical Physics 57 (8), 1262â€“1276.
Arrow, K., Hurwicz, L., Uzawa, H., 1958. Studies in linear and non-linear programming. In: Chenery, H., Johnson, S., Karlin, S., Marschak, T., Solow, R. (Eds.), Stanford Math- ematical Studies in the Social Sciences, Vol. II. Stanford University Press, Stanford.
Attouch, H., Bolte, J., Redont, P., Teboulle, M., 2004. Singular Riemannian barrier meth- ods and gradient-projection dynamical systems for constrained optimization. Opti- mization 53 (5-6), 435â€“454.
Attouch, H., Cabot, A., Frankel, P., Peypouquet, J., 2011. Alternating proximal algorithms for linearly constrained variational inequalities: application to domain decomposition for pdeâ€™s. Nonlinear Analysis: Theory, Methods & Applications 74 (18), 7455â€“7473.
Attouch, H., Chbani, Z., Fadili, J., Riahi, H., 2020. First-order optimization algo- rithms via inertial systems with hessian driven damping. Mathematical Programming doi:10.1007/s10107-020-01591-1.
Attouch, H., Chbani, Z., Peypouquet, J., Redont, P., 2018. Fast convergence of inertial dy- namics and algorithms with asymptotic vanishing viscosity. Mathematical Program- ming 168 (1-2), 123â€“175.
Attouch, H., Redont, P., Soubeyran, A., 2007. A new class of alternating proximal min- imization algorithms with costs-to-move. SIAM Journal on Optimization 18 (3), 1061â€“1081.
Attouch, H., Teboulle, M., 2004. Regularized lotka-volterra dynamical system as contin- uous proximal-like method in optimization. Journal of Optimization Theory and Ap- plications 121 (3), 541â€“570.
Auslender, A., Teboulle, M., 2006. Asymptotic cones and functions in optimization and variational inequalities. Springer Science & Business Media.
Auslender, A., Teboulle, M., 2006. Interior gradient and proximal methods for convex and conic optimization. SIAM Journal on Optimization 16 (3), 697â€“725.
Auslender, A., Teboulle, M., 2009. Projected subgradient methods with non-euclidean dis- tances for non-differentiable convex minimization and variational inequalities. Math- ematical Programming 120 (1), 27â€“48.
Bach, F., 2015. Duality between subgradient and conditional gradient methods. SIAM Journal on Optimization 25 (1), 115â€“129.
Bach, F., Levy, K.Y., 2019. A universal algorithm for variational inequalities adaptive to smoothness and noise. In: Beygelzimer, A., Hsu, D. (Eds.), Proceedings of the Thir- ty-Second Conference on Learning Theory, Vol. 99 of Proceedings of Machine Learn- ing Research, PMLR, Phoenix, USA, pp. 164â€“194.
Baes, M., 2009. Estimate sequence methods: extensions and approximations, Institute for Operations Research. ETH, ZÃ¼rich, Switzerland.
Bah, B., Rauhut, H., Terstiege, U., Westdickenberg, M., 2019. Learning deep linear neural networks: Riemannian gradient flows and convergence to global minimizers. Infor- mation and Inference: A Journal of the IMAIaaa039.
Baimurzina, D.R., Gasnikov, A.V., Gasnikova, E.V., Dvurechensky, P.E., Ershov, E.I., Kubentaeva, M.B., Lagunovskaya, A.A., 2019. Universal method of searching for equi- libria and stochastic equilibria in transportation networks. Computational Mathemat- ics and Mathematical Physics 59 (1), 19â€“33.
Banert, S., BoÅ¢, R.I., Csetnek, E.R., 2021. Fixing and extending some recent results on the admm algorithm. Numerical Algorithms 86 (3), 1303â€“1325.
Bauschke, H., Borwein, J., Combettes, P., 2003. Bregman monotone optimization algo- rithms. SIAM Journal on Control and Optimization 42 (2), 596â€“636.
Bauschke, H.H., Bolte, J., Teboulle, M., 2016. A descent lemma beyond lipschitz gradient continuity: First-order methods revisited and applications. Mathematics of Operations Research 42 (2), 330â€“348.
Bauschke, H.H., Combettes, P.L., 2016. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer - CMS Books in Mathematics.
Bayandina, A., Dvurechensky, P., Gasnikov, A., Stonyakin, F., Titov, A., 2018. Mirror de- scent and convex optimization problems with non-smooth inequality constraints. In: Giselsson, P., Rantzer, A. (Eds.), Large-Scale and Distributed Optimization. Springer International Publishing, pp. 181â€“215. Ch. 8
Beck, A., 2017. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics.
Beck, A., Guttmann-Beck, N., 2019. FOM â€“a matlab toolbox of first-order methods for solving convex optimization problems. Optimization Methods and Software 34 (1), 172â€“193.
Beck, A., Shtern, S., 2017. Linearly convergent away-step conditional gradient for non-strongly convex functions. Mathematical Programming 164 (1-2), 1â€“27.
Beck, A., Teboulle, M., 2003. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters 31 (3), 167â€“175.
Beck, A., Teboulle, M., 2004. A conditional gradient method with linear rate of conver- gence for solving convex linear systems. Mathematical Methods of Operations Re- search 59 (2), 235â€“247.
Beck, A., Teboulle, M., 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences 2 (1), 183â€“202.
Beck, A., Teboulle, M., 2009. Gradient-based algorithms with applications to signal re- covery. In: Palomar, D.P., Eldar, Y.C. (Eds.), Convex optimization in signal processing and communications. Cambridge University Press, pp. 42â€“88.
Beck, A., Teboulle, M., 2012. Smoothing and first order methods: A unified framework.
SIAM Journal on Optimization 22 (2), 557â€“580.
Ben-Tal, A., Margalit, T., Nemirovski, A., 2001. The ordered subsets mirror descent opti- mization method with applications to tomography. SIAM Journal on Optimization 12 (1), 79â€“108.
Ben-Tal, A., Nemirovski, A., 2020. Lectures on Modern Convex Opti- mization (Lecture Notes). Personal web-page of A. Nemirovski. URL https://www2.isye.gatech.edu/~nemirovs/LMCOLN2020WithSol.pdf
BenaÃ¯m, M., 1998. Recursive algorithms, urn processes, and the chaining number of chain recurrent sets. Ergodic Theory and Dynamical Systems 18, 53â€“87.
Benveniste, A., MÃ©tivier, M., Priouret, P., 1990. Adaptive Algorithms and Stochastic Ap- proximations. Springer, Berlin.
Bian, W., Chen, X., 2015. Linearly constrained non-lipschitz optimization for image restoration. SIAM Journal on Imaging Sciences 8 (4), 2294â€“2322.
Bian, W., Chen, X., Ye, Y., 2015. Complexity analysis of interior point algorithms for non-lipschitz and nonconvex minimization. Mathematical Programming 149 (1), 301â€“327.
Bolte, J., Nguyen, T.P., Peypouquet, J., Suter, B.W., 2017. From error bounds to the com- plexity of first-order descent methods for convex functions. Mathematical Program- ming 165 (2), 471â€“507.
Bolte, J., Teboulle, M., 2003. Barrier operators and associated gradient-like dynamical systems for constrained minimization problems. SIAM Journal on Control and Opti- mization 42 (4), 1266â€“1292.
Bomze, I.M., Mertikopoulos, P., Schachinger, W., Staudigl, M., 2019. Hessian barrier algo- rithms for linearly constrained optimization problems. SIAM Journal on Optimization 29 (3), 2100â€“2127.
Bottou, L., Curtis, F., Nocedal, J., 2018. Optimization methods for large-scale machine learning. SIAM Review 60 (2), 223â€“311.
Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and TrendsÂ® in Machine learning 3 (1), 1â€“122.
Boyd, S., Vandenberghe, L., 2004. Convex optimization. Cambridge university press.
Bruckstein, A., Donoho, D., Elad, M., 2009. From sparse solutions of systems of equations to sparse modeling of signals and images. SIAM Review 51 (1), 34â€“81.
Bubeck, S., 2015. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning 8 (3-4), 231â€“357.
BÃ¼hlmann, P., van de Geer, S., 2011. Statistics for high-dimensional data. Springer Series in Statistics. Springer-Verlag Berlin Heidelberg.
BÃ¹i, M.N., Combettes, P.L., 2021. Bregman forward-backward operator splitting. Set-Val- ued and Variational Analysis 29 (3), 583â€“603.
Candes, E., Tao, T., 2007. The Dantzig selector: Statistical estimation when ğ‘ is much
larger than ğ‘›. The Annals of Statistics 35 (6), 2313â€“2351.



Candes, E.J., Romberg, J., Tao, T., 2006. Robust uncertainty principles: exact signal re- construction from highly incomplete frequency information. IEEE Transactions on Information Theory 52 (2), 489â€“509.
Canon, M.D., Cullum, C.D., 1968. A tight upper bound on the rate of convergence of frank-wolfe algorithm. SIAM Journal on Control 6 (4), 509â€“516.
Carderera, A., Besancon, M., Pokutta, S.. Simple steps are all you need: Frank-wolfe and generalized self-concordant functions.
Carderera, A., Diakonikolas, J., Lin, C.Y., Pokutta, S., 2021. Parameter-free locally accel- erated conditional gradients. In: Meila, M., Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, PMLR, pp. 1283â€“1293.
Censor, Y., Zenios, S.A., 1992. Proximal minimization algorithm withd-functions. Journal of Optimization Theory and Applications 73 (3), 451â€“464.
Cesa-Bianchi, N., Lugosi, G., 2006. Prediction, Learning, and Games. Cambridge University Press.
Chambolle, A., Pock, T., 2011. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision 40 (1), 120â€“145.
Chen, C., He, B., Ye, Y., Yuan, X., 2016. The direct extension of admm for multi-block con- vex minimization problems is not necessarily convergent. Mathematical Programming 155 (1-2), 57â€“79.
Chen, G., Teboulle, M., 1993. Convergence analysis of a proximal-like minimization algo- rithm using Bregman functions. SIAM Journal on Optimization 3 (3), 538â€“543.
Chen, Y., Lan, G., Ouyang, Y., 2017. Accelerated schemes for a class of variational in- equalities. Mathematical Programming. 10.1007/s10107-017-1161-4
Chernov, A., Dvurechensky, P., Gasnikov, A., 2016. Fast primal-dual gradient method for strongly convex minimization problems with linear constraints. In: Kochetov, Y., Khachay, M., Beresnev, V., Nurminski, E., Pardalos, P. (Eds.), Discrete Optimiza- tion and Operations Research: 9th International Conference, DOOR 2016, Vladivos- tok, Russia, September 19-23, 2016, Proceedings. Springer International Publishing,
pp. 391â€“403.
Cohen, M., Diakonikolas, J., Orecchia, L., 2018. On acceleration with noise-corrupted gra- dients. In: Dy, J., Krause, A. (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR. StockholmsmÃ¤ssan, Stockholm Sweden, pp. 1019â€“1028.
Cohen, M.B., Sidford, A., Tian, K., 2021. Relative lipschitzness in extragradient methods and a direct recipe for acceleration. In: Lee, J.R. (Ed.), 12th Innova- tions in Theoretical Computer Science Conference (ITCS 2021), pp. 62:1â€“62:18. doi:10.4230/LIPIcs.ITCS.2021.62. Vol. 185 of Leibniz International Proceedings in Informatics (LIPIcs), Schloss Dagstuhlâ€“Leibniz-Zentrum fÃ¼r Informatik, Dagstuhl, Ger- many
Combettes, P.L., Pesquet, J.C., 2011. Proximal splitting methods in signal processing.
Springer, pp. 185â€“212.
Combettes, P.L., Wajs, V.R., 2005. Signal recovery by proximal forward-backward split- ting. Multiscale Modeling & Simulation 4 (4), 1168â€“1200.
Cox, B., Juditsky, A., Nemirovski, A., 2014. Dual subgradient algorithms for large-scale nonsmooth learning problems. Mathematical Programming 148 (1), 143â€“180.
Damla Ahipasaoglu, S., Sun, P., Todd, M.J., 2008. Linear convergence of a modified frankâ€“wolfe algorithm for computing minimum-volume enclosing ellipsoids. Optimi- sation Methods and Software 23 (1), 5â€“19.
Danilova, M., Dvurechensky, P., Gasnikov, A., Gorbunov, E., Guminov, S., Kam- zolov, D., Shibaev, I., 2020. Recent theoretical advances in non-convex optimization. arXiv:2012.06188Accepted to be a part of Springer volume â€High Dimensional Opti- mization and Probabilityâ€.
dâ€™Aspremont, A., 2008. Smooth optimization with approximate gradient. SIAM J. on Op- timization 19 (3), 1171â€“1183.
dâ€™Aspremont, A., Scieur, D., Taylor, A.. Acceleration methods.
Daubechies, I., Defrise, M., De Mol, C., 2004. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on pure and applied mathematics 57 (11), 1413â€“1457.
Davis, D., Drusvyatskiy, D., Kakade, S., Lee, J.D., 2020. Stochastic subgradient method converges on tame functions. Foundations of Computational Mathematics 20 (1), 119â€“154.
Devolder, O., 2013. Exactness, inexactness and stochasticity in first-order methods for large-scale convex optimization Ph.d. thesis.
Devolder, O., Glineur, F., Nesterov, Y., 2014. First-order methods of smooth convex opti- mization with inexact oracle. Mathematical Programming 146 (1), 37â€“75.
Diakonikolas, J., Orecchia, L., 2018. Alternating randomized block coordinate descent. In: Dy, J., Krause, A. (Eds.), Proceedings of the 35th International Conference on Ma- chine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stock- holmsmÃ¤ssan, Stockholm Sweden, pp. 1224â€“1232.
Doljansky, M., Teboulle, M., 1998. An interior proximal algorithm and the exponential multiplier method for semidefinite programming. SIAM Journal on Optimization 9 (1), 1â€“13.
Donoho, D.L., 2006. Compressed sensing. IEEE Transactions on Information Theory 52 (4), 1289â€“1306.
Dragomir, R.-A., Taylor, A.B., dâ€™Aspremont, A., Bolte, J., 2021. Optimal complex- ity and certification of bregman first-order methods. Mathematical Programming doi:10.1007/s10107-021-01618-1.
Duchi, J., Hazan, E., Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12, 2121â€“2159. (Jul.)
Duchi, J., Shalev-Shwartz, S., Singer, Y., Tewari, A., 2010. Composite objective mirror descent. In: COLT 2010 - The 23rd Conference on Learning Theory, pp. 14â€“26.
Duchi, J.C., Ruan, F., 2018. Stochastic methods for composite and weakly convex opti- mization problems. SIAM Journal on Optimization 28 (4), 3229â€“3259.
Dunn, J.C., 1979. Rates of convergence for conditional gradient algorithms near singu- lar and nonsingular extremals. SIAM Journal on Control and Optimization 17 (2), 187â€“211.
Dvinskikh, D., Gorbunov, E., Gasnikov, A., Dvurechensky, P., Uribe, C.A., 2019. On primal and dual approaches for distributed stochastic convex optimization over networks. In: 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 7435â€“7440.
Dvinskikh, D., Ogaltsov, A., Gasnikov, A., Dvurechensky, P., Spokoiny, V., 2020. On the line-search gradient methods for stochastic optimization. IFAC-PapersOnLine 53 (2), 1715â€“1720.
Dvurechensky, P., Dvinskikh, D., Gasnikov, A., Uribe, C.A., NediÄ‡, A., 2018. Decentral- ize and randomize: Faster algorithm for Wasserstein barycenters. In: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Ad- vances in Neural Information Processing Systems, Vol. 31. Curran Associates, Inc.,
pp. 10783â€“10793. NeurIPS 2018
Dvurechensky, P., Gasnikov, A., 2016. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. Journal of Optimization Theory and Appli- cations 171 (1), 121â€“145.
Dvurechensky, P., Gasnikov, A., Gasnikova, E., Matsievsky, S., Rodomanov, A., Usik, I., 2016. Primal-dual method for searching equilibrium in hierarchical congestion popu- lation games. In: Supplementary Proceedings of the 9th International Conference on Discrete Optimization and Operations Research and Scientific School (DOOR 2016) Vladivostok, Russia, September 19 - 23, 2016, pp. 584â€“595.
Dvurechensky, P., Gasnikov, A., Kroshnin, A., 2018. Computational optimal transport: Complexity by accelerated gradient descent is better than by Sinkhornâ€™s algorithm. In: Dy, J., Krause, A. (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, pp. 1367â€“1376.
Dvurechensky, P., Gasnikov, A., Tiurin, A., Zholobov, V.. Unifying framework for acceler- ated randomized methods in convex optimization.
Dvurechensky, P., Gorbunov, E., Gasnikov, A., 2021. An accelerated directional derivative method for smooth stochastic convex optimization. European Journal of Operational Research 290 (2), 601â€“621.
Dvurechensky, P., Kamzolov, D., Lukashevich, A., Lee, S., Ordentlich, E., Uribe, C.A., Gasnikov, A.. Hyperfast second-order local solvers for eï¬ƒcient statistically precon- ditioned distributed optimization.
Dvurechensky, P., Ostroukhov, P., Safin, K., Shtern, S., Staudigl, M., 2020. Self-concordant analysis of frank-Wolfe algorithms. In: DaumÃ© III, H., Singh, A. (Eds.), Proceedings of the 37th International Conference on Machine Learning, Vol. 119 of Proceedings of Machine Learning Research, PMLR, Virtual, pp. 2814â€“2824.
Dvurechensky, P., Safin, K., Shtern, S., Staudigl, M.. Generalized self-concordant analysis of Frank-Wolfe algorithms.
Dvurechensky, P.E., Gasnikov, A.V., Nurminski, E.A., Stonyakin, F.S., 2020. Advances in Low-Memory Subgradient Optimization. Springer International Publishing, Cham,
pp. 19â€“59.
Eckstein, J., 1994. Some saddle-function splitting methods for convex programming. Op- timization Methods and Software 4 (1), 75â€“83.
Eckstein, J., Bertsekas, D.P., 1992. On the douglasâ€”rachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Program- ming 55 (1-3), 293â€“318.
Epelman, M., Freund, R.M., 2000. Condition number complexity of an elementary algo- rithm for computing a reliable solution of a conic linear system. Mathematical Pro- gramming 88 (3), 451â€“485.
Facchinei, F., Pang, J.S., 2003. Finite-dimensional variational inequalities and comple- mentarity problems - volume i and volume II. Springer Series in Operations Research.
Feizollahi, M.J., Ahmed, S., Sun, A., 2017. Exact augmented lagrangian duality for mixed integer linear programming. Mathematical Programming 161 (1), 365â€“387.
Fercoq, O., Qu, Z., 2020. Restarting the accelerated coordinate descent method with a rough strong convexity estimate. Computational Optimization and Applications 75 (1), 63â€“91.
Fercoq, O., RichtÃ¡rik, P., 2015. Accelerated, parallel, and proximal coordinate descent.
SIAM Journal on Optimization 25 (4), 1997â€“2023.
Frank, M., Wolfe, P., et al., 1956. An algorithm for quadratic programming. Naval research logistics quarterly 3 (1-2), 95â€“110.
Freund, R.M., Grigas, P., 2016. New analysis and results for the frankâ€“wolfe method.
Mathematical Programming 155 (1-2), 199â€“230.
Frostig, R., Ge, R., Kakade, S., Sidford, A., 2015. Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization. In: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd International Conference on Machine Learn- ing, Vol. 37 of Proceedings of Machine Learning Research, PMLR, Lille, France,
pp. 2540â€“2548.
Gabay, D., 1983. Applications of the method of multipliers to variational inequalities, 15.
Elsevier, pp. 299â€“331. Ch. ix
Garber, D., Hazan, E., 2015. Faster rates for the frank-wolfe method over strongly-convex sets. In: 32nd International Conference on Machine Learning, ICML 2015.
Garber, D., Hazan, E., 2016. A linearly convergent variant of the conditional gradient algo- rithm under strong convexity, with applications to online and stochastic optimization. SIAM Journal on Optimization 26 (3), 1493â€“1528.
Gasnikov, A., Dvurechensky, P., Gorbunov, E., Vorontsova, E., Selikhanovych, D., Uribe, C.A., Jiang, B., Wang, H., Zhang, S., Bubeck, S., Jiang, Q., Lee, Y.T., Li, Y., Sid- ford, A., 2019. Near optimal methods for minimizing convex functions with lipschitz
ğ‘-th derivatives. In: Beygelzimer, A., Hsu, D. (Eds.), Proceedings of the Thirty-Second
Conference on Learning Theory, Vol. 99 of Proceedings of Machine Learning Research, PMLR, Phoenix, USA, pp. 1392â€“1393.
Gasnikov, A.V., Dvurechensky, P.E., 2016. Stochastic intermediate gradient method for convex optimization problems. Doklady Mathematics 93 (2), 148â€“151.
Gasnikov, A.V., Nesterov, Y.E., 2018. Universal method for stochastic composite optimiza- tion problems. Computational Mathematics and Mathematical Physics 58 (1), 48â€“64.




lems with an oracle producing a (ğ›¿, l)-model of function at the requested point. Com- Gasnikov, A.V., Tyurin, A.I., 2019. Fast gradient descent for convex minimization prob-
putational Mathematics and Mathematical Physics 59 (7), 1085â€“1097.
Ghadimi, S., Lan, G., 2012. Optimal stochastic approximation algorithms for strongly con- vex stochastic composite optimization i: A generic algorithmic framework. SIAM Jour- nal on Optimization 22 (4), 1469â€“1492.
Ghadimi, S., Lan, G., 2013. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: Shrinking procedures and optimal al- gorithms. SIAM Journal on Optimization 23 (4), 2061â€“2089.
Ghadimi, S., Lan, G., Zhang, H., 2019. Generalized uniformly optimal methods for non- linear programming. Journal of Scientific Computing 79 (3), 1854â€“1881.
Glowinski, R., Tallec, P.L., 1989. Augmented Lagrangian and operator-splitting methods in nonlinear mechanics. SIAM.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial nets. In: Advances in neural information processing systems, pp. 2672â€“2680.
Gorbunov, E., Danilova, M., Gasnikov, A., 2020. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (Eds.), Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., pp. 15042â€“15053.
Gorbunov, E., Danilova, M., Shibaev, I., Dvurechensky, P., Gasnikov, A.. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavyâ€“ tailed noise.
Gorbunov, E., Dvinskikh, D., Gasnikov, A.. Optimal decentralized distributed algorithms for stochastic convex optimization.
Gorbunov, E., Dvurechensky, P., Gasnikov, A.. An accelerated method for derivative-free smooth stochastic convex optimization.
Gorbunov, E., Rogozin, A., Beznosikov, A., Dvinskikh, D., Gasnikov, A., d. Recent the- oretical advances in decentralized distributed convex optimization. arXiv preprint arXiv:2011.13259Accepted to be a part of Springer volume â€High Dimensional Opti- mization and Probabilityâ€.
Gower, R.M., Schmidt, M., Bach, F., RichtÃ¡rik, P., 2020. Variance-reduced methods for machine learning. In: Proceedings of the IEEE, Vol. 108, pp. 1968â€“1983.
GuÃ©lat, J., Marcotte, P., 1986. Some comments on wolfeâ€™s â€˜away stepâ€™. Mathematical Pro- gramming 35 (1), 110â€“119.
Guminov, S., Dvurechensky, P., Tupitsa, N., Gasnikov, A., 2021. On a combination of alternating minimization and Nesterovâ€™s momentum. In: Meila, M., Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, PMLR, Virtual, pp. 3886â€“3898.
Guminov, S.V., Nesterov, Y.E., Dvurechensky, P.E., Gasnikov, A.V., 2019. Accelerated pri- mal-dual gradient descent with linesearch for convex, nonconvex, and nonsmooth optimization problems. Doklady Mathematics 99 (2), 125â€“128.
Haeser, G., Liu, H., Ye, Y., 2018. Optimality condition and complexity analysis for linearly- constrained optimization without differentiability on the boundary. Mathematical Programming doi:10.1007/s10107-018-1290-4.
Hanzely, F., RichtÃ¡rik, P., Xiao, L., 2021. Accelerated bregman proximal gradient methods for relatively smooth convex optimization. Computational Optimization and Applica- tions 79 (2), 405â€“440.
Helmke, U., Moore, J.B., 1996. Optimization and dynamical systems. Communications & Control Engineering. Springer Berlin Heidelberg.
Hendrikx, H., Xiao, L., Bubeck, S., Bach, F., Massoulie, L., 2020. Statistically precondi- tioned accelerated gradient method for distributed optimization. In: DaumÃ© III, H., Singh, A. (Eds.), Proceedings of the 37th International Conference on Machine Learn- ing, Vol. 119 of Proceedings of Machine Learning Research, PMLR, pp. 4203â€“4227.
Hiriart-Urrut, J.-B., LemarÃ©chal, C., 2001. Fundamentals of Convex Analysis. Springer. Holloway, C.A., 1974. An extension of the frank and wolfe method of feasible directions.
Mathematical Programming 6 (1), 14â€“27.
Ivanova, A., Dvurechensky, P., Gasnikov, A., Kamzolov, D., 2020. Composite optimization for the resource allocation problem. Optimization Methods and Software 0 (0), 1â€“35. doi:10.1080/10556788.2020.1712599.
Jaggi, M., 2013. Revisiting frank-wolfe: Projection-free sparse convex optimization. Inter- national Conference on Machine Learning 427â€“435.
Jain, P., Kar, P., 2017. Non-convex optimization for machine learning. Found. Trends Mach. Learn. 10 (3-4), 142â€“336.
with applications to ğ“1 -minimization. Mathematical Programming 142 (1), 269â€“310. Juditsky, A., KÄ±lÄ±nÃ§ Karzan, F., Nemirovski, A., 2013. Randomized first order algorithms
Juditsky, A., Nazin, A.V., Tsybakov, A.B., Vayatis, N., 2005. Recursive aggregation of estimators by the mirror descent algorithm with averaging. Problems of Information Transmission 41 (4), 368â€“384.
Juditsky, A., Nemirovski, A., 2011. First order methods for nonsmooth convex large-scale optimization, i: general purpose methods. In: Optimization for Machine Learning. MIT Press, pp. 121â€“148. Ch. 5
Juditsky, A., Nemirovski, A., 2011. First order methods for nonsmooth convex large-scale optimization, ii: utilizing problems structure. In: Optimization for Machine Learning. MIT Press, pp. 149â€“183. Ch. 6
Juditsky, A., Nesterov, Y., 2014. Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stochastic Systems 4 (1), 44â€“80.
Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., Dâ€™Oliveira, R.G.L., Eichner, H., Rouay- heb, S.E., Evans, D., Gardner, J., Garrett, Z., GascÃ³n, A., Ghazi, B., Gibbons, P.B.,
Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., KonecnÃ½, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., Ã–zgÃ¼r, A., Pagh, R., Qi, H., Ram-
age, D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S.U., Sun, Z., Suresh, A.T.,
TramÃ¨r, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F.X., Yu, H.,
Zhao, S., 2021. Advances and open problems in federated learning. Foundations and TrendsÂ®in Machine Learning 14 (1-2), 1â€“210. doi:10.1561/2200000083.
Kamzolov, D., Dvurechensky, P., Gasnikov, A.V., 2020. Universal intermediate gradient method for convex problems with inexact oracle. Optimization Methods and Software 0 (0), 1â€“28. doi:10.1080/10556788.2019.1711079.
Kerdreux, T., dâ€™Aspremont, A., Pokutta, S., 2019. Restarting frank-wolfe. In: Chaudhuri, K., Sugiyama, M. (Eds.), Proceedings of Machine Learning Research, Vol. 89 of Proceed- ings of Machine Learning Research, PMLR, pp. 1275â€“1283.
Kroshnin, A., Tupitsa, N., Dvinskikh, D., Dvurechensky, P., Gasnikov, A., Uribe, C., 2019. On the complexity of approximating Wasserstein barycenters. In: Chaudhuri, K., Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference on Ma- chine Learning, Vol. 97 of Proceedings of Machine Learning Research, PMLR, Long Beach, California, USA, pp. 3530â€“3540.
KuczyÅ„ski, J., WoÅºniakowski, H., 1992. Estimating the largest eigenvalue by the power and lanczos algorithms with a random start. SIAM Journal on Matrix Analysis and Applications 13 (4), 1094â€“1122.
Kushner, H.J., 1984. Approximation and Weak Convergence Methods for Random Pro- cesses. The MIT Press.
Lacoste-Julien, S., Jaggi, M., 2015. On the global linear convergence of frank-wolfe opti- mization variants. Advances in neural information processing systems 28, 496â€“504.
Lan, G.. The complexity of large-scale convex programming under a linear optimization oracle.
Lan, G., 2012. An optimal method for stochastic composite optimization. Mathematical Programming 133 (1), 365â€“397.
Lan, G., 2020. First-order and Stochastic Optimization Methods for Machine Learning.
Springer.
Lan, G., Zhou, Y., 2017. An optimal randomized incremental gradient method. Mathemat- ical Programming doi:10.1007/s10107-017-1173-0.
Lan, G., Zhou, Y., 2016. Conditional gradient sliding for convex optimization. SIAM Jour- nal on Optimization 26 (2), 1379â€“1409.
Lee, Y.T., Sidford, A., 2013. Eï¬ƒcient accelerated coordinate descent methods and faster algorithms for solving linear systems. In: Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS â€™13. IEEE Computer Society, Washington, DC, USA, pp. 147â€“156.
Levitin, E.S., Polyak, B.T., 1966. Constrained minimization methods. USSR Computational mathematics and mathematical physics 6 (5), 1â€“50.
Levy, K.Y., Yurtsever, A., Cevher, V., 2018. Online adaptive methods, universality and ac- celeration. In: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems, Vol. 31. Cur- ran Associates, Inc., pp. 6500â€“6509.
Lin, F., Fardad, M., JovanoviÄ‡, M.R., 2012. Sparse feedback synthesis via the alternating direction method of multipliers. In: 2012 American Control Conference (ACC). IEEE,
pp. 4765â€“4770.
Lin, H., Mairal, J., Harchaoui, Z., 2015. A universal catalyst for first-order optimization. In: Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPSâ€™15. MIT Press, Cambridge, MA, USA, pp. 3384â€“3392.
Lin, Q., Lu, Z., Xiao, L., 2014. An accelerated proximal coordinate gradient method. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q. (Eds.), Advances in Neural Information Processing Systems, Vol. 27. Curran Associates, Inc.,
pp. 3059â€“3067.
Lin, T., Ho, N., Chen, X., Cuturi, M., Jordan, M., 2020. Fixed-support wasserstein barycen- ters: Computational hardness and fast algorithm. In: Larochelle, H., Ranzato, M., Had- sell, R., Balcan, M.F., Lin, H. (Eds.), Advances in Neural Information Processing Sys- tems, Vol. 33. Curran Associates, Inc., pp. 5368â€“5380.
Lin, T., Ho, N., Cuturi, M., Jordan, M.I.. On the complexity of approximating multimarginal optimal transport.
Lin, T., Ho, N., Jordan, M., 2019. On eï¬ƒcient optimal transport: An analysis of greedy and accelerated mirror descent algorithms. In: Chaudhuri, K., Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings of Machine Learning Research, PMLR, Long Beach, California, USA,
pp. 3982â€“3991.
Ljung, L., Pflug, G., Walk, H., 2012. Stochastic approximation and optimization of random systems, 17. BirkhÃ¤user.
Lu, H., Freund, R., Nesterov, Y., 2018. Relatively smooth convex optimization by first-order methods, and applications. SIAM Journal on Optimization 28 (1), 333â€“354. Malitsky, Y., Pock, T., 2018. A first-order primal-dual algorithm with linesearch. SIAM
Journal on Optimization 28 (1), 411â€“432.
Martinet, B., 1970. RÃ©gularisation dâ€™inÃ©quations variationnelles par approximations suc- cessives. Revue franÃ§aise dâ€™informatique et de recherche opÃ©rationnelle. SÃ©rie rouge 4 (R3), 154â€“158.
Mertikopoulos, P., Staudigl, M., 2018. On the convergence of gradient-like flows with noisy gradient input. SIAM Journal on Optimization 28 (1), 163â€“197.
Mertikopoulos, P., Staudigl, M., 2018. Stochastic mirror descent dynamics and their con- vergence in monotone variational inequalities. Journal of Optimization Theory and Applications 179 (3), 838â€“867.
Monteiro, R., Svaiter, B., 2013. An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods. SIAM Journal on Optimization 23 (2), 1092â€“1125.
Moreau, J.J., 1965. ProximitÃ© et dualitÃ© dans un espace hilbertien. Bulletin de la SociÃ©tÃ© mathÃ©matique de France 93, 273â€“299.
Necoara, I., Nesterov, Y., Glineur, F., 2019. Linear convergence of first order methods for non-strongly convex optimization. Mathematical Programming 175 (1), 69â€“107.
Nemirovski, A., 1982. Orth-method for smooth convex optimization. Izvestia AN SSSR, Transl.: Eng. Cybern. Soviet J. Comput. Syst. Sci 2, 937â€“947.



Nemirovski, A., 2004. Prox-method with rate of convergence ğ‘œ(1âˆ•ğ‘¡) for variational in-
equalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization 15 (1), 229â€“251.
Nemirovski, A.S., Yudin, D.B., 1983. Problem Complexity and Method Eï¬ƒciency in Opti- mization. Wiley, New York, NY.
Nemirovskii, A., Nesterov, Y., 1985. Optimal methods of smooth convex minimization.
USSR Computational Mathematics and Mathematical Physics 25 (2), 21â€“30.
Nesterov, Y., 2019. Implementable tensor methods in unconstrained convex optimization.
Mathematical Programming doi:10.1007/s10107-019-01449-1.
rate  ğ‘œ(1âˆ•ğ‘˜2 ).  Soviet  Mathematics  Doklady  27  (2),  372â€“376. Nesterov, Y., 1983. A method of solving a convex programming problem with convergence
Nesterov, Y., 2005. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization 16 (1), 235â€“249.
Nesterov, Y., 2005. Smooth minimization of non-smooth functions. Mathematical Pro- gramming 103 (1), 127â€“152.
Nesterov, Y., 2007. Dual extrapolation and its applications to solving variational inequal- ities and related problems. Mathematical Programming 109 (2), 319â€“344.
Nesterov, Y., 2008. Accelerating the cubic regularization of newtonâ€™s method on convex problems. Mathematical Programming 112 (1), 159â€“181.
Nesterov, Y., 2009. Primal-dual subgradient methods for convex problems. Mathematical Programming 120 (1), 221â€“259.
Nesterov, Y., 2012. Eï¬ƒciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization 22 (2), 341â€“362.
Nesterov, Y., 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140 (1), 125â€“161.
Nesterov, Y., 2015. Universal gradient methods for convex optimization problems. Math- ematical Programming 152 (1), 381â€“404.
Nesterov, Y., 2018. Complexity bounds for primal-dual methods minimizing the model of objective function. Mathematical Programming 171 (1-2), 311â€“330.
Nesterov, Y., 2018. Lectures on convex optimization. Vol. 137 of Springer Optimization and Its Applications. Springer International Publishing.
Nesterov, Y., Gasnikov, A., Guminov, S., Dvurechensky, P., 2020. Primal-dual accelerated gradient methods with small-dimensional relaxation oracle. Optimization Methods and Software 1â€“28. doi:10.1080/10556788.2020.1731747.
Nesterov, Y., Nemirovski, A., 1994. Interior Point Polynomial methods in Convex pro- gramming. SIAM Publications.
Nesterov, Y., Spokoiny, V., 2017. Random gradient-free minimization of convex functions.
Found. Comput. Math. 17 (2), 527â€“566.
Nesterov, Y., Stich, S.U., 2017. Eï¬ƒciency of the accelerated coordinate descent  method  on  structured  optimization  problems.  SIAM  Jour- nal on Optimization 27 (1), 110â€“123. First presented in May 2015 http://www.mathnet.ru:8080/PresentFiles/11909/7_nesterov.pdf
Odor, G., Li, Y.-H., Yurtsever, A., Hsieh, Y.-P., Tran-Dinh, Q., Halabi, M.E., Cevher, V., 2016. Frank-wolfe works for non-lipschitz continuous gradient objectives: scalable poisson phase retrieval. In: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Ieee, pp. 6230â€“6234.
Opial, Z., 1967. Weak convergence of the sequence of successive approximations for non- expansive mappings. Bulletin of the American Mathematical Society 73 (4), 591â€“597. Parikh, N., Boyd, S., 2014. Proximal algorithms. Foundations and TrendsÂ® in Optimization
1 (3), 127â€“239.
Pedregosa, F., Negiar, G., Askari, A., Jaggi, M., 2020. Linearly convergent frank-wolfe with backtracking line-search. In: International Conference on Artificial Intelligence and Statistics, PMLR, pp. 1â€“10.
Pokutta, S., 2020. Restarting algorithms: Sometimes there is free lunch. In: Hebrard, E., Musliu, N. (Eds.), Integration of Constraint Programming, Artificial Intelligence, and Operations Research. Springer International Publishing, Cham, pp. 22â€“38.
Polyak, B.T., 1964. Some methods of speeding up the convergence of iteration methods.
USSR Computational Mathematics and Mathematical Physics 4 (5), 1â€“17.
Polyak, B.T., 1987. Introduction to Optimization. Optimization Software.
Robbins, H., Monro, S., 1951. A stochastic approximation method. The Annals of Mathe- matical Statistics 22 (3), 400â€“407.
Robinson, S.M., 1982. Generalized equations and their solutions, part ii: applications to nonlinear programming. In: Optimality and Stability in Mathematical Programming. Springer, pp. 200â€“221.
Rockafellar, R.T., 1970. Convex Analysis. Princeton University Press, Princeton.
Rockafellar, R.T., 1976. Augmented lagrangians and applications of the proximal point algorithm in convex programming. Mathematics of operations research 1 (2), 97â€“116. Rockafellar, R.T., 1976. Monotone operators and the proximal point algorithm. SIAM jour-
nal on control and optimization 14 (5), 877â€“898.
Rockafellar, R.T., Wets, R.J.B., 1998. Variational analysis. Vol. 317 of A Series of Com- prehensive Studies in Mathematics. Springer-Verlag, Berlin.
Rogozin, A., Bochko, M., Dvurechensky, P., Gasnikov, A., Lukoshkin, V., 2021. An acceler- ated method for decentralized distributed stochastic optimization over time-varying graphs. In: 2021 60th IEEE Conference on Decision and Control (CDC).
Roulet, V., dâ€™Aspremont, A., 2017. Sharpness, restart and acceleration. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems, Vol. 30. Curran Asso- ciates, Inc., pp. 1119â€“1129.
Shalev-Shwartz, S., Zhang, T., 2014. Accelerated proximal stochastic dual coordinate as- cent for regularized loss minimization. In: Xing, E.P., Jebara, T. (Eds.), Proceedings of the 31st International Conference on Machine Learning, Vol. 32 of Proceedings of Machine Learning Research, PMLR, Bejing, China, pp. 64â€“72.
Shapiro, A., Dentcheva, D., RuszczyÅ„ski, A., 2009. Lectures on stochastic programming.
Society for Industrial and Applied Mathematics.
Shefi, R., Teboulle, M., 2014. Rate of convergence analysis of decomposition methods based on the proximal method of multipliers for convex minimization. SIAM Journal on Optimization 24 (1), 269â€“297.
Shi, B., Du, S.S., Su, W., Jordan, M.I., 2019. Acceleration via symplectic discretization of high-resolution differential equations. Advances in Neural Information Processing Systems 5744â€“5752.
Sorin, S., 2000. A First-Course on Zero-Sum Repeated Games. Springer.
Stonyakin, F., Gasnikov, A., Dvurechensky, P., Alkousa, M., Titov, A.. Generalized Mirror Prox for monotone variational inequalities: Universality and inexact oracle.
Stonyakin, F., Tyurin, A., Gasnikov, A., Dvurechensky, P., Agafonov, A., Dvinskikh, D., Alkousa, M., Pasechnyuk, D., Artamonov, S., Piskunova, V., 2020. Inexact model: A framework for optimization and variational inequalities. Optimization Methods and Software doi:10.1080/10556788.2021.1924714.
Stonyakin, F.S., Dvinskikh, D., Dvurechensky, P., Kroshnin, A., Kuznetsova, O., Aga- fonov, A., Gasnikov, A., Tyurin, A., Uribe, C.A., Pasechnyuk, D., Artamonov, S., 2019. Gradient methods for problems with inexact model of the objective. In: Khachay, M., Kochetov, Y., Pardalos, P. (Eds.), Mathematical Optimization Theory and Operations Research. Springer International Publishing, Cham, pp. 97â€“114.
Sturm, J.F., 1999. Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones. Optimization methods and software 11 (1-4), 625â€“653.
Su, W., Boyd, S., Candes, E.J., 2016. A differential equation for modeling nesterovâ€™s accel- erated gradient method: Theory and insights. Journal of Machine Learning Research. Sun, A.X., Phan, D.T., Ghosh, S., 2013. Fully decentralized ac optimal power flow algo-
rithms. In: 2013 IEEE Power & Energy Society General Meeting. IEEE, pp. 1â€“5.
Sun, R.. Optimization for deep learning: theory and algorithms.
Teboulle, M., 1992. Entropic proximal mappings with applications to nonlinear program- ming. Mathematics of Operations Research 17, 670â€“690.
Teboulle, M., 2018. A simplified view of first order methods for optimization. Mathemat- ical Programming 170 (1), 67â€“96.
Todd, M.J., 2016. Minimum-volume ellipsoids. Society for Industrial and Applied Mathe- matics.
Tran-Dinh, Q., Alacaoglu, A., Fercoq, O., Cevher, V., 2020. An adaptive primal-dual frame- work for nonsmooth convex minimization. Mathematical Programming Computation 12 (3), 451â€“491.
Tran-Dinh, Q., Cevher, V., 2014. Constrained convex minimization via model-based exces- sive gap. In: Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPSâ€™14. MIT Press, Cambridge, MA, USA, pp. 721â€“729.
Tran-Dinh, Q., Fercoq, O., Cevher, V., 2018. A smooth primal-dual optimization frame- work for nonsmooth composite convex minimization. SIAM Journal on Optimization 28 (1), 96â€“134.
Tseng, P., 1991. Applications of a splitting algorithm to decomposition in convex program- ming and variational inequalities. SIAM Journal on Control and Optimization 29 (1), 119â€“138.
Tseng, P., 2008. On accelerated proximal gradient methods for convex-concave optimiza- tion. Tech. rep.. MIT.
Tupitsa, N., Dvurechensky, P., Gasnikov, A., Uribe, C.A., 2020. Multimarginal optimal transport by accelerated alternating minimization. In: 2020 59th IEEE Conference on Decision and Control (CDC), pp. 6132â€“6137.
Uribe, C.A., Dvinskikh, D., Dvurechensky, P., Gasnikov, A., NediÄ‡, A., 2018. Distributed computation of Wasserstein barycenters over networks. In: 2018 IEEE Conference on Decision and Control (CDC), pp. 6544â€“6549.
Van Nguyen, Q., 2017. Forward-backward splitting with bregman distances. Vietnam Journal of Mathematics 45 (3), 519â€“539.
Von Hohenbalken, B., 1977. Simplicial decomposition in nonlinear programming algo- rithms. Mathematical Programming 13 (1), 49â€“68.
Vorontsova, E.A., Gasnikov, A.V., Gorbunov, E.A., 2019. Accelerated directional search with non-euclidean prox-structure. Automation and Remote Control 80 (4), 693â€“707. Vorontsova, E.A., Gasnikov, A.V., Gorbunov, E.A., Dvurechenskii, P.E., 2019. Accelerated gradient-free optimization methods with a non-euclidean proximal operator. Automa-
tion and Remote Control 80 (8), 1487â€“1501.
Wibisono, A., Wilson, A.C., Jordan, M.I., 2016. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences 113 (47), E7351.
Wolfe, P., 1970. Convergence theory in nonlinear programming. In: Abadie, J. (Ed.), In- teger and nonlinear programming. North-Holland, Amsterdam.
Wright, S.J., 2018. Optimization algorithms for data analysis. The Mathematics of Data 25, 49.
Yang, J., Zhang, Y., 2011. Alternating direction algorithms for âˆ–ell_1-problems in com-
pressive sensing. SIAM journal on scientific computing 33 (1), 250â€“278.
Yuan, X., 2012. Alternating direction method for covariance selection models. Journal of Scientific Computing 51 (2), 261â€“273.
Zhang, Y., Xiao, L., 2015. Stochastic primal-dual coordinate method for regularized em- pirical risk minimization. In: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd Interna- tional Conference on Machine Learning, Vol. 37 of Proceedings of Machine Learning Research, PMLR, Lille, France, pp. 353â€“361.
Zhao, R., Freund, R.M.. Analysis of the Frank-Wolfe method for logarithmically-homoge- neous barriers, with an extension.
