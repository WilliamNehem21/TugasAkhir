Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 341 (2018) 45–72
www.elsevier.com/locate/entcs

Cartesian Integral Categories and Contextual Integral Categories
J. R. B. Cockett 1,2
Department of Computer Science University of Calgary
Calgary, Alberta, CANADA
J-S. Lemay 3,4
Department of Computer Science University of Oxford
Oxford, UK

Abstract
The notion of a Cartesian integral category is introduced and motivated. Morally, this notion should be the coKleisli category of a (tensor) integral category. However, unfortunately, integral categories, as introduced, do not in general have enough structure for this to be true. The objective of this paper is then to find an appropriate (tensor) integral structure on a coalgebra modality which actually produces as its coKleisli category a Cartesian integral category. The main theorem of this paper is that any contextual integral category with a pseudo-linearizing transformation has its coKlleisli category a Cartesian integral category. The result seems to be quite general as, in particular, it covers all example in which the linear maps arise from a differential.
Keywords: Integral categories, coalgebra modality, context, simple slice, coKleisli category.


Introduction
Calculus is perhaps the most applied area of mathematics. It is, in particular, fun- damental to engineering and physics. Without calculus bridges would fall down and we certainly would not have gotten to the moon. In fact, it is perhaps so ingrained in applied mathematical culture that we rarely think to consider its foundations. Even when we do consider its foundations we often think of the ϵ and δ arguments

1 The first author would like to acknowledge the support of NSERC of Canada.
2 Email: robin@ucalgary.ca
3 The second author would like to thank Kellogg College, the Clarendon Fund, the Oxford-Google Deep- Mind Graduate Scholarship, and his department for financial support.
4 Email: jean-simon.lemay@cs.ox.ac.uk

https://doi.org/10.1016/j.entcs.2018.11.004
1571-0661/© 2018 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

of analysis rather than of the algebraic properties of differentiation and integration. Yet the algebraic perspective has led a significant broadening of our view of calculus and to the realization that analysis is the study of just one (albeit very important) model of calculus and that there are many others – some of which have commensu- rate importance. This has led to a reinvestment in the foundations of calculus so that the commonality between different settings which provide models of calculus can be recognized and exploited.
Ehrhard’s work [8] was seminal in the process of algebraically characterizing differentiation as it linked ‘linearity” in the analytical sense to its meaning in linear logic. This work was then abstracted in [4] to provide a categorical account of differentiation based in monoidal or “tensor” categories enriched in commutative monoids with a coalgebra modality. Just as for linear logic this gave a linear non- linear adjoint between the “tensor” and “Cartesian” semantics: the latter being given by the coKleisli construction on the coalgebra modality. The development of differential categories started with “tensor” differential categories [4] not only because they followed naturally on the heels of Ehrhard’s work [8,9] but also because they are axiomatically relatively simple. However, this left a powerful imperative to develop the “Cartesian” side of the story, as this side underlies the basic theory of multi-variable differential calculus. While the step from the “tensor” to the “Cartesian” side is mediated by the coKleisli construction, as the Cartesian side of the story developed, see [5], its axiomatization became independently driven by expectations from multi-variable differential calculus. This importantly meant that the Cartesian structure could be used to corroborate the axiomatization of the tensor structure ... and, indeed, this had a pay-off as it revealed that an important axiom (the interchange law) had been overlooked. However, it also meant that Cartesian differential categories were not simply coKleisli categories of (tensor) differential categories as they covered a broader range of models.
Of course, in calculus integration plays an equally important role. While it may be characterized as “just” anti-derivation, it is still useful to have an independent algebraic understanding of integration. In developing a categorical formulation of integration [7], it was advantageous develop the “tensor” side first as it is simpler and integral structure tends to be more complex than differential structure. As before, the development of the “Cartesian” side is crucially important as it is this which underlies classical integral calculus. This paper is concerned with the process of moving from the tensor side to Cartesian side for integral categories. While it is clear that this step should be mediated by the coKleisli construction, again we have been forced to recognize that the form of the result is independently driven by our expectations from integral calculus. Frustratingly, we stumbled on a rather subtle – but quite fundamental – mismatch as our expectations of the Cartesian side needed to exhibit features which did not arise from the tensor side without postulating further structure. Faced with this we were forced back to reconsider the axiomatization of “tensor” integral categories provided in [7]. Our solution, which is the main topic of this paper, is to add “contextual” structure to the “tensor” side in order to get the correspondence going again.

It may sound as if the formulation of “tensor” integral categories in [7] was therefore entirely wrong: but this is not the case. The issue is quite subtle because the correspondence did already essentially work 5 for integral categories with prod- ucts and a monoidal coalgebra modality (i.e. new Seely categories [2,14]). However, embarrassingly, some of the key emerging examples were not of that nature (such as smooth functions via free C∞-rings) and, thus, we found ourselves with a gen- eral theory which failed to cover some obviously important examples! The decision to turn back and collect those examples was also encouraged by the possibility of clarifying some of the underlying structural components.
The first objective of the paper is to introduce and motivate the definition of a Cartesian integral category (Section 5). This is a little complicated as the notion of integration must work in “context” and, furthermore, with respect to a notion of “linear” map. This means that not only must we recall how “context” is described categorically using simple slice categories (Section 2), but also how “linear” maps themselves are described (Section 4). Furthermore all this needs to be described in the context of a left additive category (Section 3). How these notions combine is most clearly seen by expressing the axioms using the term logic (which we introduce informally). It is worth noting that the standard notion of integration which is being captured in integral categories is not the measure theoretic integral but rather the line integral. It is this latter notion of integration which behaves well algebraically. The interpretation of our integral as a line integral is described in more detail in Example 5.3 (iii).
Our next objective is to revisit the “tensor” side and to introduce the appro- priate “contextual” structure so that the coKleisli category will actually produce a Cartesian integral category. The first step in this direction is to have a method of producing “linear maps” in context which is achieved using a pseudo-linearizing transformation (Section 6). Next the notion of a contextual integral category (Sec- tion 7) is introduced: this is an embellishment of the basic notion of an integral category which supports a notion of integration in context. The main theorem is then that the coKleisli category of a contextual integral category with a pseudo linearizing transformation is, indeed, a Cartesian integral category (Theorem 7.6): the proof is in Appendix B.

Conventions and the Graphical Calculus
We shall use diagrammatic order for composition: explicitly, this means that the composite map fg is the map which first does f then g. Furthermore, to simplify working in symmetric monoidal categories, we will allow ourselves to work in strict symmetric monoidal categories and so will generally suppress the associator and unitor isomorphisms. For a symmetric monoidal category we will use ⊗ for the tensor product, I for the unit, and c⊗ : A ⊗ B → B ⊗ A for the symmetry isomor- phism. For a category with finite products, we will use × for the product, πi for

5 In fact, the issue of how to define the “linear maps” also needed to be resolved. Although, for “Calculus” categories, [7], which have a compatible differential structure, the linear structure is determined by the differential.

the projections, ⟨−, −⟩ for the pairing, and for the c× : A × B → B × A for the canonical symmetry isomorphism.
We shall use the graphical calculus [12] for symmetric monoidal categories, in particular, the graphical calculus for integral categories as introduced in [7]. Note, however, that our diagrams are to be read down the page – from top to bottom. We will be working with coalgebra modalities: these are based on a comonad (!, δ, ε) where ! is the functor, δ is the comultiplicaton and ε is the counit. As in [4,7], we will use functor boxes when dealing with string diagrams involving the functor !: a map f : A → B will be encased in a circle while !(f ): !A → !B will be encased in a box:
A	!A

f =	!(f )= 

B	!B

Simple Slice Categories
From a categorical perspective, a map in “context” C is interpreted as a map in the simple slice category over C. Thus, simple slices categories play a crucial role in this paper, we start therefore by recalling their definition in some detail. In particular, we also introduce notation which simplifies working with the composition in a simple slice and, generally, makes the notation (particularly for Section 5) less crowded. Simple slice categories can be collected together into a fibration, called the simple fibration [11, Chapter 1], and are the coKleisli categories of comonads (see Section 6). The direct definition is:
Definition 2.1 Let X be a category with finite products ×. For each object C, the simple slice category [11, Definition 1.3.1] over C is the category X[C] whose objects are the objects of X, ob (X[C]) := ob (X), and whose homsets are defined as X[C](A, B) := X(C × A, B). The composition in X[C] of maps f : C × A → B and g : C × B → D is the map f ·C g := ⟨π0,f⟩g : C × A → D, while the identities maps are the projections π1 : C × A → A.
Each simple slice X[C] has finite products with projection maps π1πi : C × (A0 × A1) → Ai, and where the pairing of maps in X[C] is the same as the pairing of maps in X. For maps f : C × A → B and g : C × X → Y , we denote their simple slice product by f ×C g : C × (A× X) → B × Y , where f ×C g = ⟨(π1π0) ·C f, (π1π0) ·C g⟩.
Definition 2.2 For each map h : D → C in X, define the substitution functor
h∗ : X[C] → X[D] on objects as h∗(A) := A and on maps as h∗(f ) := (h × 1)f .
Simple slice categories generalize to symmetric monoidal categories by insisting that the context be a (commutative) comonoid.

Cartesian Left Additive Categories
In order to express the rules of a Cartesian integral category (particularly the Rota- Baxter rule [10]), we will require that the category has (skew [6]) additive structure. Here by “additive” is meant being a commutative monoid as in [4]. We do not assume negatives nor do we assume biproducts – this differs from additive categories in the sense of [13].
Definition 3.1 A left additive category [5, Definition 1.1.1] is a category such that each hom-set is a commutative monoid, with addition + and zero 0, such that composition on the left preserves the additive structure, that is f (g + h)= fg + fh and f 0 = 0. A map h in a left additive category is additive [5] if composition on the right by h preserves the additive structure, that is (f + g)h = fh + gh and 0h = 0.
Definition 3.2 A cartesian left additive category [5, Definition 1.2.1] is a left additive category with finite products such that all projections πi are additive and if f and g are additive then ⟨f, g⟩ is additive.
Here are three examples of cartesian left additive categories we will consider throughout this paper:
Example 3.3
Any category with finite biproducts is a cartesian left additive category where every map is additive. And conversely, in a cartesian left additive category where every map is additive, the finite product is a finite biproduct.
Let R be a commutative rig (also known as a semiring). Define the cate- gory of polynomials with coefficients in R, POLYR, whose objects are nat- ural numbers n ∈ N, and where a map P  : n → m is an m-tuple P := ⟨p1(x1,..., xn),..., pm(x1,..., xn)⟩ where pi(x1,..., xn) ∈ R[x1,..., xn]. POLYR is a cartesian left additive category where composition is given by the standard composition of polynomials, identities are n-tuples of monomials
⟨x1, ..., xn⟩, and the additive structure is given by the sum of polynomials. The product on objects is given by the sum of natural numbers.
Define the category of smooth real functions, SMOOTHR, whose objects are natural numbers n ∈ N, and where a map F : n → m is smooth map F : Rm → Rn. SMOOTHR is a cartesian left additive category where composition is given by the standard composition of smooth functions, identities and projections are also standard, and the additive structure is given by the sum of smooth functions. The product structure on objects is given by the sum of natural numbers. Note that a smooth map F : Rm → Rn is in fact an m-tuple F :=
⟨f1,..., fm⟩ where fi : Rm → R is smooth, and therefore POLYR is a sub- cartesian left additive category of SMOOTHR.
Every simple slice of a cartesian left additive category is again a cartesian left additive with the same sum and zero maps. In a cartesian left additive category X,

a map f : C × A → B is additive in its second argument, A, when it is an additive map in X[C], that is, 0 ·C f =0 and (g + h) ·C f = g ·C f + h ·C g.
Systems of Linear Maps
Systems of linear maps [3] were introduced in order to study how to pass from cartesian differential categories [5] to tensor differential categories [4]. For cartesian integral categories, the linear maps play a crucial role as a map is integrable only when it is linear in the appropriate argument (see Section 5). A system of linear maps is a special sub-fibration of the simple fibration [11, Chapter 1]:
Definition 4.1 A system of linear maps [3, Definition 2.1.1] L[−] for a category X with finite products is an all-object including product preserving subfibration of the simple slice fibration, given by inclusions L[C] ⊆ X[C]. Explicitly is means:
[LS.1] Closed under composition: All identities π1 are in L[C], and the com- posite f ·C g is in L[C] whenever f and g are in L[C];
[LS.2] Closed under product structure: All projections π1πi and terminal maps t are in L[C], and the pairing ⟨f, g⟩ is in L[C] whenever f and g are in L[C];
and such that:
[LS.3] Preserved by cancellation: If g ∈ L[C] is a retraction in X[C] (whose section is not necessarily in L[C]) and g ·C h is in L[C], then h is in L[C];
[LS.4] Preserved by substitution: For every map g : C → D in X, if f is in
L[D], then g∗(f )= (g × 1)f is in L[C].
Maps f : C × A → B ∈ L[C] are referred to being linear in their second argument
A. A map g : C × A × B → D ∈ L[C × A] (so is linear in B) which also has (1 × c×)g : C ×B ×A → D ∈ L[C ×B] (so is also linear in A) is said to be bilinear in A and B.
For cartesian left additive categories, we ask that linear maps be additive:
Definition 4.2 For a cartesian left additive category, a system of additive linear maps is a system of linear maps L[−] such that:
[L.A1] The zero maps 0 are in L[C] and the sum f + g is in L[C] whenever f and
g are in L[C];
[L.A2] If f is in L[C], then f is additive in X[C] (i.e. additive in its second argument).
Here are the systems of additive linear maps for our three main examples:
Example 4.3
For a category with finite biproducts ⊕, define a system of additive linear maps as follows: a map f : C ⊕ A → B is linear in its second argument A if and only if π1ι1f = f (where ι1 is the injection map).

Let R be a commutative rig. We define a system of additive linear maps on POLYR as follows: a polynomial p(x1,..., xm, y1,..., yn), seen as a map p : m × n → 1, is linear in its second argument n if it is linear in each yi, that is, there exists n polynomials qi(x1,..., xm) such that:
n
p(x1,..., xm, y1,..., yn)=	qi(x1,..., xn)yi
i=1
A   map   p	:    m  ×  n	→	k,   P	=
⟨p1(x1,..., xm, y1,..., yn),..., pk(x1,..., xm, y1,..., yn)⟩, is linear in its second argument n if each pi(x1,..., xm, y1,..., yn) is linear in its second argument n.
We define a system of additive linear maps on SMOOTHR as follows: a smooth function f : Rm × Rn → R, seen as a map f : m × n → 1, is linear in its second argument Rn if it is linear in the classical sense for vector spaces, that is, there exists a smooth function h : Rm → Rn such that f (v, w) = h(v) · w, where on the right side of the equality, · is the dot product between vectors. A map G : m × n → k, G = ⟨g1,..., gk⟩ is linear in its second argument n if each gi is linear in its second argument n. Note that the system of linear maps on POLYR from Example 4.3 (ii) arises in this way.
Every cartesian differential category [5, Definition 2.1.1] admits a system of linear maps [3, Proposition 4.1.3] given by the maps which are linear in the cartesian differential category sense [5, Definition 2.2.1]. The above three ex- amples of systems of linear maps are all examples of this.
Cartesian Integral Categories
A cartesian integral category is a cartesian left additive category, X, with a system of additive linear maps, L[ ] and an integral combinator which is for each object C, a map:
SC : L[C × A](A, B) → X[C](A, B)
Written as an inference rule this takes the form
f : C × A × A → B ∈ L[C × A]
SC[f ]: C × A → B ∈ X[C]	.
The integral combinator, S, must satisfy the following properties:
[CS.1] Additivity: SC[f + g]= SC[f ]+ SC[g];
[CS.2] Constant Rule: Sends identities of L[C×A] to identities of X[C]: SC[π1]= 
π1;
[CS.3] Context Substitution: For any map g : D → C, g∗ SC[f ] = SD[g∗ (f )];
[CS.4] Linear Scaling: If f : C × A × A → B ∈ L[C × A] and k : C × B → D ∈ L[C], so both are linear in their last arguments A and B respectively, then: SC[f ·C k]= SC[f ] ·C k;

[CS.5] Linear Substitution: If h : C × A → B ∈ L[C] and g : C × B × B → D ∈ L[C × B], so both are linear in their last arguments A and B respectively, then: SC[(h ×C h) ·C g]= h ·C SC[g]
For the remaining three axioms, for each pair of objects C and B, we define the combinator S˜C,B as follows:
f : C × A × A × B → D	(1C × c×)f ∈ L[C × B × A]
S˜C,B [f ] := (1C × c×) SC×B [(1 × c×)f ]: C × A × B → D ∈ X[C]
[CS.6] Persistent Linearity: If f : C × A× A× D → B is bilinear in its last two arguments A and D, then S˜C,D[f ]: C × A × D → B is linear in D;
[CS.7] Interchange Rule: If f : C × A × A × A → B is bilinear in its last two arguments A, then SC S˜C,A[f ] = SC S˜C,A[(1C × 1A × c×)f ] ;
[CS.8] Rota-Baxter Rule: If f : C × A × A × B × B → D is bilinear in its third argument A and last argument B, then:
SC×A S˜C,B×B [f ] = SC (1C × 1A × 1B × π1) S˜C,B×B [f ] 
+SC (1C × 1A × 1B × π0)(1C × 1A × c×) SC×A×A[f ] 
Note that persistent linearity [CS.6] implies that linearity is preserved every- where in context, thus making the second part of the sum in Rota-Baxter rule [CS.8] well-defined.
To help us better understand integral combinators, inspired by the term logic of cartesian differential categories [5, Section 4], we introduce an informal term logic for cartesian integral categories borrowing notation from calculus. Developing a full- blown formal term logic for cartesian integral categories is feasible and important, but a story for another time.
The key to the informal term logic for cartesian integral categories is interpreting integrable functions f , with integral  f (x)dx, not as functions in one variable, but as a function in two variables x and dx which become bound, and which is linear in dx. In the same spirit of the system of linear maps from Example 4.3 (iii), we express maps f : C × A → B which are linear in their second argument as f (c) · a. Then an integrable map f : C ×A×A → B is a function of three variables f (c, x) · dx, where we emphasize that x and dx are (unrelated) variables. We express the integral as:


a
SC[f ](c, a)= 
x·dx

f (c, x) · dx

The subscripts of the integral indicate the variables over which the term is being integrated: these variables become bounded in the integral term. One can interpret this integral as the integral from 0 to a, as we will see in Example 5.3 (iii). The first integral combinator axiom says that the integral of a sum of maps is equal to the sum of the integral of each map. Context substitution [CS.3] implies that we can freely modify the context without affecting the integral. Persistent linearity [CS.6],

while straightforward to understand, is very important as it allows the possibility for iterated integrals. The remaining axioms can be expressed as follows:
[CS.2] Constant Rule:



[CS.7] Interchange Rule:
∫a


x·dx
dx = a



∫a  ∫
a
f (c, x) · dx · dy =
x
f (c, y) · dx · dy

y·dy x·dx	x·dx y·dy


[CS.4] Linear Scaling:
∫a

⎛ ∫a	⎞



x·dx
k(c) · (f (c, x) · dx)= k(c) · ⎝


x·dx
f (c, x) · dx⎠

[CS.5] Linear Substitution:

a	h(c)·a

∫

x·dx
g(c, h(c) · x) · (h(c) · dx)= 
∫

x·dx
g(c, x) · dx

[CS.8] Rota-Baxter Rule:


∫a  ∫b

f (c, x, y)·dx·dy =
(a,b)
z
f (c, x, y)·dx·dy+
(a,b)
z
f (c, x, y)·dx·dy

x·dx y·dy
(x,z)·(dx,dz) y·dy
(z,y)·(dz,dy) x·dx


Here the Rota-Baxter rule [CS.8] is slightly different than simply re-expressing integration by parts using only integrals [10]. Instead of multiplying two integrals together, we consider an iterated integral. Linear substitution [CS.5] can also be interpreted as the substitution rule for integration but only for substituting linear maps. It may be tempting to think that the interchange rule [CS.6] is related to Fubini’s theorem. However it is not closely related at all [7, Section 3.6]: the interchange rule involves integrating twice over the same variable, while Fubini’s theorem involves integrating twice over different variables while keeping the other constant. In the informal term logic, Fubini’s theorem is expressed as:


∫a  ∫b
b
f (c, x, y) · dx · dy =
a
f (c, x, y) · dx · dy

x·dx y·dy	y·dy x·dx
and in fact, follows directly from the Rota-Baxter rule [CS.8].

Proposition 5.1 In any cartesian integral category, Fubini’s theorem holds, that is, for any map f : C × A × A × B × B → D which is bilinear in its third argument A and last argument B, the following equality holds:
SC×A S˜C,B×B [f ] = S˜C,B SC×(A×A)[f ] 

Proof. Note first that S˜C,B SC×A×A[f ]  = (1 × c×)SC×B S˜C,A×A[(1 × c×)f ] .
Now using the Rota-Baxter rule [CS.8], linear substitution [CS.5], and commuta- tivity of addition, we have that:
S˜C,B SC×A×A[f ] = (1 × c×)SC×B S˜C,A×A[(1 × c×)f ] 
= (1 × c×)SC (1C × 1B × 1A × π0)(1C × 1B × c×) SC×B×B[(1 × c×)f ] 
+ (1 × c×)SC (1C × 1B × 1A × π1) S˜C,A×A[(1 × c×)f ] 
= SC (1C × 1A × 1B × π0)(1C × 1A × c×) SC×(A×A)[f ] 
+ SC (1C × 1A × 1B × π1) S˜C,B×B [f ] 
= SC (1C × 1A × 1B × π1) S˜C,B×B [f ] 
+ SC (1C × 1A × 1B × π0)(1C × 1A × c×) SC×A×A[f ] 
= SC×A S˜C,B×B [f ] 
□
Before giving some examples, we provide some basic identities:
Proposition 5.2 In a cartesian integral category:
SC[0] = 0;
0 ·C SC[f ]= 0;
SC[⟨f, g⟩]= ⟨SC[f ], SC[g]⟩
Proof. Since zero maps are linear, (i) and (ii) follow from linear substitution
[CS.5]. Since π1π0 and π1π1 are linear, using linear scaling [CS.4] we have that:
SC[⟨f, g⟩] ·C π1π0 = SC[⟨f, g⟩ ·C π1π0]= SC[f ]
SC[⟨f, g⟩] ·C π1π1 = SC[⟨f, g⟩ ·C π1π1]= SC[g]
Then by uniqueness of the pairing, we have that SC[⟨f, g⟩]= ⟨SC[f ], SC[g]⟩.   □
Proposition 5.2 (iii) simplifies defining integral combinators:
Example 5.3
Every category with finite biproducts is a cartesian integral category with the system of additive linear maps from Example 4.3 (i) and where the integral combinator is defined as: SC[f ] := (1C ⊕ ι1)f .

Let Q≥0 denote the set of non-negative rationals, and let R be a commutative rig such that Q≥0 ⊆ R. Then POLYR is a cartesian integral category with the system of additive linear maps from Example 4.3 (ii) whose integral combi- nators is given by polynomial integration – though there is a subtlety as it is not quite what one might expect. By the additive rule [CS.1] and Proposi- tion 5.2 (iii), it is sufficient to define the integral combinator on polynomials p : k × n × n → 1 which are of the form:
p(z1,..., zk, x1,..., xn, y1,..., yn)= q(z1,..., zk)xr1 ... xrn yi
1	n
For these polynomials, the integral combinator is defined as:


k	1	r1
ri+1	rn

S [p]=	n
j=1 rj
q(z1,..., zk)x1 ... xi	...x 

At first glance this may seem bizarre. One might expect the integral trans- formation to integrate the monomial only with respect to the variable xi, and thus only multiply by  1  . However, this classical idea of integration fails the
i
Rota-Baxter rule [CS.8] for any n > 1. Note also that we only integrate over
the xi: the zi are considered as context and are fixed. This integral combinator is, in fact, the classical notion of line integration as seen in the next example.
SMOOTHR is a cartesian integral category 6 with the system of additive linear maps from Example 4.3 (iii) whose integral combinator is given by line integration.  As before, by the additive rule [CS.1] and Proposition
5.2 (iii), it is sufficient to define the integral combinator on smooth func- tions f : Rk × Rn × Rn → R which are linear in its last argument. Let h : Rk × Rn → Rn be the smooth map such that f (c, v, w) = h(c, v) · w, and define for each c ∈ Rk the smooth function hc : Rn → Rn defined as hc(v) = h(c, v). Consider the smooth homotopy r : [0, 1] × Rn → Rn be- tween the zero map and the identity function given by scalar multiplication r(t, v) = tv. For each v ∈ Rn, this induces a smooth path rv : [0, 1] → Rn, which is a parametrization of the straight line between 0 and v, which we de- note as the curve C. We then define Sk[f ]: Rk × Rn → R as the line integral of hc over the straight line parametrized by rv:


S[f ](c, v)= 
C

1
hu(rv) · d(rv)= 
0

hu(rv(t)) · rj (t) dt

1
=	h(c, tv) · v dt =
0
1
f (c, tv, v) dt
0

We now clearly see that we are only integrating over v and leave c in context. For polynomials, this integral combinator is precisely the integral defined in Example 5.3 (ii).

6 This example is due to Geoff Cruttwell, Rory Lucyshyn-Wright, and the second author.

Coalgebra Modalities in Context
We now turn our attention towards obtaining cartesian integral categories as coK- leisli categories of contextual integral categories (Theorem 7.6). In this section we discuss simple slices of coKleisli categories and provide a system of linear maps coKleisli categories of coalgebra modalities.
Recall that for a comonad (!, δ, ε) on a category X, the coKleisli category of (!, δ, ε) is the category X! whose objects are the same as X, ob (X!) := ob (X), and whose homsets are X!(A, B) := X(!A, B). Composition of coKleisli maps f : !A → B and g : !B → C is the map f ·! g : !A → C defined as f ·! g := δ !(f ) g, while identity maps are ε : !A → A. Furthermore, if X admits finite products, then so does X! where the projection maps are επi and the pairing of coKleisli maps is the same as in X.
Simple slice categories can equivalently be defined as coKleisli categories. Indeed, if X is a category with finite products, then for each object C, the triple (C×−, Δ××
−, π1) is a comonad on X. One can quickly check that the coKelsili category of (C×−, Δ× ×−, π1) is precisely the simple slice over C, XC×− = X[C]. In particular, this says that f ·C×− g = f ·C g. However, to avoid confusion we will use the notation
introduce in the previous section for simple slice categories as we will now study simple slices of coKleisli categories.
For a (!, δ, ε) comonad on a category with finite products, there is a comonoidal natural transformation σ : !(C×A) → !C×!A (over the product) given by the pairing σ := ⟨!(π0), !(π1)⟩ [3]. This gives a natural transformation λ[C]: !(C ×−) ⇒ C × !− given by λ[C] := σ (ε × 1), and we observe:
Lemma 6.1 For each object C, λ[C] is a distributive law [1] of (!, δ, ε) over (C ×
−, Δ× × 1, π1).
Every distributive law between comonads induces a comonad structure on the source functor of the distributive law [1]. In this case, for each object C, the induced comonad structure on the functor !(C × −) is given by the natural transformations δ[C]: !(C × −) ⇒ !(C × !(C × −)) and ε[C]: 1X ⇒ !(C × −) defined as:
δ[C] := !(C × A) !(Δ××1) /!(C¸× C × A)	δ	/!!(¸C × C × A) !(λ[C]) /!(C¸× !(C × A))
ε[C] := !(C × A) 	ε	/C¸× A 	π1	/A¸

Corollary 6.2 For each object C, the triple (!(C × −), δ[C], ε[C]) is a comonad.
The coKleisli category of the comonad (!(C × −), δ[C], ε[C]) is precisely the simple slice over C of X!, that is, X![C] = X!(C×−). We will refer to X![C] as the coKleisli simple slices when there is no confusion. In particular, the coKleisli simple slice composition is given by f ·!(C×−) g := δ[C] !(1 × f ) g, the identities are ε[C], and the projections are ε[C]πi. The product of maps f : !(C × A) → B and g : !(C×X) → Y is the map f×!(C×−)g : !(C×A×X) → B×Y defined as f×!(C×−)g =

⟨!(1C ×π0), !(1C ×π1)⟩ (f ×g). To define the substitution functors between coKleisli simple slices, first consider the natural transformation ν[C]: !(C × −) ⇒ !(!C × −) defined as follows:
ν[C] := !(C × A) 	δ	/!!(¸C × A)	!(σ)	/!(!¸C × !A)	!(1×ε)	/!(!¸C × A)
For a coKleisli map g : !D → C, the substitution functor g∗ : X![C] → X![D] is defined on objects as g∗(A) := A and on maps as g∗(f ) := ν[C] !(g × 1) f .
Coalgebra modalities are comonads on symmetric monoidal categories with the added property that for each object A, the object !A is naturally a cocommutative comonoid.
Definition 6.3 A coalgebra modality [4] on a symmetric monoidal category is a quintuple (!, δ, ε, Δ, e) consisting of a comonad (!, δ, ε), a natural transformation Δ : !A → !A ⊗ !A, and a natural transformation e : !A → I, such that for each object A, (!A, Δ, e) is a cocommutative comonoid, and δ is a comonoid morphism, that is:

			

Δ	Δ	Δ
Δ	=	Δ	=
Δ	Δ
Δ
=	=





=
Δ
Δ
=


When (!, δ, ε, Δ, e) is a coalgebra modality on a symmetric monoidal category with finite products, as δ is a comonoid morphism, it follows that for each object C both δ[C] and ν[C] are comonoid morphisms. In particular, this implies that:
Lemma 6.4 For each object C, (!(C × −), δ[C], ε[C], Δ, e) is a coalgebra modality on X[C].
To obtain a system of linear maps for the coKleisli category of a coalgebra modality, we require that the natural transformation l◦ : !(C ×A) → !C ⊗A defined as:
l◦ := !(C × A)  Δ /!(C¸× A) ⊗ !(C × A)  1⊗ε /!(C¸× A) ⊗ (C × A)  !(π0)⊗π1 /!C¸⊗ A
have a natural section:
Definition 6.5 For a coalgebra modality (!, δ, ε, Δ, e) on a symmetric monoidal category with finite products, a pseudo-linearizing transformation is a natural transformation l : !C ⊗ A → !(C × A) satisfying:

[l.1] Section of l◦:
ll◦ =1 
[l.2] Constant Rule:
l ε[C]= e ⊗ 1
[l.3] Context Rule:
l ν[C]= (δ ⊗ 1) l

!C	A
!C	A


!C	A

!C  A


!C	A	!C	A



=
=	=



!C	A
!C	A
A	A	!(!C × A)
!(!C × A)


[l.4] Comultiplication Rule:	[l.5] Interchange Rule:

lδ[C]!(1×l◦)(1×l)= (Δ⊗1)(1⊗l)l = lδ[C]l◦l

(l ⊗ 1)l!(1 × c×)= (1 ⊗ c⊗)(l ⊗ 1)l

!C	A
!C	A
!C	A



!C	A	B



!C	A	B


=	=
=



!(C × !(C × A))
!(C × !(C × A))
!(C × !(C × A))
!(C × B × A)
!(C × B × A)


We use the term pseudo-linearizing transformation since in reality l◦l is the “linearizing transformation”:
Proposition 6.6 The coKleisli category of a coalgebra modality on a symmetric monoidal category with ﬁnite products, and equipped with a pseudo-linearizing trans- formation l, has a system of linear maps L![−] where f : !(C × A) → B is linear in its second argument if and only if l◦lf = f.
Proof. The proof can be found in Appendix A.	□
While we will not go into detail, we note that – in the same spirit as Example
4.3 (iv) – every tensor differential category admits a pseudo-linearizing transforma- tion. This strengthens the argument that pseudo-linearizing transformations are the correct notion for obtaining systems of linear maps for coalgebra modalities.
Proposition 6.7 The coalgebra modality of a differential category [4] with ﬁnite biproducts admits a pseudo-linearization transformation deﬁned as follows:
l := !C ⊗ A	!(ι0)⊗ι1	/!(C¸× A) ⊗ (C × A)	d	/!(C¸× A)
where ιj are the injection maps and d is the deriving transformation [4, Deﬁnition 2.5].

Contextual Integral Categories
In this section we introduce contextual integral categories – which are the “tensor” analogue to cartesian integral categories, as shown with Theorem 7.6.
Definition 7.1 An additive category is a left additive category where every map is additive. An additive symmetric monoidal category is a symmetric monoidal category which is also an additive category in which the tensor product is compatible with the additive structure in the sense that k ⊗ (f +g) ⊗ h = k ⊗f ⊗ h+k ⊗g ⊗ h, 0 ⊗ h = 0, and h ⊗ 0= 0.
For the system of linear maps from Proposition 6.6, the additive symmetric monoidal structure gives us:
Corollary 7.2 For a coalgebra modality equipped with a pseudo-linearizing trans- formation on an additive symmetric monoidal category with ﬁnite products, the system of linear maps of Proposition 6.6 is additive.
Definition 7.3 Let (!, δ, ε, Δ, e) be a coalgebra modality on an additive symmetric monoidal category with finite products. A contextual integral transformation on (!, δ, ε, Δ, e) is a natural transformation s[C]: !(C × A) → !(C × A) ⊗ A (natural in both C and A), expressed in the graphical calculus as:



s[C] :=
!(C × A)

!(C × A)	A
and such that the following equalities hold:


[CXS.1] Constant Rule: s[C](e ⊗ 1) =
ε[C]
[CXS.2] Context Substitution:


!(C × A)
!(C × A)
s[C](ν[C] ⊗ 1) = ν[C]s[!C]


=

A	A

!(C × A)


=
!(C × A)

!(!C × A)	A	!(!C × A)	A

[CXS.3] Context Scaling:	[CXS.4] Interchange Rule:
s[C] (Δ⊗1)(!(π0)⊗1) = Δ(!(π0)⊗1)(1⊗s[C]) s[C] (s[C] ⊗ 1) = s[C] (s[C] ⊗ 1) (1 ⊗ c⊗)

!(C × A)
!(C × A) Δ

!(C × A)

!(C × A)

=	=


!C  !(C × A) A
!C	!(C × A)	A

!(C × A)	A	A

!(C × A)	A	A


[CXS.5] Linear Substitution Rule: δ[C] s[C] (!(1×l◦)⊗l◦)= s[C] (Δ⊗1⊗1) (δ[C]⊗ 1 ⊗ 1) (!(1 × l◦) ⊗ !(π0) ⊗ 1)

!(C × A)
!(C × A)


=


! (C × (!C ⊗ A))
!C	A

! (C × (!C ⊗ A))  !C	A


[CXS.6] Rota-Baxter Rule: s[C × A] (˜s[C × B] ⊗ 1) = s[C] (s[C × A] ⊗ π0) (1 ⊗
c⊗)+ s[C] (˜s[C × B] ⊗ π1)


!(C × A × B)
!(C × A × B)
!(C × A × B)


=	+


!(C × A × B) A
B
!(C × A × B) A	B
!(C × A)	A	B


Where the natural transformation ˜s[C × B]: !(C × A × B) → !(C × A × B) ⊗ A is defined as follows:
˜s[C × B] := !(C × A × B) !(1×c×) /!(C¸× B × A) s[C×B] /!(C¸⊗ B × A) ⊗ C

	!(1×c×)⊗1	/!(C¸× A × B) ⊗ A

!(C × A × B)
!(C × A × B)

=
!(C × A × B) A

!(C × A × B) A
A contextual integral category is an additive symmetric monoidal category with finite products and a coalgebra modality which is equipped with a contextual integral transformation.

We observe that contextual integral categories have multiple contextual integral transformations:
Proposition 7.4 In a contextual integral category, for each object D, s[C × D] : 
!(C × D × A) → !(C × D × A) ⊗ A is a contextual integral transformation for the coalgebra modality (!(C × −), δ[C], ε[C], Δ, e).

Proof. Note that if one were to apply the constructions of Section 6 to the comonad (!(C×−), δ, ε), one would obtain precisely the comonad (!(C×D×−), δ[C×D], ε[C× D]). Then the proposition follows immediately from the definition of a contextual integral transformation.	□

Fubini’s theorem also holds for contextual integral categories.

Proposition 7.5 In a contextual integral category, the contextual integral transfor- mation satisﬁes Fubini’s Theorem, that is: s[C ×A] (˜s[C ×B] ⊗ 1) = ˜s[C ×B] (s[C × B] ⊗ 1) (1 ⊗ c⊗)

!(C × A × B)	!(C × A × B)

=


!(C × A × B) A	B	!(C × A × B) A	B

We now providing the main result of this paper:

Theorem 7.6 The coKleisli category of a contextual integral category with contex- tual integral transformation s[C], and with a pseudo-linearizing transformation l, is a cartesian integral category on the system of additive linear maps induced by l

(from Proposition 6.6), where the integral combinator is given by:
!(C × A)


SC[f ] := !(C × A) s[C] /!(C¸× A) ⊗ A	l /!(C¸× A × A)	ƒ	/B¸ SC[f ] :=


B

Proof. The proof can be found in Appendix B.	□

Integral Categories
In this section we review integral categories [7] and compare them to contextual integral categories.
Definition 8.1 For an additive symmetric monoidal category with a coalgebra modality (!, δ, ε, Δ, e), an integral transformation [7, Section 3.3] is a natural transformation s : !A → !A ⊗ A which is represented in the graphical calculus as:

!A

s :=
!A	A


such that s satisfies the following equations:


[s.1]: Integral of Constants: s(e ⊗ 1) =
ε
[s.2]:	Interchange rule:	s(s ⊗ 1) =
s(s ⊗ 1)(1 ⊗ c⊗)


=
=

[s.3]: Rota-Baxter Rule: Δ(s ⊗ s)= s(Δ ⊗ 1)(s ⊗ 1 ⊗ 1) + s(Δ ⊗ 1)(1 ⊗ c⊗)(1 ⊗ 1 ⊗ s)



Δ
—–	—– =
—–	—–
+



An integral category [7] is an additive symmetric monoidal category with a coal- gebra modality which comes equipped with an integral transformation.

Many examples of integral categories can be found in [7, Section 7]. Interesting properties of integral categories include polynomial integration [7, Proposition 7] and that the coKleisli category of the coalgebra modality is skew-enriched [6] over Q≥0-modules [7, Theorem 1, Corollary 9].
We observe that contextual integral categories are themselves integral categories in many ways:
Proposition 8.2 In a contextual integral category, for each object C, s[C]: !(C ×
A) → !(C × A) ⊗ A is an integral transformation for the coalgebra modality (!(C ×
−), δ[C], ε[C], Δ, e).
Proof. As noted above, [CXS.1] and [CXS.4] are precisely [s.1] and [s.2] respec- tively. Therefore it remains only to show the Rota-Baxter rule [s.3] – which follows from context scaling [CXS.3] and the contextual Rota-Baxter rule [CXS.7].  □
A particular case of Proposition 8.2 is when the context is the terminal object
T. We define the natural transformation s[−]: !A → !A ⊗ A as follows:
s[−] := !A  ∼= /!(T¸× A) s[T] /!(T¸× A) ⊗ A	∼=  /!A¸⊗ A
Corollary 8.3 In a contextual integral category, s[−] is an integral transformation for (!, δ, ε, Δ, e).
The converse to Proposition 8.2 is not automatically true however. Indeed, given an integral category with finite products, one’s first attempt at building a contextual integral transformation might be:
!(C × A)

!(C × A)	A
However this is not (in general) a contextual integral transformation, as it will include C in the integration, failing (amongst others) the context scaling rule [CXS.3]. In order to obtain a contextual integral transformation from an inte- gral transformation, one must have the Seely isomorphisms:
Definition 8.4 In a symmetric monoidal category with finite products, a coalgebra modality (!, δ, ε, Δ, e) has Seely isomorphisms [2,3] if the natural transformations χ : !(A × B) → !A ⊗ !B and χT : !(T) → I, defined respectively as:
!(A × B)   Δ	/!(A¸× B) ⊗ !(A × B)   !(π0)⊗!(π1)	/!A¸⊗ !B	!(T) 	e	/I ¸
are isomorphisms, so !(A × B) ∼= !A ⊗ !B and !(T) ∼= K. A monoidal stor- age category [3] (also known as a new Seely category [2,14]) is a symmetric monoidal category with finite products and a coalgebra modality which has Seely isomorphisms.

The underlying comonad of a coalgebra modality with Seely isomorphisms is in fact a symmetric monoidal comonad [3, Theorem 3.1.6], which in particular implies that there is a monoidal natural transformation m⊗ : !A⊗ !B → !(A⊗ B) satisfying the expected coherences [2,3]. We represent m⊗ graphically as:


!A	!B
m⊗ =
!(A ⊗ B)


It should be noted that the converse is also true: coalgebra modalities whose under- lying comonad is symmetric monoidal, known as monoidal coalgebra modalities [2,3], have Seely isomorphisms.
For a coalgebra modality, define the natural transformation d◦ : !A → !A ⊗ A – called the coderiving transformation [7, Section 2.2] – as follows:


!A

d◦ := !A 	Δ	/!A¸⊗ !A 	1⊗ε	/!A¸⊗ A	d◦ :=
!A	A

For a list of identities d◦ satisfies see [7, Proposition 2.1]. Note that l◦ = d◦(!(π0) ⊗
π1).

Definition 8.5 An integral storage category is an integral category whose coal- gebra modality has Seely isomorphisms and whose integral transformation satisfies the monoidal rule [7, Definition 9]:
[s.m] Monoidal Rule: m⊗s = (s ⊗ d◦)(1 ⊗ c⊗ ⊗ 1)(m⊗ ⊗ 1 ⊗ 1) = (d◦ ⊗ s)(1 ⊗ c⊗ ⊗
1)(m⊗ ⊗ 1 ⊗ 1)


!A	!B
!A	!B
!A	!B



=	=


!(A ⊗ B) A	B
!(A ⊗ B)	A	B
!(A ⊗ B)	A	B


Proposition 8.6 Every integral storage category is a contextual integral category where the contextual integral transformation is deﬁned as:

s[C] := !(C × A)  χ /!C¸⊗ !A  1⊗s /!C¸⊗ !A ⊗ Aχ−1⊗1/!(C¸× A) ⊗ A


!(C × A)



!(C × A)	A
!(C × A)



:=

!(C × A)	A


and furthermore s[−]= s.

References
Beck, J., Distributive laws, in: Seminar on triples and categorical homology theory, Springer, 1969, pp. 119–140.
Bierman, G., What is a categorical model of intuitionistic linear logic?, Typed Lambda Calculi and Applications (1995), pp. 78–93.
Blute, R., J. R. B. Cockett and R. A. G. Seely, Cartesian differential storage categories, Theory and Applications of Categories 30 (2015), pp. 620–686.
Blute, R. F., J. R. B. Cockett and R. A. G. Seely, Differential categories, Mathematical structures in computer science 16 (2006), pp. 1049–1083.
Blute, R. F., J. R. B. Cockett and R. A. G. Seely, Cartesian differential categories, Theory and Applications of Categories 22 (2009), pp. 622–672.
Campbell, A., Skew-enriched categories, Applied Categorical Structures 26 (2018), pp. 597–615.
URL  https://doi.org/10.1007/s10485-017-9504-0

Cockett, J. R. B. and J. S. Lemay, Integral categories and calculus categories, Mathematical Structures in Computer Science (2018), pp. 1–66.
Ehrhard, T., On K¨oethe sequence spaces and linear logic., Mathematical Structures in Computer Science 12 (2001), pp. 579–623.
Ehrhard, T., An introduction to differential linear logic: proof-nets, models and antiderivatives, Mathematical Structures in Computer Science (2017), pp. 1–66.
Guo, L., “An introduction to Rota-Baxter algebra,” International Press Somerville, 2012.
Jacobs, B., “Categorical logic and type theory,” Elsevier, 1999.
Joyal, A. and R. Street, The geometry of tensor calculus, I, Advances in Mathematics 88 (1991),
pp. 55–112.
Mac Lane, S., “Categories for the working mathematician,” Springer-Verlag, New York, Berlin, Heidelberg, 1971, revised 2013.
Melli`es, P.-A., Categorical models of linear logic revisited (2003), working paper or preprint. URL https://hal.archives-ouvertes.fr/hal-00154229

Proof of Proposition 6.6
Before proving Proposition 6.6, we note the following identities which l◦ satisfies (and leave to the reader as an exercise to check):
Lemma A.1 The natural transformation l◦ satisfies the following identities:
[l◦.1] l◦(e ⊗ 1) = ε[C];
[l◦.2] ν[C] l◦ = l◦ (δ ⊗ 1);

[l◦.3] l◦ (Δ ⊗ 1) = δ[C] l◦ (1 ⊗ l◦);
[l◦.4] (1 × c×) l◦ (l◦ ⊗ 1) = l◦ (l◦ ⊗ 1) (1 ⊗ c⊗).

We now prove Proposition 6.6 by showing [LS.1] to [LS.4] (see Section 6 for a reminder of the notation).
[LS.1]: That the identities ε[C] are linear in their second argument follows from [l.1] and [l◦.1]:

l◦lε[C]= l◦(e ⊗ 1) = ε[C]

Regarding composition, first note that for a map ƒ which is linear in its second argument, l◦lƒ = ƒ , and an arbitrary map g, using [l.4], [l◦.3], and the naturality of l and l◦, we have that:

l◦l (ƒ ·!(C×−) g)= l◦l δ[C] !(1 × ƒ ) g
= l◦l δ[C] !(1 × l◦) !(1 × l) !(1 × ƒ ) g
= l◦ (Δ ⊗ 1) (1 ⊗ l) l !(1 × ƒ ) g
= δ[C] l◦ (1 ⊗ l◦) (1 ⊗ l) l !(1 × ƒ ) g
= δ[C] l◦ (1 ⊗ l◦) (1 ⊗ l) (1 ⊗ ƒ ) l g 
= δ[C] l◦ (1 ⊗ ƒ ) l g 
= δ[C] !(1 × ƒ ) l◦l g 
= ƒ ·!(C×−) (l◦l g) Then if g is linear in its second argument, we obtain that:
l◦l (ƒ ·!(C×−) g)= ƒ ·!(C×−) (l◦l g)= ƒ ·!(C×−) g

[LS.2]: The projections are ε[C]πi, and are linear in their second argument by the same argument as the identities in [LS.1]. It is straightforward to see that the terminal maps are as well, and that pairing preserves linearity.
[LS.3]: Let g be linear in its second argument with a coKleisli simple slice retraction ƒ , that is, ε[C]= ƒ ·!(C×−) g. Furthermore, suppose that g ·!(C×−) h is linear in its second argument as well. Then by the identity from [LS.1] above, we have that: l◦l (ƒ ·!(C×−) h) = g ·!(C×−) (l◦l h). It then follows that:

h = ε[C] ·!(C×−) h
= ƒ ·!(C×−) g ·!(C×−) h
= ƒ ·!(C×−) g ·!(C×−) (l◦l h)
= ε[C] ·!(C×−) (l◦l h)
= l◦l h 

[LS.4]: Let ƒ be linear in its second argument and g : !C → D an arbitrary coKleisli map, then using [l.3]
and [l◦.2] we have that:

l◦l g∗(ƒ )= l◦l ν[C] !(g × 1) ƒ = l◦ (δ ⊗ 1) l !(g × 1) ƒ = ν[C] l◦l !(g × 1) ƒ
= ν[C] !(g × 1) l◦l ƒ = ν[C] !(g × 1) ƒ = g∗(ƒ )

Proof of Theorem 7.6
We must check the integral combinator axioms [CS.1] to [CS.8] (see Section 6 for a reminder of the notation).
[CS.1] Additivity: This follows from the additive enrichment of the base category:

SC [ƒ + g]= s[C] l (ƒ + g)= s[C] l ƒ + s[C] l g = SC [ƒ ]+ SC [g]

[CS.2] Constant Rule: Sending identities to identities translates to showing SC [ε[C × A]] = ε[C], which follows from the constant rule [CXS.1] and the constant rule [l.2]:




!(C × A)


C


!(C × A)

!(C × A)



SC [ε[C × A]] =	=
=
= ε[C]



A	A

A



[CS.3] Context Substitution: First note that by definition the diagram on the left commutes, and implies that, along with the context rule [l.3], the diagram on the right also commutes:


!(C × A × A) 	ν[C×A]	/!(!(¸C × A) × A)	!(C × A) ⊗ A 	Æ	/!(C¸× A × A)


ν[C]

!(σ×1)
¸,		 

⇒ ν[C]⊗1

	/¸

ν[C]
  

!(!C × A × A)	!(!C × !A × A)
!(1×ε×1)
!(!C × A) ⊗ A
!(!C × A × A)
Æ




Then if g : !C → D is an arbitrary coKleisli map, by the above right diagram, naturality of l and s[C], and the context substitution rule [CXS.2], we have that:


!(C × A)
!(C × A)
!(C × A)
!(C × A)





SD[g∗

(ƒ )] =

=	=
1

B	B	B

=




B

= g∗ SC

[ƒ ] 




[CS.4] Linear Scaling: Note that by the comultiplication rule [l.4] we have the following equality:



!(C × A)	A	!(C × A)	A




=





!(C × !(C × A))	!(C × !(C × A))


Then by this equality, naturality of l◦ and l, linearity of k and ƒ , and context scaling [CXS.3] we have that:




!(C × A)
!(C × A)
!(C × A)

!(C × A)





SC [ƒ ] ·
!(C×−) k =	=
=	=









B	B	B	B

!(C × A)	!(C × A)
!(C × A)




=	=	=
= SC





B
[ƒ ·!(C×−) k]

B	B






[CS.5] Linear Substitution: Note that by the comultiplication rule [l.4] we have the following equality:







!(C × A)	A

!(C × A)	A




=


!(C × !(C × A) × !(C × A))
!(C × !(C × A) × !(C × A))


Then by this equality, naturality of s[C], linearity of h, and the linear substitution rule [CXS.5] we have that:


!(C × A)

!(C × A)


!(C × A)

!(C × A)





h ·!(C×−)
SC [g]=	=

B
=	=





B	B	B

!(C × A)



!(C × A)



=	=










B
= SC [(h ×!(C×−) h) ·!(C×−) g]




B

For the remaining three axioms, it is straightforward to see that S˜C,B is given by:

!(C × A × B)



S˜C,B [ƒ ]= 





B

[CS.6] Persistent Linearity: To show that S˜C,D [ƒ ] is linear in D (if ƒ is as well), it is sufficient to give a map h : !(C × A) ⊗ D → B such that S˜C,D [ƒ ]= l◦h. Note that by the context scaling rule [CXS.3], we have:
!(C × A × B × D)	!(C × A × B × D)


S˜C,D [ƒ ]=	=



!(C × A × B)	D	A	!(C × A × B)	D	A


And that by the interchange rule [l.5] and [l◦.4], we have that:


!(C × B)	A
!(C × B)	A




=


!(C × A)	B
!(C × A)	B




Then using these two identities, we obtain that:


!(C × A × D)
!(C × A × D)
!(C × A × D)


!(C × A × D)



S˜C,D [ƒ ]= 
=	=	=






B

B	B	B



Which implies that S˜C,D [ƒ ] is indeed linear in D.
[CS.7] Interchange Rule: Translating carefully, the interchange rule is given by:


SC S˜C,A[ƒ ] = SC S˜C,A[!(1 × 1 × c×)ƒ ] 



To prove this equality, we use the same calculations as in [CS.6], [l.1], the interchange rule [CSX.4]:




!(C × A)

!(C × A)

!(C × A)

!(C × A)

!(C × A)




SC S˜C,A[ƒ ] =	=	=
=	=








B	B	B
B

B


!(C × A)





=	= SC S˜C,A[(1C ×! 1A ×! c× ) ·! ƒ ] 


B



[CS.8] Rota-Baxter Rule: Carefully translating, the Rota-Baxter rule is given by:


SC×A S˜C,B×B [ƒ ] = SC !(1C × 1A × 1B × π1) S˜C,B×B [ƒ ] 

+ SC !(1C × 1A × 1B × π0)!(1C × 1A × c×) SC×A×A[ƒ ] 



The prove this equality, we use the same calculations as in [CS.6], [l.1], the naturality of l, l◦, and s[C], and the Rota-Baxter rule [CSX.7]:


!(C × A × B)
!(C × A)
!(C × A)
!(C × A)





SC×A S˜C,B×B [ƒ ] =	=	=	+









D	D	D	D


!(C × A)

!(C × A)


!(C × A)

!(C × A)






=	+	=	+


D	D

D	D


!(C × A × B)	!(C × A × B)





=	+





D

= SC !(1C × 1A × 1B × π1) S˜C,B×B [ƒ ] 






B

+ SC !(1C × 1A × 1B × π0)!(1C × 1A × c×) SC×A×A[ƒ ] .
