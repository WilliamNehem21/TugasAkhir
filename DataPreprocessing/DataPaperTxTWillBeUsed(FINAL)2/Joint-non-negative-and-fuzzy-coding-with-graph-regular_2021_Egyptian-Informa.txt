Egyptian Informatics Journal 22 (2021) 91–100








Full length article
Joint non-negative and fuzzy coding with graph regularization for efficient data clustering
Yong Peng a,b,⇑, Yikai Zhang a, Feiwei Qin a, Wanzeng Kong a,c
a School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China
b Provincial Key Laboratory for Computer Information Processing Technology, Soochow University, Suzhou 215123, China
c Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou 310018, China



a r t i c l e  i n f o 

Article history:
Received 11 February 2020
Revised 4 May 2020
Accepted 10 May 2020
Available online 30 May 2020

Keywords:
Non-negative matrix factorization Fuzzy coding
Local coordinate coding Graph regularization Clustering
a b s t r a c t 

Non-negative matrix factorization (NMF) is an effective model in converting data into non-negative coef- ficient representation whose discriminative ability is usually enhanced to be used for diverse pattern recognition tasks. In NMF-based clustering, we often need to perform K-means on the learned coefficient as postprocessing step to get the final cluster assignments. This breaks the connection between the fea- ture learning and recognition stages. In this paper, we propose to learn the non-negative coefficient matrix based on which we jointly perform fuzzy clustering, by viewing that each column of the dictionary matrix as a concept of each cluster. As a result, we formulate a new fuzzy clustering model, termed Joint Non-negative and Fuzzy Coding with Graph regularization (G-JNFC), and design an effective optimization method to solve it under the alternating direction optimization framework. Besides the convergence and computational complexity analysis on G-JNFC, we conduct extensive experiments on both synthetic and representative benchmark data sets. The results show that the proposed G-JNFC model is effective in data clustering.
© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intel-
ligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

In the community of machine learning, the basic pipeline is first transforming data into its feature representation based on which we can perform pattern recognition tasks. As an effective learning model, the NMF [1,2] can transform the original data into the cor- responding coefficient representation which can be viewed as the feature representation of data with usually enhanced discrimina- tive ability. Then diverse pattern recognition tasks can be con- ducted on the learned coefficient matrix such as clustering [3,4], semi-supervised learning [5,6], classification [7,8] and some speci- fic applications [9,10]. Generally, most of the improved NMF vari- ants achieved acceptable performance in diverse applications.
In the field of data clustering, after obtaining the coefficient rep- resentation matrix in NMF, we often need to perform K-means to get the final cluster assignment of each sample. This suggests that the coefficient representation matrix itself cannot act as the cluster

* Corresponding author at: 1158 No.2 Avenue, Xiasha Higher Education Zone, Jianggan District, Hangzhou 310018, China.
E-mail address: yongpeng@hdu.edu.cn (Y. Peng).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
indicator matrix to directly provide us the clustering results. Therefore, such two-stage paradigm, ‘feature extraction plus recog- nition’, becomes the usual way to NMF-based recognition tasks and this paradigm is shared by many other models such as sparse sub- space clustering [11–13], and low rank representation based sub- space clustering [14–16]. However, there exist at least two limitations in this paradigm. One is that it inevitably breaks the connection between these two steps and causes extra computa- tional burden in the postprocessing step. The other is that a promising clustering method should not only determine the mem- bership of each data point to a specified cluster, but also provide us more detailed information towards the samples in the boundary areas of different clusters. Therefore, fuzzy clustering [17,18] which can depict the confidence of each data point to different clusters sometimes more coincides with our intuitive understand- ing to the knowledge of data when compared with hard clustering. To overcome the above mentioned two limitations in conven- tional matrix factorization based clustering, we propose an improved model to make the learned coefficient matrix simultane- ously achieve the final clustering results and provide the member- ship degree of each data point to different clusters. The newly formulated model is termed as Joint Non-negative and Fuzzy Cod- ing with Graph regularization (G-JNFC). In G-JNFC, apart from the


https://doi.org/10.1016/j.eij.2020.05.001
1110-8665/© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



graph regularization and local coordinate coding [19], we addition- ally constrain the summation of row elements in the coefficient representation matrix to be one. This allows the proposed G-JNFC model to be illustrated from two different perspectives, improved local coordinate coding and joint representation learning and fuzzy clustering. We propose an efficient algorithm to optimize the objec- tive function of G-JNFC. Once G-JNFC is solved, we can categorize each data point to a specified cluster by checking the largest value of each row of the coefficient representation matrix. Moreover, if there are multiple non-zero values in each row, this means the cor- responding data point has fuzzy membership to different clusters. The computational complexity and convergence of the optimiza- tion method are given. Extensive experiments on both synthetic and benchmark data sets show the effectiveness of G-JNFC.
The remainder of this paper is organized as follows. We give a brief introduction to the related works, NMF and local coordinate coding, in Section 2. The proposed G-JNFC model including the model formulation, optimization, convergence and complexity analysis are given in Section 3. In Section 4, we conduct extensive experiments to evaluate the effectiveness of G-JNFC. Section 5 con- cludes the whole paper.

Related works
Obviously, we can find that JNFC objective additionally enforces the summation of each row of the coefficient matrix to one. Though it is a minor modification, we declare that 1) JNFC can directly achieve the clustering results by checking the largest value of each row of the coefficient matrix without performing post-processing; and 2) JNFC can provide us the membership degree of each data point to different clusters, leading to a fuzzy clustering model.
There are two different perspectives to depict the motivation of the second term in (3) which are respectively explained below.
Improved local coordinate coding [21]. Mathematically, we can view NMF as linearly representing each data point by columns
of the dictionary matrix. From this point of view, it is obvious that the corresponding representation coefficient vik should be larger if the data point xi is closer to a specific column uk. If
we use the squared Euclidean distance to measure the close- ness, one reasonable objective to reach this fact is
min	vik  xi — uk  2.	(4)
U,V i=1 k=1
Joint representation learning and fuzzy clustering. To group data into each cluster with a certain membership degree,
the objective of fuzzy K-means clustering can be defined as

n  c

The NMF was proposed to learn parts-based representation by mathematically approximating the data matrix by two non-
min
Y1=1,YP0,lj
XXyij  xi — l 2,	(5)
i=1 j=1

negative factor matrices as
min X — UVT 2, s.t.U P 0, V P 0,	(1)
U,V

where X ∈ Rd×n is the data matrix, U ∈ Rd×c is the basis matrix in points in one cluster, V ∈ Rn×c is the coefficient matrix. Here, which each column is expected to capture the essence of data d, n, c are the dimensionality, number of data points and number
of classes (clusters), respectively. This objective in (1) can be opti-
mized by alternately updating the two variables. Then, V is usually employed as feature in pattern recognition tasks.
If we view the basis matrix U as a set of anchor points and enforce each data point to be represented by a linear combination of only a few nearby anchor points, the non-negative local coordi- nate factorization (NLCF) [20] was formulated by incorporating the following constraint [19] into the NMF objective as a regularization term
where c is the number of clusters and Y ∈ Rn×c is the fuzzy mem- clustering centroid matrix is C , [l1, l2, ··· , lc ]∈ Rd×c. Each bership. Moreover, lj is the centroid of the j-th cluster and the element yij of matrix Y represents the membership degree of
the i-th sample belonging to the j-th cluster [22]. By following the convention that the number of columns of the basis matrix is set as the number of clusters, each column of the basis matrix may be called a ‘concept’ to characterize the essential property of a cluster. In other words, each column of the basis matrix should be approximately equivalent to the centroid of a certain cluster. Based on this analysis, we can view the second term in (3) as a modified fuzzy K-means objective. Then, the proposed JNFC model can be seen as the combination of NMF and fuzzy K-means.

In addition, we also consider the local consistency of the learned representation coefficient matrix by incorporating the

n  c	graph regularizer [23] into the JNFC model and then formulate
minXXvik  xi — uk  2.	(2)	the Joint Non-negative and Fuzzy Coding with Graph regulariza-

U,V
2
i=1 k=1
tion (G-JNFC) model as

To be specific, if xi is closer to uk, we will have a larger vik .
T 2	XX		2	T

Otherwise, we will have a smaller vik when xi and uk are dissimilar. In clustering tasks, both NMF and NLCF rely on performing K-
min X — UV
U,V
2  +k
i=1 k=1
vik xi — uk 2 + cTr(V LV),
(6)

means on the learned V to obtain the final results.

The proposed model

In this section, we make an improvement on the existing matrix factorization model to let it learn direct and interpretable cluster- ing results.

Model formulation

We formulate the objective function of the joint non-negative and fuzzy coding (JNFC) model as
s.t.U P 0, V P 0, V1 = 1.
Obviously, G-JNFC will degenerate to JNFC when the second
regularization parameter c approaches zero. In general, our pro- posed G-JNFC have the following three main advantages.
First, G-JNFC is a unified model for joint data representation learning and clustering. In most of existing studies, the learned
coefficient matrix is treated as the data representation on which K-means is additionally performed to get the final cluster assignments. In G-JNFC, once the coefficient matrix is obtained, we can obtain the final cluster assignment of each data point by checking the largest value of each row of the coefficient matrix.

n  c	● Second, G-JNFC takes the data manifold into consideration

min  X — UVT 2 + kXXvik  xi — uk  2,
along which the learned coefficient is constrained to vary

U,V	2
2
i=1 k=1
(3)
smoothly. This local invariance idea is of great significance in

s.t.U P 0, V P 0, V1 = 1.
improving the performance of learning models.


Third, G-JNFC takes the connection between the basis matrix and the coefficient matrix into consideration. That is, G-JNFC
uses a binary constraint relating both variables in the model, which can be seen as a second-order constraint.

Optimization method

Below we give an elegant optimization method to the objective function of G-JNFC in (6). Generally, it is under the alternating direction optimization framework in which we put the emphasis

as dii =	n sij. Therefore, we propose to row-wisely optimize V in (6), and the objective function associated with vi is
minvT Avi — vT b, s.t.vi P 0, vT 1 = 1,	(13)
vi
where A , UT U + cdiiI ∈ Rc×c, b = (UT X — kET )i + 2cPn 1sijvj ∈ Rc×1
and (UT X — kET )i is the i-th column of matrix UT X — kET .
To solve (13), we introduce an auxiliary variable z (and simulta-
neously remove the subscript to simplify the expression) to rewrite it as

on the derivation of updating rule to V.
Update U with V fixed. We can rewrite the objective function
min
vP0,vT 1=1,z
zT Av — zT b.	(14)

of (6) as
min  X — UVT 2 + kX  (x 1T — U)K1/2  2,	(7)
Based on the augmented Lagrangian multiplier (ALM) method, problem (14) can be rewritten as

UP0	2
i	i	2
i=1
T	T	l¨	b¨

min  z Av — z b +	v — z +	,	(15)
vP	1,z	l

row of representation coefficient matrix V. The corresponding Lagrangian function is
Accordingly, an alternative optimization method is applied to solve problem (15).

L(U)= X — UVT 2 + kX  (xi1T
i=1
2 + Tr(UU ).
Fixing v, update z. By taking the derivative of problem (15)
with respect to z and setting it to zero, we have

Taking the derivative of L with respect to U, we have
∂L = 2UVT V — 2XV + kX(—2xi1T Ki + 2UKi)+ U.
z = v + b + b — Av .	(16)
l
Fixing z, update v.When z fixed, problem (15) becomes

∂U	i=1
T	l¨	 b¨

Using KKT condition / ujk = 0 (j = 1, ··· , d, k = 1, ··· , c), we get the following equation
(UVT V) ujk  —(XV) ujk + k(XUKi) ujk
min  z Av +	v — z +	,	(17)
v	=1	l
which can be reformulated to the following form by completing the
squared form of v

jk	jk
i=1	jk
l¨	1	T  ¨

—k(X
x 1T K ) u  = 0
(8)
min
vP0,vT 1=1
v — z + (b + A z) .	(18)
l	2

i
i=1
i	jk
jk
Subproblem (18) is a Euclidean projection on the simplex [24–

This leads to the updating rule of U as
(XV + kXx 1T K )
26]. Here we present an effective algorithm to solve (18).
Denote m = z — 1 (b + AT z) and then (18) is equivalent to

i	i	1	2

ujk
← ujk
	i=1	jk ,	(9)
(UVT V + kXUKi)
min
vP0,vT 1=1
v — m  2.	(19)

i=1	jk
The Lagrangian function of (19) is

which can be rewritten in compact matrix form as
L(v, d, g	1 v — m  2 — d(vT 1 — 1)— gTv,	(20)

)=	2

ujk
 ((k + 1)XV)jk 
← ujk	T	.	(10)
(UV V + kUH)jk
where d, g are Lagrangian multipliers to be determined. If v* is the optimal solution, and d*, g* are the corresponding multipliers, based

Here H ∈ Rc×c is a diagonal matrix whose elements are column
sums of V.
on the KKT condition, we have the following equations for each
t ∈ [1, c]

Update V with U fixed. Denoting e	x	u  2 as the i k -
8> v* — mt — d* — g* = 0,

■
th element of matrix E ∈ Rn×c, we have
ik = i — k 
( , )
t	t
>< v* P 0,
(21)

min	X —
VP0,V1=1
UVT 2
+ kTr(EVT )
g* P 0,
>: v*g* = 0

→⇒	min Tr(VU UV )— Tr(V(2U X — kE ))
VP0,V1=1
In this paper, we denote the i-th row of V as vT , and v is a col-
The first equation in (21) can be rewritten as
v* — m — d*1 — g* = 0.	(22)

i	i

umn vector whose elements are corresponding to those of vT . Then
the third term in (6) can be rewritten as
According to the constraint vT 1 = 1, the above equation can be reformulated into

n  n	T	T *

min
XX  v — v
2s ,	(12)
d* = 1 — 1 m — 1 g  .	(23)

i
v P0,vT 1=1
j 2 ij	c

i	i	i=1 j=1
By substituting (23) into (22), we have

where sij is the (i, j)-th element of the graph affinity matrix S. The relation between the graph Laplacian L and S is L = D — S, where D ∈ Rn×n is a diagonal degree matrix with the i-th diagonal element
v* = m —
11T


c
m + 1 1 —
c
1T g*
c
1 + g*.	(24)

Denote g¯* = 1T g* and g = m — 11T m + 1 1, we have

c	c	c
v* = g + g* — g¯*1.	(25)
Hence, for each t ∈ [1, c], we have
v* = g + g* — g¯*.	(26)
According to (21) and (26), we have gt + g* — g¯* = (gt — g¯*) ,
Algorithm 3 The overall procedure of G-JNFC.


Input: the data matrix X ∈ Rd×n (d and n represent the feature vector length and the data size, respectively), the number of
clusters c, regularization parameters k and c; Output: The clustering result.
1: Calculate the graph affinity matrix S ∈ Rn×n based on the

where (a)+
t
= max(a, 0). Therefore, we have
+
‘HeatKernel’ function with the neighboring size 5 and the
bandwidth parameter as the mean of the squared pairwise

v* = (gt — g¯*) .	(27)

t	+
It is obvious that if g¯* can be determined, the optimal solution to v* can be obtained from the above equation. In a similar way,
we can rewrite (26) as g* = v* + g¯* — gt such that g* = (g¯* — gt ) .
distances;
2: Randomly initialize V to satisfy V P 0, V1 = 1;
3: // The outer loop is to optimize the objective (6) of G-JNFC
4: while not converged do

t	t	t
+	5:	// Update the basis matrix U

Therefore, g¯* can be calculated by
*	1	*
t +
t=1
According to v* T 1 = 1 and (27), we can define a function as
c
f (g¯)=	(gt — g¯)+ — 1.	(29)
t=1
When the above function equals to zero, the optimal g¯* can be obtained by Newton method [27] to find the root of (29) as
6:	Update U based on (10);
7:	// The inner loop is to optimize the coefficient matrix V
rowwisely
8:	for i = 1 : n
9:	Prepare matrix A and vector b w.r.t. the i-th row of V;
10:	Update vi by optimizing the objective (13) based on Algorithm 2;
11:	end for
12: end while
13: Obtain the clustering results by checking the largest value of each row of V.

g¯k+1
= g¯k
 f (g¯k)

— f '(g¯k) .	(30)

Consequently, the procedure of row-wisely optimizing V in objective (13) is given in Algorithm 2 and the whole procedure of our proposed G-JNFC model is summarized in Algorithm 3.

Analysis on convergence and complexity

On the convergence of the proposed G-JNFC model, here we give the high level analysis on its optimization procedure. Since the optimization of U is identical to that in NLCF, we can construct the same auxiliary function, i.e., Eq. (14), as shown in the appendix



Input:  the  given  vector  m  ∈ Rc; Algorithm 1 The algorithm to solve subproblem (19).
Output: the target vector v ∈ Rc.
1: Calculate g = m — 11T m + 1 1;
of [20] to prove that the objective function of G-JNFC is nonincreas- ing under the updating rule of (10). When updating the variable V, objective (13) is a quadratic programming problem with con- straints [28]. Since the matrix A is symmetric and positive semi- definite, the optimal solution could be obtained. Then, Algorithm 2 converges to the global optimal solution. As a whole, we con- clude that the convergence to the optimization of G-JNFC can be guaranteed. In the section of experiments, we will show the mono- tonically decreasing of objective function values in terms of the iterations.
The complexity analysis of the optimization procedure of G- JNFC objective function is based on the big O notation and the
usual case of n > d  c.
Updating matrix U. The complexity of updating U by (10) is O(dnc)                  complexity. caused by the matrix multiplication operation, leading to
Updating matrix V. When updating each row of the variable V,

c	c	the main complexity is O(c2) which comes from the calculation

2: Obtain the root g¯* of (29) based on Newton’s method;
of v in (13), leading to
nc2
complexity in updating V.

3: Get the optimal solution v* = (gt — g¯*)
for each t ∈ [1, c];	i
O(	)

t	+	● Updating matrix A and vector b. When calculating the intermedi-
with complexities O(dc2) and O(dnc) can be ignored since it ate matrix A and vector b, the multiplication of UT U and UT X can be calculated out of the loop of Algorithm 2.



Input: matrix A ∈ Rc×c and vector b ∈ Rc; Algorithm 2: The algorithm to solve subproblem (13)
Output: vector v ∈ Rc.
1: Initialize v, l, b, and q; 2: while not converged do
3:	Update z by (16);
5:	Update   b   =   b  +  l(v  —  z); 4:	Update v by solving problem (18) via Algorithm 1;
6:	Update l = ql;
7: end while
Assuming that the iteration numbers of Algorithms 1 and 2 are
posed G-JNFC model is O(t1(dnc + t2nc2)). From the following t1 and t2, respectively, we have the overall complexity of the pro- experiments, we will find that the optimization converges in a
few iterations.


Experiments

In this section, experiments are conducted on both synthetic and representative benchmark data sets to evaluate the perfor- mance of our proposed G-JNFC model.



Experiment on synthetic data

To intuitively show the fuzzy effect of G-JNFC in data clustering, we generate a synthetic data set composed of five Gaussian dis- tributed clusters, as shown in the left rand side of Fig. 1. The blue cluster has 200 data points and each of the remaining clusters has 75 data points. We visualize the clustering results obtained by G- JNFC in the right hand side of Fig. 1 in which the numbers of the data points with fuzzy membership towards different clusters are highlighted. The learned coefficient matrix, which simultaneously acts as the fuzzy clustering indicator matrix, is shown in Fig. 2. In Fig. 2, the abscissa values represent the 500 data points and ordinate values of the five subplots show the membership degrees of data points to the blue, red, green, cyan and purple clusters, respectively. We can observe the obvious correspondence between the numbered data points in Fig. 1 and their non-zero membership degrees towards different clusters in Fig. 2.
Below we give the explanations to some representative num- bered samples of two different scenarios. 1) The first one is that though the data point was generated by one cluster, it is sur- rounded by data points from the other cluster. For example, the 33rd data point was originally generated by the blue Gaussian dis- tribution; however, it is obviously much closer to the data points from the purple cluster based on our visualization. Therefore, it




Fig. 1. Synthetic Gaussian distributed data (left) and the clustering results obtained by G-JNFC (right) which are best viewed in color.
is more reasonable to group it into the purple cluster. The 214th and 433rd data points share similar properties. 2) The other sce- nario is that if a data point is in the boundary area of multiple clus- ters, it may have non-zero membership degrees to those clusters. For example, the learned coefficient vector corresponding to the
70th data point, i.e., vT , is [0.721,0,0.279,0,0], meaning that it is
in the boundary area of the first (blue) cluster and the third (green) cluster. However, it is more reasonable to group it into the blue cluster if we have to determine its only one membership
(0.721>0.279). From Fig. 1, we can find that the 70th data point
is actually more closer to the blue cluster, reflecting the consis-
tency between the visualization and the computational result. There are also some similar data points such as the 110th, 140th, and 145th ones.
Based on the above experimental results on synthetic data, we can conclude that the fuzzy information depicted by G-JNFC is more consistent to our intuitive understanding to the knowledge involved in the data.

Experiment on benchmark data

Evaluation metrics
To evaluate the clustering results, we employ the three widely used metrics, i.e., Accuracy (ACC), Normalized Mutual Information (NMI) and Purity, to measure the clustering performance [29] whose definitions are given below.
Accuracy exploits the one-to-one relationship between clusters and classes. It measures the extent to which each cluster contained samples from the corresponding class, which has the following definition
n
Acc	map r  l	31
n i=1
truth label, n is the sample size, d(x, y) equals to one if x = y and zero otherwise. map(ri) is the permutation mapping function which where ri denotes the cluster label of xi and li denotes its ground
can map each cluster label ri to the equivalent class label.
Normalized Mutual Information (NMI) is used to determine the quality of clusters. Given a clustering result, the NMI is esti- mated by









Fig. 2. The membership degrees of each data point to different clusters reflected by the learned coefficient matrix V of G-JNFC.


c  c

XXn log  nij 
TDT2 text data set, dig1-10 digit image data set, Yale face image

ij
i=1 j=1
ni n^j
data set and the Isolet spoken letter data set. Their basic properties

NMI = vuﬃﬃXﬃﬃﬃcﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃXﬃﬃﬃcﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
(32)
including the numbers of samples, feature dimensions and clusters

ut(
n log ni )(
n^ log n^j )
were summarized in Table 1.

i
i=1
n	j	n
j=1
In order to show the effectiveness of our G-JNFC model, we compare  it  with  some  state-of-the-art  clustering  methods

Ci(1 6 i 6 c), n^j is the number of samples belonging to the class where ni represents the number of samples in the cluster Lj(1 6 i 6 c), and nij is the number of samples in the intersection
between cluster Ci and Lj.
Purity measures the extent to which each cluster primarily con- tained samples from one class. The clustering purity is the weighted sum of individual cluster purity values, given by
Purity = X ni P(Si), P(Si   1 max(nj),	(33)
including K-means, Non-negative Matrix Factorization (NMF) [30], concept factorization (CF) [31], normalized cut (NCut), fuzzy-C-means (FCM), nonnegative local coordinate factorization (NLCF) and its graph regularized version [20]. Moreover, we include the comparison of JNFC, which is equivalent to setting
the regularization parameter c to zero in G-JNFC. For graph-
based clustering models including the NCut, GNLCF and G-JNFC, we set the number of nearest neighbors to 5 as suggested in [23]. The ‘Heatkernel’ function was used to measure the similarity

i=1 n
ni  j	i
of data pairs where the bandwidth parameter was set as the average value of pairwise distances.

where Si is a certain cluster with size ni, nj is the number of the i-th
input class which was assigned to the j-th cluster.
If there is(are) free regularization parameter(s), we tuned it
(them) from n10—5, 10—4, ··· , 105o to let the related methods

Data sets and experimental settings
Five representative benchmark data sets were used in the experiment including the Caltech101 natural image data set,

Table 1
Basic properties of the five data sets used in the experiments.

was selected from {1, 1.1, ··· , 2}. For methods which rely on achieve the best performance. The fuzzy order parameter in FCM K-means as postprocessing method to get the final results includ-
ing NMF, CF, NCut, NLCF and GNLCF, we repeated K-means five times with different initializations and recorded the best results. For fuzzy clustering methods including FCM, JNFC and G-JNFC, we investigated the largest membership degree corresponding to the data point to determine its assignment.

Experimental results and analysis
Tables 2–6 show the clustering results on the Caltech101, TDT2, dig1-10 and Isolet data sets, respectively. In order to randomize the experiments, we conduct the evaluations with different cluster



Table 2
Clustering performance (%) on of compared algorithms Caltech101 data set.



Table 3
Clustering performance (%) of compared algorithms on TDT2 data set.



numbers. For each given cluster number c, 20 test runs were con- ducted on different randomly selected clusters (except the case when the whole data set is used). The mean and standard error of the performance are reported in the tables where the best results are highlighted in boldface.
From these five tables, we can observe that the proposed G- JNFC model obtains better performance than the other methods in most cases, reflecting its effectiveness in data clustering. In G- JNFC, the two regularization terms bring more desirable properties into the variables, i.e., the basis matrix and the coefficient matrix; specifically, the second term in (6) builds bidirectional connection- ship between the basis matrix and the coefficient matrix and the third term in (6) enforces the learned coefficient to respect the local manifold of data. By pairwisely comparing the results obtained by JNFC and NMF, we can conclude that the local coordi- nate coding is beneficial to boost the clustering performance. Sim- ilarly, since the results of G-JNFC are better than those of JNFC, meaning that exploring the structure information of data can is beneficial to efficient coefficient learning. It worths mentioning that though G-JNFC obtained weaker performance in some cases, it provides us a direct way to achieve the final clustering results and therefore there is no necessary to perform any postprocessing step. What is more, we may find the data points in boundary area based on the fuzzy clustering indicator.
To illustrate the statistical significance between our G-JNFC method and other algorithms, we performed the paired students t-test on their clustering results. Here the hypothesis is ‘‘the clus- tering performance (accuracy, normalized mutual information, purity) obtained by G-JNFC is greater than that obtained by the other (given) method”. Each test was run on two sequences corre-
sponding to clustering results of 20 splits by our method and the given method. In Tables 2–6, the statistical results were reported in which the binary value in brackets. For each entity in brackets, ‘‘1” means that the hypothesis is correct (true) with probability 0.95, and ‘‘0” means that the hypothesis is wrong (false) with prob- ability 0.95. For example, on dig1-10 data set when the number of clusters is 9 in Table 4, the accuracy of GNLCF over 20 runs is
80.3 ± 4.9 and the accuracy of G-JNFC over 20 runs is 83.1 ± 3.8. The appended ‘‘(1)” means the hypothesis of G-JNFC is superior to GNLCF is correct based on the statistical test. In summary, from Tables 2–6, we see the conclusion that our G-JNFC achieves better performance is correct in most of cases on all the data sets.
As shown in (6), there are two regularization parameters in the G-JNFC objective function. To show how they affect the perfor- mance of G-JNFC in clustering, we show the variation of G-JNFC performance in terms of the three metrics on Caltech101 and TDT2  data  sets  by  grid  searching  both  parameters  from
{10—5, 10—4, ··· , 105} in Fig. 3. Obviously, there are wide ranges for the candidate values selection to parameter combination (k, c) to let G-JNFC achieve promising performance. Roughly, G-JNFC
tends to select a slightly large k and a slightly small c, which fur- ther indicates the importance of the second term in (6). Similar landscapes can be found on the remaining two data sets. From this figure and the analysis, we can generally conclude that G-JNFC
have stable performance with respect to parameters k and c.
Besides the theoretical analysis on the convergence of G-JNFC in Section 3.3, we empirically show the variation of its objective val- ues in terms of iterations on the five used data sets in Fig. 4. From this figure, we observe that our model has desirable convergence speed, usually within 10 iterations.


Table 4
Clustering performance (%) of compared algorithms on dig1-10 data set.





Table 5
Clustering performance (%) of compared algorithms on Yale data set.



Table 6
Clustering performance (%) of compared algorithms on Isolet data set.




Fig. 3. The performance of G-JNFC in terms of parameters k and c on Caltech101 and TDT2 data sets.



Fig. 4. Convergence property of G-JNFC on the five used data sets.



Conclusion

In this paper, we proposed a joint non-negative and fuzzy cod- ing with graph regularization model, termed G-JNFC in which the learned coefficient matrix has two roles. One is that it can be viewed as the data representation and the other is that it is the fuzzy membership degree matrix. Based on the learned coefficient matrix, we can not only determine the membership of each data point to a specified cluster, but also can obtain the membership degree of each data point to different clusters if it is near the clus- tering boundary. Compared with existing matrix factorization based clustering methods, G-JNFC effectively avoids the limitations in the two-stage clustering methods. Finally, we perform extensive experiments to demonstrate the effectiveness of G-JNFC.

Funding

This work was partially supported by Natural Science Founda- tion of China (61971173, 61602140, U1909202, 61972121), China
Postdoctoral Science Foundation (2017M620470), Provincial Key Laboratory for Computer Information Processing Technology, Soo- chow University (KJS1841), Planted Talent Plan of Zhejiang Pro- vince (2019R407030) and Open Fund of Engineering Research Center of Cognitive Healthcare of Zhejiang Province, Sir Run Run Shaw Hospital (2018KFJJ05).

References

Wang Y-X, Zhang Y-J. Nonnegative matrix factorization: a comprehensive review. IEEE Trans Knowl Data Eng 2012;25(6):1336–53.
He Y-C, Lu H-T, Huang L, Shi X-H. Non-negative matrix factorization with pairwise constraints and graph laplacian. Neural Process Lett 2015;42 (1):167–85.
Peng Y, Long Y, Qin F, Kong W, Cichocki A. Flexible non-negative matrix factorization with adaptively learned graph regularization. In: IEEE International Conference on Acoustics, Speech and Signal Processing. p. 3107–11.
Zhang Z, Zhang Y, Li S, Liu G, Zeng D, Yan S, Wang M. Flexible auto-weighted local-coordinate concept factorization: A robust framework for unsupervised clustering. IEEE Trans Knowled Data Enghttps://doi.org/10.1109/TKDE.2019. 2940576..
Li Z, Tang J, He X. Robust structured nonnegative matrix factorization for image representation. IEEE Trans Neural Networks Learn Syst 2018;29(5):1947–60.
Zhang Z, Zhang Y, Liu G, Tang J, Yan S, Wang M. Joint label prediction based semi-supervised adaptive concept factorization for robust data representation. IEEE Trans Knowl Data Eng 2020;32(5):952–70.
Long X, Lu H, Peng Y, Li W. Graph regularized discriminative non-negative matrix factorization for face recognition. Multimedia Tools Appl 2014;72 (3):2679–99.
Wu C, Song Y, Zhang Y. Multi-view gait recognition using NMF and 2DLDA. Multimedia Tools Appl 2019;78(24):35789–811.
Fodeh SJ, Tiwari A. Exploiting medline for gene molecular function prediction via nmf based multi-label classification. J Biomed Inform 2018;86:160–6.
Wang M, Zhang B, Pan X, Yang S. Group low-rank nonnegative matrix factorization with semantic regularizer for hyperspectral unmixing. IEEE J Selected Top Appl Earth Observ Remote Sens 2018;11(4):1022–9.
Elhamifar E, Vidal R. Sparse subspace clustering. IEEE Conference on Computer Vision and Pattern Recognition 2009:2790–7.
Elhamifar E, Vidal R. Sparse subspace clustering: algorithm, theory, and applications. IEEE Trans Pattern Anal Mach Intell 2013;35(11):2765–81.
Peng Y, Lu B-L. Discriminative extreme learning machine with supervised sparsity preserving for image classification. Neurocomputing 2017;261:242–52.
Liu G, Lin Z, Yu Y. Robust subspace segmentation by low-rank representation. In: International Conference on International Conference on Machine Learning.
p. 663–70.
Liu G, Lin Z, Yan S, Sun J, Yu Y, Ma Y. Robust recovery of subspace structures by low-rank representation. IEEE Trans Pattern Anal Mach Intell 2013;35 (1):171–84.
Peng Y, Lu B-L, Wang S. Enhanced low-rank representation via sparse manifold adaption for semi-supervised learning. Neural Networks 2015;65:1–17.
Simhachalam B, Ganesan G. Performance comparison of fuzzy and non-fuzzy classification methods. Egypt Inform J 2016;17(2):183–8.
Alruwaili M, Siddiqi MH, Javed MA. A robust clustering algorithm using spatial fuzzy C-means for brain MR images. Egypt Inf J 2020;21(1):51–66.
Yu K, Zhang T, Gong Y. Nonlinear learning using local coordinate coding. Advances in neural information processing systems 2009:2223–31.
Chen Y, Zhang J, Cai D, Liu W, He X. Nonnegative local coordinate factorization for image representation. IEEE Trans Image Process 2013;22(3):969–79.
Liu H, Yang Z, Yang J, Wu Z, Li X. Local coordinate concept factorization for image representation. IEEE Trans Neural Networks Learn Syst 2013;25 (6):1071–82.
Zhang R, Nie F, Guo M, Wei X, Li X. Joint learning of fuzzy k-means and nonnegative spectral clustering with side information. IEEE Trans Image Process 2019;28(5):2152–62.
Cai D, He X, Han J, Huang TS. Graph regularized nonnegative matrix factorization for data representation. IEEE Trans Pattern Anal Mach Intell 2010;33(8):1548–60.
Kyrillidis A, Becker S, Cevher V, Koch C. Sparse projections onto the simplex. In: International Conference on Machine Learning. p. 235–43.
Nie F, Yang S, Zhang R, Li X. A general framework for auto-weighted feature selection via global redundancy minimization. IEEE Trans Image Process 2018;28(5):2428–38.
Chen X, Yuan G, Nie F, Zhong M. Semi-supervised feature selection via sparse rescaled linear square regression. IEEE Trans Knowl Data Eng 2020;32 (1):165–76.
Sherman AH. On newton-iterative methods for the solution of systems of nonlinear equations. SIAM J Numer Anal 1978;15(4):755–71.
Delbos F, Gilbert JC. Global linear convergence of an augmented lagrangian algorithm to solve convex quadratic optimization problems. J Convex Anal 2005;12(1):45–69.
Huang J, Nie F, Huang H. A new simplex sparse learning model to measure data similarity for clustering. In: International Joint Conference on Artificial Intelligence. p. 3569–75.
Xu W, Liu X, Gong Y. Document clustering based on non-negative matrix factorization. In: International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 267–73.
Xu W, Gong Y. Document clustering by concept factorization. In: International ACM SIGIR Conference on Research and Development in Information Retrieval.
p. 202–9.
