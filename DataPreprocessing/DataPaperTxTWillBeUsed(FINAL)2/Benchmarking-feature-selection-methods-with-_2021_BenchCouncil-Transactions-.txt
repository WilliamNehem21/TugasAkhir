BenchCouncil Transactions on Benchmarks, Standards and Evaluations 1 (2021) 100004








Benchmarking feature selection methods with different prediction models on large-scale healthcare event data
Fan Zhang a, Chunjie Luo a,b,c, Chuanxin Lan a, Jianfeng Zhan a,b,c,∗
a Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China
b School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing 100049, China
c International Open Benchmark Council (BenchCouncil)


A R T I C L E  I N F O	A B S T R A C T


Keywords:
Feature selection Genetic algorithm Deep neural networks Healthcare prediction


With the development of the Electronic Health Record (EHR) technique, vast volumes of digital clinical data are generated. Based on the data, many methods are developed to improve the performance of clinical predictions. Among those methods, Deep Neural Networks (DNN) have been proven outstanding with respect to accuracy by employing many patient instances and events (features). However, each patient-specific event requires time and money. Collecting too many features before making a decision is insufferable, especially for time-critical tasks such as mortality prediction. So it is essential to predict with high accuracy using as minimal clinical events as possible, which makes feature selection a critical question. This paper presents detailed benchmarking results of various feature selection methods, applying different classification and regression algorithms for clinical prediction tasks, including mortality prediction, length of stay prediction, and ICD-9 code group prediction. We use the publicly available dataset, Medical Information Mart for Intensive Care III (MIMIC-III), in our experiments. Our results show that Genetic Algorithm (GA) based methods perform well with only a few features and outperform others. Besides, for the mortality prediction task, the feature subset selected by GA for one classifier can also be used to others while achieving good performance.





Introduction

Over the past decades, the Electronic Health Record (EHR) tech- nique is developed; vast volumes of digital clinical data are generated, making it possible for Clinical Decision Support Systems (CDSSs) to make better decisions. For example, public databases such as MIMIC- III [1] have promoted the research in clinical predictions. Based on those databases, different severity scoring systems, traditional ma- chine learning algorithms, and DNNs are developed and continuously improved to achieve better clinical prediction tasks such as patient mortality, disease classification, and length of hospital stay.
Traditional severity scores like Simplified Acute Physiology Score (SAPS-II) [2], the Sepsis-related Organ Failure Assessment (SOFA) [3], and Acute Physiology and Chronic Health Evaluation (APACHE) [4] are standard for mortality prediction in practice. Clinicians usually choose the patient-specific events they used based on their experience. Then a standard process is implemented. First, a severity score is calculated based on the relative events, usually measured within the first 24 h after ICU admission. Second, a simple model such as logistic regression is applied to the score to predict the final death probability.
Recent work shows that DNN and Super Learner (SL) algorithms perform better than single traditional classifiers and severity scoring
systems [5–8]. To improve the predictive performance, many DNN models are developed. Purushotham et al. [6] proposed a Multimodal Deep Learning Model (MMDL) to process an extensive feature set, which consists of 141 features, and got very good predicting results. Harutyunyan et al. [7] proposed a multitask LSTM-based method to predict four clinical prediction tasks. In addition to DNN, SL is also studied extensively and shows promising results. Pirracchio et al. [5] provided and assessed the performance of the Super ICU Learner Algo- rithm (SICULA). Lee et al. [8] trained case-specific Random Forests (RF) to make mortality prediction and exhibited the best AUROC compared with other single models such as death counting, logistic regression, and decision tree.
No matter which method we use, feature selection is an important part. First, medical databases store vast amounts of clinical events and not all of them are related to the target task. Second, minimal clinical events enable doctors to make timely decisions. For severity scoring systems, a set of alternated related events is chosen based on clinicians’ experience. A simple subset of those events is selected according to correlation coefficient or other index associated with the target concept of the prediction task [2–4,10]. Traditional machine learning and deep learning algorithms usually take the same features directly used in

∗ Corresponding author at: Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China.
E-mail addresses: zhangfan@ict.ac.cn (F. Zhang), luochunjie@ict.ac.cn (C. Luo), lanchuanxin@ict.ac.cn (C. Lan), zhanjianfeng@ict.ac.cn (J. Zhan).

https://doi.org/10.1016/j.tbench.2021.100004
Received 6 August 2021; Received in revised form 11 October 2021; Accepted 20 October 2021
Available online 3 November 2021
2772-4859/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).


Table 1
Comparison of benchmarking works.


severity scores [6,7] or take a similar method to select features [9]. In this paper, we do an exhaustive evaluation of various feature selection methods, and our main contributions are listed below.
Table 2
Summary statics of cohort selection.


Data	Total




tasks.
(2) Compare feature subsets selected by GA for different classifiers. The results show that for the mortality prediction task, the features chosen by GA are universal for different classifiers.
The rest of this paper is organized as follows: in Section 2, we provide an overview of the related work; in Section 3, we describe the dataset and methods we employed; in Section 4, the benchmarking experiments and results are reported and discussed in detail; in Section 5, we summarize the paper.

Related work

First, we summarize the feature selection algorithms that are ap- plied in the medical field. Then we discuss the existing benchmarks on healthcare datasets, especially for MIMIC-III. The comparison of benchmarks is listed in Table 1.
The first severity scores proposed such as APACHE [4], APACHE- II [11], and SAPS [12] selected features based on experience of medical experts. Further work usually used statistical methods to calculate correlation coefficient associated with target prediction task, such as [2, 13,14]. Since publication, all of the methods have been continuously modified to improve the predictive performance [15]. A lot of work employed GA to select risk factors and predict in-hospital mortality [9, 10,16–19]. In this work, we report an exhaustive set of benchmarking results of feature selection methods, including GA.
Public datasets such as MIMIC-III have promoted the benchmarking of models for clinical prediction tasks. Purushotham et al. [6] bench- marked deep learning models based on an extensive feature set and get high Area Under the Receiver Operating Characteristic Curve (AUROC) and Area under Precision–Recall Curve (AUPRC). The complete feature set we used is the same as the feature set C in [6], which contains 136 time-series features and five non-time series features. Harutyunyan et al. [7] first benchmarked four clinical prediction tasks and presented a multitask classifier. The most significant difference between us and previous works is that we benchmark feature selection algorithms, es- pecially GA, instead of classification or regression algorithms. Krishnan et al. [9] proposed a GA-based model to make mortality prediction. We extend the benchmark to the other two prediction tasks and combine GA with DNN models to get higher AUROC and AUPRC. Johnson et al. [20] reproduced 28 published works for mortality prediction, and the results showed that it is a big challenge to reproduce other people’s work without public code.


Materials and methods

Dataset preprocessing

MIMIC-III is developed by the Massachusetts Institute of Technology (MIT)’s Laboratory for Computational Physiology and contains around 60,000 intensive care unit admissions. MIMIC-III (v1.4) consists of 46,520 distinct patients and 58,976 admissions, from where we select 35,643 admissions for our experiments. We extracted data from 5 commonly used tables, namely inputevents, outputevents, chartevents, labevents, prescriptions tables. The statistics of cohort selection are tabulated in Table 2. We selected the patient cohort based on the following criteria:
Only adult patients, whose age was >15 years at the time of ICU admission, were selected.
Only the first admission was included for each patient. This decision uses the earliest available data to predict and ensure similar data selection compared to other related works.
We only include the patients who died 24 h after the first admis- sion.
Because the original data from MIMIC-III has erroneous records such as missing values, inconsistent units, etc., we clean data according to [6], which includes the following procedures: (a) Unify the units. (b) Select one valid record. For multiple records simultaneously, take the average values for numerical data and bring the first for categorical data. (c) Re-sample and fill-in the data. Time-series data is divided into hours and a forward–backward imputation is done to impute the missing values.

Prediction tasks

For benchmark, we select three clinical prediction tasks which are important in critical care research and are commonly studied by machine learning researchers. The first is in-hospital mortality which is important for doctors to take effective actions for patient care in Intensive Care Units (ICUs) [9]. The second is ICD-9 code group predic- tion, where we divide the ICD-9 codes into 20 groups according to [6] and treat it as a multi-classification problem. The third is length of stay prediction, which is to predict the hospital stay after admission.



Table 3
Genetic algorithm. Genetic Algorithm
Input: Dataset D(X, Y) , classifier C and target number of features N
Output: Optimal N features X′ for C
Randomly generate N features from X as X0
Calculate fitness for X0, which is AUC for D(X0, Y) and C.
Set X′ =X
while i ≤ 10000 do
Generate a new feature set X𝑖
by randomly replacing one feature in X′
Calculate fitness for X𝑖
if X .fitness > X′ .fitness:
X′ =X
if X′ .fitness ⩾ 1
return X′
return X′


Feature selection/extraction methods

We extract 136 time-series features and five non-time series features as our full feature set according to [6]. Those features are selected based on clinical significance and missing rate while containing all fea- tures used in severity scoring systems such as SAPS-II and SOFA. Based on this feature set, different feature selection methods are evaluated including GA based methods, scoring methods and machine learning methods.
GA is a metaheuristic inspired by the process of natural selection. It can be used to generate high-quality solutions to optimization and search problems by the process of mutation, crossover, and selec- tion [21]. It is proved to be useful in feature selection as a wrapper feature selection technique [9]. Table 3 lists the GA procedure we used.

For scoring methods, we choose two popularly used severity scores, namely SAPS-II and SOFA. SAPS-II [2] is designed to predict the prob- ability of hospital mortality. It can be calculated based on 17 variables which can be expand into 20 raw features of our complete feature set. SOFA [3] score can be calculated based on 6 variables which can be expand into 17 raw features of our complete feature set.
For machine learning methods, Principal Component Analysis (PCA) [22] and Recursive Feature Elimination (RFE) are chosen.
PCA is a widely used filter feature extraction technique, which projects the data to a new orthogonal space and then chooses a few of the essential features to achieve dimensionality reduction. RFE is a wrapper feature selection technique that selects features based on the accuracy of the subsequent classifiers.

Classification/regression methods

For machine learning we use three common commonly used algo- rithms: decision tree, Bayesian ridge regression, and logistic regres- sion. For DNN we use three types of deep models, namely Feedfor- ward Neural Networks (FNN), Recurrent Neural Networks (RNN), and Multimodal Deep Learning Model (MMDL) according to [6].

Benchmarking results

Based on the MIMIC-III dataset, we report the experimental results for three prediction tasks, which answer the following questions: (a) Can DNN models use relatively small feature subsets to perform as well as the full feature set? (b) Whether the subset of features selected by GA is universal for different classifiers of the same task?
Mortality prediction task evaluation

Tables 4, 5 show the results of mortality prediction task. Because PCA cannot handle time-series data, it is blank for RNN and MMDL results. We can observe that: (a) Deep learning-based prediction mod- els perform better than traditional machine learning-based models and obtain around 2%–20% and 10%–30% improvement for AUROC and AUPRC, respectively. (b) GA performs better and obtains around 6%–18% and 10%–40% improvement over other methods for AUROC and AUPRC, respectively. (c) Compared with using all features (141 features), GA gets similar or even better results with only 20 features. Fig. 1 shows the result of applying the features selected by GA- MMDL to other classifiers. We can observe that: (a) Although GA is a wrapper feature selection method, the features that GA-MMD chooses can also be used to other classifiers and achieve almost as good results
as the GA combined with the specific classifier.

ICD-9 code prediction task evaluation

We divided the dataset into 20 classes according to [6] and treated it as a multi-classification task. However, because Bayes and LR in the package of scikit-learn do not support multi-classification tasks, we perform binary classification for these two algorithms and then calculate the average AUROC and AUPRC as the final results.
Tables 6, 7 show the results of icd-9 code group prediction task. We can observe that: (a) Deep learning prediction models perform better than traditional machine learning models and obtain around 9%–25% and 20%–35% improvement for AUROC and AUPRC, respectively. (b) GA performs better and obtains around 1%–10% improvement over other methods for both AUROC and AUPRC. (c) Compared with using all features (141 features), GA gets similar or even better results with only 20 features.
Fig. 2 shows the result of applying the features selected by GA- MMDL to other classifiers. We can observe that: (a) Only for deep learning models, the features that GA-MMDL chooses achieve similar results with the GA combined with the specific classifier. For machine learning classifiers, the features that GA-MMDL chooses do not perform well.

Length of stay prediction task evaluation

Table 8 shows the results of the length of the stay prediction task. We remove the LR algorithm since it is not capable of processing regression problems. We can observe that: (a) GA performs better than others and obtains around 6%–30% improvement over other methods in terms of MSE (in hours). (b) Compared with using all features (141 features), GA gets similar or even better MSE with only 20 features, save time and money.
Fig. 3 shows the result of applying the features selected by GA- MMDL to other regressors. We can observe that: (a) Only for the RNN model, the features GA-MMDL selects have good performance. For the other classifiers, the features that GA-MMDL chooses do not perform well.

Statistical significance tests of GA

From the above results, we can see that GA always performs better than other feature selection methods. We think this is because as the number of iterations (epochs) increases, GA can reach the local optimum. If there are enough iterations, GA can even reach the global optimum. The price of high precision is time overhead. However, this feature selection method only need to be trained once offline and in actual application doctors can quickly make a diagnosis with a few features.
To further check whether GA’s improved performance is statistically significant compared with others we conducted statistical tests. The


Table 4
AUROC of in-hospital mortality prediction task.
Algorithm	Score method (Features)	Feature extraction/selection (Features)	All features (141)


Table 5
AUPRC of in-hospital mortality prediction task.
Algorithm	Score method (Features)	Feature extraction/selection (Features)	All features (141)


/ig. 1. Apply the features selected by GA-MMDL to other classifiers for the mortality prediction task.



Table 6
AUROC of icd-9 code group prediction task.
Algorithm	Score method (Features)	Feature extraction/selection (Features)	All features (141)


Table 7
AUPRC of icd-9 code group prediction task.
Algorithm	Score method (Features)	Feature extraction/selection (Features)	All features (141)


results are tabulated in Table 9. We can see that GA is statistically significant for mortality and length of stay prediction tasks but not for ICD-9 code group classification. This may be because it is more difficult to improve AUROC and AUPRC of multi-classification than binary-classification tasks.
Summary

This paper presented comprehensive benchmarking results of dif- ferent feature selection methods and classification algorithms on three clinical prediction tasks. We demonstrated that: (a) GA always performs




/ig. 2. Apply the features selected by GA-MMDL to other classifiers for icd-9 code prediction task.


Table 8
MSE of length of stay prediction.
Algorithm	Score method (Features)	Feature extraction/selection (Features)	All features (141)





/ig. 3. Apply the features that GA-MMDL selects to other classifiers for the length of the stay prediction task.

Table 9
Whether is statistically significant with significance level 0.05.


Task	Metric	SAPS-II	SOFA	PCA	RFE


Mortality	AUROC	Yes	No	Yes	Yes AUPRC	Yes	Yes	Yes	Yes

ICD9	AUROC	No	No	No	No
AUPRC	No	No	No	No


Length of stay	MSE	Yes	Yes	Yes	Yes





better than other feature selection methods; for mortality and length of stay tasks, the improved performance is statistically significant.
Compared with using all features, GA gets similar or even better predictive results with much fewer features, save time and money, which makes it more advantageous to detect and collect clinical data.
Other classifiers can also use the features that GA-MMDL selects for the mortality prediction task, achieving good performance.
As part of future work, we plan to make a severity-scoring system based on the features that GA-MMDL selects for the mortality task. This system promises doctors to quickly and accurately assess the severity of a patient’s disease with a few simple variables.

References

A.E. Johnson, T.J. Pollard, L. Shen, H.L. Li-Wei, M. Feng, M. Ghassemi, B. Moody,
P. Szolovits, L.A. Celi, R.G. Mark, MIMIC-III, a freely accessible critical care database, Sci. Data 3 (1) (2016) 1–9.
J.-R. Le Gall, S. Lemeshow, F. Saulnier, A new simplified acute physiology score (SAPS II) based on a European/North American multicenter study, JAMA 270
(24) (1993) 2957–2963.
J.-L. Vincent, R. Moreno, J. Takala, S. Willatts, A. De Mendonça, H. Bruining,
C. Reinhart, P. Suter, L.G. Thijs, The SOFA (Sepsis-related Organ Failure Assessment) score to describe organ dysfunction/failure, Springer-Verlag, 1996.
G. Bhandoria, J.D. Mane, Can surgical apgar score (SAS) predict postoperative complications in patients undergoing gynecologic oncological surgery? Indian J. Surg. Oncol. 11 (1) (2020) 60–65.
R. Pirracchio, M.L. Petersen, M. Carone, M.R. Rigon, S. Chevret, M.J. van der Laan, Mortality prediction in intensive care units with the super ICU learner algorithm (SICULA): a population-based study, Lancet Respir. Med. 3 (1) (2015) 42–52.
S. Purushotham, C. Meng, Z. Che, Y. Liu, Benchmarking deep learning models on large healthcare datasets, J. Biomed. Inform. 83 (2018) 112–134.
H. Harutyunyan, H. Khachatrian, D.C. Kale, G. Ver Steeg, A. Galstyan, Multitask learning and benchmarking with clinical time series data, Sci. Data 6 (1) (2019) 1–18.
J. Lee, Patient-specific predictive modeling using random forests: an ob- servational study for the critically ill, JMIR Med. Inform. 5 (1) (2017) e3.
G.S. Krishnan, S. Kamath, A novel GA-ELM model for patient-specific mortality prediction over large-scale lab event data, Appl. Soft Comput. 80 (2019) 525–533.
A.E. Johnson, A.A. Kramer, G.D. Clifford, A new severity of illness scale using a subset of acute physiology and chronic health evaluation data elements shows comparable predictive accuracy, Crit. Care Med. 41 (7) (2013) 1711–1718.
W.A. Knaus, E.A. Draper, D.P. Wagner, J.E. Zimmerman, APACHE II: a severity of disease classification system, Crit. Care Med. 13 (10) (1985) 818–829.
J.-R. Le Gall, P. Loirat, A. Alperovitch, P. Glaser, C. Granthil, D. Mathieu, P. Mercier, R. Thomas, D. Villers, A simplified acute physiology score for ICU patients, Crit. Care Med. 12 (11) (1984) 975–977.
M. Hoogendoorn, A. El Hassouni, K. Mok, M. Ghassemi, P. Szolovits, Prediction using patient comparison vs. modeling: a case study for mortality prediction, in: 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC, IEEE, 2016, pp. 2464–2467.
J. Calvert, Q. Mao, J.L. Hoffman, M. Jay, T. Desautels, H. Mohamadlou, U. Chettipally, R. Das, Using electronic health record collected clinical variables to predict medical intensive care unit mortality, Ann. Med. Surg. 11 (2016) 52–57.
R. Pirracchio, Mortality prediction in the icu based on mimic-ii results from the super icu learner algorithm (sicula) project, in: Secondary Analysis of Electronic Health Records, Springer, 2016, pp. 295–313.
R. Ahmad, P.A. Bath, Identification of risk factors for 15-year mortality among community-dwelling older people using cox regression and a genetic algorithm,
J. Gerontol. (A Biol. Sci. Med. Sci.) 60 (8) (2005) 1052–1058.
L.J. Adams, G. Bello, G.G. Dumancas, Development and application of a genetic algorithm for variable optimization and predictive modeling of five- year mortality using questionnaire data, Bioinform. Biol. Insights 9 (2015) BBI–S29469.



C.-L. Chan, H.-W. Ting, Constructing a novel mortality prediction model with Bayes theorem and genetic algorithm, Expert Syst. Appl. 38 (7) (2011) 7924–7928.
M.C. Engoren, R. Moreno, D.R. Miranda, A genetic algorithm to predict hospital mortality in an ICU population, Crit. Care Med. 27 (12) (1999) A52.
A.E. Johnson, T.J. Pollard, R.G. Mark, Reproducibility in critical care: a mortality prediction case study, in: Machine Learning for Healthcare Conference, 2017, pp. 361–376.
M. Mitchell, An Introduction to Genetic Algorithms, Vol. 1996, MIT press, Cambridge, Massachusetts. London, England, 1996.
K. Pearson, LIII. on lines and planes of closest fit to systems of points in space, Lond. Edinb. Dublin Philos. Mag. J. Sci. 2 (11) (1901) 559–572.
