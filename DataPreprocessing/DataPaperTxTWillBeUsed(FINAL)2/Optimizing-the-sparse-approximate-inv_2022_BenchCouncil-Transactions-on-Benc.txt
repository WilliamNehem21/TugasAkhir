BenchCouncil Transactions on Benchmarks, Standards and Evaluations 2 (2022) 100087







Research Article
Optimizing the sparse approximate inverse preconditioning algorithm on GPUâœ©
Xinyue Chu, Yizhou Wang, Qi Chen, Jiaquan Gao âˆ—
Jiangsu Key Laboratory for NSLSCS, School of Computer and Electronic Information, Nanjing Normal University, Nanjing 210023, China


A R T I C L E  I N F O	A B S T R A C T


Keywords:
Sparse approximate inverse Preconditioning
CUDA GPU


In this study, we present an optimization sparse approximate inverse (SPAI) preconditioning algorithm on GPU, called GSPAI-Opt. In GSPAI-Opt, it fuses the advantages of two popular SPAI preconditioning algorithms, and has the following novelties: (1) an optimization strategy is proposed to choose whether to use the constant or non-constant thread group for any sparse pattern of the preprocessor, and (2) a parallel framework of optimizing the SPAI preconditioner is proposed on GPU, and (3) for each component of the preconditioner, a decision tree is established to choose the optimal kernel of computing it. Experimental results validate the effectiveness of GSPAI-Opt.





Introduction

Given their many-core structures, graphic processing units (GPUs) have become an important resource for scientific computing in re- cent years. Following the introduction of the programming interfaces such as the compute unified device architecture (CUDA) by NVIDIA in 2007 [1], GPUs have been increasingly used as tools for high- performance computation in many fields [2â€“8].
Sparse approximate inverse (SPAI) preconditioners based on the Frobenius norm minimization have proven to be effective in improv- ing the convergence of iterative methods based on Krylov subspaces, e.g., the generalized minimal residual method (GMRES) [9] and the biconjugate gradient stabilized method (BiCGSTAB) [10]. However, due to the high cost of constructing the SPAI preconditioners, many researchers have attempted to accelerate the SPAI preconditioner con- struction on GPU. Gao et al. follow Chowâ€™s work [11], and use a
sparse approximate inverse of ğ´ as the preconditioner in [12]. Rupp
et al. [13] show several static and dynamic SPAI implementations on
GPU. In [14], Dehnavi et al. propose a static SPAI preconditioner on GPU called GSAI. Recently, He and Gao et al. [15] propose a GPU-based static SPAI preconditioning algorithm called SPAI-Adaptive, and verify the effectiveness of SPAI-Adaptive for large-scale matrices. However, when the number of nonzero entries in each column of the precondi- tioner has significant difference, the performance of SPAI-Adaptive is greatly decreased. Furthermore, He and Gao et al. [16] present a sorted static SPAI preconditioning algorithm, called GSPAI-Adaptive, in order to avoid the drawback of SPAI-Adaptive.
SPAI-Adaptive and GSPAI-Adaptive both can be applied to large- scale matrices, and have their own advantages. When the difference
in the nonzero number of each column of the preconditioner is small, the performance of SPAI-Adaptive is generally better than that of GSPAI-Adaptive; when the nonzero number of each column of the
formance than GSPAI-Adaptive. For example, assuming that ğ‘›2ğ‘˜ is the preconditioner has significant difference, SPAI-Adaptive has worse per- nonzero number of the ğ‘˜th column of the preconditioner, ğ‘›2ğ‘šğ‘ğ‘¥ = maxğ‘˜{ğ‘›2ğ‘˜}, and ğ‘›2ğ‘ğ‘£ğ‘” = âˆ‘ğ‘›  ğ‘›2ğ‘˜âˆ•ğ‘›, where ğ‘› is the row number of the preconditioner, we take two integers ğ›¼ and ğ›½, which satisfy 2ğ›¼âˆ’1 <
ğ‘›2ğ‘šğ‘ğ‘¥ â©½ 2ğ›¼ and 2ğ›½âˆ’1 < ğ‘›2ğ‘šğ‘ğ‘¥ â©½ 2ğ›½ , respectively. If ğ›¼ = ğ›½, we
preconditioner is small; if ğ›¼ âˆ’ ğ›½ â©¾ 3, we say that the nonzero number of say that the difference in the nonzero number of each column of the
each column of the preconditioner has significant difference. However, when the difference is large but not significant, which one of SPAI- Adaptive and GSPAI-Adaptive has better performance? For example,
1 â©½ ğ›¼ âˆ’ ğ›½ < 3. There are no conclusions in [15,16].
Inspired by these observations, we further investigate how to highly
optimize the static SPAI on GPU in this paper. Utilizing the advantages of SPAI-Adaptive and GSPAI-Adaptive, we propose an optimized SPAI preconditioning algorithm on GPU, called GSPAI-Opt. Compared to SPAI-Adaptive and GSPAI-Adaptive, the proposed algorithm has the following distinct characteristics:
First, an optimization strategy is presented. Using this strategy, for a given sparsity pattern of the preconditioner, we can obtain the optimization scheme of choosing whether to use the constant or nonconstant thread-group size to calculate the preconditioner.
Second, when the constant thread-group size is applied, for each
dices ğ¼ and ğ½ , constructing the local submatrix, decomposing the one of main components of the preconditioner such as finding in-



âœ© The research has been supported by the Natural Science Foundation of China under grant number 61872422.
âˆ— Corresponding author.
E-mail addresses: 2316607219@qq.com (X. Chu), 1966224230@qq.com (Y. Wang), 1337223917@qq.com (Q. Chen), springf12@163.com (J. Gao).

https://doi.org/10.1016/j.tbench.2023.100087
Received 11 October 2022; Received in revised form 26 February 2023; Accepted 26 February 2023
Available online 3 March 2023
2772-4859/Â© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).


	

/ig. 1. Parallel framework of GSPAI-Opt.


local submatrix into ğ‘„ğ‘…, and solving the upper triangular linear
system, a decision tree is established to choose the optimization kernel of calculating it.
Third, when using the nonconstant thread-group size, for each
posing the local submatrix into ğ‘„ğ‘… and solving the upper trian- one of some components of the preconditioner such as decom-
gular linear system, a decision tree is constructed to choose the optimization kernel to calculate it.
conditioner,  not  just  the  same  sparsity  pattern  as  ğ´. â€¢ Finally, GSPAI-Opt can apply to any sparsity pattern of the pre-
The experimental results show that GSPAI-Opt is effective, and effi- ciently fuses the advantages of SPAI-Adaptive and GSPAI-Adaptive, and outperforms the static SPAI preconditioning algorithm in the ViennaCL library [13], the recent SPAI-Adaptive [15] and GSPAI-Adaptive [16].

Optimizing SPAI on GPU

We present an optimization sparse approximate inverse precondi- tioning algorithm on GPU, called GSPAI-Opt. Fig. 1 lists the parallel framework of GSPAI-Opt, which is composed of the following stages.
Pre-GSPAI stage: Compute the dimensions, choose whether to allocate the constant thread-group size or nonconstant thread- group size for each column of the preconditioner according to the proposed optimization strategy, and allocate the global memory of GPU;
Compute-GSPAI stage: Find indices ğ½ğ‘˜ and ğ¼ğ‘˜, construct local




/ig. 2. Main procedure of selecting the First/Second strategy.



Pre-GSPAI stage

computing ğ‘šğ‘˜ (one column of ğ‘€ ), ğ‘˜ = 1, 2, â€¦ , ğ‘›, the dimensions of First, we compute the dimensions of all local submatrices. When the local submatrices (ğ‘›1ğ‘˜, ğ‘›2ğ‘˜) constructed for each column of the
preconditioner are usually different. To simplify the accesses of data in the memory and enhance the coalescence, the dimensions of all local
submatrices are uniformly defined as (ğ‘›1ğ‘šğ‘ğ‘¥, ğ‘›2ğ‘šğ‘ğ‘¥). Here ğ‘›1ğ‘šğ‘ğ‘¥ =
maxğ‘˜{ğ‘›1ğ‘˜} and ğ‘›2ğ‘šğ‘ğ‘¥ = maxğ‘˜{ğ‘›2ğ‘˜}.
Next, we choose whether to use the constant or nonconstant thread-
group size for each column of the preconditioner. GSPAI-Opt fuses the advantages of SPAI-Adaptive [15] and GSPAI-Adaptive [16]. For SPAI-Adaptive, a thread-adaptive allocation strategy with the con- stant thread-group size is presented, and for GSPAI-Adaptive, a thread- adaptive allocation strategy with the nonconstant thread-group size is presented. For the convenience of readers, in the following contents, we introduce them respectively.
group size: The optimized number of threads ğ‘ is obtained by the Thread-adaptive allocation strategy with the constant thread-
following formula:
ğ‘ = min(2ğ‘ , ğ‘›ğ‘¡),	(1)
ğ‘ .ğ‘¡.

submatrix ğ´Ì‚ , decompose ğ´Ì‚
into ğ‘„ ğ‘… , and solve ğ‘… ğ‘šÌ‚
= ğ‘„ğ‘‡ ğ‘’Ì‚ ;

ğ‘˜	ğ‘˜
ğ‘˜  ğ‘˜
ğ‘˜ ğ‘˜
ğ‘˜ ğ‘˜
2ğ‘ âˆ’1 < ğ‘›2ğ‘šğ‘ğ‘¥ â©½ 2ğ‘ .	(2)

Post-GSPAI stage: Assemble the preconditioner ğ‘€ in the com-
pressed sparse column (CSC) storage format.
Based on the sparsity pattern of the preconditioner, when the thread allocation strategy with the constant thread-group size is more suit- able for computing the preconditioner, the thread-adaptive allocation strategy (First strategy) proposed in [15] is adopted; otherwise, the thread-adaptive allocation strategy with the nonconstant thread-group size (Second strategy) proposed in [16] is utilized. Given a matrix, should we use the first strategy or the second strategy? Here we present
Here ğ‘›ğ‘¡ is the number of threads per block, and ğ‘ threads are grouped
into a thread group.
group size: First, for each ğ‘›2ğ‘˜, ğ‘˜ = 1, 2, â€¦ , ğ‘›, the number of threads ğ‘ğ‘˜ Thread-adaptive allocation strategy with the nonconstant thread- assigned to the ğ‘˜th column of the preconditioner is computed by the
following formula:
ğ‘ğ‘˜ = min(2ğ‘ , ğ‘›ğ‘¡),	(3)
ğ‘ .ğ‘¡.

a selection method, whose main procedure is shown in Fig. 2.
Let us illustrate the selection method in Fig. 2 by apache2. For
2ğ‘ âˆ’1 < ğ‘›2ğ‘˜
â©½ 2ğ‘ .	(4)

apache2, we have n2max = 8 and n2avg = 6.74. Obviously, n2max, n2avg âˆˆ (22, 23], and ğ›¼ = ğ›½ = 3. Based on the selection method in
Fig. 2, the first strategy is chosen.
Second, all ğ‘ğ‘˜ values are sorted in descending order. Finally, the
thread-group size of each block is assigned by the procedure shown
in Fig. 3.





/ig. 3. Main procedure of assigning the thread-group size.


Table 1
Arrays used in GSPAI-Opt.




Finally, we allocate global memory for arrays in Table 1, and RCol, BCol, and WSize values are transferred to the GPU global memory if the second strategy is applied.

Compute-GSPAI stage
/inding indices: This part is to find indices ğ½ and ğ¼ by the con- stant/nonconstant thread-group size.
Finding ğ½ and ğ¼ by the constant thread-group size: In this case, the thread-group size that is used to find ğ½ and ğ¼ is same in all blocks. For the kernel that finds ğ½ , the threads inside each thread group read one column of the sparsity pattern ğ‘€ in parallel and store them to one subset of ğ½ . And then on this basis of ğ½ , we implement the construction of ğ¼ . We establish a decision tree to find ğ¼ based on the GPU feature
parameters. Utilizing the decision tree, an optimized kernel for finding
ğ¼ is obtained for any given ğ‘›2ğ‘šğ‘ğ‘¥ and ğ‘›1ğ‘šğ‘ğ‘¥. Assume that the threads
a segment of the decision tree for finding ğ¼ . Here ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘†ğ‘–ğ‘§ğ‘’  = per block are 256 and NIVIDA GTX1070 GPU is used, Fig. 4 shows
number of columns of the preconditioner computed in a thread block
Ã— upper boundary closest to ğ‘›1ğ‘šğ‘ğ‘¥. For example, when ğ‘›1ğ‘šğ‘ğ‘¥ â©½ 8,
ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘†ğ‘–ğ‘§ğ‘’ = 32 Ã— 8 and cuFindIBySharedMemory kernel with shared
each thread group finds one subset of ğ¼ , e.g., ğ¼ğ‘˜, which mainly includes memory of 256 size is used. In the cuFindIBySharedMemory kernel,
indices of the first column referenced in one subset of ğ½ , e.g., ğ½ğ‘˜, to the following steps. First, the threads in the thread group load the row shared memory ğ‘ ğ¼ . Second, the index vectors of successive columns referenced by ğ½ğ‘˜ are compared in parallel with values in ğ‘ ğ¼ and new indices are appended to ğ‘ ğ¼ by utilizing the atomic operations. Third, inside the thread group, the indices of ğ‘ ğ¼ are sorted in ascending order in parallel. Finally, the indices of ğ‘ ğ¼ are copied to ğ¼ğ‘˜. cuFindI kernel is
similar to cuFindIBySharedMemory kernel except that the operations
are executed on global memory instead of shared memory.
Finding ğ½ and ğ¼ by the nonconstant thread-group size: The thread-group size of finding ğ½ and ğ¼ is same in a block while it is usually different for different blocks. For the kernel that finds ğ½ , the
pattern ğ‘€ in parallel and store them to one subset of ğ½ . The main threads inside each thread group read one column of the sparsity procedure of the kernel that finds ğ¼ is as same as that in [16]. Each thread group is assigned to find one subset of ğ¼ , e.g., ğ¼ğ‘˜, which includes
the thread-group size ğ‘¤ğ‘ğ‘Ÿğ‘ğ‘†ğ‘–ğ‘§ğ‘’. In the second stage, the row indices the following three stages. In the first stage, the thread group obtains













/ig. 4. A segment of the decision tree of using constant threads to find ğ¼ .



of the first column referenced in ğ½ğ‘˜ are first loaded into ğ¼ğ‘˜, and the row index vectors of successive columns that are referenced by ğ½ğ‘˜ are calculated in parallel with values in ğ¼ğ‘˜, and the new indices are appended to ğ¼ğ‘˜ by utilizing the atomic operations. In the third stage, the indices in ğ¼ğ‘˜ are sorted in ascending order in parallel.
Constructing the local submatrix: Using ğ½ and ğ¼ obtained above,
the local submatrix set, ğ´Ì‚, is computed by the constant/nonconstant
thread-group size.
size: Each thread group is assigned to compute one subset of ğ´Ì‚, e.g., ğ´Ì‚ , (1) Constructing the local submatrix by the constant thread-group
parameters, we establish a decision tree for constructing ğ´Ì‚. For any and all thread groups are the same size. Based on the GPU feature given ğ‘›2ğ‘šğ‘ğ‘¥ and ğ‘›1ğ‘šğ‘ğ‘¥, an optimized kernel for constructing ğ´Ì‚ is
achieved by using the decision tree. For example, on NIVIDA GTX1070
segment of the decision tree for constructing ğ´Ì‚. When 4 < ğ‘›2ğ‘šğ‘ğ‘¥ â©½ 8, GPU, assume that the threads per block are 256, Fig. 5 shows a corresponding to different ğ‘›1ğ‘šğ‘ğ‘¥, cuComputeTildeABySharedMemory kernel with shared memory of ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘†ğ‘–ğ‘§ğ‘’ size and cuComputeTildeA
kernel with non shared memory are selected. The main procedure of cuComputeTildeABySharedMemory kernel is listed as follows. For
the thread group that calculates ğ´Ì‚ , all threads in the thread group
first read values in ğ¼ğ‘˜ into shared memory ğ‘ ğ¼ in parallel, and ğ´Ì‚ is
by ğ½ğ‘˜ and matching them to ğ‘ ğ¼ in parallel. cuComputeTildeA kernel established on the global memory by loading columns that are indexed is similar to cuComputeTildeABySharedMemory kernel except that ğ¼ is
executed on global memory instead of shared memory.
(2) Constructing the local submatrix by the nonconstant thread-
subset of ğ´Ì‚, e.g., ğ´Ì‚ , and the thread-group size is same in a block but it group size: In this case, each thread group is assigned to calculate one
that  constructs  ğ´Ì‚  is  as  same  as  that  in  [16]. can be different for different block. The main procedure of the kernel
Decomposing the local submatrix into ğ‘„ğ‘…: This part is used to
decompose the local submatrix into ğ‘„ğ‘… by the constant/nonconstant
thread-group size.
Decomposing the local submatrix into ğ‘„ğ‘… by the constant
submatrix into ğ‘„ğ‘… is same in all blocks. Based on the GPU feature thread-group size: The thread-group size of decomposing the local
submatrix into ğ‘„ğ‘…. For example, on NIVIDA GTX1070 GPU, assume parameters, we establish a decision tree for decomposing the local
decision tree for decomposing the local submatrix into ğ‘„ğ‘…. When that the threads per block are 256, Fig. 6 shows a segment of the
4 < ğ‘›2ğ‘šğ‘ğ‘¥ â©½ 8, two shared memories ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘… and ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘„ are













/ig. 5. A segment of the decision tree of using constant threads to construct ğ´Ì‚.



local          submatrix          into          ğ‘„ğ‘…. /ig. 6. A segment of the decision tree of using constant threads to decompose the

into                                    ğ‘„ğ‘…. /ig. 7. Decision tree of using nonconstant threads to decompose the local submatrix


/ig. 8. Decision tree of using constant threads to solve the upper triangular linear system.



in Fig. 7 when the threads per block are 256. Obviously, utilizing
corresponding to shared memory of ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘… size or cuSortedQR kernel the decision tree, an optimized kernel cuSortedQRByRSharedMemory is chosen for a given ğ‘›2ğ‘šğ‘ğ‘¥ value. The main procedure of cuSort-
edQRByRSharedMemory kernel is as same as that in [16]. cuSortedQR kernel is similar to cuSortedQRByRSharedMemory kernel except that
the shared memory ğ‘ ğ‘… is not utilized.
Solving the upper triangular linear system: The values of ğ‘šÌ‚ğ‘˜ =
ğ‘…âˆ’1ğ‘„ğ‘‡ ğ‘’Ì‚ğ‘˜ are computed by the constant/nonconstant thread-group size.

ğ‘˜	ğ‘˜
Solving the upper triangular linear system by the constant

used in the optimized kernel. Here the size of ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘„ is related to
ğ‘›1ğ‘šğ‘ğ‘¥. In the cuQRByQRSharedMemory kernel, each thread group is responsible for one ğ‘„ğ‘… decomposition. In a thread group, the local submatrix, e.g., ğ´Ì‚ , is decomposed into ğ‘„ğ‘… by the following four steps at each iteration ğ‘–. In the first step, the threads read the ğ‘–th column of ğ‘„ğ‘˜ into shared memory ğ‘ ğ‘„ in parallel. In the second step, the ğ‘–th row of the upper triangle matrix ğ‘…ğ‘˜ are computed in parallel and are put into shared memory ğ‘ ğ‘…. In the third step, the column ğ‘– of ğ‘„ğ‘˜ and ğ‘ ğ‘„ are concurrently normalized, and the projection factors ğ‘…ğ‘˜ and ğ‘ ğ‘… are calculated. In the fourth step, the values of all columns of ğ‘„ğ‘˜ are updated by using shared memory ğ‘ ğ‘„ and ğ‘ ğ‘… in parallel.
kernel except the shared memory ğ‘ ğ‘„ is not utilized. cuQRByRSharedMemory kernel is similar to cuQRByQRSharedMemory
Decomposing the local submatrix into ğ‘„ğ‘… by the nonconstant
submatrix into ğ‘„ğ‘… is same in a block while it is usually different thread-group size: The thread-group size of decomposing the local
local submatrix into ğ‘„ğ‘…. For example, on NIVIDA GTX1070 GPU, the for different blocks. We establish a decision tree for decomposing the decision tree for decomposing the local submatrix into ğ‘„ğ‘… is shown
thread-group size: Each thread group computes one subset of ğ‘šÌ‚ by
solving an upper triangular linear system, and the thread-group size
is same in all blocks. In this case, assume that the threads per block are 256, the decision tree for solving the upper triangular linear system
is shown in Fig. 8. For any given ğ‘›2ğ‘šğ‘ğ‘¥ value, an optimized kernel,
group size of ğ‘¤ğ‘ğ‘Ÿğ‘ğ‘†ğ‘–ğ‘§ğ‘’, is chosen. In the cuSolverBySharedMemory cuSolverBySharedMemory with shared memory of 256 size and thread- kernel, each thread group calculates a subset of ğ‘šÌ‚, e.g., ğ‘šÌ‚ğ‘˜, and its procedure includes two steps. First, Calculate ğ‘„ğ‘‡ ğ‘’Ì‚ğ‘˜ in parallel and save the result to the shared memory ğ‘¥ğ¸. Second, the values of ğ‘šÌ‚ğ‘˜ are obtained by solving the upper triangular linear system ğ‘…ğ‘˜ğ‘šÌ‚ğ‘˜ = ğ‘¥ğ¸, in
parallel.
(2) Solving the upper triangular linear system by the nonconstant thread-group size: Each thread group is responsible for obtaining a
subset of ğ‘šÌ‚ by solving an upper triangular linear system, and the thread-
group size is same inside a block but it can be different for different
blocks. A decision tree is established to solve the upper triangular linear system. For example, Fig. 9 lists the decision tree for solving the upper triangular linear system on NIVIDA GTX1070 GPU. For any




















/ig. 9. Decision tree of using nonconstant threads to solve the upper triangular linear system.

Table 2
Overview of GPUs.



given ğ‘›2ğ‘šğ‘ğ‘¥ value, we always choose an optimized kernel, which may be a cuSortedSolverBySharedMemory kernel that uses shared memory
of ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘†ğ‘–ğ‘§ğ‘’ size, or a cuSortedSolver kernel. The main procedure
of cuSortedSolverBySharedMemory kernel is as same as that in [16].
kernel except that the shared memory ğ‘¥ğ¸ is not used. cuSortedSolver kernel is similar to cuSortedSolverBySharedMemory

Post-GSPAI stage

In the Post-GSPAI stage, the preconditioner ğ‘€ is assembled in the CSC storage format which contains three arrays of ğ‘€ğ‘ƒ ğ‘¡ğ‘Ÿ, ğ‘€ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ and
ğ‘€ğ·ğ‘ğ‘¡ğ‘. Fig. 10 illustrates the procedure of assembling these arrays. First, MPtr is assembled utilizing jPTR. Second, ğ‘€ğ·ğ‘ğ‘¡ğ‘ and ğ‘€ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ are assembled using ğ‘šÌ‚ and ğ½ . In order to reduce the cost of array transfer,
thread group is responsible for generating one ğ‘šÌ‚ğ‘˜ to ğ‘€ğ·ğ‘ğ‘¡ğ‘ and one we assemble all arrays mentioned above on the GPU memory, and each
ğ½ğ‘˜ to ğ‘€ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥.
Experimental results

In this section, we take two NVIDIA GPUs (GTX1070 and A40) shown in Table 2 to evaluate the performance of GSPAI-Opt. The test matrices are listed in Table 3, which are chosen from the SuiteSparse Matrix Collection [17], and have been widely used in some publica- tions [14â€“16]. Table 3 summarizes the information of the sparse matri- ces, including the name, kind, number of rows, total number of nonze- ros, average number of nonzeros, maximum number of nonzero entries
/ig. 10. Assemble ğ‘€ .



of columns, and minimum number of nonzero entries of columns. The matrices in Table 3 are chosen due to the following reasons. The matri- ces such as cbuckle, ASIC_320ks, power9, and Fault_639 are chosen to test whether the second strategy is chosen when the nonzero number of
each column of the preconditioner has significant difference (ğ›¼ âˆ’ğ›½ â©¾ 3).
The matrices such as 2cubes_sphere, offshore, apache2, G3_circuit are
chosen to test whether the first strategy is chosen when the difference in the nonzero number of each column of the preconditioner is small
(ğ›¼ = ğ›½). The matrices such as msdoor and thermal2 are chosen to
test whether the predicted strategy is well matched with the measured
the preconditioner is large but not significant (1 â©½ ğ›¼âˆ’ğ›½ < 3). The source strategy when the difference in the nonzero number of each column of
codes are compiled and executed using the CUDA toolkit 11.1 [18].

Accuracy of selection method

We take GTX1070 to test the accuracy of the proposed selection method of using the first strategy (denoted by S1) or the second strategy (denoted by S2). The sparse pattern of the preconditioner is a priori, so we test the accuracy in two popular patterns [14â€“16],
(ğ¸ + |ğ´|)ğ‘˜, ğ‘˜ = 1, 2. The matrices in Table 3 are used as the test
matrices. For all test matrices, both the optimal strategy predicted by
tests are shown in Table 4. Note that if |ğ‘¡1 âˆ’ ğ‘¡2|âˆ• max(ğ‘¡1, ğ‘¡2) â©½ 0.05, both the proposed selection method and the strategy obtained from actual
otherwise, the strategy corresponding to min(ğ‘¡1, ğ‘¡2) is chosen as the S1 and S2 can be considered as the measured optimization strategy; measured optimization one. Here ğ‘¡1 and ğ‘¡2 are the time of constructing
the preconditioner using S1 and S2, respectively. We can observe that for the two sparsity patterns, the estimated and measured optimal strategies are matched very well for the test cases. This verifies good accuracy of our proposed selection method.

Performance comparison

take the sparsity pattern (ğ¸ + |ğ´|) to compare it with a static SPAI In order to test the effectiveness of our proposed GSPAI-Opt, we
preconditioning algorithm in ViennaCL (denoted by SSPAI-VCL) [13], and two recent SPAI preconditioning algorithms SPAI-Adaptive [15] and GSPAI-Adaptive [16] on GTX1070 and A40, and their comparison results are listed in Tables 5 and 6, respectively. Moreover, since SSPAI-
VCL cannot be suitable for the sparsity pattern (ğ¸ + |ğ´|)2, only the
comparison results of SPAI-Adaptive, GSPAI-Adaptive and GSPAI-Opt
on two GPUs are shown in Table 7. In each table, for any matrix and


Table 3
Descriptions of test matrices.



Table 4
Predicted and measured sparsity pattern.
Table 6
Comparison of four algorithms with (ğ¸ + |ğ´|) on A40.

N/A	3.989	1.485	1.114

msdoor	S1	S1/S2	S1	S1/S2 thermal2	S1	S1	S1	S1 Fault_639	S2	S2	S2	S2
Table 5
Comparison of four algorithms with (ğ¸ + |ğ´|) on GTX1070.
Matrix	SSPAI-V	SPAI-A	GSPAI-A	GSPAI-Opt N/A	7.976	2.046	1.815





offshore



ASIC_320ks



cbuckle




2cubes_sphere




offshore




ASIC_320ks




apache2
N/A	8.338	2.402	2.163
7.278	0.833	0.697	0.539
0.025	0.300	0.296	0.294

7.303	1.133	0.993	0.833
20.468	2.177	2.052	1.380


20.521	2.500	2.376	1.707
N/A	5.000	1.398	0.846
N/A	0.347	0.342	0.339
N/A	5.347	1.740	1.185
5.722	0.238	0.328	0.222
13.685	3.821	3.902	3.807
/	0.148	0.170	0.148
G3_circuit









msdoor



>10 000	472	472	472









N/A	656	656	656




SPAI-Adaptive + GPUPBICGSTAB, GSPAI-Adaptive + GPUPBICGSTAB
and GSPAI-Opt + GPUPBICGSTAB are denoted by SSPAI-V, SPAI-A,
GSPAI-A and GSPAI-Opt, respectively.
From Tables 5 and 6, we can see that as compared to SSPAI-VCL on two GPUs, for some matrices such as thermal2 and G3_circuit, GPUPBICGSTAB with SSPAI-VCL cannot converge in 10,000 iterations




any given preconditioner, the first two rows are the execution time of the preconditioning algorithm and GPUPBICGSTAB, respectively, and the third row is the iteration number of GPUPBICGSTAB, and the fourth row is the total of the first two rows; if the iteration number of GPUP- BICGSTAB is more than 10,000, we record the number of iterations
â€˜â€˜>10 000â€™â€™ in the third row, and the other rows that record the time are
represented by â€˜â€˜/â€™â€™; if the out-of-memory error for GPUPBICGSTAB is
(ğ‘ ), and the minimum value of the fourth row for each matrix is marked encountered, all rows will be denoted by â€˜â€˜N/Aâ€™â€™. The time unit is second in the red font. For the convenience, SSPAI-VCL + GPUPBICGSTAB,
that of GSPAI-Opt and GPUPBICGSTAB. This verifies that GSPAI-Opt is much better than SSPAI-VCL for all test matrices. Compared with SPAI-Adaptive and GSPAI-Adaptive, GSPAI-Opt does not only have smaller execution time, but also the total time of GSPAI-Opt and GPUP- BICGSTAB is much less than that of SPAI-Adaptive and GPUPBICGSTAB and that of GSPAI-Adaptive and GPUPBICGSTAB for all test cases. Fig. 11 shows the execution time ratios of SPAI-Adaptive to GSPAI-Opt and GSPAI-Adaptive to GSPAI-Opt on two GPUs. On the GTX1070 GPU, the minimum and maximum execution time ratios of SPAI-Adaptive to GSPAI-Opt are roughly 1.0 and 4.95, respectively, and the average ratio is roughly 2.62; the minimum and maximum execution time ratios of GSPAI-Adaptive to GSPAI-Opt are roughly 1.13 and 3.73, respectively, and the average ratio is roughly 1.57. On the A40 GPU, the minimum and maximum execution time ratios of SPAI-Adaptive to GSPAI-Opt



Table 7
Comparison of three algorithms with (ğ¸ + |ğ´|)2 .











































msdoor









Fault_639




GSPAI-Opt    for    ğ¸   +   |ğ´|    on    two    GPUs. /ig. 11. Execution time ratios of SPAI-Adaptive vs GSPAI-Opt and GSPAI-Adaptive vs



are roughly 1.12 and 9, respectively, and the average ratio is roughly
2.79; the minimum and maximum execution time ratios of GSPAI- Adaptive to GSPAI-Opt are roughly 1.21 and 4.83, respectively, and the average ratio is roughly 2.03. These observations verify that GSPAI-Opt outperforms SPAI-Adaptive and GSPAI-Adaptive.




GSPAI-Opt    for    (ğ¸   +  |ğ´|)2     on    two    GPUs. /ig. 12. Execution time ratios of SPAI-Adaptive vs GSPAI-Opt and GSPAI-Adaptive vs

For the sparsity pattern of (ğ¸ + |ğ´|)2, from Table 7, we can observe that comparing with SPAI-Adaptive and GSPAI-Adaptive, we can draw
the same conclusion as the sparsity pattern of (ğ¸ + |ğ´|) for GSPAI-Opt.
GSPAI-Opt is much better than SPAI-Adaptive and GSPAI-Adaptive.
This can also be confirmed from Fig. 12. On the GTX1070 GPU, the minimum and maximum execution time ratios of SPAI-Adaptive to GSPAI-Opt are roughly 1.99 and 6.02, respectively, and the average ratio is roughly 2.59; the minimum and maximum execution time ratios of GSPAI-Adaptive to GSPAI-Opt are roughly 1.01 and 2, respectively, and the average ratio is roughly 1.28. On the A40 GPU, the minimum and maximum execution time ratios of SPAI-Adaptive to GSPAI-Opt are roughly 1.14 and 5.45, respectively, and the average ratio is roughly
2.55; the minimum and maximum execution time ratios of GSPAI- Adaptive to GSPAI-Opt are roughly 1.05 and 2.5, respectively, and the average ratio is roughly 1.39.

Conclusion

In this study, we propose an optimized sparse approximate inverse preconditioners on GPU called GSPAI-Opt. In the proposed GSPAI- Opt, for any given sparsity pattern of the preconditioner, a selection strategy is presented to determine the size of the thread group for each column of the preconditioner. Furthermore, no matter which strategy we choose, each column of the preconditioner is performed in parallel within a thread group. The experimental results verify that GSPAI-Opt can well fuse the advantages of SPAI-Adaptive and GSPAI-Adaptive and is highly effective.
Next, we will further do research in this field, and apply the proposed GSPAI-Opt to more practical problems.

Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

References

CUDA C Programming guide 1.0, 2007, https://developer.nvidia.com/content/
cuda-10.
X. Chu, J. Gao, B. Sheng, Efficient concurrent L1-minimization solvers on GPUs, Comput. Syst. Sci. Eng. 38 (3) (2021) 305â€“320.
J. Gao, Y. Xia, R. Yin, G. He, Adaptive diagonal sparse matrixvector multiplication on GPU, J. Parallel Distrib. Comput. 157 (2021) 287â€“302.
K. Li, W. Yang, K. Li, A hybrid parallel solving algorithm on GPU for quasi- tridiagonal system of linear equations, IEEE Trans. Parallel Distrib. 27 (10) (2016) 2795â€“2808.
S.C. Rennich, D. Stosic, T.A. Davis, Accelerating sparse cholesky factorization on GPUs, Parallel Comput. 59 (2016) 140â€“150.



H. Anzt, M. Gates, J. Dongarra, M. Kreutzer, G. Wellein, M. Kohler, Preconditioned Krylov solvers on GPUs, Parallel Comput. 68 (2017) 32â€“44.
E. Chow, A. Patel, Fine-grained parallel incomplete LU factorization, SIAM J. Sci. Comput. 37 (2) (2015) C169â€“C193.
J. Gao, X. Chu, X. Wu, J. Wang, G. He, Parallel dynamic sparse approximate inverse preconditioning algorithm on GPU, IEEE Trans. Parallel Distrib. 33 (12) (2022) 4723â€“4737.
Y. Saad, M.H. Schultz, GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems, SIAM J. Sci. Stat. Comput. 7 (3) (1986) 856â€“869.
H.A. van der Vorst, Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of non-symmetirc linear systems, SIAM J. Sci. Stat. Comput. 12
(3) (1992) 631â€“644.
E. Chow, A priori sparsity patterns for parallel sparse approximate inverse preconditioners, SIAM J. Sci. Comput. 21 (5) (2000) 1804â€“1822.
J. Gao, K. Wu, Y. Wang, P. Qi, G. He, GPU-accelerated preconditioned GMRES method for two-dimensional Maxwellâ€™s equations, Int. J. Comput. Math. 94 (10) (2017) 2122â€“2144.
K. Rupp, R. Tillet, F. Rudolf, et al., ViennaCL-linear algebra library for multi- and many-core architectures, SIAM J. Sci. Comput. 38 (5) (2016) S412â€“S439.
M.M. Dehnavi, D.M. Fernandez, J.L. Gaudiot, Parallel sparse approximate inverse preconditioning on graphic processing units, IEEE Trans. Parallel Distrib. 24 (9) (2013) 1852â€“1861.
G. He, R. Yin, J. Gao, An efficient sparse approximate inverse preconditioning algorithm on GPU, Concurr. Comput.-Pract. Exp. 32 (7) (2020) e5598, http:
//dx.doi.org/10.1002/cpe.5598.
J. Gao, Q. Chen, G. He, A thread-adaptive sparse approximate inverse pre- conditioning algorithm on multi-GPUs, Parallel Comput. 101 (2021) 102724, http://dx.doi.org/10.1016/j.parco.2020.102724.
T.A. Davis, Y. Hu, The university of florida sparse matrix collection, ACM Trans. Math. Software 38 (1) (2011) 1â€“25.
CUDA C Programming guide 11.1, 2021, http://docs.nvidia.com/cuda/cuda-c- programming-guide.
