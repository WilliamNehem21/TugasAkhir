Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 296 (2013) 183–197
www.elsevier.com/locate/entcs

Markov Chain Simulation with Fewer Random Samples
Dimitrios Milios 1 Stephen Gilmore 2
School of Informatics University of Edinburgh Edinburgh, UK

Abstract
We propose an accelerated CTMC simulation method that is exact in the sense that it produces all of the transitions involved. We call our method Trajectory Sampling Simulation as it samples from the distribution of state sequences and the distribution of time given some particular sequence. Sampling from the trajectory space rather than the transition space means that we need to generate fewer random numbers, which is an operation that is typically computationally expensive. Sampling from the time distribution involves approximating the exponential distributions that govern the sojourn times with a geometric distribution. A proper selection for the approximation parameters can ensure that the stochastic process simulated is almost identical to the simulation of the original Markov chain. Our approach does not depend on the properties of the system and it can be used as an alternative to more efficient approaches when those are not applicable.
Keywords: Markov chain, simulation, trajectory, random variable

Introduction
Continuous Time Markov Chains (CTMCs) have been used for many years for describing systems that exhibit stochastic behaviour. Stochastic simulation is a traditional approach for exploring the transient and steady-state properties of mas- sive CTMCs, since it does not require an explicit representation of the state-space. There are models however, such as bio-chemical reaction networks, whose state- space is too large even for this kind of approach. The standard CTMC simulation approach is known as the direct method (DM) [9]. In the case of very large models, it has a high computational cost because it simulates every possible transition hap- pening. Several accelerated methods have been proposed that are either exact or approximate. Exact methods typically involve optimisations over the standard al- gorithm, such as the next reaction method [8], the optimised DM [5], the logarithmic

1 Email: D.Milios@sms.ed.ac.uk
2 Email: Stephen.Gilmore@ed.ac.uk

1571-0661 © 2013 Elsevier B.V. Open access under CC BY-NC-ND license.
http://dx.doi.org/10.1016/j.entcs.2013.07.012

DM [17] and ER-leap [19]. Most of these approaches involve the use of appropriate data structures in order to generate the simulation events efficiently. For example, the optimised DM makes use of a dependency graph to avoid recalculating rates that remain unchanged.
An approximate simulation method tries to skip some of the simulation events, resulting in a significantly faster process when compared to exact methods. For example, τ-leaping [10] advances time by a pre-selected τ , during which many tran- sitions may occur. Similarly in R-leaping [1], stochastic simulation was accelerated by advancing by a predefined number of transition firings. K-leap [3] is an approach that also advances time by a specified number of events. All these methods assume that a single transition causes only small changes to the state of the system. Other approaches such as [20] and [4] make use of the notion of time-scale separation. It is assumed that the model can be partitioned to two sub-systems: slow and fast. The behaviour of the fast sub-system is approximated, while only the slow sub-system is simulated. Such assumptions may not always hold for arbitrary models, mean- ing that either significant error is introduced or the approximate method fails to accelerate the simulation process.
The Trajectory Sampling Simulation (TSS) algorithm that we propose is a mod- ification of the DM that can be characterised as almost exact, in the sense that it can be arbitrarily precise. In the case of the DM, each step requires sampling from two distributions: the state distribution and the time distribution, both conditioned on the current state. In a similar way, TSS involves sampling from the distribution of state sequences. This reduces the number of random samples generated, a fact that implies a faster simulation algorithm. The algorithm is still exact, since no transitions are skipped. The same approach is extended to sample from the time distribution given some particular sequence. That is achieved by approximating the exponentially distributed sojourn times with a discrete random variable. Time discretisation allows us to consider the time distribution as a discrete state Markov chain, and therefore employ the TSS technique. This modification essentially ren- ders our approach approximate, as part of the stochastic behaviour of the CTMC is suppressed. However, we have proved that our method in the limit converges to the solution of the original process, a fact that explains the term ‘almost exact’ used earlier in the paragraph. We show that an appropriate selection of the approxima- tion parameters can result in a behaviour very close to the original CTMC, and in a reasonable speedup at the same time. Our implementation is based on the optimised DM (ODM), hence the ODM is used as a baseline for efficiency comparisons.
Our approach is related to K-skip method I in [2], or simply K-skip. While their strategy for sampling from the state sequence distribution is similar, their approach for sampling from the time distribution is different. In order to reduce the random samples that determine the sojourn times, they approximate the sum of k exponen- tial random variables with a Gamma distribution, assuming that the exit rates are similar for subsequent states. This assumption is reasonable for many bio-chemical systems, however it may introduce errors for some models as we demonstrate in the experiments’ section, while our approach can be generalised for arbitrary models.

We have implemented K-skip following its description in the original paper, in or- der to produce some comparative results. The error parameter that we have used for K-skip is 0.01, which is the smallest value used in the original work. We also highlight some computational issues not considered in [2] that arise from the fact that one random number is used to produce an entire trajectory.
In Section 2, we introduce some concepts used throughout the paper. Sections 3 and 4 contain the theoretical details of our work. Some implementation issues are discussed in Section 5. Experimental results are presented in Section 6. Finally, we summarise the conclusions in Section 7.
Preliminaries
A CTMC can be represented as a triple (S, Q, π0), where S is a finite set of states, Q ∈ R|S|×|S| is a generator matrix, and π0 is the initial probability distribution over S. Each s ∈ S is associated with an exponentially distributed random variable Ls ~ Exp(λs), where λs = s′/=Qs ss′ is the rate of exiting state s. The jump chain of a CTMC is discrete-time Markov with probability matrix P where:

,⎨0,	s /= sj and Qs =0	Σ

Pss′ =
0,	s = sj
,
,	where Qs = and Qs /=0 
Qss′	(1)
s′/=s

,1,	s = sj and Qs =0 
A transition in a CTMC is associated with two random variables that depend on the current state s: Xs that determines the next state and Ls that determines the amount of time spent in s. The DM involves sampling from Xs and Ls to generate the next event. The distribution of Xs is categorical conditional on s, and its probability mass function is given by the s-th row of the jump matrix P . We assume an ordering of states such as s < sj, if s corresponds to a row of the transition matrix with a smaller index than sj. If sk—1 is the state of the system after k — 1 transitions, sampling from Xsk−1 involves using a uniform sample U ~ C (0, 1) and selecting the next state sk with probability:
Pr(Xsk−1 = sk)= Pr(ask < U ≤ bsk )	(2)
where bsk is the cumulative probability of state sk given sk—1, while ask is the cumulative probability of the state that precedes sk in the ordering:

ask = Σ
sk′<sk
Psk−1sk′	and	bsk = Σ
sk′≤sk

Psk−1sk′	(3)

In order to sample from Lsk−1 , we have to draw a new uniform sample U ~ C (0, 1) and calculate the time tsk−1 spent in sk—1 as follows:
ln(1 — U )

tsk−1 = —
λsk−1
(4)

From a trajectory point of view, the random variables are different. A CTMC trajectory involves a sequence of states and a sequence of positive numbers that represent the amount of time spent in each state. Let Sk be a collection that stands for the family of state sequences of length k. Therefore, we define XEk as the variable that represents the k-length sequence distribution. Given some particular sequence of states, namely s0:k, its duration is represented by the Ls0:k random variable. Ideally, we would like to directly sample from XEk and Ls0:k to determine the state history and the time of the system after k transitions. Exact stochastic simulation algorithms actually sample from those distributions implicitly by advancing by one state at each event. In the sections that follow, we discuss how we can directly sample from the trajectory-related distributions, XEk and Ls0:k .

Sampling from the State Sequence Distribution
The sampling from the state sequence distribution discussed in this section can be applied to both discrete and continuous time processes. Without loss of generality, we can assume that there is one initial state in some Markov chain. This will be the root of a tree whose paths represent all the possible state sequences. Each path of a tree with k levels corresponds to a sequence of k + 1 states or k transitions. Then, the probability of a path can be defined as the product of the transitions involved:

Pr(XEk = s0:k)=   Psn−1sn	(5)
n=1
In fact, XEk follows a categorical distribution with |Sk| parameters. Sampling from the sequence distribution requires us to compute its cumulative distribution func- tion, which means that we have to define an ordering of the possible sequences.

Definition 3.1 [Lexicographical Ordering of Sequences] Given an ordering of states, we define an ordering of sequences such as s0:k < s0:kj if one of the fol- lowing holds:
s0:k—1 < s0:k—1j or
s0:k—1 = s0:k—1j and sk < skj

Therefore, we can calculate the cumulative probabilities for the sequences. Given a uniform random variable U ~ C (0, 1), we can choose directly a sample from the sequence space. The relationship between U and XEk is shown in the following equation:
Pr(XEk = s0:k)= Pr(as0:k < U ≤ bs0:k )	(6)
The term bs0:k is defined as the cumulative probability of the s0:k sequence. In the same way, as0:k will be the cumulative probability of the sequence that precedes s0:k

according to the ordering. More formally:


as0:k
=	Σ
s0:k′<s0:k
Pr(s0:kj)


bs0:k
=	Σ
s0:k′≤s0:k
Pr(s

0:k
j) = a


s0:k
+ Pr(s

0:k
(7)
)

Although sampling from the sequence distribution is well-defined, it cannot be practically applied in its current form. The number of possible sequences grows exponentially as k increases, a fact that renders Equations (5) and (7) computa- tionally expensive. What we can do instead, is to draw a sample from U ~ C (0, 1) that determines the sequence, and recursively generate the transitions involved. A recursive definition for the cumulative sequence probabilities would be useful for this task. Using Definition 3.1, the cumulative probability of the sequence that precedes s0:k can also be written recursively as follows:

as0:k = Pr(s0:k—1) Σ
sk′<sk

Psk−1sk′ + as0:k−1	(8)

Since the uniformly distributed sample U determines the entire k-length se- quence, it follows that it also determines all of the k transitions involved. In the DM however, the sequence of the transitions would have been determined by a se- quence of uniform samples Un ~ C (0, 1), with 0 ≤ n ≤ k. Thus, the sequence Un is equivalent to the sample U for the state sequence. We shall next try to define the last sample Uk in terms of U , which gives rise to the following theorem:
Theorem 3.2 If U ~ C (0, 1) is used to draw a state sequence sample s0:k, then sk
is determined by:

U =	U — as0:k−1
(9)

k	bs

0:k−1
— as

0:k−1

Proof. We have to show that ask < Uk ≤ bsk , which means that Uk will select the state sk, according to Equation (2). Since s0:k was selected, Equation (6) implies:
as0:k < U ≤ bs0:k ⇔

U > Pr(s0:k—1) Σ
sk′<sk
and	U ≤ Pr(s0:k—1) Σ
sk′<sk

Psk−1sk′ + as0:k−1

Psk−1sk′ + as0:k−1 + Pr(s0:k)

We subtract as0:k−1 from all terms, and divide everything by Pr(s0:k—1):

Pr(s0:k—1) Σ
sk′<sk
Psk−1sk′ < U — as0:k−1 ≤ Pr(s0:k—1) Σ
sk′<sk
Psk−1sk′ + Pr(s0:k)

Σ	U — as0:k−1	Σ
 Pr(s0:k) 


sk′<sk
Psk−1sk′ <
Pr(s
≤
0:k—1)

sk′<sk
Psk−1sk′ + Pr(s
0:k—1)

We substitute  Pr(s0:k )  with Ps
0:k  1

k−1sk
and Pr(s0:k—1) with bs


0:k−1
as

0:k−1 :



Σ Psk−1sk′ < b
U — as0:k−1
— a
≤ Σ Psk−1sk′ + Psk−1sk

sk′<sk
s0:k−1
s0:k−1
sk′<sk

Σ Psk−1sk′ < b
U — as0:k−1
— a
≤ Σ Psk−1sk′

sk′<sk
s0:k−1
s0:k−1
sk′≤sk

which eventually yields:
ask < Uk ≤ bsk
2
Theorem 3.2 can be used to calculate any of the Un samples that determine the transitions by simply considering k = n, with n > 1. For the special case where k = 1, the sequence probabilities will be equal to the transition probabilities of the first step. We could then calculate the uniform sample Uk+1 needed for the next step and recursively update as0:k+1 and bs0:k+1 to get the new cumulative sequence probabilities using Equation (8). This strategy might not be optimal though, as it requires keeping track of two cumulative probabilities. A cleaner and more efficient solution would be to write Uk in terms of the previous uniform sample Uk—1.
Theorem 3.3 If Uk—1 ~ C (0, 1) is used to draw a state sample sk—1, then sk is determined by

U = Uk—1 — ask−1
(10)

k	bs

k−1
as

k−1


Proof. Given a uniform sample U that determines the sequence, the samples Uk and Uk—1 can be written as specified in (9). If we solve w.r.t. U in both cases, we obtain the following equality:
UkPr(s0:k—1)+ as0:k−1 = Uk—1Pr(s0:k—2)+ as0:k−2


which yields:

U  = Uk—1Pr(s0:k—2) — as0:k−1 — as0:k−2

(11)

k	Pr(s0:k—1)
Pr(s0:k—1)

Using (8), the numerator of the second fraction above can be written as:
as0:k−1 — as0:k−2 = Pr(s0:k—2)ask−1 + as0:k−2 — Pr(s0:k—3)ask−2 — as0:k−3

We also know that as0:k−2 = Pr(s0:k—3)ask−2 + as0:k−3 because of (8), so we can rewrite (11) as:
U  = Uk—1Pr(s0:k—2) — ask−1 Pr(s0:k—2)

k	Pr(s0:k—1)
Pr(s0:k—1)

According to the definition of sequence probabilities in (5), we have Pr(s0:k—1) = 
Pr(s0:k—2)Psk−2sk−1 , which implies:
U = Uk—1 — ask−1

k	Ps
k−2sk−1

Finally, we can write Psk−2sk−1 as a difference of cumulative probabilities to obtain Equation (10).	2
Starting from some initial transition, we can recursively generate an entire se- quence of random samples that are uniformly distributed between 0 and 1. If the previous step utilised a sample Uk—1 ~ C (0, 1), we know that ak—1 < Uk—1 ≤ bk—1. If we define Uk according to (10), it is easy to show that 0 < Uk ≤ 1, which means that Uk ~ C (0, 1). Although this sequence is produced deterministically, we have shown that it corresponds to the uniform sample needed to sample from the sequence distribution.
Thus, assuming that the quantities ask−1 and bsk−1 — ask−1 have to be calcu- lated anyway, generating a sample at each step requires a subtraction followed by a division, as Equation (10) implies. This procedure is more efficient than most of the random generator algorithms, in particular the ones that produce high quality random numbers.
Sampling from the Time Distribution
Time Discretisation
If we select some particular sequence s0:k, the duration of the total of the transitions involved is represented by a Ls0:k random variable. In the case of CTMCs this will be the sum of k exponentially distributed independent random variables that determine the duration of each transition, or more formally:
Ls0:k = Σ Lsi	(12)
i=0
where Lsi ~ Exp(λsi ). Therefore, Ls0:k will follow hypo-exponential distribution with k parameters, or equivalently Ls0:k ~ Hypo(λs0 ,..., λsk ). To sample directly from Ls0:k is only feasible for special cases such as the Erlang distribution, which is a hypo-exponential with k similar parameters. It is possible to transform Ls0:k to an Erlang distributed variable by applying uniformisation [13]. This approach is problematic though, as the probability matrix of the embedded DTMC will contain self-loops, in contrast to the original jump chain as defined in (1). This means that the uniformised CTMC will involve a larger number of events, a fact that could actually slow the simulation down.
Our attempt of sampling from the hypo-exponential Ls0:k efficiently will focus on approximating the exponentially distributed sojourn times with a discrete ran- dom variable. The use of a discrete distribution implies that we divide time into

intervals, since it involves discrete time-steps rather than continuous. Thus, while a continuous distribution indicates the probability of a transition happening up to a time t, a discrete one indicates the transition probability up to the n-th interval.
The geometric distribution seems to be a reasonable choice for the task, since it is the discrete analogue of the exponential. Given some exponential random variable L ~ Exp(λ), this can be approximated by a geometrically distributed Y ~ G(p) that denotes the number of Bernoulli trials needed to fire a transition with probability
p. The geometric distribution is supported in N excluding zero. Given the length of intervals l, we can map a geometric random variable to R+ by considering that it is supported in {1l, 2l, ... }. Since Y is geometric, its expected value will be 1/p intervals, or l/p in terms of time units. If we make L and Y correspond to the same expected value, that is 1/λ = l/p, it is easy to show that the interval length will be:
p
l = 	(13)
λ
Therefore, to determine the amount of time spent in state sk will involve two steps:
Sample from Ysk ~ G(p). Using a uniform sample U ~ C (0, 1), we choose a
n ∈ N∗ with probability:
Pr(Ysk = n)= Pr(aYsk < U ≤ bYsk )	(14) where bYsk = Pr(Ysk ≤ n) and aYsk = Pr(Ysk ≤ n — 1).
Calculate the time spent in state sk:


p
tsk = nlsk = n
sk
(15)

The advantage of time discretisation is that we can use the sequence sampling technique presented in Section 3, and therefore reduce the random samples gener- ated. To illustrate how this is possible, let us consider the stochastic process {Ysk }t that denotes the collection of geometrically distributed random variables used to approximate the sojourn times in some CTMC. If we set the same parameter p for these random variables, then they will be independent and identically distributed. We can easily verify that {Ysk }t is essentially a Markov process, which means that it is possible to generate an entire state sequence using a single uniform sample, as demonstrated in the previous section. The time discretisation was necessary, otherwise it would not be possible to define the discrete state Markov chain {Ysk }t, and therefore employ the trajectory sampling technique.
One desirable property of our approach is that it gives an estimation for the duration of all of the transitions involved in a trajectory. On the contrary, the Gamma sampling used in K-skip only produces the total duration of k transitions. While both approaches use a single random number to determine the duration of trajectories, K-skip is expected to be superior from a performance point of view. However, our method produces trajectories that are as detailed as the ones of the original Markov chain.

One other strength of our approach is that its applicability does not rely on particular model properties. The Gamma sampling used in K-skip assumes that exit rates do not change much during the k steps. This assumption, which is similar to the leap condition in τ -leaping methods, may not hold for some models meaning that it could be an extra source of error. Our method is not exact however, due to the time discretisation employed. The quality of this approximation is discussed in the rest of this section.
Quality of Approximation
Since the interval length l is dependent on the parameter p of the Geometric distri- bution, it is rather intuitive that smaller values for p result in better approximation, as l also tends to get smaller. We are going to characterise the quality of this ap- proximation in a rigorous manner.
Theorem 4.1 Let us consider some stochastic process that approximates some CTMC featuring the same state-space S, the same transition probability matrix P, and the same initial distribution π0. The approximate process is only different in the sense that the sojourn times are determined by Ysk ~ G(p), as described in (14) and (15). Then, the approximate process converges to the corresponding CTMC, as p → 0.
Proof. Let us define P (t) as the transition probability matrix of a CTMC at time
t. Given an initial state distribution vector π0, the distribution vector of the CTMC at time t will be:
πt = π0P (t)	(16)
P (t) can be calculated as a weighted sum of different powers of the probability matrix P of the underlying jump chain. The state distribution at t can then be rewritten as follows:
πt = π0 Σ Pk × Pr(k steps until t)	(17)
k=0
The modified stochastic process that resulted from this geometric approximation will have the same underlying jump chain as the original CTMC. The only term in (17) that is different in those two kinds of processes is the probability of k transitions happening until time t. This probability can be expressed as a sum of the probabilities of all sequences with duration less than or equal to t, weighted by the sequence probabilities:

Pr(k steps until t)=  Σ
s0:k∈Ek
Pr(Ls0:k ≤ t)Pr(s0:k)	(18)

In order to show that the behaviour of some CTMC as given in (17) is well approximated, it is sufficient (although not necessary) to show that the probability of k transitions until t is approximately the same for the two kinds of processes.

The modified process will have same sequence probabilities as its corresponding CTMC, since the jump process is the same. Thus, two corresponding processes are only different w.r.t the distribution of Ls0:k . Therefore, it is sufficient (although not necessary again) to show that the cumulative distribution functions for the sojourn times are approximately the same:
Pr(Ls ≤ t) ≈ Pr(Ys ≤ n) ⇔ 1 — e—λt ≈ 1 — (1 — p)n
⇔ e—λt ≈ (1 — p)n
We can now plug in the equation the interval length l to our convenience:
e—λt ≈ (1 — p)nl/l

Since we are only interested for t such that t = nl, we can discard t and nl:
e—λ ≈ (1 — p)1/l
We can also substitute the l on the exponent according to (13).
e—λ ≈ (1 — p)λ/p
Finally, we can also discard λ in both sides to obtain:
1/e ≈ (1 — p)1/p	(19)
The last equation is valid for values of p close to zero, as it can be easily shown by the limit:
lim(1 — p)1/p = 1/e	(20)
p→0
2
The p parameter is a probability, so we have 0 < p ≤ 1. Equations (19) and
(20) imply that smaller values of p result in much better approximation. However, a value for p that is too small can make the geometric sampling described by (14) inefficient, as the cumulative probabilities of the form Pr(Ysk ≤ n) will involve too many terms. Hence, we need a trade-off between approximation quality and efficiency. In the experiments that follow, we use two different values: p = 1 that implies deterministic time-steps that depend on the current state only, and p = 0.1 which we think that it is a more appropriate choice, judging by the experimental results of Section 6.
Implementation Issues
Although sampling from the sequence distribution as discussed in Section 3 is the- oretically correct, it gives rise to some computational issues. In most computer systems, the mantissa for the double-precision floating-point format contains 53 bits. That is why most random generators produce doubles of the form: m × 2—53,

where m is a uniformly distributed integer. In other words, a random generator is capable of producing 253 different values. The TSS algorithm will have 253 different inputs resulting in 253 different trajectories at most. The number of the possible trajectories can easily exceed this value even for not so long simulation runs, since it grows exponentially with the number of simulation events. Therefore, it is inevitable that we will miss a significant number of possible state sequences.
This effect can be eliminated if we sample trajectories of some particular length k, such that the number of possible sequences are significantly smaller than the number of uniformly distributed doubles. A value k = 10 is a reasonable choice that suits most of the models that we have encountered in practice. Given 53-bits of precision for the mantissa, we have 253 ≈ 9 × 1015 different possible random numbers. The largest model that we have tested is LacY [14] involving 21 bio- chemical reactions, which means that the maximum number of transitions available is also 21. If we set k = 10, we have 21k ≈ 1.66 × 1013  253.
Each step in TSS consists of two actions: state sequence sampling as described in Section 3, and time sampling using geometric approximation. These concepts are summarised in Algorithm 1, which involves two parameters: p that controls the granularity of the geometric distribution and the length k of the trajectories to sample. In our implementation the probabilities of the geometric distribution have been pre-calculated for efficiency.

Algorithm 1 Trajectory Sampling Simulation
1: Initialise system state and set 0 < p ≤ 1 and k ≥ 1 2: Draw samples UL ∼ U (0, 1) and UX ∼ U (0, 1)
3: while t < tfinal do
4:	Given M transitions, calculate the transition rates λm, ∀m ∈ {1, 2,...,M}
5:	Calculate λ = ΣM	λm, which is the rate of leaving the current state
6:	Using sample UL, draw n from the geometric distribution G(p) 7:	Using sample UX , pick transition m with probability λm/λ
8:	Update time: t = t + np/λ
9:	Update state with effect of transition m
10:	if iteration mod k /= t0hen
11:		Update UL and UX according to Equation (10) 12:	else
13:		Draw samples UL ∼ U (0, 1) and UX ∼ U (0, 1) 14:	end if
15: end while

Experiments
The efficiency of our algorithm stems from the fact that it generates fewer random numbers. One of the most popular random generators in the literature is Mersenne Twister (MT) [18]. It produces high quality random numbers while it exhibits performance comparable to the most efficient algorithms of its kind, as can be seen in [15]. We have developed our algorithm in Java using a number of open-source libraries that contain implementations of MT, namely Apache Commons, CERN Colt, JAMES II [12] and SSJ [16]. The implementations used produce doubles whose mantissa precision is 53 bits.
We have applied our approach to simulate two different models of bio-chemical reaction networks. The first model is LacY, which involves 21 reactions and 22

species, as appeared in [14]. The second example is Goldbeter’s oscillatory model
[11] as presented in [7], which involves 7 reactions and 6 species. Both models have been simulated using the ODM, K-skip, and TSS. The implementation of both K- skip and TSS is based on the ODM, hence any efficiency comparisons have ODM as a baseline.
Two different parameters were used for the geometric approximation: p = 1 and p = 0.1. Table 1 contains the running times for different random generators.

The experiments have been performed in an Intel⃝R
XeonTM E5410 @ 2.33GHz PC

running Scientific Linux 6. The results imply that TSS is about 15 ~ 20% faster than the ODM. A second observation is that using p = 1 is not significantly faster than TSS using p = 0.1 for the geometric distribution. This means that there is no need to use a value for p greater than 0.1, as this would not result in a significant improvement in efficiency.
Comparing running times for K-skip and TSS, we see that K-skip is clearly the most efficient of the two. This is because it determines the total duration of k events by sampling from a Gamma distribution, while we determine the duration of every single event happening. We note that the speedups observed for K-skip are smaller than the values reported in [2]. This is due to the MT random number generator, which is more efficient than the ran2 algorithm used in Cai & Wen paper, as pointed out in [15]. Because we are using a more efficient random generator there is less scope to deliver speedups over the ODM. If we consider this difference, the results we have found for K-skip seem to comply with the ones reported in the original work.
Table 1
Execution times in seconds for 105 simulation runs
LacY model, tfinal = 1000

Goldbeter’s model, tfinal = 10


A second issue that has to be explored is whether the stochastic process de- scribed by Algorithm 1 is equivalent to the original Markov chain. The convergence is ensured as p → 0 when k = 1. The simulation will be still exact even if k > 1 as implied by Theorems 3.2 and 3.3. However, the use of the geometric approximation means that we have a slightly altered process that approximates the original. To assess the quality of this approximation we construct the histograms for various rewards (i.e. species populations) in the models used, as it would have been im-

practical to compare the entire state-space distribution for models of that size. We then calculate the histogram distance [6], which is the euclidean distance between the histograms of the true and the approximate distribution.
It is important to note that the histogram distance will always be larger than zero, even if the simulation is exact, since the empirical distributions which result from simulation are always going to be different. In order to determine whether the distance calculated is significant, it has to be compared with the corresponding self-distance. The histogram self-distance depends on the number of samples drawn and the number of histogram intervals used. A value for the histogram distance that is smaller than the self-distance implies that the two distributions are practi- cally identical for the given number of samples. According to [6], an upper bound for the average histogram self-distance given N samples is independent from the distribution and it can be calculated using  (4K)/(πN ), where K is the number of intervals in the histogram. For the examples that follow, we have considered K = 50.
Table 2 summarises the histogram distances for several species populations and time-points in the models considered. For TSS with p = 1, some of the distances are slightly larger than the self-distance (the values written in italics). This im- plies that we have a reasonably good approximation but the error introduced by using fixed times is observable for the number of samples considered. However, the approximation quality is better when using TSS with p = 0.1, as it was expected. The histogram distance from the true distribution is at the same level or smaller than the self-distance estimated almost in all cases. This means that the error observed is within the limits of the error inherently introduced by the simulation process. Those findings support the claim that TSS with parameter p = 0.1 for the geometric approximation is an accelerated simulation approach that is almost exact.
While K-skip proved to be more efficient than our approach for the LacY and Goldbeter models, Table 2 suggests that it is not as accurate in some cases. Most of the histogram distances for the LacY model are greater than either the self-distance or the corresponding distances calculated for both versions of TSS. It seems that the assumption that the rates of subsequent states are similar might introduce some errors, a fact that renders K-skip less appropriate for some models. Our approach generalises to systems where this assumption is not valid. Moreover, our use of the geometric approximation specifies the duration of every single event happening, which can be important for some systems.

Conclusions
Trajectory sampling simulation requires fewer random samples to generate Markov chain trajectories. This is achieved by using a single random number to determine an entire sequence of transitions. We have proved that the random number required to select the next transition can be written in terms of the random number that selected the previous transition. This leads to a recursive update of a single random number

Table 2
Histogram distances for 106 simulation runs (self-distance: 0.0080)
LacY model

Goldbeter’s model

that determines an entire state sequence. In the case of CTMCs a second random number is used to determine the length of this sequence. The same concept has been used by approximating the exponentially distributed times with a geometric distribution with parameter p that controls the quality of this approximation.
We have simulated two bio-chemical models of different nature to assess the efficiency and the accuracy of the our method. The experimental results show that our approach is about 15 ~ 20% faster than the ODM, while the errors observed were found to be negligible. K-skip method I, which is a similar approach, was found to be more efficient but in some cases less accurate. Thus, TSS can be thought of as an alternative to K-skip in cases where this is possibly inappropriate.
There are also some practical considerations with respect to the length k for the trajectories to be sampled. A too large value for k might result in missing possible state sequences, while a value too small will degenerate trajectory sampling simulation to the ODM. We have used k = 10 for the experiments produced, but in the case of larger models we would have to set a smaller value for k. We think that k = 5 is appropriate even for very large models. For example, given a model with 500 reactions we have: 500k ≈ 3.125 × 1013  253.
Acknowledgement
The authors are supported by SynthSys, a Centre for Integrative Systems Biology (CISB) funded by BBSRC and EPSRC, reference BB/D019621/1.

References
A. Auger, P. Chatelain, and P. Koumoutsakos. R-leaping: accelerating the stochastic simulation algorithm by reaction leaps. The Journal of Chemical Physics, 125(8):084103, 2006.
X. Cai and J. Wen. Efficient exact and K-skip methods for stochastic simulation of coupled chemical reactions. The Journal of chemical physics, 131(6):064108, 2009.
X. Cai and Z. Xu. K-leap method for accelerating stochastic simulation of coupled chemical reactions.
The Journal of chemical physics, 126(7):074102, Feb. 2007.

Y. Cao, D. T. Gillespie, and L. R. Petzold. The slow-scale stochastic simulation algorithm. The Journal of Chemical Physics, 122(1):14116, 2005.
Y. Cao, H. Li, and L. Petzold. Efficient formulation of the stochastic simulation algorithm for chemically reacting systems. The Journal of Chemical Physics, 121(9):4059–4067, 2004.
Y. Cao and L. Petzold. Accuracy limitations and the measurement of errors in the stochastic simulation of chemically reacting systems. Journal of Computational Physics, 212(1):6–24, Feb. 2006.
F. Ciocchetta and J. Hillston. Bio-PEPA: A framework for the modelling and analysis of biological systems. Theoretical Computer Science, 410(33-34):3065–3084, 2009.
M. Gibson and J. Bruck. Efficient exact stochastic simulation of chemical systems with many species and many channels. The Journal of Physical Chemistry, 104(9):1876–1889, 2000.
D. T. Gillespie. Exact stochastic simulation of coupled chemical reactions. The Journal of Physical Chemistry, 81(25):2340–2361, 1977.
D. T. Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems. The Journal of Chemical Physics, 115(4):1716–1733, 2001.
A. Goldbeter. A minimal cascade model for the mitotic oscillator involving cyclin and cdc2 kinase. Proceedings of the National Academy of Sciences of the United States of America, 88(20):9107–9111, 1991.
J. Himmelspach and A. M. Uhrmacher. The JAMES II Framework for Modeling and Simulation. 2009 International Workshop on High Performance Computational Systems Biology, pages 101–102, 2009.
A. Jensen. Markov chains as an aid in the study of Markov processes. Skand. Aktuarietidskrift, 36:87– 91, 1953.
A. M. Kierzek. STOCKS: STOChastic Kinetic Simulations of biochemical systems with Gillespie algorithm. Bioinformatics, 18(3):470–481, 2002.
P. L’Ecuyer. TestU01: A C library for empirical testing of random number generators. ACM Transactions on Mathematical Software, 33(4):1–40, 2007.
P. L’Ecuyer and E. Buist. Simulation in java with SSJ. Proceedings of the Winter Simulation Conference 2005, pages 611–620, 2005.
H. Li and L. Petzold. Logarithmic direct method for discrete stochastic simulation of chemically reacting systems. Technical report, Department of Computer Science, University of California, Santa Barbara, 2006.
M. Matsumoto and T. Nishimura. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Transactions on Modeling and Computer Simulation, 8(1):3– 30, Jan. 1998.
E. Mjolsness, D. Orendorff, P. Chatelain, and P. Koumoutsakos. An exact accelerated stochastic simulation algorithm. The Journal of Chemical Physics, 130(14):144110–14, Apr. 2009.
C. V. Rao and A. P. Arkin. Stochastic chemical kinetics and the quasi steady-state assumption: application to the Gillespie algorithm. The Journal of Chemical Physics, 118(11):4999–5010, 2003.
