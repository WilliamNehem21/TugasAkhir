Electronic Notes in Theoretical Computer Science 123 (2005) 165–177 
www.elsevier.com/locate/entcs


Probabilistically Checkable Proofs Over the Reals
Klaus Meer1 ,2
Department of Mathematics and Computer Science Syddansk Universitet, Campusvej 55, 5230 Odense M, Denmark

Abstract
Probabilistically checkable proofs (PCPs) have turned out to be of great importance in complexity theory. On the one hand side they provide a new characterization of the complexity class NP, on the other hand they show a deep connection to approximation results for combinatorial optimization problems. In this paper we study the notion of PCPs in the real number model of Blum, Shub, and Smale. The existence of transparent long proofs for the real number analogue NPR of NP is discussed.
Keywords: PCP, real number model, self-testing over the reals.


Introduction
One of the most important and influential results in theoretical computer science within the last decade is the PCP theorem proven by Arora et al. in 1992, [1,2]. Here, PCP stands for probabilistically checkable proofs, a notion that was further developed out of interactive proofs around the end of the 1980’s. The PCP theorem characterizes the class NP in terms of languages accepted by certain so-called verifiers, a particular version of probabilistic Turing machines. It allows to stabilize verification procedures for problems in NP in the following sense. Suppose for a problem L ∈ NP and a problem

1 partially supported by the EU Network of Excellence PASCAL Pattern Analysis, Statis- tical Modelling and Computational Learning and by the Danish Natural Science Research Council SNF.
2 Email: meer@imada.sdu.dk



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.04.047


instance x ∈ {0, 1}∗ we want to check whether a potential proof y ∈ {0, 1}∗ of polynomial size verifies x ∈ L. Then it is quite likely (unless P = NP) that we have to read all components of y before we can make up a decision whether y proves x ∈ L. The stabilization idea behind the PCP theorem asks for using different proofs which have the following property: if x /∈ L, then all proofs will be rejected with high probability by inspecting only a certain amount of components in the proof. The surprising result is that the above term ‘certain amount’ can be taken as ‘constantly many’. Being of immense interest already by itself, the PCP theorem has been shown to be at the bottom of approximability properties of NP-hard combinatorial optimization problems. It was used to show several (non-) approximability results not known before. For an introduction and a proof of the PCP theorem see [3,12].
Parallelizing structural complexity theory over finite alphabets and uni- fying existing approaches in algebraic complexity theory, Blum, Shub, and Smale in 1989 introduced a uniform real number model of computation, now called the BSS machine [6]. In the meanwhile, a lot of work has been done in the BSS model. The textbooks [7,5,4] shed an excellent light on problems considered in this framework.
However, so far neither PCP results nor a notion of approximation over the reals that seems appropriate to the classical one 3 have been studied for the BSS model (interactive proofs, however, have been considered in [13]). Such a study seems both tempting and important due to the significance PCP results have shown in the Turing model.
Here, such an attempt will be described. Note that the original question posed in the PCP theorem makes perfect sense in the real framework as well:

Can we have a verification procedure for an NPR problem L ⊆ R∗
∞
:=	Rn
n=1

such that in case x /∈ L for any potential proof y only constantly many real
components of y have to be checked in order to reject with high probability? We introduce real verifiers as well as real PCPR-classes. It is then shown how stable verification proofs for NPR-complete problems can be constructed. More precisely, we design transparent (i.e. exponentially) long proofs that use a con- stant number of components, only. This will show NPR ⊆ PCPR(poly, O(1)). In the final section we briefly outline the relation to approximation problems over the reals. Most details can be found in [11].



3 Here, we do not have in mind approximation issues as treated in numerical analysis. Such a notion likely will lead to problems in the original BSS model unless analytic functions are introduced, see [8] and [10]. Therefore, a more combinatorial notion seems appropriate.

The problem setting
The BSS model; Quadratic Polynomial Systems
In the BSS model over R real numbers are considered as entities. The basic arithmetic operations +, −, ∗, : can be performed at unit costs, and there is a test-operation “is x ≥ 0?” reflecting the underlying ordering of the reals.

Decision problems now are subsets L ⊆ R∗
∞
:=
n=1
Rn. The (algebraic) size

of a point x ∈ Rn is n. Having fixed these notions it is easy to define real analogues PR and NPR of the classes P and NP as well as the notion of NPR-
completeness. For more details on the BSS model we refer to [5].
Example 2.1 (Quadratic Polynomial Systems) Let us consider a typical de- cision problem over the reals. The QPS (Quadratic Polynomial Systems) decision problem is given as:
Input: Integers n, m ∈ N, a system p := (p1,... , pm) of m real polynomials in n variables, each of degree at most 2; moreover, each pi depends on at most 3 variables.
question: Is there a common real solution a ∈ Rn such that p(a)= 0 ∈ Rm? Without loss of generality we can assume m = O(n) (by adding dummy variables). The QPS problem is NPR-complete [5]. Let us here just indicate why it is in NPR : For an input n, m, p1,... , pm we guess a y ∈ Rn, evaluate pi(y) for all i and accept iff all results vanish. It is easy to see that this verifi- cation procedure needs polynomial (algebraic) running time in the (algebraic)
input size, only.
It is the QPS problem for which we are going to construct a real number verifier later on.
Veriﬁers and the classical PCP theorem
In this section first we briefly recall the basic notions used to state the clas- sical PCP theorem. Considering an input Φ(x1,..., xn) for the NP-complete 3-SAT decision problem, the ‘natural’ NP-verification is given by guessing an assignment y ∈ {0, 1}n, plugging it into Φ and checking Φ(y)= 1. Obviously, unless P = NP this procedure in general requires to inspect all components of y in order to get the right answer. The same holds for the above ‘natural’ verification procedure showing that QPS belongs to NPR.
The idea behind PCP theorems now is to show the existence of other veri- fication procedures that are more stable in that only a constant number of proof-components have to be checked. The price to pay for it is that an input not belonging to the language under consideration might be accepted with a


certain, though small, probability (recall that in the definition of NP a false verification is always rejected). Formally, this idea is captured by introducing the notion of a verifier.
Definition 2.2 a) Let r, q : N '→ N be two functions. A (r(n), q(n))-restricted verifier V in the Turing model is a particular randomized Turing machine working as follows. For an input x ∈ {0, 1}∗ of size n and another vector y ∈
{0, 1}∗ representing a potential membership proof of x in a certain language, the verifier first produces a sequence of r(n) many random bits (under the uniform distribution on {0, 1}r(n)). Given x and these r(n) many random bits
V computes in deterministic polynomial time in n the indices of q(n) many components of y. Finally, V uses the input x together with the values of the chosen components of y in order to perform a deterministic polynomial time algorithm. At the end of this algorithm V either accepts or rejects (x, y). We denote by V (x, y, ρ) the result of V supposed the random sequence generated for input (x, y) was ρ.
b) Part a) can be adapted almost word by word in order to define verifiers for the BSS model. The randomized part will be a real number algorithm that tosses coins. The input x and the verification y now belong to R∗. The bit-length of x is replaced by its algebraic size.
Remark 2.3 i) It is important to note that the probability notions used in the definition of real number verifiers still refer to discrete sample spaces and their uniform distribution.
ii) Probabilistic BSS machines whose randomization relies on coin toss are studied in [9]. There BPPR = PR is shown. Our results actually give cer- tain lower bound information on deterministic algorithms simulating BPPR computations (if PR /= NPR).
Verifiers now are used to define PCP- and PCPR-complexity classes. Since the latter are the newly introduced ones, here we only give the definition for the real classes. The PCP-classes in the Turing framework are defined similarly by replacing once again the obvious terms in the BSS setting through those in the Turing model.
Definition 2.4 Let r, q : N '→ N; a real number decision problem L ⊆ R∗ is in class PCPR(r(n), q(n)) iff there exists a (r(n), q(n))-restricted verifier V such that conditions i) and ii) below hold:
For all x ∈ L there is a y ∈ R∗ such that for all randomly generated strings
ρ ∈ {0, 1}r(sizeR(x)) the verifier accepts: Pr{V (x, y, ρ)= 'accept'} = 1.
ρ
For any x /∈ L and for each y ∈ R∗ it is Pr{V (x, y, ρ)= 'reject'} ≥ 1 .
ρ	2


The probability is chosen uniformly over all strings ρ ∈ {0, 1}r(sizeR(x)).
Example 2.5 It is easy to see that NP = PCP(0, poly), NPR = PCPR(0, poly) as well as PCP(O(log n), O(1)) ⊆ NP, PCPR(O(log n), O(1)) ⊆ NPR. Try yourself proving P = PCP(O(log n), 2) in the Turing setting. What’s about the analog statement in the BSS model?
The PCP theorem gives the following surprising characterization of NP :
Theorem 2.6 (PCP theorem, [1,2]) NP = PCP(O(log n), O(1)).
A central first step in the proof of the above theorem is to show that NP has transparent long proofs, i.e. NP ⊂ PCP(poly, O(1)). Some of the important techniques used to prove the full PCP theorem come into play already here. The latter are so-called self-testing and self-correction of linear functions over Zn. In the real framework these techniques have to be generalized to very different domains.

Transparent long proofs for NPR
Our main result establishes the existence of transparent long proofs for NPR. Formally, it states
Theorem 3.1 NPR ⊂ PCPR(poly, O(1)) .
‘Transparent proofs’ refers to the fact that only constantly many com- ponents of a verification proof have to be inspected. ‘Long proofs’ reflects that when producing a polynomial number of random bits there are expo- nentially many different random strings and thus exponentially many proof- components that in principle might be inspected. The corresponding state- ment NP ⊂ PCP(poly, O(1)) is the first major ingredient for proving the PCP theorem over Zn, see Theorem 5 in [1]. We thus may hope for establishing as well the following
Conjecture: NPR = PCPR(O(log n), O(1)).

Where new diﬃculties come from
Since polynomial time reductions can be included in the computation of a veri- fier, for showing Theorem 3.1 it is sufficient to prove that the NPR-complete QPS problem admits a (poly, O(1))-verifier. The main question to solve is to find out what a stable verification proof for QPS should look like. Since arith- metization of the 3-SAT problem is a major ingredient of the classical PCP theorem, it is natural to follow a similar approach. This approach replaces


a satisfying assignment by certain linear (and later: polynomial) functions it generates. Then, tables of the function values replace the assignment itself. Stability of this verification is implied by randomly checking internal consis- tency (i.e. that the tables do represent linear functions which all arise from a single assignment) and solvability (i.e. that the assignment is satisfying).
However, two severe difficulties arise. First, almost all probability state- ments needed in the classical proof are heavily relying on closeness of Zn under addition and an additive shift invariance of the uniform distribution over Zn.
This means that for fixed a, b ∈ Zn we have
Pr {x = b} = Pr {a + x = b} .
x∈Zn	x∈Zn
Secondly, over Zn there are no other constants present beside {0, 1}. Thus, linearity over Zn is equivalent to the condition
∀x, y ∈ Zn : f (x + y)= f (x)+ f (y).
As soon as real numbers come into play both conditions are violated. Since we cannot enlarge our function tables to Rn, the first major problem to solve is: What is the right domain X ⊂ Rn on which we should guess function values for the verification procedure.
This domain X has to be finite and has to involve real numbers present in the concrete problem instance.The uniform distribution on such a X usually will neither be shift invariant nor will X even be closed under additive shifts. This problem gets worse by the fact that linearity on X now also requires the consideration of real scalar factors. Again, it is not obvious from which finite sets of reals those scalar factors should be taken. Rubinfeld and Sudan
[14] have treated related problems with respect to so-called rational domains which are certain well-structured subsets of Q. We shall show how such ideas can be generalized to those subsets of reals that come up in relation with NPR-complete problems.

Linear functions related to QPS
Consider an instance of QPS. For real polynomials p1,... , pm over n variables define

m
2	r˜ = (r˜1,... , r˜m) ∈ Zm.	(1)
i=1
Then it is P (y, r˜) ≥ 0 for all y ∈ Rn, r˜ ∈ Zm; moreover, P (a, r˜) = 0 for all

r˜ ∈ Zm only if a ∈ Rn is a common zero and otherwise


Pr {P (a, r˜) > 0}≥ 
r˜∈Zm
1
.	(2)
2

The structure of P is most important for what follows. It is easily seen that this structure can be splitted into two different parts, one only depending on the real coefficients of the input polynomials pi, the other depending on an assignment a ∈ Rn for the variables y. We have
Lemma 3.2 There are linear functions A, B, C, D depending on a ∈ Rn, only,

as well as linear functions LA
tively, such that
,... , LD
,E : Zm '→ Rn, Rn2 , Rn3 , Rn4 , R, respec-

∀ r˜ ∈ Zm P (a, r˜)= E(r˜)+ A ◦ LA(r˜)+ ... + D ◦ LD(r˜).


More precisely,

n




Σn	n



B : Rn2
n
'→ R , B(y1,..., yn2 )= 
n
aiaj · yij ∀ y ∈ Rn

i=1 j=1
(where we denote by yij the argument (i − 1)n + j);

C : Rn3
n	n
'→ R , C(z1,... , zn3 )= 
n
aiajak · zijk ∀ z ∈ Rn

i=1 j=1 k=1
(where zijkdenotes the argument (i − 1)n2 + (j − 1)n + k);

D : Rn4
n	n
'→ R , D(w1,... , wn4 )= 
Σn  Σn
aiajakal · wijkl ∀ w ∈ Rn

i=1 j=1 k=1 l=1
(where wijkl denotes the argument (i − 1)n3 + (j − 1)n2 + (k − 1)n + l).

Whereas functions like A,... ,D occur as well in the Turing setting, the LA,... , LD are new. Their presence creates most of the problems because evaluation on r˜ ∈ Zm gives real vectors depending on the actual input (i.e. its real coefficients). Thus, A,... ,D have to be evaluated on real number domains. This results in the necessity of developing new techniques for self- testing and -correcting those linear functions.
A natural first attempt for the right real domain on which to test linearity of, say, A is LA(Zm). However, in general when choosing x, y ∈ LA(Zm) the
2	2
element x + y will not any more belong to this set (due to the fact that for
r˜, t˜∈ Zm we now have to compute r˜+t˜over Rm instead of Zm). Thus, this idea
2	2


eventually leads to infinite sets which cannot be used as domains on which to guess function values. Instead, we extend an idea developed by Rubinfeld and Sudan [14] for so-called rational domains. Our extension applies to a much larger amount of finite subsets over R and covers all those potential domains that might be generated in the above explained way by an instance for QPS. A second problem that has to be taken into account is that linearity is not any longer equivalent to the additivity condition, only. It might be the case that LA(Zm) splits into several components that will not result from each other by additive shifts. Then even if A satisfies additivity on the considered domain we have to exclude the situation where A corresponds to different linear functions, one on each component. Therefore, another test checking scalar multiplicativity has to be performed. Here, once more a question is from which domain we have to choose the scalars.
The outline of how the verifier works is as follows:
As guess the verifier uses function tables for A, B, C, D on appropriate domains. These tables will have an exponential size in the input size.
Check that all functions are linear with high probability.
Check that all functions arise from the same vector a ∈ Rn with high probability.
For randomly chosen r˜ ∈ Zm compute the value of P (a, r˜) by looking into the function tables; check whether the result is 0.

Proof details
In this section we collect the mathematical details necessary to prove the main result. Full proofs can be found in [11].

Testing additivity
Let us first consider the map A. For the linear map LA : Zm '→ Rn let C0 :=
{λ1,... , λK} ⊂ R be the multiset of all entries in the matrix representation of LA, without loss of generality λ1 := 1,K = O(n). The set X0 is the domain on which we want to guarantee additivity of A with high probability. It is defined as


X0 :=

K


i=1
n
si · λi | si ∈ {0, 1} for 1 ≤ i ≤ K

⊂ Rn .	(3)

Note that the following inclusions hold: Zn ⊆ X0, LA(Zm) ⊆ X0 and all
2	2
sums of ≤ K many terms λ · z for λ ∈ C0,z ∈ Zn belong to X0. As indicated


before, in order to perform self-testing successfully we consider a function table of A’s values on a much larger set. This set is defined by means of


X2 :=

K


i=1
n
si · λi | si ∈ {−n3, −n3 + 1,..., n3} for 1 ≤ i ≤ K

⊂ Rn .
(4)

The verification proof that is used to show linearity of A on X0 is given as the table of values of A on elements from X2 ⊕ X2 ⊕ X2. 4 It is important to note that the use of X2 ‘stabilizes’ the verification procedure in that it is much larger than X0. This results in the fact that X2 is almost invariant under additive shifts with elements from X0 :

Lemma 4.1 ∀ x ∈ X0 it is |x+X2∩X2| ≥ 1 −  c
for a constant c > 0.

|X2 |	n
The verifier performs a first test
Test 1:  Choose 0 < δ1 < 1. For i =1 to k := [  2 | do
pick randomly elements x, y from X2;
if A(x + y) /= A(x)+ A(y) reject.
If all test pairs satisfy additivity accept A.
Proposition 4.2 If A passes Test 1, then with high probability A is close (in a certain probabilistic sense deﬁning g+(a) as the majority result among A(a + x) − A(x),x ∈ X2) to a function g+ that satisﬁes additivity on X0.

Scalar multiplicativity
Similarly as above we enlarge C0 to a set C1 given as
  K

C1 is almost invariant with respect to scalar multiplication with a λ ∈ C0 :
Lemma 4.3 ∀ λ ∈ C0 it is |λ·C1∩C1| ≥ 1 − 1 .

The next test is
|C1|	n

Test 2: Let δ2 > 0 be fixed. For i =1 to k := [  2 | do
pick random elements µ ∈ C1,x ∈ Zn;

if
A(µ · x)


µ
/= A(x) reject.



4 Though there is no X1 used here it is in the proofs of [11]. We therefore prefer not to change notations.


If all test tuples satisfy equality accept A.
Proposition 4.4 Let 0 < δ2 < 1 . For n large enough it holds
If A passes Test 2 with respect to δ2 without rejection, then there exists a

set M ⊆ Zn containing a basis of Rn and a function g∗
that satisﬁes scalar

2	A
multiplicativity for scalars from C0 and values x ∈ M such that with high

probability A is close to g∗
(in a similar, though more complicated sense as

above).
Suppose Tests 1 and 2 were performed without rejection for A. If A is a linear function on X0 (with respect to additivity) and on C0 × M (with

respect to multiplicativity), then A equals g+ and g∗
and both are the same

A	A
linear function on Rn. We denote the latter by gA; similarly for gB, gC, and
gD, respectively.
The verifier performs similar tests for B, C, D ( on the appropriate do- mains). At that point of the verification procedure we know that inconsisten- cies in the four function tables with respect to linearity are realized with high probability (arbitrarily close to 1) by constantly many inspections of certain function values.

Self-correction, consistency, solvability
It remains to check whether functions A,... ,D all result from a single as- signment a ∈ Rn and whether the latter is a zero. Due to the particular representation for P (a, r˜) we used most of the remaining steps now can be performed with small changes similarly as it is done over Z2.
In a third block of tests we compare pairs of functions (A, B), (A, C), and (A, D) to figure out whether they result from the same assignment a. This is done by self-correcting these functions in the usual manner. For example,

checking whether gA
: Rn '→ R and gB
: Rn2 '→ R result from the same a we

check for randomly chosen points x, x' ∈ Zn whether gA(x) · gA(x')= gB(x ⊗
x'), where x ⊗ x' := (x1x' , x1x' ,... , xnx' ) corresponds to the appropriate
1	2	n
assignment for the variable vector y used in the definition of B, see Lemma
3.2. Self-correction is used in order to make sure that with high probability the correct values for gA and gB are computed.
Definition 4.5 The random function SC-A is defined as follows: For x ∈ Zn
(note that Zn ⊂ X0) pick a random y ∈ X2 and return as result the value
A(x + y) − A(y). Similarly for SC-B.
Test 3 (Consistency): Let 0 < δ < 1 be fixed. For i =1 to k := [ log δ4 |
4	log 7
do

pick x, x' ∈ Zn randomly.
Pick y, y', y'' ∈ X2 according to the uniform distribution on X2.
If SC-A(x) · SC-A(x') /= SC-B(x ⊗ x') reject. If all test points satisfy equality accept.
Proposition 4.6 Suppose A and B pass Tests 1 and 2; suppose furthermore that the corresponding linear function gA : Rn '→ R originates from a vector

a = (a1,..., an) ∈ Rn
n
via gA(x)= 
ai · xi and the corresponding gB : Rn '→

i=1
R originates from a vector b = (b11,... , bnn)
∈ Rn2
n
via gB(y)= 
n
bij · yij.

i=1 j=1
If a ⊗ a /= b, then Test 3 rejects with a probability of at least 1 − δ4.
Finally, checking the a ∈ Rn to be a zero is done according to (2):
Test 4 (Satisfiability): For i =1 to k := [ log δ5 | do
2
pick r˜ ∈ Zm randomly according to the uniform distribution on Zm.
2	2
Evaluate P (a, r˜); if the result is different from 0 reject. If P (a, r˜) vanishes for all test points r˜ accept.
Proof. (of Main Theorem 3.1) All above tests are applied to all functions A,... ,D for appropriately chosen values of the probabilities involved. If one of the tests gives a contradiction the verifier rejects. If all tests pass without contradiction it accepts. It is clear that if the input is a solvable instance of QPS and if the guessed function tables result from a common zero, then no test will fail. The verifier accepts with probability one. Contrary, if the given QPS instance has no common root, then one of the conditions checked in one of the tests is violated with high probability. This will happen by inspecting constantly many values of the function tables involved, only. The size of these tables is exponential.	 

Conclusions
It is clearly the most interesting future question to prove a full version of the classical PCP theorem in the BSS model, i.e. to show (or disprove) the
Conjecture : NPR = PCPR(O(log n), O(1)).
Another interesting topic is the relation to approximation issues. Here, we think of a kind of semi-combinatorial approximation. Note that in the typical setting of combinatorial approximation the set of feasible solutions for


a problem instance (like Hamilton cycles for the TSP problem) is finite. Each of them gives a value for the objective function, and the task is to approximate the optimal solution as best as possible by polynomial time algorithms. The situation over the real is different in that we cannot require the set of feasible solutions to be finite any more. Usually, we then also have to take care about existence problems for optimal solutions. A typical optimization problem we do not consider to be appropriate in such a framework would be to compute the minimal norm solution of a polynomial system. A typical problem we do consider to be meaningful in relation with approximation matters and real PCPs is the following:
Definition 5.1 The MAX-QPS optimization problem is defined as follows: Given an instance n, m ∈ N, p1,... , pm of the QPS decision problem, find the maximal number of polynomials among the pi’s that simultaneously can be made zero by an assignment y ∈ Rn.
Here, the set of feasible solutions might be infinite, but the optimal value of the objective function does exist. Taking into account this and some other aspects (like not requiring an approximation algorithm to compute a feasible solution, but only guaranteeing the existence of a solution of a certain quality), we can define real versions of approximation classes such as APXR, PTASR, FPTASR. It is then, for example, possible to show
Theorem 5.2 Suppose the conjecture NPR = PCPR(O(log n), O(1)) holds. Then there exists no fully polynomial time approximation scheme (FPTASR) for MAX-QPS in the BSS model unless PR = NPR.	 
An even more interesting problem w.r.t. the above conjecture is the Maximum-Circuit-Acceptance-Problem MAX-q-CAP. Here, as input we con- sider numbers n, m ∈ N and m many algebraic circuits C1,... , Cm. Each circuit has a constant number q of input nodes labelled by indices i1,... , iq ∈
{1,..., n}. There might be additional input nodes labelled by real constants. Each circuit computes as result ‘accept’ or ‘reject’. Then the question is to compute the maximal number of circuits that simultaneously accept an input y ∈ Rn (where for y ∈ Rn a circuit takes as its q inputs the corresponding components yi1 ,... , yiq ).
Note that MAX-q-CAP is NPR-hard for q ≥ 3.. It then can be shown
Theorem 5.3 Let q ∈ N be constant. Then NPR = PCPR(O(log n), q) iff there exists a polynomial time BSS reduction Φ from QPS to MAX-q-CAP and an ϵ > 0 such that
if a polynomial system p := (p1,... , pm) has a common zero all circuits Φ(p)
are simultaneously satisﬁable and



if p has no common zero at most  1 
many circuits among Φ(p) can be

simultaneously satisﬁed.
The theorem is a real number version of a well known similar statement concerning MAX-3-SAT. However, it is unclear whether it holds as well for the ‘more natural’ analogue MAX-QPS.
Discussions and proofs of these results are postponed to a future paper.

References
S. Arora, C. Lund, R. Motwani, M. Sudan, M. Szegedy: Proof verification and hardness of approximation problems. Proc. 33rd Annual IEEE Symposium on Foundations of Computer Science, IEEE Computer Society, 14–23, 1992.
S. Arora, S. Safra: Probabilistic checking proofs: A new characterization of NP. Journal of the ACM 45, 70–122, 1998. Preliminary version: Proc. of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, 2–13, 1992.
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., Protasi, M.: Complexity and Approximation: Combinatorial Optimization Problems and Their Approximability Properties. Springer (1999).
S. Basu, R. Pollack, M.F. Roy: Algorithms in Real Algebraic Geometry. Vol. 10 of Algorithms and Computation in Mathematics. Springer, 2003.
L. Blum, F. Cucker, M. Shub, S. Smale: Complexity and Real Computation. Springer, 1998.
L. Blum, M. Shub, S. Smale: On a theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines. Bull. Amer. Math. Soc., vol. 21, 1–46, 1989.
P. Bu¨rgisser, M. Clausen, M.A. Shokrollahi: Algebraic Complexity Theory, volume 315 of
Grundlehren. Springer, 1997.
T. Chadzelek, G. Hotz: Analytic machines. Theoret. Comput. Sci., 219(1– 2), 151–167, 1999.
F. Cucker, M. Karpinski, P. Koiran, T. Lickteig, K. Werther: On real Turing machines that toss coins. Proc. 27th STOC, 335–342, 1995.
J. Makowsky, K. Meer: On the complexity of combinatorial and metafinite generating functions of graph properties in the computational model of Blum, Shub and Smale. In: Proceedings CSL 2000; Peter G. Clote and Helmut Schwichtenberg (Eds.), Springer LNCS 1862, 399–410, 2000.
K. Meer: Transparent long proofs: A first PCP theorem for NPR. Extended abstract to appear in: 31st International Colloquium on Automata, Languages and Programming ICALP 2004, Turku, Springer LNCS.
D. Hougardy, H.J. Pr¨omel, A. Steger: Probabilistically checkable proofs and their consequences for approximation algorithms. Discrete Mathematics 136, 175–223, 1994.
S. Ivanov, M. de Rougemont: Interactive Protocols on the reals. Computational Complexity 8, 330–345, 1999.
R. Rubinfeld, M. Sudan: Self-testing polynomial functions efficiently and over rational domains. Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms (Orlando, FL, 1992), ACM, 23–32, 1992.
