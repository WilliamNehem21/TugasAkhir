Expert Systems with Applications: X 4 (2019) 100016

		





Stock market prediction using Firefly algorithm with evolutionary framework optimized feature reduction for OSELM method
Smruti Rekha Das a,∗, Debahuti Mishra a, Minakhi Rout b
a Department of Computer Science and Engineering, Siksha ‘O’Anusandhan Deemed to be University, Bhubaneswar, Odisha, India
b School of Computer Engineering, KIIT Deemed to be University, Bhuabaneswar, Odisha, India


a r t i c l e	i n f o	a b s t r a c t

	

Article history:
Received 8 April 2019
Revised 13 July 2019
Accepted 29 August 2019
Available online 30 August 2019

Keywords:
Stock market prediction Extreme Learning Machine
Online Sequential Extreme Learning Machine
Factor analysis Firefly Optimization
Firefly with evolutionary framework
Forecasting future trends of the stock market using the historical data is the exigent demand in the field of academia as well as business. This work has explored the feature optimization capacity of firefly with an evolutionary framework considering the biochemical and social aspects of Firefly algorithm, along with the selection procedure of objective value in evolutionary notion. The performance of the proposed model is evaluated using four different stock market datasets, such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index. The datasets are regenerated using the proper mathematical formulation of the funda- mental part belonging to technical analysis, such as technical indicators and statistical measures. The feature reduction through transformation is carried out on the enhanced dataset before employing the experimented dataset to the prediction models such as Extreme Learning Machine (ELM), Online Sequen- tial Extreme Learning Machine (OSELM) and Recurrent Back Propagation Neural Network (RBPNN). For feature reduction, both statistical and optimized based feature reduction strategies are considered, where Principal Component Analysis (PCA) and Factor Analysis (FA) are examined for statistical based feature reduction and Firefly Optimization (FO), Genetic Algorithm (GA) and Firefly algorithm with evolutionary framework are well thought out for optimized feature reduction techniques. An empirical comparison is established among the experimented prediction models considering all the feature reduction techniques for the time horizon of 1 day, 3 days, 5 days, 7 days, 5 days and 30 days in advance, applying on all the datasets used in this study. From the simulation result, it can be clearly figured out that firefly with evolutionary framework optimized feature reduction applying to OSELM prediction model outperformed over the rest experimented models.

© 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)



Introduction

Predicting the future price of the stock market is extremely im- portant for the investors as knowing the appropriate movement of the stock price will reduce the risk for determining future trends so as to invest money. It’s a challenging issue to know the fu- ture movements due to its highly fluctuating nature, hence, suit- able computational methods are required to forecast the stock price movement. Regarding stock market predictability (Baek & Kim, 2018) many controversies have been rising from decades long. Initially, the movement of the stock price was characterized by the random walk theory (Cootner,1964; Fama, 1965) and later the re-

∗ Corresponding author.
E-mail addresses: smrutirekhadas@soa.ac.in (S.R. Das), debahutimishra@soa.ac.in (D. Mishra), minakhi.routfcs@kiit.ac.in (M. Rout).
search on price changes was carried out based on the Eﬃcient Market Hypothesis (EMH) (Malkiel & Fama, 1970). According to their perspective, the future price movement does not depend on the present and past value and also from their point of view fu- ture stock price prediction is impossible. In another way, various studies have made effort to experimentally disprove the EMH and the observational evidences have delineated that the stock market can be predicted to some extent. Numerous traditional approaches have been developed by the researchers in the field of forecasting the stock price movements such as Autoregressive Moving Average (ARMA), Autoregressive Integrated Moving Average (ARIMA) etc., but (Abu-Mostafa & Atiya, 1996) these methods are having some limitations, they are not capable to handle the nonlinear relation- ship exist in time series data as they make the assumption of a linear form of the structure of the model. Furthermore, they are presumed to have constant variance, whereas, the financial time


https://doi.org/10.1016/j.eswax.2019.100016
2590-1885/© 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)



series data are very noisy by nature. Later the issues regarding the existence of non linearity in time series data has been solved by the approach of so many machine learning techniques such as an Artificial Neural Network (ANN), Functional Link Artificial Neural Network (FLANN), Support Vector Machine (SVM) etc. In the last decades, numerous researchers have explored the prediction abil- ity of neural networks for financial market prediction.
This study emphasizes on increasing the ability of the predic- tion model through feature reduction techniques. From the im- mense literature study it has observed that various kinds of neural networks have been used for stock market prediction. Since the literature study hardly divulges the eﬃciency of Extreme Learning Machine (ELM), Online Sequential Extreme Learning Machine (OS- ELM) and Recurrent Back Propagation Neural Network (RBPNN) us- ing both statistical and optimized feature reduction techniques in the domain of financial market prediction. In this study, a compar- ison has made on the outcome of the prediction model considering statistical based feature reduction along with swarm evolutionary based feature reduction technique. The proposed OSELM and fire- fly with an evolutionary framework strategy model has not used in the financial domain, specifically in stock market prediction and this work basically focused on prediction of the future value of the stock price, which is entirely application oriented and not reported or published yet in the financial domain. An empirical compari- son is established among the performance of prediction models using both statistical and optimized feature reduction techniques. For statistical based feature reduction Principal Component Analy- sis (PCA) and Factor Analysis (FA) and for optimized feature reduc- tion Firefly Optimization (FO), Genetic Algorithm (GA) and Firefly algorithm with evolutionary framework are considered for the ex- perimental work. In the group of heuristic algorithm FO is consid- ered as the most interesting and novel population based heuris- tic algorithm, which follows the behavior of fireflies. The beauty of firefly is that, each firefly supposed to be attracted (Kuo & Li, 2016; Nhu, Nitsuwat, & Sodanil, 2013) by each other firefly irrespective of their sex, here the attraction between the fireflies is directly pro- portional to their brightness that means the lesser brighter is at- tracted towards the more brighter one and here in experimental point of view the brightness is decided by calculating the fitness function. On the other hand to generate new fireflies with bet- ter performance genetic algorithm is employed on FO, which is termed as Firefly algorithm with evolutionary framework in this study. The Firefly algorithm with evolutionary framework, which is the combination of FO and GA explores the optimal solutions more exactly than individual FO and GA. All these features reduc- tion techniques are applied to the experimented prediction models such as ELM, OSELM and RBPNN. Empirically the model validation is realized over four stock market datasets such as BSE Sensex, NSE Sensex, S&P 500 index and the FTSE index within a given period of time. The dataset is regenerated using technical indicators and statistical measures, based on the available features such as open price, close price, maximum price, minimum price and change of price considering the window size seven. The proposed work provides better prediction ability than the other evaluated learn- ing techniques and prediction models used in this experimental work.
The rest of the paper is organized as follows: Section 2 con-
tains the related work which basically focuses on the promising techniques used by various researchers in financial domain to get best possible results. Section 3 covers the details of data sam- ples and the dataset regeneration part. Section 4 introduces all the predictive models in details and Section 5 covers the particulars of statistical and optimized based feature reduction techniques. Section 6 provides the result analysis, which includes a schematic layout of the proposed prediction model, parameter setup, detailed steps of the proposed algorithm, the analysis of the experimental
outcome in the form of simulation graph and the training result in MSE along with the performance evaluation measures. Finally, Sections 7 and 8 present the overall work analysis and conclusions respectively.
Related work

In the development of various prediction models, ANN is real- ized as a promising technique from the significant contributions of various researchers, basically in the field of financial domain. Pre- dicting future stock price through the application of various neural network technologies such as ANN, Back propagation Neural Net- work (BPNN), FLANN, ELM etc. has proven to be eﬃcient, mak- ing the provision of the days ahead from 1 day to 1month to be predicted. Some researchers have explored ANN for the predic- tion of (Moghaddam, Moghaddam, & Esfandyari, 2016; Yetis, Ka- plan, & Jamshidi, 2014) NASDAQ stock value considering the input data on NASDAQ stock index value. Though, ANN shows its poten- tial for solving the existence of non linearity in financial data, but the random selection of weights may create some non optimal so- lutions. So, for this issue so many optimization techniques have been developed and applied to ANN by the researchers. Though, ANN is having very distinguished learning ability, but due to the availability of the noise data it may perform very inconsistently and unpredictably. Hence, Kim (2006) proposed GA to optimize the weights between the layers and selecting the task for the rel- evant instances. Zhang et al. modified bacterial chemotaxis opti- mization (BCO) (Zhang & Wu, 2009) and named it as Improved BCO and integrated to BPNN for getting the better performance and realized the validation over S&P 500. In addition to this FLANN is eﬃciently used in the financial domain, as it is less complex and simultaneously able to solve the dynamic and computational complexity problem. Majhi et al. proposed FLANN as a predictor and to train the weight of the prediction (Majhi, Panda & Sahoo, 2009) model least mean square and recursive least square algo- rithms is used and the eﬃciency of the model is evaluated by ap- plying over DJIA and S&P 500. To iteratively adjust the parameters of the FLANN, PSO and GA and the gradient descent are used by Naik et al. and compared the (Naik, Nayak & Behera, 2015) result with the result of individuals like FLANN, GA adjusting the param- eter of FLANN, FLANN adjusting the parameter by PSO. Tuning of parameter is one of the important issues in the working of NN, many researchers have introduced various learning techniques to tune the parameters. Bebarta et al. projected meta-heuristic Firefly algorithm to train (Bebarta & Venkatesh, 2016) FLANN and com- pared with FLANN trained with the learning rule of conventional BPNN. In another research (Shen, Guo, Wu, & Wu, 2011) article Ar- tificial Fish Swarm Algorithm (AFSA) has used to learn the RBFNN and compared the result with other widely accepted optimization algorithm such as Particle Swarm Optimization (PSO) and GA. Aré- valo et al. introduced a dynamic scheme (Arévalo, García, Guijarro, & Peris, 2017), where the author basically focus on the loss to be stopped and the profit to be taken, which is updated on the basis of quarterly. In addition to this the author also gives em- phasis on technical analysis indicator, which is calculated both for 15 min and 1 day time frame. Similarly exploring on time conver- gence theorem, Xu et al. finally achieved success (Xu, Ke, Xie & Zhou, 2018) on fix time controlling on stock price fluctuation sys- tem. More over by analyzing over hybridization model, Jujie and Danfeng (2018) stated hybridized model is always having the best predictive performance compared to single algorithm. Fluctuation leads to rise or fall of the trading market. Hence, based on the concept of rise or fall, bullish or bearish market strategies arises, which define the thought of the trader about future. Tsinaslanidis focused on this concept of bullish and bearish (Tsinaslanidis, 2018), where the author analyzed bearish class predictions, which has



generated on average gives significantly maximum potential prof- its, whereas the performance of bullish does not give significant profit.
Though, traditional BPNN is having the potential to solve a complex problem, but it may suffer from trapping into local min- ima and slow convergence speed. Another batch learning algo- rithm called ELM has developed by Huang, Zhu, and Siew (2006) to train single layer feed forward neural network and the beauty of ELM is that the parameters of hidden layer need not to be tuned at each iteration and the output weights are generated analyti- cally (Huang, Zhou, Ding, & Zhang, 2012; Bueno-Crespo, García- Laencina, & Sancho-Gómez, 2013). For many real applications ELM has shown good generalization performance. Apart from the ad- vantages of ELM, some of the issues regarding random choosing of number of nodes in the hidden layer also arise in ELM. Comparing to other conventional neural network ELM needs more number of hidden layer nodes.
For predicting the improved outcomes feature reduction tech- nique is addressed in so many research articles, as high dimension- ality may cause high computational cost at the time of learning the network. Furthermore, most relevant features can also give more accurate results in predicting the outcome. Dimensionality reduc- tion is process, where the number of attributes is reduced and a set of principal attributes is obtained, to get better performance re- sult in the working of the network. Initially statistical based meth- ods were used for the purpose of feature reduction, but now opti- mized based feature reduction is widely used in machine learning approach. PCA is a statistical technique for the reduction of dimen- sion with the aim of minimizing the loss of variance in the (Jolliffe, 2011; Lam & Lee, 1999) original data. For extracting features, it’s a domain of independent technique which can be applied to a wide variety of data. Through the calculation of eigenvectors of the co- variance matrix belongs to the original inputs PCA transforms lin- early the high dimensional input vector to the low dimensional vector with (Cao, Chua, Chong, Lee, & Gu, 2003) uncorrelated com- ponents. There is another statistical data reduction method called FA, which explains about the existence of correlations among the multiple outcomes as a result of one or more number of underly- ing factors. FA involve in data reduction technique representing a set of variables with a smaller number. Many optimized based fea- ture reduction techniques have been developed by the researchers basically to improve the learning algorithm, feature selection and optimization of the topology. Kim et al. proposed ANN hybridized with (Kim & Han, 2000) GA for feature discretization and feature discretization is associated very closely (Liu & Motoda, 2012) to the dimensionality reduction. GAs is mostly appropriated for higher di- mension and the (Painton & Campbell, 1995) stochastic problems with the available nonlinearities and discontinuities. Likewise fire- fly is one popular algorithm extensively used in numerous area of research. Xiong et al. used Firefly algorithm with (Xiong, Bao & Hu, 2014) support vector regression (SVR) in forecasting stock price index. In addition to this Kazem et al. proposed chaotic firefly (Kazem, Sharifi, Hussain, Saberi & Hussain, 2013) algorithm to op-
which uses the process of randomization at the (Fister, Fister, Yang, & Brest, 2013) time of searching for a set of solutions.


Data acquisition

For empirical evaluation of ideas four stock indices such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index have consid- ered in this study covering the range 24th February 2011to 27th November 2017 for BSE, 27th October 2000 to 27th November 2017 for NSE, 2nd January 2008 to 16th March 2018 for S&P500 and 2nd January 2008 to 16th March 2018 for FTSE100. The total number of samples collected from the time duration of the respective years is derived through a process of windowing by providing 7 as the win- dow size. The experimentation is carried out for a different pre- diction time horizon of 1 day, 3 days, 5 days, 7 days, 15 days and 30 days in advance, so accordingly the dataset is reset for the re- spective time horizon. Out of this total number of sample patterns 70% data are considered for training and 30% data are considered for testing. All the experimented datasets with its details are ex- pressed in Table 1.
The experimental model has used various technical indicators and statistical measures, which transform the price data by adding some additional value and then use this price data as input to the model. The technical indicators and the statistical measures are chosen based on the usage of previous literature (Das, Mishra, & Rout, 2019; Nayak, Mishra, & Rath, 2015) and the combination of available data with technical indicator and statistical measures is able to capture the unsteady properties of financial market. A growing number of technical indicators and statistical measures are available to study, but a few numbers are considered in this study for experimentation are detailed on Table 2.


Prediction models

ELM

ELM works on the principle of Single Layer Feed-forward Neu- ral Network (SLFN), with the basic difference is that unlike SLFN the hidden layer nodes and biases of ELM need not to be tuned at each iteration. In almost all practical learning algorithms of feed forward neural network, it has been marked that the input weights and hidden layer biases (Huang et al., 2006) are needed to be ad- justed at each iteration. Comparing to SLFN, the learning speed of ELM is very faster with smallest training (Deo, Sahin, 2015; Huang, Zhu, & Siew, 2004; Sajjadi et al., 2016) error and smallest norm of the weights. For developing ELM, Moore-Penrose generalized in- verse takes an important task.
For  N  number  of  (Huang  et  al.,  2006)  samples,  let x j = [x j1 , x j2 , . . . , x jn]T ‹ Rn  are  the  input  to  ELM  and t j = [t j1 , t j2 , . . . , tjm]T ‹ Rm  are  the  target  output  for  the  re-

timize the hyperparameters of SVR and evaluated the performance
spective input. Let H˜
and f(x) is set as the number of hidden

using the stock market index. The result of predicted outcome is compared with the SVR optimized with firefly, where chaotic Fire- fly algorithms with SVR proved its eﬃciency over firefly with SVR. Firefly is a population based algorithm work on meteheuristic ap- proach, developed by Yang (2010a, 2009) inspired by the behav- ior of flashing characteristics of fireflies. The flashing light is de- veloped in a way that it is related to the objective (dos Santos Coelho, de Andrade Bernert, & Mariani, 2011) function to be opti- mized. Simultaneously the concept of chaos theory is added to the optimization algorithm design, as in recent years, researchers have grown their interest on chaos theory and its features for better per- formance. This Firefly algorithm comes under stochastic algorithm,
nodes and activation function respectively. The input weight vector wj = [wj1 , wj2 , . . . , wjn]T is calculated by connecting input nodes with the jth hidden nodes and the output weight vector βj = [βj1, βj2, ... , βjm] is generated analytically, which connects jth hidden nodes to the output nodes. Let bj is the bias of jth hidden node. The equation of ELM can be written as

Hm × β = T	(1)
where Hm is the hidden layer output matrix, β is the output weight matrix and T is the target vector and these three values are calculated by using the Eqs. (1)–(3)


Table 1
Description of data samples and data range.




Table 2
List of selected technical indicators with statistical measures and their formulas.

Technical Indicators and Statistical
Measures	Formula

SMA (Simple Moving Average)	Summation of t days open price
Del C (Difference between open price)	(ith + 1)open price − ith open price
EMA (Exponential Moving Average)	(Price[today] × SF ) + (EMA[Yesterday] × (1 − SF ))
SF (Smoothing Factor) =   2  , k = the length of the EMA
RSI (Relative Strength Index)	RSI = 100 −   100 
 Average of t days up opens  Average of t days down opens
RSI = If(Average loss = 0, 100, 100 − (   100  ))
ATR (Average True Range)	AT Rt−1 X(n−1)+TRt
TR=Max((Todayr s high − Todayr slow), (Todayr s high − Todayr s open), (Todayr s open − Todayr s low))
 (Highest high − Open)  (Highest high − Lowest low)
Close − Close n periods ago Close n periods ago

Closing Price−Lowest Low Highest High−Lowest Low
× 100

ADX	First the directional movement (+DM and –DM) is calculated:
UM = todayr s high price − yesterdayr s high price DM = yesterdayr sprice − todayr slow price
if UM > DM and UM > 0, then + DM = UM, else + DM = 0
if DM > UM and DM > 0, then − DM = DM, else − DM = 0 Number of periods is selected, +DI and −− DI are:
+DI = 100 periods the refined moving average of (+DM) divided by average true range
−DI = 100 periods the refined moving average of (−DM) divided by average true range
ADX = 100 times the refined moving average of the absolute value of (+DI − −DI) divided by (+DI + −DI)
  Closing Price(P)  
Closing Price (P−n)×100
TRIX	Single − Smoothed EMA = 7 − period EMA of the closing price
Double − Smoothed EMA = 7 − period EMA of Single − Smoothed EMA Triple − Smoothed EMA = 7 − period EMA of Double − Smoothed EMA TRIX = 1 − period percent change in Triple Smoothed EMA
Mean	 1 ΣN  ai, The dataset containing the values area1, a2, . . . , an.

n−1	i
Skewness	ΣN (a −a¯)3 /N , a¯ is the mean, sis the standard deviation and N is the number of data points
Kurtosis	ΣN (a −a¯)4 /N − 3, a¯ is the mean, sis the standard deviation and N is the number of data points
Range	Max _range–Min _range, where Max _range = MAX (open, high, low) for a range of t days and Min _range = MIN (open, high, low) for a range of t days.



Hm¡w1, . . . , wH˜ , b1, . . . , bH˜ , x1, . . . , xH˜ ¢

  	 

Hm. The beauty of ELM is that it avoid multiple iterations as the

= ⎢,	.
. . .	.
⎥,	(2)
ization ability with faster learning rate.

f (w1 . xN + b1)	. . .	f wH˜ . xN + bH˜

T	T
1	1
.	.
.	.
βT	tT




(3)

OSELM

From the batch learning approach of ELM, OSELM is generated (Huang & Siew, 2004; Huang et al., 2004; Liang, Huang, Saratchan- dran, & Sundararajan, 2006). OSELM can learn the training data

H˜ H˜×m
N  N×m
in both the ways such as one-by-one and chunk by chunk (Liang

The choice of wj and bj is random, which is important for (Wang, Cao, & Yuan, 2011) accelerating the rate of learning. β is calculated by using the Moore-Penrose generalized inverse of
et al., 2006; Rong, Huang, Sundararajan, & Saratchandran, 2009) and the size of the chunk can be fixed or can be varied. The train- ing observations are discarded as soon as the learning procedure is



completed. Unlike ELM, at any time, only the newly arrived obser- vations that may be single or chunk is sequentially learned in OS- ELM. Here, the learning algorithm does not have any prior knowl-
The equation can be generalized and represented in the form of (K + 1)th chunk data, Let it be MK+1 chunk of data and the β value is calculated

edge about the number of training observations. The major differ- ence between batch ELM and OSELM is that in batch ELM all the training data available at the time of training, whereas in real ap- plication training data may arrive one by one or chunk by chunk, hence this problem can be solved through the online sequential
β(K+1) = β(K) + K −1

RBPNN
T
K+1
× ¡TK+1

— HK+1
× β(K)¢	(16)

process (Liang et al., 2006).
Let M0 is set as the chunk of an initial set of total N training samples
RBPNN is having an activation feedback with the short term
memory, where the hidden (Sakim & Mustaffa, 2014) layer is up- dated by both external input and activation from the previous for-

M0 = n¡
x j, t

j

M0
j=1
(4)
ward propagation. Using some set of weights the feedback is mod-
ified so as to enable the usual adaption through the network learn- ing.

where the number of hidden layer nodes Hnis less than M0. Con- sidering the batch algorithm, it needs to minimize H0β − T0, where
, f (w1 . x1 + b1 )	. . .	f (wHn . x1 + bHn ) ,

Feature reduction techniques

When the data of all the features are given to the prediction



f w1

T
1
T0 =	.
tT


. xM0

+ b1¢	. . .	f ¡wHn


. xM0
+ bHn ¢



M0×Hn



(6)


work models, cannot be well trained properly. Hence, the feature reduction technique is introduced in this study to get the most rel- evant features of original datasets, for improving the performance of the model. Both statistical based and optimized based feature reduction methods are employed to the dataset. The details of which are described in this section.

M0  M0×m

For minimizing H0β − T0, the solution is given by
β(0) = K −1 × HT × T0, where K0 = HT × H0	(7)
Let M1 is another chunk of data of total N training samples
Statistical based feature reduction

PCA
PCA used in multivariate data analysis as a standard tool for


M1 =
n¡xj, t j
M0+M1 j=M0+1

(8)
reducing the dimension. The basic aim is to consider the reduced number of data variables keeping the intact (Jolliffe, 2011) infor- mation available in the original data. As it requires only the eigen

Again to minimize
decomposition or singular decomposition, hence it is most popu-


H0 × β − T0 H1	T1

(9)
lar in the perception of computation. To obtain the PCA solution, two alternative optimization approach exists (Xanthopoulos, Parda- los, & Trafalis, 2013) such as variance maximization and minimum

The output β is calculated by considering both the training
datasets such as M0 and M1 ,
error formulation. For feature extraction, PCA is a very prominent method, which calculate the eigenvectors of the covariance ma-

T
β(1) = K −1 × H
H1

where
× T0 T1

(10)
trix belongs to the original inputs and transforms linearly the high
(Cao et al., 2003) dimension input vector to low dimension with uncorrelated components. The step wise procedure of PCA is given below (Giri et al., 2013):


K1 =
T
0	× H0
H1	H1

(11)
Step-1: The covariance matrix of order M × M is calculated for M
dimensional signal samples.
Σ = 1 n¡X − X ¢¡X − X ¢T }	(17)

K1 = K0 + HT × H1	(12)
H1 and T1 is calculated as follows
, f ¡w1 . xM +1 + b1¢	. . .	f ¡wH . xM +1 + bH ¢ ,

where Xs is the signal matrix of D points, each of which having M dimension and xs is denoted as the mean vector.
Step-2: Find out Eigen vectors and eigen values using the covari- ance matrix Σ by the given Eq. (18).



f ¡w1 . xM0 +M1 + b1¢	. . .	f ¡wHn . xM0 +M1 + bHn ¢


T
Mo+1
T1 =	.
tT


M ×H
(13)



(14)

where Ev matrix of Eigen vectors, and DE is the matrix, whose diagonal elements are the eigen values of covari- ance matrix Σ
Step-3: Eigen vectors are sorted in the descending order of eigen values in D.
Step-4: The data is projected in the direction of Eigen vectors tak- ing into account the dot product between the given data

M0+M1
M1×m
and the Eigen vectors.

Eq. (10) is derived and rewritten as
β(1) = β(0) + K −1 × HT × ¡T1 − H1 × β(0)¢	(15)
Step-5: Through the restriction of a given percentage of variabil- ity depending on the given problem, first few components are selected.




Fig. 1. Schematic layout of proposed prediction model.


FA
FA accepts the observed variables are the linear combination of unobservable factors and among these factors some are common to two or more variables and some expect unique to each vari- able. The factors which are unique do not contribute to the co- variance between the variables or it (Kim & Mueller, 1978) can be said among the observed variables the common factors, which are smaller in number than the observed variable numbers contribute to the covariation. The basic difference of PCA from FA is that, in PCA, the principal components are having certain mathemat- ical functions of the observed variables, while the combinations of observed variables is not able to express the common factors, Whereas, the main objective of FA is to determine the common factors which can produce the correlations among the observed variables satisfactorily.

Optimized based feature reduction

GA
GA is an evolutionary algorithm based on Darwinian rules of (Villegas, 2007) evolutionary dynamics and can be used eﬃciently to solve problems with many possible solutions. Initially, the ran- dom population is generated and then through the process of crossover, mutation and survival of the fittest, the individuals are selected to go to (Jarvis & Goodacre, 2004) the next generation un- til the particular stopping condition is satisfied. The following steps are the evolutionary stage of GA:
Step-1: From the parent population the fittest individuals are ex- tracted.
Step-2: The selected off springs are recombined using the crossover operation.
Step-3: The populations after crossover are mutated.
Step-4: The newly generated off springs are passed to evaluate the fitness.
Step-5: The worst parents are replaced with the new off springs and reinserted into the population.
Step-6: The process is repeated until the stopping condition is satisfied.

FO technique
FO is a metaheuristic algorithm, developed by Xin-Shi Yang, based on the flashing characteristics of fireflies. The flashing char- acteristics can be idealized (Arora & Singh, 2013; Gandomi, Yang, Talatahari & Alavi, 2013; Yang, 2010b,c) by using these following rules:
All the fireflies are unisex and attracted towards each other despite of their sex.
The attraction between the two fireflies is directly propor- tional to their brightness and for any two flashing fireflies always the lesser bright is attracted towards the brighter one. If for a particular firefly their no longer brighter fire- fly is available then that particular firefly moves randomly in the space.
The brightness or we can say the light intensity is deter- mined by the landscape of the objective function. Brightness of Firefly can also be defined in the way of fitness function of GA is defined.



Basically the light intensity variation and attractiveness formu- lation are the two important issues in Firefly algorithm. Simply we can say the attractiveness is determined by the brightness or the intensity of the light, which is successively associated with the ob- jective function. The computational complexity of the (Lukasik & Zak, 2009) algorithm is considered to be O(m2) but the large popu- lation size may lead to substantial increase in the time complexity.

Result analysis

This section covers the details of experimental outcome through various steps. At first the schematic layout of the proposed model along with its step wise description is analyzed. Further the ap- propriate parameter value set for the techniques used in this ex- perimental work is studied. Then the step wise description of the proposed algorithm is designed. The simulation result is delineated in the form of actual versus predicted graph along with the Mean Square Error (MSE) during the training phase. In addition to this the performance evaluation part is figured out, considering the test outcome of the prediction models through different performance measures.

The system framework

In the process of planning to take decision for investing money in financial markets, prediction is a powerful tool. Three predic- tion models such as ELM, OSELM and RBPNN are considered in this study and the eﬃciency of the prediction models are evaluated by four stock market datasets, BSE Sensex, NSE Sensex, S&P 500 index and FTSE index. The framework of the proposed prediction model is shown in Fig. 1. The experimental work has done phase wise; in the first phase the dataset has regenerated, where the regenera- tion occurs by adding some new features such as technical indica- tors and statistical measures to the existing stock market datasets. The dimension of the input has increased with the expectation of getting closer predicted value towards the actual value. The techni- cal indicators and statistical measures are generated by using the mathematical formula taking into account 7 as the window size.
Parameter setup

This section discussed the parameters of all the experimented methods one by one. ELM works on the principle of SLFN, hence it has one hidden layer. The number of nodes in the hidden layer is considered as 15 and the execution is carried out for 100 num- ber of iterations. Apart from the parameters of ELM, OSELM is hav- ing one extra parameter such as the number of batches. This study considered 30 numbers of batches to get better predicted outcome. Similarly, for the execution of RBPNN, this experimental work set threshold value and learning rate to 0.1 and momentum coeﬃcient to 0.05. Further the parameters of two optimized methods such as GA and FO is analyzed. GA is found to have two basic parameters, one is crossover rate and another is mutation rate. The parameter value 0.8 for crossover rate and 0.1 for mutation rate is most op- timal according to the convergence time. In addition to this, the basic parameters of FO, which controls the algorithm are noticed. They are attraction coeﬃcient and absorption coeﬃcient. The ap- propriate values for these two parameters has to be chosen cor- rectly to get optimal solutions. This study set attraction coeﬃcient (β0) to 1.0 and absorption coeﬃcient (γ ) to 0.8.
Apart from the individual algorithm specific parameters, pop- ulation size and number of iterations are two common control- ling parameters, which are kept fixed for all the experimentation of this work. Population size has a great role in determining the optimal solutions as a small population may miss the part of the solution space and the large populations may increase the compu- tational time, hence a 100 number of populations is considered for a standard comparison. In addition to this, number of iterations is another parameter which effect the running time to get a good so- lution, hence number of iterations is also fixed to 100 for all the experimental work.

Detailed steps of stock exchange prediction using Firefly algorithm and Firefly algorithm with evolutionary framework applying on OSELM prediction model


Algorithm: Firefly algorithm.

In the second phase, data preprocessing occurs that involves nor-		

malization and feature reduction. For normalization, min-max nor- malization is well thought-out in this study, which scales the data between 0 and 1. In case of feature reduction two types of feature reduction occurs, one is statistical based feature reduction and an- other one is optimized based feature reduction. PCA and FA are used for statistical based feature reduction whereas GA, FO and Firefly algorithm with evolutionary framework are designed for op- timized feature reduction. Now the reduced data are ready for in- put to ELM, OSELM and RBPNN prediction models.
In optimized based feature reduction GA, FO and Firefly algo-
Input: Training Data, Training Labels Output: Transformation Matrix Begin
Initialize population of firefly Pi for i = 1, 2, . . . , n
Find objective value Oi for each population Pi by function f (.)
Define absorption coeﬃcient γ
Define attraction coeﬃcient β0
Find best population Best_P and best cost Best_O by finding minimum of Oi
While maximum iteration not reached do:
For each Pi do
For each Pj where Pi /= Pj do
If Oj<Oi then do
r = ( 1 Σd  (Pk − Pk )2 )

features before giving input to the models. FO is an interesting population based heuristic algorithm, where each brighter firefly attracts its partner in spite of their sex, which help to explore the search space eﬃciently. But to explore the search space more pre- cisely for getting optimal solution, GA is added to Firefly, which is designed as Firefly algorithm with (Rahmani & MirHassani, 2014) evolutionary framework is designed. The reduced features obtained from of all the feature reduction techniques are individually set in- put to the prediction models. An empirical comparison is carried among all the prediction models for all types of feature reduction techniques using all the datasets. From the simulation result of all
β = β0.e−γ .rij 
New_Pk = P + β. rand().(Pk − Pk ) for k = 1, 2, . . . , d
New_Oi = f (New_Pi )
If New_Oi<Oi
Replace Pi with New_Pi
If New_Oi<Best_O
Replace Best_P with New_Pi
End If
End If
End If
End For
End For
End While
Return Best_P

the experimental work, the proposed OSELM based on Firefly Al-		

gorithm with evolutionary framework optimized feature reduction model proves its eﬃciency over the rest of the combinations of feature reduction approach with prediction models.
Description: The best population, which is obtained out of this FO technique is the transformation matrix. This transformation matrix is multiplied to the input data, by which the reduced fea-



tures are obtained. Instead of all the data only the data of those reduced features are set input to the OSELM prediction model.
Algorithm: Firefly algorithm with evolutionary framework.

Input: Training Data, Training Labels Output: Transformation Matrix Begin
Initialize population of firefly Pi for i = 1, 2, . . . , n
Find objective value Oi for each population Pi by function f (.)
Define absorption coeﬃcient γ
Define attraction coeﬃcient β0
Find best population Best_P and best cost Best_O by finding minimum of Oi
While maximum iteration not reached do:
For each Pi do
For each Pj where Pi /= Pj do
If Oj<Oi then do
r = ( 1 Σd  (Pk − Pk )2 )
From these figures it can be clearly figured out that the perfor- mance of RBPNN in predicting the above mentioned stock value is poorer than ELM and OSELM.
The simulated graph of ELM, OSELM and RBPNN for BSE Sensex data considering six time horizon such as 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead to be predicted is depicted in Figs. 5–7, where PCA feature reduction technique is set for reduc- ing feature before giving stock data input to the prediction model. In the experimental outcome though all the simulated graph of ac- tual versus predicted is achieved but due to lack of space, in some experimentation 1 day, 5 days and 15 days ahead is given, whereas in other experimentation 3 days, 7 days and 30 days ahead predic- tion result has shown but the information can be retrieved from the table through MSE result during training. Similarly, Fig. 8, Fig.


β = β0.e−γ .rij 
New_Pk = P + β. rand().(Pk − Pk ) for k = 1, 2, . . . , d
New_Oi = f (New_Pi )
If New_Oi<Oi
Replace Pi with New_Pi
If New_Oi<Best_O
Replace Best_P with New_Pi
End If
End If
End If
End For
End For
Addition of evolutionary framework concept
Rank all the population and find the current best populations
Cross over mechanism is applied over two current best populations and two best feasible solutions is selected out of the parent and children according to their fitness value using the fitness function().
One population is randomly selected for mutation operation and then it is replaced
End While
Return Best_P

Description: Some improvements are added here to this FO technique (Rahmani & MirHassani, 2014) for transforming the cur- rent solutions to some enhanced solutions. The concept of GA id added to the updated solutions moving towards the best solutions, and for this the basic operation of GA such as selection, crossover and mutation is carried out. The basic reason behind this mod- ification is ability to find good solutions in the reasonable time and avoiding of trapping in local minima. Hence, the two current best populations or in the concept of firefly, two populations of higher light intensity are selected through an attractiveness pro- cess. Crossover mechanism is applied over these two current best populations and two best solutions are selected based on their fit- ness value using the fitness function, eliminating the rest among the parent and children feasible solutions. In the next step, the mutation operation is performed by choosing one random popula- tion. If the new solution or new firefly is feasible and giving better solution through the functional value then the new one replaced the old solution. The process is continued till the termination con- dition is satisfied. Through, this improve method a best solution is generated which is a transformation matrix. The above transfor- mation matrix is multiplied to the input data, through, which the reduced features are obtained. Now the data of these reduced fea- tures are considered as input to the OSELM prediction model.

Experimental outcome analysis

Four datasets such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index are used to explore the forecasting ability of the prediction models. Each dataset is divided into training and testing in the ratio of 7:3. The actual versus predicted graph of all the ex- perimented datasets using Firefly algorithm with an evolutionary framework applying on ELM, OSELM and RBPNN for 1 day ahead closing price is depicted in Fig. 2, Fig. 3 and Fig. 4 respectively.
data using FA feature reduction technique applying on ELM, OSELM and RBPNN respectively. Further the actual versus predicted graph of ELM, OSELM and RBPNN based on GA is pictured in Fig. 11, Fig. 12 and Fig. 13 respectively. From the figure it can be visualized that the predicted graph of GA is more closer to actual than the statisti- cal based feature reduction method. The same result is reflected in the actual versus predicted graph presented in Fig. 14, Fig. 15 and Fig. 16, for firefly optimized feature reduction techniques applied on ELM, OSELM and RBPNN respectively. Moreover, Fig. 17, Fig. 18 and Fig. 19 represents the actual versus predicted graph of Fire- fly algorithm with evolutionary framework for the prediction mod- els ELM, OSELM and RBPNN respectively, which shows the pre- dicted graph of all the datasets considered here in this study with respect to the experimented time horizon is more closer to the ac- tual graph in comparison to the rest of the feature reduction tech- niques. The experimental work covers the six different time hori- zon such as 1 day, 3 days, 5 days, 7 days, 15 days and 30 days of BSE, NSE, S&P 500 and FTSE dataset, but due to the limitation on space only the actual verses predicted graph of BSE for 1 day to 30 days time horizon is given in this study. The total result of all the prediction models reducing features with the statistical as well as optimized feature reduction techniques for BSE, NSE, S&P 500 and FTSE datasets is shown in the form of MSE during the training phase, which is given in the respective tables.
The experimental results obtained from the MSE during the
training phase of PCA based ELM, OSELM and RBPNN for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead to be predicted using BSE, NSE, FTSE and S&P500 dataset are given in the Table 3, Table 4 and Table 5 respectively. The most notable result that we observed from the above tables is that, for 1 day ahead prediction, PCA-ELM is showing minimum error than the rest of days ahead to be predicted. As the number of days ahead prediction increases the performance of the model is significantly decreases. The above result helps to visualize that, through this model short term invest- ment is more predictable than long term investment. Furthermore, the MSE result of PCA-OSELM exhibit that BSE and FTSE during 3 days, NSE during 5 days and S&P 500 during 30 days ahead to be predicted is outperformed over the rest time horizon for the re- spective datasets. In addition to this, the MSE result of PCA-RBPNN is analyzed, which make to know that the performance of RBPNN is worse for all the time horizon of all the experimented dataset.
Table 6–8 shows the prediction accuracy for the same dataset during the training period considering the time horizon of 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead. For all the datasets it can be observed that like PCA-ELM, FA-ELM is showing better performance for 1 day ahead prediction than the rest of the days ahead considered here in this experimental work. In addition to this, the MSE result of FA-OSELM is analyzed and found that, for BSE prediction the above model is showing better performance for 1 day, for NSE the performance is better for 7 days whereas for FTSE and S&P500 the model is giving better prediction accuracy


	

	




Fig. 2. Actual versus predicted graph of (a) BSE, (b) NSE, (c) S&P 500 index and (d) FTSE 100 index using Firefly algorithm with evolutionary framework applying on ELM for 1 day ahead closing price.

Table 3
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using PCA based ELM.



on 3 days ahead prediction. Further, FA-RBPNN, which is used for a comparison, as it is a widely accepted model, is showing the worst performance for all the datasets irrespective of the time horizon.
The ability of the firefly optimized feature reduction for train- ing the prediction models are shown in Tables 9–11. According to the MSE result of the above model, it can be seen that, fire- fly based feature reduction well performed in 1 day ahead predic- tion for ELM and OSELM and predicting better compared to the statistical based feature reduction method, in the regard of mini- mum MSE value. The MSE result of ELM and OSELM is compara- tively less than the PCA and FA irrespective of the different exper- imented time horizon, applicable for all the datasets. The Firefly
algorithm is very eﬃcient in generating optimized features, which implies that its success rate to find the global optima is more than the statistical based feature reduction techniques. Though RBPNN result is worse compared to ELM and OSELM, but for a compari- son purpose, this study has included RBP neural network model, which is a bench mark model to the analysis of financial market forecasting.
The training phase result of GA in the term of MSE is illustrated in Tables 12–14. The performance of ELM and OSELM using GA as feature reduction is better for one day ahead prediction, same as FO, than the rest of time horizon ahead to be predicted. According to the result, if we consider the MSE of one day ahead, for BSE,


































Fig. 3. Actual versus predicted graph of (a) BSE, (b) NSE, (c) S&P 500 and (d) FTSE 100 index using Firefly algorithm with evolutionary framework applying on OSELM for 1 day ahead closing price.
Table 4
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using PCA based OSELM.


Table 5
Day(s)n ahead prediction of BSE, NSE, FTSE and S&P Sensex data using PCA based RBPNN.


Table 6
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using FA based ELM.



























































Fig. 4. Actual versus predicted graph of (a) BSE, (b) NSE, (c) S&P 500 and (d) FTSE 100 index using Firefly algorithm with evolutionary framework applying on RBPNN for 1 day ahead closing price.



























Fig. 5. Closing price prediction of BSE Sensex using PCA based on ELM for 1 day, 5 days and15 days ahead.


























Fig. 6. Closing price prediction of BSE Sensex using PCA based on OSELM for 3 days, 7 days and 30 days ahead.



Fig. 7. Closing price prediction of BSE Sensex using PCA based on RBPNN for 1 day, 5 days and 15 days ahead.
























Fig. 8. Closing price prediction of BSE Sensex using FA based on ELM for 3 days, 7 days and 30 days ahead.

Table 7
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using FA based OSELM.


Table 8
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using FA based RBPNN.



























Fig. 9. Closing price prediction of BSE Sensex using FA based on OSELM for 1 day, 5 days and 15 days ahead.


Fig. 10. Closing price prediction of BSE Sensex using FA based on RBPNN for 3 days, 7 days and 30 days ahead.




Fig. 11. Closing price prediction of BSE Sensex using GA based on ELM for 1 day, 5 days and 15 days ahead.

Table 9
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly based ELM.


Table 10
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly based OSELM.



		

Fig. 12. Closing price prediction of BSE Sensex using GA based on OSELM for 3 days, 7 days and 30 days ahead.



Fig. 13. Closing price prediction of BSE Sensex using GA based on RBPNN for 1 day, 5 days and 15 days ahead.



Fig. 14. Closing price prediction of BSE Sensex using Firefly based on ELM for 3 days, 7 days and 30 days ahead.

Table 11
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly based RBPNN.


Table 12
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using GA based ELM.



	



Fig. 15. Closing price prediction of BSE Sensex using Firefly based on OSELM for 1 day, 5 days and 15 days ahead.



Fig. 16. Closing price prediction of BSE Sensex using Firefly based on RBPNN for 3 days, 7 days and 30 days.





Fig. 17. Closing price prediction of BSE Sensex using Firefly algorithm with evolutionary framework based on ELM for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead.


		

 	 	 



Fig. 18. Closing price prediction of BSE Sensex using Firefly algorithm with evolutionary framework based on OSELM for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead.


Table 13
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using GA based OSELM.




Table 14
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using GA based RBPNN.



FTSE and S&P dataset, GA is presenting lesser error than FO in ELM prediction model, whereas for NSE dataset FO is performing better. Similarly, by establishing the comparison of FO and GA applying on OSELM, in case of NSE dataset, FO is found to be performing better than GA but for the rest of the dataset GA is performing better. The Table 14 result shows it worse performance compared to ELM and OSELM regardless of all the feature reduction technique.
The performance of Firefly algorithm with evolutionary frame- work, which is the combination of FO and GA is better than the individual performance of FO and GA for all the prediction models for the same datasets, which is briefly illustrated in Tables 15–17. The aspect of combining FO with GA is to explore the search space more particularly than the individual explore of search space by
FO. FO can search more eﬃciently but, whenever the concept of GA is added, which is described briefly in the algorithm part, it is able to explore more precisely. The result shows that Firefly algorithm with evolutionary framework, proficiently predict 1 day ahead data than the rest of the days ahead prediction, which is seen in both ELM and OSELM. Comparing all the prediction outcome of ELM and OSELM with respect to both statistical and optimized based feature reduction techniques, it is observed that not only for 1 day but also for 3 days, 5days, 7 days, 15 days and 30 days the performance of Firefly algorithm with evolutionary framework is better than PCA, FA, FO and GA. Though the result of RBPNN is worse than ELM and OSELM but one thing can be noticed that the result of Firefly algorithm with evolutionary framework based on RBPNN is found


		

 	 	 




Fig. 19. Closing price prediction of BSE Sensex using Firefly algorithm with evolutionary framework based on RBPNN for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead.

Table 15
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly algorithm with evolutionary framework based ELM.

Table 16
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly algorithm with evolutionary framework based OSELM.


to be performing significantly better than the rest of the feature reduction methods based on RBPNN.

Performance evaluation criterion


the testing phase, whose mathematical formulation is described in Eqs. (17)–(21).
, 1 ΣN


 


culating the deviation of predicted stock price and the actual stock price through the calculation of Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE), Theil’s U and Average Relative Variance (ARV). The above measures are used to assess the performance (Das et al., 2019; Das, Mishra, & Rout, 2017; Ferreira, Vasconcelos, & Adeodato,


N	A −P

  i=1	
MAPE =	× 100	(18)
N

MAE = 1 Σ ¯A − P¯ ¯	(19)



Table 17
Day(s) ahead prediction of BSE, NSE, FTSE and S&P Sensex data using Firefly algorithm with evolutionary framework based RBPNN.


Table 18
Performance evaluation measures for all the models during testing.



, 1 ΣN


(Ai − Pi)2
days ahead, the convergence speed decreases, it means 1day and
3 days ahead is converging faster than the rest of days consid-

, 1 ΣN


A2 + , 1 ΣN  P2
	
BSE, NSE, S&P 500 and FTSE 100 using MSE for Firefly algorithm


ΣN (Pi − Ai)2
dataset is converging at about 6, NSE is at 10, S&P 500 is at 25

ARV =
N
i=1
¡P − A˜¢	(21)
and FTSE is at 51 number of iterations. The convergence speed of BSE, NSE, S&P 500 and FTSE 100 using MSE for GA, Firefly and Fire-

Here, Ai and Pi are the ith day actual and predicted stock price and N is the number of total experimented samples. Lesser the value of RMSE, MAPE, MAE, Theil’s U and ARV leads to a better prediction model.
Table 18 shows the summary of the verification result obtained from different performance measures during the testing phase. The performance of different feature reduction techniques with respect to the prediction models is analyzed briefly. In case of PCA fea- ture reduction technique, PCA-OSELM is showing better results in RMSE, MAPE, Theil’s U and ARV, except MAE where PCA-ELM is having minimum value. Similarly, in FA, the performance of FA- OSELM is better in terms of minimum value of MAPE, Theil’s U and ARV. The rest two measures such as RMSE and MAE is mini- mum for FA-ELM. In addition to this the performance of firefly and GA with respect to all the prediction model is examined, where it is found that Firefly-OSELM is showing minimum value in terms of MAPE, Theil’s U, MAE and ARV error, except RMSE. Similarly GA- OSELM is giving a minimum of RMSE, Theil’s U, MAE and ARV error except MAPE, GA-ELM is giving minimum value. Further the eﬃ- ciency of Firefly algorithm with evolutionary framework is viewed through the result of performance measures presented in Table 18, where it is clearly depicted that the proposed Firefly algorithm with evolutionary framework-OSELM model is outperformed over all the experimented model with the minimum value of RMSE, MAPE, MAE, Theil’s U and ARV. All the above performance mea- sures are calculated for 1 day ahead to be predicted for BSE Sensex dataset.
The convergence graph of Firefly, GA and Firefly algorithm with
evolutionary framework using MSE for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead for BSE Sensex data is depicted in Fig. 20. The figure itself clearly outlined that with the increase of
fly algorithm with evolutionary framework is produced in Fig. 22, where the convergence speed of different optimized feature re- duction techniques can be marked. From the convergence graph it can be clearly figured that the Firefly algorithm with evolutionary framework, which is the combination of Firefly and GA, is converg- ing faster than the individual converging rate of Firefly and GA op- timization algorithm.
Overall work analysis

Analyzing the stock market is one of the important task, where many methods are proposed by the researchers to predict stock price with the expectations of getting predicted results closer to the actual result. The empirical results in predicting four differ- ent stock values using the feature reduction methods for prediction models are shown in this experimental work.
BSE Sensex, NSE Sensex, S&P 500 index and FTSE 100 in- dex are four stock market datasets are selected to evaluate the prediction model. BSE Sensex is one of the main stock market indices, which includes 30 blue chip company. Sim- ilarly, NSE popularly known as nifty fifty, which covers 50 specialized company and these companies is focused on in- dex as a core product. Apart from these two Indian stock exchange, this study has considered two non Indian stock exchange such as S&P 500 and FTSE 100, which is belongs to American and UK exchange respectively. S&P 500 based on 500 market capitalizations whereas FTSE 100 based on share index of 100 companies with highest market capital- ization. For a better evaluation of the experimented models these four different stock exchange value is considered for experimental purpose.


 	

	






Fig. 20. Convergence speed of (a) Firefly, (b) GA, (c) Firefly algorithm with evolutionary framework using MSE for 1 day, 3 days, 5 days, 7 days, 15 days and 30 days ahead for BSE Sensex data.


The dataset is regenerated by using technical indicators and statistical measures considering 7 as window size as in our previous work of exchange rate prediction, the window size has explored, where 7 window size is found to be giving better result (Das et al., 2019). The prediction result of six different time horizon such as 1 day, 3 days, 5 days, 7 days,
15 days and 30 days has observed. The enhanced datasets for different time horizon are the input to the prediction models.
This study compares the prediction result of ELM, OSELM and RBPNN. All these three models are neural network based model and in many of financial market research analysis, they are found to be giving better prediction outcome com- pared to the statistical based model.
For statistical based feature reduction, both PCA and FA em- ploying on ELM, OSELM and RBPNN is giving nearly equal to result for all the dataset with the different experimented
time horizon. In both the cases ELM with the reduced fea- tures obtained from PCA and FA is performing better for one day ahead prediction than the rest of the time horizon days ahead to be predicted. Similarly the result of optimized based feature reduction is analyzed. GA, firefly and Firefly al- gorithm with the evolutionary framework optimized feature reduction methods are well thought out for this study. GA is a stochastic algorithm, where there is no possibility of trap into local optima rather it jumps out of locality. Similarly FO is a heuristic population based algorithm, which explore the search space very precisely. Combining the concept of Fire- fly algorithm with GA, Firefly algorithm with evolutionary framework is designed, where GA updates the current best solutions generated by Firefly Optimization to move towards the best solutions. The motivation behind this integration of firefly with GA is to find good solutions and simultaneously avoiding for trapping into local minima.


	






Fig. 21. Convergence speed of (a) BSE, (b) NSE, (c) S&P 500 and (d) FTSE 100 using MSE for Firefly algorithm with evolutionary framework.



For all the optimized feature reduction techniques the com- mon controlling parameters such as number of populations and number of iterations is set to 100 to make the comparison stan- dardize. Apart from this common controlling parameters each al- gorithm is having some specific parameters. Hence, for a particu- lar algorithm proper parameter value set is based on the architec- ture of the algorithm, as tuning of this algorithm specific param- eter has a vital role in the performance of the model. For GA the two basic parameters, crossover rate and mutation rate, where the crossover rate value at 0.8 and the mutation rate at 0.1 is giving more optimal result according to the time of convergence. Simi- larly the attraction coeﬃcient and absorption coeﬃcient are two specific parameters of Firefly algorithm, where the attraction co- eﬃcient with 1 and absorption coeﬃcient with 0.8 configurations achieved the best result in the course of executing test runs. The simulation result of GA and firefly with respect to the number days in terms of MSE is nearly equal for all the experimented stock mar- ket data. In both these optimized feature reduction based method ELM and OSELM are giving better results than RBPNN. In com- parison to firefly and GA, Firefly algorithm with the evolutionary framework showing better performance irrespective of the predic- tion models and datasets. The features reduced by Firefly algorithm with the evolutionary framework for OSELM outperforms over all
the combinations of both statistical and optimized feature reduc- tion methods for all the experimented prediction models, whose relative improvements result for BSE Sensex is shown in Tables 19–
23. The relative improved results of the proposed Firefly algorithm with evolutionary framework for OSELM in predicting BSE Sensex follow-up days ahead by,


More than 59%, 29%, 28%, 21%, 32%, 35% (1, 3, 5, 7, 15, 30
days ahead respectively) compared to PCA-ELM,
More than 90%, 35%, 91%, 66%, 94%, 50% (1, 3, 5, 7, 15, 30
days ahead respectively) compared to PCA-OSELM,
More than 99% for 5, 7, 15, 30 days ahead respectively, and almost 100% improved for 1day and 3 days ahead com- pared to PCA-RBPNN, FA-RBPNN, Firefly-RBPNN, GA-RBPNN
and Firefly algorithm with evolutionary framework-RBPNN
Furthermore the relative improvements of Firefly algorithm with evolutionary framework-OSELM is compared with FA based prediction models, where it is found that, improve- ments is more than 79%, 75%, 40%, 68%, 36%, 36% (1, 3, 5, 7, 15, 30 days ahead respectively) compared to GA-ELM,
More than 77%, 95%, 75%, 68%, 69%,69% (1, 3, 5, 7, 15, 30
days ahead respectively) compared to FA-OSELM,




Table 19
Comparison of overall prediction accuracy of Firefly algorithm with evolutionary framework-OSELM with PCA-ELM, PCA-OSELM and PCA-RBPNN network for stock price prediction base on the MSE for BSE Sensex data.
Comparison level (%)	Comparison level (%)	Comparison level (%)




Table 20
Comparison of overall prediction accuracy of Firefly algorithm with evolutionary framework-OSELM with FA-ELM, FA-OSELM and FA-RBPNN network for stock price pre- diction base on the MSE for BSE Sensex data.
Comparison level (%)	Comparison level (%)	Comparison level (%)




Table 21
Comparison of overall prediction accuracy of Firefly algorithm with evolutionary framework-OSELM with GA-ELM, GA-OSELM and GA-RBPNN network for stock price prediction base on the MSE for BSE Sensex data.
Comparison level (%)	Comparison level (%)	Comparison level (%)




Table 22
Comparison of overall prediction accuracy of Firefly algorithm with evolutionary framework-OSELM with Firefly-ELM, Firefly-OSELM and Firefly-RBPNN network for stock price prediction base on the MSE for BSE Sensex data.
Comparison level (%)	Comparison level (%)	Comparison level (%)







































	

Fig. 22. Convergence speed of (a) BSE, (b) NSE, (c) S&P 500 and (d) FTSE 100 using MSE for GA, Firefly and Firefly algorithm with evolutionary framework.

Table 23
Comparison of overall prediction accuracy of Firefly algorithm with evolutionary framework-OSELM with Firefly algorithm with evolutionary framework-ELM, Firefly algo- rithm with evolutionary framework-RBPNN network for stock price prediction base on the MSE for BSE Sensex data.


More than 13%, 12%, 0.14%, 4.54%, 8.86%, 0.54% (1, 3, 5, 7, 15,
30 days ahead respectively) compared to GA-ELM,
More than 13%, 12%, 0.79%, 4.6%, 8.91%, 1.45% (1, 3, 5, 7, 15,
30 days ahead respectively) compared to GA-OSELM,
More than 18%, 10%, 14%, 1.34%, 9.18%(1, 3, 7, 15, 30 days
ahead respectively) compared to Firefly-ELM, but there is no improvements for 5 days ahead to be predicted,
More than 18%, 10%, 14%, 1.33%, 9.19% (1, 3, 7, 15, 30 days
ahead respectively) compared to Firefly-OSELM, here also no improvements occur for 5 days ahead prediction.
More than 6%, 4%, 0.04%, 0.13%, 0.01% (1, 3, 7, 15, 30 days
ahead respectively) compared to Firefly algorithm with evo- lutionary framework-ELM and there are no improvements for 5 days ahead prediction.



The aforementioned remarks about the performance of Firefly algorithm with evolutionary framework for OSELM should be ac- companied by a conclusion that the proposed model is signifi- cantly improved and better than other feature reduction through transformation method applied on the prediction models used for the experimentation by this study.

Conclusions

This study presents the Firefly algorithm with evolutionary framework based feature reduction through transformation model for OSELM to predict the future stock price. The proposed model’s performance is evaluated using four stock market dataset such as BSE Sensex, NSE Sensex, S&P 500 index and FTSE index. The objec- tive on feature reduction focused on two factors: statistical based feature reduction and optimized feature reduction. PCA and FA are considered for statistical based feature reduction, whereas Firefly, GA and Firefly algorithm with evolutionary framework are used to reduce the features using the respective optimization technique. After reducing the features the datasets are employed to three pre- diction models such as ELM, OSELM and RBPNN, exploring the pre- dicted outcome of six time horizon. Though, all the feature reduc- tion methods have the ability to reduce features with the expecta- tion of getting better result, but the potential of the proposed Fire- fly algorithm with evolutionary framework is very high and also delineating outstanding performance for OSELM prediction model. In future, this work can be extended to other financial aspects with analyzing their influencing factors.

Declaration of Competing Interest

There is no conflict of interest.

CRediT authorship contribution statement

Smruti Rekha Das: Conceptualization, Data curation, Formal analysis, Methodology. Debahuti Mishra: Supervision, Validation. Minakhi Rout: Writing - review & editing.

References

Abu-Mostafa, Y. S., & Atiya, A. F. (1996). Introduction to financial forecasting. Applied Intelligence, 6(3), 205–213.
Arévalo, R., García, J., Guijarro, F., & Peris, A. (2017). A dynamic trading rule based on filtered flag pattern recognition for stock market price forecasting. Expert Systems with Applications, 81, 177–192.
Arora, S., & Singh, S. (2013). The firefly optimization algorithm: Convergence analy- sis and parameter selection. International Journal of Computer Applications, 69(3), 48–52.
Baek, Y., & Kim, H. Y. (2018). ModAugNet: A new forecasting framework for stock market index value with an overfitting prevention LSTM module and a predic- tion LSTM module. Expert Systems with Applications, 113, 457–480.
Bebarta, D. K., & Venkatesh, G. (2016). A low complexity FLANN architecture for forecasting stock time series data training with meta-heuristic firefly algorithm. In Computational intelligence in data mining: vol. 1 (pp. 377–385). New Delhi: Springer.
Bueno-Crespo, A., García-Laencina, P. J., & Sancho-Gómez, J. L. (2013). Neural ar- chitecture design based on an extreme learning machine. Neural Networks, 48, 19–24.
Cao, L. J., Chua, K. S., Chong, W. K., Lee, H. P., & Gu, Q. M. (2003). A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine. Neurocomputing, 55(1–2), 321–336.
Cootner, P. H. (1964). The random character of stock market prices.
Das, S. R., Mishra, D., & Rout, M. (2017). A hybridized ELM-Jaya forecasting model for currency exchange prediction. Journal of King Saud University-Computer and Information Sciences.
Das, S. R., Mishra, D., & Rout, M. (2019). An optimized feature reduction based cur- rency forecasting model exploring the online sequential extreme learning ma- chine and krill herd strategies. Physica A: Statistical Mechanics and its Applica- tions, 513, 339–370.
Deo, R. C., & S¸ ahin, M. (2015). Application of the extreme learning machine algo- rithm for the prediction of monthly Effective Drought Index in eastern Australia. Atmospheric Research, 153, 512–525.
Ding, S., Zhao, H., Zhang, Y., Xu, X., & Nie, R. (2015). Extreme learning machine: Algorithm, theory and applications. Artificial Intelligence Review, 44(1), 103–115.
dos Santos Coelho, L., de Andrade Bernert, D. L., & Mariani, V. C. (2011). A chaotic firefly algorithm applied to reliability-redundancy optimization. In 2011 IEEE congress of evolutionary computation (CEC), June (pp. 517–521). IEEE.
Fama, E. F. (1965). The behavior of stock-market prices. The Journal of Business, 38(1), 34–105.
Ferreira, T. A., Vasconcelos, G. C., & Adeodato, P. J. (2008). A new intelligent system methodology for time series forecasting with artificial neural networks. Neural Processing Letters, 28(2), 113–129.
Fister, I., Fister, I., Jr, Yang, X. S., & Brest, J. (2013). A comprehensive review of firefly algorithms. Swarm and Evolutionary Computation, 13, 34–46.
Gandomi, A. H., Yang, X. S., Talatahari, S., & Alavi, A. H. (2013). Firefly algorithm with chaos. Communications in Nonlinear Science and Numerical Simulation, 18(1), 89–98.
Giri, D., Acharya, U. R., Martis, R. J., Sree, S. V., Lim, T. C., Thajudin Ahamed, et al. (2013). Automated diagnosis of coronary artery disease affected patients using LDA, PCA, ICA and discrete wavelet transform. Knowledge-Based Systems, 37, 274–282.
Huang, G. B., & Siew, C. K. (2004). Extreme learning machine: RBF network case. In ICARCV 2004 8th control, automation, robotics and vision conference, 2004, De- cember: 2 (pp. 1029–1036). IEEE.
Huang, G. B., Zhou, H., Ding, X., & Zhang, R. (2012). Extreme learning machine for regression and multiclass classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 42(2), 513–529.
Huang, G. B., Zhu, Q. Y., & Siew, C. K. (2004). Extreme learning machine: A new learning scheme of feedforward neural networks. Neural Networks, 2, 985–990.
Huang, G. B., Zhu, Q. Y., & Siew, C. K. (2006). Extreme learning machine: Theory and applications. Neurocomputing, 70(1–3), 489–501.
Jarvis, R. M., & Goodacre, R. (2004). Genetic algorithm optimization for pre-process- ing and variable selection of spectroscopic data. Bioinformatics, 21(7), 860–868.
Jolliffe, I. (2011). Principal component analysis (pp. 1094–1096). Berlin Heidelberg: Springer.
Jujie, W., & Danfeng, Q. (2018). An experimental investigation of two hybrid frame- works for stock index prediction using neural network and support vector regression. Economic Computation & Economic Cybernetics Studies & Research, 52(4), 193–210.
Kazem, A., Sharifi, E., Hussain, F. K., Saberi, M., & Hussain, O. K. (2013). Support vec- tor regression with chaos-based firefly algorithm for stock market price fore- casting. Applied Soft Computing, 13(2), 947–958.
Kim, J. O., & Mueller, C. W. (1978). Factor analysis: Statistical methods and practical issues. Sage (No. 14).
Kim, K. J. (2006). Artificial neural networks with evolutionary instance selection for financial forecasting. Expert Systems with Applications, 30(3), 519–526.
Kim, K. J., & Han, I. (2000). Genetic algorithms approach to feature discretization in artificial neural networks for the prediction of stock price index. Expert systems with Applications, 19(2), 125–132.
Kuo, R. J., & Li, P. S. (2016). Taiwanese export trade forecasting using firefly algo- rithm based K-means algorithm and SVR with wavelet transform. Computers & Industrial Engineering, 99, 153–161.
Lam, S. L., & Lee, D. L. (1999). Feature reduction for neural network based text cat- egorization. In Proceedings. 6th international conference on advanced systems for advanced applications (pp. 195–202). IEEE.
Liang, N. Y., Huang, G. B., Saratchandran, P., & Sundararajan, N. (2006). A fast and accurate online sequential learning algorithm for feedforward networks. IEEE Transactions on Neural Networks, 17(6), 1411–1423.
Liu, H., & Motoda, H. (2012). Feature selection for knowledge discovery and data min- ing: vol. 454. Springer Science & Business Media.
Łukasik, S., & Z˙ ak, S. (2009). Firefly algorithm for continuous constrained optimiza-
tion tasks. In International conference on computational collective intelligence, Oc- tober (pp. 97–106). Berlin, Heidelberg: Springer.
Majhi, B., Rout, M., & Baghel, V. (2014). On the development and performance eval- uation of a multiobjective GA-based RBF adaptive model for the prediction of stock indices. Journal of King Saud University-Computer and Information Sciences, 26(3), 319–331.
Majhi, R., Panda, G., & Sahoo, G. (2009). Development and performance evaluation of FLANN based model for forecasting of stock markets. Expert Systems with Ap- plications, 36(3), 6800–6808.
Malkiel, B. G., & Fama, E. F. (1970). Eﬃcient capital markets: A review of theory and empirical work. The Journal of Finance, 25(2), 383–417.
Moghaddam, A. H., Moghaddam, M. H., & Esfandyari, M. (2016). Stock market in- dex prediction using artificial neural network. Journal of Economics, Finance and Administrative Science, 21(41), 89–93.
Naik, B., Nayak, J., & Behera, H. S. (2015). A novel FLANN with a hybrid PSO and GA based gradient descent learning for classification. In Proceedings of the 3rd inter- national conference on frontiers of intelligent computing: Theory and applications (FICTA) 2014 (pp. 745–754). Cham: Springer.
Nayak, R. K., Mishra, D., & Rath, A. K. (2015). A Naïve SVM-KNN based stock market trend reversal analysis for Indian benchmark indices. Applied Soft Computing, 35, 670–680.
Nhu, H. N., Nitsuwat, S., & Sodanil, M. (2013). Prediction of stock price using an adaptive neuro-fuzzy inference system trained by firefly algorithm. In 2013 international computer science and engineering conference (ICSEC), September (pp. 302–307). IEEE.
Painton, L., & Campbell, J. (1995). Genetic algorithms in optimization of system re- liability. IEEE Transactions on Reliability, 44(2), 172–178.



Rahmani, A., & MirHassani, S. A. (2014). A hybrid firefly-genetic algorithm for the capacitated facility location problem. Information Sciences, 283, 70–78.
Rong, H. J., Huang, G. B., Sundararajan, N., & Saratchandran, P. (2009). Online se- quential fuzzy extreme learning machine for function approximation and clas- sification problems. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(4), 1067–1072.
Sajjadi, S., Shamshirband, S., Alizamir, M., Yee, L., Mansor, Z., & Manaf, A. A. (2016). Extreme learning machine for prediction of heat load in district heating sys- tems. Energy and Buildings, 122, 222–227.
Sakim, H. A. M., & Mustaffa, M. T. (2014). The 8th international conference on robotic, vision, signal processing & power applications.
Shen, W., Guo, X., Wu, C., & Wu, D. (2011). Forecasting stock indices using radial basis function neural networks optimized by artificial fish swarm algorithm. Knowledge-Based Systems, 24(3), 378–385.
Tsinaslanidis, P. E. (2018). Subsequence dynamic time warping for charting: Bullish and bearish class predictions for NYSE stocks. Expert Systems with Applications, 94, 193–204.
Villegas, F. J. (2007). Parallel genetic-algorithm optimization of shaped beam cov- erage areas using planar 2-D phased arrays. IEEE Transactions on Antennas and Propagation, 55(6), 1745–1753.
Wang, Y., Cao, F., & Yuan, Y. (2011). A study on effectiveness of extreme learning machine. Neurocomputing, 74(16), 2483–2490.
Xanthopoulos, P., Pardalos, P. M., & Trafalis, T. B. (2013). Principal component analy- sis. In Robust data mining (pp. 21–26). New York, NY: Springer.
Xiong, T., Bao, Y., & Hu, Z. (2014). Multiple-output support vector regression with a firefly algorithm for interval-valued stock price index forecasting. Knowl- edge-Based Systems, 55, 87–100.
Xu, Y., Ke, Z., Xie, C., & Zhou, W. (2018). Dynamic evolution analysis of stock price fluctuation and its control. Complexity, 2018, 1–9.
Yang, X. S. (2009). Firefly algorithms for multimodal optimization. In International symposium on stochastic algorithms, October (pp. 169–178). Berlin, Heidelberg: Springer.
Yang, X. S. (2010a). Nature-inspired metaheuristic algorithms. Luniver press.
Yang, X. S. (2010b). Firefly algorithm, stochastic test functions and design optimisa- tion. arXiv preprint arXiv:1003.1409.
Yang, X. S. (2010c). Firefly algorithm, Levy flights and global optimization. In Research and development in intelligent systems XXVI (pp. 209–218). London: Springer.
Yetis, Y., Kaplan, H., & Jamshidi, M. (2014). Stock market prediction by using artificial neural network. In 2014 world automation congress (WAC), August (pp. 718–722). IEEE.
Zhang, Y., & Wu, L. (2009). Stock market prediction of S&P 500 via combination of improved BCO approach and BP neural network. Expert Systems with Applica- tions, 36(5), 8849–8854.
