BenchCouncil Transactions on Benchmarks, Standards and Evaluations 1 (2021) 100009








Latency-aware automatic CNN channel pruning with GPU runtime analysis
Jiaqiang Liu, Jingwei Sun ‚àó, Zhongtian Xu, Guangzhong Sun
School of Computer Science and Technology, University of Science and Technology of China, Hefei, China


A R T I C L E  I N F O	A B S T R A C T


Keywords:
GPU runtime analysis Inference latency Channel pruning
Convolutional neural network


The huge storage and computation cost of convolutional neural networks (CNN) make them challenging to meet the real-time inference requirement in many applications. Existing channel pruning methods mainly focus on removing unimportant channels in a CNN model based on rule-of-thumb designs, using reduced floating- point operations (FLOPs) and parameter numbers to measure the pruning quality. The inference latency of pruned models is often overlooked. In this paper, we propose a latency-aware automatic CNN channel pruning method (LACP), which aims to search low latency and accurate pruned network structure automatically. We evaluate the inaccuracy of measuring pruning quality by FLOPs and the number of parameters, and use the model inference latency as the direct optimization metric. To bridge model pruning and inference acceleration, we analyze the inference latency of convolutional layers on GPU. Results show that the inference latency of convolutional layers exhibits a staircase pattern along with channel number due to the GPU tail effect. Based on that observation, we greatly shrink the search space of network structures. Then we apply an evolutionary procedure to search a computationally efficient pruned network structure, which reduces the inference latency and maintains the model accuracy. Experiments and comparisons with state-of-the-art methods on three image classification datasets show that our method can achieve better inference acceleration with less accuracy loss.





Introduction

Convolutional Neural Networks (CNNs) have demonstrated state-of- the-art achievements in various tasks, such as image classification [1], object detection [2], and image segmentation [3]. Such a success is built upon a large number of model parameters and convolutional op- erations. As a result, the huge storage and computation cost make these models difficult to be deployed on resource-constrained devices, such as phones and robots. To address this problem, a common approach is to use model compression techniques, including quantization [4], dis- tillation [5], and pruning [6‚Äì9]. Among them, neural network pruning has been recognized as one of the most effective tools for compressing CNNs.
Neural network pruning methods aim to remove redundant weights in a dense model. According to the pruning granularity, these methods can be categorized into either weight pruning or channel pruning. In weight pruning, individual weights are zeroed out, leaving a sparse set of weight tensors. Weight pruning can significantly reduce the model size, but it also introduces irregular memory access, leading to very limited or even negative speedups on general-purpose hard- ware (e.g. CPU, GPU) [10]. Differing from weight pruning, channel pruning methods remove entire channels to compress the model. Since channel pruning only changes the dimension of weight tensors, the pruned model still adopts a dense format, which is well-suited to
general-purpose hardware and off-the-shelf libraries. As a result, chan- nel pruning can achieve better acceleration on inference performance than weight pruning.
Due to the promising performance improvement in model compres- sion, channel pruning methods have been widely studied for many years. Existing methods use the reduced floating-point operations (FLOPs) and parameter numbers to measure the pruning quality by default. However, the inference latency of neural network is influenced by many factors, such as the network architecture, the implementa- tion of operators, and the hardware property. Therefore, using FLOPs or the number of parameters as a proxy for inference latency is insufficient, and may lead the algorithm to sub-optimal result. For instance, Fig. 1 shows the relationship between FLOPs, model size, and inference latency of VGG16 network. We randomly prune channels in convolutional layers, then measure the pruned model‚Äôs FLOPs, number of parameters, and inference latency. Results show that FLOPs or parameter reduction does not necessarily result in latency reduction. For example, the pruned model A has smaller FLOPs than model B, but shows larger inference latency. The same for model C and model D, the smaller model C shows larger inference latency. This observation motivates us to investigate a latency-aware channel pruning method, instead of only focusing on FLOPs or parameter numbers.

‚àó Corresponding author.
E-mail addresses: jqliu42@mail.ustc.edu.cn (J. Liu), sunjw@ustc.edu.cn (J. Sun), xuzt@mail.ustc.edu.cn (Z. Xu), gzsun@ustc.edu.cn (G. Sun).

https://doi.org/10.1016/j.tbench.2021.100009
Received 6 August 2021; Received in revised form 11 October 2021; Accepted 20 October 2021
Available online 3 November 2021
2772-4859/¬© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).


	

/ig. 1. The relationship between FLOPs, number of parameters, and inference latency of pruned models.


Another motivation of this work is that conventional channel prun- ing methods crucially rely on human expert knowledge and hand- crafted designs, and focus on selecting unimportant channels. Li et al.
[9] take ùëô1-norm as significance criteria to determine which channels
will be pruned. Luo et al. [11] use the input of (ùëñ + 1)-th layer to guide the pruning of ùëñth layer. Lin et al. [12] rank channels with
high rank of feature maps, then prunes the least important channels. However, Liu et al. [13] find that the pruned network can achieve the same accuracy no matter it inherits the weights in the original network or not. This study inspires us that the essence of channel pruning lies in finding optimal channel numbers in each layer, instead of selecting unimportant channels based on rule-of-thumb designs. Following that idea, Lin et al. [14] use artificial bee colony algorithm to search optimal pruned network structure. However, like many conventional channel pruning methods, Lin et al. [14] use the reduced FLOPs and parameter numbers to measure the pruning quality, the latency speedup of pruned model cannot be guaranteed.
In this paper, we propose a latency-aware automatic channel prun- ing (LACP) method. Differing from conventional methods, we take channel pruning in an automatic manner. Our method aims to search the optimal pruned network structure, i.e., the channel number in convolutional layers, instead of selecting important channels. An in- tuitive challenge in finding optimal network structure is that it is impractical to exhaustively searching all the possible combinations of pruned network structures. To make the algorithm feasible, effective shrinkage on search space is necessary. We first analyze the inference latency of pruned convolutional layers on GPU. Results show that the inference latency of convolutional layers presents a staircase pattern with the number of channels, which means the inference latency of a convolutional layer changes suddenly at certain channel number intervals. Based on this observation, we greatly shrink the search space of pruned structures. Then we apply an evolutional procedure
each candidate structure, we encode it to a vector ùê∂ = ùëê1, ùëê2, ùëê3, ‚Ä¶ , ùëêùëñ , to efficiently search low-latency and accurate networ[k structure. Fo]r where ùëêùëñ represents the channel numbers in ùëñth convolutional layer.
model accuracy and inference latency. At each population, ùêæ candi- The fitness of candidate pruned network structure is measured in both
dates with highest fitness will survive to next population, crossover and mutation will take place in these survived structures to generate new structures. Finally, the best candidate is selected as the optimal pruned network structure.
Overall, the main contributions of this paper are as follows:
We propose a latency-aware automatic channel pruning method LACP. Compared to conventional methods, LACP does not require hand-crafted designs on selecting unimportant channels. It focus on the inference latency speedup, instead of the FLOPs reduction.
We analyze the inference latency of convolutional layers on GPU. Based on the analysis results, we greatly shrink the search space of pruned network structures, which enables efficient search of low-latency and accurate network structure.
We conduct a detailed evaluation to compare the proposed method and existing methods on standard datasets. Results show that our method can achieve more latency reduction with less accuracy loss.
The rest of this paper is organized as follows. Section 2 reviews related works. Section 3 presents the proposed latency-aware automatic channel pruning method in detail. Section 4, show the experimental results and analysis. Finally, we draw the paper to a conclusion in Section 5.

Related work

Deep neural networks are usually over-parameterized [15,16], lead- ing to huge storage and computation cost. There are extensive studies on compressing and accelerating neural networks. We classify current related research works into two major types: network pruning methods and neural architecture search (NAS) methods.
Pruning methods reduce the storage and computation cost by re- moving unimportant weights from the origin network. Existing pruning algorithms can be categorized into weight pruning and channel prun- ing. In weight pruning, individual weights are zeroed out. LeCun et al.
[6] present the early work about network pruning using second-order derivatives as the pruning criterion. Han et al. [7] first propose itera- tive pruning, which prunes individual weights below a monotonically increasing threshold. Guo et al. [17] and Mocanu et al. [18] point out that some previously unimportant weights may tend to be important later. Inspired by this idea, LIU et al. [19] propose a trainable mask- based method to dynamically get sparse network during the training phase. Dettmers and Zettlemoyer [20] propose sparse momentum that used the exponentially smoothed gradients as the criterion for pruning and regrowth. A fixed percentage of parameters are pruned at each pruning step. Weight pruning can significantly reduce the model size. However, the non-structured random connectivity in DNN introduces irregular memory access. It adversely affects practical acceleration in hardware platforms [10]. Differing from weight pruning, channel pruning methods focus on removing the entire redundant channels.
Li et al. [9] use ùëô1-norm to determine the importance of channels.
He et al. [8] formulate channel pruning as an optimization problem,
which selects the most representative channels to recover the accuracy of pruned network with minimal reconstruction error. Luo et al. [11] use the next layer‚Äôs input to guide the pruning of the previous layer. Lin et al. [12] use the feature map rank as sensitivity metric to prune the least important channels. Differing from these magnitude-based or sensitivity-based channel pruning methods, our work performs channel pruning in an automatic manner.
Although network pruning methods have achieved great success, they crucially rely on human expert knowledge and hand-crafted de- signs. Automatically optimizing the neural network architecture has been widely studied in recent years, known as neural architecture search (NAS). Prior works mainly sample a large number of networks from search space and train them from scratch to obtain a supervision signal, e.g. validation accuracy, for optimizing the sampling agent with reinforcement learning [21‚Äì23] or updating the population with an evolutionary algorithm [24]. Bender et al. [25] and Pham et al. [26] introduce weight-sharing paradigm in NAS to boost search efficiency, where all candidate sub-networks share the weights in a single one-shot model that contains every possible architecture in the search space. Liu et al. [27] relax the search space to be continuous with architecture




/ig. 2. The overall framework of LACP algorithm.


parameters and then efficiently optimized model parameters and archi- tecture parameters together via gradient descent. Most prior NAS based pruning methods are implemented in a bottom-up and layer-by-layer manner. In contrast, our work mainly focuses on the optimal channel number of each layer.

Methodology

Overview

Cheng et al. [28] point out that convolutional layers take up most of the computation cost in convolutional neural networks. Our work focuses on reducing the channel number of convolutional layers to effectively compress the neural network. Fig. 2 presents the overall
framework of our LACP algorithm. Consider a CNN model ùëÅ that
contains L convolutional layers. We refer to ùê∂ = [ùëê1, ùëê2, ‚Ä¶ , ùëêùêø] as the network structure of ùëÅ , where ùëêùëñ is the channel number of the
ùëñth convolutional layer. We regard channel pruning as an optimal
network structure search process, rather than manually designed strate-
gies to remove unimportant channels. The algorithm aims to find a thinner network structure than the unpruned model, meanwhile, keeping a comparable accuracy. We adopt an evolutionary algorithm to achieve the goal of our search algorithm. A certain number of candidate network structures make up a population, the candidate
best ùêæ candidate network structures are survive to the next population, network structures are evaluated using fitness. At each population, the
and those survival candidates will produce new network structures through crossover and mutation. In the end, the best candidate network structure in the whole process is selected to be the optimal pruned net- work structure, we then fine-tune it to restore the accuracy. Formally, the algorithm is equivalent to solve an optimization problem as Eq. (1) shows.
numbers of ùëñth convolutional layer in original dense model. It is im-
practical to exhaustively searching all the possible network structures,
therefore, effective constraints on the search space are necessary. To solve these problems, we further describe the detailed implementation of our method in the following sections.

Search space definition

Exhaustively searching every possible pruned network structure is impractical. To make the search algorithm feasible, we need to shrink the search space. In this section, we conduct analysis on inference latency of convolutional layers to find an efficient search space design. Convolutional layers are widely used in modern neural networks. A convolutional layer consists of a certain number of channels to extract data features. To reduce the computation cost of convolutional layer, channel pruning aims to remove a portion of channels. Intuitively, with the decrease of the channel number, the FLOPs of a convolutional layer will decrease linearly. However, due to the complex nature of convolutional layer‚Äôs execution environment, its inference latency does not vary linearly with the FLOPs. To better understand the execution mechanism convolutional layer, we analyze how channel pruning af- fects the inference latency of convolutional layer. As Fig. 3 illustrates, the inference latency of convolutional layers shows a staircase pattern with different number of channels, which means with increasing a certain number of channels, there will be a significant step increase in latency. By analyzing the intrinsic mechanism of DNN deployment on GPU, this phenomenon can be explained. The computation of a convolutional layer is parallelized using multiple threads. These threads are first grouped into different blocks, then loaded to streaming mul- tiprocessors (SMs) on a GPU. The maximum number of blocks loaded on one SM is determined by GPU‚Äôs physical capacity. If the number of thread blocks in need exceeds the GPU capacity, then GPU will divide these thread blocks into multiple consecutive waves, and run

ùê∂ùëúùëùùë°ùëñùëöùëéùëô = arg max
ùëÜ
ùêπ (ùê∂, ùëä , ùê∑ùë°ùëüùëéùëñùëõ, ùê∑ùë°ùëíùë†ùë°)	(1)
these waves in sequence. Since the SMs are executed in parallel, one wave takes the same amount of time, no matter it is fully occupied or

ùëÜ is the search space of pruned network structures. ùê∂ ‚àà ùëÜ is the
candidate network structure. ùëä is the weight of pruned network, which is assigned from the pre-trained model. ùê∑ùë°ùëüùëéùëñùëõ and ùê∑ùë°ùëíùë†ùë° repre- sent the training data and testing data, respectively. The function ùêπ
evaluates the fitness of candidate network structure to decide whether keeping current candidate in next population. The effectiveness and efficiency of the search algorithm mostly rely on the fitness evaluation and the search space definition. To find a low-latency and accurate pruned model, the fitness function should consider both model infer-
contains ùêø convolutional layers, the possible combination of pruned ence latency and test accuracy. For a convolutional neural network that
network structure can be ‚àèùêø  ùëê , where ùëê  represents the channel
not. This phenomenon is called ‚Äò‚ÄòGPU tail effect‚Äô‚Äô. For different channel number settings of a convolutional layer, their execution time can be very similar if they need the same amount of waves to compute. Therefore, with the increase of channel number, the computation cost of convolutional layer will increase. Once a critical point is exceeded, an extra wave is needed to finish the computation, which leads to a significant step increase in latency. Then, the inference latency of convolutional layer will change slowly, until the last wave is fully occupied.
Inspired by the ‚Äò‚ÄòGPU tail effect‚Äô‚Äô phenomenon, we can greatly shrink the search space of pruned network structure. Since the inference
latency shows a staircase pattern, which means under a certain range

ùëñ=1 ùëñ	ùëñ


			

/ig. 3. Inference latency of convolutional layers with varying number of channels.


of channel number settings, the inference latency changes very slowly. Within a staircase step, set the channel number to the right endpoint, then we can maximize the representational capacity of network with a little latency cost.

Algorithm 1 Latency-aware Automatic Channel Pruning Algorithm

Input: Search Cycles: ùëÜ, Population Size: ùëÅ , Number of Mutation: ùëÄ , Number of Crossover: ùê∂, Target latency: ùëá
Output: Optimal pruned network structure ùê∂‚àó
1: ùê∫0 = Random(N)
Optimal network structure search

In this section, we describe the detailed implementation of our LACP method. As Algorithm 1 shows, our method adopts evolutionary search as the overall framework. In the beginning, the initial population is ran- domly generated from the search space. Each sample in the population
represents a pruned network structure, formalized as ùê∂ = [ùëê1, ùëê2, ‚Ä¶ , ùëêùêø],
where ùëêùëñ represents the channel number in ùëñth convolutional layer.
At each population, the fitness of every candidate pruned network
structure is evaluated as below:
[ ùêøùëéùë°ùëíùëõùëêùë¶(ùê∂) ]ùë§

2: ùê∫ùë°ùëúùëùùêæ = ‚àÖ
3: for ùëñ = 0;ùëñ < ùëÜ;ùëñ + + do
ùëì ùëñùë°ùëõùëíùë†ùë†(ùê∂) = ùê¥ùëêùëê(ùê∂) √ó
ùëá
{
(2)

4:	ùê∫ùëèùëíùë†ùë° = Top1(ùê∫ùëñ)
5:	if ùê∫ùëèùëíùë†ùë° better than ùê∂‚àó then
6:	ùê∂‚àó = ùê∫ùëèùëíùë†ùë°
ùë§ =
0,  ùëñùëì  ùêøùëéùë°ùëíùëõùëêùë¶(ùê∂) < ùëá ,
(3)
‚àí1,	ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí.

7:	end if
8:	ùê∫ùë°ùëúùëùùêæ = TopK(ùê∫ùëñ)
9:	ùê∫ùëöùë¢ùë°ùëéùë°ùëñùëúùëõ = Mutation(ùê∫ùë°ùëúùëùùêæ ,M)
10:	ùê∫ùëêùëüùëúùë†ùë†ùëúùë£ùëíùëü = Crossover(ùê∫ùë°ùëúùëùùêæ ,C)
11:	ùê∫ùëñ+1 = ùê∫ùëöùë¢ùë°ùëéùë°ùëñùëúùëõ + ùê∫ùëêùëüùëúùë†ùë†ùëúùë£ùëíùëü
12: end for
13: return ùê∂‚àó

We analyze the inference latency variation of different convolu- tional layers in VGG and ResNet. Results show that the width of the staircase step is a multiple of 32. For the first few convolutional layers, the width of the staircase step is 32. As the layers deepen, the max- pooling operation or the down-sampling operation makes the feature map smaller, thus a single GPU wave can compute more convolution operation. As a result, in the subsequent convolutional layers, the width of the staircase step can increase to 64 or 128. Heuristically, for each convolutional layer, we set its possible channel number in pruned network structure to a multiple of 32. Taking VGG16 as an exam- ple, the possible channel number in the sixth convolutional layer is
[32, 64, 96, 128, 160, 192, 224, 256], where the initial number of channels
is 256. The other convolutional layers are also set up in the same way.
sidered in the fitness evaluation, where ùê¥ùëêùëê(ùê∂) represents the test As Eq. (2) shows, both accuracy and inference latency are con- accuracy of the pruned network. ùêøùëéùë°ùëíùëõùëêùë¶(ùê∂) is the inference latency of the pruned network and ùëá is the target latency, which is specified
before running the algorithm. To measure the test accuracy of a net- work structure, it is very time-consuming to completely train and test the pruned model. In our implementation, we initialize the candidate pruned network with the pre-trained model, for a pruned network
ùê∂ = [ùëê1, ùëê2, ‚Ä¶ , ùëêùêø], the ùëñth convolutional layer is initialized with ùëêùëñ
channels in the corresponding ùëñth convolutional layer in the pre-trained model, which have larger ùëô1-norm value. Then we train the pruned
model with 2 epochs and evaluate its test accuracy. Besides, we add a
inference latency of pruned network is less than the target latency ùëá , we latency constraint in the fitness function. Given a target latency, if the
simply use the test accuracy as the fitness value, otherwise, we penalize the fitness value with a coefficient less than 1. In such a mechanism, the algorithm will tend to select the model whose inference latency reaches the target latency constraint.
At each population, ùêæ candidate pruned network structures with
tion will take place in these ùêæ candidate structures to generate new largest fitness will survive to next population. Crossover and muta-



structures. The objective of the crossover operation is to integrate ex- cellent information from the parents. For example, given two preserved network structures:
[ùüëùüê, ùüëùüê, 128, 96, ùüëùüê, ùüèùüóùüê, 224, 192], [64, 64, ùüóùüî, ùüîùüí, 160, 96, ùüëùüêùüé, ùüêùüñùüñ]
one new structure will be generated by combining pieces of two parent structures:
[32, 32, 96, 64, 32, 192, 320, 288]
Mutation operation is used to promote population diversity. For exam- ple, given a network structure:
[ùüëùüê, 32, ùüèùüêùüñ, 96, ùüëùüê, 192, 224, 192]
its fragments are randomly changed, generating a new network struc- ture:
[64, 32, 160, 96, 128, 192, 224, 192]
The preserved ùêæ candidate network structures and the structures
algorithm will repeat such search iteration for ùëÜ times. In the end, the generated by crossover and mutation form the next population. The
best candidate is selected to be the optimal pruned network structure. We then fine-tune it to restore the accuracy.

Evaluation

In this section, we conduct experiments on standard datasets with different models to evaluate the performance of our algorithm.

Experimental settings

We implement our algorithm with Pytorch 1.5.0. All the experi- ments are run on NVIDIA GeForce RTX 2080 Ti GPU, which is made up of 4352 CUDA Cores and 68 SMs. We choose three standard image classification datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) to evaluate our method. CIFAR-10 dataset consists of 60,000 colored images, which are classified into 10 classes. Each class has 5000 training images and 1000 testing images. Similar to CIFAR-10, CIFAR- 100 contains 100 classes of images. Each class has 500 training images and 100 testing images. Tiny-ImageNet contains 100,000 images of 200 classes (500 for each class) colored images. Each class has 500 training images, 50 validation images, and 50 test images.
We use two kinds of models in our experiments: VGG and ResNet. VGG is a single-path network. The 16-layer model is adopted for com- pression. ResNet consists of a series of blocks, and there is a shortcut between two adjacent blocks. For dimensional matching in the pruned network, the last convolutional layer in each block will not be pruned. Two different depths of ResNet are adopted, including ResNet18 and ResNet34.
For each group of experiments, we report test accuracy, the reduc- tion of network inference latency, the reduction of FLOPs, the reduction of parameter numbers, and the reduction of channel numbers as the performance metrics. We use the PyTorch expansion package thop to count the FLOPs and parameter numbers of network. To measure inference latency of network, we run the model 10 times for GPU warm up, then run the model 300 times with input batch size 128, and take the average inference time.
For each pre-trained model used in our experiments, we train it with 200 epochs using Stochastic Gradient Descent with momentum 0.9, and the batch size is set to 128, the initial learning rate is set to 0.1, which decays by 10 every 50 epochs. The weight decay is set to 1e-4.
Comparative methods

We compare our method with three representative algorithms to show its effectiveness.

pruning method. PFEC calculates and sorts the ùëô1-norm value of ‚Ä¢ PFEC [9] is a representative traditional magnitude-based channel channels. Channels with smaller ùëô1-norm value are less important,
then those channels and corresponding feature maps are pruned.
Thinet [11] formulates channel pruning as an optimization prob- lem, and prunes channels of current layer based on statistics information computed from its next layer.
ABCPruner [14] is a state-of-the-art automatic channel prun- ing method. It adopts artificial bee colony algorithm to search
optimal pruned network structures. For ùëñth convolutional layer
of unpruned model that contains ùëêùëñ channels, ABCPruner de- fines its search scope to 10%ùëêùëñ, 20%ùëêùëñ, 30%ùëêùëñ, ‚Ä¶ , ùõº%ùëêùëñ , where the maximum preserve percent ùõº is used to restrict the width of
pruned network, so that the FLOPs and parameter numbers can be reduced.

Evaluation results

We conduct our experiments on CIFAR-10, CIFAR-100 and Tiny- ImageNet datasets with VGG and ResNet models. To search for optimal pruned network structures, we set the number of search cycles to 10 and the population size is 30, so LACP searches 300 pruned network structures in the whole process. In each population, the numbers of new pruned network structures that generated from mutation and crossovers are both set to 15. In the end, we fine-tune the best pruned network structure for 200 epochs with a learning rate of 0.1, which is divided by 10 every 50 epochs. The weight decay is set to 1e-4. All algorithms use the same pre-trained model, and the number of fine-tuning epoch is set to 200. For a fair comparison with ABCPruner, we set its maximum searching number of pruned network structures to the same 300.
The experimental results are shown in Table 1, compared with PFEC, Thinet and ABCPruner, our method achieves better model in- ference acceleration, while maintaining similar or higher accuracy. It is worth noting that, as we have discussed before, more FLOPs or parame- ter reduction does not necessarily lead to better inference acceleration. Take CIFAR-100 dataset experiments as an example, ABCPruner-50% prunes VGG16 with 87.29% FLOPs reduction and 88.22% param- eter reduction, while LACP-0.5 prunes 69.03% FLOPs and 81.14% parameters. However, LACP-0.5 achieves more latency reduction and a significantly higher accuracy than ABCPruner-50%. Another draw- back of ABCPruner can be observed from the experimental results. ABCPruner compresses the model by limiting the maximum preserve channel number of convolutional layers. As a result, once the max preserve percent is small, the width of the pruned network is limited and the representational capacity of the pruned model is thus limited. To verify that point, we show a case study in Fig. 4. As shown in the figure, compared with ABCPruner, our method achieves less accuracy loss, while reducing the same percent of inference latency. As a supple- mentary analysis, we compare the pruned network structure of LACP and ABCPruner, result shows that our method preserves more channels in the first several convolutional layers, which is more important for neural network to extract feature information. On the contrary, the pruned network of ABCPruner has a narrower head structure due to the maximum preserve setting, leading to more accuracy loss.

Conclusion

In this paper, we propose a novel latency-aware automatic CNN channel pruning method. Differing from conventional channel prun- ing methods, our method get rid of selecting unimportant channels based on hand-crafted design, and search for optimal pruned network


Table 1
The experiment results on three datasets for different models. We compare LACP with three other methods.
Note: LACP-ùõº means we set the target latency to ùõº √ó ùêø, where ùêø is the unpruned model‚Äôs inference latency. ABCPruner-ùõΩ means the maximal preserved channel number in each convolutional layer is ùõΩ √ó ùê∂, where ùê∂ is the original channel number in that layer.


structure automatically. By analyzing the inference latency of pruned networks, we indicate that neither FLOPs nor the number of parameters can accurately represent the real inference acceleration. Besides, we analyze the execution mechanism of convolutional layers on GPU. Results show that the inference latency of convolutional layers presents a staircase pattern with different number of channels. Based on this observation, we greatly shrink the combinations of network structure,
enabling efficient search of low-latency and accurate pruned network. We conduct extensive evaluations to compare our method with ex- isting studies on public datasets, and report the real latency metric. Experimental results show that our method can achieve better inference acceleration, while maintaining higher accuracy.
Although we have achieved desired pruning effect on our experi- ments, our method can be further improved. As we discussed before,


	 

/ig. 4. The pruning results of LACP and ABCPruner for VGG16 on CIFAR-10 dataset.


we shrink the search space of pruned network structure through the analysis of the GPU tail effect. However, our analysis is based on empirical profiling. A more thorough and general investigation of the GPU tail effect could be helpful. Besides, how to generalize our method to different hardware platforms is also worth studying in future work.

Acknowledgments

This work is supported by National Natural Science Foundation of China (No. 61772485). It is also funded by Youth Innovation Promotion Association of Chinese Academy of Sciences (CAS) and JD AI research. Experiments in this study were conducted on the supercomputer system in the Supercomputing Center of University of Science and Technology of China.

References

Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770‚Äì778.
Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierar- chies for accurate object detection and semantic segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 580‚Äì587.
Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully convolutional networks for semantic segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431‚Äì3440.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio, Quantized neural networks: Training neural networks with low precision weights and activations, J. Mach. Learn. Res. 18 (1) (2017) 6869‚Äì6898.
Chenglin Yang, Lingxi Xie, Chi Su, Alan L. Yuille, Snapshot distillation: Teacher- student optimization in one generation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2859‚Äì2868.
Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, Lawrence D Jackel, Optimal brain damage, in: NIPs, Vol. 2, Citeseer, 1989, pp. 598‚Äì605.
Song Han, Jeff Pool, John Tran, William Dally, Learning both weights and connections for efficient neural network, in: C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 28, Curran Associates, Inc., 2015.
Yihui He, Xiangyu Zhang, Jian Sun, Channel pruning for accelerating very deep neural networks, in: Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1389‚Äì1397.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, Pruning filters for efficient convnets, 2016, arXiv preprint arXiv:1608.08710.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li, Learning structured sparsity in deep neural networks, in: Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016, pp. 2082‚Äì2090.
Jian-Hao Luo, Jianxin Wu, Weiyao Lin, Thinet: A filter level pruning method for deep neural network compression, in: Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 5058‚Äì5066.
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, Ling Shao, Hrank: Filter pruning using high-rank feature map, in: Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1529‚Äì1538.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell, Rethink- ing the Value of Network Pruning, in: International Conference on Learning Representations, 2018.
Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu, Yonghong Tian, Channel Pruning via Automatic Structure Search, in: Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI, 2020, pp. 673‚Äì679.
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus, Ex- ploiting linear structure within convolutional networks for efficient evaluation, in: Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 1, 2014, pp. 1269‚Äì1277.
Jimmy Ba, Rich Caruana, Do deep nets really need to be deep? in: NIPS, 2014.
Yiwen Guo, Anbang Yao, Yurong Chen, Dynamic network surgery for efficient DNNs, in: NIPS, 2016.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, Antonio Liotta, Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science, Nature Commun. 9 (1) (2018) 1‚Äì12.
Junjie LIU, Zhe XU, Runbin SHI, Ray C. C. Cheung, Hayden K.H. So, Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers, in: International Conference on Learning Representations, 2020.
Tim Dettmers, Luke Zettlemoyer, Sparse networks from scratch: Faster training without losing performance, 2019, arXiv preprint arXiv:1907.04840.
Bowen Baker, Otkrist Gupta, Nikhil Naik, Ramesh Raskar, Designing neural network architectures using reinforcement learning, in: International Conference on Learning Representations, 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V Le, Learning transfer- able architectures for scalable image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8697‚Äì8710.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, Mnasnet: Platform-aware neural architecture search for mobile, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2820‚Äì2828.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, Alexey Kurakin, Large-scale evolution of image classifiers, in: International Conference on Machine Learning, PMLR, 2017, pp. 2902‚Äì2911.
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le, Understanding and simplifying one-shot architecture search, in: International Conference on Machine Learning, PMLR, 2018, pp. 550‚Äì559.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean, Efficient neural architecture search via parameters sharing, in: International Conference on Machine Learning, PMLR, 2018, pp. 4095‚Äì4104.
Hanxiao Liu, Karen Simonyan, Yiming Yang, Darts: Differentiable architecture search, in: International Conference on Learning Representations, 2018.
Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, Han-qing Lu, Recent advances in efficient computation of deep convolutional neural networks, Front. Inf. Technol. Electron. Eng. 19 (1) (2018) 64‚Äì77.
