Egyptian Informatics Journal 21 (2020) 159–167








Full length article
Gender identification for Egyptian Arabic dialect in twitter using deep learning models
Shereen ElSayed ⇑, Mona Farouk
Cairo University, Faculty of Engineering, Egypt



a r t i c l e  i n f o 

Article history:
Received 26 June 2019
Revised 27 October 2019
Accepted 23 April 2020
Available online 21 May 2020

Keywords:
Gender identification
Egyptian Arabic text classification Deep learning
Natural language processing Social Media analysis and mining
a b s t r a c t 

Although the number of Arabic language writers in social media is increasing, the research work targeting Author Profiling (AP) is at the initial development phase. This paper investigates Gender Identification (GI) (male or female) of authors posting Egyptian dialect tweets using Neural Networks (NN) models. Various architectures of NN are explored with extensive parameters’ selection such as simple Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Long–Short Term Memory (LSTM), Convolutional Bidirectional Long-Short Term Memory (C-Bi-LSTM) and Convolutional Bidirectional Gated Recurrent Units (C-Bi-GRU) NN which is tuned for the GI problem at hand. The best acquired GI accuracy using C-Bi-GRU multichannel model is 91.37%. It is worth noting that the presence of the bidi- rectional layer as well as the convolutional layer in the NN models has significantly enhanced the GI accuracy.
© 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelli-
gence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

The world is evolving now around social media and its signifi- cance to our daily life. Although many individuals use social media to connect with people, others use bogus accounts to commit cyber stalking, intimidation and violence. Around 30% of cyber harassers’ gender is unknown*. Although author profiling through English text has been extensively investigated in the literature, Arabic- based author profiling in general, and GI in specific, is under studied.
PAN** (a community that investigated text based tasks) started to add Arabic in their GI task dataset in 2017. The approaches investigated in PAN GI tasks were not specific for Arabic language in addition to the low GI accuracies achieved by their proposed solutions. There is a scarcity in the work that targets GI for Egyp- tian dialect in specific. Previous work in GI investigated several

* Corresponding author.
E-mail addresses: shereenhussein@rocketmail.com (S. ElSayed), mona_farou- k@eng.cu.edu.eg (M. Farouk).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
architectures such as CNN, GRU, LSTM, attention layers.. .etc. This work builds a state of the art NN model for the GI in Egyptian dia- lect. The model aims to achieve higher GI accuracy over multiple Egyptian dialect datasets.
In this paper, the GI problem is investigated on a labelled Egyp- tian dialect dataset obtained from Twitter. The main research ques- tions considered are: i) What is the best NN model that achieves the highest GI accuracy through the Egyptian dialect tweets? ii) What is the key component/layer in the NN model for the problem at hand? Multiple NN models were studied with architectural lay- ers that have been previously employed for text classification problems in general, and for the GI problems in specific. Examples of the NN architectures are: i) Convolutional Neural Network (CNN), ii) Long-Short Term Memory (LSTM), iii) Bidirectional Long-Short Term Memory (Bi-LSTM), iv) Convolutional Bidirec- tional Gated Recurrent Unit (C-Bi-GRU), and finally, v) a simple NN architecture with fully connected layers. The rest of the paper is organized as follows: Section 2 represents the related work on GI problem considering Arabic language. Next, Section 3 introduces the datasets used in learning and testing phases. Additionally, Sec- tion 3 explains in details the conducted neural network models for the GI. Section 4 illustrates the experiment as well as the results for the GI, when EDGAD and PAN AP’17 datasets are considered. Sec- tion 5 provides an analysis of the results obtained for the proposed NN models in comparison with the state-of-the-art competitors. Finally, concluding remarks are drawn in Section 6.



https://doi.org/10.1016/j.eij.2020.04.001
1110-8665/© 2020 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



Related work

Hussein, S. et al. [7] proposed a classical machine learning approach for the GI problem considering Egyptian dialect. The authors worked on extracting gender features from the text as semantic features. Their proposed model considered two feature vectors; i) A mixed feature vector combining embedding vector representation and semantic features employed with Random For- est classifier, and ii) N-gram feature vector used with Logistic Regression classifier. They achieved 87.6% accuracy for the GI prob- lem over a labeled dataset they retrieved from Twitter for Egyptian dialect tweets.
Bsir, B. and Zrigui, M. [2] used NN model with GRU to identify the gender of authors from Facebook’s posts and Twitter. The input to the NN model is preprocessed and split into two: embedding layer and stylometric features extraction phases. The output of the embedding layer is connected to a bidirectional GRU layer then
to an activation layer. On the other hand, the stylometric features are normalized and connected directly to the same activation layer used after the GRU layer. They compared their results with the best results by Basile et al. [1] in PAN’ AP (2017). The achieved result is less than the best result in PAN’AP (2017).
Estruch et al. [8] enhanced an early fusion model, which was based on performing fusion after the decision level single source classification. The authors achieved 91% GI accuracy on an English dataset in Singapore retrieved from Foursquare, Instagram and Twitter.
Kodiyan et al. [11] investigated gender detection for different languages with multiple dialects, such as English, Spanish, Por- tuguese and Arabic. They investigated the validity of CNN, and the bidirectional Recurrent Neural Network (RNN) with GRU. It turns out that RNN with GRU outperforms CNN in terms of accu- racy. As a result, they achieved 79.03%, 72.57%, 79.5%, and 71.58% GI accuracy for English, Spanish, Portuguese, and Arabic, respectively.
Franco-Salvador et al. [9] used the same dataset considered by Kodiyan et al. [11] to identify the language and author gender as well. The authors used skip-gram model [4] for word embedding exploiting the words’ morphology by means of character N-gram embedding. Their architecture was a Deep Average Network (DAN) consisting of multiple hidden layers with Rectified Linear Unit (ReLU) function. They achieved 76.6%, 79.6%, 76.9%, and 77.2% GI accuracy for Arabic, English, Portuguese, and Spanish, lan- guages respectively. The approach of N-gram embedding under- performed when considered in conjunction with word embedding model.
Miura et al. [13] proposed a model for word and character embedding. They considered RNN for the word embedding case, while CNN for character embedding case. Additionally, the authors added multiple attention layers. The embedding layer was initial- ized by a model that was trained through Twitter dataset. For the GI task, the average achieved accuracy for dialectical Arabic was 79.17%.
Deep learning usually required a massive amount of data for training. As a result, Veenhoven et al. [22] translated the gold stan- dard data to the language of interest. Additionally, using PAN-AP’ 18 dataset the authors used Bi-LSTM and CNN architectures to solve the GI problem. However, when the RNN was considered, the highest GI accuracy obtained was 79.3%, 80.4%, and 74.9%, for English, Spanish and Arabic languages, respectively.
Ranjan, S. et al. [17] proposed an approach for identifying fake user accounts in Twitter. They used multiple features, such as post- ings (text, image, video.. .etc.), followed accounts, followers’ accounts, replies, retweets, media contents, tagging, likes, direct messages and URLs. They used graph-based and machine learning methods to identify the fake accounts by extracting their features.
Materials and methods

Datasets

The dataset used in this work consists of Egyptian dialect tweets. There are two datasets that have been considered, Egyptian Dialect Gender Annotated Dataset (EDGAD) from [7] and PAN (part of CLEF – conference of labs and evaluation forum) Author Profiling task 2017 (PAN AP’17).

Edgad
EDGAD has been compiled by [7] for the purpose of research on gender identification problem using Egyptian dialect. The dataset consists of Egyptian dialect tweets collected from 70 public Twitter accounts per gender. The public accounts included active social media influencers and famous people with native Egyptian dialect. For each account, 1000 tweets were collected. The tweets of each account are stored in a separate file with a unique identifier. Some of the balanced EDGAD statistics are represented in Table 1. For more details on the dataset, the interested readers can refer to [7].

Pan AP’17 dataset
The PAN Author Profiling in 2017 (PAN AP’17) dataset was com- piled by [15] for the purpose of author profiling considering multi- ple languages with different dialects. This dataset consists of 4 Arabic dialects tweets: Egyptian, Gulf, Levantine and Moroccan. There are 600 authors for each dialect, with 100 tweets per author. In this paper, the authors with Egyptian dialect tweets will only be considered.

Deep learning models

Deep learning has proven its success in multiple Natural Lan- guage Processing (NLP) problems with different models and archi- tectures. In [3] the authors proposed NN-based models to solve the GI problem. The models were designed based on a mixture of building units that achieved good performance in several text clas- sification tasks. For the GI problem, several architectures such as CNN, LSTM have been studied in the literature and have shown great promise in solving the problem at hand. This work first starts by the simple NN models and utilizes them to solve the GI prob- lem. Afterwards, a level of sophistication is added to the NN mod- els by combining different layers from several NN models in order to achieve higher GI accuracy. In this section, a set of NN architec- tures that were investigated in this work will be presented. For each NN architecture, the parameters have been fine-tuned to opti- mize the GI accuracy.
The optimal parameters were identified for the NN architec- tures by conducting experiments with different number of layers, number of nodes per layer, activation functions, DropOut (DO) probability and arrangements of layers.
In the following subsections, first, a brief illustration about the layers types is given. Afterwards, various NN models are built by employing these layers.



Table 1
EDGAD Statistics from Hussein, S. et al. [7].




Layers
Embedding layer. The embedding layer creates a represen- tation of the words in a vector form with a fixed dimension. Miyato et al. [14] discussed the usage of the embedding layer in compar- ison with one-hot vector representation. It turned out, that the embedding layer achieved improvements in the word representa- tion and created a model which is less prone to over-fitting in the learning phase. An embedding layer was added to all the mod- els as the first layer, where the vector dimension is set to 100. The input to this embedding layer is a single tweet transformed to a vector of word-identifiers. These word-identifiers are obtained by compiling a vocabulary dictionary, where each unique word is given a unique number. The output of this embedding layer, for each input tweet, is a two-dimensional matrix, where each word is represented by the corresponding embedding vector. The embedding layer operation is depicted in Fig. 1. The embedding layer is followed by a drop-out with probability of 0.15. The drop- out was shown to make the network less sensitive to specific weights of neurons. This, in turn, results in a network that is cap- able of a better generalization and is less prone to over-fitting [21]. An additional flattening layer is added after the embedding layer. The flattening layer is used to reshape the two- dimensional input matrix into a single vector. In Fig. 2, the embed- ding vectors of each word are concatenated to form a single vector representing the input tweet.
Convolutional 1-D layer. For every input tweet, the input to the convolutional layer is the output of the flatten layer which is a vector of length equals to the maximum number words in a tweet multiplied by the embedding vector size. Since the filter window is (4 × 1), the convolutional layer used is 1 dimension (1D). Hence, the convolutional layer can operate on each input tweet independently. The number of filters considered is 516 fil- ters, and the window size varies from 3 to 6 words. These param- eters are fine-tuned for Arabic language expressions, where the words correlation drops significantly beyond the considered win- dow sizes range. Several activation functions have been investi- gated; it turns out that tanh provides the best performance in terms of GI accuracy.

Max-Pooling layer. The max-pooling layer is required to down-sample the input representation, i.e., decreases its dimen- sionality and prevents over-fitting as has been reported by Col- lobert et al. [6]. The operation of this layer is illustrated in Fig. 3. The pool size range is chosen from 2 to 4. The pool size range is identified based on several experiments considering 2, 4, 6 and 8 pool sizes. The results show that no performance difference is observed when the pool size is changed from 4 to either 6 or 8. This means that using 6 or 8 pool sizes will add unnecessary redun- dancy. This layer is followed by a dropout layer with probability of 0.15.




Fig. 1. Embedding layer illustration



Fig. 2. Flattening layer illustration






Fig. 3. Maxpool functionality illustration.


Lstm. LSTM succeeds in keeping the contextual information of the inputs through different steps. It is widely used in text clas- sification problems, where the premise here is that the current word is highly dependent on the previous ones. LSTMs networks overcomes two main problems in neural networks’ learning, the vanishing gradients and exploding gradients. This layer consists of recurrently connecting blocks. Each block consists of three gates, an input, an output and a forget gate. The main purpose of these gates is to read, write and reset the memory block. The used recur- rent activation for the gates is Sigmoid and the activation function for the states (hidden and output states) is Scaled Exponential Lin- ear Unit (SELU).

Gru. The GRU is a modified version of the LSTM layer. It was initially introduced by [5]. It consists of two gates, a reset gate, and an update gate. Intuitively, the reset gate determines how to com- bine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. The used recurrent activation for the gates is Sigmoid and the activa- tion function for the states (hidden and output states) is Scaled Exponential Linear Unit (SELU).

Proposed Deep learning models
Model 1 - simple Artificial Neural Network (ANN). This is a model of a simple NN architecture that consists of fully connected layers without the recourse to complex layers such as convolu- tional or LSTM layers. It consists of an embedding layer as described in Section 3.2.1.1, followed by four fully connected layers containing 80, 40, 30, and 1 node(s), respectively. A DO was added with 0.15 dropping probability. The activation function for all the
fully connected layers is ReLU, except for the last layer where the sigmoid function is used instead. The simple ANN is depicted in Fig. 4. For all of the NN architectures, the learning phase is divided into batches, where in each batch, 800 tweets are entered.

Model 2 - CNN. In this model the embedding layer is fol- lowed by a convolutional 1D layer. The window size is 3, the num- ber of filters is 516, and the activation function is tanh. Afterwards, a max pooling layer with pool size 4 is added. Next, a set of two dense layers containing 30 nodes, and ReLU activation function are added. Additionally, a DO operation is performed with 0.15 dropping probability. The last layer is a dense layer with one node and sigmoid activation function. The overall layers diagram is depicted in Fig. 5.

Model 3 - Multichannel CNN. This model consists of three parallel group of layers, also called channels, having different layer parameters as shown in Fig. 6. Each group includes an embedding layer, convolutional layer, and max-pooling layer. The number of filters in the convolutional layers is 516. The window sizes are cho- sen as 2, 4 and 8 for each of the three parallel groups, respectively. The activation function is tanh and the DO operation is performed with 0.15 probability. The pool size considered for the max pooling layer is 4. The outputs of these channels are concatenated and fed into two dense layers with 30 nodes and ReLU activation function. The last layer is a dense layer with one node and sigmoid activation function.

Model 4 - Recurrent NN – Single channel LSTM. As shown in Fig. 7, the first layer is the embedding layer followed by two LSTM layers. Each of the LSTM layers includes 70 nodes and tanh activa- tion function. Next, a dense layer with 100 nodes is added. After- wards, another dense layer with 70 nodes is placed. The last layer is a dense layer with one node and sigmoid activation function.

Model 5 - convolutional bidirectional LSTM NN. This model consists of a single channel including the embedding layer, convo- lutional 1D layer with window size of 6, and finally a max pooling layer with pool size of 4 as shown in Fig. 8. Next, a bidirectional


	

Fig. 4. Model 1- The simple ANN.	Fig. 5. Model 2- CNN.












Fig. 6. Model 3- Multichannel CNN.


Fig. 7. Model 4- LSTM NN.


LSTM layer is added with 40 nodes. Afterwards, two dense layers are added consisting of 40 nodes and employing the ReLU activa- tion function. The output layer for this model is similar to the one illustrated in the previous models.

Model 6 - Multichannel convolutional bidirectional GRU
NN. This model consists of three channels as shown in Fig. 9. Each channel starts with an embedding layer, followed by a convolu- tional layer with 32 filters, DO operation and max-pooling of size
2. The kernel sizes for the three channels are 2, 4 and 8, respec- tively. The values for the pool size and the kernel sizes are realized after several experiments to achieve the highest GI accuracy for the model. After that, for each channel, there is a bidirectional GRU layer with 40 nodes. Next, the output of the three channels is con-
Fig. 8. Model 5 - C-Bi-LSTM NN.


catenated and fed to a set of two dense layers with 30, and 20 nodes respectively. The dense layers employed the ReLU activation function. The output layer for this model is similar to the one illus- trated in the previous models.


Results

In this section, the experiments using the explained models in the previous section are conducted and analyzed. First, the


Fig. 9. Model 6 - Multichannel C-Bi-GRU NN.



different models are investigated with various parameters. The tuned parameters are the embedding layer vector size, number of layers, number of nodes per layer, and the activation function per layer. Afterwards, we investigated the types of the NN layers. The values of the parameters used in all the models are optimized to achieve the highest GI accuracy by conducting a wide range of experiments. In the next subsections, the investigations conducted over the hyper-parameters tuning will be illustrated. Additionally, the performance of the NN models is evaluated using multiple input sizes on both EDGAD and PAN AP’17 Egyptian dialect dataset.

Hyper-Parameters tuning

Embedding vector size
We consider a single channel CNN model with a set of four con- catenated tweets as input. The maximum tweet length is 50 char- acters. The experiments are carried on EDGAD. The results are shown in Table 2, note that using vector size of 100 for words rep- resentation has led to the best performance. Additionally, it is worth noting that this conclusion is extendable regardless of the NN model.

Drop-Out
The DO probability used in all the models is 0.15. This probabil- ity is achieved after investigating the range [0.05, 0.5]. The exper- iments are held on the ANN model while fixing all the parameters
i.e. number of layers, number of nodes per layer and activation functions. Additionally, the input tweet is a single tweet with max- imum length of 50 characters. The GI accuracy with DO probabili- ties 0.05 and 0.1 is in range 70.1% +/-0.3%. In the experiment with
the DO probabilities in the range [0.2, 0.5], the GI accuracy dropped to 68.3% +/-0.5%. The highest GI accuracy achieved using the ANN model with DO probability 0.15 is 71.05% +/-0.3% while fixing all the other parameters.

Convolutional layer
The window sizes for the convolutional layer and the max- pooling are investigated over the CNN model. For the convolutional layer, the window size is in the range of [2,8]. Higher values of win- dow size are investigated, but the GI accuracy did not show any significant change. The filter sizes are experimented in the range [12, 1024], it is noticed that small values (from 12 to 100) for the filters are not enhancing the accuracy. However, higher values (from 500 to 1024) achieved a noticeable enhancement in the GI accuracy on average 2%. The final value for the filter size is chosen as 516 as it was proved by experiments that higher numbers of fil- ters don’t cause increase in the GI accuracy.

Models evaluation

Edgad
First, preprocessing for input data is carried out as follows:

The non-Arabic characters (English characters, digits, special characters and links) are removed.
A whitespace between emojis is added to help the tokenizer in
separating and extracting emojis types and counts.
Tweets less than five words are discarded along with the dupli- cate tweets.




Table 2
Embedding vector size versus the GI accuracy on EDGAD using single channel CNN, 4 concatenated tweets and 50 maximum length.




Table 3
Model 1 - ANN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Table 4
Model 2 - CNN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Table 5
Model 3 – Multi-channel CNN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Table 6
Model 4 - LSTM NN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Table 7
Model 5 - Convolutional Bidirectional LSTM NN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Table 8
Model 6 - Multichannel Convolutional Bidirectional GRU NN accuracies for GI on EDGAD using multiple input lengths and number of concatenated tweets.



Several experiments are carried out for the NN models in Sec- tion 3.2.2 using different number of concatenated input tweets (single, 4, 8 and 12) as input. Additionally, the maximum number of characters in the set of concatenated tweets belongs to (50, 80, 100, 120 and 140). For all the models, Adam optimizer [10] and binary cross-entropy loss function are used. Additionally, the training phase consists of 8 epochs. The validation methodology used is cross-validation with K = 7, where the accounts used in learning are not used in testing. The GI accuracy results for the six models are represented in Tables 3, 4, 5, 6, 7 and 8. Table 9.

PAN AP’17 Egyptian dataset
Similar to EDGAD, the same preprocessing steps are applied on the PAN AP’17 dataset. The experiments results are represented in Table 8. Note that model 6, the multichannel convolutional bidirec- tional GRU NN, outperforms the other competitor NN models.

Discussion

By analyzing the results of the experiments performed using different NN models, different number of concatenated tweets, dif- ferent maximum characters length, and the following observations are drawn: i) Since our problem is a text classification problem, LSTM was expected to achieve high accuracy. However, low GI accuracy is achieved using LSTM alone as in model 4. The reason for the low accuracy is that LSTM requires a large amount of data in text classification problems in general which is not fulfilled with EDGAD or PAN datasets. ii) The importance of the bi-direction is emphasized when added to the LSTM, the GI accuracy is dramati- cally improved from model 4 to 5 by 37.3%. iii) The convolutional layer for text classification is proven to be extremely effective in the GI problem as achieved by model 2, where CNN achieved 88.6% for 12 concatenated tweets and 140 input length. iv) The multichannel models (3 and 6) show that combining the output of parallel convolutional layers with different window sizes enhance the GI accuracy. v) The GI accuracy is slightly improved
by adding GRU layer to the existing multichannel CNN model, i.e., model 6. vi) Finally, the multichannel convolutional bidirec- tional GRU model achieves the highest accuracy in GI using EDGAD, which is 91.37% for 12 concatenated tweets and 140 input length.
The best model’s accuracy was compared with three competing approaches. i) [7] used the EDGAD, they used classical machine learning approaches to achieve 87.6% accuracy, where the pro- posed deep learning model (Multichannel C-Bi-GRU NN) surpassed it by 3.77% as illustrated in table 10. ii) The GI accuracy in PAN AP’17 using their dialectical Arabic dataset is 80.06% and 0.82% by classical machine learning and deep learning respectively. The GI accuracy achieved using Multichannel C-Bi-GRU model on the Egyptian dialect subset of their dataset is 83.7%. iii) The models proposed by Sboev et al. [18] are also investigated, their first model which was based on convolutional layers, achieved 68.6% in GI on EDGAD. The second model [18], which was based on bidirectional LSTM layers with convolutional layers in between, achieved GI accuracy of 74.3% on EDGAD.
Other approaches were investigated such as i) Schaetti [19] and Franco-Salvador et al. [9] who used character embeddings, charac- ter embedding is investigated with the proposed CNN model and achieved GI accuracy less than the word embedding accuracy by 4.2% on average. Word embedding in Arabic outperformed the character embedding in GI. ii) Kodiyan et al. [11] experimented the bidirectional RNN with GRU combined with attention mecha- nism. However, during the design process of the proposed models, the GRU was experimented with the attention mechanism, it achieved lower accuracy than the Multichannel C-Bi-GRU NN (model 6). iii) Sezerer et al. [20] used the attention layer with a CNN model. The attention layer did not achieve better GI accuracy when investigated with the models proposed by this work. iv) Veenhoven et al. [22] worked on translated data across different languages. The concept of using translated data could introduce noise to the system, as well as incorrect data translation across dif- ferent dialects. This methodology cannot be used with dialectical languages. However, it helps in systems that work with MSA.


Table 9
GI percentage accuracies for all the models on PAN AP’17 Egyptian dialect using different number of concatenated tweets (single, 4, 8 and 12) with input size 140.



Table 10
Comparison between the proposed C-Bi-GRU NN model (model 6) with the best GI accuracy obtained by Hussein, S. et al., [7] and Basile et al., [1] on EDGAD and PAN’AP 2017 Arabic datasets.


EDGAD	PAN’AP 2017


Hussein, S. et. al. [7]	87.6%	N/A
Basile et al., [1]	N/A	0.82%
Our C-Bi-GRU NN (model 6)	91.37%	83.7%.





The tweets and their corresponding gender classification are analyzed. It was observed that tweets that are very obvious for human to belong to a certain class are mostly correctly classified by all the proposed models. An example of such tweets is ‘‘ﺿﻮﺍﻓﺮﻱ ”ﺧﺎﻟﺺ ﻛﺪﻩ ﻗﺪﺍﻣﻲ ﺑﺘﻜﺒﺮ ﻫﻲ ﻭ ﺑﻴﻬﺎ ﻣﺒﺴﻮﻃﻪ ﻭ ﺗﻄﻮﻝ ﺍﺳﻴﺒﻬﺎ ﻣﺮﻩ ﻷﻭﻝ (For the first time I leave my nails to grow and I am so happy to see it growing in front of me) which is correctly classified as female by all the models. On the other hand, tweets that seem vague for human that he can’t correctly identify the gender of their author are mostly misclassified by the models but can be exclusively classified by the C-Bi-GRU. An example of such tweets is ‘‘ﻑ ﺳﻤﻚ ﻓﻴﻪ ﺑﻴﺒﻘﻲ ﺍﻟﻲ ﺍﻟﻴﻮﻡ ”ﺣﻴﺎﺗﻲ ﺍﻳﺎﻡ ﺃﺳﻮﺃ ﻣﻦ ﺑﻴﺒﻘﻲ ﺩﻩ ﺍﻟﺒﻴﺖ (The day where there is fish in the home, is one of the worst days in my life). This tweet is correctly classified as female by C-Bi-GRU model.
The time of running the models increases as the model’s com- plexity increases. As an example for this, the C-Bi-GRU and the C-Bi-LSTM models took around one hour to run over the Google Colab’s GPU for the single tweets experiments. However, there is another factor for the time complexity, as the number of inputs decrease, but their size increase due to concatenation of tweets together, the time required to run the experiment almost decreased to 15 min.
Since the work in this field is still in its initial phases, the exper- iments can be extended to investigate the images posted by each account. The input vector for the NN models can be extended to include language specific features for GI i.e. usage of female suf- fixes and prefixes.


Conclusion

In this paper, a solution for the GI problem is proposed and experimented using an Egyptian dialect labelled dataset retrieved from Twitter. Six NN architectures are built and evaluated in terms of GI accuracy. The addition of the convolutional layer and the bidi- rectional LSTM/GRU layer has significantly improved the GI accu- racy. Finally, the C-Bi-GRU model achieves the highest performance for EDGAD and PAN’AP 17, where the GI accuracies are up to 91.37% and 83.7%, respectively.


Future work

There are many directions for expanding the work in this open research topic which include:
Creating a generic solution for gender identification for all the Arabic dialects.
Using the posted images in the detection of the author’s gender
as experimented by
Expanding the work to more author related tasks, such as author’s age, identification and obfuscation.



References

Basile, A., Dwyer, G., Medvedeva, M., Rawee, J., Haagsma, H. and Nissim. M., N- GrAM: New Groningen Author-profiling Model—Notebook for PAN at CLEF 2017. In Linda Cappellato, Nicola Ferro, Lorraine Goeuriot, and Thomas Mandl, editors, CLEF 2017 Evaluation Labs and Workshop – Working Notes Papers, 11-14 September, Dublin, Ireland, September; 2017. CEUR-WS.org. ISSN 1613-0073.
Basir, B., Zrigui, M., Enhancing Deep Learning Gender Identification with Gated Recurrent Units Architecture in Social Text. In Computación y Sistemas (An international journal for computing science and applications). Vol. 22, No. 3; 2018, pp. 757–766. doi: 10.13053/CyS-22-3-3036. Available from: http:// www.cys.cic.ipn.mx/ojs/index.php/CyS/article/view/3036.
Belinkov, Y., Glass, J., A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects. In: The Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial). Osaka, Japan, December; 2016. Available from: https://arxiv.org/abs/1609.07568.
Bojanowski, P., Grave, E. and Joulin, A. and Mikolov, T., Enriching Word Vectors with Subword Information. In: arXiv: 1607.04606; 2016. Available from: https://arxiv.org/abs/1607.04606.
Cho, K. Merriënboer, B., Gulcehre, C., Bougares, F., Schwenk, H., Bahdanau, D., Bengio, Y., 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In: Empirical Methods in Natural Language Processing. Doha, Qatar, October 25-29, pp. 1724-1734. Available from: https://arxiv.org/abs/1406.1078.
Collobert, R. Weston, J. Bottou, L. Karlen, M. Kavukcuoglu, K. Kuksa, P., Natural Language Processing (Almost) from Scratch. J Mach Learn Res 2011; 12:1532- 4435, pp: 2493-2537. Available from: http://www.jmlr.org/papers/ volume12/collobert11a/collobert11a.pdf.
Hussein, S., Farouk, M. Hemayed, E., Gender identification of egyptian dialect in twitter. Egypt Informat J; 2019;20:109–116. Doi: 10.1016/j.eij.2018.12.002.
Estruch, C. P., Paredes, R. Rosso, P., Learning Multimodal Gender Profile using Neural Networks. Rec Adv Nat Language Process 2017; Varna: Bulgaria, pp: 577-582. Available from: http://users.dsic.upv.es/~prosso/resources/ PerezEtAl_RANLP17.pdf.
Franco-Salvador, M. Plotnikova, N. Pawar, N. Benajiba, Y., Subword-based Deep Averaging Networks for Author Profiling in Social Media. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2017 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2017: 1866. Available from: http://ceur-ws.org/Vol-1866/paper_192.pdf
Kingma, D., Ba, J., 2014. Adam: {A} Method for Stochastic Optimization. In: International Conference on Learning Representations. Available from: http:// arxiv.org/abs/1412.6980.
D. Kodiyan F. Hardegger S. Neuhaus M. Cieliebak. Author Profiling with Bidirectional RNNs using Attention with GRUs L. Cappellato N. Ferro J.L. Nie L. Soulier CLEF; 2017 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org, vol. 1866. Available from: http://ceur-ws.org/Vol- 1866/paper_77.pdf.
Miura, Y., Taniguchi, T., Taniguchi, M., Ohkuma, T., Author Profiling with Word
+Character Neural Attention Network In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2017 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings; 2017. CEUR-WS.org, vol. 1866. Available from: http:// ceur-ws.org/Vol-1866/.
Miyato, T. Dai, A., Goodfellow, I., Adversarial Training Methods for Semi- Supervised Text Classification. In: 5th International Conference on Learning Representations, arXiv: 1607.04606. 2017; Toulon: France. Available from: https://arxiv.org/abs/1605.07725.
Rangel, F., Rosso, P., Potthast, M., Stein, B., Overview of the 5th Author Profiling Task at PAN 2017: Gender and Language Variety Identification in Twitter. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2017 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2017. Vol. 1866. Available from: http://ceur-ws.org/Vol-1866/.



Sahoo SR, Gupta BB. Hybrid approach for detection of malicious profiles in twitter. Comput Electr Eng 2019;76:65–81. doi: https://doi.org/10.1016/ j.compeleceng.2019.03.003.
Sboev, A. Moloshnikov, I. Gudovskikh, D. Selivanov, A. Rybka, R. Litvinova, T. Automatic Gender Identification of Author of Russian Text by Machine Learning and Neural Net Algorithms in Case of Gender Deception. In: 8th Annual International Conference on Biologically Inspired Cognitive Architectures. 2018; 123: 417-423. Available from: https:// www.sciencedirect.com/science/article/pii/S1877050918300656.
Schaetti, N., Character-based Convolutional Neural Network and ResNet18 for Twitter Author Profiling. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2018 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2018: vol. 2125. Available from: http://ceur-ws. org/Vol-2125/paper_100.pdf.
Sezerer, E. Polatbilek, O. Sevgili, O. Tekir, S. Gender Prediction From Tweets With Convolutional Neural Networks. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2018 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2018: vol. 2125. Available from: http:// ceur-ws.org/Vol-2125/paper_116.pdf.
N. Srivastava G. Hinton A. Krizhevsky I. Sutskever R. Salakhutdinov Dropout: A Simple Way to Prevent Neural Networks from Overfitting. J Mach Learn Res
2014; 15: 1929-1958 Available from http://jmlr.org/papers/volume15/ srivastava14a.old/srivastava14a.pdf.
Veenhoven, R. Snijders, S. Hall, G. Noord, R. Using Translated Data to Improve Deep Learning Author Profiling Models. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2018 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2018: vol. 2125. Available from: http:// ceur-ws.org/Vol-2125/paper_178.pdf.

Further reading

Lotan, G. Graeff, E. Ananny, M. Gaffney, D. Pearce, I., The Arab Spring the revolutions were tweeted: Information flows during the 2011 Tunisian and Egyptian revolutions Int J Commun; 2017. Available fromhttps://www. researchgate.net/publication/235354320_The_Revolutions_Were_Tweeted_ Information_Flows_During_the_2011_Tunisian_and_Egyptian_Revolutions.
Rangel F., Rosso P., Montes-y-Gmez M., Potthast M., Stein B., Overview of the 6th Author Profiling Task at PAN 2018: Multimodal Gender Identification in Twitter. In: Cappellato L., Ferro N., Nie J.L., Soulier L. (Eds.) CLEF 2017 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings. CEUR-WS.org; 2018: vol. 2125. Available from: http://ceur-ws.org/Vol-2125/.
