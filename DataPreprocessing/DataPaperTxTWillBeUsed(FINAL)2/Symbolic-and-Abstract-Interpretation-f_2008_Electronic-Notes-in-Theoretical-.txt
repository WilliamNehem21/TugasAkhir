	Electronic Notes in Theoretical Computer Science 217 (2008) 113–131	
www.elsevier.com/locate/entcs

Symbolic and Abstract Interpretation for C/C++ Programs
Helge L¨oding2 ,4
GESy Graduate School of Embedded Systems University of Bremen
and Verified Systems International GmbH Germany
Jan Peleska1 ,3
Centre of Information Technology University of Bremen
Germany

Abstract
We present a construction technique for abstract interpretations which is generic in the choice of data ab- stractions. The technique is specialised on C/C++ code, internally represented by the GIMPLE control flow graph as generated by the gcc compiler. The generic interpreter handles program transitions in a symbolic way, while recording a history of symbolic memory valuations. An abstract interpreter is instantiated by selecting appropriate lattices for the data types under consideration. This selection induces an instance of the generic transition relation. All resulting abstract interpretations can handle pointer arithmetic, type casts, unions and the aliasing problems involved. It is illustrated how switching between abstractions can improve the efficiency of the verification process. The concepts described in this paper are implemented in the test automation and static analysis tool RT-Tester which is used for the verification of embedded systems in the fields of avionics, railways and automotive control.
Keywords: automated testing, static analysis, abstract interpretation, Galois connections


Introduction
Objectives and Overview
Concrete and abstract interpretation are core mechanisms for automated static analysis, test case/test data generation and property checking of software: The

1 Email:jp@tzi.de
2 Email: hloeding@tzi.de
3 Partially supported by the BIG Bremer Investitions-Gesellschaft under research grant 2INNO1015B
4 Supported by a research grant of the Graduate School in Embedded Systems GESy http://www.gesy.info

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.06.045

concrete interpretation helps to explore program (component) behaviour with con- crete data values without having to compile, link and execute the program on the target platform. The abstract interpretation reduces the complexity of verification goals or, more general, reachability problems, by abstracting from details which are unnecessary for the goal under consideration.
Consider the building blocks typically present in tools supporting test automa- tion, static analysis and/or property checking as shown in Fig. 1: The program code to be analysed or a specification model are transformed into a uniform intermedi-
ate model representation (IMR) which is independent of the concrete SUT code or
specification syntax. This reduces the dependencies between concrete syntax and
analysis algorithms. Most of the problems arising in automated test case/test data generation, static analysis and property verification can be paraphrased as reacha- bility problems, as has been pointed out in [10]. Therefore a path selector performs
a choice of potential paths through the model to be checked with respect to feasi-
bility: The goal is solved if concrete input data can be found so that the software component under analysis executes along one of the suggested paths. While the
general reachability problem is undecidable, concrete goals can often be realised in a highly efficient way. To this end, the constraint generator constructs a collection of constraints to be met in order to provoke an execution along the selected paths. The construction requires a symbolic interpreter, a tool component for collecting the guard conditions along the selected paths. With a sufficient collection of con- straints at hand, the constraint solver tries to construct concrete data solving the constraints or to prove their infeasibility.
The choice of the abstract interpretation technique considerably influences the efficiency of automated solvers used for these purposes: For proving that a constraint
collection can never be satisfied it is often more efficient to show this for an ab- stracted program version, so that this also implies infeasibility for the concrete pro- gram. Conversely, some abstractions are especially useful for under-approximating the solution set of the constraints given, so that any data vector of this approxima- tion represents a solution.
In this paper we focus on interpreters for C/C++ programs. For this task it is necessary to capture all “side effects” of aliasing, pointer arithmetic, type casts and unions possibly occurring in C/C++ software, so that no hidden effects of in- structions on the valuation of symbols not occurring in the statement are missed during the interpretation process. We first present operational rules for a concrete semantics covering these aspects (Section 3). Next we observe that for a given col- lection of constraints, the efficiency of the solver strongly depends on the choice of abstraction. As a consequence it is desirable to switch abstractions for one and the same data type during the interpretation while still ensuring the correctness of the
interpretation results. This objective is met by means of a symbolic interpreter for
C/C++ programs (Section 4): This tool component handles program transitions
in a symbolic way, while recording a history of symbolic memory valuations. The valuations are represented by memory addresses (these are necessary in order to cope with the aliasing problems), value expressions and application conditions: A











Test Data:
Input Assignment

Solution Set Approximation

Fig. 1. Building blocks of tools for test automation, static analysis and property verification.
memory item is only valid if a valuation of inputs can be found so that the appli- cation condition becomes true. Finally we describe how abstract interpreters can be constructed by instantiating the symbolic interpreter with lattices to be used for abstracting the data types involved (Section 5). As a consequence, the basic interpretation algorithm can be completely re-used for each choice of abstraction lattice, only functions for the valuation of expressions in the context of the selected lattices have to be added. In Section 6 an example is given which illustrates the mechanics and the effects of symbolic and abstract interpretation.
Background and Related Work
The full consideration of C/C++ aliasing situations with pointers, casts and unions is achieved at the price of lesser performance. In [4,2], for example, it is pointed out how more restrictive programming styles, particularly the avoidance of pointer arithmetics, can result in highly effective static analyses with very low rates of false alarms. Conversely it is pointed out in [14] that efficient checks of pointer arithmetics can be realised if only some aspects of correctness (absence of out- of-bounds array access) are investigated. As another alternative, efficient static analysis results for large general C-programs can be achieved if a higher number of false alarms (or alternatively, a suppression of potential failures) is acceptable [5], so that paths leading to potential failures can be identified more often on a syntactic basis without having to fall back on constraint solving methods.
On the level of binary program code verification impressive results have been achieved for certain real-world controller platforms, using explicit representation models [12]. These are, however, not transferable to the framework underlying our work, since the necessity to handle floating point and wide integer types (64 or 128 bit) forbids the explicit enumeration of potential input values and program variable states.
All techniques described in this paper are implemented in the RT-Tester tool

developed by the authors and their research group at the University of Bremen in cooperation with Verified Systems International GmbH [15]. In [10] we have moti- vated in more detail why testing, static analysis and property checking of software code should be considered as an integrated verification task, so integrated tool sup- port for these complementary aspects of software verification is highly desirable. The approach pursued with the RT-Tester tool differs from the strategies of other authors [4,2,14]: We advocate an approach where test and verification activities focus on small program units (a few functions or methods) and should be guided by the expertise of the development or verification specialists. Therefore the RT- Tester tool provides mechanisms for specifying preconditions about the expected or admissible input data for the unit under inspection as well as for semi-automated stub (“mock-object”) generation showing user-defined behaviour whenever invoked by the unit to be analysed. As a consequence, programmed units can be verified immediately and interactive support for bug-localisation and further investigation of potential failures is provided. The SMT constraint solver used in the tool is based on ideas described in [11,1,6].

Theoretical Foundations
Recall that a binary relation ± on a set L is called a (partial) order if ± is reflexive, transitive and anti-symmetric. An element y ∈ L is called an upper bound of X ⊆ L if x ± y holds for all x ∈ X. The lower bound of a set is defined dually. An upper bound y' of X is called a least upper bound of X and denoted by HX if y' ± y holds for all upper bounds y of X. Dually, the greatest lower bound HX of a set X is defined.
An ordered set (L, ±) is called a complete lattice, if HX and HX exist for all subsets X ⊆ L. Lattice L has a largest element (or top) denoted by T =def HL and a smallest element (or bottom) denoted by ⊥ =def HL. Least upper bounds and greatest lower bounds induce binary operations H, H : L × L → L by defining x H y =def H{x, y} (the join of x and y) and x H y =def H{x, y} (the meet of x and y), respectively. If the join and meet are well-defined for an ordered set (L, ±) but HX, HX do not exist for all X ⊆ L then (L, ±) is called an (incomplete) lattice.
From the collection of canonic ways to construct new lattices from existing ones
(L, ±), (L1, ±1), (L2, ±2), we need (1) cross products (L1 × L2, ±') where the partial order is defined by (x1, x2) ±' (y1, y2) if and only if x1 ±1 y1 Λ x2 ±2 y2 and (2) partial function spaces (V /→ L, ±') where f ±' g for f, g ∈ V /→ L if and only if dom f ⊆ dom g Λ (6x ∈ dom f : f (x) ± g(x)).
Mappings φ : (L1, ±1) → (L2, ±2) between ordered sets are called monotone if x ±1 y implies φ(x) ±2 φ(y) for all x, y ∈ L. Mappings φ : (L1, ±1) → (L2, ±2) between lattices are called homomorphisms if they respect meets and joins, that is, φ(x H1 y)= φ(x) H2 φ(y) and φ(x H1 y)= φ(x) H2 φ(y) for all x, y ∈ (L1, ±1). Since
x ±1 y implies x H1 y = y and x H1 y = x, homomorphisms are monotone.
A Galois connection (GC) between lattices (L1, ±1), (L2, ±2) is a tuple of map- pings ¢ : (L1, ±1) → (L2, ±2) (called right) and ¢ : (L2, ±2) → (L1, ±1) (called

left) such that a¢ ±2 b ⇔ a ±1 b¢ for all a ∈ L1,b ∈ L2. This defining property implies that Galois connections are monotone in both directions.
Given any transition system TS = (S, S0, −→) with state space S, initial states in S0 ⊆ S and transition relation −→⊆ S × S, the most fine-grained state space abstraction possible is represented by the power set lattice LP(S)= (P(S), ⊆) with join operation ∪ and meet ∩. We introduce an abstract interpretation semantics on LP(S) by turning it into a state transition system TSP = (LP(S), {S0}, −→P) by lifting the original transition relation to sets: Using Plotkin-style notation, this can be specified as
 6i ∈ I, si, s' ∈ S : si −→ s'
{si | i ∈ I} −→P {s' | i ∈ I}
Compared to the original transition system TS, this abstract interpretation
−→P introduces no loss of information, since its restriction to pairs of singleton sets is equivalent to the original transition relation:
6s1, s2 ∈ S : s1 −→ s2 ⇔ {s1} −→P {s2}
It is, however, an abstraction, since for transitions between states with cardinality higher than one, say {s1, s2,.. .} −→P {s' , s' ,.. .}, only the possible resulting states
1	2

are listed (s' , s' ,.. .) but the information whether, for example, s1 −→ s'
or s1 −→

1	2	1
' is no longer available.
Now, given any other transition system TSL = (L, L0, −→L) based on a lattice
(L, ±) we can check whether TSL is a valid abstract interpretation of TS by the aid of TSP and Galois connections:
Definition 2.1 Transition system TSL = (L, L0, −→L), based on a lattice (L, ±), is a valid abstract interpretation of TS = (S, S0, −→) if (i) there exists a Galois
¢

connection (P(S), ⊆) ←− (L, ±), (ii) the transition relation −→L
¢
is a valid abstract

relation the sense that 6a, a',b ∈ L : (a −→L a' Λ b ± a ⇒ ∃b' ∈ L : b −→L b' Λ b' ±
a'), (iii) the transition relation −→L satisfies 6(p, p') ∈−→P: ∃a' ∈ L : p¢ −→L
a' Λ p'¢ ± a' and (iv) the transition relation −→L satisfies 6(a, a') ∈−→L: ∃p' ∈
P(S): a¢ −→P p' Λ p' ⊆ a'¢.
The following theorem provides a “recipe” for constructing valid abstract inter- pretations, as soon as a GC according to Definition 2.1, (i) has been established:


¢
Theorem 2.2 Given lattice (L, ±) and Galois connection (P(S), ⊆) ←− (L, ±), de-
¢

ﬁne transition system TS  = (L, L , −→
) by (i) L
= {S }¢, (ii) p¢ ¢ −→Pp'
and

L	0	L
0	0	p¢ −→Lp'¢

a¢ −→Pp'
a−→Lp'¢
Then TSL
is a valid abstract interpretation of TS in the sense of

Deﬁnition 2.1.
For more details about lattices and GC and the proof of Theorem 2.2 the reader is referred to [3,9].

Control Flow Graphs and GIMPLE, Concrete Se- mantics
GIMPLE Programs
We use the gcc compiler to transform a given C/C++ program into GIMPLE code. As described in [7,8], this semantically equivalent representation of a program con- stitutes an intermediate transformation result from source to assembler, where all expressions appearing in statements contain at most one operator and (with the exception of function invocations) at most two operands. Operands may only be variable names or nested structure and array accesses (henceforth called selectors) as well as constant values. By introducing auxiliary variables, all original statements will be transformed to adhere to this requirement. Statements may therefore only be assignments from expressions to variables (or atomic selectors in the above sense). Casting and referencing/dereferencing of variables (or selectors) form expressions in themselves, and may therefore not be used as operands, but instead need to be executed as separate assignments to auxiliary variables. GIMPLE programs contain no loop constructs. Instead, all loops from the original source are transformed into conditional jumps to preceeding labelled statements. GIMPLE therefore contains only two different types of branching statements:
<if-else-stmt> ::= if ( <condition> ) goto <label>; else goto <label>;
<switch-stmt>  ::= switch ( <variable> ) { <cases>  <default>_opt }
<cases>	::= <cases>_opt case <value>: goto <label>;
<default>	::= default: goto <label>;

For the description of concrete GIMPLE semantics we encode each GIMPLE function as a control flow graphs (CFGs). Each function/method of a C/C++ program is associated with a CFG. Each CFG G has a distinguished initial node
I(G) corresponding to function entry and a terminal node O(G) corresponding to function return. Each CFG node is labelled with a single GIMPLE statement, each edge with a GIMPLE branching condition. For sequences of non-branching statements, the edges are labelled with true. Branching statements are represented as edges labelled with the applicable branching conditions, each edge pointing to the target node referenced in the goto <label> statement in the GIMPLE code.
The concrete operational semantics of a GIMPLE program P , represented by a collection of control flow graphs as described above, will now be explained by associating a transition system with P .

GIMPLE state space
For representing the semantics of GIMPLE programs P , we use the following class of transition systems TSG = (SG, S0, −→G). The program state space is defined as
SG = N (P ) × (Seg × N0 /→ Symbols) × (Seg × N0 /→ BY TE∗)
with typical element (n, ν, μ) ∈ SG. Set N (P ) comprises all nodes in the CFGs associated with any function of P . The second and third component of this Carte- sian product represent function spaces for address mappings and memory state: For modelling the association between variables, their aliases and their associated mem-

ory portions, we introduce (1) a partial function ν : Seg × N0 /→ Symbols mapping existing virtual addresses on the segment of type Seg = {stack, heap, global, code} to a symbol (variable or function) associated with this address and (2) a partial function μ : Seg × N0 /→ BY TE∗ associating with each existing virtual address a sequence of bytes, representing the current memory valuation of the given address. The set Symbols only contains the basic symbol names, that is, the name a of an array, but not the array element a[4] and the name of a structured variable x but
not the name of x.y.z[5] of a structure component. Component and array element identifiers are called selectors and comprised in a (possibly infinite) set Selectors which is a superset of Symbols, since each basic name is a selector, too.
The initial state of SG is S0 = {(I(f ), ν0, μ0)}, where I(f ) is the initial node of the CFG associated with the GIMPLE function of interest, ν0 contains all addresses of global variables and actual parameters used in the invocation of f () and μ0 contains the memory portions associated with these actual parameters and of all global variables, initialised according to the precondition on which the execution of f () should be based.

Auxiliary functions
For recording state changes in SG and determining the current state of variable valuations some auxiliary functions are needed.
Given an arbitrary selector, function β : Selectors → Symbols returns its base symbol, e.g. for β(x.y.z[5])= x. This will be required to retrieve base addresses for selectors by means of ν.
Since virtual addresses are unique across memory segments, a function νˆ :
Symbols /→ N0 mapping identifiers to their respective address is well-defined when taking scoping into account. For a given symbol that is defined both within the stack and global segments, νˆ will return the virtual address corresponding to the symbol definition within the stack.
νˆ can be extended to map from selectors to virtual base addresses to yield
ν− : Selectors /→ N0 with ν−(sel) =def νˆ(β(sel)).
Given an arbitrary selector, function ω : Selectors → N0 returns the bit offset of the selector’s memory location from its base address. The offset is measured in bits so that also operations on bitfields can be captured. This information is obviously platform-specific: ω is constructed from the size and alignment information provided
by the gcc compiler on the specific platform it is used. As with ν−, the appropriate memory segment for multiply defined base symbols is determined by first assessing symbol definitions within the stack segment.
Function τ : Selectors /→ Types returns the type for any given selector. The type information is then gained from the internal type data gathered by the gcc compiler. Again, scoping is taken into account.
Function τ may be extended to determine the type of a given expression form- ing τ ∗ : Expr /→ Types by taking (return) types of used operands and opera- tors into account. If a given selector corresponds to a pointer type, then function

→τ : Selectors /→ Types may be used to obtain its target type.
Function σ : Types → N0 is used to determine a given type’s size in bits.
The state space only records the current memory state as sequences of bytes. Function ι : BY TE∗ × Types /→ D is used to interpret a given sequence of bytes as a specific type. Here, D denotes the union of all atomic domains. It is only defined for byte sequences long enough to hold a value of given type. Conversely, we define ι− : D × Types /→ BY TE∗ to be the byte representation for a given value with known type. For these functions, the size of atomic types, encoding methods and the little or big endianess of the platform has to be determined. This information is retrieved from the gcc type- and debugging information.
For reading data from memory, we initially define ϵa : SG × N0 × N0 /→ BY TE∗. Function application ϵa((ν, μ), a, s) reads a bit sequence of a given length s beginning from a specified address a within the memory, and returns its contents as byte sequence. For this, we find the segment and base address (seg, abase) within dom(ν), for which byte sequence μ(seg, abase) encloses address a. If specified size s exceeds byte sequence μ(seg, abase) beginning from a, ϵa has to take direct successor byte sequences within seg into account to be defined. If size s is not a multiple of 8, the resulting byte sequence will be constructed by adding additional high order 0 bits until its bitsize reaches the next higher multiple of 8.
Using ϵa, we now construct a function to read raw byte data from memory using selectors. We define ϵs : SG × Selectors : BY TE∗ as
єs((ν, μ), sel)=	a	−

def є ((ν, μ),ν 
(sel)+ ω(sel), σ(τ (sel)))

We now define a function ϵe : SG × Expr /→ BY TE∗, which evaluates a given GIMPLE expression according to the current memory valuation. As GIMPLE ex- pressions contain at most one operator, we can do this by distinguishing different expression types. For expressions consisting of constant values or selectors, ϵe cor- responds to applications of ι− or ϵs respectively. Other types of expressions may be evaluated using one of the following definitions of ϵe:
Let ∈ {+, —, ∗, /, %, Λ, V, >, <, ≥, ≤, =, /=} be a binary arithmetic or boolean operator, and let exp =def exp1 exp2 be an application to two operand expressions. We define
єe((ν, μ), exp) =def
ι−(ι(єe((ν, μ), exp1),τ ∗(exp1))  ι(єe((ν, μ), exp2),τ ∗(exp2)),τ ∗(exp))
Concurrently, for an unary arithmetic or boolean operator  ∈ {+, —, !} and ex- pression exp =def  exp1 we define
єe((ν, μ), exp) =def ι−(  ι(єe((ν, μ), exp1),τ ∗(exp1)),τ ∗(exp))
For a bitwise operator ◦ ∈ {&, |, XOR} and expression exp =def exp1 ◦ exp2, the operation is performed on raw byte data, and we define
єe((ν, μ), exp) =def єe((ν, μ), exp1) ◦ єe((ν, μ), exp2)
For bitwise unary operator ~ and according expression exp =def ~ exp1, we define
єe((ν, μ), exp) =def ∼ єe((ν, μ), exp1)
For a shift operator Δ ∈ { , } and expression exp =def exp1 Δ exp2, the opera-

tion is performed as follows (note that exp2 must correspond to an integral type):
єe((ν, μ), exp) =def єe((ν, μ), exp1) Δ ι(єe((ν, μ), exp2),τ (exp2))
Dereferencing of a selector, exp =def ∗sel, may be evaluated using
єe((ν, μ), exp) =def єa((ν, μ), ι(єs((ν, μ), sel), N0), σ(→τ (sel)))
Conversely, referencing of a selector, exp =def &sel, is defined as
єe((ν, μ), exp) =def ι−(ν−(sel)+ ω(sel), N0)
For a cast expression exp =def (t)(exp1) with target type t, we define
єe((ν, μ), exp) =def ι−((t)C ι(єe((ν, μ), exp1),τ ∗(exp1)), t)
where cast operator ()C uses C cast operator semantics for atomic types t and
τ ∗(exp1).
For purposes of legibility, we henceforth denote ϵe by ϵ unless noted otherwise.
For specifying the effect of write operations on the memory, we use function
φ : (N0 × N0) × SG × BY TE∗ /→ (Seg × N0 /→ BY TE∗)
To begin with, function application φ((atgt, otgt), (ν, μ), valbyte) determines the target memory segment for target base address atgt and offset otgt. It then returns a new memory valuation μ', which differs from μ only in the new valuation of the target segment, starting at target base address atgt but unchanged before offset otgt. Starting at the offset, the memory is changed according to the byte sequence valbyte.
Transition relation: operational rules.
The operational rules specifying the transition relation —→G⊆ SG × SG on the GIMPLE state space are based on the control flow graph representation of each GIMPLE function. In Plotkin-style notation, each rule is of the form
g
n1 −→CF G n2, ι(є((ν, μ), g), int) /=0 
(n1, ν, μ) −→G (n2,ν', μ')
Informally speaking, a transition (n1, ν, μ) —→G (n2, ν', μ') is possible if (1) there exists an edge from n1 to n2 in the respective CFG, (2) the guard condition g associated with this edge evaluates to true (for C-like languages this means that it evaluates to an integral value not equal to zero) in the current valuation (ν, μ).
For each type of statement encoded in the nodes n1 it remains to define the effect
of this statement on (ν, μ), resulting in the new valuation state (ν', μ'). Below we
give some examples of detailed rule specifications.
The effect of a stack variable definition, n1 =def typex x;, is to allocate the required space on stack. The values, however, are still undefined. As a consequence the effect on the memory valuation can be specified by
'	'ν = ν ⊕ [(stack, ) '→ x]
where a' is a fresh address not occurring in dom ν (in fact, we use the proper offset of x from the base of the stack frame for building a'). The effect on the memory is
'	'
μ = μ ⊕ [(stack, a ) '→ ⟨?,. .. , ?⟩ ]
si|zeof{(ztype}x)
where “?” denotes that the byte values are still undefined.

The effect of an assignment to a selector, n1 =def sel = expr;, is to change the memory at the base address plus offset, as defined by the selector according to the expression valuation. As the left-hand and right-hand sides of the assignment need not necessarily be typed identically, we first construct the artificial cast ex-
pression expr' = (τ (sel))(expr). As we have now ensured expr' to be of the type corresponding to sel, we go on and assign
ν' = ν, μ' = φ((ν−(sel), ω(sel)), (ν, μ), є((ν, μ), expr'))
The effect of an assignment to a de-referenced selector, n1 =def ∗sel = expr;, is to change the memory at the address pointed to by sel according to the expression valuation and the pointer target type of the selector. We therefore need to calculate the target address atrg of the write operation first. This is done by evaluating atrg =
ι(ϵs((ν, μ), sel), N0). Again using an artificial cast expression expr' = (→τ (sel))(expr), we can now construct a new state space by assigning
ν' = ν, μ' = φ((atrg , 0), (ν, μ), є((ν, μ), expr'))
The effect of copying memory, n1 =def memcpy(trg, src, s);, is to copy s successive bytes starting with address src to the memory indicated by trg. This may be accomplished by defining
asrc = ι(є((ν, μ), src), N0)
atrg = ι(є((ν, μ), trg), N0)
to be the addresses specified in src and trg respectively. We can now construct
μ0 = φ((atrg, 0), (ν, μ), єa((ν, μ), asrc, 8))
.. .
μi = φ((atrg, 8 ∗ i), (ν, μi−1), єa((ν, μi−1), asrc +8 ∗ i, 8))
.. .
μs−1 = φ((atrg, 8 ∗ (s − 1)), (ν, μs−2), єa((ν, μs−2), asrc +8 ∗ (s − 2), 8))
and finally
ν' = ν
μ' = μs−1
The effect of a function invocation, n1 =def sel = f(x1,... , xn);, for a func- tion with prototype t f(t1 z1,...,tk zk) is calculated according to the following operational rule:

g	∗
n1 −→CF G n2, ι(є((ν, μ), g), int) /= 0, (I(Gf ), ν1, μ1) −→G (O(Gf ), ν2, μ2)
(n1, ν, μ) −→G (n2,ν', μ')
In this rule, ν1, μ1 are extensions of ν, μ which comprise the initial settings of the formal parameters and the return value:
ν1 = ν[(stack, a) '→ xReturn, (stack, a1) '→ z1,.. ., (stack, ak) '→ zk]
Here a, a1,... , ak are fresh address values and xReturn is an auxiliary name for the stack location storing the return value. The initial valuation of xReturn is

undefined, but the zi carry the valuation of their actual parameters xi:
μ1 = μ1

1
μ1 = φ((ν−
(z1), 0), (ν1, μ2), є((ν1, μ2), (t1)(x1)))

1	1
... 
i	−
1

i+1
1

i+1

μ1 = φ((ν1 (zi), 0), (ν1, μ1 ), є((ν1, μ1 ), (ti)(xi)))
... 

k	−	k+1
k+1

μ1 = φ((ν1 (zk), 0), (ν1, μ1  ), є((ν1, μ1  ), (tk)(xk)))
μk+1 = μ[(stack, a) '→ ⟨?, ... , ?⟩]
|size{ozf(t)}

Now the precondition (I(Gf ), ν1, μ1) —→∗
(O(Gf ), ν2, μ2) in the operational rule

above requires that a sequence of transitions through the CFG of f should exist, starting with valuation ν1, μ1, so that the final valuation before function return, ν2, μ2, defines the target state (n2, ν', μ') via
'	−
μ = φ((ν2 (sel), ω(sel)), (ν2, μ2), є((ν2, μ2), (τ (sel))(xReturn)))
Finally, the local variable addresses and associated memory valuations of f are removed from ν', μ'.

Symbolic Interpretation of GIMPLE-Programs
For symbolic interpretation the state space is defined as
SS = N (P ) × N0 × M
M = dataSegment × heapSegment × stackSegment dataSegment = M-Item∗
heapSegment = M-Item∗ stackSegment = stackFrame∗ stackFrame = M-Item∗
M-Item = N0 × (N0 ∪ {∞}) × BaseAddress ×
Types × Offset × Length × Value × Constraint BaseAddress = String
Offset = Length = Value = Constraint = Expr(SymbolsS ) SymbolsS = Symbols × N0
Each symbolic state consists of a triple (node, n, mem) where node is a node in the GIMPLE control flow graph representing the current “program counter state” of the symbolic execution, n serves as an instruction counter and mem is the current
history state of symbolic memory valuations, called memory items m ∈ mem. The
collection of memory items generated so far is structured according their allocation
in the data segment, heap or stack, respectively. The stack is further sub-divided into frames, so that the validity of stack variables during their associated function executions can be clearly specified.
The components of a memory item are accessed using m.v0, m.v1, m.a, m.t, m.o, m.l, m.val, m.c for the respective projections. Component m.a represents the base address of a memory item, typically denoted by &x if the memory location corresponds to a variable x or by a fictitious address symbol representing the start address of a dynamic memory allocation. Component m.o denotes the offset from the base address, where the value specified in m.val is written to. For writing one value m.val, the memory portion starting at m.a + m.o is used, and the length of

this portion is determined by the type information m.t. If the length specification
m.l is a multiple of sizeof(t) this specifies that m.l/sizeof(t) copies of m.val are written into the respective memory segment, starting at m.a+m.o. Component m.c represents a symbolic validity constraint for the existence of the item. For concrete or abstract interpretations this means that the memory item is only feasible if m.c – after having been properly resolved – evaluates to true.
For the symbolic specification of offsets, lengths, values and constraints GIMPLE expressions over symbols from SymbolsS are used: Such an expression addresses each identifier as a pair (x, n) where x ∈ Symbol is an ordinary GIMPLE symbol and n is a version information. Components m.v0, m.v1 represent validity information: When resolving a symbol (x, n) ∈ SymbolsS occurring in offset, length, value or constraint expressions of some memory item m', only the items m with m.v0 ≤ n ≤ m.v1 are considered.
In symbolic interpretation expressions are never resolved to concrete or ab- stracted variable values, instead, a resolution stops if the expression only contains literals (including base addresses which are considered as string literals), operators and symbols from a given set V and with a specific version range n0 ≤ n ≤ n1. A
typical resolution variant is to take V as the set of base addresses, function call pa- rameters and global input variables, and specify n = 0, meaning “resolve expression until it only contains literals, operators and input variables in their initial version”. The constraints of the memory items involved are part of the resolution result ρ, so
in general ρ is of the form
ρ = if c11 ∧ .. . ∧ c1k1 then e1 elseif c21 ∧ .. . ∧ c2k2 then e1 .. . else eÆ
with expressions ei ∈ Expr(V ), that is, without version information. Examples for handling memory items in SS are given in Section 6.
Symbolic interpretation is performed according to rules of the pattern
g
n1 −→&F C n2	 ,
(n1, n, mem) −→C (n2,n + 1, mem')
so a transition can be performed on symbolic level whenever a corresponding edge exists in the control flow graph 5 . To illustrate the effect of symbolic transitions on the state space SS we present three transition rules explaining stack variable definition, assignment to a variable (selector) and assignment to a de-referenced pointer.
A stack variable definition , n1 =def typex x; only affects the current stack frame. Value expression T marks that the value is still undefined.
mem' = (mem.data, mem.heap, front(mem.stack) - ⟨last(mem.stack) - ⟨m⟩⟩
m = (n + 1, ∞, &x, typex, 0, 8 · sizeof(typex), T, (g, n))
The effect of an assignment to a stack variable, n1 =def sel = expr; affects the current stack frame only:
mem' = (mem.data, mem.heap, front(mem.stack) - ⟨h'⟩)
h' = up=(sel, expr, n, last(mem.stack), g)

5 It may turn out, however, on abstract or concrete interpretation level, that such a transition is infeasible in the sense that no valuation of inputs exists where the constraints of all memory items involved evaluate to true.



function up=(sel : Selectors; expr : Expr; n : N0; h : M-Item∗; g : Expr): M-Item∗ m' := (n + 1, ∞, &β(sel),τ (stack, sel), ωA(sel), 8 · sizeof(sel), (expr, n), (g, n)); up= := up(m', n, h);
end

Fig. 2. Effect of normal assignments on history of memory items.

Function up=() (Fig. 2) specifies (1) how a new memory item m' is created for the stack frame history, carrying the right-hand side expression as its value and the CFG guard condition as validity constraint and (2) which memory items m have
to be invalidated due to the new assignment, possibly leading to the creation of
“replacements” for these m involving new constraints. The details of this invali-
dation/creation process are specified in function up() (Fig. 3): All memory items m matching with the new item m' with respect to base address and validity infor- mation have to be invalidated. It may be the case, however, that m' “overwrites” only a portion of m. As a consequence, it has to be specified that the “remains” of m not affected by the assignment m' are still valid. Therefore a new memory item m1 is created and its constraint specifies that outside the range of m', the old m valuation still exists. Observe that the constraint of m1 always evaluates to false if m and m' are of the same type and have the same offset. This indicates that m1 is infeasible, so m is completely overwritten.
The effect of assignments to variables in the data segments are specified analo- gously; they affect the mem.data-portion of the memory state.
An assignment to a de-referenced pointer, n1 =def *p = expr; may affect the data segment, heap or stack, depending on the potential target addresses p
points to. The details are specified by function up=p (Fig. 4).
mem' = up= (p, expr, n, mem, g)
At first, a list ml of all possible pointer targets is generated, using auxiliary function γ() (Fig.5): Depending on the valuation of different constraints associated with different memory items, p may point to one or more locations in stack, data segment or heap. For each of these possible situations, ml contains the new memory item for the respective pointer target. The effect of each new item on the invalidation of existing items and creation of new ones is performed again as specified by up() and explained above.

Abstract Interpretation of GIMPLE-Programs
Based on the symbolic interpreter introduced in the preceding section it is now possible to construct a variety of abstract interpreters according to the following rules:
For every datatype t in the concrete program component chose a suitable
¢
abstraction lattice (L(t), ±), so that a Galois connection (P(t), ⊆) ←− (L(t), ±) ex-
¢
ists.
Lift each operation   defined on t to L(t) by means of the canonic con-



function up(m' : M-Item; n : N0; h : M-Item∗): M-Item∗
u := ⟨ ⟩; w := ⟨ ⟩;
for m = last(h) downto head(h) do if (m.v1 = ∞∧ m'.a = m.a) then
m1 := (n + 1, ∞, m.a, m.t, o, l, m.val,
m.c ∧ m'.c ∧ 0 < l ∧ m.o ≤ o ∧ o < l ∧ l ≤ m.l ∧ (o + l ≤ m'.o ∨ m'.o + m'.l ≤ o)); m.v1 := n;
u := ⟨m1⟩ - u;
endif
w := ⟨m⟩ - w;
enddo
up := w - u - ⟨m'⟩;
end

Fig. 3. Effect of new memory item on history h ∈ M ∗.

function up=p (p : Symbols; expr : Expr; n : N0; mem : M ; g : Expr): M hd := mem.data; hh := mem.heap; hs := last(mem.stack);
ml := γ(p, expr, n, mem, g);
forall m' in ml do
if (σ(m')= data) then hd := up(m', n, hd) elseif (σ(m')= heap) then hh := up(m', n, hh) else hs := up(m', n, hs);
enddo
upp= := (hd, hh,front(mem.stack) - ⟨hs⟩);
end

Fig. 4. Effect of assignments to de-referenced pointers on history of memory items.

function γ(p : Symbols; expr : Expr; n : N0; mem : M ; g : Expr): M-Item∗
ml := ⟨ ⟩;
if σ(p)= data then h := mem.data else h := last(mem.stack);
forall mp in h do
if mp.a = &p ∧ mp.v0 ≤ n ≤ mp.v1 then
pl := ξ(mp.val, mem);
forall q in pl do
a := base address from expression pl;
o := offset expression from expression pl;
c := conjunction over all conditions of memory items occurring in pl; m' := (n + 1, ∞, a, →τ (p), o, 8 · sizeof(→τ (p)), (expr, n),c ∧ (g, n));
ml := ml - ⟨m'⟩;
enddo endif
enddo
γ := ml;
end

Fig. 5. Function γ finds list of memory items potentially affected by assignment to de-referenced pointer.



function ξ((expr, n): Expr × N0; mem : M ): M-Item − Expr∗ el := ⟨expr⟩;
e := head(el);
while e is not resolved to base address plus M-Item-expression for offset do
x := next unresolved identifier from e;
h := if σ(x)= stack then last(mem.stack) else mem.data;
for m := last(h) downto head(h) do
if m.a = &β(x) ∧ m.v0 ≤ n ≤ m.v1 then
e1 := e;
In e1: exchange each occurrence of x by m; el := el - ⟨e1⟩;
endif enddo
el := tail(el);
e := head(el);
enddo
ξ := el;
end

Fig. 6. Function ξ finds list of base addresses potentially associated with a pointer.

struction  L : L(t) × L(t) → L(t); p   p =
(p ¢ 
¢
) . In this definition,

1  L 2
def	1	P  2

 P denotes the canonic lifting of  to the powerset lattice over t: a1 Pa2 =def
{x1 x2 | xi ∈ ai,i = 1, 2}
Having defined all abstraction lattices L(t), lift all Boolean operators
Δ : t × t' → B to [Δ]: L(t) × L(t') → L(B) by
8>< T if {x1Δx2 | x1 ∈ p1¢ , x2 ∈ p2¢ } = {false, true}

p1[Δ]p2 =
false if {x1Δx2 | x1 ∈ p1¢ , x2 ∈ p2¢ } = {false}
>: true if {x1Δx2 | x1 ∈ p1¢ , x2 ∈ p2¢ } = {true}

Lift the symbolic state space SS = N (P ) × N0 × M defined above to its lattice representation SL = N (P ) × N0 × L(M ), where L(M ) is the interpretation of memory items over the respective abstraction lattices chosen for offsets, length, values and constraints.
The transition rules for the abstract interpretation semantics over SL are of the form
g	'
n1 −→&F C n2, (n1, n, mem) −→C (n2,n + 1, mem ), L(mem) |=L (g /= false)
(n1, n, L(mem)) −→L (n2,n + 1, L(mem'))
where L(mem) denotes the lattice interpretation of memory items. Informally speaking, an abstract transition between CFG nodes n1 and n2 with changes in abstract memory valuations from L(mem) to L(mem') is possible in SL if (a) there exists a corresponding edge in the CFG, (b) the lattice valuation of the guard con- dition g is true or T and (c) the collection of memory items changes from mem to mem' in the symbolic interpretation.

Application Example
The following example illustrates some of the advantages obtained by the higher flexibility resulting from the interplay between symbolic and abstract interpretation.

Consider the GIMPLE function 6 shown in Fig. 7 and an associated invocation x = f (i0, z0); Applying the symbolic interpretation rules described in Section 4 for the two possible paths through the function results in the symbolic state of the stack frame as shown in the list of memory items on the right-hand side of Fig. 7, valid
at function return in line 11. Consider the following verification goals: (Goal 1): f () only assigns to valid de-referenced pointers., (Goal 2): f () never returns an undefined value.




float f(int i, float z) {
float *p, *q;
float a[10];
p = &a;
q = p + 4*i;
if ( 0 < z ) { 
6	*q = 10 * z;
7	else {
8	*q = 0;
9	}
10	return a[i];
11	}
Line No. Resulting M-Item
0. (1, ∞, &i, int, 0, 32, (i0, 0), true)
0. (1, ∞, &z, float, 0, 32, (z0, 0), true)
(1, 6, &xReturn, float, 0, 32, ⊥, true)
(2, 3, &p, float∗, 0, 32, ⊥, true)
(2, 4, &q, float∗, 0, 32, ⊥, true)
(3, 5, &a, float, 0, 320, ⊥, true)
(4, ∞, &p, float∗, 0, 32, &a, true)
(5, ∞, &q, float∗, 0, 32, (p +4 · i, 4), (0 ≤ i < 10, 4))
6. (6, ∞, &a, float, (32 · i, 5), 32, (10 · z, 5), (0 < z, 5))
6. (6, ∞, &a, float, o, l, ⊥, (0 < z ∧ 0 < l ∧ 0 ≤ o ∧ o + l ≤
320 ∧ (o + l ≤ 32 · i ∨ 32 · i + 32 ≤ o), 5))
8. (6, ∞, &a, float, (32 · i, 5), 32, 0, (z ≤ 0, 5))
8. (6, ∞, &a, float, o, l, ⊥, (z ≤ 0 ∧ 0 < l ∧ 0 ≤ o ∧ o + l ≤
320 ∧ (o + l ≤ 32 · i ∨ 32 · i + 32 ≤ o), 5))
(7, ∞, &xReturn, float, 0, 32, (a[i], 6), true)



Fig. 7. GIMPLE Code sample and associated symbolic interpretation result.



Alternative 1: Interpretation with is-defined and interval lattices.
Chose lattice LD = ({⊥, Δ, T}, ±) with ⊥ ± Δ ± T as an appropriate ab- straction for checking well-definedness of float z; float a[10]; (Δ stands for is-defined, ⊥ for is-undefined). For checking pointer addresses we abstract inte-
gers to intervals over Z: LI = (I(Z), ⊆). With these lattices, we now perform the
corresponding abstract interpretation on the history of memory items in Fig. 7, each time resolving the associated to symbols down to constants, base addresses or input variables i0, z0 as explained in Section 4. Additionally we assume that a precondition i0 ∈ [3, 5] has been asserted. Then the abstract interpretation results in
0. (1, ∞, &i, LI , 0, 32, [3, 5], true)
(1, ∞, &z, LD, 0, 32, Δ, true) (z is well-defined, since it is initialised with input z0)
0. (1, 6, &xReturn, LD, 0, 32, ⊥, true)
(2, 3, &p, LI , 0, 32, [−∞, +∞], true)
(2, 4, &q, LI , 0, 32, [−∞, +∞], true)
(3, 5, &a, LD, 0, 320, ⊥, true)
(4, ∞, &p, LI , 0, 32, [&a, &a], true)  (symbolic single-point interval [&a, &a])
(5, ∞, &q, LI , 0, 32, &a +4 · [3, 5], true) (([0, 0][≤][3, 5][<][10, 10]) is true in LI
6. (6, ∞, &a, LD, 32 · [3, 5], 32, Δ, T) (0 < Δ evaluates to T, 10 · Δ evaluates to Δ over LD)

6 Observe that in contrast to C/C++, GIMPLE always uses byte values in pointer arithmetic. As a conse- quence, we find assignment q = p + 4*i; in line 4, whereas we would write q = p + i; in the corresponding C/C++ program.

6. (6, ∞, &a, LD, o, l, ⊥, 0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · [3, 5] ∨ 32 · [3, 5] + 32 ≤ o))
8. (6, ∞, &a, LD, 32 · [3, 5], 32, Δ, ⊥) ({0}¢ = Δ)
8. (6, ∞, &a, LD, o, l, ⊥, 0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · [3, 5] ∨ 32 · [3, 5] + 32 ≤ o))
10. (7, ∞, &xReturn, LD, 0, 32, (a[[3, 5]], 6), true) (not yet resolved – see next paragraph)

Now we apply the resolution rules to 10: First it is noted that a([[3, 5]], 6) matches all memory items of the form
m = (v0, v1, &a, LD, 32 · [3, 5], 32, val, c), v0 ≤ 6 ∧ 6 ≤ v1
As a consequence the valuation candidates are those from lines 6. and 8. above. We only have to investigate the feasibility of memory items with undefined valuation
⊥, so it remains to show that
0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · [3, 5] ∨ 32 · [3, 5] + 32 ≤ o) ∧ o = 32 · [3, 5] ∧ l = 32
has no solution; this proof obligation is simplified to showing that no solution of
[3, 5] + 1 ≤ [3, 5] ∨ [3, 5] + 1 ≤ [3, 5]
exists. Unfortunately this predicate evaluates to T in LI because we can select (different) numbers from [3, 5] in each of its occurrences so that the predicate eval- uates either to true or to false. As a consequence it is necessary to perform 2 partitioning steps of the i0 interval valuation [3, 5] into [3, 3] [4, 4], [5, 5], in order to prove that this predicate is always false.

Alternative 2: Interpretation with is-defined and predicate lattice.
As we have seen in the discussion of alternative 1 above, the interval lattice is suitable for proving well-definedness of pointer de-referencings but is quite inefficient to prove the crucial step for well-definedness of the return value. We can fix this by taking the solution of verification goal 1 as constructed above, but using another lattice to represent pointer and integer expressions for discharging goal 2: Let LP the lattice of predicates over programming variables, together with their comparison operators 7 . Use LD as above. Abstract interpretation now results in
0. (1, ∞, &i, LP , 0, 32,i = i0, true)
0. (1, ∞, &z, LP , 0, 32, Δ, true)
0. (1, 6, &xReturn, LD, 0, 32, ⊥, true)
(2, 3, &p, LP , 0, 32, ⊥, true)
(2, 4, &q, LP , 0, 32, ⊥, true)
(3, 5, &a, LD, 0, 320, ⊥, true)
(4, ∞, &p, LP , 0, 32,p = &a, true)
(5, ∞, &q, LP , 0, 32,q = &a +4 · i0, true)
6. (6, ∞, &a, LD, 32 · i0, 32, Δ, T)
6. (6, ∞, &a, LD, o, l, ⊥, 0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · i0 ∨ 32 · i0 + 32 ≤ o))
8. (6, ∞, &a, LD, 32 · i0, 32, Δ, T)
8. (6, ∞, &a, LD, o, l, ⊥, 0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · i0 ∨ 32 · i0 + 32 ≤ o))
10. (7, ∞, &xReturn, LD, 0, 32, (a[i0], 6), true) (not yet resolved – see next paragraph)

7 More formally, the quantifier-free Presburger formulae over program variables are suitable for our purpose because efficient solvers exist for problems of this type [13].

Now, for the resolution of (a[i0], 6), all memory items of the form
m = (v0, v1, &a, LD, 32 · i0, 32, val, c), v0 ≤ 6 ∧ 6 ≤ v1
match, and the condition for returning an undefined value is
0 < l ∧ 0 ≤ o ∧ o + l ≤ 320 ∧ (o + l ≤ 32 · i0 ∨ 32 · i0 + 32 ≤ o) ∧ o = 32 · i0 ∧ l = 32
which – applying the rules on term replacement and arithmetics in LP – boils down to i0 +1 ≤ i0 which is obviously false.

Conclusion
We have described techniques for concrete and abstract interpretation of C/C++ programs represented in GIMPLE, which basically produces a control flow graph model for each C/C++ function or method. The results are implemented in a tool and they are currently applied for integrated module testing and static analysis of safety-critical embedded systems software in the railway and avionic domains. Applications in the field of automotive control are currently prepared; they focus, however, on model-based test case generation. Due to the intermediate model rep- resentation of the tool which uses the same class of hierarchic transition systems for code (control flow graph) and model (e. g. UML 2.0 Statechart) representation, the test case generation mechanisms are the same for code-based and model-based testing. Currently a correctness proof for the abstract interpretation semantics constructed according to the rules given in Section 5 is elaborated: We show that application of these rules always result in a valid abstract interpretation semantics according to Definition 2.1.

References
Bahareh Badban, Martin Fr¨anzle, Jan Peleska, and Tino Teige. Test automation for hybrid systems. In Proceedings of the Third International Workshop on SOFTWARE QUALITY ASSURANCE (SOQUA 2006), Portland Oregon, USA, November 2006.
P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min, D. Monniaux, and X. Rival. Combination of abstractions in the Astr´ee static analyzer. In M. Okada and I. Satoh, editors, Eleventh Annual Asian Computing Science Conference (ASIAN’06), pages 1–24, Tokyo, Japan, LNCS, December 6–8 2006. Springer, Berlin. (to appear).
B. A. Davey and H. A. Priestley. Introduction to Lattices and Order. Cambridge University Press, 2002.
Bruno Blanchet et. al. Design and implementation of a special-purpose static program analyzer for safety-critical real-time embedded software. In T. AE. Mogensen et al., editor, The Essence of Computation, volume 2566, pages 85–108, 2002.
Ansgar Fehnker, Ralf Huuck, Patrick Jayet, Michel Lussenburg, and Felix Rauch. Goanna - a static model checker. In Proceedings of 11th International Workshop on Formal Methods for Industrial Critical Systems (FMICS), Bonn, Germany, 2006.
Martin Fr¨anzle, Christian Herde, Tino Teige, Stefan Ratschan, and Tobias Schubert. Efficient solving of large non-linear arithmetic constraint systems with complex bo olean structure. Journal on Satisfiability, Boolean Modeling and Computation, 2007.
GCC, the GNU Compiler Collection. The GIMPLE family of intermediate representations. See
http://gcc.gnu.org/wiki/GIMPLE .
Helge L¨oding. Behandlung komplexer Datentypen in der automatischen Testdatengenerierung. Master’s thesis, University of Bremen, May 2007.


Jan Peleska and Helge L¨oding. Static Analysis By Abstract Interpretation. University of Bremen, Centre  of  Information  Technology,  2008.	available under http://www.informatik.uni-bremen.de/agbs/lehre/ws0708/ai/saai script.pdf.
Jan Peleska, Helge L¨oding, and Tatiana Kotas. Test automation meets static analysis. In Rainer Koschke, Karl-Heinz R¨odiger Otthein Herzog, and Marc Ronthaler, editors, Proceedings of the INFORMATIK 2007, Band 2, 24. - 27. September, Bremen (Germany), pages 280–286.
S. Ranise and C. Tinelli. Satisfiability modulo theories. TRENDS and CONTROVERSIES–IEEE Magazine on Intelligent Systems, 21(6):71–81, 2006.
Bastian Schlich, Falk Salewski, and Stefan Kowalewski. Applying model checking to an automotive microcontroller application. In Proc. IEEE 2nd Int’l Symp. Industrial Embedded Systems (SIES 2007). IEEE, 2007. ISBN 1-4244-0840-7.
Ofer Strichman. On solving presburger and linear arithmetic with sat. In M.D. Aagaard and J.W. O’Leary, editors, Formal Methods in Computer-Aided Design (FMCAD),, number 2517 in LNCS, pages 160–170. Springer, 2002.
Arnaud Venet and Guillaume Brat. Precise and efficient static array bound checking for large embedded c programs. In Proceedings of the PLDI’04, June 9-11, 2004, Washington, DC, USA. ACM 1581138075/04/0006.
Verified Systems International GmbH, Bremen. RT-Tester 6.2 – User Manual, 2007.
