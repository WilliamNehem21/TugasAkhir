

Electronic Notes in Theoretical Computer Science 265 (2010) 161–176
www.elsevier.com/locate/entcs

From Focalization of Logic to the Logic of Focalization 1
Michele Basaldellaa,2	Alexis Saurinb,3	Kazushige Teruia,4
a RIMS, Kyoto University, Japan
b PPS & INRIA πr2, Paris, France

Abstract
Andreoli originally discovered focalization as a concrete proof search strategy in proof theory of linear logic, putting to the foreground the role of polarity in logic.
The aim of the present paper is to give a more abstract account on focalization in the framework of ludics. We describe focalization as a map (embodied by an untyped proof/design) from an unsynthesized to a
synthesized type/behaviour. The map turns out to be a retraction of another map, that is related to invertibility of negative connectives. In this way we formalize the common intuition that focalization of positive connectives is dual to invertibility of negative ones.
Keywords: Linear logic, ludics, focalization, proof-search, interaction.


Introduction
Focalization is a deep outcome of linear logic (LL) proof theory, putting to the foreground the role of polarity in logic. It resulted in important advances in various fields ranging from proof search (the original motivation for Andreoli’s study [2] of focalization) to game semantical analysis of logic.
Focalization in linear logic
In linear logic, one distinguishes two classes of logical connectives: positive (⊗, ⊕, 0, 1, E, !) and negative (`, &, T, ⊥, 6, ?) connectives. The distinction can be easily understood in terms of proof search in (one-sided) sequent calculus. The intro- duction rules for negative connectives `, &, T, ⊥, 6 are invertible: in the bottom-up

1 A preliminary version of this work has been presented in [3].
2 Email: mbasalde@kurims.kyoto-u.ac.jp
3 Email: alexis.saurin@pps.jussieu.fr
4 Email: terui@kurims.kyoto-u.ac.jp

1571-0661 © 2010 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2010.08.010

reading, the rule is deterministic, i.e., there is no choice to make and provability of the conclusion implies provability of the premisses. On the other hand, the in- troduction rules for positive connectives involve choices: e.g., splitting the context in ⊗ rule, or choosing between ⊕L and ⊕R rules, resulting in possibly erroneous choices during proof search. Still, positive connectives satisfy a strong property called focalization [2]: let us consider a sequent ⊕ F1,... , Fn containing no negative formulas, then there is (at least) one formula Fi which can be used as a focus for the search by hereditarily selecting Fi and its positive subformulas as principal formulas up to the first negative subformulas. This property induces the following strategy of proof search called focalization discipline:
The sequent ⊕ Γ contains a negative formula : choose any negative formula (e.g. the leftmost one) and decompose it using the only possible negative rule.
The sequent ⊕ Γ contains no negative formula : choose some positive for- mula and decompose it (and its subformulas) hereditarily until we get to atoms or negative subformulas.
It is proven in [2] that the focalization discipline is a complete proof search strategy. Other approaches to focalization consider proof transformation tech- niques [4,12,15].
Focalization is not only concerned with efficiency of proof search, but also a key to understand the CPS translation [6,16,18] and the space compression in compu- tational complexity [17]. It also underlies game semantics [11,14]. In view of its importance, it would be worthwhile to study focalization from as many perspectives as possible. While focalization is originally explained by permutability of inference rules, we look for a more abstract, “algebraic” account in the framework of ludics. Some other recent works provide an enlightening view of the structure of focal- ization and its connections with interaction-based semantics. Though focalization induces a sequential view of logic, it is worth mentioning Abramsky’s discussion [1] on sequential and concurrent approaches to games and logic: pursuing the concur- rent game direction, Melli`es developed a model of linear logic based on asynchronous games [13]. Melli`es and Tabareau recently suggested to consider tensor logic (i.e., a logic equipped with a tensorial, noninvolutive negation) as being more primi- tive than linear logic. This leads to consider dialogue categories [14]. Finally, one can also mention Munch-Maccagnoni’s recent work on providing a polarized ap- proach [16] to Curien and Herbelin’s duality of computation and shed interesting
light on focalization. This work uses Krivine’s classical realizability.
Focalization in ludics
Ludics [9] is a pre-logical framework proposed by Girard which aims to analyze various logical and computational phenomena at a foundational level.
One reason why we adopt ludics is that types (called behaviours) are built as sets of untyped proofs (called designs), as in some realizability interpretations and semantic/operational types. This view of types as proof sets allows us to analyze various properties of proofs, including focalization, at the level of types. Further-

more, ludics has the following prominent features that are relevant for focalization.

Synthetic connectives.
According to the focalization discipline, one decomposes a positive formula per- sistently until reaching atomic or negative subformulas. This strategy is inter- nally expressed in ludics by considering synthetic connectives that combine several connectives of the same polarity into one, and demanding that every behaviour (type) be a strict alternation of positive and negative synthetic connectives. For instance, a compound formula N ⊗ (M ⊕ K) of LL with two positive connectives
⊕, ⊗ is expressed by a behaviour ⊗⊕ (N, M, K) of ludics with one synthetic con- nective ⊗⊕. With ⊗ and ⊕ inseparable, the latter only admits focalized proof search. On the other hand, unfocalized proof search can be simulated by consid- ering N ⊗ ↑(M ⊕ K), where a dummy negative connective ↑ is artificially inserted between two positive connectives. This allows us to think of focalization as a map f from P = N ⊗ ↑(M ⊕ K) to Pf = ⊗⊕ (N, M, K).

Behaviours and biorthogonality.
Notice that focalization is intrinsically a context-sensitive phenomenon, as it states that one can obtain a proof of ⊕ Γ, Pf from that of ⊕ Γ, P for any con- text Γ. In ludics, interaction with contexts is taken into account by demanding that behaviours be closed under the biorthogonal operation: P = P⊥⊥. Indeed, this biorthogonal-closedness (together with associativity of normalization) implies the closure principle: informally speaking, a design D belongs to a “sequent of behaviour” ⊕ Γ, P if and only if D[E/x] (D applied to E) belongs to P for all designs E in Γ⊥. The effect is sort of modularity: even though focalization involves contexts, one can “project” them on a single behaviour P.

Internal completeness.
A difficulty of biorthogonal closure is that it may obscure the content of a be- haviour P, and make it hard to verify the correctness of the focalization map f directly. Fortunately, ludics enjoys internal completeness, that allows us to remove biorthogonal closure and to give a concrete description to the designs in P. This is a key to ensure that the focalization map f : P −→ Pf is a total function.


Putting these three features together, we can informally explain focalization as follows: given an unfocalized proof D of ⊕ Γ, P (seen as design), we derive:
D ∈ ⊕Γ, P =⇒ D[E/x] ∈ P	for every E ∈ Γ⊥
=⇒ f (D)[E/x]= f (D[E/x]) ∈ Pf	for every E ∈ Γ⊥
=⇒ f (D) ∈ ⊕Γ, Pf
to obtain a focalized proof f (D) of ⊕ Γ, Pf .

In this paper, we shall describe the focalization map f as a retraction of a map u which may explain the invertibility of negative inference rules. This way we promote an “algebraic” view of focalization in the setting of ludics: focalization as a retraction of invertibility.
Organization of the paper
In Section 2, we first recall the basic objects of ludics, namely designs, and introduce functionals, which work as morphisms between behaviours. In Section 3, we define synthetic signatures and synthetic connectives, which are the starting point for studying focalizing and inverting functionals, that we first introduce in the untyped setting in Section 4. In Section 5, we move to a (semantically) typed setting and reconsider functionals as functions between behaviours (types). Logical behaviours are described in Section 6 together with internal completeness. Finally, we complete our study in Section 7 providing a ludics account of focalization.
Untyped designs
Syntax
We recall the (identity-free) syntax of designs following the notation of [17], inspired by the close relations between ludics and linear π-calculus [7].
Designs are built over a given signature A = (A, ar), where A is a set of names
a, b, c,... and ar : A −→ N a function which assigns an arity ar(a) to each name
a. Let V be a countable set of variables V = {x, y, z,.. .}. Over a fixed signature A, a positive action is a with a ∈ A, and a negative action is a(x1,... , xn) where x1,... , xn are distinct variables and ar(a)= n. In the sequel, an expression of the form a(x) always stands for a negative action.
Definition 2.1 The positive P (resp. negative N ) designs are coinductively gen- erated by the following grammar:
P ::= Ω	(partiality, divergence),
  H	(daimon, termination),
  x|a⟨N1,... , Nn⟩	(head normal form),
  N |a⟨N1,... , Nn⟩	(cut),
N ::= Σ a(x).Pa	(abstraction),
where ar(a) = n, x = x1,... , xn and the formal sum Σ a(x).Pa is built from |A|- many components {a(x).Pa}a∈A.
Designs may be considered as infinitary λ-terms with named applications and
named and superimposed abstractions. We use meta-variables P, Q,... (resp. N, M,
.. ., resp. D, E,.. .) to denote positive (resp. negative, resp. arbitrary) designs. Any subterm E of D is called a subdesign of D.

We use Ω to encode partial sums: given a set α = {a(x), b(y),... } of nega- tive actions, we write a(x).Pa + b(y).Pb + ··· or Σα a(x).Pa to denote the design Σ a(x).Ra, where Ra = Pa if a(x) ∈ α, and Ra =Ω otherwise.
A design D may contain free and bound variables. An occurrence of subterm
a(x).Pa binds the free-variables x in Pa. Variables which are not under the scope of any binder a(x) are free. fv(D) denotes the set of free variables occurring in D. Designs are always considered up to α-equivalence, that is up to renaming of bound variables (see [17] for further details).
A positive design which is neither Ω nor H is either of the form (Σ a(x).Pa)|
a⟨N1,... , Nn⟩ and called a cut or of the form x|a⟨N1,... , Nn⟩ and called a head normal form. The head variable x in the design above plays the same role as a pointer does in a strategy from Hyland-Ong’s games model and an address (or locus) in Girard’s ludics.
In the first case a cut reduces to another positive design via the following reduc- tion rule, written −→:
 Σ a(x1,... , xn).Pa |a⟨N1,... , Nn⟩ −→ Pa[N1/x1,... , Nn/xn];
where the expression D[N1/x1,... , Nn/xn] denotes the design obtained by the si- multaneous and capture-free substitution of Ni for xi, 1 ≤ i ≤ n, in D.

Example 2.2 Let N = a(x).H + b(x).y|d⟨M ⟩ + c(x).x|e⟨M ⟩, with x ∈/ fv(M ). We have:
N |a⟨L⟩ −→ H (termination);
N |d⟨L⟩ −→ Ω (divergence);
N |b⟨L⟩ −→ y|d⟨M ⟩ (reduction to head normal form);
N |c⟨L⟩ −→ L|e⟨M ⟩ (reduction to another cut).
We write −→∗ for the transitive reflexive closure of −→.
Given a design D, we define its normal form JD) by corecursion as follows:
JP ) = H, if P −→∗ H;
JP ) = x|a⟨JN1),... , JNn)⟩, if P −→∗ x|a⟨N1,... , Nn⟩;
JP ) = Ω, otherwise (i.e., if either P −→∗ Ω or P −→−→ ··· diverges);
JΣ a(x).Pa) = Σ a(x).JPa).
An important property of ludics is associativity of normalization:
JD[N1/x1,... , Nn/xn]) = JJD)[JN1)/x1,... , JNn)/xn]).
A design is said:
total, if D /= Ω (not to be confused with totality of strategies in game semantics);
linear (or affine), if for any subdesign of the form N0|a⟨N1,... , Nn⟩, the sets
fv(N0), .. ., fv(Nn) are pairwise disjoint;
cut-free, if it does not contain a cut as a subdesign.
Designs which are total, linear and cut-free correspond to the original notion of

design [9] and for this reason we call them standard.
A very important subclass of standard designs is the one consisting of atomic designs. A positive standard design P is atomic if fv(P ) ⊆ {x0} for a certain ﬁxed variable x0 (the variable x0 plays the same role as the empty address “⟨⟩” does in [9], i.e., it is a fixed and predetermined “location”). A negative standard design N is atomic if fv(N ) = ∅. In the sequel, we denote by D the set of the atomic designs, by D+ (resp. D−) its restriction to positive (resp. negative) designs.

Functionals
We now introduce a class of designs that work as morphisms between behaviours.
Definition 2.3 A functional (f, g, h,.. .) is any negative standard design N such that fv(N ) ⊆ {x0}.
A functional f can be thought as a polarity preserving map f : D −→ D. Indeed, given an atomic positive design P , we can apply f to P by f ∗(P ) := JP [f /x0]). The result is either a positive atomic design or Ω, which can be seen as a coding of “undefined.” So, the operation f ∗ can be seen as a partial map f ∗ : D+−~ D+.
Similarly, given an atomic negative design N , f∗(N ) := Jf [N/x0]) is an atomic negative design. So, f∗ : D− −→ D−.
By associativity, we immediately have the following duality principle:
Lemma 2.4 For any P ∈ D+ and N ∈ D−, Jf ∗(P )[N/x0]) = JP [f∗(N )/x0]).
We now introduce some basic functionals and their notation.
Identity: We call identity (also called fax in [9], or copycat strategy in game se- mantics) the functional id corecursively defined by the equation:
id := Σ a(x1,... , xn).x0|a⟨id(x1),... id(xn)⟩,
where id(xk) := id[xk/x0] for any 1 ≤ k ≤ n. The design id plays the role of the identity function for (standard) designs, in particular: id∗(P ) = P , for any P ∈ D+ and id∗(N )= N , for any N ∈ D− (see [17]).
Renaming: Given two n-ary names a, b, we call renaming the functional rn(a,b)
defined as:

rn(a,b) := a(x1,... , xn).x0|b⟨id(x1),... id(xn)⟩.
When rn(a,b) is applied to a positive atomic design P , it works as follows. For

P = x0|c⟨N1,... , Nn⟩, we have rn∗
(P ) = x0|b⟨N1,... , Nn⟩ if c = a (in this

case rn(a,b) just “renames” the first action of P ); rn∗
(P )= Ω, i.e., “undefined”

when c /= a. We use the special notation ida for the identical renaming rn(a,a).
Notice that	ida = id.
Functionals can be composed: for any f, g, we define the positive composition
g ◦∗ f := Jf [g/x0]) and the negative composition g ◦∗ f := Jg[f /x0]).
Example 2.5 rn(a,b) ◦∗ rn(b,a) = rn(b,a) ◦∗ rn(a,b) = idb.

The operations ◦∗ and ◦∗ are associative with unit id. Furthermore,
Proposition 2.6 (Untyped composition) For any P ∈ D+ and N ∈ D−,
(g ◦∗ f )∗(P )= g∗(f ∗(P )),	(g ◦∗ f )∗(N )= g∗(f∗(N )).
We have thus constructed two categories P0 and N0: P0 (resp. N0) has D+ (resp. D−) as the unique object and functionals as morphisms. While the set of morphisms is the same in P0 and N0, composition is defined differently as described above. The fundamental principle is that Pop ∼= N0. We shall later refine them to the categories of positive and negative behaviours.

Synthetic connectives
Synthetic signature
Let A = (A, ar) be a signature. Let An be the set of names of arity n, i.e., An :=
{a ∈ A : ar(a)= n}. A signature is synthetic if:
for any a ∈ An, b ∈ Am and 1 ≤ i ≤ n, there exists a name a[b/i] ∈ An+m−1;
a[b/i]= c[d/j] only if a = c, b = d and i = j.
From now on, we assume that our signature A = (A, ar) is synthetic and equipped with a unary name ↑, that we call the dummy shift operator. We denote by ↓ the positive action ↑, and abbreviate ↑(x).x|a⟨N ⟩ by ↑a⟨N ⟩.
As a convention, given disjoint sequences of variables x = x1,... , xn and y and 1 ≤ i ≤ n, we denote by x[y/i] the sequence x1,... , xi−1, y, xi+1,... , xn.
Logical and synthetic connectives
Informally, a logical connective is specified by (i) placeholders for subformulas, and
(ii) inference rules associated to the connective. In our setting, (i) is embodied by a sequence of variables and (ii) by a set of negative actions as follows:
Definition 3.1 Let z = z1,... , zn be a sequence of distinct variables. An n-ary logical connective α(z1,... , zn) is a finite set of negative actions {a1(x1),... , am (xm)}, such that a1,... , am are distinct names and {xi}⊆ {z}, for any 1 ≤ i ≤ m.
Since variables are just used as placeholders, we naturally identify two logical connectives if one is obtained from another by renaming the variables. In other words, the variables z are bound in the expression α(z). Hence, given two logical connectives α(z) and β(u), we may always assume that z and u are disjoint. When the variables z are clear from the context, we often write α instead of α(z).
A synthetic signature allows us to synthesize two logical connectives.
Definition 3.2 Let α(z1,... , zn) and β(u1,... , um) be logical connectives. Given
1 ≤ i ≤ n, we call synthetic connective associated to (α, β, i), noted by synth(α, β, i), the logical connective γ(z1,... , zi−1, u1,... , um, zi+1,... , zn) consist- ing of negative actions:

a[b/j](x[y/j]) such that a(x) ∈ α, b(y) ∈ β, x = x1,... , xk and zi = xj;
aβ(x) such that a(x) ∈ α and zi ∈/ {x}, where aβ is a fresh name of arity ar(a).
Observe that the freshness condition in (b) above ensures that the actions in
synth(α, β, i) have pairwise distinct names.
Standard connectives, shifts and synthetic connectives of MALL can be ex- pressed in a synthetic signature containing unary names ↑, π1, π2 and a binary name
℘. We define:
T := ∅,	`(x1, x2) := {℘(x1, x2)},
↑ (x) := {↑ (x)},	&(y1, y2) := {π1(y1), π2(y2)}.
We may build a synthetic connective synth(`, &, 1) = γ(y1, y2, x2) = {℘[π1/1] (y1, x2), ℘[π2/1](y2, x2)}, which is a logical connective with inference rules:

⊕ Γ, P, R	⊕ Γ, Q, R γ	⊕ Γ, N	⊕ Δ, K	γ	⊕ Γ, M	⊕ Δ, K	γ

⊕ Γ, γ(P, Q, R)
⊕ Γ, Δ, γ(N, M, K)	1
⊕ Γ, Δ, γ(N, M, K)	2

It is clear that the rule γ is a combination of the standard MALL rules for & and `, while γ1 and γ2 are combinations of the rules for ⊗, ⊕1 and ⊕2. Indeed, thinking of γ(P, Q, R) as (P&Q)`R and γ(N, M, K) as (N⊕M)⊗K, we have:


⊕ Γ, P, R ⊕ Γ, Q, R
&
⊕ Γ, N
⊕1
⊕ Γ, M
⊕2

⊕ Γ, P&Q, R
`
⊕ Γ, (P&Q)`R
⊕ Γ, N ⊕ M	⊕ Δ, K
⊗
⊕ Γ, Δ, (N⊕M)⊗K
⊕ Γ, N ⊕ M	⊕ Δ, K
⊗
⊕ Γ, Δ, (N⊕M)⊗K

Focalizing designs
In the sequel, given a sequence of variables z = z1,... , zk we denote by the expres- sion id(z) the sequence of functionals id(z1),... , id(zk). With this notation, id can be succinctly expressed by  a(x).x0|a⟨id(x)⟩.
Given logical connectives α(z1,... , zn), β(u1,... , um) and 1 ≤ i ≤ n, we define two functionals: focalizing design f(α,β,i) and inverting design u(α,β,i) as follows. The focalizing design is built from |α| components indexed by (names of) actions a(x) ∈ α, whereas the inverting one from |γ| components indexed by (names of) actions c(v) ∈ γ, where γ = synth(α, β, i). We have two sorts of components (LHS and RHS of the table below), corresponding to the two cases (a), (b) of Definition
3.2.
For a(x) ∈ α with x = x1,... , xk, we set:

where x = xl, xj, xr in the definition of ua[b/j], and rn(a,aβ) , rn(βa,a) are renaming

functionals. We finally take the formal sum of the components: f(α,β,i) := Σα fa
(α,β,i)	γ  c

To see how they work, consider:
f = f(`,&,1) = ℘(x1, x2).x1|↓⟨Σi=1,2 πi(yi).x0|℘[πi/1]⟨id(yi), id(x2)⟩⟩, u = u(`,&,1) = Σi=1,2 ℘[πi/1](yi, x2).x0|℘⟨↑πi⟨id(yi)⟩, id(x2)⟩.
Consider also the following atomic designs:
P1 := x0|℘⟨↑ π1⟨M ⟩,N ⟩, N1 := ℘(x1, x2).x1|↓⟨π1(y1).Q1 + π2(y2).Q2⟩, P2 := x0|℘[π1/1]⟨M, N ⟩, N2 := ℘[π1/1](y1, x2).Q1 + ℘[π2/1](y2, x2).Q2.
We can calculate f ∗(P1) by normalization:
f ∗(P1) = Jf | ℘⟨↑π1⟨M ⟩,N ⟩)

= J ↑π1⟨M ⟩ | ↓⟨π1(y1).x0|℘[π1/1]⟨id(y1), id(x2)[N/x2]⟩⟩)



= J π1(y1).x0|℘[π1/1]⟨id(y1), id(x2)[N/x2]⟩  | π1⟨M ⟩)
= Jx0|℘[π1/1]⟨id(y1)[M/y1], id(x2)[N/x2]⟩)

= x0|℘[π1/1]⟨id∗(M ), id∗(N )⟩ = P2.
Similarly, we obtain  u∗(P2)= P1,  u∗(N1)= N2,  f∗(N2)= N1.
Observe that f ∗ (resp. u∗) “collapses” three polarity layers into one when applied to a positive (resp. negative) design, while u∗ (resp. f∗) “cancels” the effect of f ∗ (resp. u∗). In particular, we could informally claim that f ∗ internalize focalization as morphism. We shall see later that u∗ can be seen as an internal expression of the invertibility of negative connectives.

Proposition 4.1 (Focalization-inversion) Let f = f(α,β,i) and u = u(α,β,i).
f ◦∗ u = idγ, where γ = synth(α, β, i) and idγ = Σc(x)∈γ idc.
u ◦∗ f is idempotent: (u ◦∗ f ) ◦∗ (u ◦∗ f )= u ◦∗ f.

The equation f ◦∗ u = idγ roughly states that f and u are opposite operations. Later we shall state more precisely that focalizing designs are retractions of inverting designs. That will formally verify the intuition that focalization of positive rules is dual to invertibility of negative rules [9].


Functionals on behaviours
We first recall the notion of behaviour and then discuss functionals in the typed setting.

Orthogonality and behaviours
Two atomic designs P, N of opposite polarities are said orthogonal (written P ⊥N ) when JP [N/x0]) = H. If X is a set of atomic designs of the same polarity, then its orthogonal set is defined by X⊥ := {E : 6D ∈ X, D⊥E}.
The duality of Lemma 2.4 is nicely expressed in terms of orthogonality as:
f ∗(P )⊥N if and only if P ⊥f∗(N ).	(1) A behaviour is a set X of atomic designs of the same polarity such that
X⊥⊥ = X; according to the polarity of its designs, it is positive and noted by
letters P, Q, R,... or negative and noted by N, M, K ... .
There are the least and the greatest behaviours among all positive (resp. nega- tive) behaviours with respect to set inclusion:

where H— := Σ a(x).H is called negative daimon in [9]. We are now ready to assign “types” to functionals.
Definition 5.1 Let P, Q be positive behaviours. We define the positive function space as the set of functionals P −→ Q := {f : 6P ∈ P,f ∗(P ) ∈ Q}. Analogously, given negative behaviours N, M, we define N −→ M := {f : 6N ∈ N, f∗(N ) ∈ M}. We write f : X −→ Y whenever f ∈ X −→ Y.
For instance, we have id : P −→ P and id : N −→ N for any P, N. We also have the following characterization of function spaces:
f : P −→ Q ⇐⇒ 6N ∈ Q⊥, 6P ∈ P, JP [f [N/x0]/x0]) = H;
f : N −→ M ⇐⇒ 6P ∈ M⊥, 6N ∈ N, JP [f [N/x0]/x0]) = H.
As an immediate consequence, we have f : P −→ Q ⇐⇒ f : Q⊥ −→ P⊥. Also, the above makes clear that every function space is a “nonatomic” behaviour.
Composition of typed functionals is naturally typed: if f : P −→ Q, g : Q −→ R
and f j : N −→ M, gj : M −→ K, we have by Proposition 2.6:
Proposition 5.2 (Composition) g ◦∗ f : P −→ R and gj ◦∗ f j : N −→ K.
Given identity and composition, it is tempting to think of a category in which objects are positive (or negative) behaviours and morphisms are functionals. This, however, does not work, since the identity morphism is not unique. As an extreme case, consider the “minimal” positive function space 0 −→ 0. In the usual sense, there is only one (total) function here: the one which sends H to H. On the other hand, 0 −→ 0 contains all the functionals, since we have f ∗(H) = H for any f . They all play the same role, sending H to H. Hence they are equivalent from the viewpoint of 0 −→ 0.
To obtain a category, we therefore need to quotient functionals f : X −→ Y
by such an equivalence relation depending on X and Y. Similarly, elements of a

behaviour X can be equipped with such a relation: D and E are equivalent in X whenever they only differ in useless parts i.e., subdesigns which play no active role when normalizing against designs of X⊥. Interestingly, ludics is already equipped with such a relation, “equality up to materiality ” [9].
Materiality, section, retraction and isomorphism
Informally, two functionals f, g ∈ X −→ Y, are “equal up to materiality” in X −→ Y if they share the “minimal” part h which is really necessary during any computation with designs of X and Y⊥. For example, given f : 0 −→ 0, no part of f is necessary for computations, because JH[f /x0]) immediately gives H, whatever f is.
To formalize this concept, we first recall the notion of stable ordering ± between designs [9,5,17]. Informally, D ± E whenever D is obtained from E by replacing some positive subdesign with Ω. Formally, ± is the largest binary relation on standard designs such that:
if Ω ± D, then D is a positive design;
if H ± D, then D = H;
if x|a⟨N1,... , Nn⟩± D, then D = x|a⟨M1,... , Mn⟩, Ni ± Mi, 6i, 1 ≤ i ≤ n;
if Σ a(x).Pa ± D then D = Σ a(x).Qa and Pa ± Qa for every a ∈ A.
An important property of the stable ordering is monotonicity: given standard designs D, E, N, M such that D ± E and N ± M , we have JD[N/x0]) ± JE[M/x0]) (see [17]).
We also define the intersection ∩ of standard designs by corecursion as:
P ∩ Ω=Ω ∩ P = Ω;
H ∩ H = H;
x|a⟨N1,... , Nn⟩∩ x|a⟨M1,... , Mn⟩ = x|a⟨N1 ∩ M1,... , Nn ∩ Mn⟩ if Ni ∩ Mi are defined for every 1 ≤ i ≤ n;
Σ a(x).Pa ∩ Σ a(x).Qa = Σ a(x).(Pa ∩ Qa) if Pa ∩ Qa is defined 6a ∈ A;
D ∩ E is not defined otherwise.
Let A be a behaviour X or function space X −→ Y. We define the material part of D ∈ A as |D|A :=  {E ± D : E ∈ A}. Designs D, E are said equal up to materiality in A, D ∼A E, whenever |D|A = |E|A.
The definition of materiality is justified by the fact that |D|A is the minimal design in A such that |D|A ± D (see [9,17] for a proof). So, each equivalence class induced by ∼A has a canonical and unique representative in A.
For example, in the function space 0 −→ 0 all the functionals are equal up to materiality. Indeed, for any f we have |f |0—→0 = 0, where 0 := a(x).Ω (called negative skunk in [9]) is the minimal negative design w.r.t. ±.

Lemma 5.3 D ∼A E if and only if EF ∈ A such that F ± D and F ± E.
Theorem 5.4 (Preservation of ∼)
If P ∼P Q and f : P −→ Q then f ∗(P ) ∼Q f ∗(Q);
If P ∈ P and f ∼P—→Q g then f ∗(P ) ∼Q g∗(P ). Similarly for negative behaviours and negative function spaces.
Proof. (i) By Lemma 5.3, if P ∼P Q then ER ∈ P such that R ± P and R ± Q. Applying f , we have that f ∗(R) ∈ Q and by monotonicity f ∗(R) ± f ∗(P ) and f ∗(R) ± f ∗(Q). By Lemma 5.3 again, we conclude f ∗(P ) ∼Q f ∗(Q). For (ii) and for negatives we use a similar reasoning.	2
We are now ready to build the category P of positive behaviours: the objects of P are positive behaviours and HomP (P, Q) consists of equivalence classes of functionals f : P −→ Q with respect to ∼P—→Q. the category N of negative behaviours is defined similarly. As before, we have Pop ∼= N where the isomorphism is given by P '→ P⊥ on objects and identity on morphisms.
We may thus employ categorical concepts in ludics. In particular, we have the notions of section, retraction and isomorphism, which may be described in elementary terms as follows:
Definition 5.5 Let P, Q be positive behaviours and r, s be functionals such that r : P −→ Q and s : Q −→ P. When r ◦∗ s ∼Q—→Q id, we say that s is a section of r and r is a retraction of s. If s ◦∗ r ∼P—→P id holds in addition, s and r are called isomorphisms. Similarly for negatives.
In Section 7, we shall show that f(α,β,i) is a retraction of u(α,β,i). To state it precisely, we shall however need to clarify in which function spaces they live.
Logical behaviours
We now describe how behaviours can be generated by the logical connectives we gave in Section 3.2. Let α(z1,... , zn) = {a1(x1),... , am(xm)} be an n-ary logical connective. Since {xi} ⊆ {z} for any 1 ≤ i ≤ m, x it is of the form zi1 ,. , zik
with k = ar(ai) and i1,... , ik ∈ {1,... , n}. Let N1,... , Nn, P1,.  , Pn be arbitrary
behaviours. We define:
α⟨N ,... , N ⟩ :=	a ⟨N ,... , N	⟩ ⊥⊥, where indices i ,... ,i	are
given by xi = zi1 ,... , zik  for each 1 ≤ i ≤ m and ai⟨Ni1 ,... , Nik ⟩ :=
{x0|ai⟨N1,... , Nk⟩ : N1 ∈ Ni1 ,... , Nk ∈ Nik };
α(P1,... , Pn) := α⟨P⊥,... , P⊥⟩⊥.
1	n
Categorically, every n-ary logical connective α defines (covariant) functors α :
Pn −→ N and α : N n −→ P.
A remarkable property of logical connectives is internal completeness [9]: we can give a precise and direct description of the elements in logical behaviours without using the orthogonality nor referring to any proof system:

α⟨N1,... , Nn⟩ =  1≤i≤m ai⟨Ni1 ,... , Nik ⟩∪ {H}.
α(P1,... , Pn) = {Σ ai(xi).Pai  : Pai  |= zi1  : Pi1 ,... , zik  : Pik  for every
1 ≤ i ≤ m}, where the expression Pai |= zi1 : Pi1 ,... , zik : Pik is a short for 6N1 ∈ Pi1 ⊥,... , 6Nk ∈ Pik ⊥, JPai [N1/zi1 ,... , Nk/zik ]) = H. Notice that the components b(y).Pb’s can be arbitrary when b(y) ∈/ α.
Recall that we have expressed the standard MALL connectives T, ↑, `, & as logical connectives in Section 3.2. With these logical connectives we can build (semantic versions of) usual linear logic types. For sake of readability, we use the notation 0 = T, ↓= ↑, • = ℘, ⊗ = `, ιi = πi, and ⊕ = &.
By using the infix notation and taking into account the internal completeness, we obtain concrete descriptions (irrelevant components of sums are suppressed by “··· ”):

N ⊗ M = •⟨N, M⟩∪ {H},	P ` Q = {℘(x1, x2).P + ·· · : P |= x1 : P, x2 : Q}, N ⊕ M = ι1⟨N⟩∪ ι2⟨M⟩∪ {H},	P & Q = {π1(x0).P + π2(x0).Q + ·· · : P ∈ P,Q ∈ Q},
↓ N = ↓ ⟨N⟩∪ {H},	↑ P = {↑(x0).P + ·· · : P ∈ P},
0 = {H},	T = D−.

As to the synthetic connective γ = synth(`, &, 1), we have:

γ(N, M, K) = ℘[π1/1]⟨N, K⟩∪ ℘[π2/1]⟨M, K⟩∪ {H};
γ(P, Q, R) = {℘[π1/1](y1, x2).P + ℘[π2/1](y2, x2).Q + ··· 
: P |= y1 : P, x2 : R and Q |= y2 : Q, x2 : R}.

An analysis of focalization in ludics
Let us collect all we have done so far in order to obtain our main results.

Focalization in behaviours
Let α(z1,... , zn), β(u1,... , um) be logical connectives, 1 ≤ i ≤ n and N1,... , Ni—1, M1,... , Mm, Ni+1,... , Nn behaviours. We can form an “unsynthe- sized” behaviour P and a “synthesized” one Q by:

P = α⟨N1,... , Ni—1, ↑ β⟨M1 ... Mm⟩ , Ni+1,... , Nn⟩,
Q = γ⟨N1,... , Ni—1, M1 ... Mm, Ni+1,... , Nn⟩,
where γ = synth(α, β, i). As we have noted in the introduction, the internal com- pleteness theorem provides an easy proof to the following:
Proposition 7.1 For P and Q as above, we have that
f(α,β,i) : P −→ Q,	u(α,β,i) : Q −→ P.

Hence, if P ∼P P j then f ∗
∗
(α,β,i)
(P j). The same for u(α,β,i).

Proof. For instance, suppose that P = (↑ (M1 ⊕ M2)) ⊗ N. If P ∈ P and P /= H, then by internal completeness P must be of the form
x0|• ⟨↑(y).y|ιj ⟨M ⟩,N ⟩,
where j = 1 or 2, M ∈ Mj and N ∈ N. Given such a concrete description, it is easy to calculate f(`,&,1)(P ) and to verify it belongs to γ⟨M1, M2, N2⟩. The argument is similar for u(α,β,i) : Q −→ P.	2
Before we proceed to the next theorem, let us explain why the functional u(α,β,i) is related to invertibility of negative connectives. Consider γ = synth(`, &, 1). Then the behaviour γ(P, Q, R) can be thought as if it were a “sequent” |= P & Q, R by identifying ` with comma. As we have seen above, internal completeness tells us that every design N in γ(P, Q, R) uniformly arises as N = ℘[π1/1](y1, x2).P +
℘[π2/1](y2, x2).Q+··· from designs P |= y1 : P, x2 : R and Q |= y2 : Q, x2 : R. This is nothing but invertibility of & (under the above identification). Notice that we do not have such a uniform description for the unsynthesized behaviour ↓ (P & Q) `R. On the other hand, the inverting functional u = u(`,&,1) sends a design in the latter to one in γ(P, Q, R) for which we have a uniform description. This is the reason why we call u an inverting design. We finally arrive at a formal statement of our slogan: focalization is a retraction of invertibility.
Theorem 7.2 (Section-retraction) For P and Q as above, f(α,β,i) : P −→ Q is a retraction of u(α,β,i) : Q −→ P.
Proof. By Proposition 7.1, we have that f = f(α,β,i) : P −→ Q and u = u(α,β,i) : Q −→ P and by Proposition 5.2 that f ◦∗ u : Q −→ Q. By Proposition 4.1, f ◦∗ u = idγ and since idγ ± id, f ◦∗ u ∼Q—→Q id.	2
As we have noted in the introduction, focalization is a context-sensitive phe- nomenon. Hence in order to claim that the focalizing designs really capture the essence of focalization, we must ensure that they work in context as well. Rather than dealing with “sequents of behaviours” |= Γ, P (for which the closure principle would be enough), we consider focalization inside a logical connective α(P1,.  , Pn).
Theorem 7.3 (Focalization in context) Let {si : Qi −→ Pi}1≤i≤n be a family of sections and {ri : Pi −→ Qi}1≤i≤n be the family of corresponding retractions. For any logical connective α(z1,... , zn), there exists a section s : α(Q1,. , Qn) −→
α(P1,... , Pn) with a corresponding retraction r.
That is to say, the section-retraction relation is preserved by logical connective α seen as functor α : Pn −→ N . This automatically ensures that focalizing and inverting designs work inside a logical connective as well.
Focalization for MALL
Finally we sketch how to combine our treatment of focalization with full complete- ness of ludics w.r.t. MALL [9] to obtain focalization for MALL. The argument

below is not intended to be a real proof, since the focalization theorem for LL is already proven many times and our argument is not easier (ours rather involves a delicate issue of exactness/linearity). Our purpose is just to illustrate how our results can be seen as a key “factor” of focalization for MALL under suitable factorization.
We consider the constant-only fragment of MALL, where formulas are generated by the grammar:
F ::= 0 | T | F ⊗ F | F ⊕ F | F ` F | F & F
and rules are just standard. Inductively, we define the following interpretation function ( )• which sends a formula into a negative behaviour:

0• := ↑ 0,	T• := T,	(F ⊗ F)• := ↑ (F• ⊗ F•),
(F ⊕ F)• := ↑ (F• ⊕ F•), (F ` F)• := ↓ F• ` ↓ F•, (F & F)• := ↓ F• & ↓ F•.
Let π be a cut-free proof of MALL formula F. By soundness theorem, we get a de- sign D ∈ F•. Now, we can repeatedly apply focalizing designs f1,... , fn (in context) in order to get a design (fn ◦∗ ... ◦∗ f1)∗(D) in which positive layers are maximally synthesized. For negative layers, we can apply sequences of inverting designs. We finally get a design Df in a behaviour F• which is maximally synthesized. For an example of positive layer, the formula (F ⊕ G) ⊗ H is sent to ↑ (↑ (F• ⊕ G•) ⊗ ↑ H•) from which we can find (a maximal) synthetic connective ↑ (γ⟨F•, G•, H•⟩). Now, we can apply the corresponding un,... , u1, where ui are respectively the sections of fi. We get a new design Dfu in F• built by MALL connectives with shifts and still focalized. It is clear that this procedure preserves H-freeness (while it is not immediate that it also preserves linearity/exactness [9]). Provided that it preserves exactness, full completeness of ludics yields a proof πj of F in MALL.

Conclusion and future works
We have attempted to analyze focalization in ludics. More specifically, we have detailed how to synthesize behaviours, and internally described focalization as fo- calizing functionals (designs) from unsynthesized to synthesized behaviours. We have also pointed out that every focalizing design is a retraction of an inverting design, which is related to invertibility of negative connectives. This in a way for- malizes the common intuition that focalization of positives is dual to invertibility of negatives.
Our work naturally leads to several directions for extension and we wish to stress one of them which seems particularly promising. It is related with an analysis of usual computability and complexity theory by logical means. Indeed, our analysis of focalization in ludics was primarily motivated by by the concluding remarks of the third author’s paper on computational ludics [17] where focalization on data designs (designs which represent usual first order data, such as natural numbers,

lists, etc) was conjectured to correspond to the tape compression theorem of Turing machines. Some further work is required to make this correspondence formal and then enlarge our focalization results.
Another important thing to be done is to reformulate ludics from a more general, categorical perspective and to relate our work with other approaches, like polarized categories (for polarized MALL) [8] and categories arising from game semantics. In doing so, it would be interesting to see ludics as a dialogue category, a fundamental concept for linear logic and polarized systems identified by Melli`es and Tabareau [14].

References
Abramsky, S.: Sequentiality vs. concurrency in games and logic. Mathematical Structures in Computer Science 13(4) (2003) 531–565.
Andreoli, J.-M.: Logic Programming with Focusing Proofs in Linear Logic. Journal of Logic and Computation 2(3) (1992) 297–347.
Basaldella, M., Saurin, A., Terui, K.: On the meaning of focalization. In: Proceedings of Workshop on Games, Dialogues and Interaction (2009).
Chaudhuri K.: Focusing Strategies in the Sequent Calculus of Synthetic Connectives. In: Proceedings of Logic for Programming, Artificial Intelligence and Reasoning (2008) 467–481.
Curien, P.-L.: Introduction to linear logic and ludics, part II. Advances in Mathematics (China) 35(1) (2006) 1–44.
Curien, P.-L., Munch-Maccagnoni, G.: The duality of computation under focus. In: Proceedings of IFIP TCS (2010).
Faggian, C., Piccolo, M.: Ludics is a model for the finitary linear pi-calculus. In: Proceedings of Typed Lambda Calculi and Applications (2007) 148–162.
Hamano, M., Scott, P.J.: A Categorical Semantics for Polarized MALL. Annals of Pure and Applied Logic 145(3) (2007) 276–313.
Girard, J.-Y.: Locus solum: From the rules of logic to the logic of rules. Mathematical Structures in Computer Science 11(3) (2001) 301–506.
Laurent, O.: E´tude de la polarization en logique. PhD thesis, Universit´e Aix-Marseille II (2002).
Laurent, O.: Polarized games. Annals of Pure and Applied Logic 130(1-3) (2004) 79–123.
Laurent, O.: A proof of the focalization property of Linear Logic. Unpublished note (2004).
Melli`es, P.-A.: Asynchronous Games 4: A Fully Complete Model of Propositional Linear Logic. In: Proceedings of Logic in Computer Science (2005) 386–395.
Melli`es, P.-A., Tabareau, N.: Resource modalities in tensor logic. Annals of Pure and Applied Logic
161(5) (2010) 632–653.
Miller, D., Saurin, A.: From proofs to focused proofs: a modular proof of focalization in Linear Logic. In: Proceedings of Computer Science Logic. (2007) 405–419.
Munch-Maccagnoni G.: Focalisation and Classical Realisability In: Proceedings of Computer Science Logic (2009) 409–423.
Terui, K.: Computational ludics. (2008) To appear in Theoretical Computer Science.
Zeilberger, N.: The logical basis of evaluation order and pattern matching. PhD Thesis, Carnegie Mellon University (2009).
