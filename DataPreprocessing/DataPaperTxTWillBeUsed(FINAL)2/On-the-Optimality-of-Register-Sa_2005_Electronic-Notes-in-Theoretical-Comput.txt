Electronic Notes in Theoretical Computer Science 132 (2005) 131–148 
www.elsevier.com/locate/entcs


On the Optimality of Register Saturation
Sid-Ahmed-Ali Touati1
University of Versailles, PRiSM laboratory, France

Abstract
In an optimizing compiler, the register allocation process is still a crucial phase since it allows to reduce spill code that damages the performances. The register constraints are generally taken into account during the instruction scheduling phase of an acyclic data dependence graph (DAG) : any schedule must minimize the register requirement. However, in a previous work [14], we introduced and mathematically studied the register saturation (RS) concept. It consists of computing the exact upper-bound of the register need for all the valid schedules, independently of the functional unit constraints. The goal of RS is to decouple register constraints from instruction scheduling. In this paper, we continue our theoretical efforts and we present two main results. First, we give an exact solution with integer linear programming for both the problems of computing the RS of a DAG and reducing it. Our integer program brings a new way to model register constraints that allows us to produce the lowest number of constraints and variables in the literature (till now).
Indeed, given a DAG with n nodes and m arcs, we need O(n2) integer variables and O(m + n2) linear constraints, which is better than the actual size complexity in the literature that model register constraints. Second, we prove that the problem of reducing the register saturation is NP- hard. Our detailed experiments in this paper show that our previous heuristics [14] are nearly optimal. We provide a discussion too in order to argument why the RS approach should be better that minimizing the register requirement.
Keywords: Register Pressure, Instruction Level Parallelism, Integer Linear Programming, Optimizing Compilation.


Introduction
Because of the introduction of instruction level parallelism (ILP), the classical techniques of register allocation for sequential code semantics are not adapted any more. Thus, the old graph coloring techniques should be reconsidered to be efficient in optimizing compilers for modern architectures. In [5], the au- thors showed that there is a phase ordering problem between the old register

1 Email: touati@prism.uvsq.fr



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2005.01.033


allocation techniques and ILP instruction scheduling. If a classical register al- location is done early, the introduced false dependences inhibit a good further ILP extraction. However, this conclusion does not prevent any compiler from performing effectively an early register allocation, but with the condition that the used allocator should be sensitive to the scheduler as done in [12,11,8].
Some other studies [13,10,5] claim that it is better to combine instruction scheduling and register allocation in a single complex pass, argumenting the fact that applying each method separately has a negative influence on the efficiency of the other. However, this phase ordering problem arises only if the applied first pass (ILP scheduler or register allocator) is “selfish”. Indeed, we still can effectively decouple register constraints from instruction scheduling if enough care is taken. In this paper, we show how we can treat register constraints before scheduling and we explain why we should do it.
The principal reason for handling register constraints before instruction scheduling our believe that the register allocation is more important as an optimization issue than code scheduling. This is because, usually, the code performances are far more sensitive to memory accesses than to fine-grain scheduling (memory gap) : a cache miss may inhibit the processor from achiev- ing a high dynamic ILP, even if the scheduler has extracted it at compile time. Even if someone would expect that spill codes exhibit high locality, and hence would likely produce cache hits, we cannot assert it at compile time since mem- ory access latencies are non predictable at compile time (we cannot guarantee where the date would be located). Furthermore, memory requests, even if they are data independent, exhibit high potential conflicts because of micro- architectural restrictions and simplifications in the memory disambiguation mechanisms (load/store queues) and possible banking structure in cache lev- els [7]. These possible conflicts may cause severe performance degradation even if enough ILP exist, and even if the data is located in the cache. Of course, our claim that spill code is more damaging is appropriate for the ar- chitectures where the memory access delay is very long compared to the delay of calculation. This is the case in almost all high performance processors. If memory access delay is not critical, the register saturation concept may be less useful.
Another reason for handling register constraints prior to ILP scheduling is that register constraints are much more complex than resource constraints. Scheduling under resource constraints is a performance issue. Given a data dependence graph (DDG), we are sure to find at least one valid schedule for any underlying hardware properties (a sequential schedule in extreme case, i.e., no ILP). However, scheduling a DDG with a limited number of registers is more complex. We cannot guarantee the existence of at least one schedule.



Fig. 1. Early Register Pressure Management

In some cases, we must introduce spill code and hence we change the problem (the input DDG). Also, a combined pass of scheduling with register allocation presents an important drawback if not enough registers are available. During scheduling, we may need to insert load-store operations if no enough free registers exist. We cannot guarantee the existence of a valid issue time for these introduced memory access in an already scheduled code; resource or data dependence constraints may prevent from finding a valid issue slot inside an already scheduled code. This fact forces to iteratively apply scheduling followed by spilling until reaching a solution.
All the above arguments make us re-think new ways of handling register pressure before starting the scheduling process, so that the scheduler would be free from register constraints and would not suffer from excessive serializations. For this reason, we presented in [14] our register saturation (RS) concept that prevent a DAG from producing an excessive number of values simultaneously alive for all the valid schedules. Our pre-pass analyzes a DAG (with respect to control flow) to deduce the maximal register need for all schedules. We call this limit the register saturation (RS) because the register need can reach this limit but never exceed it. If RS exceeds the number of available registers, we introduce new arcs to reduce it, see Figure 1. In this paper, we provide exact (optimal) methods to both the problems of computing RS and reducing it. After our RS analysis pass, the DAG is free from register constraints and can be sent to the scheduler and the register allocator.
This article is organized as follows. Section 2 presents our DAG and pro- cessor model which can be used for most of existing ILP architectures (super- scalar, VLIW, EPIC/IA64). Computing the optimal RS by intLP is given in Section 3. Our intLP formulation use the linear writing of logical formulas (=⇒, ⇐⇒, ∨) and the max operator (max(x, y)) by introducing extra binary variables, as previously described in [15]. The optimal solution of reducing RS


is provided in Section 4. Our large range of experiments show that our initial heuristics [14] are nearly optimal in Section 5. Before concluding, we make a discussion in Section 6 to argument why the RS concept is a better way for handling register constraints prior to ILP scheduling.

DAG and Processor Model
A DAG G = (V, E, δ) in our study represents the data dependences between the operations and any other serial constraints. The DAG is defined by its set of operations V , its set of edges E = {(u, v)/ u, v ∈ V }, and δ such that δ(e) is the latency of the edge e in terms of processor clock cycles. Let n be the number of nodes and m the number of arcs.
A schedule σ of G is a function which gives an integer execution (issue) time for each operation :
σ is valid ⇐⇒ ∀e = (u, v) ∈ E,  σ(v) − σ(u) ≥ δ(e) The set of all valid acyclic schedules of G is denoted by Σ(G).
We consider a target RISC-style architecture with multiple register types, where T denotes the set of register types (for instance, T = {int, float}). Some statements and some precedence constraints of a DAG have more at- tributes than others, depending if they refer to values to be stored in registers or not. VR,t is the set of values to be stored in registers of type t ∈ T . We consider that each statement u ∈ V writes into at most one register of a type t ∈ T . The statements which define multiple values with different types are accepted in our model if they do not define more than one value of a certain type 2 . ER,t is the set of flow dependence edges through a value of type t ∈ T . The set of consumers (readers) of a value ut is then the set :
Cons(ut)= {v ∈ V | (u, v) ∈ ER,t}
Some values in may not be consumed in the considered DAG. In order to model such exit values, we assume that the considered DAG contains a bottom node
⊥ that is the sink of the flow dependences of these exit values. Also, there is a serial arc from any other node of the DAG to this bottom node. The latency of such virtual arc is equal to the latency of the source operation. The bottom node ⊥ is always the last scheduled node of the DAG.
In order to consider static issue VLIW and EPIC/IA64 processors in which the hardware pipeline steps are visible to compilers (we consider dynamically

2 This model limitation, that has little impact on actual ILP processors, allows us to have some graph characteristics used for formal proofs in [15,14].


scheduled superscalar processors too), we assume that reading from and writ- ing into a register may be delayed from the beginning of the schedule time, and these delays are visible to the compiler (architectural visible). We define two delay (offset) functions δr and δw in which : the read cycle of ut from a register of type t is σ(u)+ δr(u), and the the write cycle of ut into a register of type t is σ(u)+ δw(u).
For instance, in superscalar and EPIC/IA64 processors, δr and δw are equal to zero.
When a schedule is fixed, we can easily compute how much register we need of each register type t in order to build a valid register allocation. It is the standard concept of the maximal number of values of type t simultane- ously alive, that is also equal to the maximal clique in the interference graph. To recall, two variables are said to be simultaneously alive iff their lifetime intervals interfere, and thus they cannot share the same register. The register requirement (or register need) of type t for a DAG G given a fixed schedule σ is noted RNσ(G).
After defining the DAG and processor model, the next section shows how we compute the exact optimal register saturation (RS).


Computing the Optimal Register Saturation
First of all, if |VR,t|, the total number of values of type t, is less than or equal to Rt, the number of available registers of type t, then we are sure that any schedule cannot require more than |VR,t| ≤ Rt registers. Otherwise, we must analyze the register saturation (RS).
The RS of a register type t for a DAG G is the maximal register need for all valid schedules of this DAG :


RSt(G)= max
σ∈Σ(G)
RNσ(G)


We proved in [14] that computing this parameter is an NP-complete problem and we provided a heuristics. Below, we give the set of variables and con- straints of an exact intLP for computing RSt(G). Our intLP formulation use the linear writing of logical formulas (=⇒, ∨, ⇐⇒) and the max operator (max(x, y)) by introducing extra binary variables, as previously described in [15]. However, that linear writing of logical and max operators requires to bound the domain set of the integer variables.

Scheduling Variables
For all operations u ∈ V , we define the integer variable σu ≥ 0 that holds the schedule time. Note that these schedule variables do not represent the final schedule under resource constraints (that will be computed after our RS pass), but they only represent intermediate variables for our intLP formulation. The first linear constraints are those that describe precedence relations, so we write into the intLP system :
∀e = (u, v) ∈ E,	σv − σu ≥ δ(e)
There are O(n) scheduling variables and O(m) linear scheduling constraints. In order to bound the domain set of our variables, we define T a worst possible schedule time. We choose T sufficiently large, where for instance T = e∈E δ(e) is a suitable worst total schedule time (case of no ILP). Then, we write the following constraint :
∀u ∈ V, σu ≤ T
As a consequence, we deduce for any u ∈ V :
σu ≥ σu = LongestP athT o(u) is the “as soon as possible” schedule time;
σu ≤ σu = T − LongestP athF rom(u) is the “as late as possible” schedule time according to the worst total schedule time T .
Register Need Constraints
Interference Graph
The lifetime interval of a value ut of type t is (given a schedule σ)

LTσ(ut) =]σu + δw(u),	max
v∈Cons(ut)
 σv + δr(v) ]

That is, we assume that a value written at instant c in a register is available one step later (the lifetime interval is left open). Thus, if an operation u reads from a register at instant c while another operation v is writing in it at the same time, u does not get v’s result but gets the value previously stored in this register. Note that this is a choice and not a limitation of the model.
We define for each value u the variable kut ≥ 0 that computes its killing date (the last time that this value is read). The number of such defined variables isO(n). Since our variable domains are bounded (assuming a finite T ), we know that kut is bounded by the two following finite schedule times :

∀t ∈ T , ∀u ∈ VR,t :	kut ≤ kut ≤ kut
where

kut = σu + δw(u) is the first possible definition date of ut;



kut = maxv∈Cons(ut)  σv + δr(v)  is the latest possible killing date of ut.
We use the linear constraints of the max operator to compute kut as explained in [15]. We write into the intLP system :

∀u ∈ VR,t :	kut =	max
v∈Cons(ut)
 σv + δr(v) 

The total complexity to define all killing dates for all registers types is bounded by O(n2) variables and O(n2) constraints.
Now, we can consider Ht the undirected interference graph of G for the register type t. For any couple of distinct values ut, vt ∈ VR,t, we define

a binary variable st
∈ {0, 1} such that it is set to 1 if the two lifetimes

intervals of type t interfere : ∀t ∈ T , ∀ couple ut, vt ∈ VR,t :
⎧⎨ 1 if LTσ(ut) ∩ LTσ(vt) /= φ

t u,v
=
⎩ 0	otherwise

The number of variables st	is the number of combinations of 2 values among

|VR,t|, i.e.,  |VR,t|× (|VR,t|− 1) /2.
LTσ(ut) ∩ LTσ(vt) = φ means that one of the two lifetime intervals is “before” the other, i.e.,  LTσ(ut) ≺ LTσ(vt)  ∨  LTσ(vt) ≺ LTσ(ut) , where
≺ denotes the “before” relation in the interval algebra. Then, we have to express the following constraints :



t u,v
=1 ⇐⇒ ¬ LTσ(ut) ≺ LTσ(vt) ∨ LTσ(vt) ≺ LTσ(ut) 

where LTσ(u ) ≺ LTσ(v ) iff kut ≤ σv + δw(v). The negation of this constraint is kut > σv + δw(v),i.e.,
kut − σv − δw(v) − 1 ≥ 0. Since su,v ∈ {0, 1}, these variables are constrained
as follows :



t u,v
≥ 1 ⇐⇒ ⎧⎨ kut − σv − δw(v) − 1 ≥ 0
⎩ kvt − σu − δw(u) − 1 ≥ 0

Given three logical expressions (P, Q, S), (P ⇐⇒ (Q ∧ S)) is equivalent to the expression (P ∧ Q ∧ S) ∨ (¬P ∧ ¬Q) ∨ (¬P ∧ ¬S). We write these two dis- junctions with linear constraints by introducing binary variables (see [15]) and by computing the finite lower bounds of the linear functions. The complexity
of computing all the st	variables is bounded by O(n2) binary variables and
constraints.


Maximal Clique in the Interference Graph
The maximum number of values of type t simultaneously alive corresponds to a maximal clique in Ht = (VR,t, Et), where (ut, vt) ∈ Et iff their lifetime in-

tervals interfere (st
= 1). For simplicity, rather than considering the interfer-

ence graph itself, we prefer to consider its complementary graph H' = (VR,t, E')
t	t

where (ut, vt) ∈ E' iff their lifetime intervals do not interfere (st
= 0). Then,

t	u,v
the maximum number of values of type t simultaneously alive corresponds to
a maximal independent set in H'.
To write the constraints that describe independent sets (IS), we define a
t
binary variable xut ∈ {0, 1} for each value u ∈ VR,t such that xut = 1 iff
ut belongs to some IS of H'. We express in the model the following linear
constraints :

∀xut , xvt ∈ VR,t :	su,v =0 =⇒ xut + xvt ≤ 1

This equations means that if two nodes u and v are connected in H', then one and only one of them may belong to an IS. The number of variables xut is O(n). The number of introduced binary variables to express all the implications is bounded by O(n2). The number of linear constraints to define the IS is bounded by O(n2).

Linear Function of Register Need
The register requirement of type t is a maximal IS in H', i.e., the maximal
Σ	t


Maximize	Σ
ut∈VR,t


xut


The total number of integer variables in our whole intLP is bounded by O(|V |2), and the total number of constraints is at most O(m + n2). Note that our intLP formulation may be optimized by considering that:
an edge e = (u, v) in the initial DAG is redundant for the scheduling con- straints and can be safely ignored if lp(u, v) > δ(e) where lp(u, v) denotes the longest path from u to v (with the condition that this arc doesn’t belong to this longest path);
two values (ut, vt) ∈ VR,t can never be simultaneously alive iff for all the possible schedules, one value is always defined after the killing date of the

other. This is the case if any of the two following conditions is satisfied :
∀v' ∈ Cons(vt):	lp(v', u) ≥ δr(v') − δw(u)
∨ ∀u' ∈ Cons(ut):	lp(u', v) ≥ δr(u') − δw(v)
After computing the optimal RS, the next section shows how we reduce it if it exceeds a limit.

Optimal Register Saturation Reduction
In the case where the register saturation RSt(G) exceeds the number of avail- able registers Rt of the type t, then we must add extra serial arcs into the DAG G to reduce RSt(G) below this limit. The new added arc must save ILP as much as possible by taking care of the critical path. We note by E the set of extra edges that we add to G to build a new extended DAG, namely G = G\E, such that RSt(G) ≤ Rt. We want to solve the formal problem stated below.
Definition 4.1 [ReduceRS Problem] Let G = (V, E, δ) be a DAG. Let Rt and P be two positive integers. Does there exist an extended DDG G = G\E of G such that :


and
RSt(G) ≤ Rt

CriticalP ath(G) ≤ P 

Theorem 4.2 ReduceRS problem is NP-hard.
Proof.
We prove that ReduceRS problem reduces from the problem of scheduling under register constraints. Let us start by defining the latter problem. For the sake of clarity of this proof, we assume that the considered register type t is implicit (we do not include t in our notations inside this proof).
Definition 4.3 [SRC problem]
Let G = (V, E, δ) be a DAG, R be a positive integer, and P be a length.
Does there exist a valid schedule σ ∈ Σ(G) such that :
RNσ(G) ≤ R 
and
total schedule time ≤ P 


SRC problem has been proven NP-hard in [4]. Now we prove that both ReduceRS and SRC problems are equivalent in terms of computational com- plexity.
ReduceRS =⇒ SRC
Let G be a solution for the ReduceRS problem. Then trivially, any as soon as possible schedule σ ∈ Σ(G) is a solution for SRC.
SRC =⇒ ReduceRS
Let σ be a solution for SRC, i.e., RNσ(G) ≤ R and the total schedule time is ≤ P. We build an extended DDG G by adding serial arcs to impose value lifetimes of any schedule of G to have the same precedence relations as defined by σ. ∀u, v ∈ VR/LTσ(u) ≺ LTσ(v) then we add the following arcs :
if v ∈ Cons(u), then add serial arcs from the other u’s readers (except v) to v; the set of added arcs is :
 e = (u', v)/ u' ∈ Cons(u) − {v}}
else, add serial arcs from all u’s readers to v; the set of added arcs is :
 e = (u', v)/ u' ∈ Cons(u)}
The latency of these added arcs has to be chosen depending on the target codes. We have two cases,
in the case of superscalar codes, the semantics is sequential. So, the latency of each added arc is set to 1;
in the case of VLIW or EPIC/IA64, there exist reading and writing offsets 3 . Thus, for each added arc e = (u', v), the latency is set to δ(e)= δr(u') − δw(v).
Indeed, the added arcs and the chosen latencies force the following asser- tion :
LTσ(u) ≺ LTσ(v) =⇒ ∀σ ∈ Σ(G):	LTσ' (u) ≺ LTσ' (v)
Then, for all values non simultaneously alive according to σ, there is no sched- ule σ' of G that makes them simultaneously alive. Formally, it is written :

¬ ∃u, v ∈ VR, LTσ(u) ≺ Lσ(v), ∃σ' ∈ Σ(G)/ LTσ' (u) ∩ LTσ' (v) /= φ 

3 On EPIC/IA64 architectures, a writer and a reader can be scheduled at the same instruc- tion group, so the writing delay is statically considered as zero.



In other words, we ensure that any schedule of G will guarantee the precedence relations between the lifetime intervals of G according σ. Consequently, any schedule σ' of G cannot need more than the register need of σ and

RS(G)= RNσ(G) ≤ R 

A solution for SRC problem may create a circuit in the solution of Re- duceRS. We are sure that if any circuit is introduced in G, then it must be non-positive because there exists at least the valid schedule σ ∈ Σ(G). Con- sequently, a solution of the ReduceRS problem may produce a cyclic DDG. We will see later how to eliminate these solutions.
With regard to the critical path of G, the introduced serial arcs ensure that at least σ ∈ Σ(G). Since there exists such a schedule with a total time
≤ P, the critical path of G cannot be longer than P.	 

The proof of Theorem 4.2 gives the intuition for optimal solution of the ReduceRS problem using integer programming. It is computed in two steps :
we first compute a valid schedule σ such that the register need of type t is maximized and does not exceed Rt, while the total schedule time is bounded. Again, this schedule is different from the final one to be computed under resource constraints;
then, we add serial arcs as described by the proof of Theorem 4.2. This results in an extended DDG that has a bounded register saturation with a minimized critical path.
To compute such a schedule, we use our intLP formulation previously de- fined in Section 3 that maximizes the register need. We keep all the constraints and variables of Section 3, except those that compute a maximal independent set. Now, we use a binary variable xi t which is set to 1 if the value ut is stored in the register i. Since there are Rt available registers, we have at most |V |× Rt variables. Since Rt is a constant is our problem (the number of registers in the target machine), the number of these variables is O(|V |).
The intLP system tries to build a coloring of the interference graph with exactly Rt colors (the maximal number of available registers). If no solution can be found with Rt registers, then solve another intLP after decrementing Rt (until to 1). If no final solution can be found when reaching one avail- able register, then the register saturation cannot be reduced and spilling is unavoidable. The variables xi t are computed using following constraints.

a value ut is stored in only one register of type t :

Rt
∀t ∈ T , ∀ut ∈ VR,t :	i
i=1
if two values interfere, then they cannot share the same register :

∀t ∈ T , ∀ couple ut, vt ∈ VR,t : st
≥ 1 =⇒ xi t + xi t ≤ 1, ∀i = 1, ..., Rt 

There are at most O(V 2 × Rt)= O(V 2) such constraints.
The objective function minimizes the total schedule time :
Minimise σ⊥
We add at most O(|V |2) constraints to the previous intLP system of Sec- tion 3.
As explained before, our DAG and processor model include writing and reading offsets. Consequently, in some cases, the optimal RS reduction may need to introduce non-positive circuits into the original DAG. Even if such non-positive circuits do not prevent the graph from being scheduled, they still violate the DAG property and impose hard scheduling constraints that may not be satisfiable under resource constraints in the subsequent pass of instruction scheduling. We must eliminate such optimal solutions as explained in [15]. Basically, we add at most O(n3) variables and O(m + n3) constraints to guarantee the existence of a topological sort for extended DDG. Again, these constraints have only to be added for VLIW and EPIC codes, not for superscalar ones.

Experiments
We did an extensive set of experiments on some scientific codes extracted from SpecFP, whetstone, livermore and linpack We used CPLEX to solve our intLP programs. Since all the problems of RS computation and reduction are NP- hard, reaching the optimal solutions were very time consuming (from many seconds to many days). The experimented DAGs are simply some loop bodies (excluding branches). Detailed numerical results and plots are shown in [15]. Basically, all our heuristics presented in [14] are nearly optimal.
Regarding RS computation, the maximal empirical error is one register (in very few cases). For RS reduction, we have to explore two parameters : the reduced RS and the critical path. We note RS and ILP the RS reduction and ILP loss resulted from optimal intLP programs; we note RS∗ and ILP ∗ the RS


reduction and ILP loss resulted from our heuristics. Then, our experiments are divided into the following sets.

In the case where RS = RS∗, our algorithm succeeds in optimally reduc- ing RS. Then, the ILP loss may be :
ILP = ILP ∗ (72.22% of all the results). Our algorithm succeeds in optimally reducing RS with the optimal ILP loss.
ILP < ILP ∗ (18.5% of all the results). Our algorithm succeeds in optimally reducing RS but with sub-optimal ILP loss.
ILP > ILP ∗ impossible !
In the case where RS > RS∗, our algorithm did not succeed in optimally reducing RS. Then, the ILP loss may be :
ILP = ILP ∗ (4.63% of all the results ). Our algorithm has sub- optimal RS reduction but optimal ILP loss.
ILP < ILP ∗ (less than 1% of all the results). Our algorithm has sub-optimal RS reduction with sub-optimal ILP loss.
ILP > ILP ∗ (3.7% of all the results). Our algorithm has sub-optimal RS reduction but with super-optimal ILP loss. This case is interest- ing : since our algorithm has sub-optimal RS reduction, then it gets extra registers which allow him to exploit more ILP.
The case where RS < RS∗ is impossible because our heuristics computes a valid RS∗.
Clearly, our RS reduction algorithm is very efficient : it, in most of times, optimally reduces RS with optimal ILP loss. Sub-optimal ILP loss is, in most of times, accompanied with optimal RS reduction, while sub-optimal RS reduction is mostly accompanied with super-optimal ILP loss. We get both sub-optimal ILP loss and sub-optimal RS reducing in less than 1% of the cases.

Discussion : to minimize or to saturate the register need ?
The literature contains a lot of techniques about minimizing the register re- quirement in superscalar (sequential) codes that are sensitive to ILP schedul- ing [6,8]. Others prefer to combine ILP scheduling with register allocation [2,5,9]. All these techniques try to minimize the register requirement. In our method, we use the contrary approach : we maximize the register requirement in order to minimize the amount of added arcs as previously done by Berson in [1]. We provided in [14] a comparison with his method to show its limitations that we fixed in our work.
Minimizing the register requirement is inherently a worse technique than

saturating it because of many reasons. We explain them below.

Case where register pressure is zero
Given a DAG, we do not need to add serial arcs if RS does not exceed the number of available registers. While the minimization approach add extra arcs, our method doesn’t. For instance, look at Figure 2, where bold circles are the values to be stored in registers and bold arcs are the flow dependences. The initial DAG has clearly a register saturation equal to 4 : this is because we can schedule the 4 operations {a, b, c, d} so as to produce 4 values simultaneously alive. If the processor has at least 4 registers, then the DAG is let as it is for the subsequent scheduler. However, with a minimization approach, the new DAG in Part (b) is restricted to not require more than 2 registers 4 , regardless the number of available ones. As can be seen, the DAG in Part (b) is more restrictive than the initial one that is let free by the RS analysis pass.

How much arcs we introduce
If the inherent data dependences of a considered DAG produce a restric- tive register pressure on a further ILP scheduler (case where RS exceeds the number of available registers), the minimization approach add more arcs than the RS reduction approach. This is because our method introduce only the sufficient amount of arcs to reduce RS below the considered limit. However, the minimization approaches try to reduce the register need at the lowest pos- sible level, which is not the appropriate method since it does not fully utilize the available registers. For instance, look at Figure 2 and assume we have 3 registers available. Part (c) shows the new DAG produced by the RS reduc- tion pass : here, RS is reduced from 4 to 3, and hence we have less arcs than what produces the minimization approach in Part (b). For the former, the final allocator would use 1, 2 or 3 registers depending on the schedule; for the latter, we would use only 1 or 2 registers , which is more restrictive. Hence, the RS concept helps to better take benefit from available registers.

When both methods are equivalent
If the target processor is a superscalar out of order, and if its dynamic scheduler is optimal and the register renaming hardware support has an in- finite number of hidden registers, both methods (RS and register need mini- mization) should be equivalent. With limited number of hidden registers for renaming, and a sub-optimal runtime scheduler, our RS method is likely to produce better codes because it makes better use of the available registers.

4 Here, we minimize the register requirement under critical path constraints.





1

17












(a) Intial DAG	(b) Minimal Register Need	(c) RS Reduction with 3 available registers

Fig. 2. RS Reduction vs. Minimal Register Requirement

Our method apply for explicit reading/writing offsets
Our DAG and processor model admit explicit reading from and writing to registers. Thus, our method is more generic than the existing techniques, and can be applied for superscalar, VLIW and EPIC codes. For the two later cases, a special care must be taken when reducing RS : we must prohibit non-positive circuits in the resulted DAGs.



In the case of a global scheduler
Our model assumes that there is only one possible definition per value. This assumption is correct inside a basic bloc (BB), i.e., if the code does not contain branches. In the case of a global control flow graph (CFG), a static data dependence analysis may provide for some values more than one definition because it cannot determine which execution path is taken. Actually, we have shown in [15] how to extend RS analysis to a global acyclic CFG (excluding loops) and its interaction with a global instruction scheduler that may move operations from one BB to another. However, we must be aware that in global register allocation, the number of allocated registers may be superior to MAXLIVE, because of the possible insertion of extra “move” operations. We think that we can prove that the optimal difference is at most one extra register by using the theoretical results of [3]. So, we can still use the register saturation concept in a global acyclic CFG by taking into account this possible extra register. For example, this can be done by decrementing R the number of available registers, so that the final register allocation cannot exceed R, even if move operations have been inserted.


Similar Work
Our theoretical framework is an extension to the previous study done by Berson [1]. A better explanation of the differences between our heuristics [14] and Berson’s one can be retrieved from [14]. To summarize :
our work is formal, so we could prove all our assertions. We are able to guarantee that the generated code will be correct ;
we showed in [14] that Berson’s work contained some mistakes and incom- plete proofs. For instance, the killing relations between the nodes have to be carefully chosen, otherwise the computed register saturation would be incorrect. This important aspect wasn’t covered in [1], so its proof of the NP-completeness of computing RS was incomplete.
our work is an extension to VLIW and EPIC codes, with multiple register types ;
our experiments show that our heuristics are nearly optimal, while Berson’s heuristics wasn’t proved so.
Pinter’s work [12] has some relation to ours, because she computes the maximal interference graph (with graph coloring) for all possible schedules of the basic blocks. However, first, her technique works only for superscalar codes, and does not fit neither for VLIW nor EPIC codes. Second, her model does not take into account multiple register types. Third and last, she didn’t prove that her method is close to the optimal.

Conclusion
In this paper, we continue our early work about the register saturation (RS) notion to manage register pressure and to avoid spill code before scheduling and register allocation steps. We believe that register constraints must be taken into account before ILP scheduling, but by using the RS concept and not by the existing minimization strategies. Otherwise, the subsequent ILP scheduler would be restricted even if enough registers exist.
Computing the register saturation of a DAG is NP-complete. An intLP exact formulation is presented. Our formal mathematical modeling and the- oretical study in [14] enables us to give nearly optimal heuristics. In the presence of branches, global RS of an acyclic CFG is brought back to RS in DAGs (basic blocs) by inserting entry and exit values with the corresponding flow arcs (see [15]).
If RS exceeds the number of available registers, we must reduce it while minimizing the increase of critical path. This is an NP-hard problem. An op- timal exact RS reduction method based on integer programming is presented.


If we assume writing offsets (VLIW and EPIC codes), some optimal solutions may require to insert non-positive circuits in the original DAG. These cir- cuits may prevent the extended DDG from being scheduled in the presence of resource constraints. A sufficient and necessary condition to overcome this problem is to guarantee the existence of a topological sort for the extended graph. This is done by adding new constraints to the intLP formulation. We have also proved that our initial algorithmic heuristics for RS reduction is very efficient compared to the optimal solutions.
An important problem (let for a future work) is the minimal spill code insertion in data dependence graphs. The existing studies insert spill opera- tions either in sequential codes (regardless on FUs usage), or by iterating ILP scheduling followed by spilling. We think that this problem must be taken into account at the data dependence graph level in order to break this iterative problem.

References
David A. Berson. Uniﬁcation of Register Allocation and Instruction Scheduling in Compilers for Fine-Grain Parallel Architecture. PhD thesis, Pittsburgh University, 1996.
Thomas S. Brasier. FRIGG: A New Approach to Combining Register Assignment and Instruction Scheduling. Master thesis, Michigan Technological University, 1994.
Dominique de Werra, Christine Eisenbeis, Sylvain Lelait, and Bruno Marmol. On a Graph- Theoretical Model for Cyclic Register Allocation. Discrete Applied Mathematics, 93(2-3):191– 203, July 1999.
Christine Eisenbeis, Franco Gasperoni, and Uwe Schwiegelshohn. Allocating Registers in Multiple Instruction-Issuing Processors. In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT’95, pages 290–293. ACM Press, June 27–29, 1995.
S. M. Freudenberger and J. C. Ruttenberg. Phase Ordering of Register Allocation and Instruction Scheduling. In Code Generation – Concepts, Tools, Techniques. Proceedings of the International Workshop on Code Generation, pages 146–172, London, 1992. Springer-Verlag.
R. Govindarajan, H. Yang, C. Zhang, J. N. Amaral, and G. R. Gao. Minimum Register Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs. In Proceedings of the 15th International Parallel and Distributed Processing Symposium (IPDPS-01), pages 26–26, Los Alamitos, CA, April 23–27 2001. IEEE Computer Society.
William Jalby and Christophe Lemuet. WBTK: A New Set of Microbenchmarks to Explore Memory System Performance. In Los Alamos Computer Science Institute (LACSI) Symposium, October 2002.
Johan Janssen. Compilers Strategies for Transport Triggered Architectures. PhD thesis, Delft University, Netherlands, 2001.
D. Kaestner and M. Langenbach. Code Optimization by Integer Linear Programming. Lecture Notes in Computer Science, 1575:122–136, 1999.
Waleed M. Meleis. Dural-Issue Scheduling for Binary Trees with Spills and Pipelined Loads.
SIAM J. Comput., 30(6):1921–1941, March 2001.


Cindy Norris and Lori L. Pollock. A Scheduler-Sensitive Global Register Allocator. In IEEE, editor, Supercomputing 93 Proceedings: Portland, Oregon, pages 804–813, 1109 Spring Street, Suite 300, Silver Spring, MD 20910, USA, November 1993. IEEE Computer Society Press.
Schlomit S. Pinter.  Register Allocation with Instruction Scheduling: A New Approach.
SIGPLAN Notices, 28(6):248–257, June 1993.
Rau´l Silvera, Jian Wang, Guang R. Gao, and R. Govindarajan. A Register Pressure Sensitive Instruction Scheduler for Dynamic Issue Processors. In Proceedings of the 1997 International Conference on Parallel Architectures and Compilation Techniques (PACT-97), pages 78–89, San Francisco, California, November 1997. IEEE Computer Society Press.
Sid-Ahmed-Ali Touati. Register Saturation in Superscalar and VLIW Codes. In Proceedings of The International Conference on Compiler Construction, Lecture Notes in Computer Science. Springer-Verlag, April 2001.
Sid-Ahmed-Ali Touati. Register Pressure in Instruction Level Parallelisme. PhD thesis, Universit´e de Versailles, France, June 2002. ftp.inria.fr/INRIA/Projects/a3/touati/thesis.
