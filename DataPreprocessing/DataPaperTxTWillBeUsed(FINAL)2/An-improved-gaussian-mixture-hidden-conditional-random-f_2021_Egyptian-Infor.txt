Egyptian Informatics Journal 22 (2021) 45–51








Full length article
An improved gaussian mixture hidden conditional random fields model for audio-based emotions classification
Muhammad Hameed Siddiqi
Department of Computer Science, Jouf University, Sakaka, Saudi Arabia



a r t i c l e  i n f o 

Article history:
Received 28 November 2019
Revised 23 January 2020
Accepted 29 March 2020
Available online 15 April 2020

Keywords:
Emotion classification Conditional random fields Hidden markov model Gaussian mixture model
a b s t r a c t 

The analysis of human emotions plays a significant role in providing sufficient information about patients in monitoring their feelings for better management of their diseases. Audio-based emotions recognition has become a fascinating research interest for such domains during the last decade. Mostly, audio-based emotions systems depend on the recognition stage. The existing model has a common issue called objec- tivity suppositions problem, which might decrease the recognition rate. Therefore, this study investigates the improved version of a classifier that is based on hidden conditional random fields (HCRFs) model to classify emotional speech. In this model, we introduced a novel methodology that will incorporate mul- tifaceted dissemination with the help of employing a combination of complete covariance Gaussian con- creteness function. Due to this incorporation, the proposed model tackle most of the limitations of existing classifiers. Some of the well-known features like Mel-frequency cepstral coefficients (MFCC) are extracted in our experiments. The proposed model has been validated and evaluated on two publicly available datasets likes Berlin Database of Emotional Speech (Emo-DB) and the eNTER FACE’05 Audio-Visual Emotion dataset. For validation and comparison against the existing techniques, we utilized 10-fold cross validation scheme. The proposed method achieved significant improvement under the p-value <0.03 for classification. Moreover, we also prove that computational wise, our computation technique is less expensive against state of the art works.
© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intel-
ligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

Commonly, emotion is a psychological status that impulsively performed and raised. Emotion is not only a good pointer to deter- mine the mental status of a human but also an effective source of conveying our intentions in daily conversations. This is because the automatic recognition of human sentiments is a fascinating param- eter in order to improve the superiority of the facilities delivered by the computer like human–computer interaction [1,2], daily life observing in omnipresent health care systems [3]. There are certain physiological variations like talking, blood density, heart signal, facial expressions, etc. that express the human sentiments (emo- tions). Among these variations, most of the researchers point out that audio speech is the main source of emotions [4–10]. Because audio signals are the most widely and naturalistic method for human to human communication.

E-mail address: mhsiddiqi@ju.edu.sa
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
Generally, there are two steps in a typical audio-based recogni- tion system: First step extracts the most prominent features from the input data; while, the second step decides the appropriate label for incoming input data. In audio-based emotion recognition sys- tem, many methods have been proposed for the feature extraction stage to extract the most significant features. There are four cate- gories for such features (named continuous speech features) like pitch of sound, formant, vitality [11,1,10], voice quality features [1,12] (e.g harsh, tense, breathy), spectral features [11,13] (e.g undeviating extrapolation measurements, Mel-frequency cep- strum coefficients), and Teager energy operator (TEO) [14]. In liter- ature of speech classification [7], certain systems [15] suggested that some suitable feature selection are highly dependent on the recognition task; therefore, it should be considered. Moreover, they pointed out that for speech demonstration, the MFCC features are the most significant features. As a new feature extraction method is not in the scope of this study; therefore, we are utilizing an existing technique to excerpt the MFCC features that further will be employed in the proposed model.
Although there is relatively an enormous number of latest research  concentrating  on  refining  the  classification  phase


https://doi.org/10.1016/j.eij.2020.03.001
1110-8665/© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



[11,8,12,9,2]. Mostly, in the existing audio-based emotion recogni- tion systems, the authors utilized conformist learning model [1] like Gaussian mixture model, hidden Markov model, artificial neu- ral networks, support vector machines, etc. Certain studies [16– 18,13,19–22] highlighted that HMM is one the most significant model used in audio-based emotion recognition system. Further- more, other areas like speech classification [23], posture classifica- tion [24,25] pointed out that HMM is a multiplicative learning method, due to which it is least precise than its discriminative cor- responding part such as HCRFs. The following contributions have been made in this study.

The previous HCRF technique is insufficient through unconven- tionality norms that might decrease the recognition rate. There- fore, in this work, a recognition model has been presented that diminish the supposition by utilizing the full covariance distri- bution, which is the first objective of this study.
The second objective of this study is to show that the proposed
model significantly reduced the complexity against the previ- ous techniques. In this method, the Limited-memory Broyden- Fletcher-Goldfarb-Shanno (L-BFGS) technique has been utilized in order to find the optimal point, which is the alternative goal of this model to determine certain factors at the training stage to extend the conditional probability of the training data. So, in order to calculate the conditional probability, we only employed the forward and backward methods that further used for computing the gradients. Due to which the computational time has thoroughly condensed.
In order to show the significance of the proposed model, a set of
experiments is performed that showed the best performance compared to the latest works.

Related work

Recently, there has been growing research work in the field of emotion recognition based on speech data to increase the accuracy of such systems [1]. However, very few of these attempts truly add to the efficiency of learning models for speech data. Authors of [1] have rightly pointed out that, although numerous classification methods [26–29] have been tested and used to improve the effi- ciency of learning model, HMM still remains as the most common and efficient method. The accuracies of HMM on various datasets as compared to GMM, ANN, and SVM etc. is still comparable. More- over, HMM presents the advantage and ability to process sequen-
tial data e.g. processing of frame-level features whereas GMM,
state normalization, and additionally, HCRF maintains hidden states to be capable to absorb unknown construction of successive records. As result, CRF and HCRF can work with weighted scores making the set of parameters used comparatively larger as com- pared to MEMM and HMM. We refer the reader for a comprehen- sive and detailed analysis of HCRF and its limitations to [34].
There have been certain approaches that utilized HCRF model and showed good results. These are explained and presented in [35,36]. However, these systems do not address the limitations of HCRF. In [34], the authors argue that only HCRF might be utilized diagonal (slanting) covariance Gaussian dissemination. Particu- larly, the variables are considered pairwise autonomous. From now, this model is referred diagonal covariance Gaussian Mixture hidden conditional random field. Additionally, the authors stated, through a particular set of fixed prices, the solidity of observations at every state converges to the Gaussian procedure. However, this assumptions is not supported by a training algorithm, and hence these assumptions may be counter productive i.e. decreasing the efficiency of the model. For more in-depth study, we refer the reader to [37,38].
To address the limitations of HCRF and other learning models for emotion recognition on speech data, in the following section, we present our novel approach based on HCRF method, which has the ability to overtly exploit combination of full covariance Gaussian dissemination. Our method gets the benefits from exist- ing HCRF. We apply and test our model on speech data to recognize emotions and compare its results with those obtained by HMM and HCRF with diagonal covariance Gaussian functions.


The proposed method

As presented in the previous section, the current version of HCRF model didn’t utilize full covariance matrix and cannot assure the convergence of parameters. This leads the existing HCRF model not being able to generate a set of values, where the provisional probability is modeled as a mixture of usual solidity functions. Therefore, in the feature functions, we include combinations of Gaussian dissemination in order to solve the above problem. Our function (feature) are given by the following equations:

Prior

f  Z; K; Y = d (k1 = k).z; y 6k;	(1)


T

ANN, and SVM lack this and cannot process sequence of feature vectors.
HMM, however, presents some limitations as pointed out in
Transition	
k k'	—
t=1
= k) d kt = k' .z; y 6k; k';	(2)

[23,25,24]. The main limitations of HMM are due to its propagative nature and the objectivity hypothesis among its states and the
f Observe  Z; K; Y  = X

log
 XL
CObsr N y2; uk l ; Rk l !

.d(kt = k).z; (3)

interpretations. To address the limitations of HMM, a technique called maximum entropy Markov model was proposed. This model shows better results for certain operations/tasks including part-of- speech tagging (POS) [30], evidence abstraction [31], and the recognition of automatic speeches [32]. However, maximum
k


then,
t=1
l=1
k;l	t	;	;





0 — 1 y2 — uk l '. 1

entropy Markov model itself suffers from the weakness/problem
N y2; uk;l; Rk;l =
exp B X	 
C;	(4)

of label biasness [33]. Label biasness in MEMM is mainly caused	t




(2p)  R  1
@	y2 — uk l	A




notch conservation at every notch.
To address the label biasness in MEMM, conditional random field [33] and hidden conditional random field (HCRF) [23,25] were proposed. CRF and HCRF are generalizations of MEMM are general- ized forms of MEMM and hence inherit its properties. Both, CRF and HCRF use global normalization in contrast to MEMM’s per-
Here, L represents the amount of solidity functions, Dim shows the dimension of observations, CObsr indicates the fraternization
mass of the nth factor having average uk;l, and surrounding sub- stance (covariance matrices)	k;l . We might modify C; u and
in order to build any fusion of standard solidities through investi-

gating against Eq. (3). Hence, the conforming reflection weighti- out the training phase, and this allows us to fix
KObsr = 6k	(5)
Consequently, we can write the conditional probability as follows:
fScore Z|Y; X; u; C; K = XaT (k) = Xb1(k).	(11)
	
So far, in the initial (training) phase, we were focused on finding the parameters ( ; u; C; K) of the training data, which have the capability to make best use of conditional probability.
We now are more interested in utilizing one of the existing well
known techniques in the proposed model to look the optimum fact. Nevertheless, to compute this, we used only the forward

0RKPriorf Prior Z; K; Y +	1

Xexp B
Transition Transition	 C
probability (as utilized by other existing works [23]). Furthermore,
we re-used its resultant value for computing the inclines. This

kk'

K	Observe
Posterior Z|Y; X; u; C; K =	k k	P

expectedly reduces computational cost significantly. In the follow-
;	ing, we show the gradient computation method briefly:

Norm(Y ; ; u; C; K)
Let us denote
(6)	c Z; K¯ ; Y = XKPriorf Prior Z; K¯ ; Y 


0  Prior

XT 
k	k
Transition	1	X

X  exp B
Kk1
+

 XL

t=1
Kkt—1 ;kt	+
! C
+
kk'
Transition Transition kk'	kk'

K¯ =k1 ;k2 ;..;kT
@ log
CObsr N y2; u  ; R	A
X Observe Observe  ¯

Posterior Z|Y; X; u; C; K =

l=1


kt ;l
t  kt ;l
kt ;l
+	Kk	f k
Z; K; Y
(12)


Posterior Z Y X u


C; K

= fScore(Z|Y; P; u; C; K)


(8)
dScore(Z|Y; P; u; C; K) = X dc Z; K; Y exp c Z; K; Y 

| ;	; ;
Norm(Y ; P; u; C; K)
dKPrior


dKPrior

where Norm(Y; P; u; C; K) is the normalization factor.	= Xf Prior Z; K; Y exp c Z; K; Y 

Utilizing equations(7) and (8), re-evaluate the conditional probability using forward and backward algorithms, as shown below step by step:


K
= b1(k)	(13)

0  Prior	Xs   Transition	1
	 

dScore(Z|Y; P; u; C; K) = X dc Z; K; Y exp c Z; K; Y 




as =



exp
@
 XL
	 ! CA
X Transition	 




X	 ' 







Transition

l=1
kt ;l




 XL
t






Obsr
kt ;l
kt ;l



	 ! !
K T
=	a(t; k)b t + 1; k'	(14)
t=1
 	 

k k
k'	l=1
k;l
k;l
k;l
dScore(Z|Y; ; u; C; K)
=
dCObsr


dc Y; S; X
dCObsr


exp c Z; K; Y 

(9)
k;l


K	k;l



Prior k
B

T
Transition
kt—1 ;kt
t s
Observe k


=	Obsr
k;l






exp c Z; K; Y 




log
Ckt ;l N. yt ; ukt ;l; Rkt ;l


K t=1 XCObsr N yt ; uk l ; Rk l 


= X L
N yt ; uk;l ; Rk;l 

a(t; k)c(t + 1)

X	 XL	! !
t=1 XCObsr N yt ; uk l ; Rk l 

k' exp
Transition
log
Obsr N y u
k;l	;	;

=	bs+1
k'
Kkk'	+

l=1
Ck;l	.  s; k;l; Rk;l
;

(10)
l=1
(15)


Table 1
Average recognition rate accuracy for the proposed model along with diverse number of states and mixtures under 10-fold cross validation scheme against Emo-DB dataset (%).





Fig. 1. Recognition rates of three techniques with 2 states and 3 mixtures against Emo-DB dataset.




Table 2
Average recognition rate accuracy for the proposed model model along with diverse number of states and mixtures under 10-fold cross validation scheme against eNTER FACE’05 dataset (%).




Fig. 2. Recognition rates of three techniques with 2 states and 6 mixtures against eNTER FACE’05 dataset.


c(t) =	b(t; k)	(16)
l

dScore(Z|Y;	; u; C; K)
du
Obsr cN(y ;u ;R )
X 	k;l	cuk;l	 a(t; k)c(t + 1)

k;l
t=1 XCObsr N y ; u  ; R

Moreover, we get the gradients with respect to u, and	simi- larly in the following:

l=1
k;l
t  k;l
k;l

(17)


	






Fig. 3. Comparison gradient computation time of the proposed method against forward/backward on different mixture numbers, state numbers, and input sequence lengths (as shown in Eq. (17)).


Obsr cN(yt ;uk;l ;Rk;l )
initial values/points and collect the results of each run in a set.

dScore(Z|Y;	; u; C; K)
dR	=
Ck;l
L
cRk;l
a(t; k)c(t + 1)
Among those results, we then choose the set of parameters. This

k;l
t=1 XCObsr N y ; u  ; R
method provides the best results as we show in the following section.


l=1
k;l
k;l
k;l

(18)


Results evaluation and discussion

Having the gradients computed as shown in Eqs. (13)–(18), we now make use of the L-BFGS algorithm and find a local maximum for the conditional probability. Since we cannot define or find a glo- bal maximum, we execute the algorithm several times with different

This section presents the experimental results of the designed approach. Moreover, the experimental framework, the datasets, and the outcomes are also presented in this section.



To perform a fair assessment on publicly benchmark datasets, we use the Emo-DB dataset [39] and the eNTER FACE’05 dataset [40]. First, from each dataset, we extract the Mel-frequency cep- stral coefficients (MFCCs). Then, 10-fold cross validation scheme has been employed to separate each of the above two datasets into a training part and a validation part. We then execute classification algorithms on these datasets and the algorithms are: 1) our pro- posed FCGM-HCRF, 2) HMM model and 3) the HCRFs model which uses diagonal covariance Gaussian mixtures (DCGM-HCRF). Next, for a fair comparison of our algorithm with others, we compute p-values using the paired t-testing. The evaluation results of those comparisons are presented in the following.

Berlin database of emotional speech – Emo-DB

The Emo-DB dataset consists of expressive exclamations of 10 actors and actresses from German. Of which 50% are male and 50% are female. The utterances (or the spoken sentences) are a set of pre- defined sentences in one of the 7 defined emotional states: 1) neu- tral, 2) boredom, 3) disgust, 4) fear, 5) sadness, 6) Joy and 7) anger. Each successful attempt by the actors and actresses have been eval- uated by a group of 20 judges and the final concluding utterance is only selected if 80% of the listeners have correctly recognized.
In our experimentation for this dataset, we run HMM along with diverse numbers of states and Gaussian mixtures. In Table 1, for each pair of state and mixture numbers, we present the average classification rates of 10 folds.
From Table 1, it can be seen that the HMM with exactly two states and three Gaussian mixtures gives the highest accurate results. Therefore, we apply the initial values of this HMM to train and evaluate FCGM-HCRF and DCGM-HCRF algorithms. The results of these algorithm compared to HMM are shown in Fig. 1.

The eNTER FACE’05 Audio-Visual Emotion Dataset

We now present our evaluation on the eNTER FACE’05 dataset. This dataset consists of one thousand three hundred and twenty (1320) videos produced by 44 subjects/actors. Each of these actors in videos tries to simulate six different emotions: 1) anger, 2) dis- gust, 3) fear, 4) happiness, 5) sadness, and 6) surprise. These emo- tions are simulated while reading 5 different pre-defined sentences. First of all, we extract the audio from the original video files/data and then extract the MFCC coefficients. We then use these coefficients to generate the test and traning datasets simi- larly to what we did in the Emo-DB dataset. We also repeat the same process of computing the initial points and evaluating the proposed model as we did for Emo-DB dataset. Results of our benchmark algorithms compared to the proposed approach on eNTER FACE’05 dataset are presented in Table 2, and Fig. 2.

Computational complexity

In this section, we briefly discuss the computational complexity of the proposed approach as compared to others. The existing HCRF algorithm computes the gradients by a series of forward and back- ward algorithms. We however, execute the forward and backward algorithm once and cache the results in-memory for later use. For-
ward and backward algorithm has the complexity O TQ 2M  for
input sequences of length T, states of size Q and the M number of mixtures. This complexity can be seen/derived from Eqs. (9) and
(10). On the other hand, our proposed algorithm with caching has the complexity O(TM) for computing gradients. The proof for these complexity theoretic results can be seen in Eqs. (13)–(18).
In 3, we present a comparative analysis of the total execution time when inclines are calculated using the forward method and
Table 3
Comparison result of the proposed model along with the existing methods under the two standard datasets (Unit: %).


State of the art methods	Accuracy	Standard Deviation


[41]	72.0	±1.2
[42]	70.5	±3.8
[43]	63.6	±2.2
[44]	77.0	±2.7
[45]	67.7	±1.5
[46]	78.3	±2.5
[47]	79.2	±1.1
[48]	72.3	±2.5
[49]	76.4	±2.5
Proposed Method	79.3	±3.8


backward method as compared to the proposed approach with cach- ing. The reported execution times are measured using the Matlab R2013a running on an Intel machine with Duo 3.6 GHz processor and 4 GB of main memory. The proposed model has been compared with state of the art methods using. The corresponding recognition rates of the existing methods along with the proposed model on Ber- lin Database of Emotional Speech (Emo-DB) and eNTER FACE’05 Audio-Visual Emotion dataset are presented in Table 3. It is clear from the Table 3 that the proposed model showed better perfor- mance compared to other latest systems. This is because the pro- posed model utilized full-covariance distribution that considered most of the coefficients of the matrix, and that is one of the main rea- son to improve the performance. Moreover, this work showed that the existing HCRFs model has a common problem due to which it might decrease the recognition rate. This drawback is called objec- tivity suppositions problem. Therefore, full-covariance distribution based model is proposed to reduce the supposition that make the proposed model capable to consider the all coefficients of the matrix.

Conclusions

Audio-based emotion recognition has received lots of attention over the past decade. Several audio-based emotion recognition sys- tems have been proposed; however, still it is major issue for most of the systems to correctly classifying the emotions. There are some attributes which may degrade the accuracy, e.g extraction of the prominent features, and high similarity among different emotions that occurs in the presence of low between-class vari- ance in the feature space.
Accordingly, we have presented a new version of the HCRF algo- rithm that uses full covariance Gaussian density functions. Then, we proved it theoretically and experimentally that the recognition rates of the proposed approach is comparatively precise than exist- ing algorithms. We also proved that these improvements are statis- tically correct by using p-values for testing and comparisons. Moreover, our algorithm does not only add to the accuracy of recognition of emotions, it also has less theoretical complexity as compared to others in training the HCRFs model. As shown in pre- vious section, our proposed approach has a linear complexity while the exiting methods are of quadratic complexity. This extends the functionality of HCRF and enables it to be used in more practical and scalable applications. Although the scope of this paper is restricted to audio-based emotion recognition, however, it is com- pletely possible to extend it to other related ares of recognition including speech recognition, acoustic based context awareness, and gesture recognition among others.

References

Cowie R, Douglas-Cowie E, Tsapatsoulis N, Votsis G, Kollias S, Fellenz W, Taylor JG. Emotion recognition in human-computer interaction. IEEE Signal Process Mag 2001;18(1):32–80.



Schuller B, Rigoll G, Lang M. Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture. 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1. IEEE; 2004. pp. I–577.
Tacconi D, Mayora O, Lukowicz P, Arnrich B, Setz C, Troster G, Haring C. Activity and emotion recognition to support early diagnosis of psychiatric diseases. In: 2008 Second International Conference on Pervasive Computing Technologies for Healthcare. IEEE; 2008. p. 100–2.
Rahman MA, Hossain MF, Hossain M, Ahmmed R. Employing pca and t- statistical approach for feature extraction and classification of emotion from multichannel eeg signal. Egypt Inf J.
Alsayat A, Elmitwally N. A comprehensive study for arabic sentiment analysis (challenges and applications). Egypt Inf J.
Nalini N, Palanivel S. Music emotion recognition: the combined evidence of mfcc and residual phase. Egypt Inf J 2016;17(1):1–10.
El Ayadi M, Kamel MS, Karray F. Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recogn 2011;44 (3):572–87.
Bitouk D, Verma R, Nenkova A. Class-level spectral features for emotion recognition. Speech Commun 2010;52(7–8):613–25.
Iliev AI, Scordilis MS, Papa JP, Falcão AX. Spoken emotion recognition through optimum-path forest classification using glottal features. Comput Speech Language 2010;24(3):445–60.
Lee CM, Narayanan SS, et al. Toward detecting emotions in spoken dialogs. IEEE Trans Speech Audio Process 2005;13(2):293–303.
Banse R, Scherer KR. Acoustic profiles in vocal emotion expression. J Personality Soc Psychol 1996;70(3):614.
Gobl C, Chasaide AN. The role of voice quality in communicating emotion, mood and attitude. Speech Commun 2003;40(1–2):189–212.
Nwe TL, Foo SW, De Silva LC. Speech emotion recognition using hidden markov models. Speech Commun 2003;41(4):603–23.
Teager H. Some observations on oral air flow during phonation. IEEE Trans Acoust Speech Signal Process 1980;28(5):599–601.
Xue Y, Xue B, Zhang M. Self-adaptive particle swarm optimization for large- scale feature selection in classification. ACM Trans Knowledge Discovery Data (TKDD) 2019;13(5):50.
Cairns DA, Hansen JH. Nonlinear analysis and classification of speech under stressed conditions. J Acoust Soc Am 1994;96(6):3392–400.
Fu L, Mao X, Chen L. Speaker independent emotion recognition based on svm/ hmms fusion system. In: 2008 International Conference on Audio, Language and Image Processing. IEEE; 2008. p. 61–5.
Lee CM, Narayanan SS, Pieraccini R. Combining acoustic and language information for emotion recognition. In: Seventh International Conference on Spoken Language Processing.
Otsuka T, Ohya J. Recognizing multiple persons’ facial expressions using hmm based on automatic extraction of significant frames from image sequences. In: Proceedings of International Conference on Image Processing, vol. 2, IEEE; 1997. p. 546–49.
Schuller B, Rigoll G, Lang M. Hidden markov model-based speech emotion recognition. In: 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP’03), vol. 2, IEEE; 2003, p. II–1.
Ververidis D, Kotropoulos C. Emotional speech recognition: resources, features, and methods. Speech Commun 2006;48(9):1162–81.
Womack BD, Hansen JH. N-channel hidden markov models for combined stressed speech classification and recognition. IEEE Trans Speech Audio Process 1999;7(6):668–77.
Gunawardana A, Mahajan M, Acero A, Platt JC. Hidden conditional random fields for phone classification. In: Ninth European Conference on Speech Communication and Technology.
Wang SB, Quattoni A, Morency L-P, Demirdjian D, Darrell T. Hidden conditional random fields for gesture recognition. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2. IEEE; 2006. p. 1521–7.
Quattoni A, Wang S, Morency L-P, Collins M, Darrell T. Hidden conditional random fields. IEEE Trans Pattern Anal Mach Intell 2007;10:1848–52.
Farzaneh-Gord M, Mohseni-Gharyehsafa B, Arabkoohsar A, Ahmadi MH, Sheremet MA. Precise prediction of biogas thermodynamic properties by using ann algorithm. Renewable Energy 2020;147:179–91.
Ramezanizadeh M, Ahmadi MH, Nazari MA, Sadeghzadeh M, Chen L. A review on the utilized machine learning approaches for modeling the dynamic viscosity of nanofluids. Renew Sustain Energy Rev 2019;114:109345.
Kahani M, Ahmadi MH, Tatar A, Sadeghzadeh M. Development of multilayer perceptron artificial neural network (mlp-ann) and least square support vector machine (lssvm) models to predict nusselt number and pressure drop of tio2/ water nanofluid flows through non-straight pathways. Numer Heat Transfer, Part A: Appl 2018;74(4):1190–206.
Baghban A, Kahani M, Nazari MA, Ahmadi MH, Yan W-M. Sensitivity analysis and application of machine learning methods to predict the heat transfer performance of cnt/water nanofluid flows through coils. Int J Heat Mass Transf 2019;128:825–35.
Ratnaparkhi A. A maximum entropy model for part-of-speech tagging. In: Conference on Empirical Methods in Natural Language Processing.
McCallum A, Freitag D, Pereira FC. Maximum entropy markov models for information extraction and segmentation. ICML 2000;17:591–8.
Kuo H-KJ, Gao Y. Maximum entropy direct models for speech recognition. IEEE Trans Audio, Speech, Language Process 2006;14(3):873–81.
Lafferty J, McCallum A, Pereira FC. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
Siddiqi MH, Ali R, Khan AM, Park Y-T, Lee S. Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields. IEEE Trans Image Process 2015;24(4):1386–98.
Reiter S, Schuller B, Rigoll G. Hidden conditional random fields for meeting segmentation. In: 2007 IEEE International Conference on Multimedia and Expo. IEEE; 2007. p. 639–42.
Mahajan M, Gunawardana A, Acero A. Training algorithms for hidden conditional random fields. In: 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, IEEE; 2006. p. I–I.
Siddiqi MH, Alruwaili M, Ali A, Alanazi S, Zeshan F. Human activity recognition using gaussian mixture hidden conditional random fields. Computat Intell Neurosci 2019.
Lee S, Lee Y-K, et al. Emotional speech classification using hidden conditional random fields. In: Proceedings of the Second Symposium on Information and Communication Technology. ACM; 2011. p. 146–50.
Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of german emotional speech. In: Ninth European Conference on Speech Communication and Technology.
Martin O, Adell J, Huerta A, Kotsia I, Savran A, Sebbe R. Multimodal caricatural mirror. In: eINTERFACE’05-Summer Workshop on Multimodal Interfaces; 2005.
Lotz AF, Faller F, Siegert I, Wendemuth A. Emotion recognition from disturbed speech-towards affective computing in real-world in-car environments. Studientexte	zur	Sprachkommunikation:	Elektronische Sprachsignalverarbeitung 2018;2018:208–15.
Zamil AAA, Hasan S, Baki SMJ, Adam JM, Zaman I. Emotion detection from speech signals using voting mechanism on classified frames. In: 2019 International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST). IEEE; 2019. p. 281–5.
Kerkeni L, Serrestou Y, Mbarki M, Raoof K, Mahjoub MA. Speech emotion recognition: methods and cases study. In: ICAART (2); 2018. p. 175–182.
Tursunov A, Kwon S, Pang H-S. Discriminating emotions in the valence dimension from speech using timbre features. Appl Sci 2019;9(12):2470.
Choudhury AR, Ghosh A, Pandey R, Barman S. Emotion recognition from speech signals using excitation source and spectral features, in IEEE Applied Signal Processing Conference (ASPCON). IEEE 2018;2018:257–61.
Bhavan A, Chauhan P, Shah RR, et al. Bagged support vector machines for emotion recognition from speech. Knowl-Based Syst 2019;184:104886.
Avots E, Sapin´ ski T, Bachmann M, Kamin´ ska D. Audiovisual emotion recognition in wild. Mach Vis Appl 2019;30(5):975–85.
Hajarolasvadi N, Demirel H. 3d cnn-based speech emotion recognition using k- means clustering and spectrograms. Entropy 2019;21(5):479.
Ma Y, Hao Y, Chen M, Chen J, Lu P, Košir A. Audio-visual emotion fusion (avef): a deep efficient weighted approach. Inf Fusion 2019;46:184–92.
