Electronic Notes in Theoretical Computer Science 141 (2005) 5–31	
www.elsevier.com/locate/entcs

Interactive Computation:
Stepping Stone in the Pathway From Classical to Developmental Computation 1
Antˆonio Carlos da Rocha Costaa,b,2 Grac¸aliz Pereira Dimuroa,3
a Escola de Inform´atica, Universidade Cato´lica de Pelotas, Pelotas, Brazil
b PPGC, Universidade Federal do Rio Grande do Sul, Porto Alegre, Brazil

Abstract
This paper reviews and extends previous work on the domain-theoretic notion of Machine Devel- opment. It summarizes the concept of Developmental Computation and shows how Interactive Computation can be understood as a stepping stone in the pathway from Classical to Developmen- tal Computation. A critical appraisal is given of Classical Computation, showing in which ways its shortcomings tend to restrict the possible evolution of real computers, and how Interactive and Developmental Computation overcome such shortcomings. The idea that Developmental Compu- tation is more encompassing than Interactive Computation is stressed. A formal framework for Developmental Computation is sketched, and the current frontier of the work on Developmental Computation is briefly exposed.
Keywords: Interactive computation, developmental computation, domain theory, classical theory of computation


Introduction
In [5], the first author introduced a domain-theoretic approach to the con- ceptual analysis of Interactive and Developmental Computation. That thesis consisted of an epistemological analysis of the principles of Artificial Intelli- gence and the Theory of Computation, aiming:

1 Work partially supported by CNPq and FAPERGS.
2 Email: rocha@atlas.ucpel.tche.br
3 Email: liz@atlas.ucpel.tche.br




1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2005.05.014

to establish a sound, constructivist foundation for the notion of machine intelligence as a regulation structure for the functional interactions be- tween computing machines and their environments;
to outline a naturalistic approach to Artificial Intelligence, so that the central purpose of AI becomes the study of machine intelligence as a physical symbol systems-based structure that naturally occurs in com- puting machines, not the attempt to simulate the human mind 4 ;
to show that such structure can only be apprehended according to a constructivist approach, where the intelligence of machines arises as a limit structure in a developmental process occurring in a suitable domain;
to make clear that such developmental processes can only happen in the context of interactive computing, but also that interaction, although necessary, is not sufficient for such purpose, internal developmental oper- ations being the indispensable complementary components of such pro- cesses.
In connection to the third and fourth goals mentioned above, the work in [5] aimed:
to make clear, by means of a historical review of Computer Science, that the notions of Interaction and Development were present in the area since the very beginning (and even before, in areas such as Cybernetics, and beyond); but that, for various reasons (mainly the too restrictive notion of computational effectiveness that was adopted in Classical Computation), they were always kept latent, and never fully explored;
to show that interactive and developmental machines can go beyond the models of Classical Computation (CC), in the sense of introducing a shift in the scope of the notion of computation, bringing it from the strictly al- gorithmic computational processes to the non-algorithmic computational processes;
to introduce, in a tentative way, some elementary developmental mecha- nisms capable of supporting processes of machine development.
Considering that [5] was elaborated in the late 1980’s and early 1990’s, before the seminal papers by Peter Wegner [29,30,31] on Interactive Com- putation (IC), and also before his immediately subsequent papers with Dina Golding [33,34,12,11] – so, being unable to benefit from such papers –, it is remarkable how close the results in [5] concerning the third and fourth goals match the general results of the work by Wegner & Golding.

4 We take the expression physical symbol system right in the sense introduced by A. Newell [17].

In particular, it is notable that both works coincided in the identifica- tion of the three main shortcomings of Classical Computation, namely, the requirements that:
the machine operates as a closed system, during the computation, thus forbidding interaction;
only finitely many resources be used during the computation, thus dis- missing infinite computations;
the structure of the machine remains fixed during the computation, thus forbidding machine evolution and development.
On the other hand, [5] being based on different epistemological principles, namely those of Jean Piaget’s Genetic Epistemology [18,19] (for the epistemo- logical foundation of P. Wegner’s work, see [32]), it is not surprising that some differences in purposes and results have appeared between that thesis and the work by Wegner & Goldin.
Also, having been put to sleep for ten years, it is not surprising that the work started in [5] did not achieve the degree of formalization achieved by Goldin and co-authors in their later works [11].
The present paper concerns the computation-theoretic aspects of [5]. It’s goals are, in the first place, to define Developmental Computation and to relate it to Interactive Computation. It also aims to highlight the domain- theoretic basis that [5] adopted. Finally, it aims to show that Developmental Computation is more encompassing than Interactive Computation.
The paper is organized as follows. Section 2 summarizes Domain Theory. Section 3 shows that Interaction was already embedded in the well known von Neumann’s computer architecture. Section 4 introduces Developmental Computation.
Section 5 sketches a domain-theoretic framework for Developmental Com- putation. Section 6 presents a sample model of machine development. Sec- tion 7 concerns the Conclusion, related work, and a brief overview of the current frontier of Developmental Computation.

A Domain-based Appraisal of CC
Classical Computation (CC) was settled by the foundational works of Turing, Church, Kleene, Post, Curry and others, and was consolidated in widely used text-books, such as the ones by Kleene [15] and Rogers [22].
Domains [1,14] were introduced by Dana Scott as mathematical structures that allow for a model of the λ-calculus, and support the denotational seman- tics of programming languages. Scott himself gave various presentations of

the structure of domains, e.g., [23,25,26], and specifically [24], in which the general ideas of Domain Theory are presented in an informal way that is well compatible with the way they are used here.
Domain Theory officially introduced in Computer Science the idea of par- tial object, that is, the result of a partial (unﬁnished ) computation. Using partial objects, Domain Theory was able to give inﬁnite computations the status of first order citizens. Each infinite computation can be assigned a non-trivial meaning, thus allowing infinite computations to be distinguished from each other, so that they may not be simply dismissed as divergent.

Domains
A domain is an ordered structure whose elements are called objects. Objects of a domain are considered to be results of a computation. Computations are seen as processes that construct objects of a domain.
Objects of domains are ordered according to the way each object partici- pates in the construction of other objects. That is, if x and y are objects of a domain D and x is a part of y, one denotes this by x ± y. The relation
± is called approximation relation, and x is said to be an approximation of
y. Objects are said to be partial objects, since – in general – it is possible to aggregate new (partial) objects to a given object, to make it become a more complete object. Objects from which it is not possible to construct other ob- jects, because nothing can be added to them, are said to be total (complete) objects. Total objects are the maximal elements of the ordering ±.
The concept of domain involves the concept of chain: given a domain D ordered by the relation ±, a chain in D is any finite or infinite sequence X = {xi}i∈N of elements of D, such that xi ± xj whenever i ≤ j. Chains are the simplest ways of modelling constructions in domains.

Examples of domains
For the examples below, we have chosen to use the simplest form of domains, namely, the CPOs. A CPO is a partially ordered set that has a least element and where every chain has a least upper bound.
Example 2.1 Let Σ = {a, b} be an alphabet, and ω the cardinality of N. The domain of strings over Σ, ordered by the prefix order, is the structure DΣ = (Σ∞; ⊥, ±) where:
Σ∞ = Σ∗ ∪ Σω
where Σ∗ = {w : {0, 1,..., n} → Σ | n ∈ N} ∪ {⊥} is the set of finite strings over Σ, and Σω = {w | w : N → Σ} is the set of infinite strings

over Σ
for all s, t ∈ Σ∞, s ± t iff:
s, t ∈ Σ∗ and length(s) ≤ length(t), and for all 0 ≤ i ≤ lenght(s) it happens that s(i)= t(i); or
s ∈ Σ∗, t ∈ Σω and s(i)= t(i) for all 0 ≤ i ≤ lenght(s); or else
s, t ∈ Σω and s = t
⊥ is the empty string, and ⊥± s for all s ∈ Σ∞
D∞ is a CPO because every finite or infinite chain X ⊆ D∞ has a least upper bound HX ∈ D∞: either X is finite and HX is its last element xn ∈ Σ∗, or X is infinite and HX ∈ Σω (see, e.g., [1]).

Example 2.2 Let N = {0, 1,..., n, .. .} be the set of natural numbers. The domain of partial natural numbers is the domain P = (P; 0, ±) where
P = N ∪ {0, 1,..., n,.. .}∪ ω, where every n is called a partial natural number and every n is called a total natural number
0 ± p for all p ∈ P, is the least element of the domain
n ± n, for all n ∈ N
n /± m for all n, m ∈ N
n ± ω, for all n ∈ N
n /± ω, for all n ∈ N
In P, the most interesting chains are of one of the forms:
a finite chain ending in a partial natural number: 0, 1,..., n 
a finite chain ending in a total natural number: 0, 1,..., n,n 
the sole infinite chain of partial numbers, including no total number: 0, 1,..., n,... 
One should notice that w is the limit (lub) of the infinite chain of partial numbers.

Computations as constructions in domains.
A computation in a domain D may be seen as any finite (or, infinite) chain of elements x0, x1,..., xn (respectively, x0, x0,..., xk,.. .).
The state of a computation at a given moment t is given by the (partial) objects that have been constructed up to that moment. If, at time t, a com- putation has constructed a sequence of partial objects x0 ± x1 ± ... ± xt, then xt is the state of the computation at time t (assuming that x0 was the

first partial object constructed by the computation at time 0).
The objects constructed by a finished computation are said to be its prod- ucts, or results. If a computation has ended at time t, and up to that time it has constructed a sequence of partial objects x0 ± x1 ± ... ± xt, then xt is the ﬁnal result of the computation.
If an infinite computation constructs a chain x0 ± x1 ... ± xk ± ... of objects, the result of that computation is the limit of such chain, given by the least upper bound of that chain in the domain: H{x0, x1, x2,.. .}. Results of infinite computations are, thus, ideal objects, limits of the computations that construct them.
Often, the initial element in a computation is x0 =⊥, meaning the absence of any available result before the computation starts.
Example 2.3 Assume a program P that prints a string of characters of Σ =
{a, b} on a paper tape. The sequence ⊥, s1, s2,..., st of partial strings that it has printed up to time t may be read as the construction of the partial result st that it has produced up to time t.
Whenever P stops printing, the last string in the sequence of strings that it produced is the final result of the computation of P . If P never stops printing characters on the paper tape, the sequence of strings it produces is infinte, its computation is infinite, and the result of such infinite computation is an infinite string belonging to DΣ.
Notice that whenever we have times t, t such that t ≤ t we have st ± st' .

Example 2.4 Let P be a program that counts the sizes of lists. Given a list
L, we have the following situations for the possible results of P acting on L:
before P starts, it has counted no element in L, but P can tell us that there are at least 0 elements in L. So, P prints 0.
after counting for a while, but before reaching the end of L, P can tell us that there are at least k elements in L. So, P prints k.
in case L is finite, P will eventually reach the end of the list and will be able to tell us that there are exactly n = length(L) elements in the list. So, P prints n.
in case P never ends counting the elements in L, the best that we can get from P is an infinite sequence 0, 1,..., k,... where P informs us at each time the least number of elements it can assures us there is in L. We conclude that there are ω (infinitely many) elements in L
We notice that, strictly speaking, counting is a process that cannot be ade- quately modelled using just the set N of total natural numbers, because thus

one is not able to represent the intermediate situations where the counting is not finished yet, but some information is already available, namely, that at least a certain quantity of elements is present in the set being counted. Count- ing can only be faithfully modelled as a computational process in a domain containing “partial countings” (see [8,9] for further theoretical implications of computing with P).

Constraints on computations in domains.
Domains and operations on domains are required to satisfy a set of constraints that keep them within the acceptable limits of what are, intuitively, “com- putable operations”. The main such constraints are the following:
The computation of a result object from a given object should not reduce the structure of the produced object if additional parts are added to the initial object. That is: better inputs do not excuse worse outputs.
The computation of a finite result should not depend on operations acting on infinite objects. That is, finite outputs can only depend on finite parts of input objects.
Such requirements, that are only natural about a computing machine, are called monotonicity and continuity, respectively. Formally: computable func- tions f : D1 → D2 should be monotonic (x ± y ⇒ f (x) ± f (y)) and continu- ous (f (HX)= Hf (X), where X is any chain of objects in D).
Steps of computations of a program on a domain may be seen as functions f : D → D. Thus, steps of computations of any program are required to satisfy the above two constraints, and programs should be developed so as to generate only computation steps with such characteristics.
Example 2.5 One sees that any process whose effect is the printing of strings of characters satisfy the constraints. The same apply to processes that gen- erate strings of any other kind of objects, like partial results of counting pro- cesses.

Domains and the shift they induce in the notion of computation
The conceptual importance of domains is in that they equip the Theory of Computation with the notion of result of an infinite computation. As men- tioned before, this notion is by itself a departure from the framework of CC. To grasp the size of the conceptual shift induced on the Theory of Compu- tation by the availability of the notion of result of an infinite (non-terminating) computation, confront that notion with Rogers’ statement, in his discussion

of the mathematical notion of “effective procedure”, in his classical book [22] (section 1.1, p.5):
[. . . ] should we require that, given any input and given any set of instructions P, we have some idea, “ahead of time,” of how long the computation will take? We propose to make no such affirmative answer to the question [... ]. We thus require only that a computation terminate after some finite number of steps [... ].
Effectiveness of computations is taken, in CC, at the lowest possible level of conceptual richness: effectiveness is taken as deliverability of a finite result in finite time. It is interesting to contrast this requirement with Turing’s original ideas: [27] was concerned with the computation of real numbers, and a successful computation was one that lasted forever, computing correctly all the digits of the infinite representation of its result.
The restriction of the Theory of Computation to the study of the recursive functions on integers, and the consequent possibility of considering that only finite computations are useful, seems to us to be connected to the effort of turning that theory into a regular academic subject, effort in which Kleene, Rogers and others were strongly engaged in the 1950’s and 1960’s. The conse- quence of that restriction, however, was that the study of recursive functions on real numbers (Turing’s original aim) was put off the mainstream of the theory, along with the necessity of taking infinite computations into account. As will be shown below, Domain Theory makes clear that object construc- tion is a much richer notion than the strictly finite deliverability of results required by CC, because the effectiveness of a construction need not be con-
strained by finiteness, neither in the temporal, nor in the spatial sense. 5

Domains and the shortcomings of the Classical Theory of Computation
The sole contemplation of some computational notions inherent to the Theory of Domains is enough to expose serious shortcomings in the classical notion of computation. Shortcomings that the contemplation of the notion of inter- action makes even more overt. The first such shortcoming is:

CC-1: In CC, non-terminating computations are meaningless.

The lack of a notion of limit, based on the notion of continuity that the

5 Finiteness and finitarity are not synonyms. Finiteness concerns the total resources avail- able to a computation: a finite computation can only involve a finite amount of resources. Finitarity concerns the finiteness of the resources used in each step of a computation: the sum total of the resources involved in a finitary computation may well be infinite. Devel- opmental computations, as interactive computations, are conceived as finitary, not finite.

approximation order supports, prevents CC from being able to handle partial objects, even when the operational models that constitute its hallmarks (Tur- ing machines, λ-calculus reductions, etc.) show wide open to any attentive eye the internal handling of such partial objects.
To see that, it’s just a matter of looking at, for instance, the tape of a TM during a computation: the partial output objects that are being produced are there! What happens is that such objects cannot be output from the machine before the computation terminates, that is, the user of the machine is not oﬃcially allowed to make use of the partial results of the computation, before the computation terminates.
The second shortcoming of CC exposed by Domain Theory is:
CC-2: CC performs input-output mappings, not constructions.

The limitation to the computation of input-output mappings is the most se- rious shortcoming of CC exposed by the notion of interaction, as is well known from the research on reactive systems (e.g., [21]), and repeatedly stressed by Wegner & Goldin. This criticism is reinforced by Domain Theory. There is nothing in the notion of construction that requires that that sequence of ag- gregation steps be restricted so that just one single interaction step happens, with one initial complete object being given and one final object being (pos- sibly) received back, and with everything that happens in-between concealed inside the constructing machine, inaccessible from outside.
One may say that construction steps are input-output mappings [11], thus Turing-computable if the construction is effective in the classical sense, and that a construction is nothing more than a succession of such Turing machine computation steps. This would picture domain constructions as interactive processes in the sense of Wegner & Goldin.
But the problem is that nothing in Domain Theory requires constructions steps to be Turing-computable. That is, nothing in the structure of domains limits constructions to be classical computations: Domain Theory points to a conceptual framework where CC appears as the lower bound of a wide range of possible notions of computability, compatible with domains as universes of object constructions.
In other terms, Domain Theory is compatible with a notion of non-algorithmic computation. That is, the third shortcoming of CC is:

CC-3: In CC, effective is synonym of algorithmic.

The iterated repetition of Turing-computable macro-steps, adopted in the

Interactive Turing Machine model of [34], and in its special case called Per- sistent Turing Machine [11], construes computations as iterated sequences of Turing-machine computations. In a sense, that model places itself in the framework of a Turing-machine controlled iterated repetition of Turing ma- chine computations.
One could even conceive an interleaving of macro-steps performed by differ- ent machines, allowing for the controlled repetition of various Turing-machine programs, realized under the surveillance of a supervisor Turing machine, able to test a shared working tape to decide what particular Turing machine to ac- tivate in the next macro-step.
The notion of computation as construction in domains goes further than the interactive Turing machine models in the sense that it opens the possibility that non-Turing controlled sequences of non-Turing computable construction steps be considered effective in a concrete, physical symbol systems-based way. This possibility lies at the core of the notion of Developmental Computation.

A View of IC
In this section, we summarize the arguments presented in [5] on the centrality of the concept of Interaction to the notion of Developmental Computation.

Interaction and Church’s Thesis
Since it’s appearance, the so-called Church’s Thesis has often been subject to dispute, usually by people unsatisfied with its normative status, implying the possibility of finding interesting things that computers cannot do.
The thesis in [5] has not been written as another attempt to show that Church was wrong. The intention is quite another, namely, to make explicit the limits of the validity of Chuch’s Thesis, the limits within which Church is right.
Church’s Thesis was not built in the air 6 : there are strong conceptual and epistemological foundations sustaining it, the most important of which is the following:
Computations are computations of mathematical functions, that is, of oper- ations acting on completely deﬁned input objects.
The hallmark of Classical Computation, as captured by Church’s Thesis, is that a computation is the execution of a series of operations on an input object, taken from a well determined domain where all objects are completely defined,

6 See [13] for another account for the origins of Church’ Thesis.

producing as a result an object in a well determined output domain, where all objects are also completely defined. Nothing coming from the computation process can alter the nature and structure of the objects in both the input and output domains, nothing can alter those domains, and nothing can alter the very input object, while it is being processed.
The fourth shortcoming of CC is:
CC-4: A classical computing machine operates as a closed system, while it is computing, and thus is unable to alter its input objects, as such alterations require the active participation of the machine environment.

No classical Turing machine can model a computational process where the input object is altered while the computation is going on, because all classical Turing machines require that the input object be completely defined before the machine can start its operation. No algorithm, in the sense of Classical Computation, can be applied to input data, before such input data is completely defined.
In classical Turing machines, input-output operations are not interactive: they are respectively supposed to happen before the computation starts and after the computation ﬁnishes. Computations where input objects are subject to modifications dependent on the construction process of output objects are computations where interactive input-output operations happen.
CC models can only be extended with interactive input-output operations at the expense of dismissing the essential commitment that Church, Turing, Kleene, Post and others had to the solution of Hilbert’s Entsheidungsproblem (Decision Problem). Hilbert’s problem raised the question if every mathemat- ical problem could to be solved by a mechanical, non-creative procedure, per- formed by a mathematician thinking alone, in complete isolation from every- one else, doing calculations only with the help of paper and pencil, as vividly pictured by A. Turing [27]. And, Turing and all the others were strongly com- mitted to keeping their models of computation within the bounds suggested by the procedural model proposed by Hilbert.
Interactive Computation means more than just the possibility of having input-output objects being exchanged during the operation of the machine. It means also that the input objects need not be completely deﬁned before the computation starts, so the the output objects produced by the computa- tion need not be pre-determined by a strictly functional dependence (in the mathematical sense) of the input objects. Both input and output objects may become dynamically, and incrementally, deﬁned as the computation goes on. The essential feature allowing for interaction is the integration of the en-

vironment as a true participant of the computation process, playing an active role in the process. If the environment is an active participant in the com- putation process, the computation is no more mechanical, in Hilbert’s sense. That is, it is no more effective in the restricted sense that CC assigns to such term. But, sure, it may still be effective, and mechanical, in a wider, physical symbol system-based sense.
Interactive Computation and the von Neumann computer
The case, however, is not that some (possibly remote) idea of a (possibly hard to conceive) domain structure, modelling the computations of a (possibly futuristic) very special kind of computer, may someday reveal a (possibly weird) example of object construction that can not be performed by classical computations.
The case is that even everyday computers – the so-called von Neumann computers [3] – demonstrate, through interaction, that CC is a very restricted notion of computation. The input-output behavior of von Neumann comput- ers, allowing for interactive computations, shifts the domain of computation of such computers to areas that are very far from that contemplated by Church’s Thesis. The simple fact is that:
Turing machines are perfect operational models of von Neumann computers only when von Neumann computers are operating in a non-interactive way, that is, when they are computing mathematical functions. Only when oper- ating in such special, restricted modes of operation, von Neumann computers are subsumed by Church’s Thesis.
Besides introducing the environment as an active participant in the compu- tation, and giving it the power to influence the construction process of output objects by affecting the structure of the input objects, the architecture that von Neumann designed for the programmable computer [3] supports other important features not present in CC machine models.
von Neumann computers stretch Turing’s notion of stored program to an extent that could not be anticipated from it. Turing’s notion of stored program (in a universal Turing machine) lies on the possibility of interpreting objects stored in memory (tape) either as data or as program instructions, depending on the context in which the computer’s control unit accesses such objects.
By incorporating the notion of stored program (and the associated feature of the duality of data and program) in his model of computers, and by com- bining such feature with the possibility of dynamically entering input objects during a computation, von Neumann introduced a possibility that profoundly

departs from the very essence of the computational possibilities allowed by Turing machines, namely, the possibility of a program being dynamically mod- iﬁed by the environment, during its computation.
That is, by supporting the duality between data and program instruc- tion, concerning objects stored in memory (so that objects that are input while a computation is being carried on are allowed to be interpreted not only as data, but also as program instructions), and by combining this possibil- ity with the integration of the environment as an active participant in the computation process, von Neumann computers allow computations where not only input and output objects are not pre-determined, when the computation starts, but also where the very program that will control the computation is not pre-determined.
Any running program can aggregate new instructions, delete existing in- structions (substituting them by null instructions), or have any of its instruc- tions substituted by others, by transforming data interactively input to the machine into such instructions. In fact, it is precisely this feature of the in- teractive modification of the structure and behavior of the program running on the computer that allows for the notion of operating system, that only von Neumann computers can support and that makes them really general purpose computers. 7
So, in von Neumann computers, computers and environments (users) are allowed to cooperate, and the possibility of their cooperation amounts to the introduction of the notion of joint computation: to understand what a com- puter will do in a given situation, to be able to predict what will happen after it starts a computation, it is indispensable to look at both the computer and the environment, because what will happen in the computation (and, in gen- eral, during the whole life-time of the machine) depends on the articulation of the behavior of both such elements. In other words:
The possibility of the interactive modiﬁcation of running programs makes of von Neumann computers situated machines, that is, interactive machines whose behaviors can only be fully understood in connection with the behav- iors of the environments where they are situated.
Interaction and situatedness, embodied in von Neumann computers, show that even if machine structures are fixed, programs and computations need not be so: machines with fixed structured can compute with programs that

7 It seems that, in spite of all efforts to develop innovative non-von Neumann computers, practically no one of them turned out to be able to support full operating systems, thus turning out to become just special purpose computers.

can be modified interactively, that is, whose structure may change according to the interaction between computer and environment, which thus determine computational processes that can be modified, and thus controlled in their structures and goals, interactively.
We note, on the other hand, that Wegner & Goldin’s examples of programs for interactive machines have not yet contemplated that feature of interactive modification of running programs, although the feature can easily be intro- duced in their programs, since they have universal interactive machines [11]. Building on such double interactive computational power, already intro- duced in the field of Computation by von Neumann in the early 1950’s, one can easily grasp the possibility of developmental computational processes estab- lishing themselves in developmental computer models that can possibly emerge from physical symbol systems able to overcome structural limitations of the
von Neumann architecture, as indicated below.
This central result of [5] shows that computer technology has always been based on the notions of IC, and that Interaction is not a late novelty introduced by the development of computer technology (as suggested by Wegner [30]).
A current problem, then, is to characterize this wider notion of effective- ness, that surpasses the narrow sense of effectiveness of Classical Computation, and is able to encompass at least the undeniable effectiveness of von Neumann computers.
The work of Wegner & Goldin is, of course, a fundamental stepping stone in this direction.

Developmental Computation
The rationale behind Developmental Computation (DC) is that the principles that regulate complex organized dynamical systems are the same, indepen- dently of the nature of the elements that compose such systems, so that there should be no formal difference between the general principles that regulate the internal dynamics of biological organisms and those that regulate com- plex, interactive, developmental physical symbol system-based computational systems.
Thus, the essential step towards DC is not a technological step, because the historical analysis of Computer Science in [5] was able to show that the main technological ingredients for that step have all been there, for decades. The essential step towards DC is a conceptual, epistemological step. It consists in the superseding of the basic notions of the CC (like machines, programs and algorithmic computations), by the basic notions of a general theory of complex dynamical systems (like organization, development and adaptation).

To be able to perform such step, one such general theory of complex or- ganized dynamical systems should be adopted. In [5], the general theory of biological organisms that Jean Piaget exposed in [19] was adopted. That the- ory, more than a general biological theory, is in fact a general cybernetic theory of complex, interactive and developmental dynamical systems, and that is the real reason for its adoption.
This section introduces developmental machines and explains two of the general developmental notions that are critical to DC, namely, equilibration (development) and adaptation. It also gives an example of developmental computation, showing that DC is more encompassing than IC.

Developmental machines
By exploring the possibility of interactive input-output, a situated computer can exchange objects with the environment, objects that can be interpreted as both data and program, depending on the interpretation rules determined by the structure and functioning of the computer’s control unit.
But the consideration of this very possibility of having non-predetermined programs controlling non-predetermined constructions of output objects from non-predetermined input objects, according to a computation jointly governed by the computer and the environment, exposes an assumption implicit in Classical Computation, that still permeates the von Neumann computers: the computer operates under a fixed control structure, with a fixed set of primitive operations.
No matter how computer and environment interact, no matter how input and output objects evolve under this interaction, no matter how the environ- ment changes due to its open nature, the control structure of the computer is fixed, immune to the vagaries of the computation it controls.
Interactivity and situatedness in von Neumann computers are not enough to allow such machines to explore a further possibility, that could leverage the power of computation to a yet higher level, namely, the possibility of having the very structure of the computer change as the computation goes on, thus extending IC with developmental features that go beyond interaction.
The consideration of von Neumann computers as situated computers im- mediately exposes a fifth shortcoming of CC:

CC-5: Classical computing machines have ﬁxed control structures.

DC concerns systems where the control structure of computers can be modified dynamically. In particular, it concerns computational systems where

the modification of the control structure of machines can be understood as
development.
The question that immediately poses itself is, then, how the fixity of the computer structure can be overcome? One possible answer is to mimic the solution found by biological organisms: to arrange that elements of the system structure – material elements – be exchanged with the environment while the system is operating. Programming models inspired by molecular biology (e.g., [4]) will certainly serve as sources of answers for such question.
That is, the full exploitation of the possibilities of the notion of computa- tion requires that one envisages a computing machine that is able to exchange not only information objects with its environment, but also material objects, so that such material exchange can support processes of machine development. In a situation of joint computation involving computer and environment, and with the possibility of material exchanges between them, both the com- puter and the environment are not pre-determined in their structures, with the consequence that even the control rules of the computer’s control unit need not rest fixed during the computation. We call developmental computa- tion any computation where the structure of the computer is able to develop
as the computation goes on, and we state the following requirement for DC:
DC-1: Developmental computers may vary their control structures while computing, by ex- changing material objects with their environments.

With the help of domains, DC can be seen as involving a special kind of construction, namely, the construction of the computing machine itself. This allows the discrimination of two aspects of computations, when seen as constructions: on the one hand, a computation constructs the objects handled by the computing machine; on the other hand, a computation can construct the machine itself (if it is a developmental machine).

Machine development and machine autonomy
In the context of DC, the notion of purpose of computation has to be re- thought. For if the construction of objects by machines can be seen as an attempt to satisfy needs or requests from the environment (the users of the machine), what could be the purpose of the construction of the machine itself? The latter question seems to accept two kinds of answers. First, one can see that the construction of the machine may serve some purposes of the environment (users), since more developed machines may be expected to per- form better services. The second, somewhat unexpected answer, is that the

construction of the machine may serve some purpose of the machine itself.
The latter answer is surely an epistemological divisor, separating two dif- ferent notions of machines: autonomous machines, that is, machines endowed with goals that are of their own; and heteronomous machines, that is, ma- chines that have no goals of their own, its working being dedicated essentially for the fulfilment of goals of the environment (see [6] for an attempt to give a functional definition of autonomy in the context of multi-agent systems).
The question if a computing machine is possible, which is autonomous and yet is not a living being existing on its own is yet unsolved. We thus have to proceed noting that everything that follows is based on an unverified assumption, namely, the assumption that there may exist computing machines which are autonomous and yet are not living beings existing by their own. 8 For such computing machines, the process of machine construction should
be a development, guided by internal principles, devoted to modify the machine in order to make it function in a better way, for the sake of the machine functioning itself.

Why development is a material, not an informational concept
The question may be posed why development implies material exchanges with the environment. The answer is that, since it is not possible to have computa- tions performed by non material structures (even when performed in human minds, it seems that computations need the support of a physical system, namely, the brain) it is clear that the power of every computational process is limited by the computational power of the physical system performing it.
Thus, if development is to happen in a computing system, implying an ever growing capability of performing new computational processes, such develop- ment implies the correlative development of the underlying physical system.
Simulation is no solution to the problem: the limits of the simulated pro- cesses are determined by the limits of the simulator. There is no way, of course, for a simulator to produce a simulated system more powerful than the simulator itself.
Even metalevel processes in metalevel architectures are no solution either: hierarchies of metalevel languages are doomed to be supported by a definite computing system, whose computational power sets the limits to any possible development process that a meta-level program may intend to induce on a given object-level program.

8 We adopt such strong distinction between computing machines and living beings because otherwise, if one assumes no essential difference between those two kinds of entities, one would end by embedding Computer Science in Biology, which is not our intention.

Only if the computer itself is able to physically develop, to really create operational novelties in its control structure, is development a concept that can be really introduced in computational processes, in the form of creation of novel operational and storage structures that were not possible before.
As Biology teaches, development of a physical system requires that the system exchanges material elements with its environment. The exchange of information is insufficient for the purpose of developing the computational pro- cesses, because information can govern the realization of operations and occu- pation of storage with information, but cannot itself create neither the physical operators needed for the operations, nor the physical structures needed for the storage of data.

Why information does not exhaust the possible contents of interactions
Classical Computation sacred the conception that computation is a logical, informational process, that is independent of the physical nature of the system performing it. Such view, adequate as it is to face the Entscheidungsproblem posed by Hilbert, does not conform to the engineering view of computers as concrete machines whose performances dissipate energy and weaken the physical quality of the materials used to build them.
If information is conceived as a pattern on a physical substrate, the ex- change of patterns between two systems does not require the exchange of material elements between them. On the other hand, material exchanges can happen between two systems without necessarily information being exchanged between them: it is enough for that that such exchanges occur at a level where no disturbance in the informational interface between the systems can be no- ticed by any of them.
However, systems that exchange material elements between them, besides exchanging information, are systems that can potentially interact at much richer level them systems that only exchange information: they can directly influence each other development.
Of course, if a system is a developmental one, its development can be in- fluenced by information exchanged with its environment, because it is well possible that the system’s developmental mechanisms be regulated by infor- mation contained in the system. That is, the material development of a system may clearly be influenced by its informational exchanges. But it should be clear that information exchanges do not support development by their own.
Thus, a better picture of a developmental computing machine is one that, besides exchanging information (both programs and data) exchanges material elements with its environment.

Equilibration and Adaptation: the foundation of DC.
Classical Computation is based on three fundamental concepts, namely, ma- chines, programs and computations, which are adequate to support heteronom- ous object construction processes. DC is meant to surpass Classical Compu- tation in the sense that machines are promoted from being subordinate objects in the environment to being actors in it, from being instruments of the users to being their collaborators. DC, thus, concerns object construction processes that are oriented by purposes that belong to the machine itself. This implies that the fundamental concepts of DC should support such internal purposes, and allow for object construction processes that are able to articulate internal and external purposes.
Analyzing the general biological and psychological models presented by Pi- aget [18,19,20], including his models of development of biological and cognitive structures, we think that two fundamental concepts should be incorporated into computing machines to leverage their developmental processes, namely, a process of internally regulated object construction, called equilibration, and a dynamical concept of adaptation of the machine to the environment.

Equilibration.
A straightforward, informal definition is:
DC-2: Equilibration is the process of self-regulated construction of objects.

We note, first, that self-regulated constructions are not a new idea in the Theory of Computation. John von Neumann himself explored them, in order to define computing machines with reliability features that approximate that of the human brains [28]. Following Piaget, we construe equilibration as a process operating through a set of development stages of the computing ma- chine. At each development stage, the machine is able to construct particular kinds of internal and external objects, in certain ways, determined by the set of operations it has available for such purpose, at that stage.
Development stages are ordered according to the degree of their develop- ment, determined by some measure of the richness of the set of operations for object constructions available at that stage. When development is seen as a construction in a domain, the ordering of the stages of development is given by the approximation relation of the domain.
The equilibration process has two dimensions, namely, a diachronic di- mension and a synchronic dimension [20]. The diachronic dimension is the one that regulates the development process as such. That is, it regulates the

way the machine changes from one development stage to the next develop- ment stage. Major equilibration is the name applied to denote the diachronic process of equilibration. The synchronic dimension is the one responsible for regulating the construction of the internal and external objects, at each stage. Minor equilibration is the name applied to such process.
We further notice that the notion of object constructed in a development includes the operators that realize the internal dynamics of the machine, and thus includes the object constructors themselves. That is, Piaget’s no- tion of equilibration encompasses the notion of autopoiesis, by Maturana & Varela [16].

Adaptation.
Adaptation is correlative to equilibration, in the sense that the equilibration process produces better adaptation resources to the computing machine, while dysadaptation acts as an indicator of the need of new steps in the equilibration process. As the machine develops through its set of development stages, under the supervision of the adaptation process, it gets more and more adapted to the environment, as richer construction processes of internal and external objects become possible at each new stage, due to the richer set of object constructors that become available at each new stage.
Adaptation is defined in terms of two ancillary notions:
Assimilation: the process by which the machine is able to apply to internal and external objects the set of its currently available operations, in order to achieve its current goals.
Accommodation: the process by which the machine is able to adjust its current set of operations, in order to make them better applicable to internal and external objects, in order to achieve its current goals.
Adaptation is thus defined as:
DC-3: Adaptation is the situation where every required assimilation is possible, because every required operation on a given environment can be performed, and every required accommodation is possible, because every required adjustment in the internal and external operations can also be performed.

Major equilibration furthers the stages of adaptation, because more inter- nal and external objects can be handled with more sophisticated operations. Thus, major equilibration is the central factor of development [20]. On the other hand, the progress of adaptation requires ever more sophisticated stages of development, that can only be achieved through major equilibration.

The usual alternative notion of adaptation, conceived as a process and not as a situation as defined here, is clearly due to the usual way of taking the term “adaptation” to mean what we have called here “accommodation”.

Sketch of a Formal Framework for the Theory of DC
The distinction between development and evolution is based on the idea that development concerns individuals while evolution concerns sets of individuals, and can be formalized by requiring that development happens in a domain, so that the sequence of stages of a development (construction) guarantees the increasing richness of the operational structures of those stages, while evolution may be defined without that requirement.
The main purpose of the following preliminary formal framework is just to indicate the basis on which we think it is possible to formally prove that DC is a more encompassing notion than IC.
Let M be a developmental computing machine, T be a discrete-time tem- poral structure. Then, define:
Development stages:
DM is the set of possible stages of development of M

Dt
the set of possible stages of development of M at time t ∈ T
 

Dτ  =
t∈τ
t  the set of possible stages of development of M for τ ⊆ T

D0 ∈ DM the set of possible initial stages of development of M
Machine operations:
op(d) the operational structure of stage d, that is, set of (internal and external) machine operations available at development stage d
Approximation relation:
±⊆ DM × DM the approximation relation between development stages of
M , so that d ± d' iff d ∈ Dt , d' ∈ Dt' , with t ≤ t', and op(d) ⊆ op(d')
(development increases the richness of the operational structure).
Development steps:

Δt+1
⊆ Dt
→ Dt+1
the set of possible development steps at time t,

defined so that every δt+1 ∈ Δt+1 guarantees the inclusion relation between
t	t
the operational structures of development steps, that is, op(d) ⊆ op(δt+1(d)),
for every d ∈ Dt
Development relation:

Δt+n = Δt+n
Δt+n−1◦.. .◦Δt+2◦Δt+1 the development relation (possibly,

t	t+n−1
t+n−2
t+1	t

a function) operating from t to t + n, so that for all d, d' ∈ DM it happens
that d ± d' iff d ∈ Dt  and d' ∈ Dt' , for t, t' ∈ T with t < t', and (d, d') ∈ δt'
M	M	t
for δt' ∈ Δt' ; so that if (d, d') ∈ δt' then op(d) ⊆ op(d').
t	t	t

Let E be the evolutive environment of a developmental computing machine
M . Define:
Evolution stages:
EM the set of possible evolution stages of the environment E of M

Et
the set of possible stages of evolution of the environment at t ∈ T
 

Eτ  =
t∈τ
t  the set of possible stages of evolution of the environment

during the period τ ⊆ T
E0 ∈ EM the set of possible initial stages of evolution of the environment
Environments operations:
op(e) the set of operations that the environment is able to apply on the machine at evolution stage e ∈ Et
Approximation relation:
±⊆ EM × EM the approximation relation between stages of evolution of
the environment, so that e ± e' iff e ∈ Et , e' ∈ Et' , with t ≤ t' (with no
requirement of enrichment of the environment’s operational structure)
Evolution steps:
Υt+1 ⊆ Et → Et+1 the set of possible environment evolution steps at t
t	M	M
Evolution relation:

Υt+n = Υt+n
Υt+n−1◦.. .◦Υt+2◦Υt+1 the environment evolution relation

t	t+n−1
t+n−2
t+1	t

operating from t to t + n.
Developmental machines and their evolutive environments must interact, if the machine is to operate in the environment. The idea of interaction that underlies the following formalization is that any interaction step is a coordi- nation of two operations, one performed by the machine, the other performed by the environment. For such coordination to occur, the two operations are required to be coherent (or, compatible) in some sense.
Let M be a developmental computing machine, E its evolutive environ-

ment, d ∈ Dt
, and e ∈ Et
. Then define:

Interaction coherence:
≈⊆ op(d) × op(e) the coherence relation between operations of the devel-

opment stage d ∈ Dt
and operations of the evolution stage e ∈ Et
, so that

the meaning of od ≈ oe is that od ∈ d and oe ∈ e are coherent (or, compatible) with each other
Adaptation of development stages:

da⊆ Dt
× Et
the adaptation relation between the set of development

stages Dt
and the set of evolution stages Et
, so that d da e iff ∀od ∈ d.∃oe ∈

e.od ≈ oe and ∀oe ∈ e.∃od ∈ d.od ≈ oe
Adaptation of machines:
daτ ⊆ Mach × Env the adaptation relation between developmental comput-

ing machines and environments, during the period τ ⊆ T , so that M daτ E iff

∀t ∈ τ.∀d ∈ Dt
.∀e ∈ Et
.d da e.

A tentative model of machine development
For the sake of illustration of the idea of machine development, we introduce here a tentative model of steps of machine development. The model is tentative because it is incomplete in many senses, as will be explained. However, it seems to be clear enough to allow the grasping of what we think is essential about the idea machine development.

A tentative model of development step
Our tentative model of development step is an attempt of formalization of the basic model of system development informally introduced by Piaget in [20].
Definition 6.1 Let DM = (S; Op) be a stage of development of machine M, where S is the set of states of M at that stage of development, Op = ∪nOpn ⊆ Sn → S (n ∈ N) is the set of operations that M can perform at that stage. A
basic development step is a transformation δρ(DM )= D'  = (S'; Op') with:
S' ⊆ ℘(S) such that ρ(S) = ρ(S') = S', that is, S' contains exactly all subsets of S that satisfy ρ
ρ : ℘(S) → ℘(S) is a restriction on ℘(S), accounting for the choice of the states for the new development stage, among all possible new states
Op' ⊆ S'n → S' with o'(S'n)= {o(x) | x ∈ Sn} for all o' ∈ Op'
The central issue in this definition of development step is the employment of the subset operator ℘ to achieve two developmental results that are essential to Piaget’s model of development:
the augmentation of the set of possible states, in the new stage
the preservation of the previous set of possible states in the new set, by their inclusion in the new development stage as singleton sets
the use of a set of criteria ρ to exclude state possibilities that do not conform to the way the new development stage is construed
Combining those three features gives us a simple model of development step that may be tentatively held for the initial studies in this area. The model should not be taken as final, precisely because it lacks the most essential features that we have been discussing in this paper: it is not a model of an interactive development step, and so it does not encompasses exchanges between the machine and the environment (with the possible exception of the

situations in which the restrictions ρ come from the environment).

A simple example of machine development step
We now introduce a simple example of step of machine develop, and then confront it with what would be expected from a more complete example .
Example 6.2 Let’s consider a simple machine, with memory addressed by non-negative integers and able to perform arithmetic operations on integers:
Let M = (M ; +, −, ·, r, s) be such that
M = N → Z is the set of states of the machine memory
(ii) + : Z × Z → Z is the integer addition operation
− : Z × Z → Z is the integer subtraction operation
· : Z × Z → Z is the integer multiplication operation
r : M × N → Z is the read operation, that reads values from the memory
s : M × N × Z → M is the operation that stores values in the memory Typical programs in M are:
s(M0, 2, r(M0, 0) + r(M0, 1))
that stores at position 2 the sum of the values at positions 0 and 1
s(s(M0, 2, r(M0, 0) + r(M0, 1)), 4, r(M0, 2) · r(M0, 3))
that extends the previous program by multiplying the result stored at position 2 with the value at position 3, storing the result at position 4.
We now let M develop by performing a series of developmental operations, and becoming a new machine M' = (M'; ☒,  , ©, r', s'):
M' is able to store intervals of integers:
M' = N → I(Z)
M' is able to realize interval operations:
☒ : I(Z) × I(Z) → I(Z) is the integer interval addition operation
: I(Z) × I(Z) → I(Z) is the integer interval subtraction operation
© : I(Z) × I(Z) → I(Z) is the integer interval multiplication operation
r' : M' × N → I(Z) is the read operation, that reads integer interval values from the memory
s' : M' × N × I(Z) → M' is the operation that stores integer interval values in the memory
Typical programs in M' would be:
s'(M' , 2, r'(M' , 0) ☒ r'(M' , 1))
0	0	0
that stores at position 2 the sum of the intervals at positions 0 and 1

s'(s'(M' , 2, r'(M' , 0) ☒ r'(M' , 1)), 4, r'(M' , 2) © r'(M' , 3))
0	0	0	0	0
that extends the previous program by multiplying the interval stored at
position 2 with the interval at position 3, storing the result at position 4.
It is not difficult to see that the development step from M to M' conforms to the definition of development step given previously.

Conclusion: related work and the current frontier of DC
The first results on DC were established even before the Ph.D. work in [5] was officially begun. They supported the M.Sc. dissertation by Mart´ın Escard´o [9], where the computability of recursive functions on partial (lazy) natural num- bers were analyzed. Partiality (laziness) of natural numbers arises from the allowance of an interactive input of such numbers through successive approxi- mations [2], so that recursive functions on them realize a model of IC. A short report about that work appeared in [8].
The second author made use in her thesis [7] of the idea of construction in- dependent processes to define a structure where real numbers and intervals of real numbers are constructively obtained. The structure, called bi-structured coherence spaces is based on Girard’s coherence spaces [10]. It is said to be a bi-structure because, besides the ordered-structure of the approximation re- lation, that regulates the construction of real numbers and intervals, it also supports the algebraic structure of the operations on real numbers and inter- vals, established on the basis of the usual ordering of numbers. The operations of such algebraic structure are defined so that they are all construction inde- pendent. An effort to work out the notion of computational systems with autonomous goals is going on [6].
In this paper we have presented a summary of our ideas about DC and how it can open new ways to further the evolution of computing machines. The full theory of DC is yet to be developed. We hope that the formal framework introduced above can give some indication on at least the main features that we expect the theory to have. Of course, the main issue that still has to be clarified is the notion of material exchange between machines and environments, so better models of developmental can be obtained. DC introduces also many other issues, the most central of them being:
Axiology : the idea that computing machines have goals of their own implies the idea that computing machines have values of their own. The understanding of what such values may be, and how they should give rise to rules to which computing machines would adhere by their own, is a major

problem, that should be solved prior to the establishment of the Theory of Developmental Computation.
Teleonomy : the idea that the development of a computing machine should proceed according to principles that are internal to the computing machine is connected to the axiological problem just mentioned, but concerns specifically developmental goals and rules. The problem of teleonomy is, thus, the central problem of DC.
Acknowledgement
To the anonymous referees, for very useful comments.

References
S. Abramsky and A. Jung. Domain theory. In S. Abramsky, D. Gabbay, and T. Maibaum, editors, Handbook of Logic in Computer Science, volume 3, pages 1–168. Claredon Press, 1994.
R. Bird. Introduction to Functional Programming using Haskell. Prentice-Hall, 1998.
A. W. Burks, H. H. Goldstine, and J. v. Neumann. Preliminary discussion of the logical design of an electronic computing instrument. Part I, vol.1, 1946. In A. H. Taube, editor, John von Neumann - Collected Works, pages 34–79. MacMillan, New York, 1963.
L. Cardelli. Bioware languages. In A. Herbert and K. S. Jones, editors, Computer Systems: Theory, Technology, and Applications – A Tribute to Roger Needham, pages 59–65, Berlin, 2004. Springer.
A. C. R. Costa. Machine Intelligence: Sketch of a Constructivist Approach. PhD thesis, Programa de Posgradua¸c˜ao em Computa¸c˜ao, UFRGS, Porto Alegre, RS, Brazil, October 1993. In Portuguese.
A. C. R. Costa and G. P. Dimuro. Agent drives and the functional foundation of agent autonomy. ESIN-UCPel, 2004. To be submitted.
G. P. Dimuro. Bi-structured Coherence Spaces and the Construction of Real Numbers and Intervals of Real Number. PhD thesis, PPGC-UFRGS, Porto Alegre, Brazil, 1998. In Portuguese.
M. H. Escard´o. On lazy natural numbers with applications to computability theory and functional programming. ACM SIGACT News, February 1993.
M. H. Escard´o. Partial natural numbers. Master’s thesis, CPGCC/UFRGS, Porto Alegre, 1993. In Portuguese.
J. Y. Girard. Linear logic. Theoretical Computer Science, 59:1–102, 1987.
D. Goldin, S. Smolka, P. Attie, and E. Sonderegger. Turing machines, transition systems, and interaction. Information and Computation, 194(2):101–128, 2004.
D. Goldin, S. Smolka, and P. Wegner. Turing machines, transition systems, and interaction.
Electronic Notes in Theoretical Computer Science, 52(1), 2001.
D. Goldin and P. Wegner. The Church-Turing Thesis: Breaking the Myth. In CiE 2005 – Computability in Europe, Amsterdam, June 2005.
A. Jung. Domains and denotational semantics: History, accomplishments, and open problems.
Bulletin of ETAPS, 1996.

S. C. Kleene. Introduction to Metamathematics. D. van Nostrand, N.Y., 1952.
H. Maturana and F. Varela. Autopoiesis and Cognition - the realization of the living. D. Reidel, Dordrecht, 1980.
A. Newell. Physical symbol systems. Cognitive Science, 4:135–183, 1980.
J. Piaget. Introduction a` l’E´pist´emologie G´en´etique. PUF, Paris, 1950.
J. Piaget. Biology and Knowledge: an essay on the relations between organic regulations and cognitive processes. UCP, Chicago, 1971.
J. Piaget. The development of thought : equilibration of cognitive structures. Viking Press, New York, 1977.
A. Pnueli. Linear and branching structures in the semantics and logics of reactive systems. In
W. Brauer, editor, ICALP185 – 12th Int. Colloq. on Automata, Languages, and Programming, pages 15–32. Springer-Verlag, 1985.
H. Rogers. Theory of Recursive Functions and Effective Computability. McGraw-Hill, New York, 1967.
D. S. Scott. The lattice of flow diagrams. In E. Engeler, editor, Symp. on semantics of algorithmic languages, pages 311–366. Springer-Verlag, 1971, 1971.
D. S. Scott. Lattice theory, data types and semantics. In R. Rustin, editor, NYU Symposium on Formal Semantics, pages 64–106, New York, 1972. Prentice-Hall.
D. S. Scott. Data types as lattices. SIAM J. Computing, 5:522–587, 1976.
D. S. Scott. Domains for denotational semantics. In Proc. ICALP’82, pages 577–613, Berlin, 1982. Springer-Verlag. LNCS, vol. 140.
A. M. Turing. On computable numbers, with an application to the entscheidungsproblem.
Proc. London Math. Soc., 42:230–265, 1936.
J. von Neumann. The Computer and the Brain. Yale Univ. Press, New Haven, 1958. (2nd. ed., 1967).
P. Wegner. Machine models and simulations. In Wegner Agha and Yonezawa, editors, Research Directions in Concurent Object-Oriented Programming. MIT Press, 1993.
P. Wegner. Why interaction is more powerful then algorithms. Comm. of the ACM, May 1997.
P. Wegner. Interactive foundations of computing. Theoretical Computer Science, Feb. 1998.
P. Wegner. Towards empirical computer science. The Monist, Spring 1999.
P. Wegner and D. Goldin. Interaction, computability, and Church’s Thesis, 1999.
P. Wegner and D. Goldin. Mathematical models of interactive computing, 1999.
