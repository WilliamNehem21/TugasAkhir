
ORIGINAL ARTICLE

Color image retrieval using statistical model and radial basis function neural network
K. Seetharaman, S. Sathiamoorthy *

Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar 608 002, Tamil Nadu, India

Received 11 August 2013; revised 5 February 2014; accepted 11 February 2014
Available online 6 March 2014

Abstract This paper proposes a new and effective framework for color image retrieval based on Full Range Autoregressive Model (FRAR). Bayesian approach (BA) is used to estimate the param- eters of the FRAR model. The color autocorrelogram, a new version of edge histogram descriptor (EHD) and micro-texture (MT) features are extracted using a common framework based on the FRAR model with BA. The extracted features are combined to form a feature vector, which is nor- malized and stored in image feature vector database. The feature vector database is categorized according to the nature of the images using the radial basis function neural network (RBFNN) and k-means clustering algorithm. The proposed system adopted Manhattan distance measure of order one to measure the similarity between the query and target images in the categorized and indexed feature vector database. The query refinement approach of short-term learning based rel- evance feedback mechanism is adopted to reduce the semantic gap. The experimental results, based on precision and recall method are reported. It demonstrates the performance of the improved EHD, effectiveness and efficiency achieved by the proposed framework.
© 2014 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University.



Introduction

Rapid development in the multimedia and its related fields drastically increases the size of the image repositories in diverse fields such as medicine, media, commerce, engineering, and

* Corresponding author. Tel.: +91 (0) 9994029213.
E-mail address: ks_sathia@yahoo.com (S. Sathiamoorthy).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
entertainment. Simultaneously, the significant increase in the use of images for training, education and research in diverse fields greatly demands for effective and efficient system for storing and retrieving the images in/from the huge repositories, which is very difficult and most challenging task than ever be- fore for a research community.
The traditional image retrieval techniques rely on manually annotated textual keywords [1]. With the large-scale image repositories, the results of textual keyword based retrieval sys- tems are not completely reliable due to the usage of limited number of textual keywords, being subjective, laborious, time-consuming and tedious. The content based image retrie- val (CBIR) system overcomes the shortcomings of textual key- word based image retrieval systems and it is an effective and


1110-8665 © 2014 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. http://dx.doi.org/10.1016/j.eij.2014.02.001

60	K. Seetharaman, S. Sathiamoorthy


efficient solution to deal with the large-scale image reposito- ries. Thus, many effective CBIR systems have been presented in the literature [2–10] since 1990s with varying degree of reli- ability, capability and automation. The images in the CBIR systems are represented by a set of visual contents such as col- or, texture, spatial, and shape.
Since, color is one of the most prominent perceptual fea- tures of an image, several approaches such as color histogram, color moments, color correlogram, color autocorrelogram and MPEG-7 based scalable color descriptor (SCD), dominant col- or descriptor (DCD), color layout descriptor (CLD) and color structure descriptor (CSD) have been used in the literature [5,11] to represent the color information of an image.
Texture feature explores the structural arrangement of sur- faces and its relationship in the images. Hence, many research- ers [5,11–14] conducted studies on texture features extracted using the Markov random field, Hidden Markov random field, Autoregressive, Multiresolution Gaussian Autoregressive, Gibbs field, Homogeneous texture descriptor (HTD), BDIP and BVLC, wavelet transform based techniques and so on.
Shape features also provide potential information of an im- age. Many research efforts have been taken to describe the shape features [15–19] based on region and edge information. However, region based approach may not be easy and reliable for a diverse collection of images due to the unavailability of fully automated generalized approach [20]. Hence, most of the researchers employed the edge based techniques [21–25] for CBIR system.
In the same line, many researchers performed semantic based image retrieval [26–29]. But, they are not able to estab- lish a robust map between automatically annotated textual keywords and visual contents for a wide range of visual con- cepts. Hence, it affects the desired level of generalization and accuracy. Moreover, creating a dictionary of textual keywords is difficult due to the semantic gap problem and it also requires manual verification process, which is tedious and time consum- ing. Recently, a few numbers of studies have been conducted to construct composite feature descriptors [21,23,30], namely joint composite descriptor (JCD) and micro-structure descrip- tor (MSD).
JCD is a combination of color and edge directivity descrip- tor (CEDD) and fuzzy color and texture histogram (FCTH). The JCD describes color, texture and shape information. For color information, JCD uses 24-bin color histogram produced by the 24-bin fuzzy-linking system, for texture information it uses energy in high frequency bands of Haar wavelet transform and for shape information it exploits MPEG-7 based edge his- togram descriptor (EHD). The MSD extracts and describes the color, texture and shape information using the edge orienta- tion similarity with the underlying colors.
Although, a number of techniques have been developed by the researchers, the retrieval accuracy of the existing CBIR sys- tems is still limited and unsatisfactory. Thus, there is a need of an increased attention for extracting compact and more bal- anced visual characteristics of an image. Moreover, most of the early studies extract various visual characteristics (color, texture, shape, etc.) of an image using various kinds of tech- niques, which is a cumbersome process due to the complemen- tariness of techniques.
The main objective of the proposed work is developing an efficient and effective CBIR system, which extracts all kinds of visual features (color, texture, spatial and shape) of color
images in HSV color space using a framework based on Full Range Autoregressive (FRAR) model with Bayesian approach (BA) [12,13,31]. In the proposed system, the effectiveness of FRAR model with BA in capturing the edge and texture fea- tures of gray-scale images is successfully incorporated into col- or images in HSV color space and the same framework is used to extract the color features of an image. The extracted fea- tures are combined, normalized and stored in a separate fea- ture vector database. The feature vectors in the feature vector database are indexed [32].
Artificial neural network techniques provide a potential solution for categorization task [33]. Recently, radial basis function neural network (RBFNN) [34] has attracted much attention due to its simple architecture, very efficient in learn- ing, function approximation [35] and categorization tasks [36], and ability to escape from the local minima [37]. Hence, the present paper exploits the potential of RBFNN for image cat- egorization. In RBFNN, the k-means algorithm [38] has been incorporated to determine the number of cluster centers. Cor- respondingly, several researchers reported that CBIR systems have been successfully applied the relevance feedback (RF) mechanism [39–41] to reduce the semantic gap and it consider- ably improves the retrieval performance of CBIR systems. Thus, this paper constructs a CBIR system with RF mecha- nism in short term learning. Manhattan distance measure [42,43] of order one is used to measure the similarity between the query and target images in the categorized and indexed fea- ture vector space. Precision and recall method [42] is used to measure the performance of the proposed system.
The rest of the paper is organized as follows. The FRAR model is described in Section 2, while the feature extraction method is discussed in Section 3. Section 4 explains the RBFNN. The measure of similarity and performance is pro- vided in Section 5. The proposed retrieval system is explained in Section 6. Section 7 provides experiments and results. Final- ly, conclusion is formulated in Section 8.

FRAR model

Recent literature reports that a framework based on FRAR model [12,13,31] outperforms the existing methods in terms of capturing the edge and texture features of gray-scale images. In the proposed system, a framework based on FRAR model with BA is used for extracting color and its spatial informa- tion, shape information and micro-textures of color images in HSV color space.
Let X be a random variable that represents the intensity va- lue of a pixel at location (k, l) in an image of size L · L. The FRAR model is expressed in Eq. (1).
M	M
X(k; l)=	CrX(k + p; l + q)+ e(k; l)	(1)
p = —Mq=—M
p = q–0
where Cr =	and r = |p|+ |q|+ M(M — 1)/2.
The initial assumption about the parameters are K ∈ R;
a > 1; h, /  [0, 2p] and n  {1,2,...}. In Eq. (1), X(k + p,
l + q) is the spatial variation due to image properties and e(k, l) is the spatial variation due to additive noise and FRAR model coefficients Cr, (r = 1, 2) is the variation among the low-level primitives in the sub-image region of size M · M,

CBIR using FRAR model and RBFNN	61


(M < L). The model coefficients are interrelated. The interre- lationship is established through the model parameters K, a, h, and /, which are estimated using the BA [12,13,31].

Feature extraction

Color autocorrelogram

In the proposed system, color feature is extracted from the H and S components of an image. The H and S components of the images are uniformly quantized using the Generalized Lloyd algorithm [5]. The number of quantization level (L) is fixed to 8. The autocorrelation coefficients (ak) are derived from the FRAR model coefficients Cr as follows:
1	C1
(ac )= 1 — C ;
C2 + C1 — C2

side the confidence limit, at 80% level of significance), it is identified as untexturedness. The dimension of extracted MTs is 201 in [12,13], which is very high and it results in high computational and storage cost. Therefore, the proposed sys- tem clusters all the identified MTs into 32 categories using k- means clustering algorithm and the center of each category/ cluster is used to represent the MTs of that category. A num- ber of categories (i.e., 32) are determined empirically for our experimental image database without affecting the perfor- mance of the proposed system. Subsequently, the representa- tives (i.e., center) of all the categories are ordered in ascending manner and numbered from 0 to 31. These numbers represent the MTs and called texnum. Then, the number of MTs in each category is computed and called texspectrum. The histogram of texnum vs. texspectrum is formed to repre- sent the different types of MTs in an image. Fig. 1 depicts the example of proposed MTs.

(ac )= 
 1	2
1 — C2
and
3.3. Improved EHD

(ac )= 
 C1(C2 + 2C1 — C2)
1 — C2
The MPEG-7’s EHD [21–25] represents the edge features based on their orientations and it is constructed using the

Similarly, the kth order autocorrelation coefficient can be
obtained by solving the following equation using recurrence relation.
ak = C1ak—1 + C2ak—2;	1 6 k 6 m;	c ∈ (1, 2, .. . , L)	(2)
edges detected by the Sobel operator, which is sensitive to noise and failed to detect very minute and fine edges. Thus, the proposed system extracts very minute and fine edges using a framework based on FRAR model with BA [31], which out-



where m is the lag variable, k is the order of autocorrelation coefficient and c represents the color. In the proposed system, the color autocorrelogram, which captures the spatial correla- tion between the identical colors at a distance l is computed for H and S components of an image. Since, the smallest distance gave the most detailed local properties of the image, the pro- posed system fix it to 1.

Micro-textures (MTs)

The V component of an image is divided into number of over- lapping sub-image regions of size 3 · 3, to locally characterize the nature of the image. The FRAR model coefficients Cr, (r = 1, 2) are computed for each sub-image region by using the FRAR model parameters K, a, h and /, which are esti- mated using the BA. The autocorrelation coefficient is com- puted for each sub-image region from its model coefficients Cr. The MTs [12,13] that are below or above the threshold for humans to understand are identified in each sub-image regions by measuring the homogeneity of variances among the com- puted autocorrelation coefficients using a statistical test of
hypotheses and it is given in Eq. (3).
witt operators, and also insensitive to noise due to the basic properties of FRAR model.
The FRAR model in Eq. (1) is applied on V component im- age to estimate its image surface. Then, the difference between the V component image and estimated surface is computed and the resultant image is called residual images. The global confidence limit is measured for each sub-image region (3 · 3) of residual image with a desired significance level. The global confidence limit is calculated with the use of global mean and standard deviation. If the pixel value is greater than or equal to the global confidence limit then it is squared; other- wise the pixel value is replaced with zero. The value other than 0 represents the edges and 0 represents non-edge part in the im- age. This extracts thick edges. To suppress the pixels around the edge pixel and to obtain thin edges, the non-maxima sup- pression algorithm is applied with the use of local confidence limit. The local confidence limit is calculated with the use of lo- cal mean and standard deviation of thick edge map. Each va- lue in the thick edge map is compared with the confidence limit. If the value in the thick edge map is greater than the con- fidence limit then it is identified as edge pixel and is represented with the actual pixel value. Otherwise it is identified as non-
edge pixel and is represented as 0. After extracting the edge

Dem =

n 1
— Re
1/m
m 
(3)
pixels for the entire image, the orientation of each edge pixel is computed. Then, the orientations of edge pixels are quan- tized into 72 bins of 5 degree each, which is used to form an

where Rm is the autocorrelation matrix built by using the stan-
dardized autocorrelation coefficients, n is the number of sam-
ples and m is the lag variable. The statistical test of hypotheses is based on the measure  a(r/,n), where a is level of signifi-
cance and r is standard deviation. The autocorrelation coeffi-
cient is compared with the measure a r/ n to find the outcome of the test. If the autocorrelation is highly significant, then it is considered that there exist micro-textures in the sub- image region, or else (autocorrelation coefficients that fall in-
improved EHD.

RBFNN

The basic architecture of RBFNN is depicted in Fig. 2. The RBFNN is a three-layer feed-forward neural network, its input layer is connected with the hidden layer without weights and hidden layer is connected with the output layer with weights.




62	K. Seetharaman, S. Sathiamoorthy



























Figure 1	(a) Input image in RGB format, (b) V component image (input image is converted into HSV format then H, S and V components of image are segregated), and (c) proposed MTs of V component image.

The better values of centroids and the spread are set by trial and error techniques. The distance between the centroid and input vector pattern in n dimensional space is measured by the most commonly used Euclidean norm and is given by


I — c 
= q(ﬃﬃIﬃﬃﬃﬃﬃ—ﬃﬃﬃﬃﬃcﬃﬃﬃ)ﬃﬃ2ﬃﬃﬃ+ﬃﬃﬃﬃﬃ(ﬃIﬃﬃﬃﬃﬃ—ﬃﬃﬃﬃﬃcﬃﬃﬃ)ﬃﬃ2ﬃﬃﬃ+ﬃﬃﬃﬃﬃ·ﬃﬃ·ﬃﬃ·ﬃﬃ+ﬃﬃﬃﬃﬃ(ﬃﬃIﬃﬃﬃﬃﬃ—ﬃﬃﬃﬃﬃcﬃﬃﬃ)ﬃ2ﬃﬃ

(5)


where ||Æ|| is the norm of (I  cj).
The output of the Gaussian activation function is high, when the input is close to the centroid and it decreases rapidly to zero as the input’s distance from the centroid increases.


Similarity and performance measure


Figure 2	The basic architecture of RBFNN.

The number of hidden neurons in the hidden layer is deter- mined by the k-means clustering algorithm. Each hidden neu- ron in the hidden layer represents the Gaussian activation function and it computes the distance between the input vector pattern and centroids. For a given n dimensional input vector I  Ii  ffin, the output vector O  Oj  ffip is calculated as in Eq. (4).
Similarity measure plays a noteworthy role in CBIR systems by computing the distance between feature vector of the query and target images. Literature reports that CBIR systems use similarity measures from the computational geometry, statis- tics and information theory. In the proposed system, the most frequently used geometrical similarity measure, L1 is used, which is a special case of the Manhattan metric. The L1 simi- larity measure is more robust to outliers, performs better than L2 and it saves much computational cost. The L1 distance mea-
sure between the query and target images is expressed as

Xh	 I — ci  

Oj = b0 +
wjih exp  —
2r2
(4)
n
D(Q, T)= 
(Q — T )	(6)

i=1
b0 is the bias term and w
i
is the weight of the hidden neuron i
  I—c 
i	i
i=1


	

neuron respectively. The weights wji represent the relative importance of the Gaussian activation functions in response to an external input.
feature vector.
The performance of the proposed CBIR system is measured in three aspects namely, efficiency, effectiveness and computa-

CBIR using FRAR model and RBFNN	63

tional complexity. Efficiency is associated with the storage requirements. The effectiveness of a system is related to the re- trieval accuracy of the system and is measured using the most
also relevant) and recall (percentage of relevant images that are retrieved) methods and is written as
Ri

widely used precision (percentage of retrieved images that are
Precision = T
(7)































































Figure 3	Architecture of the proposed CBIR system.

64	K. Seetharaman, S. Sathiamoorthy


Recall	Ri T
(8)
a framework based on FRAR model with BA. The MTs are extracted from the V component image as described in

where Ri is thenumberof relevant retrieved images, Ti is thenum- ber of all retrieved images and T is the total number of relevant images in the image database. The effectiveness of the proposed system is also measured in terms of average recognition rate (AVR), which is defined as the percentage of retrieved images in top matches, which belongs to the same class as a query image.

Proposed image retrieval method

The architecture and various components of the proposed CBIR system are depicted in Fig. 3. In the proposed system, the color images in RGB color format are converted into HSV color format [44,45], where H (Hue) and S (Saturation) components are related to chromatic information and V com- ponent contains achromatic information. The color autocorre- logram is computed for H and S components of an image using
Section 3.2.
The retrieval performance of conventional EHD is im- proved in this paper by extracting the very minute and fine edges using a framework based on FRAR model with BA [31]. In the proposed system, V component image is used for edge extraction. After edge detection, orientation of each edge is computed and they are quantized into 72 bins of 5 degrees each, which is used to form an improved EHD.
The proposed system combines the automatically extracted features into one feature vector to represent each image in the experimental database. The dimension of the combined feature vector (color – 16 (H component: 8 and S component: 8), shape – 72, and texture – 32) is 120. The input feature vector is normalized to reduce the effect of outliers in the data and to obtain the same range of values for each proposed feature.

















































Figure 4	Sample images taken from the experimental database.

CBIR using FRAR model and RBFNN	65

The normalized feature vectors are classified using RBFNN. In order to train the RBFNN, the feature vectors in the feature vector database are arbitrarily divided into ten sets of approximately equal sizes to attain a ten fold cross val- idation. In each execution of the ten fold cross validation, nine image sub-classes are considered for training and one for testing. So that, every sub-class was used as test data once. The optimal number of hidden neurons and its spread are determined by using the ten fold cross validation and they are utilized for the final training of RBFNN. Categorization of feature vectors using the RBFNN significantly improves the performance of the proposed CBIR system by fil- tering out irrelevant categories of feature vectors. The
categorized feature vectors are stored in a separate feature vec- tor database and then they are indexed to increase the speed of retrieval.
In the proposed system, the semantic gap [46] between the low-level visual features and high-level semantic concepts per- ceived by the users is reduced by adopting the query refinement approach of short-term learning based RF mechanism in which the user marks the retrieved images as relevant, highly relevant and irrelevant to the query image. If the user is not satisfactory with the result, the mean feature vector of the query image and the images that are marked as highly relevant is computed, and is used as a new query image in the next iter- ation for search.

































Figure 5	(a) Average precision verses recall of an improved EHD and MPEG-7’s EHD; (b) average precision verses recall of the proposed MTs, MPEG-7’s HTD descriptor and conventional MTs.

















Figure 6	Average precision vs. recall of the proposed feature vector, the various combination of MPEG-7’s visual descriptors, JCD and MSD.

66	K. Seetharaman, S. Sathiamoorthy


While query image is posed to the proposed system, it automatically extracts the features of query image using a framework based on FRAR model with BA and finds the cat- egory of a query image using the RBFNN and performs sim- ilarity matching task using the L1 distance measure in the corresponding category of indexed feature database to fetch the target images. The retrieved images are ranked based on the similarity scores. An image with lowest similarity score is assigned higher rank. The user interaction is allowed in the fi- nal stage of retrieval process to refine the search.
Experiments and results

In order to implement the proposed system, more than two thousand color images are collected from the image databases
such as Freefoto database (http://Freefoto.com), Wang’s data- base (http://wang.ist.psu.edu/docs/related.shtml), Corel DB, VisTex DB and UCID database (http://vision.cs.aston.ac.uk/ datasets/UCID/ucid.html). For sample, some of them have been presented in Fig. 4.
A subset of 100 query images is selected at random from the experimental database to evaluate the performance of the pro- posed EHD and MTs. In the experiment, the proposed EHD and MTs are compared with the conventional EHD, MTs [12,13] and HTD. The comparative result is shown in Fig. 5. The average retrieval rate attained by the proposed EHD and MTs is 44.14% and 55.36% and the conventional EHD, MTs and HTD are 40.12%, 55.91% and 52.07% respectively. The experimental results clearly reveal that the retrieval performance of the proposed EHD and MTs is superior to




















































Figure 7	(a) Query image; and (b–k) retrieval results obtained with the proposed CBIR system.

CBIR using FRAR model and RBFNN	67












MPEG-7’s EHD and HTD respectively. The results also reveal that the retrieval performance of conventional MTs is slightly better than the proposed MTs and the difference in the retrieval performance is 0.55%, which is very less. Hence, it is neglected in the proposed system to circumvent the high computational and storage cost of conventional MTs. Subsequently, we compare the performance of the proposed feature vector with the combi- nations of MPEG-7’s CSD, EHD and HTD; MPEG-7’s SCD, EHD and HTD; JCD and MSD. It is observed from the exper- imental results that the retrieval performance of the proposed technique is better and the average retrieval rate is 84.97% for the proposed technique 80.42% 79.40%, 59.21% and 57.16% for the combination of MPEG-7’s CSD, EHD and HTD; MPEG-7’s SCD, EHD and HTD; JCD and MSD respectively. Fig. 6 depicts the average precision and recall curves of the pro- posed technique and the aforementioned combination of MPEG-7’s visual descriptors, JCD and MSD.
In order to measure the robustness of the proposed system against illumination, viewing changes, scaling and rotation, some of the images in the databases are replicated then few of them are rotated at different angles and the remaining rep- licated images are either scaled up or scaled down. The images taken in different viewing positions and lighting conditions are also considered in our experiments. We observed that the experimental results are more robust than the conventional methods with the scaling, rotation, illumination and viewing invariance. Fig. 7 depicts the query image and the retrieval re- sult of the proposed system, where the retrieved images are ranked and displayed in descending order of the similarity scores from top left to bottom right.
The proposed work is implemented with the system config- uration:  Pentium®  Dual  core  personal  computer  with
2.20 GHz processor; 2 GB RAM; Java. The dimension and computational complexity of various feature vectors used in our experiments are shown in Table 1. From Table 1, it is evi- dent that the computational complexity of the proposed tech- nique is cheaper than the various combinations of MPEG-7 based descriptors and JCD. It is also observed that though the performance of the proposed combination of feature vector is noticeably greater, the size and computational complexity of MSD is significantly less, which is negligible.
From the experiments, it is observed that the retrieval re- sults are satisfactory and matches the human visual perception to some extent. It is also demonstrated that the proposed sys- tem maintains the balance between the dimensions of feature vectors, retrieval performance and computational complexity.

Conclusion

In this paper, a common framework based on FRAR model with BA is employed to extract color autocorrelogram, im-











proved EHD and micro-textures of an image, which are dom- inant, rich in information, high discriminative power, less computational cost and low storage space requirement. The improved EHD and compact MTs yield better results, when compared to that of MPEG-7’s EHD, HTD and conventional MTs. The combination of proposed feature vector also out- performs the various combinations of MPEG-7’s visual descriptors (CSD, EHD and HTD; and SCD, EHD and HTD) and compact composite descriptors such as JCD and MSD. The proposed system is very useful for retrieving the images more efficiently and effectively in various application domains such as medicine, engineering, education, and re- search. In future, the proposed work can be extended to incor- porate other machine learning mechanisms for further improvement.

References

Chang SK, Hsu A. Image information systems: where do we go from here? IEEE Trans Knowl Data Eng 1992;4(5):431–42.
Flickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, et al. Query by image and video content: the QBIC system. IEEE Comput 1995;28(9):23–32.
Jain AK, Vailaya A. Image retrieval using color and shape. Pattern Recogn 1996;29(8):1233–44.
Rahman MM, Bhattacharya P, Desai BC. A framework for medical image retrieval using machine learning and statistical similarity matching techniques with relevance feedback. IEEE Trans Inf Technol Biomed 2007;11(1):59–69.
Chun YD, Kim NC, Jang IH. Content-based image retrieval using multiresolution color and texture features. IEEE Trans Multi- media 2008;10(6):1073–84.
Chen H, Gao Z, Lu G, Li S. A novel support vector machine fuzzy network for image classification using MPEG-7 visual descriptors. In: International conference on multimedia and information technology; 2008. p. 365–8.
Lin CH, Chen RT, Chan YK. A smart content-based image retrieval system based on color and texture feature. Image Vis Comput 2009;27(6):658–65.
Iqbal K, Odetayo MO, James A. Content-based image retrieval approach for biometric security using colour, texture and shape features controlled by fuzzy heuristics. J Comput Syst Sci 2012;78(4):1258–77.
Wang XY, Yu YJ, Yang HY. An effective image retrieval scheme using color, texture and shape features. Comput Stand Inter 2011;33:59–68.
Yuan WX, Feng C, Jiao Y. An effective method for color image retrieval based on texture. Comput Stand Inter 2012;34:31–5.
Penatti OAB, Valle E, Torres RDS. Comparative study of global color and texture descriptors for web image retrieval. J Vis Commun Image R 2012;23:359–80.
Seetharaman K. Texture analysis based on a family of stochastic models. In: Signal and image processing applications (ICSIPA), 2009 IEEE international conference; 2009. p. 518–23.

68	K. Seetharaman, S. Sathiamoorthy


Seetharaman K, Palanivel N. Texture characterization represen- tation description and classification based on full range Gaussian Markov random field model with Bayesian approach. Int J Image Data Fus 2013:1–24.
Mojsilovi´c A, Popovi´c MV, Ne˘skovi´c AN, Popovi´c AD. Wavelet
image extension for analysis and classification of infarcted myocardial tissue. IEEE Trans Biomed Eng 1997;44(9):856–66.
Gagaudakis G, Rosin PL. Incorporating shape into histograms for CBIR. Pattern Recogn 2002;35(1):81–91.
Gagaudakis G, Rosin PL. Shape measures for image retrieval. Pattern Recogn Lett 2003;24(15):2711–21.
Wong WT, Shih FY, Liu J. Shape-based image retrieval using support vector machines, Fourier descriptors and self-organizing maps. Inf Sci 2007;177(8):1878–91.
Drew MS, Lee TK, Rova A. Shape retrieval with eigen-CSS search. Image Vis Comput 2009;27:748–55.
Chiang CC, Hung YP, Yang H, Lee GC. Region-based image retrieval using color-size features of watershed regions. J Vis Commun Image R 2009;20(3):167–77.
Mu¨ller H, Michoux N, Bandon D, Geissbuhler A. A review of content-based image retrieval systems in medical applications- clinical benefits and future directions. Int J Med Inform 2004;73(1):1–23.
Liu GH, Li ZY, Zhang L, Xu Y. Image retrieval based on micro- structure descriptor. Pattern Recogn 2011;44(9):2123–33.
Chang SF, Sikora T, Puri A. Overview of the MPEG-7 standard. IEEE Trans Circ Syst Video Technol 2001;11(6):688–95.
Chatzichristofis SA, Zagoris K, Boutalis YS, Papamarkos N. Accurate image retrieval based on compact composite descriptors and relevance feedback information. Int J Pattern Recogn Artif Intell 2010;24(2):207–44.
Alefs B, Eschemann G, Ramoser H, et al. Road sign detection from edge orientation histograms. In: Proceedings of the 2007 IEEE intelligent vehicles symposium, Istanbul, Turkey; 2007. p. 993–8.
Won CS, Park DK, Park SJ. Efficient use of MPEG-7 edge histogram descriptor. ETRI J 2002;24(1):23–30.
Jeon J, Lavrenko V, Manmantha R. Automatic image annotation and retrieval using cross-media relevance models. In: ACM conference on special interest group on information retrieval (SIGIR) Toronto, Canada; 2003. p. 119–26.
Zhang D, Islam MM, Lu G. A review on automatic image annotation techniques. Pattern Recogn 2012;45:346–62.
Zhao Y, Zhao Y, Zhu Z. TSVM-HMM: transductive SVM based hidden Markov model for automatic image annotation. Expert Syst Appl 2009;36(6):9813–8.
Burdescu DD, Mihai CG, Stanescu L, Brezovan M. Automatic image annotation and semantic based image retrieval for medical domain. Neurocomputing 2013;109:33–48.
Savvas A, Chatzichristofis AA. Late fusion of compact composite descriptors for retrieval from heterogeneous image databases. In: SIGIR’10, Geneva, Switzerland; July 19–23, 2010.
Seetharaman K, Krishnamoorthi R. A statistical framework based on a family of full range autoregressive models for edge extraction. Pattern Recogn Lett 2007;28(7):759–70.
White DA, Jain R. Algorithms and strategies for similarity retrieval. In: Tech. rep. VCL-96-101, Visual Computing Labora- tory, University of California, San Diego, 9500 Gilman Drive, Mail Code 0407, La Jolla, CA 92093-0407; 1996.
Zhang GP. Neural network for classification: a survey. IEEE Trans Syst Man Cybern––Part C: Appl Rev 2000;30(4):451–62.
Broomhead DS, Lowe D. Multivariable functional interpolation and adaptive network. Complex Syst 1998;2:321–55.
Park J, Sandberg IW. Approximation and radial basis function networks. Neural Comput 1993;5:305–16.
Maglogiannis I, Sarimveis H, Kiranoudis CT, Chatziioannou AA, Oikonomou N, Aidinis V. Radial basis function neural networks classification for the recognition of idiopathic pulmonary fibrosis in microscopic images. IEEE Trans Inf Technol Biomed 2008;12(1):42–54.
Fu X, Wang L. Data dimensionality reduction with application to simplifying RBF network structure and improving classification performance. IEEE Trans Syst Man Cybern – Part B: Cybern 2003;33(3):399–409.
Darken C, Moody J. Fast adaptive k-means clustering: some empirical results. In: Proc. IEEE INNS int. joint conf. neural netw.; 1990. p. 233–8.
Leo´n T, Carello PZ, Ayala G, deVes E, Domingo J. Applying logistic regression to relevance feedback in image retrieval systems. Pattern Recogn 2007;40(10):2621–32.
Xu X, Lee D-J, Antani SK, Long LR, Archibald JK. Using relevance feedback with short-term memory for content-based spine X-ray image retrieval. Neurocomputing 2009;72:2259–69.
Squire D, Muller W, Muller H. Relevance feedback and term weighting techniques for content-based image retrieval. In: Technical report, Computer Vision Group, University of Geneva, No. 98.05; 1998.
Huang J, Kumar SR, Mitra M, Zhu W, Zabih R. Image indexing using color correlograms. Proc CVPR 1997:762–8.
Seetharaman K, Sathiamoorthy S. A statistical model based color image retrieval. In: Proceedings of the international conference on recent trends in computer applications (ICRTCA’2013), Chennai, India; 2013. p. 91–6.
Karkanis SA, Iakovidis DK, Maroulis DE, Karras DA, Tzivras
M. Computer-aided tumor detection in endoscopic video using color wavelet features. IEEE Trans Inf Technol Biomed 2003;7(3):141–52.
Cheng HD, Jiang XH, Sun Y, Wang J. Color image segmentation: advances and prospects. Pattern Recogn 2001;34:2259–81.
Su Z, Zhang H, Li S, Ma S. Relevance feedback in content-based image retrieval: Bayesian framework, feature subspaces, and progressive learning. IEEE Trans Image Process 2003;12(8):924–37.
