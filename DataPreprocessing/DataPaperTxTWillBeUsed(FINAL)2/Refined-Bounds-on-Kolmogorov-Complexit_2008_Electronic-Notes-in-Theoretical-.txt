

Electronic Notes in Theoretical Computer Science 221 (2008) 181–189
www.elsevier.com/locate/entcs

Refined Bounds on Kolmogorov Complexity for ω-Languages
J¨oran Mielke1
Institute for Computer Science Martin–Luther–University Halle(Saale), Germany

Abstract
The paper investigates bounds on various notions of complexity for ω–languages. We understand the complexity of an ω–languages as the complexity of the most complex strings contained in it. There have been shown bounds on simple and prefix complexity using fractal Hausdorff dimension. Here these bounds are refined by using general Hausdorff measure originally introduced by Felix Hausdorff. Furthermore a lower bound for a priori complexity is shown.
Keywords: Kolmogorov complexity, ω–languages, measures


Introduction
Algorithmic complexity was introduced to investigate the amount of information of strings. It measures the information of a string as the length of the shortest programme that outputs the string. A comprehensive work on the variants of complexity is the book [4] by Li and Vita´niy. The approach on ω–languages we follow is to find complexity bounds for the most complex ω–words contained in the ω–language. Here we find another witness to the obvious assumption that large ω–languages contain complex ω–words. The notion of ’large’ used in this paper is taken from geometric measure theory. In [3] Felix Hausdorff introduced the gen- eral fractal Hausdorff measure which allows a detailed investigation of infinite sets having Lebesgue measure zero. The concepts of fractal geometry are described at full length in Falconer’s book [2]. In the papers [8] and [1] have been proved lower bounds for simple and prefix complexity depending on the Hausdorff dimension. We use the general Hausdorff measures to improve these bounds and to find a lower bound on a priori complexity for ω–languages.

1 Email: mielke@informatik.uni-halle.de

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.12.016

Notation and Preliminary Results
In this section we briefly recall the concept of Kolmogorov complexity of (in)finite words and measures (of ω-languages). For more detailed information the reader is referred to the textbooks [4] and [2]. In the following X is a finite alphabet with cardinality |X| = r. By X∗ we denote the set (monoid) of words on X, including the empty word ε, and Xω is the set of infinite words (ω–words) over X. For w ∈ X∗ and η ∈ X∗ ∪ Xω let w · η be their concatenation. We extend this concatenation in the obvious way to subsets W ⊆ X∗ and B ⊆ X∗ ∪ Xω. For a language W let W∗ := n∈N Wn be the submonoid of X∗ generated by W , and by Wω := {w1 ··· wn ··· | wn ∈ W \ {ε}} we denote the subset of Xω formed by concatenating words of W . Furthermore |w| is the length of the word w ∈ X∗ and pref(B) is the set of all finite prefixes of strings in B ⊆ X∗ ∪ Xω, we abbreviate w ∈ pref({η}) by w ± η. By ξ[n] we denote the prefix of ξ ∈ X∗ ∪ Xω of length
n. And again for any language W let Wδ := {ξ | |pref(ξ) ∩ W| = ∞} the subset of
ω–words of Xω containing infinitely many prefixes of W , called δ–limit of W .
It is useful to consider the set Xω as a metric space (Cantor space) (Xω, ρ) of all ω–words over the alphabet X where the metric is ρ is defined as follows
ρ(ξ, η) := inf{r−|w| | w и ξ ∧ w и η}
The open (and simultaneously closed) balls in (Xω, ρ) are the sets of the form w·Xω, where w ∈ X∗. The diameter of these balls is d(w · Xω)= r−|w|.
Programme size complexity defines the complexity of a finite string to be the length of a shortest programme which prints the string. Let ϕ : X∗ → X∗ be a partial–recursive function. The complexity of a word w ∈ X∗ with respect to ϕ is defined as
Kϕ(w) := {|π|| π ∈ X∗ ∧ ϕ(π)= w}
It is well known that there is an optimal partial–recursive function U, that is, a function satisfying that for every partial–recursive function ϕ
∃cϕ∀w(w ∈ X∗ → KU(w) ≤ Kϕ(w)+ cϕ)
We fix an optimal function U and further on we call the complexity with respect to this function KS. The conditional complexity Kϕ(w|n) is length of a shortest programme which outputs w under the additional input n (w.r.t. function ϕ). For every w ∈ X∗ and n ∈ N holds KS(w|n) ≤ KS(w)+ c true.
If we solely consider partial–recursive functions ϕ with prefix–free domains
dom(ϕ) ⊆ X∗ we obtain an optimal function in the same way. The complexity function with respect to this (fixed) optimal function is called KP.
The third notion of complexity this paper deals with is a priori complexity. It is obtained in the following way. Consider a semimeasure m on X∗, that is a function which satisfies m(ε) ≤ 1 and m(w) ≥  x∈X m(wx), for any w ∈ X∗. In [10] Levin proved the existence of a universal semicomputable semimeasure m', that is for all semimeasures m there is a constant cm such that
∀w ∈ X∗	m(w) ≤ cm · m'(w)

Then the a priori complexity is defined as KA(w) = − logr m'(w). A well known property of KA is
Proposition 2.1 The function KA is a minimal upper semicomputable total func- tion satisfying
r−KA(w) ≤ 1,	for any preﬁx–free set M ⊆ X∗.
w∈M

Accordingly, the complexity of an infinite word ξ is a function mapping the natural number n to the complexity of the n–length prefix of ξ.
Definition 2.2 Let ξ ∈ Xω.
The function KS(ξ[·]) : N → N is called simple Kolmogorov complexity of ξ.
The function KS(ξ[·]|·): N × N → N is called conditional complexity of ξ.
The function KP(ξ[·]) : N → N is called preﬁx complexity of ξ.
The function KA(ξ[·]) : N → N is called a priori complexity of ξ. Throughout the paper the notation follows mainly Uspensky and Shen in [9].
Generalisation of Hausdorff Measure
As mentioned above, we achieve complexity bounds by a fractal (Hausdorff–) mea- sure. For the purpose of defining the desired measures we need to characterise dimension–functions, that is, functions that “behave well” in the neighbourhood of zero. The behaviour below zero is not important for the definition of our measures. In detail we have the following requirements
Definition 3.1 A function h : [0, ∞) → [0, ∞) is called dimension function if
h(t) > 0 for all t > 0, h(0) = 0,
h is increasing for t ≥ 0, and
h is continuous from the right for all t ≥ 0.
Now we use the usual construction of an outer measure.
Definition 3.2 For F ⊆ Xω and h a dimension function

Hh(F ) := lim inf	h(r−|w|) | F ⊆ W · Xω ∧ ∀w(w ∈ W → |w|≥ l)
l→∞
w∈W
is called the Hausdorff h–measure (or h–measure) of F .
Here the condition ∀w(w ∈ W → |w| ≥ l) means, that the diameters of the covering sets are at most r−l. A well–known family satisfying the conditions of Definition 3.1 is the family of exponential functions h(t)= tα, with 0 ≤ α ≤ 1. The measures derived from these functions are the common α–dimensional (Hausdorff–) measures which we call Lα throughout the paper.

First we examine how the different measures are related to each other. Given two dimension functions g and h a comparison of Hh and Hg can be achieved by simply comparing the behaviour of the functions g and h close to zero. The following lemma gives a relation between the behaviour of the dimension functions g and h and the corresponding measures Hh and Hg.
Lemma 3.3 ([3])
Let g, h dimension functions and F ⊆ Xω.
If h(t) −→ 0 for t −→ 0, then Hg(F ) < ∞ implies Hg(F )= ∞ and Hh(F ) > 0
implies Hg(F )= ∞.
If c1 · g(t) ≤ h(t) ≤ c2 · g(t) for constants c1, c2 > 0 and sufficiently small t, then for every ω–language F it holds c1 · Hg(F ) ≤ Hh(F ) ≤ c2 · Hg(F ).
¿From this lemma we derive that if g(t) ≤ h(t) for sufficiently small t, then Hg(F ) ≤ Hh(F ). Especially if g(t)= c·h(t) for some c > 0, then Hg(F )= c·Hh(F ). Additionally, the second part gives us the following equivalence of the measures Hg and Hh: the measures Hg and Hh are simultaneously zero, positive or infinite, respectively.
First we prove under which conditions an ω–language has non–zero h–measure, and subsequently use these conditions to prove our bounds. The proof of this result follows the line of Lemma 3.8 in [8].
Theorem 3.4 Let V ⊆ X∗ and h be a dimension function. Then
h(r−|v|) < ∞	implies	Hh(V δ)= 0.
v∈V

Proof. Let V (i) := {v | v ∈ V ∧|A(v)∩V | = i+1}. Then V (i) contains exactly those words of V , having i + 1 prefixes in V . Thus for every i ∈ N we have V (i) · Xω ⊇ V δ and V is disjoint union of all V (i). Further on


Hh(V δ) ≤
v∈V (i)
h(r−|v|),	for all i ∈ N.

And since the sum	v∈V h(r−|v|) converges the right hand side tends to zero for large i.		 

Refinement of Complexity Bounds
Simple Kolmogorov complexity
In this section we derive our announced refinement of bounds. First we investi- gate simple Kolmogorov complexity. We want to improve the result from [8]. There it is stated that for an ω–language F ⊆ Xω with Lα(F ) > 0 and an arbitrary func- tion f : N → N which is growing not too slow, that is  i∈N r−f(i) < ∞, there is a ξ ∈ F satisfying
KS(ξ[n]) ≥a.e. α · n − f (n)

In particular this shows that KS(ξ[n]) ≥a.e. α · n − (1 + ε) logr(n)
The next result states, that the gap between the complexity of the most complex ω–words of a language having non–zero h–measure and function − logr(h(r−n)) is at most (1 + ε) · log n.
Theorem 4.1 (Refinement of Inequality 4)
Let F ⊆ Xω and f : N → N an arbitrary function satisfying i∈N r−f(i) < ∞. Then Hh(F ) > 0 implies
∃ξ(ξ ∈ F ∧ KS(ξ[n]) ≥a.e. − logr(h(r−n)) − f (n)
Proof. Define the set of all ω–words with high (conditional) complexity with re- spect to dimension–function h and function f as
E(h, f ) := {ξ | KS(ξ[n] | n) ≥a.e. − logr(h(r−n)) − f (n)}.

Its complement consists of all ω–words having at least infinitely many prefixes w with complexity less than − logr(h(r−|w|)) − f (|w|). Thus this complement is the δ-limit of the set V := {v | KS(v | |v|) < − logr(h(r−n)) − f (n)}. Counting the number of elements of a fixed length n in this set gives us an estimate for
|V ∩Xn| = |{w | |w| = n∧KS(w | n) < − log (h(r−n))−f (n)}| ≤ r− logr (h(r−n))−f (n).
In order to utilise Lemma 3.4 we consider the following sum:
Σ h(r−|v|)= Σ |V ∩ Xn|· h(r−i)

​
v∈V
i∈N
≤ Σ r− logr (h(r−i))−f (i) · h(r−i)= Σ r−f (i)
	

Due to our assumed properties of f the sum (5) is finite. Thus Lemma 3.4 yields Hh(V δ) = Hh(Xω \ E(h, f )) = 0. Hence Hh(F ) = Hh(F ∩ E(h, f )), for any ω– language F ⊆ Xω. This gives us F ∩ E(h, f ) /= ∅, whenever Hh(F ) > 0, which proves our theorem.	 
Consider an ω–language F with Lα(F ) = ∞ and Lα' (F ) = 0, for any α' > α. Thus there exists a ξ ∈ F fulfilling Inequality 4, that is K(ξ[n]) ≥a.e. α · n − f (n). Now take h as a function converging faster to zero than r−α·n satisfying Hh(F ) > 0 and
∀ε > 0	r−α·n > h(r−n) > r−(α+ε)·n
for all n > n0 and some n0 ∈ N. Then our refined bound states there is a ξ ∈ F with K(ξ[n]) ≥a.e. − log h(r−n) − f (n) > α · n − f (n). Thus the bound has been raised.
On the other hand let Lα(F ) = 0 and Lα' (F ) = ∞, for any α' < α. Thus for any ε > 0 there is ξ ∈ F with K(ξ[n]) ≥a.e. (α − ε) · n − f (n). Now let h be again a dimension–function fulfilling
∀ε > 0	r−(α−ε)·n > h(r−n) > r−α·n

and Hh(F ) > 0. As above we can now raise the bound from (α − ε) · n − f (n) to
— log h(r−n) − f (n).
Example 4.2 Let X = {a, b}, F := {a, b} ·  ∞  {a, b}2i−1 · a  and f with
i∈N r−f(i) < ∞ fixed. One can show that L1(F ) = 0 and L1−ε(F ) = ∞, for any ε > 0. Thus there is a ξ ∈ F such that for any ε > 0
K(ξ[n]) ≥a.e. (1 − ε) · n − f (n)
Now consider the dimension function h(r−|w|)= r−|w| · |w|. The h–measure of F is
Hh(F ) = 1. Hence by Theorem 4 there is a ξ ∈ F such that
K(ξ[n]) ≥a.e. − log(r−n · n) − f (n)= n − log n − f (n).
This bound is almost everywhere strictly greater than (1 − ε) · n − f (n).
If 0 < Lα(F ) < ∞ Theorem 4.1 yields no improvement. This can be seen by Lemma 3.3.
In the proof of Theorem 4.1 we have shown that the measure of the set of ω– words of low conditional complexity is zero. Thus we can formulate a similar result for conditional complexity.
Corollary 4.3 Let F ⊆ Xω, Hh(F ) > 0 and f : N → N an arbitrary function satisfying	i∈N r−f(i) < ∞. Then there is a ξ ∈ F satisfying KS(ξ[n] | n) ≥
— logr(h(r−n)) − f (n) for almost every n ∈ N.

Prefix complexity
Due to the fact that the domains of prefix functions have to be prefix–codes, the prefix–complexity of a ω–word is higher than its simple or conditional complexity. The known bound in Equation 6 is taken from [1]. Again, let F ⊆ Xω an ω–language having Lα(F ) > 0 and c > 0 an arbitrary constant. Then there is a ξ ∈ N satisfying
KP(ξ[0 ... n]) ≥a.e. α · n − c
To refine this bound for prefix complexity we take an approach similar to the
one for simple complexity.	Here again we replace the linear function α · n by
— logr(h(r−n)).
Theorem 4.4 (Refinement of Inequality 6)
Let F ⊆ Xω, Hh(F ) > 0 and c > 0 a constant. Then there is a ξ ∈ F fulﬁlling
KP(ξ[0 ... n]) ≥a.e. − logr(h(r−n)) − c.
Proof. We prove this by showing that the set of ω–words having infinitely many prefixes of lower complexity is a Hh–null set. We consider this set as the union of δ–limits of the sets Wc = {w | KP(w) ≤ − log(h(r−|w|)) + c}, depending on the constant c ∈ N. In order to utilise Lemma 3.4 again we estimate the following

bound
1 >
w∈X∗



r−KP(w) >
w∈Wc



r−KP(w) ≥
w∈Wc



r−c · rlog h(r−|w|) ≥ r−c ·
w∈Wc



h(r−|w|)

The first part is known as Kraft’s inequality.	Since c is a constant we have

Σw∈Wc
h(r−|w|) < ∞, thus Lemma 3.4 yields Hh(Wδ) = 0, for arbitrary c ∈ N.

Now from Hh(F ) > 0 it follows that F \  c∈N Wδ /= ∅, which in turn shows our
assertion.	 
The same arguments as for simple complexity prove that we achieved a refine- ment of Inequality 6. A similar result for computable functions h is proved in [6, Theorem 2.6]. Analysing the proof of Theorem 4.4 it yields the following.
Theorem 4.5 Let F ⊆ Xω and Hh(F ) > 0. Then it holds
Hh	c∈N{ξ | KP(ξ[0 ... n]) ≤i.o. − log(h(r−|w|)) − c}  =0 
There is a ξ ∈ F, such that limn→∞ KP(ξ[0 ... n]) − (− log(h(r−|w|))) = ∞

A priori complexity
The last complexity we investigate is a priori complexity. It is known from
[4] and [9] that a priori complexity is upper bounded by prefix complexity but incomparable to simple complexity. The result we show states that ω–languages F having positive Hh–measure contain ω–words of complexity at least − logr(h(r−n)) up to a constant dependent on Hh(F ).
Theorem 4.6 Let F  ⊆ Xω and Hh(F ) > 0.  Then for any constant c >
— log Hh(F ) 2 there is a ξ ∈ F such that
KA(ξ[0 ... n]) ≥a.e. − logr(h(r−n)) − c.

Proof. As in the proof of Theorem 4.1 we define the set of ω–words not fulfilling the asserted inequality as the δ-limit of Wc = {w | KA(w) ≤ − log(h(r−n)) − c}. By Vm we denote those words of Wc having at least length m
Vm = {w | |w|≥ m ∧ KA(w) ≤ − log(h(r−|w|)) − c}
Then Vm+1 ⊆ Vm, for any m ∈ N. Let V m be the set of all words in Vm which have no prefix in Vm. Then V m is a prefix code and V m · Xω covers Wδ. Using
Proposition 2.1 we can estimate the Hh–measure of Wδ as follows
Hh(Wδ) = lim inf  Σ h(r−|v|) | V · Xω ⊇ Wc ∧ l(V ) ≥ n 

c	n→∞

≤ lim
m→∞

v∈V
h(r−|v|) = lim
m→∞
	



rlog h(r−|v|)

v∈V m	v∈V m

2 Here it is understood that − log ∞ = −∞

≤ lim
m→∞
vΣ∈V m
r−KA(v)−c ≤ r−c

Now if c > − logr Hh(F ) we have Hh(Wδ) ≤ r−c < Hh(F ), and, consequently,
Hh(F \ Wδ) > 0. Thus the set F \ Wδ is not empty, which in turn proves our
c	c
assertion.

Note that this result is not similar to Theorem 4.4. In contrast to prefix com- plexity the bound for a priori complexity is not valid for all constants. The constant depends on the ω–language and its Hh–measure. The following example states that the difference between a priori complexity and − log(h(r−|w|)) may not grow un- boundedly.
Example 4.7 Let X = {0, 1, 2} and F = (X · 0)ω. Then we have L 1 (F ) = 1.
2
Consequently there is a ξ ∈ F such that KA(ξ[n]) ≥a.e. 1 · n− c, for arbitrary c > 0.

On the other hand one can easily see that

'	1	''

(7)
KA(x10x20 ...xn 0) ≤ KA(x1x2 ...xn )+ c
≤ 2 · n + c .

Thus a similar result like the one in the second part of Theorem 4.5 for KA is not valid.
Now let V = {1, 2}∗ · 0 and F' = V · F . Then V is a prefix code and therefore (see [5])

1
L 1	2	1
2	2
v∈V
1
2
v∈V

and L 1 +ε(F') = 0, for any ε > 0. Since KA(v · ξ[n]) ≤ KA(ξ[n]) + cv all ω–words in F' have a linear upper a priori bound KA(ξ'[n]) ≤ 1 · n + cξ' . This shows that even for ω–languages of infinite measure the linear lower bound of Theorem 4.5 is, in general, not improvable.
Conclusion
We have seen that the known bounds of simple (conditional) and prefix complexity could be refined using a more general measure than the α–dimensional measure. From the result on a priori complexity one sees that the lower bound is valid for monotone complexity (see [4] or [9]), too. Moreover, it is known (see [9]) that the difference between a priori and monotone complexity is bounded by a slow growing recursive function. This leads to the conjecture that one cannot obtain better lower bounds for monotone complexity than for a priori complexity. Our last example states that in general the lower bound for a priori complexity cannot be improved.

References
Calude, C. S. , L. Staiger and S. A. Terwijn, On partial randomness, Annals of Pure and Applied Logic
138 (2006), 20–30.


Falconer, K. J. , “Fractal Geometry: Mathematical Foundations and Applications”, John Wiley & Sons, 1990.
Hausdorff, F., Dimension und ¨außeres Maß, Math. Annalen 79 (1919), 157–179.
Li, M. and P. Vit´anyi, “An Introduction to Kolmogorov Complexity and Its Applications”, 2nd Edition,
Springer Verlag, New York, 1997
Merzenich W. and L. Staiger, Fractals, Dimension, and Formal Languages, RAIRO Inf. th´eor. Appl.
28 (1994), 3–4, 361–386.
Reimann, J. , “Computability and Fractal Dimension”, Ph. D. thesis, Ruprecht–Karls–University, Heidelberg, 2004.
Roger, C. A. , “Hausdorff Measures”, Cambridge University Press, 1970.
Staiger, L. , Kolmogorov complexity and Hausdorff dimension, Inform. and Comp. 103 (1993), 2, 159–
194.
Uspensky, V. A. and A. Shen, Relations Between Varieties of Kolmogorov Complexities, Math. Systems Theory 29 (1996), 271–292.
Zvonkin, A. K. and L. A. Levin, The Complexity of finite objects and the Development of the Concepts of Information and Randomness by means of the Theory of Algorithms, Russian Math. Surveys 25 (1970), 6, 83–124.
