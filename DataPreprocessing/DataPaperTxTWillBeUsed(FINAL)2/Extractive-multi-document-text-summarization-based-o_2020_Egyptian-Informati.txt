Egyptian Informatics Journal 21 (2020) 145–157











Extractive multi-document text summarization based on graph independent sets
Taner Uçkan a,⇑, Ali Karcı b
a Van Yüzüncü Yıl University, Computer Programming Department, 65000 Van, Turkey
b I_nönü University, Department of Computer Engineering, 44000 Malatya, Turkey



a r t i c l e  i n f o 


Article history:
Received 3 November 2019
Revised 6 December 2019
Accepted 22 December 2019
Available online 3 January 2020


Keywords:
Graph independent set
Graph-based document summarization Generic document summarization Extractive text summarization
Multi document text summarization
a b s t r a c t 

We propose a novel methodology for extractive, generic summarization of text documents. The Maximum Independent Set, which has not been used previously in any summarization study, has been utilized within the context of this study. In addition, a text processing tool, which we named KUSH, is suggested in order to preserve the semantic cohesion between sentences in the representation stage of introductory texts. Our anticipation was that the set of sentences corresponding to the nodes in the inde- pendent set should be excluded from the summary. Based on this anticipation, the nodes forming the Independent Set on the graphs are identified and removed from the graph. Thus, prior to quantification of the effect of the nodes on the global graph, a limitation is applied on the documents to be summarized. This limitation prevents repetition of word groups to be included in the summary. Performance of the proposed approach on the Document Understanding Conference (DUC-2002 and DUC-2004) datasets was calculated using ROUGE evaluation metrics. The developed model achieved a 0.38072 ROUGE perfor- mance value for 100-word summaries, 0.51954 for 200-word summaries, and 0.59208 for 400-word summaries. The values reported throughout the experimental processes of the study reveal the contribu- tion of this innovative method.
© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelli- gence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

With the rapid development of Internet technology, the volume of electronic documents on the Internet have increased with extraordinary speed. In today’s era of rapid data growth, people can acquire and share information instantaneously from various sources. The Internet already provides access to billions of docu- ments. As this increases every second, an exponential growth of data has been seen over a short period of time. Search engines are able to list the most related documents or web pages by utiliz- ing several user inputs. However, even the most developed search engines lack the capacity to synthesize the information they

* Corresponding author.
E-mail addresses: taneruckan@yyu.edu.tr (T. Uçkan), ali.karci@inonu.edu.tr (A. Karcı).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.

retrieve. Therefore, this rampant growth of information raises the problem of information management, necessitating tools that can process data as a means of coping with such problems [1]. Despite the latest progress seen in document summarization, the problem has not yet been fully resolved. Automated document summariza- tion aims to find methods to detect the most important informa- tion in text, and to subsequently condense it for ease of use by readers [2]. Moreover, it is necessary to obtain certain features by processing data for the purpose of shortening the time spent accessing information [3]. These issues have increased interest in the field of automated text summarization systems [1].
This abundance of data availability undoubtedly enhances human life, but at the same time makes the quick access of accurate information increasingly difficult [4]. Automated text summariza- tion technologies are therefore increasingly studied by researchers so as to achieve greater efficiencies through enhanced or new meth- ods [5]. However, despite all of the studies conducted in the field of document summarization, the need for improvement and innova- tion has not diminished [2]. Automated document summarization is a significant subtopic of natural language processing (NLP), which has the objective to present long text documents in a com- pressed and comprehensible form [6]. Document summarization


https://doi.org/10.1016/j.eij.2019.12.002
1110-8665/© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



techniques can generally be divided into two categories: extractive and abstractive. Extractive document summarization consists of three stages: representation of texts, sentence scoring, and sen- tence selection. Abstractive document summaries interpret the main content of documents by using natural language generation approaches, and then re-expresses it to form a summary [7,8]. They can also be categorized as single-document and multi-document, depending on the number of documents to be summarized. The first studies were conducted as single-document summarization. Multi- document summarization studies have started to be performed, and methods have been developed for application to more than one textual document [6,9,10]. Summaries can also be categorized as either generic or query-focused [11]. The vast majority of studies conducted by researchers have been on the basis of generic summa- rization. In this type of summarization, a few limited assumptions about the purpose of forming the summary are made, and the gen- eral content is maintained while trying to cover as much informa- tion as possible. On the other hand, query-focused summarization is aimed at summarization based on queries determined by the user and information related to the subject in the text is returned [12– 14]. In the proposed framework, graph independent sets [15] for Automated Document Summarization are utilized within a unique approach using intensive Graph Theory techniques.
As a stage of the proposed summarization system, a new text processing tool called KUSH (named after its developers: Karcı, Uçkan, Seyyarer, and Hark) is used to ensure that the connections and relationships between the sentences are transferred to the rep- resentative graphs in the most accurate way. This innovative soft- ware tool has had a very positive effect on performance values in terms of transferring semantically distinguishable and more accu- rately measurable relationships between the sentences and the corresponding graph.
In the current study, the performance of the proposed docu- ment summarization method was tested using two publicly avail- able datasets; DUC-2002 and DUC-2004. The performance results are presented and compared with various existing methods. The innovative contributions of the proposed method can be summa- rized as follows:

Using the KUSH text processing tool, graphs are obtained by eliminating uncertainties regarding words and their meanings,
thus achieving maximum graph representation.
Two important concepts were combined to model an extractive text summarizer: Textual Graph and Maximum Independent
Sets.
Maximum Independent Set was determined on the graph of the texts obtained after the text developed processing tool.
In the next stage, the nodes forming the Independent Set were removed from the representative graphs. In this way, it was pos-
sible to determine the main ideas and concepts that should be included in the summary of the documents to be summarized.
No previous document summarization study has utilized Maxi-
mum Independent Sets. However, the current study found that
it provides a quite robust and simple framework as a result of summarizing the texts based on a mathematical approach.

The remainder of the study is organized as follows: In Section 3, information is provided on the general stages of the proposed sum- marization method, Textual Graph, Proposed Text Preprocessing Tool, and graph independent sets. In Section 4, information is given on the dataset and evaluation metrics used, the experimental results of the proposed method for summarizing the texts are presented, and the proposed model is compared with the state- of-the-art methods. Finally, in Section 5, the experimental results are discussed and interpreted.
Related work

Scoring of phrases or sentences and obtaining summaries is the most common method used in automated extractive summariza- tion. Sentence scoring is adopted in the majority of methods applied today. Scoring methods are classified as word scoring, sen- tence scoring, and graph scoring [16]. In the word scoring methods, a scoring is made considering the importance of the sentences con- taining the frequency of a word in the text [17–19], with words such as proper nouns, places and objects that are considered as a determinant being scored higher [20,7]. In the text scoring meth- ods, the formal properties of the words (emboldened, italicized, underlined) are taken into account [21]. In addition, sentences starting with phrases such as ‘‘Briefly,” ‘‘Finally” and ‘‘As a result” in the text are defined as sign phrases, and the sentences following these statements are noted as being important sentences [18]. Sim- ilarly, evaluation is based on the title of the text to be summarized. Sentences that contain words found in the title are considered to be added to the summary, and their importance levels are increased accordingly [22]. Sentence scoring methods also take into account the dimensions of sentences, attaching greater impor- tance to sentences of a larger size [21,23]. Points are assigned to sentences by determining the position of the sentence and whether or not it involves numerical values [18,20,24].
The authors of Ref. [25] described the design and evaluation of extractive summarization approach as a way to assist learners with reading difficulties. Graph-based representations are frequently used in text analysis methods as they provide very effective solu- tions. In Ref. [9], the authors proposed TextRank, which contains graph-based representation for summarization using the intersec- tions of the text contents. Similarly, LexRank was introduced in Ref.
[13] utilizing an eigenvector centrality-based algorithm, one of the node centrality methods. Both the TexRank and LexRank algo- rithms were inspired by the PageRank [26] algorithm, a document summarization framework presented to obtain central sentences in a document using mutual information between term and sen- tence sets [27].
In Ref. [28], a multi-layered representation of documents, sen- tences and words was employed. The authors of Ref. [29] described documents with graphs in their study by utilizing link generation for automated document summarization. They defined the struc- ture by revealing the text relationships in the documents, and eval- uating the summaries by comparing them with those created by human hand. In Ref. [30], a graph-based approach was presented in order to provide semantic continuity, with nodes corresponding to the terms of the documents and the edges reflecting semantic relationships between these nodes. Basically, a graph diameter cal- culation is performed for all the nodes in the graph, and the short- est and longest paths are described as the weakest and strongest bonds. While graph structures and documents were defined in Ref. [31], nodes and edges were created based on local similarities. Random Walk has been used to obtain summaries of the main documents. In Ref. [32], a summarization system for the biomedi- cal field was proposed. Using a system called Unified Medical Lan- guage, a graph was obtained based on concepts and relationships with a semi-dictionary-based application, and then the PageRank algorithm was applied. In Ref. [12], the authors proposed a new
graph based on reinforced random walk.
Although most researchers have focused on extractive text summarization, some have worked effectively on abstractive sum- marizing. In Ref. [33], abstractive multi-sentence summarization was performed using the RNN structure. Similarly, Refs. [34,35] were also works in the field of abstractive summarization.
In the current study, we introduce an innovative and extremely simple  text  summarization  system  by  taking  graph-based



automated document summarization studies one step further. The aforementioned examples from the literature are presented in Table 1 as a historical view of this research field.
As can be seen from the literature, there have been a number of graph-based studies. The differences in terms of the proposed study begin primarily at the preprocessing phase. Using the pre- processing tool developed for this study, paragraphs are prepro- cessed in order that relations between the sentences are defined to the maximum level. The other and most significant difference of the proposed method is the process of finding independent clus- ters by analyzing the nodes in the graphs. When determining the maximum independent set clusters, the degree of independence of the nodes in the graphs are taken into account. In extractive summarization studies, the aspiration is that the sentences to be selected should have maximum coverage value while determining the sentences to be included in the summary. In other words, it is expected that the rate of carrying the main idea of a sentence to be selected will be high. This requires that the sentence has a higher number of words compared to the other sentences, i.e., it is not an independent sentence. By using maximum independent sets, independent nodes are found and sentences far from the main idea of a paragraph are selected, thereby increasing the likelihood of sentences being selected for the summary.
Today, the problem of finding maximum independent sets still remains as a NP-hard problem. The process of finding maximum independent sets has not yet reached an optimal result, and can be seen in the current study as a weakness. New methods to arrive at a solution for this problem will be pursued meticulously.


Proposed summarization method

A block diagram of the stages of the Document Summarization framework proposed for text summarization is shown in Fig. 1.
In this study, a graph-based generic, extractive and multi- document summarization method is presented to extract appropri- ate summaries from the given texts. The proposed document summarization method consists of three main stages. In the first stage, non-discriminatory stop words (e.g., pronouns, prepositions, conjunctions) were removed from the dataset. A number of prepro- cesses are performed by the developed KUSH text preprocessing tool. When forming graphs of close-meaning words that differ from each other in terms of their spelling but are semantically
derived from a common word root, they are prevented from being processed as if different words. In the second stage, word common- alities between the phrases are represented mathematically and graphically. In addition, this stage includes determination of the nodes forming the Maximum Independent Set and the removal of the sentences corresponding to these nodes from the main graph. The final stage is the weighting of the sentences that make up the documents using the eigenvector node centrality method and the selection of important sentences. At this stage of the pro- posed method, the Top N phrases are combined and 200- and 400- word summaries created separately. The success of the framework was then tested in detail using a number of different ROUGE per- formance metrics.


Text preprocessing and KUSH

Most dense datasets are not configured, with few of the relevant datasets being of a structured format. Structured datasets are those that can be expressed in rows and columns of a table or utilize a number of tags. In the current study, it was possible to provide a certain structure to the data being studied by disregarding com- plex and time-consuming processes. Some level of preprocessing is necessary for datasets that do not have a specific integrity but are required to be structured. It is predicted that converting texts that will be used as input to the system into workable formats and isolating them from unnecessary data that are not discrimina- tory will increase the performance of the system because dense clusters are formed in natural language [3].
In text summarization processes, the data to be studied requires some level of preprocessing. Stop words (e.g., pronouns, preposi- tions, conjunctions) are expressions that have no distinguishing features and should be removed from the dataset prior to attempt- ing summarization. For this reason, non-representative words in the sentences are removed from the original datasets in order to perform text summarization by transferring the data to the required format so that it can be studied. Thus, the processing load is reduced and continues with words having categorical meaning. In the DUC-2002 and DUC-2004 datasets used in the scope of the current study, the text to be expressed with graphs are stored in files with the .txt extension. In this study, the normalization steps, which refers to discarding stop words, spaces, and unwanted char- acters, was carried out using Python language library and tags.







Fig. 1. Schematic outline of proposed document summarization model.



These normalization steps constitute the first part of the data pre- processing and preparation stage.
In addition, when forming graphs of close-meaning words that differ from each other in terms of their spelling but semantically derived from a common word root, they are processed as if they are different words, which makes it difficult to identify connec- tions and relationships between the sentences. It is predicted that this problem will significantly affect summarization performance, hence a text processing tool was developed within the framework of the proposed model. This software tool, which we named KUSH, was developed using C# on the .NET platform. The steps for the proposed KUSH preprocessing tool are given in Algorithm 1, and the pseudocode of the Best Alternative Search function is given in Algorithm 2.

Algorithm 1. Proposed KUSH algorithm

Input: {text} - texts in daily language
Output: {kush_text} - texts processed by KUSH algorithm
‘‘text read”;
sentences = [], words = [], alternative = [];
best_alternative = 0, kush_text = {}
sentences [] = text.Split(.);
for c to len(sentences) do
sentences [c] = clear(c);
end for
words[] = sentences.Split(.);
for k to boyut(words) do
alternative [k] = alternative_search(words [k], words);
for u to alternative do
	best_alternative = best_alternative_search (k, alternative);
end for
words [k] = best_alternative;
end for
for k to words do
kush_text+ = k+” ”;
end for
return kush_text;

Algorithm 2. Pseudocode of best_alternative_search algorithm

input: Word Vectors taken from Raw Text
Output: Upgraded word vector with best alternatives
n:Controls how many first characters of the two words to be compared must be equal
for i:0 to length(WordVector) step 1 do:
for j:0 to length(WordVector) step 1 do:
if i.th word contains j.vector
#It is checked whether the word in index j is in any subsection of the word in index i
if i.th sub n charter = j.th n character (default n = 2)
//At least n of the beginning characters of the two words to be compared must be the same.
//Thus, it is che;cked whether or not the words come from the same origin.
//Otherwise, a word can be found in the middle or end of another word and an incorrect change applied.
Replace i.th value with j.th value
end if
end if
end for j



Textual graph

Many different methods have been proposed to represent infor- mation [40,41]. In this study, weighted and non-directional graphs are used for information representation. Prior to creating the graphs, the data passes through a preprocessing and preparation phase. After data preprocessing and preparation, the proposed Document Summarization model creates graphs corresponding to the text in order to summarize the data in textual format. The operations performed in this section corresponds to the ‘‘Text Pro- cessing and Building of Textual Graph” stage as shown in Fig. 1. A simple text example is shown in Fig. 2 along with the correspond- ing graph created for that text. The Sentence-Word Graph is obtained when performing the conversion in which the nodes are represented by sentences and the edges by common word num- bers. The relationship levels of the sentence pairs are revealed by






Fig. 2. Textual diagram of document d061 in DUC-2002 dataset after conversion with KUSH tool.
calculating the number of word intersections of each sentence with all other sentences. In this way, we can express the sentences and the relationships between them graphically with high level of representation.
A node is added to the representative graph for each sentence in the texts. For the edges between the nodes, the edge weight was added by considering the number of intersecting words of the phrases in the text. The KUSH algorithm regulates the semantic accuracy of the relationship between sentences, interconnecting the nodes based on meaningful relationships. Thus, all the sen- tences were associated with each other in terms of their common content that covers all combinations, ensuring that the relation- ships between the sentences can be accurately transferred to the graph. The steps for the Textual graph are given in Algorithm 3.

Algorithm 3. Textual Graph algorithm

Input: Text taken from preprocessing stage
Result: A is the Adjacency matrix
	S_V = []#The input paragraph is separated to obtain sentence vectors(S_V).
	A = [length(S_V), Length(S_V)] # Adjacency matrix is a square matrix in sentence vector dimension.
for i:0 to length(V) step 1 do:
	wordvectori []; #The word vector of i.th sentence is obtained by using the space character between the words
for j:1 to length(V) step 1 do:
	wordvectorj [];#The word vector of j.th sentence is obtained by using the space character between the words
if count(wordvectori_i ∩ wordvectori_j > 0) #If the
number of common words between sentences i and j.th is
greater than zero, this value is assigned to the adjacency matrix as edge weight.
A[i,j] = count(wordvectori_i ∩ wordvectori_j)
Else : A[i,j] = 0 # otherwise, the adjacency matrix
is assigned a value of zero.
end if
end for j
Graph is created using the Adjacency matrix A
direct connection between the vertex values in the S set on the G graph. Similarly, a maximum independent set is not a suitable sub- set of another independent set on G graph. In other words, each vertex in an independent set S has at least one endpoint not in S, and each vertex not in S is adjacent to at least one vertex in S [42]. Erdös and Moser first raised the problem of determining the maximum number of maximum independent set values, n vertices and the graphs of these vertices [43]. After this problem was raised, various approaches have since been presented. In their study, Chan and Har-Peled presented Approximation Algorithms to find the
maximum weighted maximum independent set [44].
Description: A Graph G = (V ; E) is a set of vertices (V) together with a set of edges (E). It is thought that the complementary

E = {(i; j) R E; i; j ∈ V ; i–j} graph of this G graph G = V ; E meets the conditions. An independent set is a set of nodes in the graph.
There is no direct neighborhood between the nodes in this set. If
a S ⊂ V subset is an independent set, 6i; j_ ∈ S and the edges of these nodes should meet the conditions of (i; j) R E. A maximum indepen- dent set is an independent maximum cardinality set. Finding a
maximum independent set is equivalent to finding a maximum cli- que or minimum vertex cover, and these three problems are clas- sified as NP-hard problems [45].
Although Maximum Independent Set (MIS) has been used in many fields in graph theory, NP-Hard remains a current problem. An independent set is a set of nodes in a graph, such that no two vertices in the set are connected by an edge. That is, each edge in the undirected graph has at most one endpoint. There may be more than one maximal set in a graph. Adding another node to the max- imal clusters disrupts the independent set’s property as it would require that the set contains an edge. All maximal independent sets in a graph are determined and the maximal sets with the most nodes among the specified maximal independent sets are deter- mined as the maximum independent set. For instance, in Fig. 3,
{n2, n3} and {n2, n3, n5} are a maximal independent set, while
{n2, n3, n5, n8} is the maximum independent set, and four is the independence number of the graph.
The current study aims to remove sentences which are not to be summarized, and to reduce the number of sentences to be worked on prior to the sentence selection. Separation of unnecessary and meaningless sentences is performed by using graph independent set. The document, for which a summary is to be created, is first con- verted into a graph and then an independent set of this graph is determined. The nodes in the independent set are removed from the main document and the remaining sentences are summarized using the node-centered values. Fig. 4 explains each step of the process.
As can be seen in Fig. 4, the conversion of a small document to graphs is followed by finding the independent sets and then remov- ing them from the document. The number of sentences in the doc- ument and the number of nodes in the first graph created are assumed to be n. The number of independent set nodes obtained from the main graph is m; while the probability that the kth node
in the main sentence is present in the summary without any scoring is Pk = 1 , it is Pk =  1  after the independent sets are removed from the main sentence. Thus, before calculating node centralities, the probability of the presence of sentences, which are considered

		to be semantically strong in the paragraph, in the summary is
increased to a certain extent. In this way, when the Eigen Centrality

Graph independent set

Many problems in graph theory are based on the existence of sub-diagrams that correspond to certain constraints. One of the most emphasized problems is the maximum independent set of a graph. In a G Graph, the required independent vertex sets are indi- cated by S, which contains the vertex values of V(G). There is no
Values are obtained from the initial graph, the process is performed by considering more sentences and this increases the probability of the presence of sentences to be excluded from the summary. By removing the sentences in the independent set from the main doc- ument, the sentences, which have high eigenvector centrality val- ues but should be excluded from the summary, are determined and thus more accurate summaries are produced.


		
(a) Maximal independent set	(b) Maximal independent set	(c) Maximum independent set

Fig. 3. Illustration of difference between Maximal Independent Set and Maximum Independent Set.


Fig. 4. Finding and removing the independent set from a simple graph.


The proposed method can be briefly described as follows. G graph is obtained from the given text (T) file. A graph is
G = (V ; E); the maximum independent set of this graph is included in the method proposed in this study. S ⊂ V is this set; since the
The pseudocode of the Maximum Independent Set algorithm can be summarized as:

Algorithm 5. Maximum_Independent_Set Functions Algorithm

maximum independent set is S, a graph is defined as G2 = (V 2; E2).		
Maximum_Independent_Set(G)

T ? G
E2 ⊂ E and S ⊂ V.
E2 = {(u,v)|u ∈ V-S and v ∈ V-S} and V2 = V-S. G2 ? L Matrix (L is Laplacian Matrix of G2).
L ? eigenvector and eigenvalue
eigenvector and eigenvalue ? Summarization.

Finding the maximum independent set is a problem that is in need of a reliable approach, but a definitive and effective method has yet to be developed. Algorithm 4 was used in the current study to find the maximum independent set.
Algorithm 4. Maximum Independent Set Algorithm Input: Textual Graph (G) (Section 3.2)
Output: Maximum Independent Set (S)
V: Set of Vertices
E: Set of Edges
G ← (V,E)
S: Independent set of G and Initial value S = £
if 6v ∈ and degree(v) = 0 then result S = V.
Other cases;
u ∈ V select node.
n1 = Maximum_Independent_Set((G-{u})(See Alg. 5.)
n2 = Maximum_Independent_Set((G-{u}-{neighbors (u)})
S = maximum(n1,n2)
Return S



	Start with the set of vertices of the graph, V and an empty set for the MIS, S
While V–£:
Find a vertex of minimum degree v ∈ V
Add it to S
Remove it and its neighbors from V
End While
Return S



T.GISEC method: proposed document summarization algorithm

The proposed method obtains the Graph from the Text. The independent set is obtained from the graph and, after the indepen- dent set is removed, the eigenvalue is obtained and summarized with eigenvalue centrality. In the proposed summarization approach, based on the assumption that the sentences that should be excluded from the summary will be represented as Independent Set, the mentioned nodes are determined and removed from the representative graph. Thus, in the experimental process leading to the summary, a method which has not previously been used in any document summarization study is presented. In order to find effective nodes, ineffective nodes were first identified. As shown in Algorithm 6, the remaining nodes after the ineffective nodes have been removed, eigenvector node centrality is weighted with [5] metric. For the obtained Top N node scores, 200- and 400- word summaries were obtained.



Algorithm 6. Proposed Document Summarization algorithm

Input: Processed Text From Kush Preprocessing Step (see Section 3.1)
Output: Summary
V: Set of Vertices
E: Set of Edges
G ← (V,E)
I ← Find All Independent Set of G Graphs
for each Ii in I
	Remove ith sentences from source document (see Algorithm 2)
End For
Calculate New Vertex and New Edges
Create New Textual Graph Gnew ← (Vnew, Enew) (see Section 3.2)
Calculate Node Centrality
Sort all Node Centrality Values of Gnew by Descending
Create summary of (100, 200 and 400 Words), beginning from the highest Eigen Centrality Value
Return Summary



Experimental results
Table 2
Characteristics of the datasets.

Description	DUC-2002	DUC-2004
Number of clusters	59	50
Number of documents in each cluster	~10	10
Number of documents	567	500
Summary length	200 and 400 words	665 bytes
are evaluated on the basis of n-gram, word sequences. It is a metric based on the intersections between summaries created by summa- rization systems and ideal summaries created by human hand. In order to evaluate the performance of the proposed framework, the ROUGE assessment toolkit was utilized, which is based on n-gram statistics and highly correlated to human evaluation. The scores generated by the ROUGE metrics range from 0 to 1.
The higher the score, the more shared content is available with the model summary; and for the proposed method, the automated summary was considered to be better and more informative. In Refs. [47,48], Lin revealed a high correlation between ROUGE scores and the scores given by individuals. In the current study, we used ROUGE-N (N-1, N-2), ROUGE L, ROUGE-W-1.2 and
ROUGE-SU metrics to evaluate the performance of the proposed Document Summarization method.
ROUGE-N evaluates the number of n-grams common to the pro- posed summary and the model summary.

Description of dataset
PC∈{ReferenceSummaries}gramn∈S
P Countmatch(gramn)

In this study, two Document Understanding Conference data-
ROUGE — N =
PC∈{ReferenceSummaries}gramn ∈S
P Count(gram )	(1)

sets (DUC-2002 and DUC-2004) were used to test the accuracy of the proposed method. The DUC-2002 dataset contains documents for both abstractive and extractive summarization; however, the extractive summarization files were used based on the summariza- tion method proposed in the current study. NIST produced 60 ref- erence sets, with each containing documents, single-document summaries, and multi-document abstracts/extracts, and defined criteria such as event sets and biographical sets.
For the DUC-2002 dataset, three tasks were defined. Task 1, fully automated summarization of multiple newswire/newspaper documents (articles) on a single subject, with 60 sets of approxi- mately 10 documents each provided as system input. Task 2, auto- mated summarization of multiple newswire/newspaper documents (articles) on a single subject. Task 3, one or two pilot
where n equals the length of gramn n-gram, and Countmatch(gramn ) is the maximum number of n-grams intersecting in the potential sum-
mary and reference summary. As can clearly be seen in Eq. (1), Rouge-N is a measurement associated with recall. This is because the denominator of the equation is the sum of the number of n- grams in the reference summary. For example, (N-1) measures the number of uni-grams shared between two summaries. Similarly (N-2) focuses on the number of bi-grams shared between the pro- posed summary and the model summary. Similarly, it calculates the longest common word sequence in the two sub-word sequences given in the ROUGE-L value. X and Y are two given word sequences. Calculations of the ROUGE-L value of the sequences are made in Eqs. (2)–(4).

projects with extrinsic evaluation.
For the DUC-2004 dataset, five tasks were defined. Task 1, to create very short summaries with a maximum length of 75 charac- ters for 500 newswire/newspaper documents (articles); these sum- maries can be construed as headlines, although participants were allowed to use any format (including keyword lists). Task 2, to pro-
duce summaries with a maximum length of 665 characters for 50
Rlcs = LCS(X; Y)
m
Plcs = LCS(X; Y)
n

 1 + b2 Rlcs Plcs
(2)

(3)

clusters of 10 documents each; these summaries were general and not focused on any particular aspect of the documents. Task 3, sim-
Flcs =
Rlcs
+ b2P

lcs
(4)

ilar to Task 1, except that the document set consisted of 24 clusters of 10 documents each. Task 4, same document set as Task 3, except with general multi-document summaries for each of the 24 clus- ters. Task 5, similar to Task 2, with summaries for 50 clusters of 10 documents. In this study we selected Task 2 for 665 bytes sum- maries [46]. Characteristics of the two datasets are presented in Table 2.

Evaluation metrics

Recall-Oriented Understudy for Gisting Evaluation (ROUGE) performance metrics are the most popular assessment metrics used in summarization systems. ROUGE is a performance metric in which summaries are automatically evaluated. These metrics
ROUGE-W-1.2 calculates the longest consecutive matches between the proposed summary and the model summary. ROUGE-SU4 measures the number of skip-bigrams common to the two summaries.

Experimental works

All experimental processes were performed using a computer with an Intel Core i7-7700 CPU 3.60 GHz and 16 Gb memory using
.net and Python.
In the DUC-2002 dataset, the texts are stored in files with the . txt extension. The normalization steps, meaning the removal of non-discriminatory words, expressions, spaces and undesired characters called stop words, were performed using the Python



language library and tags. Thus, the processing load was reduced and the Document Summarization process was conducted with words having categorical meaning.
After the preprocessing stage, the developed software tool called KUSH was used to provide the most accurate transfer of rela- tionships between word phrases and textual graphs. Owing to its simple and efficient algorithm, the software assigned substitutes, chosen from the text to be summarized, for the changed words. As a result, graphs with high levels of representation were obtained.
Maximum Independent Set, which has not previously been used in any summarization study, was utilized for the current study. Based on the assumption that sentences corresponding to the nodes in the independent set should be excluded from the sum- mary, the nodes forming the Independent Sets on the graphs were determined and removed from the graph. Thus, prior to the effect of the nodes on the global graph being quantified, a limitation was applied to the summary. This limitation prevented the repetition of word groups being included in the summary, thereby resulting in more comprehensive summaries being generated. In addition, the experimental processes affected the approach for removing the Independent Sets, which has been used for the first time in a sum- marization study, as an encouraging step to include sentences with minimum words that intersect with each other in the main text. The values reported throughout the experimental processes of the study clearly demonstrate the contributions of this innovative method.
In the final stage, the centrality values of the obtained nodes were calculated with Node centrality calculations. Eigenvector Centrality was used to weight the nodes forming the graphs in the study. In order to thoroughly evaluate the performance of the proposed summarization system, the experimental process was conducted separately for summaries consisting of 200 and 400 words. The performance of the proposed Document Summariza- tion framework was evaluated using the ROUGE performance met- rics, as shown in Table 3.
The metrics used were Rouge-1, Rouge-2, Rouge-3, Rouge-4, Rouge-L, Rouge-W-1-2, Rouge-S*, and Rouge-SU*. In addition, each metric type has been reported separately with a focus on Recall, Precision, and F-Score. For summaries of 200 and 400 words, the values have been presented separately. The first column of Table 3 shows the Summarization performance metric types, while the other columns show performance evaluation scores with an aver- age reported by the recommended framework for 200- and 400- word summaries.

Comparison with state-of-the-art methods

The Summarization performance results obtained with the proposed Document Summarization framework were compared with previously published studies. As shown in Tables 4–6, 100-,
200- and 400-word summaries were compared with seven differ- ent competitive methods in order to show consistency with previ- ous summarization approaches.
Luhn [17] used statistical information obtained from word fre- quencies and distributions to calculate the significance of sen- tences with machine learning techniques, forming summaries according to the highest sentence scores obtained. Landauer et al. [49,50] presented a new approach using only general mathemati- cal methods, without utilizing any knowledge pre-acceptance. By transferring the connected structure of a text to graphs, Mihalcea realized an iterative, extractive and unsupervised application called TextRank [51,52] that scores units based on the significance of text units. An important feature of the TextRank system, which is an iterative study based on Google’s PageRank [26], is that there is no predefined and manually configured structure. In Ref. [13], Erkan et al. proposed a stochastic and graph-based approach called LexRank to calculate the significance of textual units within NLP. In their approach, they calculated the significance of sentences on the representative graph based on the eigenvector centrality (node centrality-based) measure. In their experimental work, the authors showed that LexRank performed better in most cases than both degree-based methods and centroid-based methods. SumBasic [53], a fundamental method frequently used in the literature, selects the top N sentence to be included in the summary using an effective probability function. In Ref. [54], the authors added sentences to the summary with a specified probability model, in a greedy way for multi-document summarization.
The reason for choosing the aforementioned methods is that they use mathematical, statistical or graph-based theories, and are thereby similar to the summarization method presented in the current study. In addition, all of the selected methods are unsu- pervised document Summarization methods, which is also the same as in the proposed framework.
Table 4 presents the ROUGE performance metric values of the 200-word summaries reported by Random, Luhn, LSA, TextRank, LexRank, SumBasic, KL-Sum and the document summary method proposed in the current study on the DUC-2002 dataset. The high- est values for each row in the table are highlighted in bold font. The proposed method is shown to have outperformed all of the com- petitive methods, without exception. Table 4 clearly demonstrates that the proposed approach produced excellent results when com- pared to the competitive methods in terms of 200-word summaries.
Fig. 5 displays the ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-W-
1.2, and ROUGE-SU4 average scores for all eight approaches, including the proposed method. All of the comparisons in the graph are summarized separately for the 200-word summaries based on the Recall, Precision and F-Score values. The proposed method, based on a Rouge-1 and F-Score assessment, reported a 25% higher value than Random, 8% more than Luhn, 33% more than LSA, 8% more than TextRank, 5% more than LexRank, 9% more than




Table 3
ROUGE metric scores for the proposed document summarization method.

ROUGE evaluation methods	Average

Summary for 200-words	Summary for 400-words


Table 4
Comparison with state-of-the-art methods of proposed document Summarization (200-word summaries).


ROUGE evaluation methods

Summary for 200-words (average) (DUC-2002) Methods















Rouge-1	Rouge-2











Rouge-L	Rouge-W-1.2



















Fig. 5. Graphical comparison of proposed method with state-of-the-art methods (200-word summaries).


SumBasic, and 34% more than KL-Summ summarization methods. Similar evaluation rates were obtained for all other ROUGE perfor- mance measurement methods, as can be seen in the relevant tables and figures that follow. This confirms that the proposed document summarization approach can be considered as a rather effective and efficient summarization system.
To emphasize the applicability of the proposed method, the experimental processes were repeated for 400-word summaries; again, by examining in detail all the competitive methods compared to the performance of the document summarization approach proposed in the current study. The results of this
comparison are presented in Table 5, with the highest values in each row highlighted in bold font. The proposed method, based on a Rouge-1 and F-Score assessment, reported a 12% higher value than Random, 0.5% more than Luhn, 24% more than LSA, 5% more than TextRank, 7% more than LexRank, 12% more than SumBasic, and 33% more than KL-Summ summarization methods. It is clear, there- fore, that many of the competitive methods have been left behind by the proposed method on the basis of 400-word summaries.
Fig. 6 illustrates the ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-W- 1.2, and ROUGE-SU4 average scores for all eight approaches, including the proposed method. All of the comparisons in the


Table 5
Comparison with state-of-the-art methods of proposed document Summarization (400 word summaries).


ROUGE evaluation methods

Summary for 400-words (DUC-2002) Methods















Rouge-1	Rouge-2











Rouge-L	Rouge-W-1.2


















Fig. 6. Graphical comparison of proposed method the state-of-the-art methods (400-word summaries).


graph were created separately for 400-word summaries based on Recall, Precision, and F-Score values.
In our experiments conducted with 100-word summaries from the DUC-2004 dataset, it can be seen in Table 6 that the proposed algorithm surpassed other competitive methods for all perfor- mance criterions based on ROUGE-2 values. Best values were also reported for the ROUGE-1, ROUGE-L, ROUGE-SU, and the ROUGE- W-1.2 precision metric types. The highest values for each row in the tables are highlighted in bold font. Also, Fig. 7 displays the ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-W-1.2, and ROUGE-SU4
average scores for all eight systems, including the proposed method. All of the comparisons in the graph were created
separately for the 100-word summaries based on the Recall, Preci- sion, and F-Score values.
The experimental study was examined on the basis of 100-, 200- and 400-word summaries on the DUC-2002 and DUC-2004 datasets for all approaches. The selected state-of-the-art methods and the proposed Document Summarization framework are similar as in they are all mathematical-statistical and graph-based approaches. When Tables 4 and 5 are examined, the proposed approach outperformed all of the competitive methods and reported excellent results on the basis of 200-word summaries. The proposed method also offers the best ROUGE values alongside Luhn on the basis of 400-word summaries.


Table 6
Comparison with state-of-the-art methods of proposed document Summarization (100 word summaries).


ROUGE evaluation method

Summary for 100-words (DUC-2004) Method















Rouge-1	Rouge-2












Rouge-L	Rouge-W-1.2





















Fig. 7. Graphical comparison of proposed method with state-of-the-art methods (100-word summaries).


The performance of the proposed system was tested in detail using different datasets and performance metrics. As has been reported in the presented tables and figures, the proposed method produced generally successful results. This outcome is partly related to the creation of rich textual charts using the proposed text process- ing software (named as KUSH). However, the primary reason for the success is the subtraction of nodes that make up the independent sets prior to the node-weighting phase. Thus, the selection of
appropriate sentences to be selected for summarization is more accurate as a result. This provides for graph-based preprocessing, allowing for work on simpler graphs, which in turn improves cover- age, which is an important feature of the rich summary.
The comparative results show that the unique and innovative Document Summarization framework based on the graph indepen- dent set produces superior results to the previous studies when tested against the DUC datasets.



Conclusion

In order to lessen the time necessary to access today’s ever- increasing vast capacity of information, data needs to be analyzed in order to obtain a number of properties. This challenging task has naturally increased the interest in automated summarization sys- tems. The current study therefore proposes a new, unsupervised, extractive, and generic text summarization system.
In the proposed approach, text irregularities brought about by spoken language have been eliminated using a software tool called KUSH that was developed in this study. The software tool prepares texts for summarizing using a unique algorithm, and has been shown to positively affect results in terms of revealing semantically distinguishable and measurable relationships between sentences.
Sentences represent the first finite set in representative graphs, while the number of common words represents the other finite set. In this way, texts are represented by nonoriented and weighted graphs.
As researchers, our anticipation was that the set of sentences cor- responding to the nodes in the independent set should be excluded from the summary. Based on this, the nodes forming the Indepen- dent Set on the graphs were identified and removed from the graph. Thus, prior to the quantification of the effect of the nodes on the glo- bal graph, a limitation was applied to the summary. Within the scope of the proposed Document Summarization process, the nodes forming Independent Sets were determined on the graphs repre- senting the texts to be summarized. A preliminary assessment was performed between the nodes prior to the quantitative information being obtained from the nodes forming the textual graph. With the proposed model, a summarization approach was established with an infrastructure created based on a mathematical model, which has not previously been seen in any existing summarization study. The effectiveness of the model has been demonstrated in terms of Recall, Precision, and F-Scores using the Rouge-1, Rouge-2, Rouge- 3, Rouge-4, Rouge-L, Rouge-W-1-2, Rouge-S*, and Rouge-SU* perfor- mance metrics. Experimental processes were also repeated for 100-, 200- and 400-word summaries. The proposed system’s success was verified against two datasets, with the following results obtained during the experimental processes.
With a 95% confidence interval, superior results were reported for the proposed summarization method over all existing state- of-the-art methods, without exception, in terms of 200-word sum- maries; and higher results obtained over most state-of-the-art methods in terms of 100- and 400-word summaries.
Prior to the weight of the nodes on the global graph being math- ematically sorted, a unique approach is performed, which has not been used in any other document summarization study. The approach isolates the nodes forming independent nodes from the main graph and then creates the summary. This limitation pre- vents the repetition of word groups from being included in the summary, and thereby generating much more comprehensive summaries. In addition, the experimental processes have been used as a preliminary assessment step which encourages the removal of the Independent Sets, and first used in a summarization study. So, in the summaries, the sentences with maximum rela- tions with each other are included.
The values reported throughout the experimental processes of the study demonstrate the contribution of this innovative method. The aforementioned stage of the presented method has been shown to be very effective and consistent, thanks to its mathemat- ical grounding. In addition, the proposed method depends on the correct formation of the graphs, which is still at an early stage of development. The method presented includes a robust fiction. However, the text requires a preprocessing step and a correct graph transformation, which affects the system’s performance
considerably. While maximum independent sets of charts are found, the size of the chart used is decisive on the system’s perfor- mance, and as such, where there are very large graphs, there may be issues related to insufficient memory. However, in the proposed method, no memory problems were experienced due to the dataset dimensions used during the experimental processes (average of 200 sentences). It is one of our goals to develop the proposed method by testing its success on different graph-building methods. In terms of the reported performance results, the proposed model is expected to  be quite significant and notable for
researchers.


Conflict of interest statement

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

References

Ge Yao J, Wan X, Xiao J. Recent advances in document summarization. Knowl Inf Syst 2017;53(2):297–336.
Ermakova L, Cossu JV, Mothe J. A survey on evaluation of summarization methods. Inf Process Manage 2019;56(5):1794–814.
Hark C, Seyyarer A, Uçkan T, Karci A. Doǧal dil i sleme yakla simlari ile yapisal olmayan dökümanlarin benzerliǧi. In: IDAP 2017 – international artificial intelligence and data processing symposium. p. 1–6.
Mao X, Yang H, Huang S, Liu Y, Li R. Extractive summarization using supervised and unsupervised learning. Expert Syst Appl 2019;133:173–81.
Hark C, Uçkan T, Seyyarer A, Karci Abubekir. Metin Özetleme I_çin Çizge Tabanlı
Bir Öneri. IDAP 2018 – international artificial intelligence and data processing symposium, 2018.
Joshi A, Fidalgo E, Alegre E, Fernández-Robles L. SummCoder: an unsupervised framework for extractive text summarization based on deep auto-encoders. Expert Syst Appl 2019;129:200–15.
Gambhir M, Gupta V. Recent automatic text summarization techniques: a survey. Artif Intell Rev 2017;47(1):1–66.
Tan J, Wan X, Xiao J. Abstractive document summarization with a graph-based attentional neural model. In: Proceedings of the 55th annual meeting of the association for computational linguistics. p. 1171–81.
Mihalcea R, Tarau P. A language independent algorithm for single and multiple document summarization. In: Proc. IJCNLP 2005, 2nd int. join conf. nat. lang. process.. p. 19–24.
Sarkar K, Saraf K, Ghosh A. Improving graph based multidocument text summarization using an enhanced sentence similarity measure. In: 2015 IEEE 2nd int. conf. recent trends inf. syst. ReTIS 2015 – proc.. p. 359–65.
Van Lierde H, Chow TWS. Query-oriented text summarization based on hypergraph transversals. Inf Process Manage 2019;56(4):1317–38.
Xiong S, Ji D. Query-focused multi-document summarization using hypergraph-based ranking. Inf Process Manage 2016;52(4):670–81.
Erkan G, Radev DR. Lexrank: graph-based lexical centrality as salience in text summarization. J Artif Intell Res 2004;22:457–79.
Kaynar O, Görmez Y, Is ık YE, Demirkoparan F. Comparison of graph based document summarization method. In: 2017 International conference on computer science and engineering (UBMK). p. 598–603.
Boppana R et al. Approximating maximum independent sets by excluding subgraphs. BIT 1992;32(February):180–96.
Ferreira R et al. Assessing sentence scoring techniques for extractive text summarization, 2013.
Luhn HP. The automatic creation of literature abstracts. IBM J Res Dev 1958;2 (2):159–65.
Shardan R, Kulkarni U. Implementation and evaluation of evolutionary connectionist approaches to automated text summarization, 2010.
Nasr Azadani M, Ghadiri N, Davoodijam E. Graph-based biomedical text summarization: an itemset mining and sentence clustering approach. J Biomed Inform 2018;84:42–58.
Student PG, Coe DM. A comparative study of Hindi text summarization techniques: genetic algorithm and neural network, 2015.
Gupta V. Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer; 2013. p. 717–27.
Gupta V, Lehal GS. A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2010;2(3):258–68.
Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ. Text summarization features selection method using pseudo genetic-based model. In: Proc. – 2012 int. conf. inf. retr. knowl. manag. CAMP’12. p. 193–7.
Fattah MA, Ren F. GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 2009;23(1):126–44.



Nandhini K, Balasundaram SR. Improving readability through extractive summarization for learners with reading difficulties. Egypt Inform J 2013;14 (3):195–204.
Brin S, Page L. The anatomy of a large-scale hypertextual web search engine. Comput Networks ISDN Syst 1998;30(1–7):107–17.
Parveen D, Ramsl H-M, Strube M. Topical coherence for graph-based extractive summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. p. 1949–54.
Canhasi E, Kononenko I. Weighted archetypal analysis of the multi-element graph for query-focused multi-document summarization. Expert Syst Appl 2014;41(2):535–43.
Salton G, Singhal A, Mitra M, Buckley C. Automatic text structuring and summarization. Inf Process Manage 1997;33(2):193–207.
Medelyan O. Computing Lexical Chains with Graph Clustering. In: Proceedings of the ACL 2007 student research workshop. p. 85–90.
Chen Y-N, Huang Y, Yeh C-F, Lee L-S. Spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms. In twelfth annual conference of the international speech communication association, 2011.
Plaza L, Stevenson M, Díaz A. Resolving ambiguity in biomedical text to improve summarization. Inf Process Manage 2012;48(4):755–66.
Nallapati R, Zhou B, dos Santos C, Gulçehre Ç, Xiang B. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In: CoNLL 2016 – 20th signll conf. comput. nat. lang. learn. proc. p. 280–90.
Moawad IF, Aref M. Semantic graph reduction approach for abstractive text summarization. In: Proc. – ICCES 2012 2012 int. conf. comput. eng. syst.. p. 132–8.
Linqing Liu HL, Yao Lu, Yang Min, Qu Qiang, Zhu Jia. Convolutional neural networks for sentence classification. In: EMNLP 2014 – 2014 conf. empir. methods nat. lang. process. proc. conf.. p. 1746–51.
Sinha R, Mihalcea R. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. In: ICSC 2007 international conference on semantic computing. p. 363–9.
Fattah MA, Ren F. GA, MR, FFNN, PNN and GMM based models for automatic text summarization, 2008.
Author C, Shardanand Prasad R, Kulkarni U. Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 2010;6(11):1366–76.
Gupta V. LNAI 8284 – Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents, 2013.
Quezada-Sarmiento PA, Macas-Romero J del C, Roman C, Martin JC. A body of knowledge representation model of ecotourism products in southeastern Ecuador. Heliyon 2018;4(12).
Quezada-Sarmiento PA, Enciso-Quispe LE, Jumbo-Flores LA, Hernandez W. Knowledge representation model for bodies of knowledge based on design patterns and hierarchical graphs. Comput Sci Eng 2018;PP(c):1.
Oh S. Maximal independent sets on a grid graph, 2017.
Jou MJ, Chang GJ. The number of maximum independent sets in graphs. Taiwan J Math 2000;4(4):685–95.
Chan TM, Har-Peled S. Approximation algorithms for maximum independent set of pseudo-disks, 2011.
Feo TA, Resende MGC, Smith SH. A greedy randomized adaptive search procedure for maximum independent set. Oper Res 2008;42(5): 860–78.
NIST. Document understanding conferences. NIST [Online]. Available: https:// www-nlpir.nist.gov/projects/duc/ [Accessed: 08-May-2019].
Lin CY. Rouge: a package for automatic evaluation of summaries. In: Proc. work. text Summ. branches out (WAS 2004). p. 25–6.
Lin C-Y, Hovy E. Automatic evaluation of summaries using N-gram co- occurrence statistics. In: Automatic evaluation of summaries using n-gram co- occurrence statistics. p. 150–7.
Landauer TK, Foltz PW, Laham D. An introduction to latent semantic analysis. Discourse Process 1998;25(2–3):259–84.
Landauer TK, Dutnais ST. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychol Rev 1997;104(2):211–40.
Mihalcea R. Language independent extractive summarization. In: Proc. ACL 2005 interact. poster demonstr. sess. – ACL ’05, no. June. p. 49–52.
Mihalcea R, Tarau P. TextRank: bringing order into texts. Proceedings of the ACL 2004 on interactive poster and demonstration sessions, 2004. pp. 20-es.
Vanderwende L, Suzuki H, Brockett C, Nenkova A. Beyond SumBasic: task- focused summarization with sentence simplification and lexical expansion. Inf Process Manage 2007;43(6):1606–18.
Haghighi A, Vanderwende L. Exploring content models for multi-document summarization 2009;(June),362.
