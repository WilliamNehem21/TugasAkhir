Journal Pre-proof

Hybrid distributed feature selection using PSO-MI

Khumukcham Robindro, Sanasam Surjalata Devi, Urikhimbam Boby Clinton, Linthoingambi Takhellambam, Yambem Ranjan Singh, Nazrul Hoque


PII:	S2666-7649(23)00046-2
DOI:	https://doi.org/10.1016/j.dsm.2023.10.003
Reference:	DSM 75

To appear in:	Data Science and Management

Received Date: 17 December 2022
Revised Date:	5 October 2023
Accepted Date: 7 October 2023


Please cite this article as: Robindro, K., Devi, S.S., Clinton, U.B., Takhellambam, L., Singh, Y.R., Hoque, N., Hybrid distributed feature selection using PSO-MI, Data Science and Management (2023), doi: https://doi.org/10.1016/j.dsm.2023.10.003.


This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published
in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.

¬© 2023 Xi‚Äôan Jiaotong University. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd.




Title: HDFS (PSO-MI): Hybrid Distributed Feature Selection using PSO-MI
Authors:
Khumukcham Robindro Assistant Professor
Department of Computer Science Manipur University
Imphal, Manipur-795003
rbkh@manipuruniv.ac.in

Sanasam Surjalata Devi Department of Computer Science Manipur University
Imphal, Manipur-795003 latasanasam@gmail.com

Urikhimbam Boby Clinton Department of Computer Science Manipur University
Imphal, Manipur-795003 bclintonwork@gmail.com
Linthoingambi Takhellambam Department of Computer Science Manipur University
Imphal, Manipur-795003
lin.takhel1@gmail.com
Yambem Ranjan Singh
Department of Computer Science Manipur University
Imphal, Manipur-795003
rnjnymbm@gmail.com
Dr. Nazrul Hoque Assistant Professor
Department of Computer Science Manipur University
Imphal, Manipur-795003 tonazrul@gmail.com


Hybrid Distributed Feature Selection using PSO-MI



A R T I C L E I N F O

Keywords: Feature Selection PSO
Classification Accuracy.
A B S T R A C T

Feature Selection (FS) is a data preprocessing step in Machine Learning (ML) that selects a subset of relevant and informative features from a large feature pool. FS helps ML models improve their predictive accuracy at lower computational costs. Moreover, FS can handle the model overfitting problem on a high-dimensional dataset. A major problem with the filter and wrapper FS methods is that they consume a significant amount of time during FS on high- dimensional datasets. The proposed ‚ÄúHDFS(PSO-MI): Hybrid Distribute Feature Selection using PSO-MI‚Äù is a Particle Swarm Optimization (PSO)-based hybrid method that can overcome the problem mentioned above. This method hybridizes the filter and wrapper techniques in a distributed manner. A new combiner is also introduced to merge the effective features selected from multiple data distributions. The effectiveness of the proposed HDFS(PSO-MI) method is evaluated using five ML classifiers, viz., Logistic Regression, k-NN, SVM, Decision Tree, and Random Forest, on various datasets in terms of Accuracy and Matthew‚Äôs Correlation Coefficient (MCC). From the experimental analysis, we observed that HDFS(PSO-MI) method yielded more than 98%, 95%, 92%, 90%, and 85% accuracy for the unbalanced, kidney disease, emotions, wafer manufacturing, and breast cancer datasets, respectively. Our method shows promising results comapred to other methods, such as Mutual Information, Gain Ratio, Spearman Correlation, ANOVA, Pearson Correlation, and an Ensemble Feature Selection with Ranking Method (EFSRank).



Introduction
Feature selection (FS) is an integral preprocessing step of ML that evaluates the original feature set to find the most informative and nonredundant features. The FS method evaluates features by using a selection criterion or objective function to select the most informative features (Li et al., 2017a). Owing to the tremendous increase in data volume and complexity, the FS is considered an important ML step for dimensionality reduction. The primary goal of FS is to obtain a subset of relevant and informative features that can enhance the performance of an ML model, reduce time and space complexity, and prevent model overfitting (Venkatesh and Anuradha, 2019). Based on their selection mechanism, FS methods can be classified into four types: filters, wrappers, embeddings, and hybrids (Hoque et al., 2014). The filter method uses a feature evaluation measure to determine the effectiveness of each feature. It computes a score and ranks each feature based on the calculated score. Most filter-based FS methods employ a greedy approach to iteratively determine the best features. To evaluate high-dimensional data, the filter method is considered cost-effective in terms of time. In the wrapper, an explicitly used ML method functions as an evaluator to validate all possible subsets of features generated from the original feature group. The wrapper method uses an exhaustive search to assess all possible subsets of features using a learning algorithm and selects the subset that gives the best performance. Because the wrapper method uses an exhaustive search to evaluate all possible subsets of the original feature group, its computational time is very long compared to that of the filter method. However, wrapper methods performed better than filter methods. An embedded method considers FS as an integral part of the training process. Features are selected when training a learning model. The hybrid method combines any of the three previously mentioned methods. Hybrid methods are widely used in many ML applications because of their excellent discriminative behavior in pattern recognition and data analysis. In this paper, we discuss our proposed FS method called the ‚ÄúHybrid Distributed Feature Selection using PSO-MI‚Äù abbreviated as HDFS(PSO-MI), to select the most informative features from a large feature pool.
Feature selection plays a significant role in selecting informative, relevant, and non-redundant features from a large feature space during data preprocessing. After a comprehensive study of existing feature selection techniques, we observed that many wrapper-based FS methods ignore feature redundancy, and filter-based methods select features with some redundancy among the selected features. Moreover, the computational cost of wrapper techniques is high, and the selection of an optimal subset of features from a high-dimensional dataset is a major problem for ML researchers. Among existing FS methods, incorporating a single objective function into the standalone FS method does not yield satisfactory results for high-dimensional data. To overcome these problems, we developed a hybrid feature-selection

ORCID(s):

method combining the concepts of PSO optimization as well as Mutual Information (MI). The PSO optimization is applied to a distributed dataset to select optimal features from each distribution and then combine the solutions of all distributions using MI, which yields the best subset of features. The proposed hybrid model integrates both the filter and wrapper methods. As the wrapper method consumes a significant amount of time, we applied it in a distributed manner. The wrapper methods are executed in parallel with multiple objective functions during the feature subset evaluation. This step significantly reduces the computational cost. Moreover, for each partition of the original data, the proposed method employs a filter FS using MI and combines the features from each partition to obtain the best set of optimal features. The main advantage of the proposed hybrid model is that it reduces the computational cost of feature selection by the parallel execution of the method on multiple cores.
For a given dataset D with n numbers of features, an FS method needs to select a subset of features, say ùëö, such that The major problems of FS selection in high-dimensional datasets are the computational cost and performance.
ùëö << ùëõ. The subset of ùëö features should yield better performance with reduced computational cost during prediction.
This method should work on distributed datasets to select the best feature subset using PSO and MI. The main contributions of this study are as follows:
We developed an effective hybrid feature selection method called HDFS(PSO-MI) using PSO-MI.
A new objective function is defined to help the PSO optimization algorithm.
A new combiner method is proposed to combine the subset of features selected from each dataset distribution.
The proposed hybrid feature selection method is evaluated on high-dimensional datasets.
A parallel programming is applied to execute the method on multiple cores.
The effectiveness of the HDFS(PSO-MI) is compared with some existing feature selection methods.
The remainder of this paper is organized as follows. Related studies and existing FS methods are discussed in section 2. The PSO algorithm is described in section 3. The proposed HDFS(PSO-MI) method and the corresponding algorithm with a working example are described in Section 4. Experimental results and analysis are presented in Section
5. Finally, Section 6.
Related Work
In the literature, we found several articles on FS (Baruah et al., 2020; Chandrashekar and Sahin, 2014; Kumar and Minz, 2014; Li et al., 2017b; Zhu et al., 2023). Most FS methods follow the filter approach and incorporate information-theoretic measures as feature evaluators during the feature selection (Hoque et al., 2016, 2018). In addition, evolutionary-based wrapper FS methods have shown excellent results for high-dimensional datasets (Moslehi and Haeri, 2020; Wang and Huang, 2009). To tackle the unmanageable challenges of computational costs in mining high-dimensional data, an effective FS method using PSO is developed by Simon et al. (Fong et al., 2015). In high- dimensional datasets, most FS methods have intractable computational demands because the size of the search space to find the best optimal subset is exponential. Therefore, the authors developed a lightweight FS method incorporating accelerated PSO, which yielded an enhanced performance with reduced computational cost. This method selects the most informative features on Big Data in an incremental manner, and the selected subsets of features are evaluated on multiple test case sets on Big Datasets. Researchers have developed variants of the PSO method for feature selection because evolutionary computation approaches have been found to be effective in exploring the confounding effects of feature interactions. A modified Binary PSO-based FS method was proposed by Sousa et al. (Vieira et al., 2013) and used for mortality prediction in patients with sepsis. The enhanced BPSO method optimizes the SVM kernel parameters to manage the premature convergence of the PSO. The BPSO-based FS method works as a wrapper method and evaluates feature subsets using an SVM classifier. The selected subset of features yielded a high accuracy in mortality prediction in patients with sepsis. The PSO technique is combined with other methods, such as genetic algorithms, rough-set theory, and information gain, to develop hybrid FS methods, as PSO-based hybrid methods show promising results for FS. An effective hybrid FS method was proposed by Li et al. (Li et al., 2021) that combines three techniques viz., GA-Kmeans, GA-PSO-Kmeans, and Harmony-Kmeans. This method was applied to enhance the accuracy of diabetes diagnosis applications, achieving an accuracy of 91.65 %. The subset of features selected from the diabetic

dataset was evaluated using a Metaheuristic Harmony search, and the performance was improved using the K-means clustering algorithm.
As reported in (Moradi and Gholampour, 2016), most existing PSO-based FS methods do not consider the correlation or feature-feature interactions during the search for the optimal features. As a result, the probability of selecting redundant features is high, which affects performance. To overcome this problem, Moradi et al. (Moradi and Gholampour, 2016) developed a novel hybrid PSO-FS method called HPSO-LS, which incorporates a new local search strategy. This method uses a correlation-based search strategy that guides the PSO to identify less-correlated features. This method selects less correlated features, known as dissimilar features, with a higher probability than more correlated features. An FS is an optimization problem that considers either single or multiple objectives for feature subset evaluation. If multiple objectives are used during feature selection, this method requires a significant amount of processing time. To address this issue, Bansal et al. (Bansal et al., 2022) developed a hybrid method with multiobjective optimization using PSO. This method, known as mRMR-PSO, removes redundant and irrelevant features from sign language data. Initially, the method applies a histogram of oriented gradient (HOG) technique for feature extraction and fits the SVM classifier into the PSO. We performed an experimental comparison of the proposed mRMR-PSO with the HOG (without FS), PSO, and mRMR in terms of accuracy and computation time.
Another challenging problem of FS is inadequate handling of very high-dimensional data with a smaller number of samples, and most ML methods face the model overfitting problem on those datasets. In such a situation, neither the filter nor wrapper approach can effectively select the best feature subset. In addition, feature subset instability is a common problem in small sample size data, which has not been properly addressed by many existing FS methods (Brahim and Limam, 2016). Therefore, Brahim et al. (Brahim and Limam, 2016) developed an effective hybrid FS method that incorporated a cooperative subset search technique for instance learning. They restructured the problem of a small sample size into a filter-based FS tool that can select only a few subsets of informative features. They used cancer datasets to establish the efficacy of their method and proved that their method selects the most stable feature subsets that provide high detection accuracy.
From the above discussion, it is very clear that FS is a generic data pre-processing step of ML that selects the most informative features useful for the respective learning models. Most FS methods employ a feature evaluator or objective function to assess the importance and relevance of features in an optimized manner. As discussed above, many FS methods have been developed to select features from diverse applications such as network data, gene expression, language, disease, and chemical data. We observed that hybrid FS methods are widely used on high- dimensional datasets owing to their calibration in searching for the best feature set with reduced computational cost. Moreover, evolution-based hybrid feature selection methods have gained popularity for providing the best performance by selecting a stable feature set. Considering all the benefits of the hybrid FS methods, we developed the proposed HDFS(PSO-MI) method, which works in a distributed manner with multiple fitness functions operated on multiple partitions of the original dataset.
Particle Swarm Optimization
Particle swarm optimization (PSO) is a popular optimization method developed by Kennedy and Eberhart in 1995 (Eberhart and Kennedy, 1995) that uses a common metaheuristics searching algorithm for optimization. The method was developed based on inspiration from the natural process of social behavior and the dynamic movement of animals. In PSO, solutions known as a group of random particles are initialized first, and the particles are updated iteratively to search for the optimal solution. The position vector and velocity are updated during the optimal search process. The position vector is known as ‚Äúpbest‚Äù or local best for the individual particle‚Äôs best solution (fitness) so far. The next parameter is ‚Äúgbest‚Äù which represents the best position achieved so far among all the particles. The best position is set as the current global position of the particles, called gbest. PSO uses 1 and 2 to update the positions and velocities of the particles, respectively.
ùë£(ùëò+1) = ùúîùë£ùëò + ùê∂1ùëü1(ùëùùëèùëíùë†ùë°ùëò ‚àí ùë•ùëò) + ùê∂2ùëü2(ùëîùëèùëíùë†ùë°ùëò ‚àí ùë•ùëò)	(1)

ùëñ	ùëñ
ùëñ	ùëñ
ùëñ	ùëñ



ùë•(ùëò+1) = ùë•ùëò + ùë£(ùëò+1)
(2)

ùëñ	ùëñ	ùëñ
The velocity and current position at the ùëòùë°‚Ñé iteration are denoted by ùë£ùëò and ùë•ùëò, respectively. PSO uses two random
ùëñ	ùëñ
variables, ùëü1, ùëü2 where ùê∂1, ùê∂2 are positive constants. The inertia weight, represented by ùúî, is used to balance the

Table 1
PSO parameters and objective setting on all datasets of experimental



Table 1. ùë£ùëöùëéùë• is the upper bound of the velocity in all dimensions, and controls the particle rush movement during the tradeoff between exploration and exploitation. The different parameters and their meaning used in PSO are listed in
search. PSO techniques have been successfully used in various applications such as data mining, design and modeling, prediction and forecasting, and networking. Although PSO was developed to solve unconstrained single-objective problems, people use PSO variants to solve constrained optimization problems. Our proposed method uses PSO to determine the relevant subset of features using three objective functions. The equations for the objective functions are given by Eqs. 3‚Äì5.
Objective Functions Used in PSO for Feature Selection
As previously mentioned, a single objective function may be biased in selecting the best subset of features from a distribution. Hence, we used multiple objective functions for an effective analysis of features in different data distributions. To define the objective function, we used the accuracy value obtained from the SVM classifier using the corresponding features the PSO swarm considers. This accuracy value plays a significant role in selecting the best subset of features, along with other parameters such as relevance (Rel), redundancy (red) of the features, and the ratio between the number of selected features and the total number of features. The major disadvantage of applying multiple objective functions in HDFS (PSO-MI) is that it takes an exponential time on large datasets to evaluate features with the three objective functions incorporated into the PSO.
ùëÇùëèùëóùëíùëêùë°ùëñùë£ùëí1(ùëúùëèùëó ) = ùõº √ó (1 ‚àí ùëéùëêùëê) + (1 ‚àí ùõº) ‚àó (1 ‚àí ùëÜùêπ )	(3)
ùëá ùêπ


ùëÇùëèùëóùëíùëêùë°ùëñùë£ùëí2(ùëúùëèùëó2) = ùëéùëêùëê √ó ùëÖùëíùëôùëéùë£ùëî
‚Äì (1 ‚àí ùëéùëêùëê) √ó ùëÖùëíùëë
ùëéùë£ùëî
√ó ùëÜùêπ
ùëá ùêπ
(4)



ùëÇùëèùëóùëíùëêùë°ùëñùë£ùëí3(ùëúùëèùëó ) = ùëéùëêùëê + (1 ‚àí ùëéùëêùëê) √ó ùëÜùêπ √ó (ùëÖùëíùëô
ùëá ùêπ
ùëéùë£ùëî
‚Äì ùëÖùëíùëëùëéùë£ùëî
)	(5)

Parameter Settings in PSO
The PSO algorithm uses several parameters during execution. In our experimental analysis, we initially set the parameters reported in (Shi and Eberhart, 1998) and executed the PSO method for our problem. Next, we experimentally tuned the parameters and selected those that fit the best. The swarm size is an important parameter that influences PSO in determining the optimal solution. If we set a large swarm size, the PSO complexity for finding the best solution from a large search space is high. We found a general heuristic for swarm size in the literature [10, 30]. Similarly, the number of iterations was set based on the specific problem. Significantly fewer iterations led to premature termination without obtaining the best solution, and a large number of iterations required computation time
to converge. The PSO learning factors ùê∂1 and ùê∂2 are set empirically, and incorrect values of ùê∂1 and ùê∂2 may exhibit
cyclic behavior in the PSO. The inertia weight ùúî should always be less than one to handle divergence or explosion.
The parameters and their corresponding values are shown in Table 1.

Complexity Analysis of PSO
of iterations, and acceleration coefficients. If the swarm size is ùúá, the number of iterations is ùëõ, and the dimension of The computational time of the PSO algorithm depends on multiple parameters, particularly the swarm size, number each particle is ùëë, then for ùëë dimension, the PSO takes O(ùëë √ó ùë£) and O(ùëë √ó ùëù) times to update the particle‚Äôs velocity
maximum number of iterations is reached, the total time complexity of PSO is O(ùëõ √ó ùúá √ó ùëÇ(ùëë √ó ùë£) + ùëÇ(ùëë √ó ùëù), excluding and positions, respectively. Because the position and velocity are updated for each particle until they converge or the
the swarm initialization time.
Proposed Method
This section discusses the proposed HDFS(PSO-MI) method in detail. The method consists of two phases: (i) PSO feature subset selection and (ii) a combination of feature subsets using MI. In the first phase, we divided the original
dataset into ùëò random partitions. We applied the PSO method to each partition as a wrapper to determine the best
subset of features. PSO uses a feature subset evaluation function, known as an objective function. In our proposed method, instead of applying a single objective function, we apply three different objective functions to overcome the
a subset of the relevant features for a particular partition, ùëëùëñ. This method generates three subsets of selected features possible biases of a single objective function in any distribution of data objects. The outcome of the PSO algorithm is
partitions of the datasets. If there are ùëò partitions, then the method generates ùëò feature subsets. In the next phase, the for each partition. The UNION operation combines features selected from the three subsets. This process executes all
proposed method combines the three subsets of features and ranks the individual features at the time of combination. This method uses the MI measure to compute the rank of each feature over the entire dataset. Finally, a threshold was calculated from the MI scores of the combined features, and the method selected features with MI scores greater than the threshold value. These are considered the best subsets of features selected by the proposed HDFS(PSO-MI) method. Because the proposed method applies PSO for the initial feature subset selection in a distributed manner, all feature subsets are combined using the MI score. Hence, our method is called HDFS(PSO-MI): Hybrid Distributed Feature Selection using PSO-MI‚Äù. The workflow of the proposed method is shown in Fig. 1. Next, the MI approaches used in the proposed method are discussed.
The primary learning factors of PSO, such as social learning factor (ùê∂1) and cognitive learning factor (ùê∂2), are set as 2.
Harb et al. (Harb and Desuky, 2014) suggested that the values of ùê∂1 and ùê∂2 can take any random value, but the added values of ùê∂1 and ùê∂2 should not exceed 4. The maximum number of iterations was set to 100, and the fitness function or objective of the PSO algorithm was set as ùëúùëèùëó1, ùëúùëèùëó2, ùëúùëèùëó3. After initializing the required parameters, the features of the distributed data were represented as PSO particles. The objective function ùëúùëèùëó1 is evaluated by setting the initial velocity of each particle to zero and iteration ùë° to 0 + 1. Then, we find the personal or local best for each particle and the global best for the swarm. Now, consider the random numbers ùëü1 and ùëü2 in the range (0,1), and update each particle‚Äôs velocity and position using Eqs. 1 and 2. Next, we calculate the objective function ùëúùëèùëó1 for each distribution
and check whether the stopping criteria are satisfied. Otherwise, the number of iterations is increased, and the same
objective functions, ùëúùëèùëó2 and ùëúùëèùëó3, using the same data distribution. After executing all the objective functions, the process is repeated until the stopping criteria are satisfied. The same process of PSO is performed for the other two
UNION operation is performed on the three selected subsets of features generated by the three objective functions to generate one subset of features for evaluation.
The symbols used to describe the proposed algorithm are listed in Table 2.
The algorithm begins its execution on dataset ùê∑ with ùëõ features: Initially, we divide the entire dataset into ùëö
distributions; subsequently, for each distribution ùëëùëñ ‚àà ùê∑
Initialized the PSO parameters and defined the objective ùëúùëèùëóùëì ùëõ.
Compute the binary PSO and the union of ùëÉ ùë†ùëú_ùëÜùêπ where ùëúùëèùëóùëñ ‚àà ùëúùëèùëóùëì ùëõ
and compute the correlation between feature ùëìùëñ and class ùê∂ using MI where ùëìùëñ ‚àà ùëà ùëõùëñùëúùëõ_ùëÉ ùë†ùëú_ùëÜùêπ
Then, we combine the ùëÄùêº _ùë†ùëêùëúùëüùëí for each feature ùëìùëñ ‚àà ùëÄùêº _ùë†ùëêùëúùëüùëí for all distributions ùëë1, ùëë2, ‚Ä¶ , ùëëùëö. The threshold value was set to half the maximum score. Finally, features with a value greater than or equal to the threshold value are selected.
































Figure 1: Flowchart of the proposed method


Proposed Algorithm
The steps of the proposed HDFS (PSO-MI) method are presented in Algorithm 1. To understand the algorithm better, we use the various symbols shown in Table 2 with their meanings. The algorithm consists of two phases: PSO feature selection and feature ranking using MI. The steps in both phases are presented using the same algorithm.
Working Example
To better understand the proposed method, we discuss a working example. Let us consider a label dataset ùê∑ with features ùêπ = (ùëì1, ùëì2, ùëì3, ùëì4, ùëì5, ùëì6, ùëì7) and a class label ùê∂ as listed in Table 3. First, we split the dataset into three
categories: distribution 1, distribution 2, and distribution 3, as listed in Table 4. For each distribution, we apply the basic binary PSO algorithm with three different objective functions to select the initial subset of features from the distribution. The accuracy obtained from the SVM classifier is used to define the objective functions. The three objective functions select three subsets from each distribution, which are combined using the UNION operator to generate a subset of the selected features. This process is repeated for the other two distributions.
As shown in Table 4, dataset ùê∑ is partitioned into three distributions, each of which contains six rows as instances,
seven columns as features, and ùê∂ is a label.





































Figure 2: Process of PSO with three objective functions

This method applies PSO to the first distribution and selects three feature subsets using ùëúùëèùëó1, ùëúùëèùëó2, and ùëúùëèùëó3. Using the UNION operation, the three subsets are merged to obtain a single feature set.
Definition 1 (Relevant Feature). A feature is called a relevant feature to the target variable if it can discriminate samples correctly w.r.t the given feature. The relevance of individual features can be evaluated using mathematical, statistical, and information-theoretical measures.
Definition 2 (Optimal Feature Set). A subset of feature, say ùëÜ = {ùëì1, ùëì2, ‚ãØ ùëìùëò} is called optimal if the performance measured on ùëÜ by an ML algorithm is always higher than any other subset of features say ùëÜ‚Ä≤.

Symbols used and their description


Algorithm 1: HDFS(PSO-MI) Feature Selection

Input: Dataset ùê∑ with ùëõ features, class label ùê∂, and number of distribution ùëö. Output: ùêπ ‚Ä≤: The subset of ùëò optimal features.
foreach ùëëùëñ ‚àà ùê∑ do
Initialize Binary_PSO parameters;
ùëúùëèùëóùëì ùëõ = (ùëúùëèùëó1, ùëúùëèùëó2, ùëúùëèùëó3);
foreach objective function ùëúùëèùëóùëñ ‚àà ùëúùëèùëóùëì ùëõ do
selected feature ùëÉ ùë†ùëú_ùëÜùêπ = ùêµùëñùëõùëéùëüùë¶_ùëÉ ùëÜùëÇ(ùëëùëñ, ùê∂);
ùëà ùëõùëñùëúùëõ_ùëÉ ùë†ùëú_ùëÜùêπ = ‚à™3  (ùëÉ ùë†ùëú_ùëÜùêπ );
end
foreach feature ùëìùëñ ‚àà ùëà ùëõùëñùëúùëõ_ùëÉ ùë†ùëú_ùëÜùêπ do
calculate ùëÄùêº _ùëÜùëêùëúùëüùëí = ùëÄùêº (ùëìùëñ, ùê∂);
end
end
foreach feature ùëìùëó ‚àà ùëÄùêº _ùëÜùëêùëúùëüùëí do
calculate ùëÜùëêùëúùëüùëí = ùëêùëúùëöùëèùëñùëõùëíùëü(ùëÄ ùêº _ùëÜùëêùëúùëüùëí);
end
ùëá ùêª = 1 max(ùëÜùëêùëúùëüùëí);
foreach objective feature ùëìùëó  ‚àà ùëÄùêº _ùëÜùëêùëúùëüùëí do if ùëÄùêº _ùëÜùëêùëúùëüùëí(ùëìùëó ) > ùëá ùêª then
Select ùëìùëó and put it into the set ùêπ ‚Ä≤;
end end
return features set ùêπ ‚Ä≤;


Proposition 1: The subset of features selected by HDFS(PSO-MI) is relevant and optimal
subset, say ùëÜ = {ùëì1, ùëì2, ‚ãØ ùëìùëò}. The method evaluates each feature ùëìùëñ ‚àà ùëÜ in terms of the MI between the feature Proof: HDFS(PSO-MI) first evaluates possible subsets of features using PSO techniques, and then selects the best
ùëìùëñ and the target variable ùê∂ and chooses the features highly relevant to ùê∂. Hence, HDFS(PSO-MI) always selects the
relevant and optimal values.

Example Dataset


Table 4
Distribution of the example dataset


Experimental Analysis
memory, i5 11ùë°‚Ñé Gen intel processor, and a 64-bit Windows 11 Operating System using the Python programming We implemented our proposed HDFS(PSO-MI) hybrid feature selection method on a computer with 8GB primary
language in a Jupiter notebook. During the implementation, we used various Python packages such as NumPy, Pandas, sci-kit learn, and Keras.

Table 5
Selected Features using PSO


Table 6
Dataset Description


Datasets used
Fifteen datasets were used to validate the effectiveness of the proposed method. The datasets are summarized in Table 6. Most of the datasets contained both numerical and categorical features without missing values. All the datasets were imbalanced in terms of class distributions.
Performance Measures Used
We used various performance analysis measures, as shown in Table 7 to effectively analyze the proposed method. The accuracy and Mathew‚Äôs correlation coefficients were used to analyze and compare our method with other competing methods.

































Figure 3: Architecture of the parallel execution of the HDFS(PSO-MI)


Parallel Execution of the Proposed HDFS(PSO-MI) Method
We evaluated the HDFS(PSO-MI) method using five large datasets with over 1,000 features. The proposed method takes an exponential amount of time to select an optimal subset of features because the dimensions of the dataset are very high, and the PSO method executes multiple objective functions. Because of the exponential execution time, the proposed method may fail to operate in many applications of ML methods in real-time decision-making processes. Therefore, to improve the execution performance of the HDFS(PSO-MI) method, we used parallel programming in a high-performance computing environment. We applied the parallel computing concept to reduce the computations using Python‚Äôs multiprocessing module. Initially, our proposed algorithm could take any number of distributions as a parameter; however, here, we consider only two because we have limited processor cores. The flow of the execution process is shown in Fig. 3. Using the core of the processor, the process of executing the algorithm begins, and waits until the entire process is completed. After dividing the entire input dataset into two distributions, another processor core executes each distribution. For each distribution, one processor core began the execution process and waited until the entire process was completed. Because we applied three objective functions to each distribution, we used the core

Table 7
Performance Measures

of one processor to compute each objective function. We adopted the shared memory concept to group the resulting outputs. The results of each objective function were grouped to provide one result for the distribution. Subsequently, the results of each distribution were combined, and the feature set was selected accordingly. During the program execution, we used nine processor cores.

List of 10 High-Ranked Features Selected by our Proposed HDFS(PSO-MI) Method
The proposed HDFS(PSO-MI) method ranks all features in descending order of their scores and selects only the high-ranked features. Next, the selected features are fed into the ML model to classify the instances. It is important to know the names of the features selected as the best set from a single dataset. In Table 8, we list only the first ten high-ranked features from all selected features, owing to space constraints. For some datasets, the method selected fewer than 10 features.





Table 8
Names of the Selected High-ranked Features


Comparision of HDFS(PSO-MI) with other methods using P-value on Malaria dataset


Table 10
Comparision of HDFS(PSO-MI) with other methods using P value on Breast Cancer dataset


Result Analysis on Big Datasets
In our experimental analysis, we used five large datasets: wafer manufacturing, malaria, breast cancer, skin cancer, and emotion. The description of the datasets is given in Table 6. To better generalize the performance of the HDFS(PSO-MI) method, we statistically validated it using a t-test. We compared the statistical significance of the HDFS(PSO-MI) method with those of other competing FS methods by considering the p-values. If the p-value of a pair of FS methods using a classifier is greater than 0.05, there are no significant differences in the performances of the two FS methods. Otherwise, the methods used may have differed significantly. Tables 9‚Äì12. In addition to the p-value, we validated and analyze the proposed HDFS(PSO-MI) using the accuracy and MCC scores on five high-dimensional datasets, as shown in Figs. S4‚ÄìS13.
Statistical Analysis of the Proposed Method using p values
We applied a t-test to validate the statistical significance of the proposed HDFS(PSO-MI) method and compared it with other FS methods. From the computed p-values, we observed that the proposed method behaved similarly to the FS methods. As shown in Table 9, the p-value between the proposed method and EFS_Rank method is smaller than the threshold of 0.05. Hence, the t-test reflects the significant differences between the two classifiers evaluated using the k-NN classifier. Similarly, there was a significant difference between HDFS(PSO-MI) and ANOVA using the Random Forest classifier. As shown in Table 10, on the Breast Cancer dataset, the proposed HDFS(PSO-MI) shows results similar to those of the other FS methods, as the p-values of all competing methods are greater than 0.05. Hence, there were no significant differences between the FS methods. On the emotion dataset, the proposed method yielded a p-value of 1 when compared with the other FS methods, as shown in Table 11. Therefore, there was no significant difference between HDFS-MI and other competing FS methods. Finally, Table 12 shows all p-values greater than 0.05, which indicates that there were no significant differences between HDFS-MI and the other FS methods for the Skin Cancer dataset.
Result Analysis on Big Datasets in terms of Accuracy
From the experimental results on HDFS(PSO-MI) on five real big datasets, we observed that on Malaria, Skin Cancer, Breast Cancer, and Emotion datasets, the proposed method gives very good results using all the classifiers as shown in the Figs. S6 -S12. However, the proposed method shows a slightly lower accuracy than the other methods using DT, SVM, and k-NN classifiers, as shown in Fig. S4.
Result Analysis
We used the accuracy and MCC measures to compare the HDFS(PSO-MI) method with six other FS methods. The performance metrics were computed by applying five machine learning classifiers, viz., LR, k-NN, SVM, DT, and RF.

Table 11
Comparision of HDFS(PSO-MI) with other methods using P value on Emotion dataset


Table 12
Comparision of HDFS(PSO-MI) with other methods using P value on Skin Cancer dataset



The HDFS(PSO-MI) method was compared with other FS methods, such as the ensemble feature selection with ranking method (EFS-Rank), Spearman correlation, ANOVA, Pearson correlation, mutual information, and gain ratio using five classifiers: LR, k-NN, SVM, DT, and RF. As shown in Fig. S14, the proposed method provides better accuracy on the fetal _health dataset than the EFS_Rank, Spearman correlation, ANOVA, Pearson correlation, and mutual information using LR, SVM, DT, and RF. In the Kidney_disease dataset, our method outperformed the Spearman correlation, ANOVA, Pearson correlation, and mutual information in terms of accuracy using all five classifiers, as shown in Fig. S16. Similarly, as shown in Fig. S18, our method outperformed the Spearman correlation, ANOVA, Pearson correlation, and mutual information using k-NN, SVM, DT, and RF on the ECG dataset. On the hypothyroid dataset, as shown in Fig. S20, the HDFS(PSO-MI) method outperformed the EFS_Rank, Spearman correlation, ANOVA, Pearson correlation, and mutual information using LR and SVM, but the EFS_Rank, Spearman correlation, ANOVA, and Pearson correlation measures yielded better accuracy using k-NN, DT, and RF. Using the classifiers LR, k-NN, SVM, and RM, our method showed similar accuracy to all competing feature selection methods on the unbalanced dataset. However, as shown in Fig. S22, our method outperformed the EFS_Rank, Spearman correlation, Pearson correlation, and gain ratio using the DT classifier. On the MOFP and Bioassay datasets, as shown in Fig. S24 and S26, the proposed method provided better accuracy than all the competing methods using LR, k-NN, SVM, and RF. However, the DT showed a lower accuracy for our method on both datasets. As shown in Fig. S28, using the LR, SVM, and RF classifiers, the proposed FS method outperformed the other competing methods. However, using k-NN and DT classifiers, our method yielded similar accuracy on the Parkinson‚Äôs disease dataset. On the Cancer Gene dataset, our method performed very well compared with the other methods using all classifiers, as shown in Fig. S30. However, on the Colon Cancer dataset, our method provides better accuracy using LR only; with other classifiers, it showed slightly lower accuracy, as shown in Fig. S32. The MCC performance measure plays a significant role in validating the FS method for unbalanced datasets. A high MCC score indicated good prediction results (Chicco and Jurman, 2020). MCC scores computed on all datasets using different classifiers for our proposed method were compared with other methods, as shown in Figs. S15, S17, S19, S21, S23, S25, S27, S29, S31, and S33
Discussion
We developed an efficient hybrid feature selection method, known as HDFS (PSO-MI) using PSO and MI. This method is highly effective in selecting an optimal subset of features that can yield high classification accuracy on different datasets. Although this method does not consider the class imbalance problem during feature selection, MCC values ensure that the selected features on various datasets provide a significantly high classification accuracy. The main advantage of the proposed hybrid method is that it can select optimal subset features from a high-dimensional dataset by evaluating possible subsets generated by PSO, where SVM is used as an evaluator of the subset. Hence,

PSO ensures that the best subset of features can be selected. In the next phase, the method evaluates each feature of the selected subset to compute their ranks in terms of their MI scores. Thus, the hybrid method always selects the best features from the entire feature subset. The experimental results demonstrate that the proposed method significantly reduces the dimensions and redundancies of high-dimensional datasets. In addition, the feature subset selected by the proposed method yields better results than the existing filter-based FS methods, that is, mutual information, gain ratio, Spearman correlation, ANOVA, Pearson correlation, and the ensemble feature selection with ranking method using five ML classifiers: LR, k-NN, SVM, DT, and RF. Although the proposed objective function enhances the PSO algorithm, the computational cost is slightly higher than those of the two existing objective functions used in our HDFS(PSO-MI) method.
Conclusion and Future Work
This study introduces an effective hybrid FS method called HDFS(PSO-MI) using PSO and MI, which selects a subset of optimal features from a high-dimensional dataset. This method considers distributed data, and from each distribution, it selects three subsets of features using three objective functions in the PSO optimization techniques. The UNION of the three subsets is evaluated again for each distribution to select the high-ranked features as the most optimal. The computational cost of the hybrid method is high because the evaluation of the feature subset with PSO using three objective functions requires a significant amount of time. We applied three objective functions in PSO to evaluate a possible subset of features in each distribution to reduce the chance of bias in a single objective function. From the experimental results, we observed that the proposed HDFS(PSO-MI) selected an optimal feature subset that provided high classification accuracy on various datasets. Although the proposed method works well on most datasets taken from multiple application domains, including five large real datasets, the major drawback of the method is its exponential computational time. The computational time increases asymptotically as the dimension of the dataset increases and when using three objective functions on the PSO optimizer. Therefore, an effective mechanism is required for addressing this issue. In future work, we plan to implement the proposed method in a distributed parallel programming environment to reduce computational costs.
Acknowledgement: The work is funded by the University Grant Commission (UGC) under Start-up-Grant No. F 30-592/2021(BSR).
References
Bansal, S. R., Wadhawan, S., and Goel, R. (2022). mrmr-pso: A hybrid feature selection technique with a multiobjective approach for sign language recognition. Arab J Sci Eng, 47(8):10365‚Äì10380.
Baruah, H. S., Thakur, J., Sarmah, S., et al. (2020). A feature selection method using pso-mi. In 2020 International Conference on Computational Performance Evaluation (ComPE), pages 280‚Äì284. IEEE.
Brahim, A. B. and Limam, M. (2016). A hybrid feature selection method based on instance learning and cooperative subset search. Pattern Recognit Lett, 69:28‚Äì34.
Chandrashekar, G. and Sahin, F. (2014). A survey on feature selection methods. Comput. Electr. Eng, 40(1):16‚Äì28.
Chicco, D. and Jurman, G. (2020). The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation. BMC Genom, 21(1):1‚Äì13.
Eberhart, R. and Kennedy, J. (1995). Particle swarm optimization. In Proc. Int, volume 4, pages 1942‚Äì1948. Citeseer.
Fong, S., Wong, R., Vasilakos, A., et al. (2015). Accelerated pso swarm search feature selection for data stream mining big data. IEEE Trans. Serv. Comput., 9(1):33‚Äì45.
Harb, H. M. and Desuky, A. S. (2014). Feature selection on classification of medical datasets based on particle swarm optimization. Int. J. Comput. Appl, 104(5).
Hoque, N., Ahmed, H., Bhattacharyya, D., et al. (2016). A fuzzy mutual information-based feature selection method for classification. Fuzzy Inf. Eng, 8(3):355‚Äì384.
Hoque, N., Bhattacharyya, D. K., and Kalita, J. K. (2014). Mifs-nd: A mutual information-based feature selection method. EXPERT SYST APPL, 41(14):6371‚Äì6385.
Hoque, N., Singh, M., and Bhattacharyya, D. K. (2018). Efs-mi: an ensemble feature selection method for classification: An ensemble feature selection method. COMPLEX INTELL SYST, 4:105‚Äì118.
Kumar, V. and Minz, S. (2014). Feature selection: a literature review. Smart Comput. Rev., 4(3):211‚Äì229.
Li, J., Cheng, K., Wang, S., et al. (2017a). Feature selection: A data perspective. ACM Comput Surv, 50(6):1‚Äì45. Li, J., Cheng, K., Wang, S., et al. (2017b). Feature selection: A data perspective. ACM Comput Surv, 50(6):1‚Äì45.
Li, X., Zhang, J., and Safara, F. (2021). Improving the accuracy of diabetes diagnosis applications through a hybrid feature selection algorithm.
Neural Process. Lett, pages 1‚Äì17.
Moradi, P. and Gholampour, M. (2016). A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy.
Appl. Soft Comput, 43:117‚Äì130.


Moslehi, F. and Haeri, A. (2020). An evolutionary computation-based approach for feature selection. J. Ambient. Intell. Humaniz. Comput, 11:3757‚Äì 3769.
Shi, Y. and Eberhart, R. C. (1998). Parameter selection in particle swarm optimization. In Evolutionary Programming VII. EP 1998, pages 591‚Äì600. Springer.
Venkatesh, B. and Anuradha, J. (2019). A review of feature selection and its methods. Cybern. Inf. Technol, 19(1):3‚Äì26.
Vieira, S. M., Mendon√ßa, L. F., Farinha, G. J., et al. (2013). Modified binary pso for feature selection using svm applied to mortality prediction of septic patients. Appl. Soft Comput, 13(8):3494‚Äì3504.
Wang, C.-M. and Huang, Y.-F. (2009). Evolutionary-based feature selection approaches with new criteria for data mining: A case study of credit approval data. EXPERT SYST APPL, 36(3):5900‚Äì5908.
Zhu, Y., Li, W., and Li, T. (2023). A hybrid artificial immune optimization for high-dimensional feature selection. KNOWL-BASED SYST, 260:110111.

Appendix A.	Supplementary data

Figure S4: Accuracy on Wafer Manufacturing dataset


Figure S5: MCC on Wafer Manufacturing dataset



Figure S6: Accuracy on Malaria dataset


Figure S7: MCC on Malaria dataset

Figure S8: Accuracy on Breast Cancer dataset



Figure S9: MCC on Breast Cancer dataset

Figure S10: Accuracy on Skin Cancer dataset


Figure S11: MCC on Skin Cancer dataset







Figure S12: Accuracy on Emotions dataset










Figure S13: MCC on Emotions dataset





Figure S14: Accuracy of Fetal_health dataset








Figure S15: MCC of Fetal_health dataset






Figure S16: Accuracy of Kidney_disease dataset







Figure S17: MCC of Kidney_disease dataset






Figure S18: Accuracy of ECG dataset







Figure S19: MCC of ECG dataset






Figure S20: Accuracy of Hypothyroid dataset








Figure S21: MCC of Hypothyroid dataset






Figure S22: Accuracy of Unbalanced dataset









Figure S23: MCC of Unbalanced dataset







Figure S24: Accuracy of MOFP dataset







Figure S25: MCC of MOFP dataset






Figure S26: Accuracy of Bioassay dataset








Figure S27: MCC of Bioassay dataset






Figure S28: Accuracy of Parkinson Disease dataset







Figure S29: MCC of Parkinson Disease dataset






Figure S30: Accuracy of Cancer_normal_gene dataset







Figure S31: MCC of Cancer_normal_gene dataset






Figure S32: Accuracy of Colon_Cancer gene Dataset







Figure S33: MCC of Colon_Cancer gene Dataset




Title of the Manuscript: HDFS (PSO-MI): Hybrid Distributed Feature Selection using PSO- MI
Conflict of Interest: No
