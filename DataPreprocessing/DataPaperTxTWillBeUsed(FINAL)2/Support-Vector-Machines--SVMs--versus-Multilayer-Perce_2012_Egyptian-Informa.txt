
ORIGINAL ARTICLE

Support Vector Machines (SVMs) versus Multilayer Perception (MLP) in data classification
E.A. Zanaty

Mathematics Dept., Computer Science Section, Faculty of Science, Sohag University, Sohag, Egypt

Received 5 January 2012; revised 6 August 2012; accepted 15 August 2012
Available online 13 September 2012

Abstract In this paper, we introduce a new kernel function for improving the accuracy of the Support Vector Machines (SVMs) classification. The proposed kernel function is stated in general form and is called Gaussian Radial Basis Polynomials Function (GRPF) that combines both Gaussian Radial Basis Function (RBF) and Polynomial (POLY) kernels. We implement the pro- posed kernel with a number of parameters associated with the use of the SVM algorithm that can impact the results. A comparative analysis of SVMs versus the Multilayer Perception (MLP) for data classifications is also presented to verify the effectiveness of the proposed kernel function. We seek an answer to the question: ‘‘which kernel can achieve a high accuracy classification versus multi-layer neural networks’’. The support vector machines are evaluated in comparisons with different kernel functions and multi-layer neural networks by application to a variety of non- separable data sets with several attributes. It is shown that the proposed kernel gives good classifi- cation accuracy in nearly all the data sets, especially those of high dimensions. The use of the proposed kernel results in a better, performance than those with existing kernels.
© 2012 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.



Introduction

Learning with spatially localized basis functions has become a popular paradigm in machine learning community. In the con- text of radial basis function networks [1–5], it was demon- strated that these learning methods offer an alternative to learning with global basis functions, such as sigmoidal neural

E-mail address: zanaty22@yahoo.com
Peer review under responsibility of Faculty of Computers and Information, Cario University.


networks. Today, support vector machines and along with other learning based-kernel algorithms show better results than artificial neural networks and other intelligent or statisti- cal models, on the most popular benchmark problems [6]. The scarcity of the model results from a sophisticated local learning that matches the model capacity to the data complexity ensur- ing a good performance on the future, previously unseen, data. They gave a single solution characterized by the global mini- mum of the optimized functional and not multiple solutions associated with the local minima as in the case of neural net- works. Moreover, support vector machines do not rely so heavily on heuristics, i.e. an arbitrary choice of the model and have a more flexible structure [7].
Classification is one of the important machine learning operations. It is the operation that enables organizations to

1110-8665 © 2012 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.eij.2012.08.002



discover patterns in large or complex data sets. Classification and function approximation using SVMs are formulated as Quadratic Programming (QP) problems which can be solved efficiently by using many well-documented optimization algo- rithms [8–10]. The neural networks may be regarded as the uni- versal classifications of the measured data in the multi- dimensional space. They realize two types of classification: the global and local one [11]. The most important example of global network is the Multi-Layer Perception (MLP), employing the sigmoid activation function of neurons. In MLP the neurons are arranged in layers, counting from the in- put layer (the set of input nodes), through the hidden layers, up to the output layer. The interconnections are allowed only be- tween two neighboring layers. The network is feed forward, i.e., the processing signals propagate from input to the output side.
The most representative example of local neural network is the Support Vector Machine (SVM) of different kernels func- tions. Choosing different kernel functions will produce differ- ent SVMs and may result in different performances [12,13]. In the SVM literature, there exist polynomials SVMs, radial basis function of SVMs, two-layer Neural Network (NN) SVMs and so on [11]. They correspond to the kernel functions of polynomial, radial basis function and two-layer NN, respec- tively. Once the kernel is fixed, SVM classifiers have only a user-chosen parameter (the error penalty), but the kernel is a very big rug under which many parameters have to be deter- mined. Some works have been done on limiting kernels using prior knowledge, but the best choice of a kernel for a given problem is still an open research issue [14–16].
One issue is identifying an appropriate kernel for the given data. Most algorithms rely on a priori knowledge to select the correct kernel. Tsang et al. [16] discussed a way to take advan- tage of the approximations inherent in kernel classifiers, by using a minimum enclosing ball algorithm as an alternative means of speeding up training. Training time had previously been reduced mostly by modifying the training set in some way. Their Core Vector Machine converged in linear time with space requirements independent of the number of data points. Also, Kwok and Tsang [17], they applied it to the kernel PCA problem successfully, but also found that it did not perform well when applied to the reduced set problem. It is interesting to note that the methods of [17,18] can both be applied to pre- image applications with a discrete input space, since they do not require the gradient of the objective function. Generally, in implementations of this method, the time and space com- plexities are very high because the core of the SVMs is based on approximate minimum enclosing ball algorithms which are computationally expensive.
Maji et al. [19] presented a technique for the exact evaluation of intersection kernel SVMs which is logarithmic in time. They have shown that the method is relatively simple and the classification accuracy is acceptable, but the runtimes  are  significantly  increased  compared  with  the


Despite the maturity of classification, problems remain, especially in choosing the most appropriate kernel of SVMs for a particular application [24,25]. In other words, exploration of new techniques and systematic methodology to construct an efficient kernel function for designing SVMs in a specific appli- cation is an important research direction in SVMs. Several sur- vey papers on comparing SVMs with Gaussian Kernels to Radial Basis Function Classifiers can be found in literature, but these focus on a sub-set of techniques and often only on performance accuracy [24–27].
In this study, a new kernel function called Gaussian Radial basis Polynomial Function (GRPF) is introduced that could improve the classification accuracy of Support Vector Ma- chines (SVMs) for both linear and non-linear data sets. The aim is to train Support Vector Machines (SVMs) with different kernels compared with back-propagation learning algorithm in classification task. Moreover, we compare the proposed algo- rithm to algorithms based on both Gaussian and polynomial kernels by application to a variety of non-separable data sets with several attributes. It is shown that the proposed kernel gives good classification accuracy in nearly all the data sets, especially those of high dimensions.
The rest of this study is organized as follows: In Section 2, the SVM classifier is described. The multi-layer perception classifier is designed in Section 3. In Section 4, the SVMs are presented with a new kernel function. The kernel parameters are optimized in Section 5. Section 6 gives comparison results between support vector machines and multi-layer perception classifier. Our conclusion is presented in Section 7.

SVM classifiers

The details of SVM classification are described in [28–32]. We outline the basic equations and we follow the notations of
in input space, with corresponding binary labels yi ∈ {—1; 1} (i.e. yi takes 1 if xi is in class 1 and takes —1 if xi is in class Scho¨lkopf et al. [29]. Let xi for 1 6 i 6 Nx be the input vectors
2). Let /(xi) be the corresponding vectors in feature space,
K(xi, xj) = /(xi) ● /(xj) be the kernel function, implying a where, /(xi) is the implicit kernel mapping and let dot product in the feature space. Popular kernel functions in-
clude classical kernels and recent kernels as shown in ([32]).
The optimization problem for a soft-margin SVM is:
min 1   2
w;b	i


Subject to the constraints yi(w.x + b) = 1 — ni and ni P 0 where w is the normal vector of the separating hyper plane
in feature space and C > 0 is a regularization parameter con- trolling the penalty for misclassification. Eq. (1) is referred to as the primal equation. From the Lagrangian form of (1), we derive the dual problem:
(X	1 X	)

	 


[20,21].
Zanaty et al. [22,23] described the SVMs with combination the both RBF and POLY functions to take advantage of their respective strengths. They introduced kernel functions called Polynomial Radial Basis Function (PRBF).
Subject to 0 6 ai 6 C. This is a quadratic optimization prob-
lem that can be solved efficiently using algorithms such as Sequential Minimal Optimization [32]. Typically, many ai go to zero during optimization and the remaining xi correspond- ing to those ai > 0 are called support vectors. To simplify

notation, from here on we assume that all non-support vectors have been removed, so that Nx is now the number of support vectors and ai > 0 for all i. With this formulation, the normal vector of the separating plane w is calculated as:
XNx
	


where g(k)=  ∂E  is the gradient of error function Eq. (5) and
G(k) is the Hessian approximation ([34,35]), determined by
applying the Jacobian matrix J(k):
G(k)= J(k)TJ(k)+ v.	(8)

Note that because /(xi) is defined implicitly, w exists only in
feature space and cannot be computed directly. Instead, the classification f(q) of a new query vector q can only be deter- mined by computing the kernel function of q with every sup-
port vector:
The variable is the Levenberg–Marquard parameter ad- justed step by step in a way to provide the positive definiteness of Hessian G (the value of is eventually reduced to zero).
In conjugate gradient approach, most effective for large
networks, the direction p(k) is evaluated according to the

f(q)= sign

Nx
i=1
aiyi.k(q, xi)+ b!
(4)
formula:
p(k)= —g(k)+ bp(k — 1)	(9)

where the bias term b is the offset of the hyperplane along its
normal vector, determined during SVM training [33].

Multi-level perception
where the conjugate coefficient b is usually determined accord-
ing to the Polak–Ribiere rule:
g(k)T(g(k)— g(k — 1))

p(k)= 
g(k — 1)Tg(k — 1)

The Multilayer Perception (MLP) is perhaps the most popular
network architecture in use today both for classification and regression. MLPs are feed forward neural networks which are typically composed of several layers of nodes with unidirec- tional connections, often trained by back propagation [34,35]. The learning process of MLP network is based on the data samples composed of the N-dimensional input vector x and the M-dimensional desired output vector d, called destination. By processing the input vector x, the MLP produces the output signal vector y(x, w) where w is the vector of adapted weights. The error signal produced actuates a control mechanism of the learning algorithm. The corrective adjustments are designed to make the output signal yk(k = 1, 2,..., M) to the desired re- sponse dk in a step by step manner.
The learning algorithm of MLP is based on the minimiza- tion of the error function defined on the learning set (xi, di) for i = 1, 2,.. . , N using the Euclidean norm:
In the weight update Eq. (6) the learning coefficient c should be adjusted by the user. It is done usually by applying so called adaptive way [32], taking into account the actual progress of minimization of the error function.

Proposed kernel functions

A critical step in support vector machine classification is choosing a suitable kernel of SVMs for a particular applica- tion, i.e. various applications need different kernels to get reli- able classification results. It is well known that the two typical kernel functions often used in SVMs are the radial basis func- tion kernel and polynomial kernel. More recent kernels are presented in [28–32] to handle high dimension data sets and are computationally efficient when handling non-separable data with multi attributes. However, it is difficult to find ker-
nels that are able to achieve high classification accuracy for a

N
E(w)= 
i=1
y(xi, w)— di 
(5)
diversity of data sets. In order to construct kernel functions from existing ones or by using some other simpler kernel func- tions as building blocks, the closure properties of kernel func-

The minimization of this error leads to the optimal values of
weights. The most effective methods of minimization are the gradient algorithms, from which the most effective is the Levenberg–Marquard algorithm for medium size networks and conjugate gradient for large size networks. Generally in all gradient algorithms the adaptation of weights is performed step by step according to the following scheme:
w(k + 1)= w(k)+ cp(k)	(6)
In this relation p(k) is the direction of minimization in kth step,
c is the learning coefficient, and w is the adaptation coefficient. Various learning methods differ in the way the value of p(k) is generated.
In Levenberg–Marquardt approach the least square formu- lation of learning problem is exploited:
tions are essential [36,37].
For given non-separable data, in order to be linearly sepa- rable, a suitable kernel has to be chosen. Classical kernels, such as Gauss RBF and POLY functions, can be used to transfer non-separable data to separable, but their performance in terms of accuracy is dependent on the given data sets. The fol- lowing POLY function performs well with nearly all data sets, except high dimension ones [36]:
POLY(x, z)= (xTz + 1)d	(10)
where d is the polynomial degree.
The same performance is obtained with the Gauss RBF of the following form [3]:
RBF(x, z)= exp(—c  x — z  2)	(11)

M
E(w)= 
i=1
(yi (w)— di)
where c is appositive parameter controlling the radius. Zanaty et al. [23] in presented the Polynomial Radial basis Function

and solved by using second order method of Newton type:
p(k)= —G(k)—1g(k)	(7)
(PRBF) as:
PRBF = ((1 + exp(x))/V)d	(12)

where x = |x — z|, V = p \ d and p is a prescribed parameter. Completely achieving a SVM with high accuracy classification
therefore, requires specifying high quality kernel function.
Here, we combine POLY, RBF, and PRBF into one kernel to become:
Experimental results

Data sets and parameter optimization

In this section, we perform classification experiments on a number of toy and real-world data sets 2: Letter, Pendigits,




where r is a statistic distribution of the probability density function of the input data; and the values of r(r > 1) and d can be obtained by optimizing the parameters using the train- ing data. The proposed kernel has the advantages of general- ity. However, The existing kernels such as PRBF and proposed in Zanaty et al. [22], Gaussian and polynomials ker- nel function by setting d and r in different values. For example if d = 0, we get Exponential Radial when r = 1 and Gaussian Radial for r = 2 and so on. Moreover various kernels can be obtained by optimizing the parameters using the training data.

Optimizing the kernel parameters

Let’s go back to the SVM algorithm. We assume that the ker- nel GRPF depends on two parameters d and r, encoded into a vector h = (d, r). We thus consider a class of decision func- tions parameterized by a, b, and h:
ble 1). In Table 1, we describe these data sets, but for more de- tails can be seen in [37]. The eight data have also been used in
[26] and (Zanaty et al. [22,23]) for the single-kernel SVM. Each data set has different training/test splits to give use the reliable behavior of the proposed kernel. We can divide these data sets according to the training set size into two types, large data sets (1 → 4) and small ones (5 → 8). In the proposed algorithm, we
used the optimization toolbox of Matlab to perform it. It in-
cludes second order updates to improve convergence speed. Although we used a loose stopping criterion, good value of the parameters minimizing the functional is obtained. Experi- mental results in Table 1 demonstrated that values of the parameter r are increased according to increasing attribute val- ues while the values of d are decreased in the case of largest class values. In a way this renders the technique even more applicable since this estimate is very simple. The proposed algorithm used loose stopping criterion for avoiding holding
out some data for validation and thus makes full use of the

fa, b, h(x)= sign

l
i=1
aiyiGRPFh(x, z)+ b!
(14)
training set for the optimization of parameters, contrary to cross-validation methods.

We want to choose the values of the parameters a and h such
that w (in Eq. (2)) is maximized (maximum margin algorithm) and T, the model selection criterion, is minimized (best kernel parameters). More precisely, for h fixed, we want to have a0 = arg max w(a) and choose h0 such that:
h0 = arg minT(a0, h).	(15)
h
When, h is a one dimensional parameter, one typically tries a finite number of values and picks the one which gives the lowest value of the criterion T. When both T and the SVM solution are continuous with respect to h a better approach has been pro- posed by Cristianini et al. [38]. They used an incremental optimi- zation algorithm, one can train an SVM with little effort when h is changed by a small amount. However, as soon as h has more than one component computing T(a, h) for every possible value of h becomes intractable, and one rather looks for a way to opti- mize T along a trajectory in the kernel parameter space.
In this work, we use the gradient of a model selection crite- rion to optimize the model parameters (see [39,40] for more discussions). This can be achieved by the following iterative procedure:

Initialize h to some value.
Using a standard SVM algorithm, find the maximum of the quadratic form w a0 = arg max w(a).
Update the parameters h such that T is minimized. This is typically achieved by a gradient step (Chapelle et al. [39] for these calculations).
Go to step 2 or stop when the minimum of T is reached.

Solving step 3 requires estimating how T varies with h where GRPFh can be differentiated with respect to h, more dis- cussions can be shown in [39].
Comparative results

In order to evaluate the performance of the support vector ma- chine with different kernels, we carried out some experiments with different data sets from machine learning benchmarks do- mains. We design a multi-class support vector machine classi- fier based on one versus one algorithm using the voting strategy [41]. For C classes, C(C-1)/2 binary classifiers are con- structed. The performance evaluation of the each support vec- tor machine using different kernel and the multilayer neural networks done using the following equation:
Accuracy = (n/N)· 100	(16)
where n is the number of correct classified examples and N is
the total number of the test examples.
We have experiment MLP network trained by using Leven- berg–Marquardt algorithm and SVM with POLY, RBF, PRBF and GRPF functions trained by applying method [35,16]. The training time of MLP was approximately 10 times longer than SVM. These data sets have given to the algorithm with different sizes (classes and attributes). Through the




experiments, we set parameter r = 0.5 (Gaussian RBF kernel width), and d = 2 for RBF, POLY, and GRPF.
Table 2 presents the classification accuracy for each data set using support vector machine with different kernel versus the multilayer neural networks.
The proposed method gives the best accuracy in most data sets. Table 3 presents the mean accuracy obtained from all ker- nels; it is obvious that the GRPF kernel obtains the best mean accuracy (95.91) compared to the classical RBF, POLY and PRBF. Moreover, Table 3 shows the comparison between the SVMs and MLPs classifiers, it is clear that the SVMs with the kernel (GRPF) achieve higher accuracy than MLP classi- fier and other kernels. Fig. 1 describes the mean accuracy of SVMs and MLP classifiers applied to all data sets. It is clear that the proposed kernel gives the best mean accuracy com- pared to the POLY, RBF and PRBF functions.
For future research, we focused on building a new hybrid classifier of SVMs and MLP to overcome the disadvantages of both algorithms.

Discussions

In this part of study, we attempt to investigate the best choice among SVM kernels for data classification task. Table 2




Table 3  The mean accuracy SVMs and MLP accuracy.
Support vector machine	Multilayer neural networks Poly	84.38
RBF	85.77
PRBF	92.38	84.90
GRPF	95.79
presented the performance of SVMs trained with POLY, RBF, PRBF and GRPF kernels on the different data sets.
From Table 2, the RPF function, with p = 3, d = 2 gives better accuracy with small data sets (5 → 8) than the Polyno- mial function. The Polynomial kernel with two dimension data d = 2, gives better results than RBF for large sets (1 → 4). Moreover, we noted that MLP achieved better results than POLY and RBF for these large sets (1 → 4). The proposed ker- nel, GRPF, gives the best accuracy in nearly all the data sets.
For data set 7 (ABE), although the size of data set is small and one feature is enough to distinguish a pair of ABE classes, the proposed kernel still achieves good results (the accuracy is of GRPF 97.98%) as shown in Fig. 2 shows the relation between the accuracy of RBF, POLY, PRBF, GRPF and the class number. As in Fig. 2, we noted that the accuracy of GRPF, RBPF and MLP are better than RBF and POLY kernels even in case of largest number of classes. For small class and attri- bute (data set 7), we noted that MLP achieves the worst accuracy.
Interestingly, as the number of attributes increases, the improvement in accuracy of the proposed method compared with POLY, RBF and PRBF kernels increases as shown in Fig. 3 (data set 5 which has the largest number of attributes). This improved performance is due to the fact that the pro- posed function is more complex and combines the perfor- mance of both its parents, RBF and Polynomial functions.
For largest attribute (data set 5), the proposed GRPF gives the best accuracy (99.88) at all and the MLP obtains the worst accuracy (68.4) at all.















Figure 2. The relation between accuracy of SVMs and MLP classifiers; and class number.














Figure 1.	The relation between mean accuracy of SVMs kernels and MLP.
Figure 3.	The relation between accuracy of SVMs and MLP classifiers; and attributes number.



Conclusion

In this study, we have constructed SVMs and computed its accuracy. We held a comparison between the SVMs and MLP classifier. We have used different sizes of data sets with different attributes. Then, we have compared the results of the SVM algorithm to MLP classifier. The proposed GRPF kernel has achieved the best accuracy, especially with the data sets with many attributes.
Comparing the classification accuracy of the support vector machine to the multilayer neural networks learning algo- rithms, it is obvious from that support vector machines with the proposed new kernel function (GRPF) accomplishes better accuracy than multilayer networks, especially in high dimen- sion data sets.
In MLPs classifiers, the tested data sets need more hidden units and the complexity is controlled by keeping the number of these units small, whereas the SVMs complexity does not depend on the dimension of the data sets. SVMs based on the minimization of the structural risk, whereas MLP classifi- ers implement empirical risk minimization. So, SVMs are effi- cient and generate near the best classification as they obtain the optimum separating surface which has good performance on previously unseen data points.
However, the main difference is in the complexity of the networks. The MLP network implementing the global approx- imation strategy usually employs very small number of hidden neurons. On the other side the SVM is based on the local approximation strategy and uses large number of hidden units. The great advantage of SVM approach is the formulation of its learning problem, leading to the quadratic optimization task. It greatly reduces the number of operations in the learn- ing mode. It is well seen for large data sets, where SVM algo- rithm is usually much quicker.


References

Duda R, Hart P, Stork G. Pattern classification. 2nd ed. UK: Wiley Interscience; 2001.
Boser B, Guyon I, Vapnik V, A training algorithm for optimal margin classifiers. In: Proceeding COLT ‘92 proceedings of the fifth annual workshop on computational learning, Pittsburgh, PA, USA; 1992. p. 144–52.
Taylor SJ, Cristianini N. Kernel methods for pattern analysis. Cambridge University; 2004.
Osuna E, Freund R, Girosi F. Support vector machines, training and applications. Artificial Intelligence Laboratory, MIT Press, Series, Report, AIM-1602, CBCL-144, 1997.
Boudat G, Anour F. Kernel-based methods and function approximation. In: International Joint Conference on Neural Networks (IJCNN). Washington: IEEE Press; 2001. p. 1244–9.
Meyer D, Leisch F, Hornik K. The support vector machines under test. Neurocomputing 2003;55:169–86.
Huang TM, Kecman V, Kopriva I. Kernel based algorithms for mining huge data sets, supervised, semi-supervised and unsuper- vised learning. Berlin, Heidelberg: Springer-Verlag; 2006.
Vapnik V. The nature of statistical learning theory. New York, NY, USA: Springer New York Inc.; 1995.
Cortes C, Vapnik V. Support vector networks. Mach Learn 1995;20:273–97.
Scho¨lkopf B. Support vector learning, PhD dissertation, Technical University, Berlin, Germany; 1997.
Mohammad Muamer N, Norrozila Sulaiman, Emad Khalaf T. A novel local network intrusion detection system based on support vector machine. J Comput Sci 2011;7:1560–4.
Vapnik V, Golowich S, Smola A. Support vector method for function approximation. Regression estimation and signal pro- cessing. In: Advances in neural information processing systems, vol. 9, no. 9, 1997. p. 281–7.
Williamson RC, Smola A, Scho¨lkopf B. Entropy numbers operators and support vector kernels. Cambridge: MA: MIT Press; 1999.
Chapelle O, Scho¨lkopf B. Incorporating invariances in non-linear support vector machines. In: Proc. NIPS; 2001. p. 609–16.
Lei H, Govindaraju V. Speeding up multi-class SVM evaluation by PCA and feature selection. Center for Unified Biometrics and Sensors (CUBS); 2005.
Tsang IW, Kwok JT, Cheung PM. Core vector machines: fast SVMs training on very large data sets. J Mach Learn Res 2005;6:271–363.
Kwok JT, Tsang IW. The pre-image problem in kernel method. IEEE Trans Neural Networks 2004;15:1517–25.
Bakir GH, Weston J, Scho¨lkopf B. Learning to find pre-images. Adv Neural Inf Proc Syst 2004;16:449–56.
Maji S, Berg AC, Malik J. Classification using intersection kernel support vector machines is efficient. In: Conference on computer vision and pattern recognition, CVPR IEEE; 2008. p. 1–8.
Haasdonk B. Feature space interpretation of SVMs with indefinite kernels. IEEE Trans Pattern Anal Mach Intell 2005;27(4):482–92.
Hastie T, Hsu CW, Lin CJ. A comparison of methods for multi- class support vector machines. IEEE Trans Neural Networks 2005;13:415–25.
Zanaty EA, Afifi A, Khateeb RE. Improving the accuracy of support vector machines via a new kernel functions. Int J Intell Comput Sci 2009;1:55–67.
Zanaty EA, Aljahdali S, Cripps RJ. Accurate support vector machines for data classification. Int J Rapid Manuf 2009;1(2).
Ektefa Mohammadreza, Sidi Fatimah, Ibrahim Hamidah, Jabar Marzanah, Memar Sara. A comparative study in classification techniques for unsupervised record linkage model. J Comput Sci 2011;7:341–7.
Sonkamble Balwant A, Doye D. A novel linear-polynomial kernel to construct support vector machines for speech recognition. J Comput Sci 2011;7:991–6.
Zanaty EA, Afifi A. Support vector machine (SVMs) with universal kernel. Appl Artif. Intell. 2011;25(7):575–89.
DeCoste D, Mazzoni D. Fast query optimized kernel machine classification via incremental approximate nearest support vec- tors. In: Proceedings of the 20th international conference on machine learning, Washington, ICML 2003; 2003. p. 115–22.
Scho¨lkopf B, Smola AJ. Learning with kernels. Cambridge, MA: The MIT Press; 2002.
Scho¨lkopf BS, Mika CJ, Burges P, Knirsch RR, Muller KG. Input space versus feature space in kernel-based methods. IEEE Trans Neural Network 1999;10(5):1000–17.
Burges CJ, Scho¨lkopf B. Improving the accuracy and speed of support vector machines. Neural Inform Proc Syst 1997;9:375–81.
Burges CJ. A tutorial on support vector machines for pattern recognition. Data Min Knowledge Discovery 1998;2(2):121–67.
Scho¨lkopf B, Burges C, Smola A. Advances in kernel methods support vector learning. MIT Press; 1998.
Taylor SJ, Bartlett PL, Willianmson RC, Anthony M. Structural risk minimization over data-dependent hierarchies. IEEE Trans Inf Theory 1998;5:1926–40.
Making JT. Large-scale SVM learning practical. In: Advances in kernel methods-support vector learning. Cambridge: MIT Press; 1999. p. 42–56.
Osowski S, Siwek K, Markiewicz T. MLP and SVM networks: a comparative study. In: Proceedings of the 6th nordic signal



processing symposium NORSIG, Espoo, Finland; June 2004. p. 9–11.
Cristianini N, Taylor SJ. An introduction to support vector machines. Cambridge, MA: Cambridge University Press; 2000.
http://archive.ics.uci.edu/ml/.
Cristianini N, Campbell C, Taylor SJ. Dynamically adapting kernels in support vector machines. In: Proceedings of the 1998 conference on advances in neural information processing systems II; 1999. p. 204–10.
Chapelle O, Vapnik V, Bousquet O, Mukherjee S. Choosing multiple parameters for support vector machines. Mach Learn 2002;46:131–59.
Bengio Y. Gradient-based optimization of hyper-parameters. Neural Comput 2000;12(8):1889–900.
Rifin R, Klautau A. In defense of one vs. all classification. J Mach Learn Res 2004;5:101–41.
