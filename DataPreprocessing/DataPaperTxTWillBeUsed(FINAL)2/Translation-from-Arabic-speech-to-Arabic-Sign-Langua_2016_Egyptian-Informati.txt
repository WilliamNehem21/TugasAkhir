


FULL-LENGTH ARTICLE
Translation from Arabic speech to Arabic Sign Language based on cloud computing

Mahmoud M. El-Gayyar *, Amira S. Ibrahim, M.E. Wahed

Department of Computer Science, Faculty of Computers and Informatics, Suez Canal University, Ismailia, Egypt

Received 29 August 2015; revised 13 April 2016; accepted 24 April 2016
Available online 18 May 2016

Abstract People with special-needs face a variety of different challenges and barriers that isolate them from their surroundings. Nowadays, several assistive technologies have been developed to reduce many of these barriers and simplify the communication between special-needs persons and the surrounding environment. However, few frameworks are presented to support them in the Arabic region either due to the lack of resources or the complexity of the Arabic language. The main goal of this work is to present a mobile-based framework that will help Arabic deaf peo- ple to communicate ‘on the go’ easily with virtually any one without the need of any specific devices or support from other people. The framework utilizes the power of cloud computing for the com- plex processing of the Arabic text. The speech processing produced a cartoon avatar showing the corresponding Egyptian Arabic Sign Language on the mobile handset of the deaf person.
© 2016 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).



Introduction

Special-needs persons suffer from discrimination and obstacles that limit their participation in different societal activities. Due to the lack of proper communication, they are denied from their rights to live independently, to work, or even to move freely.

* Corresponding author. Tel.: +20 10 23404433.
E-mail addresses: elgayyar@ci.suez.edu.eg (M.M. El-Gayyar), amira_ibrahim@ci.suez.edu.eg (A.S. Ibrahim), mewahed@yahoo. com (M.E. Wahed).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
A large number of special-needs cases live in developing countries, regularly neglected and in extreme poverty. This research focuses on people with hearing and speaking impair- ment (deaf and dumb) in Egypt. According to last official statistics, there are 360 million people worldwide have dis- abling hearing loss [1] where more than 3 million of them are located in Egypt [2]. Most of the Egyptian deaf lack the ability to read or to write the standard Arabic language. There- fore, the only way of communication among them is the Ara- bic Sign Language (ArSL) that is not understood by hearing people. This fact has a great impact on the social life of the Egyptian deaf community that comes to be quite isolated and has large difficulties to gain social experiences and relationships.
Recently, there has been a revolutionary change in approach, globally, to close the gap and guarantee that per- sons with handicaps enjoy the same standards of equality


http://dx.doi.org/10.1016/j.eij.2016.04.001
1110-8665 © 2016 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

296	M.M. El-Gayyar et al.


and rights as everyone else. The information technology plays an important role in the new methodology where several assis- tive systems have been developed to support deaf communities around the world.
For example the author in [3] has presented a concept that uses a pattern recognition algorithm to analyze the acoustic environment around the user, identify critical signature and give an alert to the user where an event of interest happened.
Another example presented in [4] makes the usage of low- cost portable gloves, these clothes contain sensors to collect data about hand gestures, and the collected data help to iden- tify the ArSL signs performed by the deaf person.
However, the situation was quite different in the Arabic region where there is no real consideration of governments to the disabled people. For example, as marked by the report in [5], the Egyptian government limits an ArSL translation to News Programs on Television for only 10 min up to 7 h per week. Also, the administration does not allow deaf persons to get legislative documents translated into ArSL. Definitely, the disabled community is still out of the administration inter- ests and future plans.
Even more, the assistive technologies directed to deaf communities in the Arabic countries are quite a few and limited. Besides the government ignorance, the complexity of the Arabic language and the ArSL is the main reason for this limitation. Arabic language is much wealthier than English and is considered as one of the most complex nat- ural language [6]. Regarding ArSL, a standard dictionary can be found in [7] that contains 1600 of the basic and most common signs. Nonetheless, each country has its own modified version of this dictionary that holds new signs for its colloquial language and modified signs for most of the existing words. Even more, you can find for the same word a different sign in distinct cities of the same country [8].
As a conclusion, deaf individuals in developing Arabic countries can communicate with normal people only through an ArSL human interpreter who is very hard to find and in all the cases will break the conversation privacy. To help the Arabic deaf community to integrate with the society and to communicate easily with others, there is a big need to develop an automatic machine-based interpreter that can convert from Arabic speech to ArSL and vice versa [9]. The work in this paper focuses on the first direction, the Arabic speech to the ArSL, while the other direction has been discussed in [10].
In this paper, we introduce a mobile-based application that plays the role of an intermediate interpreter between normal and deaf persons, that helps to achieve a seamless communica- tion on-the-go at almost real time.
However, the processing power of a mobile device may be inefficient for the conversion process. To solve this issue, we have connected the application to a cloud-based framework where we can delegate all heavy work to powerful resources on the cloud.
The rest of this paper is organized as follows. Section 2 shows some of the related works concerning ArSL translation systems. Section 3 describes the system architecture of the cloud-based framework. In Section 4, experiments and results
are presented. Finally, Section 5 concludes the paper and dis- cusses the future work.

Related work

There exist several attempts to convert Arabic speech to ArSL. In general, the conversion process has two main phases. First, the Arabic speech is transformed to text, and then in the sec- ond phase, the text is converted to its equivalent ArSL. Some existing work tries to convert directly from the Arabic speech to ArSL depending on third party tools for speech recognition. In this section, a review of the most popular Arabic speech to ArSL conversion systems is presented.
Authors in [11] introduce an Arabic speech to ArSL intelli- gent conversion system. The system is based on a knowledge base to resolve some of the Arabic language problems (e.g. derivational, diacritical and plural). According to the authors’ evaluation, the system has accuracy up to 96%. Conversely, the system has two main problems. It is a desktop application and the system output is a sequence of still images without any motion. Therefore, it is more suitable for educational pur- poses, not for the on-to-go real-time translation.
The Research Lab LaTICE in Tunisia is currently working on the Websign project [12,13]. This project tries to develop a web-based interpreter of the ArSL. Websign is based on the technology of avatar (animation in the virtual world). The input of the system is a text and the output is an interpretation in sign language. This interpretation is built through a lexicon of word and signs. Utilizing the developed web service, a mobile application (MMSSign) has been introduced in [14] to convert text messages to its equivalent sign language in a form of an Multimedia Messaging Service (MMS) to help hearing persons to communicate with deaf people. The MMSSign can support sending discrete information on long periods (e.g. news, or weather status) but not for real-time translation.
In [15], Halawani introduced a desktop application to con- vert Arabic speech to ArSL. First, the human speech is con- verted into text through a speech recognition system, and then the text is converted to an avatar-based representation of its equivalent ArSL. The avatar from the author’s context is simply the graphical representation of the sign (just an image). The translation process depends on a database of the ArSL signs. However, there is no provided information about how the size of the database will be limited and how the differ- ent dialects of the ArSL can be handled. As an improvement to this work, a system called ABTS-ArSL has been presented in [16].
The ABTS-ArSL is again a desktop application that is based on Microsoft Window 7 Speech Recognition Engine. The last two systems are machine-dependent and need a PC or a laptop to access them.
Al-Khalifa in [17] announced a mobile phone translator system to convert typed formal Arabic text to ArSL using a sign-avatar. The application depends on two processes (trans- lation and presentation), and a sign dictionary. In the transla- tion process, the sliding window technique is used to handle the occurrence of compound sentences. The system’s presenta- tion process relies on the Mobile 3D Graphics API (M3G) to animate a 3D signer avatar. For the sign dictionary, a local

Translation from Arabic speech to Arabic Sign Language	297
























Figure 1	Framework architecture.























Figure 2	ERD diagram for the data layer.

database has been used in the form of a set of resource files. The system evaluation was based on the System Usability Scale (SUS) [18] of only five users and claimed an average of 91%. However, the proposed system has many limitations. First, the decision to depend on a local database, on a mobile device with limited storage, has limited the number of signs that can be stored in the database. Secondly, the application has 50 characters as the maximum length of the translated sen-
tence. Third, it deals only with the formal Arabic as an input. Finally, the translation process does not consider the sentence semantics or the different aspects of the ArSL.
The main goal of this research work was to provide a close to real-time seamless communication between hearing and deaf people to enhance their social life activities. To achieve this goal, we have developed a three-layer framework that we will be discussed in detail in the following section.

298	M.M. El-Gayyar et al.


Table 2  Example of signs that represent compound sentences.
Sign	Pattern
Interface layer that represents the mobile application the user needs to access the system’s different functionalities. Each of these layers will be discussed in detail in the following subsections.




















Framework architecture


ﺍﻟﺴﻼﻡ ﻋﻠﻴﻜﻢ








ﻭﺯﺍﺭﺓﺍﻟﺸﺌﻮﻥ ﺍﻻﺟﺘﻤﺎﻋﻴﺔ
Data layer

This layer embodies the system’s database that holds the video representation of the ArSL signs. There were three different options to produce the sign videos. The first solution was the motion capture where the motion of a person or an object is recorded and then we use this information to animate a digital character model either in 2D or 3D [19]. This solution needs special hardware requirements (e.g. mocap plug-in and a Kinect sensor). The second option was real video-recording of a person that performs the sign; this requires finding the per- son who is an expert in ArSL and has no problem to give you a lot of his time to record a large number of videos. The last option is called synthetic animation [20] which enables users to exploit a sign-editor to create a video representation of signs via a 3D or a 2D avatar. Our videos mainly use two tools: the Sign Smith Studio from Vcom3D [21]; and the eSIGN author- ing kit [22].

In this section, we designed a description of a three-layered
architecture to help the deaf community in Arabic countries in general and in Egypt in specific. As shown in Fig. 1, the first layer from the bottom is the Data Layer where the avatar data- base resides. The second layer is the Service Layer that holds a set of third-party and our own web-services that are needed to achieve the system functionality. Finally, the third layer is the
In ArSL, a sign can represent a letter/digit, a word, or a compound sentence (pattern). Each of these components has been stored in a separate table in the system’s ERD as shown in Fig. 2 while the sign videos are kept in the ‘‘Signs” table. The sign for a word or a pattern can have a different represen- tation from one location to another. So, the location should be connected to the sign representation using the ‘‘Relationships”



































Figure 3	Sequence diagram of the Arabic speech to ArSL conversion process.

Translation from Arabic speech to Arabic Sign Language	299

The service layer

The service layer comprises the core processing functionalities of the system. In general, the system provides two main oper- ations to its users. First, it converts the speech of a normal per- son to ArSL. Secondly, it translates the text encoded in an image to ArSL. The first step in each of these scenarios is extracting the text that needs to be translated to ArSL.
For text recognition from an image, we have developed an optical character recognition (OCR) that utilizes the mobile’s camera to capture the image. It analyzes the image to extract text from it. However, for the speech recognition, the system relies on the Android speech engine [23] that provides a speech to text feature. For some languages, the speech engine sup- ports offline speech to text conversion which is not the case for the Arabic language. Arabic speech should be streamed in the background to the ‘‘Google Speech Service” where the voice will be converted to text and sent back to the application as illustrated in Fig. 3.
The obtained text and the current user’s location were col- lected through the GPS module or manually specified by the user, then sent to our ‘‘Cloud Service”, web service deployed on a cloud instance, where the translation to ArSL is actually performed. To simplify the conversion process, the text first needs to be preprocessed through the Microsoft Arabic Toolkit Service (ATKS) which will be explained in Section 3.2.1.
The last step before obtaining the signs’ videos is the tok- enization of the preprocessed text. First, a sliding window search is performed to identify compound sentence (patterns)

for example
ﻋﻠﻴﻜﻢ
ﺍﻟﺴﻼﻡ. If a pattern has been found, it is




Figure 4	Text preprocessing steps.



table. A sign with a universal representation has a special loca- tion id (zero in our case).
Currently, we have 588 signs in our database selected from different categories as shown in Table 1. The signs have been selected from the dictionary in [7].
Patterns are sentences that contain two or more words and have only one sign in ArSL, we have already collected 35 dif- ferent patterns, and two examples are shown in Table 2.
One important point that we have to mention is that ArSL is quite different from spoken Arabic language [8]. ArSL sen- tence does not follow the same order as a spoken language. There is no singular, dual or plural agreement in ArSL. More- over, ArSL does not make use of tenses as in spoken Arabic language. Tenses are specified at the beginning of the commu- nication and only changed when a new tense is required. Another difference between ArSL and Arabic standard lan- guage can be found also in [8].
As a final remark, the system’s database is flexible and open for modifications. The system provides a web-based interface that modifies existing signs or adds new ones. For this purpose, additional tables have been added to the database manage- ment process. Unfortunately, this is not shown in diagram due to space limitation.
inserted as a single token in the tokens’ queue. Secondly, each identified named entity in the preprocessed text (proper names, locations, or organizations) is broken down into characters that are inserted into the tokens’ queue as separated tokens. Finally, the remaining words are inserted as individual tokens in the queue in their accurate order. For each token, the corre- sponding sign video is retrieved from the database according to the given location information. Then, the videos are merged and streamed back to the mobile application where the deaf user can watch the final translation of the recorded speech. If a word cannot be found in the avatar database, a notifica- tion is sent to the database admin and the word is translated as discrete letters.

Text preprocessing
The Microsoft Advanced Technology Lab in Cairo has pro- moted the Arabic Toolkit Service (ATKS) [24] as a suite of Natural Language Processing (NLP) components that target the Arabic language. The suite contains a spell-checker to detect and correct misspelled words; a morphological analyzer (SARF) that can analyze an Arabic word and extract its root, pattern, stem, part of speech, prefixes, and suffixes; a dia- critizer that can provide an accurate diacritization for Arabic text; a named entity recognizer (NER) to classify named words into persons, locations and organizations categories; a collo- quial to Arabic converter that translates the Egyptian collo- quial text into modern standard Arabic; and a part-of-speech (POS) tagger. The ATKS describes these modules as web ser- vices hosted on Windows Azure [25].

300	M.M. El-Gayyar et al.

(a) Guest Screen	(b) Main Operations

(c) OCR Operation	(d) Avatar Translation
Figure 5	User interface.



We depend on three of these modules to preprocess the input text before applying the ArSL translation. The prepro- cessing steps are shown by example in Fig. 4.
Initially, the spell-checker module is used to enhance the quality of the text. The auto-correction feature is utilized to correct common mistakes in standard Arabic words without

Translation from Arabic speech to Arabic Sign Language	301


user effort. As shown in the given example, the module has fixed two errors related to the hamzah ﺃ and tah marbutah ﺓ rules.
This work focuses mainly on the Egyptian deaf community. Hence, most of the input will be in the Egyptian colloquial text. Using the colloquial format for the translation process is inefficient as it will result in a very large set of words and an unbounded avatar database. Therefore, the second step of text preprocessing is connected to the colloquial converter module that translates Egyptian colloquial text into standard Arabic text. Additionally, it handles mixed text that combines modern standard Arabic and colloquial text by translating only the colloquial words and selecting the best translation based on the context.
The vast majority of people, organizations and locations names have no specific signs in ArSL. Generally, they are translated as separated letters. In order to achieve this in our translation, these special entities need to be recognized in the text. The third text preprocessing step is responsible for retriev- ing named entities from standard Arabic text through the NER module of the ATKS. As illustrated in Fig. 4, step three of this specific example, three entities have been identified and classified into three classes: a person name, a location, and an organization.
Arabic NER can extract foreign and Arabic names, loca- tion entities such as cities, countries, streets, squares, as well as organizations such as sports teams, political parties, compa- nies, and ministries.

The interface layer

This layer represents the communication channel between users and the overall system. It affords a user-friendly mobile application (Android [26]) that helps users to easily access the different system’s functionalities.
Guests (unregistered users) can only access the speech-to- ArSL translator, the main functionality of the system, as demonstrated in Fig. 5(a). During registration, users have to specify the location information and whether the system should automatically update the location data whenever the GPS module is activated or not. This information helps the system to select the most appropriate sign during the transla- tion process as explained in Section 3.2.
Registered users can access four different operations given in Fig. 5(b). The OCR operations can be applied to existing photos, or the user can use the mobile’s camera to capture an instance image (see Fig. 5(c)). The final result of the trans- lation process is represented by an animated avatar along with the original Arabic sentence as illustrated in Fig. 5(d). As indicated in Fig. 5(b), the application provides an additional feature related to the barcode reader that is not covered in this paper and will be considered in a separate one.

Hypothetical sample usage scenario

the paper. But, the address was not clear for the driver and he started to ask questions that are the deaf could not hear, the deaf person gets his mobile out and used our program to translate the driver speech into ArSL and started to replay with simple signs like yes/no.
The driver dropped the deaf person on the mentioned address. Unfortunately, the deaf person discovered another problem. There are three similar buildings on the street and he cannot recognize which one is the new club. Again, he used our program to scan the text announced on each building and he was able to recognize his destination as the program has translated the titles into ArSL.

Evaluation

The accuracy of the implemented system is mainly based on two factors:

The accuracy of the Google Speech engine that is responsi- ble for converting from Arabic speech to Arabic text. According to our evaluation, the accuracy of short Arabic sentences is between 77% and 84%. Short sentences here means that a sentence with two to four words. If the user starts to speak fast or in continuous long sentences, the accuracy will be degraded to 70% or less. Thus, to get an acceptable result, users have to speak in moderate speed and in short sentences.
The completeness of the sign database: The current data- base doesn’t include the whole set of ArSL sings. We tried to solve this problem by making our database easily extend- able through a web-based interface that allows admin to add new signs to the database. If an Arabic word can’t be found in the database, the program translates it as discrete letters to help the deaf person to predict the meaning and then the system sends an email to the admin asking him to add the missing word in the database.

To evaluate the system efficiency and user satisfaction, a performance and usability tests have been conducted.
For the performance evaluation, we have deployed our ser- vices on an instance of a private cloud that has two virtual CPUs and 8 GB or RAM. Regarding the client side, we have used a Samsung S4 smartphone with a Wi-Fi connection to an ADSL line with 2 Mbps speed. Then, we have selected a test dataset that represents different types of input as shown in Table 3. For each input, the average waiting time (of three

Table 3  Performance evaluation.
Input	Type	Avg. waiting time (s)
Pattern	4.48
Word	3.72

Assume a deaf person who is not educated and cannot read or write standard Arabic language. He/she wants to meet his/her friends in a new club for the first time.
Standard Arabic Sentence
Egypt Colloquial Sentence
Noun (letter-based
4.84

5.52

So, he/she asks one of his/her friends to write down the address on a paper. Then, he/she took a taxi and gave him
translation)	4.2

302	M.M. El-Gayyar et al.












Figure 6	SUS users.



runs), from starting the speech until the beginning of the trans- lation’s video, has been recorded. As concluded from the results, the average waiting time ranges from 3.7 to 5.5 s. This is quite acceptable compared to the average quality of the Internet in Egypt.
To measure the user satisfaction, we assigned a group of users some scenarios to evaluate our program. Then we used the System Usability Scale (SUS) that is a scale-based ques- tionnaire made to grade the usability of systems. The question- naire contains ten questions and its final result is represented as a score between 0 and 100, where 100 indicates ‘‘best” usability [27]. Initially, the questionnaire has been translated into both standard Arabic and ArSL. Then, it has been sub- mitted to 68 users from the ‘‘school of deaf and dumb” and ‘‘El-Fagr El-Gded” association for deaf persons located in Ismailia, Egypt. The selected users include deaf-specialized teachers, deaf persons and normal people scattered as shown in Fig. 6.
Around 10% of the users, all from the deaf community refused to participate in the testing. These users seemed to have problems with dealing with strangers and trying new things. The other users’ SUS score ranged from 75 to 84 with an average of 79.8 and this is an acceptable score. During the testing process, we collected all clients’ feedback. Noteworthy ones are below:

The deaf community: they don’t like the avatar representa- tion; they prefer real human videos that will show better facial expressions and body motions. Additionally, they would like to be able to upload their own signs to the data- base through the mobile’s camera.
The normal people: slow and modulated discourse is obliga- tory with accurate is a must. (As a remark, this is not a problem of the application itself. It is related to the Google speech engine used for the conversion process.)


Conclusion and future work

This paper has presented a cloud-based framework that has been developed to serve the community of Arabic deaf people. The primary goal is to assist deaf individuals to communicate effectively with the great public in order to gain better social life.
The framework utilizes the computational power of cloud computing and a set of third-party services to translate Arabic speech and extract Arabic text from different sources into
ArSL. It considers the complex nature of the ArSL including the existence of different signs of the same word according to user’s location. As well, it utilizes a text processing service to enhance the Arabic text for a better translation process. Although the presented framework appears complex, this com- plexity is not delivered to its users. Users need to deal with a simple mobile application that can be used on-the-go for real-time avatar-based translation.
This application has passed two evaluation tests: A perfor- mance test with an average waiting time from 3.7 to 5.5 s; and a usability test with a 79.8 average SUS score that discloses an acceptable application.
Our users’ feedback has directed our current and future plans in the following directions. Currently, we are working on a sign database that is based on a real human representa- tion. The idea is trying to use GIF animated images instead of videos to reduce to the size of the database, then, we will check the possibility of moving the database to a local one on the mobile phone to reduce the network streaming. In the foreseeable future, a new feature will be added to the applica- tion to allow deaf users to exploit the mobile’s camera to record and upload their own signs representation to the data- base. Finally, we are currently working on extending the sys- tem to provide the translation, the other way around, from ArSL to Arabic speech.

Acknowledgment

The authors would like to thank their undergraduate project students for helping in the development of the system: Amr Ashraf, Mohamed Farouk, Mohamed Gamal, Mohamed Nageh, Mohamed Usama, and Shady Saied.

References

WHO. Deafness and hearing loss Fact Sheet 300 n.d. <http:// www.who.int/mediacentre/factsheets/fs300/> [accessed March 5, 2016].
MICT. Deafness and hearing loss Fact Sheet 300 2006. <http://www. mcit.gov.eg/Upcont/Documents/Golden Book Final2007211155321. pdf> [accessed March 5, 2016].
Mielke M, Grunewald A, Bruck R. An assistive technology for hearing-impaired persons: analysis, requirements and architec- ture. In: Eng med biol soc (EMBC), 2013 35th annu int conf IEEE; 2013. p. 4702–5.
Sarji DK. HandTalk: assistive technology for the deaf. Computer (Long Beach Calif) 2008;41:84–6.

Translation from Arabic speech to Arabic Sign Language	303


Global survey report WFD interim regional secretariat for the Arab region (WFD RSAR) global education pre-planning project on the human rights of deaf people; 2008.
Ibrahim S. Arabic language and the importance of morphology; 2003. <http://ccisdb.ksu.edu.sa/files/rep1120000.doc> [accessed
March 5, 2016].
Minisi MN. Arabic sign langauge dictionary; 2015. <http:// www.menasy.com/> [accessed March 5, 2016].
Abdel-Fattah MA. Arabic sign language: a perspective. J Deaf Stud Deaf Educ 2005;10:212–21.
El-Gayyar M, Elyamany HF, Gaber T, Hassanien AE. Social network framework for deaf and blind people based on cloud computing. Comput Sci Inf Syst (FedCSIS), 2013 Fed Conf 2013:1313–9.
El-Gayyar M, Ibrahim A, Sallam A. The ArSL keyboard for android. In: IEEE seventh int conf intell comput inf syst ICICIS’15, Cairo, Egypt. vol. 1; 2015. p. 219–24. doi: ISSN 1687-1103, ISBN 977-237-172-3.
Alfi AEEEl, Basuony MMREl, Atawy SMEl. Intelligent Arabic text to arabic sign language translation for easy deaf communi- cation. Int J Comput Appl 2014;92:22–9.
LaTICE. WebSign: to break the silence of deaf; 2015. <http:// www.latice.rnu.tn/websign/> [accessed March 5, 2016].
Aouiti N. Towards an automatic translation from Arabic text to sign language. In: Inf commun technol access (ICTA), 2013 fourth int conf; 2013. p. 1–4.
Jemni M, El Ghoul O, Yahia N Ben, Boulares M. Sign language MMS to make cell phones accessible to the deaf and hard-of- hearing community. CVHI’07 2007:28–31.
Halawani SM, Zaitun AB. An avatar based translation system from Arabic speech to Arabic sign language for deaf people. Int J Inf Sci Educ ISSN 2012;2:13–20, ISSN 2231-1262.

Halawani SM, Daman D, Kari S, Ahmad AR. An avatar based translation system from Arabic speech to Arabic sign language for deaf people. IJCSNS Int J Comput Sci Netw Secur 2013;13:43–52.
Al-Khalifa H. Introducing Arabic sign language for mobile phones. Comput Help People Spec Needs 2010;6180:213–20, Springer Berlin Heidelberg.
Lewis JR, Sauro J. The factor structure of the system usability scale. In: Proc 1st int conf hum centered des held as part HCI int 2009. Berlin, Heidelberg: Springer-Verlag; 2009. p. 94–103.
Multon F, Kulpa R, Hoyet L, Komura T. From motion capture to real-time character animation. Motion in Games 2008;5277:72–81, Springer Berlin Heidelberg.
Verlinden M, Zwitserlood I, Frowein H. Multimedia with animated sign language for deaf learners. In: Proc EdMedia world conf educ media technol 2005. Association for the Advancement of Computing in Education (AACE); 2005. p. 4759–64.
Vcom3D. Sign smith studio; 2015. <http://www.vcom3d.com/ language/sign-smith-studio/> [accessed March 5, 2016].
Hamburg-University. eSIGN Authoring Kit n.d. <http://www. sign-lang.uni-hamburg.de/esign/> [accessed March 5, 2016].
Google. Android speech engine; 2015. developer.android.com/ reference/android/speech/ [accessed March 5, 2016].
Microsoft. Arabic toolkit service (ATKS); 2015. <http://research. microsoft.com/en-US/projects/atks/> [accessed March 5, 2016].
Microsoft. Microsoft azure: cloud computing platform and services; 2015. <https://azure.microsoft.com/> [accessed March 5, 2016].
Google. Androrid operating system; 2015. <https://www.an- droid.com/> [accessed March 5, 2016].
Brooke J. SUS: a retrospective. J Usability Stud 2013;8:29–40.
