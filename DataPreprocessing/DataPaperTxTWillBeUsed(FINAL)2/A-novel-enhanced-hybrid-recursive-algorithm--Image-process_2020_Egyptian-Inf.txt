








A novel enhanced hybrid recursive algorithm: Image processing based augmented reality for gallbladder and uterus visualisation
T. Singh a, Abeer Alsadoon a,⇑, P.W.C. Prasad a, Omar Hisham Alsadoon b, Haritha Sallepalli Venkata a,
Ahmad Alrubaie c
a School of Computing and Mathematics, Charles Sturt University, Sydney, Australia
b Department of Islamic Sciences, Al Iraqia University, Baghdad, Iraq
c Faculty of Medicine, University of New South Wales, Sydney, Australia



a r t i c l e  i n f o 


Article history:
Received 20 July 2019
Revised 17 October 2019
Accepted 16 November 2019
Available online 7 December 2019


Keywords:
Augmented reality
3D-2D image registration Gallbladder surgery Bowel surgery
Surgical and invasive medical procedures
a b s t r a c t 

Background: Current Augmented Reality systems in liver and bowel surgeries, are not accurate enough to classify the hidden parts such as gallbladder and uterus which are behind the liver and bowel. Therefore, we aimed to improve the visualization accuracy of bowel and liver augmented videos to avoid the unex- pected cuttings on the hidden parts. Methodology: The proposed system consists of an Enhanced hybrid recursive matching and k-parameterization techniques to improve the visualization. In addition, Mean Shift Filter is also added to improve the matching process while image registration. Results: Results proved that, the accuracy is improved in terms of liver and bowel surgeries Visualization errors about
0.53 mm and 0.22 mm respectively. Similarly, it can produce 2 more frames/sec compared to the current system. Conclusion: The proposed system worked towards the visualization of gallbladder and uterus while liver and bowel surgeries. So, this study solved the visualization issues, which are caused by neigh- bouring and hidden parts.
© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

Liver and bowel surgeries are one of the high-risk and challeng- ing surgeries due to their deformation nature. Furthermore, these are attached to other organs such as bowel is situated on top of uterus and liver is positioned on the gallbladder. The traditional methods (Fig. 1.a.) such as minimal invasive surgery(MIS) and sin- gle incision laparoscopic surgery (SILS) are used by the surgeons but these methods could not produce the acceptable surgical out- comes in terms of visualizing the hidden anatomical structures such as uterus while bowel surgery and gallbladder while liver sur- gery [1].

* Corresponding author at: Charles Sturt University, Sydney Campus, Sydney, Australia.
E-mail address: aalsadoon@studygroup.com (A. Alsadoon).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
The above traditional methods are working mostly based on the rigid transformation methods while image registration process such as iterative closest point (ICP) and an initial manual align- ment of the CT model [1]. These approaches are suitable for the rigid areas only. Furthermore, they are incapable to manage the noise due to tissue deformation, occlusion, classification of organs etc. To overcome these limitations augmented reality has been proven a landmark step in the field of surgery.
AR basically gives 3-Dimensional view of the images by super- imposing real-time images (CT scan images) on the virtual images (captured using the 3D stereo endoscopic camera). The advantages of using AR in this scenario is its 3D view (Fig. 1.c) irrespective of the complexity and rigidity nature of surgical area such as bowel, liver, brain, heart, eye, etc. The challenging thing is, it is very hard for the surgeons to perform the surgery in such areas as areas are sensitive and deformable. Furthermore, surgeons must have to put marks(cuts) on the human body to locate the tumour as part of pre surgery procedure [2]. The advanced technologies like Augmented reality has changed the medical field now a days. However, soft tis- sue deformation and image registration in liver and bowel surg- eries is still the subject of research due to the limitations in





https://doi.org/10.1016/j.eij.2019.11.003
1110-8665/© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. (a) Traditional Surgery (b) Video-Guided Surgery (c) AR guided Surgery [Downloaded from freely available online resources, https://www.reddit.com/ r/woahdude/comments/3tvdnl/neurosurgeon_resident_explores_inside_a_patients/]


finding the structure and location of the tumour and its intercon- nected organs.
We can classify the AR display into three categories which include video-based display, see-through display, and projection- based display. Video-based display superimposes the images that are generated virtually on the video stream. Then the outcome is an augmented 3D model (Fig. 1.b). Projection based display works by superimposing the real time images directly on the projection surface. Similarly, see-through display overlays the virtual images onto the mirror which is reflected to the viewers [3–5].
Soft tissue registration, depth perception and occlusion are challenging tasks in Laparoscopic surgery. Different techniques and algorithms are available which aim to improve the visualiza- tion accuracy, registration accuracy, depth perception and process- ing time of the surgery. One of such algorithm is Hybrid Recursive Matching algorithm (HRM). It results the accuracy about to 0 mm– 5 mm of the registration error, and the processing time is 100 ms per frame [1].
The main purpose of this article is to reduce the registration error, visualization error and processing time in the domain of liver and bowel surgeries. One of the best result from the current studies is Hybrid recursive matching (HRM). It is a latest recursive algo- rithm which includes many new features such as noise reduction, image segmentation, surface reconstruction etc. The HRM algo- rithm consists of block recursive and pixel recursive. We believed that, this algorithm is more reliable to our aim.

Literature review

During our research, we reviewed various journals and anal- ysed. From this analysis, the main issues for AR surgery are local- ization of the tumour, visualization of tumours, organs, nerves and its structures, soft tissue deformations, image registration (complex part of the body such as heart, liver, eye, bowel). Some of those articles are described below.
Haouchine et al. [2,7] developed an image-guided biomechani- cal model to capture the complex deformations occurred by the liver during the surgery. This model adapted the Speeded-Up Robust Features (SURF) descriptor to obtain 3D images. During this process, the accuracy of locating the tumour was measured as 10– 25 mm, which is in the safety margin. However, this accuracy tumour is still quite low. Therefore, this algorithm does not offer further possibilities for improvement.
Peterlík et al. [6] proposed a method to reduce the registration error by using iterative closest point technique along with biome- chanical model. It is constructed by the co-rotational formulation of linear elasticity. This system provides an accuracy of 10 mm in terms of target registration error. It seems bit high error, further- more, there is no information about the tumour location and its structure. Bernhardt et al. [7] presented a new automatic approach
to register and reconstruct the intraoperative CT images with the static endoscopic view (Which is acquired by localizing the endo- scope tip in the volume data). This model is used a 3D rotational C-arm and Zhang’s method, which yields an accuracy of 1 mm in terms of registration error and processing time of 10–15 s. Whilst achieved accuracy is quite high, But the image quality is low. It may increase the visualization error. Hence, it could not help to our goal. Chu et al. [8] introduced a new method called fusion dis- play for AR-based nasal endoscopic surgery (ARNES). This model used the Ray-casting method along with real-time hybrid tracker. It produced the target registration error by 0. 1 mm by 6 fiduciary points, and reduced the registration time by 30 s. However, they used hardware tools such as markers and trackers to improve the processing time and accuracy. Therefore, we do not want to use the markers for our system.
Kersten-Oertel et al. [9] developed the augmented reality neuro navigation system that gives precise patient-to-image registration. This system merges the captured real time images with the pre- operative volume vessels. It displayed the registration error of 3.44, and the calibration & re-projection error is of 2.02 mm. How- ever, robust visualization techniques and rigorous evaluation need to be performed as there is no disruption of workflow. Therefore, this work has no further scope of improvement.
Pessaux et al. [10] invented a method that uses both robotic and AR-assisted hepatic segmentectomy. They created a 3D model by using CT scan and customary software. Then it is processed to Vir- tual Surgical planning system followed by resection pane. This sys- tem gave accurate navigation with a maximum error of 2 mm and time required to obtain AR was 8 min (range 6–10 min). However, system failed to provide high quality images and have a high pro- cessing time as it involved in manual process. Zinse et al. [11] cre- ated a computer-assisted protocol that can show the surgical navigation and in depth structures. It is supplemented by an inter- active image-guided visualization display (IGVD), which transfer virtual maxillary planning precisely. This system presents an accu- racy of 5 mm, and the mean operating time was 4.8 h (40 min). However, this operating time is 60 min longer than the conven- tional approach because of mandibular maxillary occlusion splints. As a result, the combination of algorithms does not bring forth pos- sibilities of enhancements.
Ieiri et al. [12] obtained 3D reconstructed patient images by using 3D viewer software and Multi detector low CT (MDCT) data. Registration is done by using the optical tracking system and body fiducial markers. The accuracy in terms of registration is calculated by FRE named as fiducial registration error. FRE is a method to cal- culate the distances between marker position, TRE (Target registra- tion data) and 3D volume data. However, we are not interested to use trackers and markers. Wen et al. [13] developed a surgical robot system, which is guided by hand gestures and supported by an Augmented reality-based surgical system. This model used

ICP algorithm, which provides an accuracy of 2 mm-3 mm and pro- cessing time of 2–3 s. But this system did not consider other factors which are essential for real time surgery such as occlusions, organ location and structure etc. As a result, the combination of algo- rithms does not bring forth possibilities for our goal. Wang et al.
[4] focused on pose refinement to increase the accuracy in terms of visualization in Oral and maxillofacial surgery. This paper hired ICP algorithm along with bounding box and TLD tracking, which gives us the accuracy of 1 mm in terms of target overlay error and time frame of 3–5 frames/sec. Moreover, this combination helped to identify the location of pixels where surgeons cut. How- ever, the complex image registration process decreases the visual- ization. Therefore, no interest of improving further.
Nosrati et al. [14] proposed a variation method which helps the surgeon to view the both visible and occluded structures. It seg- ments the visible pixels and recovered pixels from occlusion. As a result, it gives the fully structured image as outcome. This model provides accuracy in vessel segmentation as 0. 87 mm. Whilst, the accuracy can be further improved by utilizing the vascular pulsa- tion cues but there are no details about the equations. Wang et al. [3] identified a robust tracking algorithm in an augmented system which relies upon the stereo endoscopic camera. The Accu- racy error of the purposed framework is 1.2398 pixels and the computation time in the acceptable range. However, their comple- mentary characteristics are more robust to movements such as blur and fast motion. Therefore, this algorithm does not offer fur- ther possibilities for improvement. Reichard et al. [1] proposed a new soft tissue registration method by projecting and matching the biomechanical depth of surgical area. This method initially aligns the reconstructed images based on the endoscopic images and then projects onto the biomechanical model. This method helps identify the position, shape and update the deformations. Therefore, we considered this as the best solution among all reviewed journals. We believed that, registration process is first step in this system and it affects the important parameters of sur- gery such as visualization, location, shape, occlusion etc. If image registration is accurate then the next stages will be perfect. The below section provides in detail about this paper.

State of art solution

The below diagram (Fig. 2) illustrates the important features (highlighted in blue colour) and the limitations in the current sys- tem (highlighted in red colour). This model proposed by Reichard et al. [1] and it focused on depth matching of soft-tissue registra-
Intra-operative environment

The 3D images that are taken from the stereo endoscope camera are segmented to identify the organ of interest by using the ran- dom forest(RF) method. These segmented images are taken as input for surface reconstruction method that is Hybrid recursive matching(HRM) algorithm. It consists of two stages include block recursive stage and pixel recursive stage. Both stages will provide the information about three candidate vectors named as start vec- tor, update vector and final vector. The constructed depth map image is created by using these vectors data. These depth map images are combined with camera parameters and processed as an input for the registration process [1].
Image registration process is done by the projective depth matching algorithm. It includes Data association, Biomechanical constraints and physical simulation. This process is called as dynamic registration which helps to update each frame for every small change in the shape and location of organ of interest. The output of this algorithm will generate another Biomechanical model which we named as an Intra-operative biomechanical model (M2). Then both models are merged by using the 3D surface at mosaic method. The required projective depth map will be given as an output at this stage, which is used to create an augmented video [1]. This model provides an accuracy of 0 mm-5 mm in terms of registration error, which is quite acceptable. However, still, we can improve the accuracy by reducing the safety margin from 0 mm to 5 mm to be 0 mm-1 mm.
HRM is a recursive propagation technique to find out the neigh- bouring pixels by using disparity information. Among all the iden- tified similarities. This recursive algorithm selects the best disparity pixels by matching pixel by pixel. The identified pixels are divided into their spatial and temporal coordinate, which helps to reduce the number of mismatch pixels in a given region and then reflected to the endoscopic image. Therefore, a dense dispar- ity map of an image is produced. The work flow of HRM algorithm is depicted in Fig. 3 Table 2.

Block recursive method

It is to find the best match pixel between the two images which are considered as left and right. The spatial candidates should be distributed equally around the selected pixel position on the both images. Then the shape driven displaced block difference is as fol- lows in Eqs. (1), (2), [1].
DBD(d) = X X s (x; y). f (x; y) — f (x + d ; y + d )	(1)

is the biomechanical data of CT scan. The accuracy range of this system is 0 mm-5 mm, and a processing time is the 100 ms frame
x=0 Y=0

per second. This model is divided into two main stages as shown in
with s x y
Z 1 ; if (x; y)insideobject	2

Fig. 2 named as the pre-operative environment and intra-operative environment Table 1.
l( ; ) =
0

where,
if (x; y)outsideobject	( )

3.1. Pre-operative environment

In this phase, patient’s CT scan data is passed through organ mosaicking, and virtual dissection. The main aim of organ mosaick- ing is to remove the noise from the sensor data and partially visible surfaces. Then, this data will be matched with the camera param- eters named as (x, y, z). We could not use this data directly to the surgical site because the location and shape of patient’s internal organs can vary depending on external factors such as patient posi- tioning, deformations during surgery. Therefore, the initial regis- tration will be performed using the physics-based shape matching (PBSM) method. It results preoperative biomechanical model and it named as M1 in Fig. 2.
DBD(d) is Displaced block difference d is updated displacement vector f L is left frame and f R is right frame
M, N are the total number of pixels in the left and right images
respectively.
sl(x; y)is shape driven displaced pixel difference
(x; y) is pixel position on the image.
dx, dy are the gradient values of pixels on the image.

Pixel recursive method

This method is introduced to track the pixel deviations in the image. It is done by the optical flow method by calculating dense displacement pixels between the two consecutive image frames.




Fig. 2. State of Art System (Reichard et al. [1]) [The blue borders show the good features of this state of art solution, and the red border refers to the limitation of it].



Table 1
Hybrid recursive algorithm.

Algorithm: Hybrid recursive matching (HRM) for surface reconstruction Input: Segmented 2D image (left and right)
Output: Refined 3D pose with reduced disparity with enhanced accuracy BEGIN
Step 1: obtain the three candidate vectors by evaluating the current block position with the help of recursive block matching
Step 2: Select a block of pixels from the left and right images with same spatial and temporal coordinates and calculate the displaced block difference by using Eq. (1) & (2).
Step 3: the candidate vector with the best result is chosen as the start vector for the pixel-recursive algorithm.
Step 4: pixels deviation is measured between the frames by using optical flow method with the equation (3) above. Step 5: an update vector is generated by using the equation (4),5 & 6.
Step 6: the final vector is obtained by comparing the update vector (step 5) which is generated at the pixel recursive stage and the original/first vector (step 1) which is generated at the block-recursive stage.
Step 7: Repeat until all the disparities are disappeared then depth map of image will be created. END


To update this pixel’s deviations to the next stage it calculates the update vector d.
The displacement of pixel difference is defined as Eq. (3), [1].
DPD(d ; x; y) = f (x; y) — f (x + d ; y + d )	(3)
d(x; y) = di — DPD(di; x; y).[ux; uy]T	(4)
where, DPD(di; x; y) from the Eq. (3)
di is the gradient value of deviated pixel I on the image.

i

where,
L	R	x	y
ux; uy are threshold values of pixels’ x and y.
Threshold value of x coordinate is presented in Eq. (5), [1].

DPD (di, x, y) is Displaced pixel difference of at the position x, y with the gradient di
f Lis left frame and f R is right frame
(x; y) is pixel position on the image.
dx, dy are the gradient values of pixels on the image

df (x;y) dx
ux = : hdf (x;y)i—1; else
(5)

Then the update vector d (x, y) is presented as Eq. (4), [1].	Threshold value of y coordinate is presented in Eq. (6–8), [1].




Fig. 3. Flowchart of HRM.





Table 2
Enhanced HRM algorithm.

Algorithm: Hybrid recursive matching (HRM) for surface reconstruction Input: Segmented 2D image (left and right)
Output: Refined 3D pose with reduced disparity with enhanced accuracy BEGIN
Step 1: Get the Stereo endoscopic images from the camera
Step 2: Now get the start vector which is basically the smallest DBD. We get the smallest DBD with the help of block recursive method Step 3: We get one updated vector by comparing both start vector and update vector from pixel recursion method
Step4: Now use k optimization technique by calculating the difference of displacements of blocks and displacements of pixels by Eq. (10). Step 5: The mean shift filter method is applied to estimate and remove further disparities.
Step 6: Now the depth map is constructed by selecting the best matching image among all iterations. END


df (x;y) dy
uy = : hdf (x;y)i—1
(6)
The pseudo code of the state of art algorithm

dy

where,
; else
Flowchart for Hybrid recursive matching(HRM)

Proposed system

f (x, y) is the original image (previous frame) pixels positions [1].
From our analysis, the main factors for AR system are process- ing time, accuracy in terms of registration error, depth perception
in terms of depth mapping, visualization error due to the occlu-

df (x; y)	f (x + 1; y) — f (x — 1; y)
≈
(7)
sions and interconnected organs etc. Among all the papers we

dx df (x; y)
dy	≈

2
f (x; y + 1) — f (x; y — 1)
2	(8)
selected Reichard et al. [1] is the relevant article and it is possible to improve to achieve our goal.
Our proposed solution will consider all the useful features for improving the depth view from the solution proposed by Reichard

et al. [1]. It provided an accuracy range 0 mm – 5 mm in terms of registration error. In order to increase the visualization of surgical area and identification of hidden structures, it needs in depth vis- ibility of organs. In addition to the existing features, we chosen the k-parameterization and mean shift filter methods which are pro- posed by the Trucco et al. [15]. This paper introduced the epi- polar constraints to further remove the disparity pixels. We adapted the above two methods and implemented those tech- niques along with existing HRM algorithm. The below diagram Fig. 4 represents the proposed system features in the green rectan- gles. It consists of two stages called Pre- operative and Intra- operative environments.




Preoperative environment. In this phase, similar to state of art solution (Fig. 2) patient’s CT scan data is processed through organ mosaicking to remove the noise. This data is aligned with the camera parameters as initial registration process by using physics-based shape matching (PBSM). It still has the disparities and need to perform further refinement and verification. Therefore, this preoperative Biomechanical model is considered as partial reg- istration and named as M1. It is saved to utilize at intra-operative stage.
Intra-operative environment. The 3D images that are taken from the stereo endoscope camera are segmented and recon- structed by using the random forest(RF) method and HRM respec- tively. HRM consists of two stages such as block recursive stage and pixel recursive stage. Each stage is the combination of three vectors called start vector, update vector and final vector [1]. As a result of these two stages the output will be a constructed depth map of an image. In addition, we applied k- optimization and mean shift filter to the update vector generated at the pixel recursive stage [15]. These techniques are helpful to increase the pixels matching accuracy by following previous pixel as reference pixel to the next matching pixel. Then the depth map is constructed and named as biomechanical model 2. We named this process as surface reconstruction enhanced HRM algorithm and shown in Fig. 4.
The resulted depth map is projected onto the intra-operative surface for depth matching with biomechanical model of the cur- rent frame. While matching process both the target and reference frames should lie on the same line. The deviations, displacements and boundary conditions are tracked and updated to the update vector. The final outcome of this phase is dynamic registration. Both the pre-operative and intra-operative biomechanical models are aligned by using 3D mosaic method. The aligned models are




Fig. 4. Proposed AR system using enhanced HRM algorithm [The blue borders show the good features of this state of art solution, and the green border refers to the proposed enhancement].

projected on to the soft tissue surgical area for the accurate regis- tration process.
From our analysis on block recursive, pixel recursive and update vectors, we can get the best update vector by calculating the differ- ence between the displacements of blocks (DBD) and displace- ments of pixels (DPD). Displacements of block equation (1) gives the deviated pixels in the selected block. Similarly, DPD (Eq. (3)) describes the deformed pixels in the given image with its gradient values. Therefore, the difference between DBD and DPD reveals the changed pixels information and it is named as k-optimization. Fur- ther, these pixels are checked for the unknown disparities by add- ing mean shift filter to the final equation.
To refine the disparity pixels from the optimized image a mean shift filter is applied. It calculates the mean value of each pixel by checking its neighbouring pixels with in the specified range.
The mean shift filter of given image I by the Trucco et al. [15]. Is defined as equation (9).
Flowchart for proposed algorithm(EHRM)

The Hybrid recursive method consists of two methods called block recursive and pixel recursive. This pixel recursive method is highly reliable on the gradients of the image pixels. Each method drives three candidate vectors named as start vector, update vec- tor, final vector. Here, start and final vectors are just initialization and selection of best match respectively. The update vector is an important vector which plays major role in it. The existing HRM updates the vector with its gradients only. On the other hand, Enhance the Hybrid recursive algorithm(EHRM) generates the update vector by using difference between DBD and DPD. Further- more, we added mean shift filtering technique to our enhanced HRM. This algorithm increases the matching accuracy and registra- tion accuracy. Therefore, the visualization error also reduced by iterating the above procedure and the flowchart of EHRM is pre- sented in Fig. 5.

Im =
M
x=1

Y=1 N
k((x — xM)+ (y — yN)).	(9)
h


Results

where,
Im is mean shift filter of image I.
M, N are the data points position with the length of h. X, y are dimensional data points on the image
H is the edge length from the selected pixel k is the window function
xM and yN are the position of pixel from disparity pixel with radius of h.
From the Eqs. (1), (3) and (4), the update vector is modified as below. It is the difference between the DBD and DPD is described below as Eq. (10).
di is replaced by the DBD (d) to get the deformed pixels from the reference frame to target frame.
k(x; y) = DBD(d) — DPD(di; x; y)	(10)
The above equation is extended by us as below Eq. (11) by
inserting Eqs. (1) and (3) from the state of art system.

Matlab R2017b was used for the implementation and simula- tion of the proposed model and current model. We have collected
10 Laparoscopic Cholecystectomy surgery sample videos and 8 endoscopic and CT images from different age groups and gender that underwent Laparoscopic Cholecystectomy surgery. The length of videos varies from 10 to 20 min. Comprehensive samples of dif- ferent age group, gender and weight are taken during this process. In addition, we classified these 10 samples into two sections, it includes 5 samples of liver surgery (to visualize the gallbladder) and 5 samples of bowel surgery (to visualize uterus) and in each image has to visualize its neighbouring organs to the surgeon. All the videos used during this research are taken from freely available online resources such as YouTube channels for medical students. The image frames are extracted using Mat lab, and all the frames are considered to generate AR video.
The result is compared to the segments of the images from pre-

M	N	operative data for the navigation of gallbladder during liver sur-

k(x; y) =	sl(x; y). f L (x; y) — f R x + dx; y + dy
x=0 Y=0
— f L(x; y) — f R x + dx; y + dy	(11)
where,
DBD(d) is Displaced block difference,
DPD (di, x, y) is Displaced pixel difference of at the position x, y with the gradient di
d is updated displacement vector
f L is left frame and f R is right frame
M, N are the total number of pixels in the left and right images respectively.
sl(x; y)is shape driven displaced pixel difference
(x; y) is pixel position on the image.
dx, dy are the gradient values of pixels on the image.
To obtain the disparity free update vector, we added mean shift filter of resulted image is added to the k- optimization. Which is proposed as modified update vector in the HRM algorithm and named enhanced HRM. It is described as Eq. (12),
Ek(x; y) = k(x; y) + Im	(12)
where k(x,y) is showed in Eq. (11), and Im is presented in Eq. (9).
Now the above Eq. (12) gives the required updated update vec- tor with fewer disparities and high accuracy. Usually, these dispar- ities occur when the images (left and right) are aligned with the organ of our interest which are similar. So refine this disparities, it needs high depth image, which is directly proportional to accu- racy. So, for removing disparities, we proposed a new Eq. (12).
gery prosthetic as shown in Fig. 6(a) and 6(b) respectively. The final proposed solution result is shown in the Fig. 6(c). Here the red line is specifying the boarder of selected organ of interest. It is helpful to identify the structure at real time. Furthermore, it helps to avoid unexpected cut on the neighbouring organs.
Samples were collected from various age groups of males and females, who underwent liver and bowel surgery. We imple- mented and compared our current best solution and the proposed solution for the 10 sample videos (five livers and 5 bowel). The sample results are reported in the below as tables from 3 to 12. Furthermore, each sample is tested for different stages named as image registration, image overlay, if patient moves and if the sur- gical instrument moves. Accuracy is calculated in terms of visual- ization error, it is referred as the difference between the projected image and the original image pixel gradients. The pro- cessing time is the number of frames can be generated in a second. A total of 10 tests with 4 different cases have been conducted. The results have been calculated by taken the average for all test cases in different scenarios and presented in the Tables 3 to 12 below. Each table represents the visualization error and processing time of both systems in four different stages for the particular sample. Tables 3–7 are showing the liver surgery results and Tables 8–12 are showing the bowel surgery results. For better reader under- standing these results are shown graphically which are Figs. 7 to
10. Fig. 7 & Fig. 8 represents the average of visualization error and processing time of liver surgery respectively. Similarly, Fig. 9 & Fig. 10 represents the bowel surgery visualization error accuracy and processing time.




Fig. 5. Flowchart of Proposed Enhanced Hybrid recursive algorithm.



(a)
(b)
(a) Initial image (b) Enhanced HRM image (c) Final proposed image
(c)


Fig. 6. Implementation Sample of the Proposed System.


Discussion

Results showing that there is significant improvement in the visualization accuracy and processing time between the current best solution and the proposed solution. We can observe the differ- ence and improvement from the results Tables 3 to 12. As it is clearly proven that our purposed algorithm helps to improve the accuracy in liver surgery about 0.52 mm and processing time have been improved by two frames/sec. Similar to liver surgery, the accuracy in bowel surgery also improved about 0.22 mm and pro- cessing time have been improved by three frames/sec. These sam- ples are tested and measured using Matlab.
The enhanced HRM algorithm has been simulated in MATLAB for both systems with the 10 sample images and videos collected from an online database. Accuracy of visualization error and pro- cessing time are measured by using the mat lab built in functions and image processing methods. The results were presented for both the current best solution and the proposed solution in the form of Tables 3 to 12 and graphs (Figs. 7 to 10). From the graphs, Fig. 7 and Fig. 9 it is proven that bowel images have the more visu- alization error compared to the bowel samples. Here, we found
that, the shape and length of the organ of interest also affects the accuracy.
The major findings of this research is that, among all four stages visualization error of accuracy is quiet high while image registra- tion and low in some cases. For example, high in sample 1(Table 3), sample 3 (Table 5) & sample 7 (Table 9) and low in samples 2 (Table 4) & sample 8 (Table 10). These results are different to other stages and other samples. These differences in accuracies are caused by the sample variations such as heavy fat areas and blood flow, which effects the gradients of the image. Furthermore, for every recursion the enhanced HRM is calculating the pixel’s devia- tions between the reference and selected image. Similarly, If instruments move, then algorithm have to figure out the deviation pixels at the instrument area on the organ of interest and then it has to enable with in the update vector. If the movement is faster and many instruments used, then there is chance of increase in the visualization error. It is observed from the sample 2 (Table 4).
Our proposed model improved the registration and visualiza- tion accuracy by its following features. Those are block recursive method, pixel recursive method, update vector and dynamic regis- tration process. Furthermore, the mean shift filter is also helped to

Table 3
Accuracy and processing time results for liver surgery (Sample 1: Adult man Age 35 to 45).


Sample

Sample details

Original video	State of Art	Proposed solution

Stages
Adult man Age 35 to 45
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

Liver surgery Gallbladder length
—3mm
Video length- 8 min
Image registration

	
Image overlay

 
If patient moves?

1.90 mm	15 fps	1.8 mm	10 fps





1.86 mm	10 fps	1.61 mm	9 fps




1.58 mm	12 fps	1.11 mm	14 fps




If surgical instrument moves?
1.81 mm	11 fps	1.05 mm	13 fps




Table 4
Accuracy and processing time results for liver surgery (Sample 2: Young man Age 20–25).


Sample 2

Sample details

Original

Current solution	Proposed solution

Stages
Young man age 20 to 25
video
Processed sample
Accuracy by visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

2.1	Liver/gallbladder surgery Gallbladder
length —3mm
Video length-
7 min
Image registration

 
Image overlay

	
If patient moves?

	
If surgical instrument moves?

2.56 mm	10 fps	1.72 mm	15 fps




2.84 mm	10 fps	1.65 mm	15 fps



2.22 mm	12 fps	1.41 mm	17 fps



3.51 mm	11 fps	2.37 mm	11 fps








Table 5
Accuracy and Processing Time Results for Liver surgery (Sample 3: fatty woman age 30–35).


Sample

Sample details

Original

Current solution	Proposed solution

Stages
Woman age 30–
35
video
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

Liver/gallbladder surgery Gallbladder length
—3.3 mm
Video length-
85 min
Image registration

 
Image overlay

	
If patient moves?

 
If surgical instrument moves?

3.58 mm	10 fps	2.72 mm	15 fps



3.45 mm	10 fps	2.31 mm	15 fps



2.88 mm	12 fps	1.41 mm	14 fps




2.99 mm	11 fps	2.11 mm	13 fps




Table 6
Accuracy and Processing Time Results for Liver surgery (Sample 4: woman Age 35–45).


Sample

Sample details

Original

Current solution	Proposed solution

3 Stages
Woman age 35–45
video
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

4.1	Liver/gallbladder surgery Gallbladder length
—2.7 mm
Video length-
5 min
Image registration

	
Image overlay

	
If patient moves?

	
If surgical instrument moves?

2.42 mm	13 fps	2.12 mm	16 fps



2.04 mm	14 fps	1.92 mm	17 fps



1.88 mm	12 fps	1.75 mm	14 fps



2.39 mm	11 fps	2.11 mm	13 fps





Table 7
Accuracy and processing time results for liver surgery (Sample 5: woman age 35–45).


Sample 3

Sample details

Original

Current solution	Proposed solution

Stages
Woman age 35–45
video
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

5.1	Liver/gallbladder surgery Gallbladder length
—3 mm
Video length-
min
Image registration

 
Image overlay

	
If patient moves?

	
If surgical instrument moves?

1.97 mm	13 fps	1.91 mm	17fps




1.84 mm	10 fps	1.81 mm	13 fps



2.38 mm	11 fps	2.19 mm	14 fps



1.39 mm	10 fps	1.31 mm	13 fps










Table 8
Accuracy and processing time results for bowel surgery (Sample 6).


Sample 3 Stages

Sample details

Original video	Current solution	Proposed solution





6.1	Bowel


Image registration
error
error

surgery Bowelr length
—6mm
Video
length- 7 min

	
Image overlay

	
If patient moves?
2.97 mm	16 fps	2.91 mm	18 fps




2.84 mm	10 fps	2.81 mm	13 fps




2.51 mm	17 fps	2.35 mm	18 fps



If surgical instrument moves?
2.39 mm	12 fps	2.31 mm	14 fps


Table 9
Accuracy and Processing Time Results for bowel surgery (Sample 7).


Sample 3

Sample

Original video  Current solution	Proposed solution

Stages
details
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

7.1	Bowel
Surgery Bowel length
—7mm
Video
length- 10 min
Image registration

 
Image overlay

 
If patient moves?

 
If surgical instrument moves?

2.77 mm	17 fps	2.44 mm	20 fps




2.24 mm	20 fps	2.11 mm	22 fps




2.38 mm	17 fps	2.19 mm	18 fps




1.39 mm	10 fps	1.31 mm	13 fps






Table 10
Accuracy and processing time results for bowel surgery (Sample 8).


Sample 3

Sample

Original video	Current solution	Proposed solution

Stages
details
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

8.1	Bowel
Bowel length
—7mm
Video
length- 13 min
Image registration

	
Image overlay

	
If patient moves?

2.79 mm	8 fps	2.22 mm	10fps




1.84 mm	11 fps	2.81 mm	12 fps




2.38 mm	11 fps	2.19 mm	14 fps




If surgical instrument moves?
2.93 mm	12 fps	2.31 mm	14 fps






Table 11
Accuracy and Processing Time Results for Bowel surgery (Sample 9).


Sample 3

Sample

Original

Current solution	Proposed solution

Stages
details
video
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

9.1	Bowel
Surgery Bowel length
—7mm
Video
length- 15 min
Image registration

 
Image overlay

 
If patient moves?

 
If surgical instrument moves?

2.97 mm	13 fps	2.91 mm	1 7fps




2.66 mm	10 fps	2.32 mm	13 fps




2.38 mm	11 fps	2.19 mm	14 fps




2.39 mm	13 fps	2.31 mm	15 fps





Table 12
Accuracy and Processing Time Results for Bowel Surgery (Sample 10).


Sample 3

Sample

Original

Current solution	Proposed solution

Stages
details
video
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)
Processed sample
Accuracy by Visualization error
Processing time (Frames per second)

10	Bowel
Surgery Bowel length
—7mm
Video
length- 11 min
Image registration

 
Image overlay

 
If patient moves?

2.36 mm	10 fps	2.32 mm	12 fps




2.11 mm	8 fps	2.01 mm	13 fps




2.03 mm	11 fps	2.01 mm	14 fps




If surgical instrument moves?
1.88 mm	15 fps	1.88 mm	16 fps



































Fig. 7. Accuracy results in terms of visualization error in Liver Surgery in different stages.



































Fig. 8. Average processing time in Liver surgery in different stages.

































Fig. 9. Accuracy in terms of Visualization Error in Bowel Surgery in different stages.

































Fig. 10. Average processing time in Bowel Surgery in different stages.


estimate the disparities and remove the mismatch pixels in the image. The above combination made our system image matching robust and registration efficient.


Conclusion and future work

The main aim of this system is to improve the image registra- tion and visualization accuracy for soft tissue surgeries such as
liver and bowel. The enhanced HRM algorithm is proposed and implemented in the Matlab. This system improved visualization accuracy over the current system about 0.53 mm and 0.22 mm in the liver and bowel surgeries respectively. Similarly, it is capable to produce extra 2 frames/s and 3 frames/s compared to the exist- ing system liver and bowel samples. The tables and graphs are rep- resented for better understanding of results. Furthermore, the comparison table for state of art (current best solution) and the proposed solution is presented in Table 13.


Table 13
Comparison between the state of art and the proposed solutions.

State of Art	Proposed Solution

Applied Area	Liver Surgery	Bowel and Liver surgery

Type of tissue	It deals with soft tissue in which the error is
caused due to improper surface reconstruction
It deals with the soft tissue in which error is caused by improper visualization.

Features	block recursive, pixel recursive and dynamic registration	K-optimization, mean shift filter along with the state of art features Algorithm	Hybrid recursive matching (HRM) algorithm	Enhanced Hybrid recursive matching (EHRM)algorithm

Equation
XM XN
k(x, y)= PM 0 PN 0 sl(x, y). f L (x, y) — f R(x + dx, y + dy) 

DBD(d) =
x=0
Y=0sl(x, y). f L (x, y) — f R (x + dx, y + dy) 
x=	Y=
 	  
PM1 PN1 k((x—x
)+(y—y )).

— f L(x, y) — f R x + dx, y + dy  +
x=1
Y =1	M1	N1
h

Future research can be focus on other stages of the EHRM algo- rithm, including selecting, matching and rejection. This helps to provide more accurate samples that can be further converted into 3D model when the surgeon required.

Compliance Ethical standard

Funding

No funding

Conflict of Interest

No conflict of interest

Ethical approval

Not Applicable

Informed consent

Informed consent was obtained from all individual participants included in the study

References

Reichard D, Hantsch D, Bodenstedt S, Suwelack S, Wagner M, Kenngott H, et al. Projective biomechanical depth matching for soft tissue registration in laparoscopic surgery. International Journal Of Computer Assisted Radiology And Surgery 2017;12(7):1101–11.
Haouchine N, Cotin S, Peterlik I, Dequidt J, Lopez MS, Kerrien E, et al. Impact of Soft Tissue Heterogeneity on Augmented Reality for Liver Surgery. IEEE Trans Visual Comput Graphics 2015;21(5):584–97.
Wang R, Geng Z, Zhang Z, Pei R, Meng X. Autostereoscopic augmented reality visualization for depth perception in endoscopic surgery. Displays 2017:50–60.
Wang J, Suenaga H, Yang L, Kobayashi E, Sakuma I. Video see-through augmented reality for oral and maxillofacial surgery. Int J Med Rob Comput Assisted Surg 2017;13(2).
Wang J, Suenaga H, Liao H, Hoshi K, Yang L, Kobayashi E, et al. Real-time computer-generated integral imaging and 3D image calibration for augmented reality surgical navigation. Comput Med Imaging Graph 2015;40:147–59.
Peterlík I, Courtecuisse H, Rohling R, Abolmaesumi P, Nguan C, Cotin S, et al. Fast elastic registration of soft tissues under large deformations. Med Image Anal 2018;45:24–40.
Bernhardt S, Nicolau SA, Agnus V, Soler L, Doignon C, Marescaux J. Automatic localization of endoscope in intraoperative CT image: A simple approach to augmented reality guidance in laparoscopic surgery. Med Image Anal 2016;30:130–43.
Chu Y, Yang J, Ma S, Ai D, Li W, Song H, et al. Registration and fusion quantification of augmented reality based nasal endoscopic surgery. Med Image Anal 2017;42:241–56.
Kersten-Oertel M, Gerard I, Drouin S, Mok K, Sirhan D, Sinclair D, et al. Augmented reality in neurovascular surgery: feasibility and first uses in the operating room. International journel of neurovascular surgery 2016:1823–36.
Pessaux P, Piardi T, Mutter D, Marescaux J, Diana M, Soler L. Towards cybernetic surgery: robotic and augmented reality-assisted liver segmentectomy. Langenbeck’s Archives of Surgery 2015;400:381–5.
Zinser MJ, Mischkowski RA, Dreiseidler T, Thamm OC, Rothamel D, Zöller JE. Computer-assisted orthognathic surgery: waferless maxillary positioning, versatility, and accuracy of an image-guided visualisation display. Br J Oral Maxillofac Surg 2013;51(8):827–33.
Ieiri S, Uemura M, Konishi K, Souzaki R, Nagao Y, Tsutsumi N, et al. Augmented reality navigation system for laparoscopic splenectomy in children based on preoperative CT image using optical tracking device. Pediatr Surg Int 2012:341–6.
Wen R, Tay W-LN-B-K. Hand gesture guided robot-assisted surgery based on a direct augmented reality interface. Comput Methods Programs Biomed 2014;116(2):68–80.
Nosrati M, Amir-Khalili A, Peyrat J-M, Abinahed J, Al-Alao O, Al-Ansari A, et al. Endoscopic scene labelling and augmentation using intraoperative pulsatile motion and colour appearance cues with preoperative anatomical 2016; priors:1409–18.
Trucco E, Plakas K. Real-Time disparity maps for immersive 3-D teleconferencing by hybrid recursive matching and census transform. ResearchGate 2001.
