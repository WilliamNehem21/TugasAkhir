Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 317 (2015) 55–69
www.elsevier.com/locate/entcs

Numerical Validation of Compensated Summation Algorithms with Stochastic Arithmetic
S. Graillat,a,1 F. J´ez´equela,b,2 and R. Picota,3
a Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France CNRS, UMR 7606, LIP6, F-75005, Paris, France
b Universit´e Panth´eon-Assas, 12 place du Panth´eon, F-75231 Paris CEDEX 05, France

Abstract
Compensated summation algorithms are designed to improve the accuracy of ill-conditioned sums. They are based on algorithms, such as FastTwoSum, which are proved to provide, with rounding to nearest, the sum of two floating-point numbers and the associated rounding error. Discrete stochastic arithmetic enables one to estimate rounding error propagation in numerical codes. It requires a random rounding mode which consists in rounding each computed result toward −∞ or +∞ with the same probability. In this paper we analyse the impact of this random rounding mode on compensated summations based on the FastTwoSum algorithm. We show the accuracy improvement obtained using such compensated summations in numerical simulations controlled with discrete stochastic arithmetic.
Keywords: floating-point arithmetic, rounding errors, discrete stochastic arithmetic, error-free transformations, compensated algorithms, summation algorithms, CADNA


Introduction
The power of computational resources continues to increase. Exascale computing (1018 operations per second) is planned to be reached within a decade. In floating- point arithmetic, each operation is likely to produce a rounding error. These errors can accumulate and at the end of a computation, the computed result can be very far from the exact one. Moreover, the more operations are performed, the more the accumulation of rounding errors is likely to be important.
As a consequence, it is fundamental to have some information on the numerical quality (for example the number of exact significant digits) of the computed result.

1 Email: stef.graillat@lip6.fr
2 Email: fabienne.jezequel@lip6.fr
3 Email: romain.picot@lip6.fr

http://dx.doi.org/10.1016/j.entcs.2015.10.007
1571-0661/© 2015 The Authors. Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

To answer this question, a numerical library called CADNA [1] has been developed. It implements Discrete Stochastic Arithmetic (DSA) [2] and makes it possible to provide a confident interval of the computed result.
But if the accuracy of the computed result is not sufficient, it is necessary to increase the precision of the computation. A well-known and efficient technique for that is the use of compensated algorithms. These algorithms are based on the fact that it is often possible to compute exactly the rounding errors of some elementary operations like addition and multiplication. We now assume that we work with a floating-point arithmetic adhering to the IEEE754-2008 Standard [3]. In that case, if we use rounding to nearest, then the rounding error of an addition is a floating-point number that can be computed exactly. The algorithms that enable the computation of rounding errors are called error-free transformations (EFT). An algorithm that relies on EFT to increase the accuracy is called a compensated algorithm (see [4]).
However if we use directed rounding, the error of a floating-point addition is not necessarily a floating-point number. Yet, directed roundings are required in DSA. As a consequence, it is not clear whether we can use stochastic arithmetic to validate some numerical codes that heavily rely on the use of error-free transformations.
In this article, we show that we can use stochastic arithmetic to validate com- pensated summation. Several compensated algorithms exist for summation. The first one is Kahan’s compensated summation [5]. Another one is the doubly com- pensated summation algorithm by Priest (see [6] or chapter 4 of [7]). We mainly focus here on the compensated algorithm derived by Ogita, Rump and Oishi [8].
In Section 2, we give some definitions and notations used in the sequel. In Sec- tion 3, we present the principles of DSA. In Section 4, we analyse the impact of directed roundings on an EFT for floating-point addition, the FastTwoSum algo- rithm [9]. We show in Section 5 that we can still use stochastic arithmetic with compensated summation. Section 6 confirms the accuracy of the algorithm and shows performances.

Definitions and notations
Throughout the paper, we assume to work with a binary floating-point arithmetic adhering to IEEE 754 floating-point standard [3]. We suppose that no overflow occurs. The set of floating-point numbers is denoted by F, the relative rounding error by u. For IEEE 754 double precision, we have u = 2−53 and for single precision u = 2−24.
We denote by fl*(·) the result of a floating-point computation, where all opera- tions inside parentheses are done in floating-point working precision with a directed rounding (that is to say toward −∞ or +∞). Floating-point operations in IEEE 754 satisfy [7]
∃ ε1 ∈ R, ε2 ∈ R such that
fl*(a ◦ b)= (a ◦ b)(1 + ε1)= (a ◦ b)/(1 + ε2) for ◦ = {+, −} and |εν|≤ 2u.

This implies that
|a ◦ b − fl*(a ◦ b)|≤ 2u|a ◦ b| and |a ◦ b − fl*(a ◦ b)|≤ 2u| fl*(a ◦ b)| for ◦ = {+, −}.
We use standard notation for error estimations. The quantities γn are defined as usual [7] by
nu
γn(u) := 1 − nu	for n ∈ N,
where we implicitly assume that nu ≤ 1.
Principles of Discrete Stochastic Arithmetic (DSA)
Based on a probabilistic approach, the CESTAC method [10] allows the estimation of rounding error propagation which occurs with floating-point arithmetic. It uses a random rounding mode which consists in rounding each computed result toward
−∞ or +∞ with the same probability. The computer’s deterministic arithmetic is replaced by a stochastic arithmetic where each arithmetic operation is performed N times before the next one is executed, thereby propagating the rounding error differently each time. Therefore, for each computed result, the CESTAC method furnishes us with N samples R1,..., RN . The value of the computed result R is chosen to be the mean value of {Ri} and, if no overlow occurs, the number of exact significant digits in R can estimated as

  √N R		1 Σ
		
		
1	Σ	 2


 

τβ is the value of Student’s distribution for N−1 degrees of freedom and a probability level 1 − β.
The validity of CR is compromised if both operands in a multiplication or the divisor in a division are not significant [11]. It is essential that such numbers with no significance are detected and reported. Therefore multiplications and divisions must be dynamically controlled in order to perform a so-called self-validation of the method. The need for this control has led to the concept of computational zero [12]. A computed result is a computational zero, denoted by @.0, if ∀i, Ri =0 or CR ≤ 0. This means that a computational zero is either a mathematical zero or a number without any significance, i.e. numerical noise.
To establish consistency between the arithmetic operators and the relational operators, discrete stochastic relations [13] are defined as follows. Let X = {Xi} and Y = {Yi} be two results computed with the CESTAC method,
X = Y	if and only if	X − Y= @.0,

X > Y	if and only if	X > Y	and	X − Y /= @.0,
X ≥ Y	if and only if	X ≥ Y	or	X − Y= @.0.
Discrete Stochastic Arithmetic (DSA) is the combination of the CESTAC method, the concept of computational zero, and the discrete stochastic relationships [2].

The CADNA 4 software [1] is a library which implements DSA with N = 3 and β = 0.05. In contrast to interval arithmetic, that computes guaranteed results, the CADNA software provides, with the probability 95% the number of exact significant digits of any computed result. CADNA allows to use new numerical types: the stochastic types. In practice, classic floating-point variables are replaced by the corresponding stochastic variables, which are composed of three perturbed floating- point values. When a stochastic variable is printed, only its exact significant digits appear. Because the library contains the definition of all arithmetic operations and order relations for the stochastic types, the use of CADNA in a program requires only a few modifications: essentially changes in the declarations of variables and in input/output statements. During the execution, CADNA can detect numerical instabilities, which are usually due to the presence of numerical noise. When a numerical instability is detected, dedicated CADNA counters are incremented. At the end of the run, the value of these counters together with appropriate warning messages are printed on standard output.
FastTwoSum with faithful rounding
If Algorithm 1 [9] is executed using a binary floating-point system adhering to IEEE 754 standard, with subnormal numbers available, and providing correct rounding with rounding to nearest, then it computes two floating-point numbers s and t such that
s + t = a + b exactly;
s is the floating-point number that is closest to a + b.

Algorithm 1 FastTwoSum function [s, t] = FastTwoSum(a, b) 1: if |b|≥ |a| then
2:	exchange a and b
3: end if
4: s → a + b
5: z → s − a
 6: t → b − z	

The floating-point number t is the error on the floating-point addition of a and b if Algorithm 1 is executed with rounding to nearest. With another rounding mode this error may not be exactly representable ([14] page 125). In [15], a condition on a and b is given for the FastTwoSum algorithm to provide the exact error on the floating-point addition of a and b with directed rounding. In this paper, we aim at analysing the impact of the random rounding mode required by DSA on Algorithm 1. Therefore in the rest of this section, any arithmetic operation in Algorithm 1 is rounded using the fl* function defined in Section 2. The results

4 URL address: http://www.lip6.fr/cadna

given in this section have been established using Sterbenz’s lemma [16] which is recalled below. As a remark, Sterbenz’s lemma is valid with directed rounding. In the Propositions presented in Sections 4 and 5, we assume underflow may occur be- cause, in this case, additions or subtrations generate no rounding error if subnormal numbers are available [17].
Lemma 4.1 (Sterbenz) In a floating-point system with subnormal numbers avail- able, if x and y are ﬁnite floating-point numbers such that y/2 ≤ x ≤ 2y, then x−y is exactly representable.
In [15], it is shown that the floating-point number z in Algorithm 1 is computed exactly with directed rounding. This property is also true with the random rounding mode. This associated proof is detailed below for completeness.
Proposition 4.2 The floating-point number z provided by Algorithm 1 using di- rected rounding is computed exactly, i.e. z = s − a.
Proof. Let us distinguish two cases.
a, b ≥ 0:
Because 0 ≤ b ≤ a,
a ≤ a + b ≤ 2a	(2)
From the monotonicity of the fl* function we deduce
a ≤ fl*(a + b) ≤ 2a	(3)


Then

a ≤ s ≤ 2a	(4)

Therefore, according to Sterbenz’s lemma, z = s − a.
a ≥ 0,b ≤ 0:
if −b ≥ a , then
a
a ≥ −b ≥ 2	(5)
So a − (−b) is exactly representable because of Sterbenz’s lemma. Therefore
s = a + b which implies z = s − a.
if −b < a , then


Hence
a
0 ≥ b > − 2	(6)
a
a ≥ a + b > 2	(7)

From the monotonicity of the fl* function we deduce
a
a ≥ s ≥ 2	(8)
Therefore, from Sterbenz’s lemma, z = s − a.

The two cases a, b ≤ 0 and a ≤ 0,b ≥ 0, have a similar proof mainly using −a
and −b.	2
In general the correction t computed by Algorithm 1 using directed rounding is different from the rounding error e on the sum of a and b. We establish below a relation between t and e.
Proposition 4.3 Let s and t be the floating-point addition of a and b and the correction both computed by Algorithm 1 using directed rounding. Let e be the error on s: a + b = s + e. Then
|e − t|≤ 2u|e|.
Proof. From Proposition 4.2, z is computed exactly. However with directed round- ing, t may not be computed exactly. So δ ∈ R exists such that
t = b − z + δ	(9)


and
|δ|≤ 2u|b − z|.	(10)

From Proposition 4.2, we deduce
|δ|≤ 2u|a + b − s|	(11)
Let e be the error on the floating-point addition of a and b, then
a + b = s + e	(12)


with
|e|≤ 2u|a + b|.	(13)

From Equations (11) and (12), we deduce a bound on |δ| = |e − t|:
|δ|≤ 2u|e|	(14)
2

Compensated summation with faithful rounding
The classic algorithm for computing summation is the recursive Algorithm 2.

Algorithm 2 Summation of n floating-point numbers p = {pi}	
function res = Sum(p)
1: s1 → p1
2: for i =2 to n do
3:	si → si−1 + pi
4: end for
 5: res → sn	

If we denote by s = Σn	pi the exact summation, S = Σn	|pi| and res the
computed summation, it is possible to show [7] that |s − res| ≤ γn−1(2u)|S| with directed roundings. This accuracy is sometimes not sufficient in practice. Indeed, when the condition number |s|/S is large (greater than 1/u) then the recursive algorithm does not even return one correct digit.
In Figure 1 and Algorithm 3 [8], a compensated scheme to evaluate the sum of floating-point numbers is presented, i.e. the error of individual summation is somehow corrected. Indeed, with Algorithm 1 (FastTwoSum), one can compute the rounding error. Algorithm 1 can be cascaded and sum up the errors to the ordinary computed summation.



··· 








v
⊕

Fig. 1. Compensated summation algorithm

Algorithm 3 Compensated summation of n floating-point numbers p = {pi}	
function res = FastCompSum(p)
1: π1 → p1
2: σ1 → 0
3: for i =2 to n do
4:	[πi, qi] → FastTwoSum(πi−1, pi)
5:	σi → σi−1 + qi
6: end for
 7: res → πn + σn	
Assuming Algorithm 3 is executed with rounding to nearest, a bound on the accuracy of the result, established in [8], is recalled in Proposition 5.1.
Proposition 5.1 Let us suppose Algorithm FastCompSum is applied, with rounding to nearest, to floating-point numbers p	F, 1	i	n.  Let s :=	p  and
S := Σ |pi|. If nu < 1, then
2	nu
|res − s|≤ u|s| + γn−1(u)S	with	γn(u)= 1 − nu .	(15)
We aim at analysing the effects of the random rounding mode on Algorithm 3. In [15], the impact of directed rounding on compensated summation is presented. However the algorithm considered in [15] is slightly different from Algorithm 3. Moreover in [15], the summation is assumed to be performed using one rounding

mode, whereas DSA requires frequent changes of the rounding mode. If Algorithm 3 is executed with the random rounding mode, then the EFT are no more exact. However it is shown in Proposition 5.2 that the accuracy obtained with directed rounding is similar to the one given in Proposition 5.1. Because in the proof of Proposition 5.1, rounding mode changes are allowed, we have an upper bound on the error generated by Algorithm 3 with DSA. As a remark, in this paper, u has a constant value independent of the roundind mode and previously mentioned in Section 2.
Proposition 5.2 Let us suppose Algorithm FastCompSum is applied, with directed rounding, to floating-point numbers p   F, 1  i   n. Let s :=  p and S :=
Σ |pi|. If nu < 1 , then
2	2nu
|res − s|≤ 2u|s| + 2(1 + 2u)γn(2u)S	with	γn(2u)= 1 − 2nu .	(16)
Proof. Let ei be the error on the floating-point addition of πi−1 and pi (i = 2, ..., n):



From Proposition 4.3,
πi + ei = πi−1 + pi	(17)

|ei − qi|≤ 2u|ei|	(18)

Because s is the exact addition of the n floating-point numbers pi and πn is the associated floating-point addition,
n	n
s = Σ pi = πn + Σ ei	(19)

The error on the floating-point number res computed using Algorithm 3 with the random rounding mode is
|res − s| = | fl*(πn + σn) − s|	(20)
Therefore
|res − s| = |(1 + ε)(πn + σn) − s|	with	|ε|≤ 2u	(21)
and
|res − s| = |(1 + ε)(πn + σn − s)+ εs|	(22)
From Equation (19),
n
|res − s| = |(1 + ε)(σn −	ei)+ εs|	(23)
i=2


Therefore

n
|res − s|≤ (1 + 2u)|σn −	ei| + 2u|s|	(24)
i=2

Let us evaluate an upper bound on |σn − Σn	ei|.
n	n	n	n
|σn − Σ ei|≤ |σn − Σ qi| + | Σ qi − Σ ei|	(25)

The error on σn is [7]
i=2
i=2
i=2
i=2

n	n
|σn − Σ qi|≤ γn−2(2u) Σ |qi|	(26)

From Equation (18),
n	n	n
| Σ qi − Σ ei|≤ 2u Σ |ei|	(27)

Therefore from Equations (26) and (27),
n	n	n
|σn − Σ ei|≤ γn−2(2u) Σ |qi| + 2u Σ |ei|	(28)

Let us first evaluate an upper bound on Σn	|ei| and then an upper bound on

n i=2
|qi|. Let us show by induction that
n	n

Σ |ei|≤ γn−1(2u) Σ |pi|	(29)

From Equation (17), we deduce that if n = 2,
π2 + e2 = π1 + p2	and	π1 = p1	(30)


Therefore
|e2|≤ γ1(2u) (|p1| + |p2|)	(31)

Let us assume that Equation (29) is true for n and that an extra floating-point number pn+1 is added. Then
πn+1 = fl*(πn + pn+1)	(32)




From [7],

πn+1 = fl*
n+1
pi
i=1
(33)

n+1
|πn+1|≤ (1 + γn(2u))	|pi|	(34)
i=1
Let en+1 be the error on the floating-point addition of πn and pn+1:
πn+1 + en+1 = πn + pn+1	(35)

From Equation (34),
n+1
|en+1|≤ 2u|πn+1|≤ 2u (1 + γn(2u))	|pi|	(36)
i=1
Hence, assuming that Equation (29) is true for n,
n+1	n+1
Σ |ei|≤ (γn−1(2u)+ 2u(1 + γn(2u))) Σ |pi|	(37)

i=2
From Proposition A.1 in the appendix, we deduce
i=1

n+1	n+1
Σ |ei|≤ γn(2u) Σ |pi|	(38)

Therefore by induction Equation (29) is true.
Let us evaluate an upper bound on Σn	|qi|:
n	n	n
Σ |qi|≤ Σ |ei| + Σ |qi − ei|	(39)

From Equations (18) and (29),
n	n	n
Σ |qi|≤ γn−1(2u) Σ |pi| + 2u Σ |ei|	(40)

From Equation (29),
n	n	n
Σ |qi|≤ γn−1(2u) Σ |pi| + 2uγn−1(2u) Σ |pi|	(41)

Therefore
n	n
Σ |qi|≤ (γn−1(2u)+ 2uγn−1(2u)) Σ |pi|	(42)
i=2	i=1
From Proposition A.2 in the appendix, we deduce
n	n
Σ |qi|≤ γn(2u) Σ |pi|	(43)

From Equations (28), (29) and (43), we deduce
n	n	n
|σn − Σ ei|≤ γn−2(2u)γn(2u) Σ |pi| + 2uγn−1 Σ |pi|	(44)


Therefore
n	n
|σn − Σ ei|≤ (γn−2(2u)γn(2u)+ 2uγn−1(2u)) Σ |pi|	(45)

From Proposition A.3 in the appendix, we deduce
n	n
|σn − Σ ei|≤ 2γ2(2u) Σ |pi|	(46)
i=2	i=1
Therefore, from Equations (24) and (46),
|res − s|≤ 2u|s| + 2(1 + 2u)γ2(2u) Σ |pi|	(47)
i=1
2
Numerical results
In the numerical experiment presented here, the sum of 200 randomly generated floating-point numbers is computed in double precision with the CADNA library using the Sum and the FastCompSum algorithms. In Figure 2, one can observe the number of exact significant digits of the results estimated by CADNA from Equa- tion (1). Using the Sum algorithm, if the condition number increases, the number of exact significant digits of the result decreases and the result has no more correct digit for condition numbers greater than 1015. Using the FastCompSum algorithm, as long as the condition number is less than 1015, the compensated summation algorithm produces results with the maximal accuracy (15 exact significant digits in double precision). For condition numbers greater than 1015, the accuracy de- creases and there is no more correct digit for condition numbers greater than 1030. The results provided by CADNA are consistent with well known properties of com- pensated summation algorithms [7]: with the current precision, the FastCompSum algorithm computes results that could have been obtained with twice the working precision.
The number of numerical instabilities detected during the execution depends on the condition number. These numerical instabilities are of various types:
using the Sum algorithm,
cancellation (subtraction of two very close values which generates a sudden loss of accuracy)
using the FastCompSum algorithm,
cancellation
unstable branching (due to a non significant difference between the operands in a relational test)
non significant argument in the absolute value function.


16

14

12

10

8

6

4

2

0
1	100000  1e+10  1e+15  1e+20  1e+25  1e+30  1e+35  1e+40
condition number

Fig. 2. Accuracy estimated by CADNA using the Sum and the FastCompSum algorithms with 200 randomly generated floating-point numbers

Because no multiplication and no division is performed, no instability related to the self-validation of DSA can be detected.
Table 1 presents the execution times for the sum of 100 000 floating-point num- bers computed in double precision. Execution times have been measured with and without CADNA on an Intel Core 2 quad Q9550 CPU at 2.83 GHz using g++ version 4.8.3. The code has been run using CADNA with two kinds of instability detection:
the detection of all kinds of instabilities;
no detection of instabilities. With this mode, the execution time can be considered the minimum that can be obtained whatever instability detection chosen. This mode is usually not recommended because it does not enable the self-validation of DSA. However, as previously mentioned, using summation algorithms, no in- stability can invalidate the estimation of accuracy.
From Table 1 the cost of the FastCompSum algorithm over the classic summation is about 6 without CADNA and varies from 4 to 9 with CADNA, depending on the level of instability detection. The cost of CADNA in terms of execution time varies from 10 to 15 if no instability detection is activated. This overhead is higher if any instability is detected because of the heavy cost of the cancellation detection.

Conclusion and Perspectives
In this article, we have shown that we can validate compensated summation based on the FastTwoSum algorithm with discrete stochastic arithmetic even if EFT are not valid as we use directed rounding modes. In a future article, we will generalize this analysis to other EFT like TwoSum and TwoProduct. Then we will see if we can still use discrete stochastic arithmetic for validating compensated algorithms for dot product and polynomial evaluation (compensated Horner scheme).


Table 1
Execution times with and without CADNA for the sum of 100 000 floating-point numbers
Acknowledgement
The authors wish to thank EDF (Electricit´e De France) for its financial support.

References
F. J´ez´equel and J.-M. Chesneaux. CADNA: a library for estimating round-off error propagation.
Computer Physics Communications, 178(12):933–955, 2008.
J. Vignes. Discrete Stochastic Arithmetic for validating results of numerical software. Numerical Algorithms, 37(1–4):377–390, December 2004.
IEEE Computer Society. IEEE Standard for Floating-Point Arithmetic. IEEE Standard 754-2008, August 2008.
J.-M. Chesneaux, S. Graillat, and F. J´ez´equel. Encyclopedia of Computer Science and Engineering, volume 4, chapter Rounding Errors, pages 2480–2494. Wiley, 2009.
W. Kahan. Further remarks on reducing truncation errors. Comm. ACM, 8:40, 1965.
D. M. Priest. On Properties of Floating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations. PhD thesis, Mathematics Department, University of California, Berkeley, CA, USA, November 1992. ftp://ftp.icsi.berkeley.edu/pub/theory/priest-thesis.ps.Z.
N.J. Higham. Accuracy and stability of numerical algorithms. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, second edition, 2002.
T. Ogita, S. M. Rump, and S. Oishi. Accurate sum and dot product. SIAM J. Sci. Comput., 26(6):1955– 1988, 2005.
T.J. Dekker. A floating-point technique for extending the available precision. Numerische Mathematik, 18(3):224–242, 1971.
J. Vignes. A stochastic arithmetic for reliable scientific computation. Mathematics and Computers in Simulation, 35:233–261, 1993.
J.-M. Chesneaux. L’arithm´etique stochastique et le logiciel CADNA. Habilitation `a diriger des recherches, Universit´e Pierre et Marie Curie, Paris, France, November 1995.
J. Vignes. Z´ero math´ematique et z´ero informatique. Comptes Rendus de l’Acad´emie des Sciences - Series I - Mathematics, 303:997–1000, 1986. also: La Vie des Sciences, 4 (1) 1-13, 1987.


J.-M. Chesneaux and J. Vignes. Les fondements de l’arithm´etique stochastique. Comptes Rendus de l’Acad´emie des Sciences - Series I - Mathematics, 315:1435–1440, 1992.
J.-M. Muller, N. Brisebarre, F. de Dinechin, C.-P. Jeannerod, V. Lef`evre, G. Melquiond, N. Revol,
D. Stehl´e, and S. Torres. Handbook of Floating-Point Arithmetic. Birkh¨auser, Boston, 2010.
J. Demmel and H. D. Nguyen. Fast reproducible floating-point summation. In 21st IEEE Symposium on Computer Arithmetic, Austin, TX, USA, April 7-10, pages 163–172, 2013.
P.H. Sterbenz. Floating-point computation. Prentice-Hall series in automatic computation. Prentice- Hall, 1973.
J.R. Hauser. Handling floating-point exceptions in numeric programs. ACM Trans. Program. Lang. Syst., 18(2):139–174, 1996.

A	Appendix
The same notations as in Section 2 are used. We consider a precision-p binary floating-point system.  Let u = 2−p and γn(2u) =  2nu  .  Let us assume that

nu < 1 .
Proposition A.1


Proof.


γn−1(2u)+ 2u(1 + γn(2u)) ≤ γn(2u)	(A.1)


and
γ	(2u)	2(n − 1)u
1 − 2nu
2u
(A.2)

2u(1 + γn(2u)) = 1 − 2nu	(A.3)
Therefore from Equations (A.2) and (A.3), we deduce




Proposition A.2
2nu
γn−1(2u)+ 2u(1 + γn(2u)) ≤ 1 − 2nu	(A.4)
2

γn(2u)+ 2uγn(2u) ≤ γn+1(2u)	(A.5)

Proof. Because nu < 1 ,

Therefore

1
γn(2u) < 1 − 2nu	(A.6)
2u


and
γn(2u)+ 2uγn(2u) < γn(2u)+ 1 − 2nu	(A.7)
2(n + 1)u

γn(2u)+ 2uγn(2u) <
(A.8)
1 − 2nu

Therefore we can deduce Equation (A.5).	2
Proposition A.3
γn−2(2u)γn(2u)+ 2uγn−1(2u) ≤ 2γ2(2u)	(A.9)

Proof.

γn(2u) − 2u =

2nu	2u + 4nu2
(A.10)
1 − 2nu

Because 1 − 2nu > 0 and 2nu + 4nu2 > 2u, γn(2u) − 2u > 0. Therefore
2u < γn(2u)	(A.11)
Furthermore, because γn−1(2u) ≤ γn(2u), we can deduce Equation (A.9).	2
