Graph embedding is an important representational technique that aims to maintain the structure of a graph while learning low-dimensional representations of its vertices. Semantic relationships between vertices contain essential information regarding the meaning of the represented graph. However, most graph embedding meth- ods do not consider the semantic relationships during the learning process. In this paper, we propose a novel semantic graph embedding approach, called SemanticGraph2Vec. SemanticGraph2Vec learns mappings of vertices into low-dimensional feature spaces that consider the most important semantic relationships between graph vertices. The proposed approach extends and enhances prior work based on a set of random walks of graph vertices by using semantic walks instead of random walks which provides more useful embeddings for text graphs. A set of experiments are conducted to evaluate the performance of SemanticGraph2Vec. SemanticGraph2Vec is employed on a part-of-speech tagging task. Experimental results demonstrate that SemanticGraph2Vec outperforms two state-of-the-art baselines methods in terms of precision and F1 score.

A graph is a data structure that represent the relationships between different types of objects. One of the core goals of graph-based al- gorithms is to extract structural information from graphs, including summary graph statistics and local graph neighborhood structures [1]. The goal of graph embedding is to learn graph mappings into low- dimensional vector spaces in which the extracted features of vertices can be learned with respect to actual graph structures. Therefore, each vertex is represented as a vector.

Graph embedding has recently become an important and interesting research topic [4]. Graph embedding aims to learn low-dimensional representations of a graph or any of its components (e.g. vertices and edges), while preserving the structure of the graph and any other additional information, (e.g. vertex attributes). Such representations are used for machine learning models and Natural Language Processing (NLP) applications [5]. Many researchers have proposed graph embed- ding algorithms as components of dimensional reduction techniques,

such as Zhang et al. [6], Feng et al. [7], Isomap [8], Locally Linear Embedding (LLE) [9], and Laplacian Eigenmaps [10]. Graph embed- ding algorithms are used to calculate the similarity between pairwise data points to construct similar graphs that can be embedded into a new low-dimensional space.

The remaining of this paper is organized as follows. The main research objectives and contributions are outlined in Section 3. In Sec- tion 2, the related work on graph embedding are briefly reviewed. Sec- tion 4 describes the proposed model. Experimental results are discussed in Section 5. The conclusions are summarized in Section 6.

Over the past years, most proposed graph learning models have focused on preserving the structures of represented graphs, rather than on maintaining the semantic relationships between vertices. Meta- Graph2Vec is the first model considering semantic relationships be- tween vertices during the learning process. For semantic graph embed- ding, the MetaGraph2Vev model learns more informative embeddings by capturing rich semantic relationships between different types of ver- tices. It guides random walks in heterogeneous information networks to encode the semantic relationships between different types of vertices and generate heterogeneous walks through different types of vertices. It should be noted that there are many different datasets used for evaluating various methods and there is no common benchmark dataset that can be used for comparison. This makes it difficult and ambiguous to compare the proposed method to other methods. The majority of

researchers use Macro-F1 and Micro-F1 as evaluation measures to evaluate their approaches and demonstrate improved results compared to previous approaches and baseline algorithms [26]. This is a clear indication that greater attention has been paid to this research area in recent years. Additionally, most existing models have enhanced the baseline algorithm (DeepWalk) in terms of how walks are generated. For example, LINE uses the BFS algorithm to select steps from a target graph, and node2vec uses both the BFS and DFS algorithms for the same purpose.

There have been very few studies on graph embedding that have considered the semantic relationships between graph components (e.g., vertices or subgraphs). Additionally, most semantic graph embed- ding approaches focus on a specific application or specific type of graph (e.g., MetaGraph2Vec [22], which focuses on heterogeneous graphs). Therefore, the broad objective of our research is to develop a novel semantic graph embedding model in which the semantic relationships between words are considered during the walk generation process to enhance various NLP tasks, such as textual entailment and Part-of- Speech (POS) tagging. To summarize, the main contributions of this research are:

walks with the lowest rank (highest priority) are selected during the sampling process. The sum of all semantic relationship ranks present in each semantic walk is calculated according to this selection procedure, the semantic walks are then ordered based on their semantic rank, and finally the most important semantic walks with the lowest ranks are considered in the vertex learning process. Inspired by the skip- gram model [30], a vertex representation is learned by optimizing the semantic neighborhood objective using Stochastic Gradient Descent with negative sampling [31] as per proposed by DeepWalk model.

To evaluate the ability of the proposed model to improve the task of POS tagging, a custom dataset extracted from news articles was used in the experiments. This dataset was compiled from an Arabic news website called AlJazeera News.2 It consists of more than 3500 words that were annotated with their corresponding POS tags using the Farasa POS tagger [37]. Farasa is an open-source text-processing toolkit that offers many Arabic text processing libraries for lemmatization, POS tagging, dependency parsing, and name-entity recognition. The Farasa results are chosen as the reference tag because they provided high-quality POS tagging results with 98.1% accuracy [38].

Experiments were performed to evaluate the ability of Semantic- Graph2Vec to improve POS tagging tasks compare with other baseline graph embedding models. The evaluation metrics considered were precision, recall, and F-score. Precision is the proportion of correct decisions made over the total number of decisions for a given class. Recall refers to the fraction of correct decisions provided by the POS tagger over the total number of POS tags for a given class. The eval- uation process is a conditional decision-making process in which the judgement of the proposed model is considered to be correct if and only if it is compatible with the ground truth labels in the data collection. Finally, the F-score is the harmonic mean of precision and recall.

