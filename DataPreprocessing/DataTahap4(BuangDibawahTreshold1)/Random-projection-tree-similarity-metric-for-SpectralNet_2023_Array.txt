In order for SpectralNet to produce accurate results, it needs an affinity matrix with rich information about the clusters. Ideally, a pair of points in the same cluster should be connected with an edge carrying a large weight. If the pair belong to different clusters, they should be connected with an edge carrying a small weight, or no weight which is indicated by a zero entry in the affinity matrix.

SpectralNet uses Siamese nets to learn informative weights that ensure good clustering results. However, the Siamese nets need some information beforehand. They need some pairs to be labeled as negative and positive pairs. Negative label indicates a pair of points belonging

to different clusters, and a positive label indicates a pair of points in the same cluster. Obtaining negative and positive pairs can be done in a semi-supervised or unsupervised manner. The authors of SpectralNet have implemented it as a semi-supervised and an unsupervised method. Using the ground-truth labels to assign negative and positive labels, makes the SpectralNet semi-supervised. On the other hand, using a distance metric to label closer points as positive pairs and farther points as negative pairs, makes the SpectralNet unsupervised. In this study, we are only interested in an unsupervised SpectralNet.

cordingly. Another problem with GCN is its vulnerability to adversarial attack. Yang et al. used GCN with domain adaptive learning [9]. Domain adaptive learning attempts to transfer the knowledge from a labeled source graph to unlabeled target graph. Unseen nodes from the target graph can later be used for node classification.

The output is clustered using Gaussian mixture model (GMM), where GMM parameters are updated throughout training. A similar approach was used by Wang et al. [11], where they used autoencoders to learn latent representation. Then, they deploy the manifold learning tech- nique UMAP [12] to find a low dimensional space. The final clustering

find the cluster labels. Another approach to employ deep learning for spectral clustering was introduced by Wada et al. [15]. Their method starts by identifying hub points, which serve as the core of clusters. These hub points are then passed to a deep network to obtain the cluster labels for the remaining points.

The storage efficiency was measured by the number of total pairs used. We avoid using machine dependent metrics like the running time. We also run additional experiments to investigate how the rpTrees parameters are affecting the similarity metric based on rpTree. The

cancer, which suggests that connecting to two neighbors was not enough to accurately detect the clusters. Yan et al. reported a similar finding where clustering using rpTree similarity was better than cluster- ing using Gaussian kernel with Euclidean distance [23]. They showed the heatmap of the similarity matrix generated by the Gaussian kernel and by rpTree.

This work can be extended by changing how the pairwise similarity is computed inside the Siamese net. Currently it is done via a heat kernel. Also, one could use other random projection methods such as random projection forests (rpForest) or rpTrees with reduced space complexity. It would be beneficial for the field to see how these space-partitioning trees perform with clustering in deep networks.

Freund Y, Dasgupta S, Kabra M, Verma N. Learning the structure of man- ifolds using random projections. In: Platt J, Koller D, Singer Y, Roweis S, editors. Advances in neural information processing systems, Vol. 20. Cur- ran Associates, Inc.; 2008, URL https://proceedings.neurips.cc/paper/2007/file/ 9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf.

