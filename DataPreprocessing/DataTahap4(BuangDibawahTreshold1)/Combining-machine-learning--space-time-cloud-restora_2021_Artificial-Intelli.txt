0.001 t/ha and 0.136 t/ha on the holdout group. The two models also revealed consistent and better performance using scatter plot analysis across three datasets. The approach developed is useful to predict wheat yield at field scale, which is a rarely available but vital in many developmental projects, using optical sensors.

Crop yield estimation is at the hub of numerous global developmen- tal challenges such as food security, hunger reduction, and international crop trade among others (Kim et al., 2019; Dodds and Bartram, 2016). Yield estimation is the process of accurately predicting crop yield before harvest. Remote sensing technologies coupled with empirical methods used to predict crop yield having some comparative advantages, nota- bly, covering a large area, enabling continuous monitoring, and provid- ing timely data (Potgieter et al., 2014; Lobell et al., 2015). These methods discriminate the potential of a range of vegetation indices (VIs). The relationship between satellite-derived VIs and crop yield is established because important crop characteristics that determine crop yield such as leaf area index, biomass, and chlorophyll content are represented by VIs. Specifically, VIs record canopy level properties

Nonetheless, several issues have challenged the application of VIs for crop yield prediction. Three major categories could be identified: sensor (obstruction of images by cloud, missing orbits, and geometry artifacts), data (absence of long years of crop yield data), and statistics (the com- plex relationship between predictor and response variables). Besides, images obtained at some crop growth stages, which are commonly re- ferred to as critical stages, have paramount importance for crop yield prediction. A study revealed that using Quickbird image at end of the heading and in particular after inflorescence fully emerged, NDVI is highly correlated with yield with an average correlation coefficient of

Therefore, this study motivated to address some of the key chal- lenges of farm-level crop yield estimation using optical sensors. This study is novel as it integrates and implements four methods: application of gapfill on high-resolution S2 sensor, application of gapfill following crop phenology stage, the use of tilling approach within gapfill frame- work, and evaluation of observations from cloud restored images for yield prediction using seven machine learning and ensemble methods. We hypothesized that by discriminating the spatio-temporal potential of the S2 sensor, the gapfill method will provide a good estimate of cloudy pixels and the machine learning methods will learn the non- linearity between the NDVI variable and winter wheat yield. Hence, the two methods: gapfill and machine learning potentially complement and offer an increased skill for wheat yield prediction. Therefore, the purpose of this study is twofold. First, we implement the gapfill algo- rithm for cloud restoration on NDVI variables derived from high spatial and temporal resolution S2 sensor. Second, observations from restored images combined with original (from cloud-free images) values used for wheat yield prediction.

The input dataset for gapfill should be prepared in four- dimensional arrays: x (dimension one), y (dimension two), s (sea- son: dimension three), and a (year index: dimension four). The input dataset in this study has a spatial extent of 1486 grids along the x and 1837 grids along the y. To the season index, seven images found in the study season used. We grouped images per phenology considering the hypothesis that images found in one phenology stage represents a single physiological crop development and could yield a composite that offers reliable prediction values. The study applied data from three years: 2017, 2018, and 2019, which

The key field-collected data in this study was dry weighted wheat yield measured in ton per hectare (t/ha). It was collected from 67 farmers' fields for two crop seasons: 2017 and 2018. Fur- thermore, in this study, with the primary purpose of getting a bet- ter yield prediction method, some potential predictors added. Specifically, two types of fertilizers that have been widely used in the study area such as Urea fertilizer that contains 46% nitrogen; and NPS, which contain nitrogen, phosphorous and sulfur with the ratio of 19% N, 38% P2O5 and 7% S were used (Meessen and Petersen, 2010).

Rank the image. At this stage, for all the sub-images within the prediction set, a scoring algorithm ranks each of them. The score of a sub-image is the proportion of values in the sub-image that are larger compared to the values at the same spatial coordinates in all other sub-images. The sub-images are ranked by increasing score, i.e., a sub-image with the smallest score will be assigned rank 1 and so on.

Estimate quantile. For sub-images within the prediction set hav- ing an observed value at the spatial location of the target pixel, here, the algorithm determines to which empirical quantile level that value corresponds relatively to all values of the image. The observed values in the prediction set are used to estimate an empirical cumulative distri- bution function for each sub-image. The final quantile level will be the mean of all quantiles levels for all the sub-images in the prediction set. A tuning parameter, v, which is the minimum number of quantiles, con- trols the process.

computation of RMSE using a cross-validation procedure. The proce- dure starts with true data and removes some validation points to obtain artificially generated points, which referred to as data observed. Then, the observed data will be predicted and filled with values. The compar- ison between data filled and the original data set will yield the metric RMSE (Gerber, 2018). In addition, the validity of the restored dataset assessed visually employing spatial coherence and a non-artificial (nat- ural) look.

K-nearest Neighbors (KNN) is a supervised machine learning algo- rithms used for classification and regression. In KNN, the number of neighbors (k) is a constant used to calculate nearest neighbors distances vector (RSrivastava, 2020). KNN is an instance-based learning; the out- put is the mean of the values of its k nearest neighbors (Appelhans et al., 2015). The study tuned k for the range of 1:16 and selected k = 12 as the final value.

It aggregates the predictions made by multiple decision trees. Each tree relies on the values of a random vector sampled independently and with the same distribution (Breiman, 2001). The most important parameter, mtry that is the number of predictors that are picked ran- domly is searched for mtry = 2:6 (Breiman, 2002). The Random Forest implementation in this study obtained mtry = 2 as best value.

It generalizes linear regression by allowing the linear model to be related to the response variable via a link function by allowing the magnitude of the variance of each measurement to be a function of its predicted value (Nelder and Wedderburn, 1972; Hardin et al., 2007). This model allows us to build a linear relationship between the response and predictors, even though their underlying relation- ship is not linear. The error distribution of the response variable need not to have normal distribution, however it assumed to follow an exponential family of distribution (i.e. normal, binomial, poisson, or gamma distributions). It applies a unique link function for each of the probability distribution (McCullagh and Nelder, 1989). Thus, this study used a gaussian probability distribution with identity link function.

The dataset that contains both predictors and response variables was partitioned into 80:20 ratios for training and testing (holdout) group. All parameters put on the same scale using data scaling method. In the learning process, parameter tuning was implemented for each method. Caret package parameter tuning was implemented using the train con- trol function in R software, and the expand grid function supported the search process (Kuhn et al., 2020).To estimate the generalization error, 25 numbers of subsamples were created as training groups using bootstrapping resampling method. The bootstrap method is a resam- pling approach used to calculate statistics on a population by iteratively sampling a dataset with replacement. It is a widely applicable and pow- erful statistical technique used to measure the uncertainty associated

with a given statistical learning method (James et al., 2013). Since we work with a small sample size in this study, the use of boot strapping is very relevant to estimation problems with small sample size and un- known distribution of the actual population (Karthik and Abhishek, 2019).

For each model, the best tuning parameters were selected using Root Mean Squared Error (RMSE) metric. Then, the seven models were com- bined in Caret List function using the best tuning parameters on the training group using bootstrapping resampling procedure. The Caret List function is used as it supports building lists of caret models on the same training data.

Ensemble methods are the third major machine learning approach applied. Ensemble learning is the process of learning from the advan- tage of combining multiple algorithms for better model performance. In applying a single method of supervised algorithms, the task is to search for a solution in parameter space, while ensembles combine mul- tiple parameter spaces to form a better hypothesis. The above seven base models were combined in an ensemble learning called Stacking or Super Learning (Wolpert, 1992) that trains a second-level meta learner to get the optimal combination of the base learners (Laan et al., 2007).

In this study, using Caret Stack function we applied the 5 fold cross- validation resampling method on the training dataset. Cross-validation is one of the commonly used techniques for model evaluation and con- sidered as a better technique than residual-based metrics (Kohavi, 1995). This method ensures that every data point gets to be in a test set exactly once and disadvantageous to when used in large dataset due to increasing computation cost, which was not relevant as this study applied small dataset (Karthik and Abhishek, 2019).

Outputs from the seven base models combined in a Caret Stack using each of the seven methods, which resulted in seven ensemble versions (Deane-Mayer and Knowles, 2019). The seven ensemble models in- clude ensemble Regularized Regression (en.GLMNET), ensemble Gener- alized Linear Regression (en.GLM), ensemble Neural Network (en. NNET), ensemble K-nearest Neighbors (en.KNN), ensemble Recursive Partitioning and Regression Trees (en.RPART), ensemble Support Vector Machine (en.SVM) and ensemble Random Forest (en.RF).

Finally, in addition to the RMSE metric, this study used scatter anal- ysis to validate the performance of machine learning methods. In partic- ular, scatter plots of measured grain versus estimated grain values prepared for training and holdout groups helped to validate model per- formances (Wang et al., 2019).

As expected with an increasing percentage of cloud coverage per subset, the RMSE increases too. For instance, in column 15, the RMSE has increased from 0.04 to 0.07 and then to 0.26 as the cloud percentage increases from 28% to 54% and then to 58%. This is because with increas- ing cloud coverage the prediction method iteratively enlarges the prediction subset along the spatio-temporal dimension to get the re- quired statistics. It could add also heterogeneous values in the subset that increases the probability of offering poor prediction estimates. In addition, it contradicts the theoretical assumption that a much narrow prediction subset is expected to offer good quality prediction in line with Tobler's first law of geography that states near things are more related than distant things.

Under Section 3.2, phenology-based cloud restoration implemented using gapfill. The restored NDVI used to create the NDVI variable called sum-NDVI. Sum-NDVI is the sum of all NDVI values per each of the study farm boundaries (Mirasi et al, 2019). We also validated the accuracy of the restoration process. In this section as a prediction problem, we implement linear regression analysis by combining observations both from original cloud-free and restored images. The addition of observa- tions from the restored image increases the total number of observa- tions. Nevertheless, whether such increment positively influences the prediction problem should be explained. Hence, the purpose of the anal- ysis is to identify how the inclusion of observations from restored im- ages affects the regression process.

2018/10/26 that got increment of observations resulted in improve- ment of adj R2 values from 0.44, 0.56, 0.27 and 0.42 to 0.55, 0.57, 0.41 and 0.46 respectively. This revealed the incorporation of restored obser- vations in the regression process is advantageous in increasing predic- tion capability and implies the values are good enough.

Nonetheless, an image from the date of 2017/09/26 did not show any improvement. Furthermore, in 2018/10/16 image, though the total number of observations increased from 40 to 67, the adj R2 de- creased from 0.65 to 0.44. In this study, we applied similar tuning pa- rameters across the spatio-temporal image set; nonetheless, the performance of these tuning parameters could vary across the scenes in space and time domain. This in turn could influence the quality of the restored images and ultimately the derived variables.

In general, across the seven methods and along with the three datasets RPART, GLMNET, GLM and NNET are the four models yielding the lowest RMSE values on the training dataset. All the four models re- sulted in RMSE values of 0.001 t/ha for sum-NDVI dataset of the year 2018 and 2017, while RPART revealed the same result on the third dataset too. Random Forest is also among the models yielding smallest RMSE values on sum-NDVI and inputs (2018) dataset. Conversely, two models such as SVM and KNN showed in relative terms lower perfor- mance across the three datasets except KNN revealed one of the lowest outputs (0.001 t/ha) on sum-NDVI & inputs(2018) dataset.

0.001 t/ha on the training group to 0.006 t/ha on holdout group using sum-NDVI (2018) dataset. Likewise, the GLM model on sum-NDVI (2017) showed a slight increase of RMSE from 0.001 t/ha on the training group to 0.015 t/ha on holdout group. The neural net model has RMSE of

Conversely, RPART and RF in relative terms showed overfitting across the three datasets. For instance, RPART and RF models on sum- NDVI (2018) have RMSE 0.001 t/ha and 0.028 t/ha on the training dataset that increased to 0.161 t/ha and 0.136 t/ha on the holdout group. On the other hand, SVM and KNN models demonstrate proper- ties of underfitting where performance on the training is lower than the holdout.

The ensemble models, presented under Section 3.3.5, were imple- mented using seven base learners as input and do not show improve- ment over base models. Nonetheless, previous studies revealed, to develop an ensemble learner with better predictive potential, two con- ditions need to be met (Krogh and Vedelsby, 1995; Zhou, 2009). First,

Accordingly, the same type of ensembles such as en.RF, en.KNN, en.GLM and en.GLMNET that were better using the full base learners found to be also the better ones using the ranking method too. None- theless, the new sets of ensembles based on ranking method do not show improvement over the first group of ensembles using full base learners. In summary, in this study, though ensembles devel- oped using two different approaches, they did not result in improved performance over base learners. This could be associated with the limited diversity of base learners and the use of a small dataset where each observation point has a large weight on model perfor- mance affecting model stability.

Therefore, in terms of getting a fully restored subset, the cloud per- centage per day of the year is the most important parameter. In this study, we used square tiles that resulted in full cloud cover in some sub-images and get predictions for most of the study area. Future stud- ies might consider other tiling procedures that potentially avoid or min- imizes the number of sub-images with full cloud cover.

In applying gapfill, in this study, we started by considering two key challenges. First, since the S2 sensor is recently available, images are available only for three years. Second, S2 is a high spatial resolution sen- sor (10 m) making the total number of missing values to be too large. Hence, it demands the use of high computing infrastructure, which is unfortunately not available in many situations.

Given such limitations, the study has successfully applied gapfill under the current study area. In this study, our main goal was to under- stand the predictive skill of the sum-NDVI parameter using original and restored observations using gapfill. Except for a single date (2018/10/ 16), the incorporation of restored observations in regression analysis of- fered an increased prediction skill. Thus, the gapfill method offered

0.001 t/ha, 0.136 t/ha and 0.001 t/ha for sum-NDVI(2018) dataset as well as 0.001 t/ha, 0.192 t/ha and 0.001 t/ha for sum-NDVI(2017) dataset. In contrast, RPART and RF models in relative terms showed overfitting across the three datasets. On the other hand, models such as SVM and KNN demonstrate properties of underfitting in which per- formance on the training is lower than the holdout. The addition of other hyperparameters besides the limited parameters used in the cur- rent study could improve the performance of these models.

Random Forest is an ensemble algorithm that use bootstrap aggrega- tion method, in particular, it computes average prediction from various decision trees predictions. This enables RF models to be less influenced by outliers. This study is implemented using small number of observa- tion and decided to keep all the sample observations that potentially constitute some outliers. Thus, algorithms such as RF, which are less prone to outliers, are expected to yield increased performance. Besides, RF algorithm is powerful in handling both linear and non-linear rela- tionship (Murthy, 2020; Trehan, 2020).

As a growing field of research, the use of ML under small dataset do- main various across various disciplines. In some fields, for instance, in materials physics and chemistry, numerous studies applied ML to small dataset. A backpropagated Neural Network model using 53 points used successfully to predict the properties of ultrahigh-performance concrete (Ghafari et al., 2015). A strategy named as crude estimation property was proposed to improve ML on small datasets (around 100 data points) (Zhang and Ling, 2018). Similarly, Shaikhina et al., 2015 de- signed neural net and decision tree algorithms for prediction of antibody-mediated kidney transplant rejection using 35 bone speci- mens and 80 kidney transplants and achieved high accuracy of 98.3% and 85% respectively.

In some previous studies, ensemble models showed superiority to base models. For example, a stacking ensemble of ANN revealed a rela- tive RMSE of 6.8% and R2 of 0.68 at the predicted yield in sugar cane crop using MODIS NDVI time series (Fernandes et al., 2017). The superior performance of ensemble models is also reported on other bio- physical datasets. An Ensemble of gradient boosting, multi-narrative adaptive regression spline, random forest, and Support Vector Machine outperformed the individual models for surface soil organic carbon stocks (Mishra et al., 2020). An average-ensemble model is recom- mended as the best model in materials design using 25 numbers of ob- servations (Vanpoucke et al., 2020).

Overall, comparison of the current outputs with previous study outputs could be problematic due to factors such as discipline differ- ence and difference in study design and procedure. Even for same discipline, the implementation of different study design (resampling procedures, outlier treatments, data split, and the number and type of hyperparameters searched) might contribute to reveal different result. Therefore, the difference between the two models (GLM and RF) and the rest five models could be explained partly by the study design implemented.

The study evaluated base machine learning and ensemble methods for wheat yield prediction. Ensemble methods produced a similar per- formance with that of base models. Ensemble models using the seven predictors from base models as well as using three selected predictors based on accuracy following the ranking methodology showed similar performance. The study, employing sum-NDVI dataset, predicts wheat grain yield with RMSE of 0.001 t/ha and 0.136 t/ha using GLM and RF models respectively. The two models showed consistent and better per- formance across the three dataset groups. Besides, they showed good generalization on holdout dataset that is also supported using scatter plot analysis.

