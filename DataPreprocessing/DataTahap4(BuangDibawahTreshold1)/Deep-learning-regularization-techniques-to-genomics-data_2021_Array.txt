Many theoretical Dropout analyses have been explored [6,23,31,49, 55,58,75,79,81]. Baldi et al. [6] showed how the technique acts as adaptative stochastic Gradient Descent. Wager et al. [79] analyzed Dropout as an adaptive regularizer for Generalized Linear Models (GLMs). Ma et al. [46] attempted to explicitly quantify the gap between Dropout's training and inference phases and showed that the gap can be used to regularize the standard Dropout training loss function.

In our experiments, we started by using Logistic Regression on cancer datasets, obtained from the Expression Project for Oncology (EXPO) [60]. Then we trained FNN on one 1000 Genomes Project dataset for individual ancestry prediction according to their genetic profile [59]). All in- dividuals are represented in both datasets by their SNPS profile [14].

parallelogram at the origin. In this case, the loss function is likely to hit the vertices of the parallelogram rather than its edges. L1 regularization removes some of the parameters, thus L1 technique can be used as a feature selection technique. On the other hand, the L2 regularization

Srivastava et al. [72] suggested that, applying Dropout to a NN with n units can be seen as sampling 2n sub Networks with weight sharing. In the test phase, as it is not always practical to take the mean of 2n models, an approximate averaging method is used. The idea is to approximate the exponentially many Networks by a single NN without Dropout. To correct the fact that training outgoing weights of a layer are obtained under condition that neurons were retained with a probability p, the weights are simply multiplied by p. This approximation has been proved for lo- gistic and linear regression models [72,79]. But, for Deep Neural Net- worksDNNs, there is an unknown gap between the expected output of exponential sub Networks and the output of a single deterministic model. Ma et al. [46] showed that under some assumptions on input data, the gap is controlled and it can be used to regularize the single NN.

Dropout is known to improve training model performance when it is combined with other regularization techniques. Batch normalization, introduced by Ref. [35], is a regularization technique used to speed up the training and improve performance of Deep NNs. In the training of a

the problem complexity. Stochastic gradient descent is adopted in all experiments as an optimization strategy. Two types of datasets are included in our experiments, Expression Project for Oncology (expO) cancer datasets and 1000 Genomes Project ethnicity datasets respectively used for training logistic regression and FFN models.

Training a NN with an Autoencoder reconstruction path improved the results. However, training an Autoencoder in conjunction with the clas- sification Network makes the high dimensional optimization problem more difficult to solve than simply training the classification Network, yielding in a higher classification error. To improve the results, regula- rization techniques are used. One can notice that traditional regulariza- tion technique's application has more improved the prediction accuracy of the model compared to Dropout. This could be attributed to the fact

In this work, we have explained stochastic gradient descent optimi- zation technique with back-propagation in training DL algorithms. To prevent overfitting problem, regularization techniques are studied and, theoretical relationship between Dropout and L2 regularization is established. Experimental results have shown that Dropout, when it is combined with techniques such as batch normalization, max-norm or unit-norm gives better performance than L1 and L2 regularization techniques.

