tion and reconstruct structures in an unknown environment. As an essential part of augmented reality (AR) experience, vSLAM enhances the real-world environment through the addition of virtual objects, based on localization (location) and environment structure (mapping). From both technical and historical perspectives, this paper categorizes and summarizes some of the most recent visual SLAM algorithms proposed in research communities, while also discussing their applications in augmented reality, mapping, navigation, and localization.

Visual SLAM, according to Fuentes-Pacheco et al. [1], is a set of SLAM techniques that uses only images to map an environment and determine the position of the spectator. Compared to sensors used in traditional SLAM, such as GPS (Global Positioning Systems) or LIDAR [2], cameras are more affordable, and are able to gather more infor- mation about the environment such as colour, texture, and appearance. In addition, modern cameras are compact, have low cost, and low power consumption. Examples of recent applications that employ vSLAM are the control of humanoid robots [3], unmanned aerial and land vehicles [4], lunar rover [5], autonomous underwater vehicles [6] and endos- copy [7].

Depending on the camera type, there are three basic types of SLAM: monocular, stereo, and RGB-D. Stereo SLAM is a multi-camera SLAM that can obtain a particular degree of trajectory resolution. Additionally, stereo SLAM has the advantage of being more versatile as opposed to RGB-D SLAM which is more sensitive to sunlight and is mainly used indoors. The last two decades have seen significant success in devel-

techniques, which aim precisely at the creation and updating of a map, as well as the location of the observer in relation to the structure of the environment. This confluence between visual SLAM and AR was the motivation for the realization of this survey. The objective of this research is to carry out a survey of the main visual SLAM algorithms, as well as their applications in AR, mapping, localization, and wayfinding. The main characteristics of the visual SLAM algorithms were identified and the main AR applications on visual SLAM were found and analysed. As opposed to presenting a general analysis of SLAM, this survey provides an in-depth review of different visual SLAM algorithms. The survey also includes various datasets that might be considered for evaluation and different types of evaluation metrics. Existing studies in

errors during the localization process. A combination of laser range- finder and monocular camera might be useful since those two sensors are most commonly used for robots. Although such algorithms improve the performance of feature extraction, they are limited to particular SLAM algorithms.

Visual SLAM mapping is performed by using cameras to acquire data about an environment, followed by combining computer vision and odometry algorithms to map the environment. This allows robots to navigate themselves independently and to improve localization. The majority of robots have wheels, so measuring their distance is easy. Inertial measurement units (IMU) [28] have been added to some robots for measuring their body motion. Nonetheless, relying on odometry alone to estimate location is not helpful. However, relying on odometry alone to calculate the own position is not accurate enough because of accumulated error produced by noise. When it takes several turns for the error to accumulate, the location becomes uncertain. Loop closure plays a key role here. Considering buildings and trees to be static, there should be a loop closure [29] at the exact same place since it has already been there (loop detection) [30]. It is then possible to correct and adjust the generated map for these accumulated noises [31].

The vSLAM concept is fundamental to any kind of robotic application where the robot must traverse a new environment and generate a map. The system is not limited to robots, but can be used on smartphones and their cameras, as well. vSLAM would be one aspect of the pipeline needed by some advanced AR use cases, for example, where virtual worlds need to be accurately mapped onto real environments.

extracted from images and compared against each other. It is possible to estimate camera pose in real-time thanks to PL-SLAM [37], which sep- arates the task of tracking and mapping into two separate threads and processes them on a dual-core computer. Recent SLAM methods align the whole image rather than matching features. However, these types of methods are typically less accurate than feature-based SLAM methods for estimating pose.

outside assistance. Another limitation of SLAM is that it is not designed to close large loops. M-estimators are a general class of extremum esti- mators where the objective function is the sample average. The non- linear least squares method and maximum likelihood estimation are both special cases of M-estimators. M Estimators of trackers do not take feature map uncertainties into account, but this does not affect AR applications.

MonoSLAM. The first successful SLAM algorithm in mobile robotics was monocular SLAM (MonoSLAM). By moving rapidly along the tra- jectory of a monocular camera in an unknown environment, natural landmarks can be reconstructed into 3D maps, and an urban environ- ment can be mapped using sparse but persistent points. In this approach, a map of natural landmarks is created online in a probabilistic frame- work from a sparse but persistent set of data. A fundamental aspect of MonoSLAM is the feature-based map that is a probabilistic snapshot of

triangulating multiple views from multiple views instead of tracking it in the in-game frame. As a result, 2D tracking would be worthless since tracking a moving camera is very difficult. As well, initialization of functions in a camera with a narrow viewing angle must be completed very quickly to prevent them from being overwritten. As an alternative, after identifying and measuring a new feature, an initial 3D line should be drawn along a certain line on the map. The line grows to infinity

ORB-SLAM. ORB-SLAM is a feature-based, real-time SLAM system that works outdoors and indoors in a variety of environments. As a result of the robust system, motion clutter can be tolerated, the baseline loop can be closed and the loop re-located, and the system can be fully automatic. ORB-SLAM operates on three threads at the same time: tracking, local mapping, and loop closure.

a new keyframe should be inserted. In order to optimize the pose, motion-only Bundle Adjustment (BA) was used, along with initial feature matching with the previous frame. In the event that tracking is lost (e.g., due to occlusions or abrupt movement), relocalization is performed globally using the place recognition module. The covisibility graph of keyframes that is maintained by the system is used to retrieve the local visible map based on a first estimate of camera pose and feature matching. After all local map points are found, reprojection is used to find matches, and camera pose optimization is again performed. Last but not least, the tracking thread determines if a keyframe needs to be added.

By processing new keyframes and performing local BA, local map- ping is able to reconstruct the surrounding environment efficiently. By finding new correspondences for unmatched ORB, new points are triangulated in the covisibility graph. The point culling policy is applied some time after the points are created, based on the results of the tracking. This ensures that only high-quality points are retained. Also, redundant keyframes are culled by local mapping.

It is important to keep in mind that pixels that do not belong to the model can negatively affect tracking quality. Photometric errors over a certain threshold must be excluded from being included in the analysis. As the least squares method converges, this threshold is lowered with each iteration. As a result, this scheme makes observing unmodeled objects possible while tracking densely.

DTAM. Dense Tracking and Mapping (DTAM) is a real-time tracking and reconstruction approach that does not rely on feature extraction, but rather on dense, pixel-by-pixel tracking. When a large RGB hand-held camera flies over a static scene, a dense patchwork surface of millions of vertices is created. A detailed texture depth map is generated based on keyframes using the algorithm. In order to create a depth map, bundles of frames are reconstructed densely and with sub-pixel resolution.

case, we estimate inter-frame rotations from the model, and then refine the model in the second case, with 6DOF full pose refinement. In both cases, the Lucas-Kanadestyle nonlinear least-squares algorithm is used to minimize an as-measured photometric cost function iteratively. To achieve the global minimum, as soon as the system is initially placed within a convex basin, the true solution must be found. As a final step, we will maximize efficiency by using a power of two image pyramid.

During PMDS-SLAM, images are tracked and captured, and the se- mantic information of each pixel is extracted using the Mask-RCNN segmentation technology. The superpoint segmentation technology for segmenting the current frame into a superpoint mesh is then used. The initial mesh probability is generated from that semantic prior informa- tion, and then propagated through history and into current time. As the current frame contains positions where motion is present, it calculates the motion state for superpoint mesh points at those locations. These meshes are then updated using the Bayesian probability formula. Using the dynamic area mask generated by the tracking thread based on the mesh probability, no real moving features can be observed in the result.

VPS-SLAM. In the world of visual planar semantic SLAM (VPS-SLAM) [41], a lightweight and real-time framework is developed. As part of this method, visual/visual-inertial odometry (VO/VIO) is applied to the geometrical data representing planar surfaces derived from semantic objects. Using planar surfaces to estimate shapes and sizes of selected semantic objects allows for rapid, highly accurate improvements in metrics. A graph-based approach utilizing several state-of-the-art VO/VIO algorithms and the latest object detectors can estimate the ro-

Stereo based vSLAM rely on feature points to estimate the camera trajectory and build a map of the environment. Feature points usually are points from all the edges in an environment. The performance of such algorithms is affected in low-textured environments, where it is sometimes difficult to find a sufficient number of reliable point features. DS-PTAM. A stereo vision-based approach to SLAM is Distributed Stereo Parallel Tracking and Mapping (DS-PTAM). Its purpose is to build a map of an environment in which a robot can operate in real time while getting an accurate estimate of its position. By dividing tracking and mapping tasks into two independent execution threads and performing them both in parallel, S-PTAM achieves very good performance

After moving along its path, it will obtain new stereo images that will be processed using the described method, resulting in an incremental map whose size increases with each successive iteration. The Tracker module is responsible for this functionality. A second execution thread called Mapper will run concurrently with Tracker, which will adjust camera positions and point locations, i.e., adjust the map. Bundle Adjustment is used for these refinements. Also, this thread increases the

DOC-SLAM. The Dynamic Object Culling SLAM (DOC-SLAM) [42] system is a stereo SLAM that is able to achieve good performance in highly dynamic environments by removing the actual moving objects. By combining the semantic information from panoptic segmentation together with the optical flow points, DOC-SLAM can detect potential moving objects. To accomplish dynamic object culling, a moving con- sistency check module determines and removes feature points in objects which are in motion.

ORB-SLAM2. The ORB-SLAM2 system [43] is an integrated SLAM system for monocular, stereo, and RGB-D cameras that offers map reuse, loop closing, and relocalization functions. This system works by using standard CPUs in different environments ranging from small handheld devices inside the home to drones flying in factories and cars traveling through city streets. Bundled adjustments combined with metric scale observations allow for accurate trajectory estimation on the back end. For localization, the system provides a lightweight mode that utilizes visual odometry to track non-mapped regions and match those tracks to map points to ensure zero drift.

Whenever environmental conditions do not change significantly in the long run, the localization mode can be used to enable lightweight long-term localization. If necessary, the tracking processes relocalize the camera continuously in this mode without deactivating the local map- ping or loop closure threads. As part of this mode, points are mapped using visual odometry matches. In visual odometry, the 3D points that are created at each point of the current frame at the same position as the 3D points generated in the previous frame are matched with the ORB in the current frame. Localization is robust to regions that have not been mapped, but drift may accumulate. By matching map points, we ensure the existing map remains localized at all times.

DynaSLAM. The capability of dynamic object detection and back- ground inpainting is added to ORB-SLAM2 using DynaSLAM. Whether it is monocular, stereo, or RGB-D, DynaSLAM works well in dynamic scenarios. The system is capable of detecting moving objects either using deep learning or multiview geometry. Static maps of scenes make it possible to inpaint frame backgrounds obscured by dynamic objects.

Sensor data is processed by tracking threads, which compute the position of the current frame relative to the current map in real time. Feature projections with matched features can be made with minimal error. Further, it determines if a keyframe should be applied to the current frame. Visual-inertial modes estimate body velocity and IMU bias by including residuals from inertial sensors during optimization. Tracking threads that lose tracking attempt to relocalize the current

KITTI Dataset. Karlsruhe Institute of Technology and Toyota Tech- nological Institute (KITTI) [45] datasets have been used by mobile ro- botics and autonomous driving research. A variety of sensor technologies were used to record hours of traffic scenarios, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Although incredibly popular, the dataset does not contain sufficient ground truth to allow for semantic segmentation. A total of 7481 training images are annotated using 3D bounding boxes in the KITTI dataset.

denoted by trans (Ei). In certain situations, root mean square error is preferred rather than mean error since outliers are less affected. It is also possible to compute the median rather than the mean instead, which gives outliers less influence. Also, the rotational error can be evaluated. However, most of the time the translational errors are sufficient for comparison (since rotational errors are translated by the camera when it is moved).

Absolute trajectory error (ATE). For vSLAM systems, the absolute distance between the estimated and the ground truth trajectory is another important metric that can be used to assess the global consis- tency of the estimated trajectory. Due to the fact that both trajectories can be specified in any coordinate frame, they must be aligned first. With the Horn method [50], one can obtain a rigid-body transformation S that maps the estimated trajectory Pi:n onto the ground truth trajectory Q1:n using the least-squares solution. As a result of this transformation, the absolute trajectory error can be computed as follows:

also be used to evaluate the overall error of a trajectory. Translational and rotational errors are taken into account by the RPE, while only translational errors are considered by the ATE. Therefore, the RPE metric provides an elegant way to combine rotational and translational errors into a single measure. The ATE, however, generally also detects rotational errors indirectly because they show up in wrong translations.

Many researchers today still use older techniques and algorithms to evaluate the accuracy of their SLAM algorithm. Those evaluation ap- proaches have many limitations i.e do not work for all algorithms and especially recent ones and do not always work if the environment is large and full of obstacles. Those limitations bring the motivation to explore new ways that could potentially be used to accurately evaluate SLAM algorithms.

described algorithms, ORB-SLAM can be considered the state of the art among those who use a single camera as a sensor. Although the PL-SLAM Monocular is robust, especially in environments that are poor in texture, this result is achieved at the expense of high computational capacity. The stereo version of these algorithms, ORB-SLAM2 and PL-SLAM Ste- reo, respectively, show similar results, although there are inherent ad- vantages to stereo technology, such as easier scaling and map initialization. Among the algorithms that are based on depth sensors, associated or not with RGB cameras, some present promising results since the behaviour that is invariant to the ambient light is an important characteristic of these sensors. However, depth sensors are not as commonly found in devices as RGB cameras and do not usually perform well outdoors, due to the use of infrared rays.

