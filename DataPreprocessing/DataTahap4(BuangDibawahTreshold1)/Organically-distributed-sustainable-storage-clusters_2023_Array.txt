or perceived future access needs. This is somewhat similar to self- organized ad-hoc networks based on autonomous system control [2]. Many organizations have experimented with the development of path- ways and other forms of organic architecture within their organizations to render the paths of least resistance or the most appropriate design for efficient movement [3]. This form of architectural discovery was not possible due to the large amounts of power, cooling, engineering, and mass of traditional storage area networks [4].

Physical security is always a concern for data centers, server and engineering closets, and other areas where computational resources are deployed. This takes an additional dimension for Internet of Things devices [5]. Traditionally, if a malicious actor gained access to the compute cluster, they could access the storage or destroy the systems, causing catastrophic failure. The low cost of deployment of pods of organic clusters, combined with their ability to replicate full data storage over hundreds of devices, prevents the catastrophic loss of access in many cases. If five of ten pods are damaged, then the load for the organization is automatically distributed to the remaining pods. Each node storage can be encrypted using 256 bit encryption within distributed or protected key centers. The key centers can be located on almost any system, from wifi-enabled ESP8266 ESP12F modules to a cloud-based solution. In this case, malicious attackers could steal an entire pod, and no information would be accessible due to the loss of the encryption keys. The entire pod could be a loss of less than $1000 USD.

This artifact will be analyzed according to conventional usage, and common analysis tools available to any network or system adminis- trator against a five year old existing storage area network. These tests will be run live over the period of one year in a production environment with the next analysis occurring at approximately 400 days. Afterwards an evaluation will be provided for discussion and further recommendation.

provided by a small consumer solar panel array, such as the BLUETTI SP120, which outputs approximately 120 Watts of power. This places the organic cluster well within reach of a solar-powered environmental scope. The relationship between the power, production, maintenance, and cost would also be a consideration that allows this system to be within a sustainable realm [4].

Design Science is the pursuit of research for both deliberate design methods and the scientific annotation of the processes and results for the betterment of the scientific community [13]. In similitude of qualitative or quantitative spheres that examine the existential, design science investigates pre-existential concepts into realization, through creation and innovation. The Design Science Research Methodology (DSRM) provides a process to create a solution based on a set of needs, called an artifact. Artifacts are critical to the design science research process, as they provide the created form of a solution. In this way, the artifact is an instantiation of the concept described as a solution for the need or challenge [14,15]. Throughout the process, the artifact will follow a design, evaluation, revision process until a conclusive outcome can be determined or until the research is concluded. In this, design science research combines exploratory and experimental methods to produce an artifact that provides the opportunity for descriptive and explanatory methods.

The initial iterations of the primary artifact were developed us- ing the GlusterFS [8] system on (1) virtual machines (VMs), and (2) physical lab computers. After verification that gluster would work in a Linux Mint VM environment without issue, 24 physical machines were constructed and deployed in a laboratory. Several reliability tests were conducted by disabling between 1 and 10 systems in the cluster without issue. GlusterFS was able to recover without external intervention on each attempt. This validation allowed for the creation of the low-power organically distributed artifact to proceed.

Linux Mint 20 was installed on each of the four AtomicPis using an external USB flash drive. After installation, the storage was formatted using XFS. XFS was selected as it reportedly performed well with large file sizes and multi-threaded installations [16] on lower-performance systems. SSH was enabled and public private key pairs were created for remote access to the devices. These keys or new keys could be used for the LUKS cryptfs file system, if desired. GlusterFS was then installed with a single brick per node.

increase the network load and to test the round-robin load-balancer in DNS. After the research period, each node was rebooted with a five minute interval. The interval would allow the automatic discovery and synchronization of each rebooted node with the entire cluster pod. After the reboot, another performance test was initiated for comparison

After the final diagnostic was completed on the unmodified cluster, a reboot was conducted to provide a fresh platform for analysis. This test was performed using the same method as the initial and final diagnostics. Prior to the test, an uptime command was run against the cluster to verify the uptime and overall utilization. It should be noted that only 23 systems were available to download the diagnostic image from the cluster at the time when the cluster was rebooted.

Changes in current technological capabilities and the increased need for local data control and availability may drive a move from cloud storage to local or hybrid storage solutions. This may not be attributed to a systematic or organized move from cloud computing due to privacy, cost, or poor performance, but the opportunity to have secure, local access, of organizational data at local facilities. Developing countries may also continue to benefit from the development of ecolog- ical systems that use alternative power sources. These opportunities, coupled with the evidence that some organizations are concerned with the security of their data within the cloud [17,18], may drive another technical migration of data back to the organizations from whence it was removed.

