of machine learning, which is inspired by the artificial neural network algorithm. It is one of the breakthroughs in the area of computer vision for image classification, object detection, and localization, which has been effectively used in weed detection for robotic weed control re- search (Hasan et al., 2021).

study is important because, during the deep learning computer vision model training, crop and weeds pixels along with soil background pixels are used by convolutional neural network architecture to generalize the weed detection model (Kulawardhana, 2011). The changes in image pixels between the model training images and testing images might lead to the model performance degradation (Velumani et al., 2021).

A recent study cross evaluated the deep learning model in different background condition in disease detection area of agriculture (Ferentinos, 2018). In a previous study by Ferentinos (2018), model built from labrotory condition images was tested on images obtained in field, which caused de- cline in model performance due to having completely different background image pixel than that of labrotory images.There is higher probability of hav- ing different background in disease detection studies images than weed de- tection images, because weed detection images capture whole plant images whereas, disease detection images capture s specific region of plant affected by disease. Similar studies are important to fill the existing gap on research about effects of background in weed identification which could assist toward the development of robust real time deep learning weed detection model in agriculture industries.

After images were acquired, the next step was extracting the indi- vidual weed and crop species from the single image. A Python (Python Software Foundation, Wilmington, DE) script was developed to label a single image of a day using an OpenCV library (Bradski, 2000), which saves the bounding box coordinate in order to perform automatic cropping of the remaining images for the same day. Before applying the python script, images were manually checked if there were any dis- turbances, such as human intervention, external objects in the image, and changed illumination. The images that included disturbances

-uniform background scenario (S1) and uniform background scenario (S2). VGG16 architecture already trained on ImageNet 1000 classes was used to finetune the model with weed and crop image datasets (Abdalla et al., 2019; Espejo-Garcia et al., 2020). VGG16 model was trained up to 50 epochs with 32 steps in each epoch. This technique is called transfer learning, where convolution and pooling layers were fro- zen and fully connected layers were modified for new sets of problems. Instead of training and optimizing frozen layers, weights and biases,

Similar to VGG16, CNN based Residual Networks 50 (ResNet50) deep learning architecture was also used to train the non-uniform back- ground scenario (S1) data, uniform background scenario (S2) data, and combined-datasets scenario (C) obtained after merging both scenarios' data. For this training, a ResNet50 model already trained on the DeepWeeds image dataset by Olsen et al. (2019) was used. A transfer learning technique was used for ResNet50, where only 20,488 parame- ters were trained, and the remaining 23,583,616 parameters were fro- zen. ResNet50 was trained for 70 epochs, with 32 steps in each epoch. An outermost layer with 9 output classes and sigmoid activation func- tion was removed and a new outer layer was added for 8 output classes a with softmax activation function. Softmax assigns the decimal

In this study, to see the effects of background scenarios on model per- formance, models were tested with images with different background than the images used for building the model. Test datasets from both S1 and S2 were tested on three models trained and validated with S1 model, S2 model, and C model datasets. This helped to cross-check the model per- formance when the test dataset's background scenario was completely dif- ferent from the scenario on which the model was developed.

data instances over the total number of data instances. Accuracy may not be a good measure when datasets are imbalanced because it might not show a clear picture of model performance. However, the f1-score, which is also the geometric mean of the precision and recall depict clear pictures of model performance in terms of each class. Preci- sion is the percentage of correctly predicted images out of the total number of predicted images, whereas recall is the percentage of cor- rectly predicted images out of the total number of images in the actual class. Ideally, for good classifiers, both precision and recall values should be 1, but they counter each other, and increasing one usually reduces the other (Arya et al., 2020). The macro and weighted average were cal- culated for the precision, recall, and f1-score. A confusion matrix was also used to visualize the summary of the prediction results of the model. For calculating all the metrics and visualizing the confusion ma- trix, python sklearn API was used (Pedregosa et al., 2011).

The decline in model performance for S1 and S2 was obtained by cal- culating the percentage of error (PE). It was calculated by finding the difference between the f1-score or accuracy value of the model when it was tested on the same background scenario test datasets and the dif- ferent background scenario test data sets. Furthermore, the percentage of improvement (PI) was calculated for both background scenario models when the models were built with combined datasets. The signif- icance of the percentage of error PE and PI was also tested with a one- tailed paired t-test.

perform well for S1tedata with macro and weighted average f1-score value of 78% and 75%, respectively. For S2modelS1tedata testing, Red- root Pigweed, Sugar beet, and Palmer Amaranth had the lowest f1 score value of 57%, 57%, and 66% respectively. F1-score value was lower in Redroot Pigweed and Sugar beet due to low recall values of 45% and 40%, respectively; however, Palmer Amaranth had lower f1- score value due to low precision value of 54%.

23.37% and 28.40%, respectively. This means S1model was more gener- alized than that of S2model in both VGG16 and ResNet50 because of its higher performance on unseen test data with a different background. Moreover, the paired-t-test results shows that the differences in f1- scores between models VGG16-S1 (t(7) = 2.95, p = 0.01), VGG16-S2 (t(7) = 3.99, p = 0.0026), ResNet50-S1 (t(7) = 3.689, p = 0.0039),

In this research article, an applications of deep learning architecture to study the performance of the model with different image background conditions under greenhouse condition was performed on weed and crop classification task. Same crop and weed plants were planted in pots and two different image backgrounds scenarios as non-uniform background scenario (S1) and uniform background scenario (S2) was created. The Deep Learning (DL) models that were developed from the two scenarios was compared in terms of developing a more robust model and to understand how the models perform on different image analysis conditions than the condition the model was created. The DL robustness was checked for completely unseen images with different image background than that of images used for model training. The re- sults confirmed that model performance declined in both types of back- ground scenarios when model was cross-tested on images with different background scenario than that of images used during model training. In weed classification study, Espejo-Garcia et al. (2020) per- formed plant segmentation to check if deep learning model after

background removal performs better and found that result did not im- prove. The study performed in this article is different from the study performed by Espejo-Garcia et al. (2020), where different background condition was created by the application of the black pebble on the top of the promix mixture. In this study, two different background im- ages was created by application of black gravels in same crop and weed plants which is different from Espejo-Garcia et al. (2020) study. Beside the background in image, black gravel also affects the lighting around the plant due to more absorption of light by black surfaces than other color surfaces (Stuart-Fox et al., 2017). The study's results performed for plant disease detection by Ferentinos (2018) was in cor- respondence to our study results in weed classification. In their re- search, model performance decline from 99% to 68% when model trained with field condition image was tested with laboratory- conditions images. The performance decline even more to 33% when types of images for model training and testing was reversed.

The VGG16S1 model (model trained on non-uniform (S1) data) in terms of f1-score declined by 16.83% when tested with S2tedata (uni- form (S2) test data), whereas the VGG16 S2model (model trained on uniform (S2) data) performance declined by 21%, when the model was tested with S1tedata (non-uniform (S1) test data). Similar pattern of declination was observed in ResNet50, where ResNet50-S1 model performance in terms of F1-score declined by 23.37% and ResNet50-S2 model performance declined by 28.40%. This decline in performance was due to testing the models on image sets with different backgrounds than that of images used in model training. This result also shows the cause of difficulty of the implementation of deep learning models in real time weed detection applications due to different soil background conditions such as color and texture. The model's performance was im- proved by 18% to 35% in both CNN architectures when both types of im- ages were used to build a model. This suggests to used wider variety of training weed and training data collected from different geographical area, cultivation condition, imaging sensors, and image capturing mode as suggested by Ferentinos (2018) in plant disease detection study.

weed identification performance by 16% to 28% in the VGG16 and ResNet50 model, when models were tested on images with different background than that of model training. However, a model trained with combined datasets of two background scenarios, was able to achieve the f1-score value of 92% to 99% with eight classes of weed and crop images. The performance parameters for each weed and crop species in terms of accuracy, f1-score, and confusion matrix were greater than expected. The findings of this study suggest that DL models developed from uniform image background could not improve the per- formance of deep learning models to identify weeds in unseen environ- mental conditions. From this study we suggest that similar kind of background soil environment as that of field should be created inside the greenhouse to be able to apply the deep learning-based weed detec- tion model in field. In addition, it seems including greater variances in image background for the models seems working better when tested in individual conditions (when there is single variable such as non-uniform, or uniform, or clay, or sandy soil, or volcanic ash) than just developing the model in same condition and testing in same condition. Future work will focus on fusion of greenhouse and field images to create a field environment background condition to build robust weed detection deep learning model with localization.

This material is based upon work partially supported by the USDA - Agricultural Research Service, agreement number 58-6064-8-023. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the view of the U.S. Department of Agriculture. This work is/was sup- ported by the USDA-National Institute of Food and Agriculture, Hatch project number ND01487. In addition, we want to thank NDSU Plant Science department's research specialists and Agricultural and Biosystems Engineering department graduate students (Arjun Upadhyay and Billy Ram) for helping with some of the greenhouse sam- ple processes in this study.

