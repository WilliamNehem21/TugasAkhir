A novel shape-based image retrieval is presented in this study. The foreground and background contents of images are strongly concealed, so they are represented individually to reduce their influence on each other in the proposed approach. The Otsu method is employed for segmenting the foreground from the background, and the saliency map and edge map are then clearly identified. Saliency reduces the time cost for feature computation, so salient edges are computed for the foreground and background images based on the selective visual attention model. Autocorrelation-based chordiogram image descriptors are computed separately for the foreground and background images, which are then combined in a hierarchical manner to form the proposed new descriptor. This approach avoids the concealment of foreground and background information, and the new descriptor is rich in geometric and its underlying texture, structure and spatial information. The proposed novel shape-based descriptor performs considerably better than conventional descriptors at content-based image retrieval. The proposed shape descriptor were extensively tested at image retrieval based on the Gardens Point Walking, St Lucia, University of Alberta Campus, Corel 10 k, and self-photographed image data sets. The precision and recall values were compared for the proposed and state-of-the-art-approaches when applied for shape-based image retrieval from these databases. The proposed shape descriptor provided satisfactory retrieval results in the experiments.

(1) global and (2) local approaches. In global approaches, images are characterized by ignoring the local and spatial information in the picture elements. The global approaches are computationally efficient and robust to noise to some extent, but they are not satisfactory at handling issues such as variations in illumination and occlusion. However, all of the problems with global approaches can be addressed by using local ap- proaches where features are computed based on local patches, regions, or selected key points.

Shape is an important component used in image recognition and matching. In the present study, we propose a method based on shape information, and thus we focus on previous methods based only on shape information in the following. Several shape characterization and matching approaches have been proposed in previous studies. In general,

Wang et al. proposed a new variant of CID that collects the distribu- tions of the chord details for patches in images. The image is divided into a number of non-overlapping rectangular patches to reduce the influence of lighting. The CID method is robust to edge detector and it reduces the computational cost by employing predominant edgels. Statistical tests are conducted to identify the predominant edgels in each patch [47] and for every pair of predominant edgels in each patch, it is necessary to compute the distance among every pair of predominant edgels, the ori- entations of each predominant edgel in a pair, and the degree of the angle between a line segment among pair predominant edgels and the hori- zontal axis, before combining these geometric details in a local edgel chordiogram (LEC) [47]. The ordered collection of LECs for all patch images comprises the CID. The CID is robust to differences in illumina- tion, translation, and in-plane rotation, but it is affected considerably by noise. Thus, the patches with noise are eliminated during the matching operation by avoiding higher values in the similarity results obtained between the corresponding patches in the query and target images. CID is an appropriate method for place recognition with illumination changes, while the time cost is low and it can avoid fake edgels because edge detectors are used for edge identification instead of segmentation.

In a previous study, we enhanced the efficiency of this method by computing the CID using an autocorrelation function to obtain the autocorrelation-based CID (ACID) [48]. The ACID exploits the spatial correlation among identical predominant edgels at distance d, the orientation details for each predominant edgel in a pair of identical predominant edgels at distance d, and the degree of the angle along the line segment between a pair of identical predominant edgels and the horizontal axis. Our method neglects the length between a pair of pre- dominant edgels because the length is always 1 in our approach. We demonstrated that ACID performs better than the conventional CID.

In the method proposed in the present study, we computed the ACID based on the saliency edges for the foreground and background images, and obtain hierarchical feature descriptions by using the ACID to reduce the effects on each other of the foreground and background features. This method captures more geometric and its underlying texture, structure, and spatial details by considering a higher number of more responsive salient edgels (25% of the total number of edgels) than the conventional method (15% of the total number of edgels). The proposed method se- lects more salient edgels than the conventional approach in order to capture the rich underlying texture and structure information. By contrast, considering less salient edgels will reduce the computational cost, but this method fails to capture much of the underlying texture and structure information among the salient edgels. We experimentally evaluated the proposed approach by considering various subsets of salient edgels where each subset varied in terms of the numbers of salient edgels, and the response strengths of the salient edgels were considered when selecting them for a subset. The experimental results demonstrated that considering 25% of the salient edgels for extracting the rich un- derlying texture, structure and spatial information could obtain more accurate results, whereas reducing the number of salient edgels signifi- cantly reduced the cost but it yielded less accurate retrieval results. Our proposed method employs the Otsu algorithm [49] to segregate the image into foreground and background details, while the Canny operator is used for edge detection and the selective visual attention model [50] to exploit the salient edges. We comprehensively tested the proposed approach based on benchmark databases and the results were compared with those obtained using CID [47], ACID [48], and SEH [50]. The proposed approach obtained more accurate results than CBIR. The pro- posed retrieval approach is an enhanced version of our previously re- ported method [48].

A previous study showed that all of the edges identified by edge de- tectors are not valuable for characterizing an image [50]. Hence, several approaches have been reported for computing the salient edges. The term saliency is used to describe the difference between a pixel and those in its adjacent neighborhood [50]. Recently, Feng et al. [50] suggested a novel approach based on the visual attention model where the saliency of an edge is measured using its length and the saliency values around it.

Thus, the proposed ACID approach based on the selective visual attention model obtains a histogram with four dimensions. In the pro- posed CBIR, the ACIDs in the foreground and background are combined in a hierarchical manner to describe the image, and thus the proposed CBIR exploits both ACIDs to reduce the influence of the foreground and background images on each other in CBIR. The algorithm for the pro- posed CBIR approach is described as follows.

In the experiments, we compared selected methods comprising CID [47], ACID [48], and SEH [50] with the proposed approach. We evalu- ated the performance of these approaches based on the Gardens Point Walking [47], University of Alberta (UA) Campus [47], St Lucia [47], Corel 10 k [54], and self-photographed image databases. The superior performance of the proposed approach was validated by evaluating the precision and recall, and based on comparisons with the other ap- proaches with all five image databases. The precision defines the rela- tionship among the total number of related images retrieved for a given input image and the total number of images retrieved from the database, which gradually decreases as we increase the number of retrieved im- ages. The precision is defined as follows [48].

selected from each benchmark database and the top 100 images retrieved by CID [47], ACID [48], SEH [50], and the proposed approach were considered. All of the images in the databases were used as queries to assess the retrieval performance. The state-of-the-art methods and the proposed technique were implemented on a PC with an Intel Pentium Core 2 Duo 2.10 GHz processor and 2 GB RAM.

with the proposed method, while CID obtained intermediate perfor- mance and ACID performed well according to the precision and recall plots. The proposed approach is an enhanced version of our previously presented method [48] combined with that described by Wei et al. [49], and it performed better than the existing approaches with the St.Lucia database.

The results demonstrated that the proposed approach was more robust to differences in illumination and occlusion compared with CID and SEH. The proposed approach and ACID exhibited similar robustness to differences in illumination and occlusion. However, the proposed approach obtained better retrieval performance than CID, ACID, and SEH. The low retrieval accuracy of SEH is due to computation at the global level and a failure to capture the geometric details of salient edgels. CID obtained good accuracy but it ignores the underlying texture, structure and spatial information of dominant edgels, which is important for CBIR. Thus, the retrieval accuracy was reduced with CID. ACID considers the geometric and its underlying texture, structure and spatial details of dominant edgels, but foreground and background details are concealed in images and the method employed to determine the domi- nant edgels for computing the ACID leads to poor performance. These problems with the state-of-the-art methods are addressed in the proposed approach, and thus it obtains significantly better retrieval performance.

In this study, we developed a novel shape-based approach for image retrieval. In contrast to the previously proposed CID descriptor and its variants, the proposed descriptor splits the image into foreground and background images using the Otsu approach, before computing the sa- liency and edge maps for the foreground and background images. Next, the salient edges computed using the saliency and edge maps are employed to obtain the ACID by using the salient edgels in the fore- ground and background images in order to avoid missing concealed in- formation, which would influence the characterization of both the foreground and background images. Furthermore, the proposed approach considers a higher amount of salient edgels (25%) for feature computation to capture more of the rich underlying texture, structure and spatial information than the ACID and CID descriptors. Therefore, the information extracted by the proposed approach is more accurate and it is more robust to changes in illumination and occlusion than ACID and CID. However, the computational cost of the proposed approach is significantly higher than that of ACID, although this difference is negli- gible due to the higher retrieval accuracy. Experiments conducted based on five databases confirmed that the proposed descriptor can efficiently characterize the shape details and obtain better retrieval results compared with the state-of-the-art techniques. In future research, weights can be assigned to the contour-based details determined in the foreground and background images during the feature matching phase based on the saliency details identified.

