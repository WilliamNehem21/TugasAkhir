The satisfiability problem is still the most important open problem in computer science. Many scientific disciplines highly depend on efficient solutions of this core problem. These span computer science itself, mathematical logic, pure and applied mathematics, physics, chemistry, economics, and engineering, just to mention some. Approximations and heuristics are more than welcome in applications, since

It is also disappointing that the NP-complete problems are too many and essen- tial in real applications. It is rather simple to find a new NP-complete problem: just try to solve one of them and describe the main difficulty you encounter as a new problem if possible. So, adding a new problem to the list is relatively easy, but removing one from the list seems to be very hard. The reason is that the list of NP-complete problems degenerates to the empty list as soon as one of the prob- lems is discovered to be outside the list, that is, as soon as one of the problems is proved to be P-complete. This very fact is the main achievement of the theory of NP-completeness.

We favor in this paper the method of mathematical optimization. In another paper, we focused on formulating the satisfiability problem as non-convex, exact, exterior, penalty-based problem with a coercive objective function. The method focused on exact satisfiability (XSAT), which is NP-complete. The method falls into the category of approximation schemes for solving the satisfiability problem and is sub-optimal and partially heuristic in nature. In this paper, we treat the problem by way of geometric programming. We still focus on XSAT or more pre- cisely on 3XSAT, which has the following properties:

This section is devoted to the introduction of the needed theory of optimization and to fixing our notation. Vector quantities are written in bold if not obvious from the context (e.g. x). Vector components are indexed accordingly (e.g. xi). Unless stated differently, we assume throughout the paper continuity and differ- entiability of used functions. This assumption is not restrictive in our context and is beneficial computationally. An optimization problem (P) is to find the min- imum (or maximum) of a real-valued function f(x), so-called objective function,

Proof. The correctness of the algorithm relies directly on the results of Section 2. We need to emphasize, however, that only the infimum is approximated by the method and to refer to the remarks at the end of last section.  h

By construction of fi(x) and gi(x), we evidently see that these are posynomials and linear posynomials, respectively. Also, the positivity constraints of the vari- ables can be easily perturbed to strict positivity constraints. Thus, in fact, we need to solve (XSAT) for positive xk only. But now all assumptions of aforementioned algorithm are satisfied, whence the following claim.

There is no doubt that both Algorithm 3.1 and the procedure outlined in the proof of Claim 3.4 are polynomial-time algorithms. However, the issue that still needs further investigation is the (practical) correctness of these procedures. We intend to verify correctness experimentally. The main drawback of this approach is that faults in the implementation of mentioned optimization algorithms inevita- bly lead to erroneous conclusions. The paper shows, however, that the theory behind the algorithms is sound and that it predicts polynomial-time performance (with exact upper bounds). Despite this fact, a warning is here in order. Experience with other optimization methods has shown that theoretical investigation is not the whole story in this domain. One need only consider the Ellipsoid Method of optimization, which is provably of polynomial-time complexity, and which, how- ever, has poor performance in practice compared to the theoretically inferior Sim- plex Method with its exponential worst-case complexity.

