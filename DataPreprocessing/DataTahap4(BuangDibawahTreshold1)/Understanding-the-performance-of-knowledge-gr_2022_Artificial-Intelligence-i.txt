In this paper, we perform a detailed experimental study of various factors that can affect KGE model performance, using five model ar- chitectures (ComplEx [9], DistMult [10], RotatE [11], TransE [12] and TransH [13]) and two real-world drug discovery oriented KGs (Het- ionet [2] and BioKG [3]) with the goal of aiding better understanding, evaluation practices and reproducibility in the domain. The factors we investigate are the training setup of the model, the impact of changes in model hyperparameters, how model performance can be affected by both different random initialisations and changes in the train/test dataset splits and assessing performance on a domain specific task. All experiments are performed under a unified and consistent evaluation framework, on public data sources, using known best practices to en- sure fair and reproducible comparisons. Additionally we release code to replicate our results.1

Knowledge graph embeddings A growing number of approaches have been proposed in the literature which attempt to perform this knowl- edge graph completion task. In this work we focus upon the family of Knowledge Graph Embedding (KGE) techniques [17,18]. Typically, a KGE model learns a low-dimensional representation of each entity and relation in the graph. These embeddings are combined in vari- ous ways to produce a scalar value representing a measure of how

Over recent years, there has been increasing interest in machine learning with graph structured data, with approaches created for homo- geneous graph embeddings [19], graph-specific neural models [20] and knowledge graph embedding [17]. Whilst there have been numerous new model architectures proposed in the literature, there has been less work performed on understanding how these models are affected by the rest of the choices made in the machine learning pipeline, for example, how robust they are across hyperparameter values and model initialisa- tions, or how performance changes across dataset splits. However, the work that does exist demonstrates some interesting observations.

For example, several graph neural network approaches were com- pared under a fair evaluation procedure [21], which showed that a change in train/test split would drastically alter the ranking of the mod- els and that simpler baseline approaches, with correctly tuned hyperpa- rameters, could outperform more complex models. The performance of different graph neural networks for graph-level classification has also been compared [22], with results showing that baselines approaches not using the graph structure can outperform those that do. The need for consistent, rigorous and reproducible benchmarks for graph machine learning is also an area of increasing research interest [23,24].

Evaluation of knowledge graph embeddings A study comparing seven different knowledge graph embedding techniques under a consis- tent evaluation framework on the non-biomedical benchmark datasets FB15K-237 [7] and WNRR [8] has been performed [16]. The authors observe that as new models are introduced, they are often accompanied with new training regimes or objective functions, making assessing the value of the new model architecture alone challenging. They undertake a detailed comparison across combinations of models, training paradigms and hyperparameters, using a Bayesian search approach. They find that earlier and comparatively simpler models, are very competitive when trained using modern techniques [16]. However, the study did not con- sider how model initialisation or dataset splits can affect performance. In a similar study, 19 knowledge graph embedding approaches, im- plemented in the PyKEEN framework [25], are compared across eight different benchmark datasets [15]. One of the aims of the study was to investigate whether original published results could be reproduced, a task they found challenging. Additionally they perform detailed ex- periments over models and training paradigm combinations, searching over the hyperparameter space for a maximum of 24 h or 100 train- ing repeats. Again they find that suitably tuned simple models can out-perform complex ones. The study does not consider drug discov- ery datasets specifically and does not assess how models perform across

Biomedical domain specific evaluations The use of various homoge- neous graph embedding techniques has been assessed across a range of biomedical tasks such as drug-drug and protein-protein interac- tions [26]. Whilst not exploring knowledge graph embedding tech- niques, the work explores how various hyperparameters affect predic- tive performance. They explore random walk and neural network based techniques including DeepWalk [27] and Graph Convolution based auto-encoders [28], using various task specific homogeneous graphs. An additional review compares both graph and knowledge graph specific approaches and their use in the biomedical world, however no experi- mental comparisons are made between the different approaches [29].

being taken from the BioSNAP repository [32]. However often these graphs are less complex than resources like Hetionet, with a typically limited number of entity and relationship types being present. Results use k-fold cross validation to assess performance variability over dataset splits, with a grid-search over a range of hyperparameters also being performed.

The effect of different data splitting strategies for predicting drug- drug interaction using graph-based methods (including TransE and TransD) has been investigated [33]. The work argues that realistic data splits should be used in order to avoid over-optimistic results, with sev- eral domain specific and time-based splits being assessed. Additionally the work claims that the tuning of various hyperparameters had little impact on the overall model performance.

Recently, approaches exploiting knowledge graphs are being lever- aged within the drug discovery domain to solve key tasks [34,35]. In a drug discovery knowledge graph, entities often represent key elements such as genes, disease or drugs, whilst the relations between them cap- ture interactions. Many important tasks in drug discovery can then be considered as predicting missing links between these entities. For exam- ple, performing drug target identification, the process of finding genes involved in the mechanism of a given disease, has been addressed as link prediction between gene and disease entities using the ComplEx model on a drug discovery graph [5].

There are increasing numbers of public knowledge graphs suit- able for use in drug discovery [34]. One of the first such graphs was Hetionet [2], originally created for drug purposing through the use of knowledge graph-based approaches. Since its introduction, other datasets have been released including the Drug Repurposing Knowledge Graph (DRKG) [36], OpenBioLink [37] and BioKG [3].

As detailed in Section 2.1, many knowledge graph specific embed- ding models have been introduced, with the primary differentiator be- tween them being how they score the plausibility of a given triple. Here we briefly detail the models utilised in this study, but interested readers are referred to larger reviews for context and comparisons with other approaches [17,18,38]. The models we have selected are popular ap- proaches from the literature, cover a range of different methodologies and have begun to be explored in the context of drug discovery [5,39].

one or many-to-many relations being present in a knowledge graph [38]. TransH To help address the issues of TransE, another translational distance based model entitled TransH has been introduced [13]. TransH allows for entity embeddings to be given a different context depending upon the relation used in certain triple. This is achieved by modelling

It has been observed that biomedical knowledge graphs can ex- hibit a different topological structure than the benchmark datasets against which the models are typically tested [6]. For example, Het- ionet has a higher average degree than datasets like FB15K-237 [7] and WN18RR [8]. However it remains unknown how this impacts KGE model performance and we leave a detailed comparison between knowl- edge graphs from the biomedical and other domains for further work.

graph. However there are a series of choices one can make in this eval- uation process which can drastically alter the results, thus ultimately making direct comparisons between published results challenging [15]. Hence the evaluation protocol is one of the crucial aspects for repro- ducibility, as the choices made can have a large impact on comparative performance. Here we describe our own evaluation procedure in full which closely follows the one established in Ali et al. [15].

the model than these corrupted triples. One decision that needs to be taken is if any true triples, those that are already part of G, in the cor- rupted sets are removed before scoring. Following prior work [12,15], we use the filtered evaluation setting where we remove any corrupted triple which is already in the graph, as their presence can skew the results.

It is possible that two or more triples in the test set are given the same score by the model when all are being ranked and how this situation is handled can also affect the results. One can assume the extremes, where the true triple is assumed to be at the start or end of the ranked list. For this work we present the mean of the rank using these two assumptions.

Metrics We employ commonly used knowledge graph performance metrics including Mean Reciprocal Rank (MRR) and Hits@k (see [15] for definitions). Additionally we use the recently introduced Adjusted Mean Rank (AMR) [40] owing to its ability to allow comparison between graphs of different sizes. However we would like to highlight that using metrics alone, especially for use cases like drug discovery where model predictions will often result in real-world lab-based experiments being performed, perhaps should not be the sole way in which models are judged.

All work has been performed using the PyKEEN framework [25], a python library for knowledge graph embeddings built on top of Py- Torch [41]. Additionally we use the Optuna library to perform the hy- perparameter optimisation [42]. All experiments were performed on machines with Intel(R) Xeon(R) Gold 5218 CPUs and NVIDIA(R) V100 32 GB GPUs. Additionally, we kept the software environment consistent throughout all experimentation using python 3.8, CUDA 10.1, PyTorch 1.7, Optuna 2.3 and PyKEEN 1.0.6.

given enough repeats, a random search can happen upon near optimal parameters by chance, at least for these models and datasets. Indeed given the additional average runtime incurred by TPE, a random search may be the better balance of runtime and performance. One final ob- servation is the negative correlation between the AMR and Hits@10 performance, which is encouraging to see as the HPO search was only optimising the AMR value.

pared to ComplEx, RotatE demonstrates a decrease in performance us- ing the gene-disease tuned hyperparameters, suggesting it is seemingly not being able to discover an optimal set of hyperparameters using the much sparser signal offered by the reduced edge counts. It is also pos- sible that, given the much smaller size of the validation set used for hyperparameter tuning, that the model was over-fitting to these limited edges and not being able to generalise to unseen examples. Overall these experiments suggest that, when a model performs well overall, hyper- parameters learned across all relation types can still be optimal for use in a relation type specific setting.

Overall observations Our results show RotatE is often the best per- forming model of the five on both datasets and throughout all the ex- periments. This reinforces previous similar findings [15,30] and shows RotatE to be a strong baseline in the context of drug discovery. Our re- sults also highlight that older approaches like TransE can still be very competitive given an optimised training and hyperparameter setup. Re- garding training setup choices, we found that NSSA and AdaGrad were often the best performing approaches and could serve as starting points for further comparisons. More generally, it can seen that KGE models are more than just architectures and should be considered in combination with their training setup and hyperparameter values.

We assessed model performance at target discovery by predicting links specifically between gene and disease entities. This showed that, although predictive performance was comparable to when measured on relations of all types, there were some differences. This suggests that re- searchers should not assume that model performance at the general link prediction task is indicative of performance in a more focused applica- tion. Additionally, we highlighted that performing HPO to optimise a single relation type can actually hurt downstream performance on that type in a limited data setting. The issue of potentially trivial examples being present deserves continued attention, especially in the context of drug discovery where the complexity of the underlying data could am- plify the risks.

The authors would like to thank Ufuk Kirik, Manasa Ramakrishna, Tomas Bastys, Elizaveta Semenova and Claus Bendtsen for help and feedback throughout the preparation of this manuscript. Additionally, we would like to thank all of the PyKEEN team, especially Max Berren- dorf and Mehdi Ali for their help and support. We would also like to acknowledge the use of the Science Compute Platform (SCP ) within As- traZeneca. Stephen Bonner is a fellow of the AstraZeneca postdoctoral program.

