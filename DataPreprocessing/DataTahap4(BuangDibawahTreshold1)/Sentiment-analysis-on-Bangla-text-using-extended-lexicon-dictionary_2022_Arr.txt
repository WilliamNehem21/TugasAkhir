the Bangla text for predicting opinions. In this paper, we combine rule- based with lexicon dictionary approach and several hybrid DL models to predict the text sentiment from Bangla text. The key contributions of our article include (i) the implementation of a rule based algorithm BTSC [5] for generating score from the text automatically with the help of categorical weighted lexicon data dictionary (LDD), (ii) Aggregation of our proposed BTSC polarity with our input text corpus and building a word embedding model (Word2Vec) in three dimensional space [128d, 200d, 300d] for learning representation in order to fit and train on DL models, and (iii) Developing several combinations of hybrid novel DL models of long short term memory network (LSTM) known as hierarchical attention based LSTM (HAN-LSTM), Dynamic routing based capsule neural network with Bi LSTM (D-CAPSNET-BiLSTM) and bidirectional encoder representations from Transformers (BERT) LSTM with customized proper training and setting hyperparameter tuning factors. Our paper is structured as follows: Section 2 provides a survey of SA using DL. Section 3 discusses the methodology of our research work which includes a text preprocessing mechanism to fit the data in the neural network model. In Section 4, a description is provided for the detailed experiments of the proposed models. Section 5 shows the per- formance results of the proposed model and compares the results with the existing research. Finally, Section 6 provides concluding remarks.

Short Term Memory Network (LSTM) [12], Bi-Directional LSTM [13], Asymmetric Convolutional Bidirectional LSTM (AC_Bi-LSTM) [14], Re- current Convolutional Neural Network (RCNN) [15], Gated Recurrent Unit (GRU) [16]. Hierarchical Attention Network (HAN) based mecha- nism [17] on Bi-GRU [18] and LSTM [19] are also applied for document text classification because it works on between the hidden (encoder and decoder) layer to give weighted sum all features fed as an input. A recent NLP task, Transfer Neural Network BERT is published by the Google researchers [20], which learns contextual relation from words or text is also applied for SA [21].

the core word as annotated polarity with sentence or phrase sentiment strength. In [33,34] authors build a sentiment detection mechanism from tweets by using sentiment lexicon and according with a linguistic rule-based approach [35]. To the best of our knowledge, SA using categorical weighted LDD and rule-based algorithm BTSC in Bangla text with comprehensive DL approaches is not used yet.

with 49 filter size and convolutional 1D with (64*50) size. A flatten fully-connected layer is added with a hidden layer. Dropout layer is used before the independent weights with 50 neurons having ReLU activation layer. Finally, each neuron from the fully connected dense layer is fed as output to the sigmoid layer with three neurons.

Difference form DCNN we have used a three dimensional word em- bedding layer [128D, 200D, 300D] having with ZeroPadding1D(filter_ size -1, filter_size -1) add on three convolution1D layer with filter iteration as 3, 4, 5 sizes and GlobalMaxPool1D except k-max pooling layer. After three iterations, we get three layers (Layer_1, Layer_2, Layer_3) concatenated. This flatten the merge layer with l2(0.01) reg- ularization, dropout of 0.5 and finally attach a three dense neurons of fully connected output with sigmoid activation.

information need to update or ignore. The forget gate decides what part of the information needs to be removed from the previous cell state of the previous hidden layer. The output gate concatenates the input with sigmoid layer and decides what part of the current cell state through a tanh function and multiplies it. Our LSTM model consists of 32 unit hidden layers and Unlike RNN, at some timestamp, the

AC_Bi-LSTM layer is a hybrid model combination of CNN and LSTM approach. In our sentiment classification, we applied that hybrid model. In our experiment, we use a 128D word embedding layer, which iterated over with convolution1D layer with 100 filter size, kernel_size of 2 with ReLU activation layer along with another convolution1D layer of 100 filters and 30, 40, 50, 60 sizes of the kernel with ReLU activation layer. This is called asymmetric because of having different kernel sizes on the same filter and activation layer unit. Then these two different layers of conv1d are merged, and the input (xt, xt+1, xt+2 . . ., xt+n) is passed to the LSTM layer of 32 units and the rest is followed by the previous LSTM network.

of 0.2 and recurrent_dropout of 0.1. As similar form Bi-LSTM, Bi-GRU network has two states to resolve both contextual relations in left to right (Forward) and right to left (Backward) except maintaining no cell state mechanism. At time stamp, each hidden layer output is produced and pass it to a convolutional1D layer of 64 filters of kernel_size of 4 and the rest of the network is followed by the previous GRU network.

Capsule neural network is a group of neural network which solve the invariant of local feature problem of CNN pooling or max-pooling operations by providing vector output capsules especially in dynamic routing algorithms. The computational complexity i.e., reducing matrix dimension, intercepts the various features, is captured by the pooling operation while losing a lot of data based on spatial relationships; however, without changing each feature. Again, CNN does not capture the hierarchical relationship between the local and global features. With the help of dynamic routing, it establishes the connection on spatial relationships between entities by mapping nonlinear vectors. This mapping transmits the capsule from lower level to upper level by iterating a number of routing loops based on a weight coupling coefficient. The weights coupling coefficient determines the leaning representation of which lower-level capsule will be forwarded to the upper level capsule layer. It also detects the similarity between vectors which also predict the lower and upper-level layer capsule. In our

recall, and 79.86% F1-score. VDCNN and MVCNN both are complex models than CNN and DCNN because of using three dimensional (D) [128D, 200D, 300D] word embedding layers. CNN achieves 74.50% accuracy which is better than the 73.49% accuracy of CNN model.

high dependency on using a multichannel layer with different iteration filters [filter kernel size = 3, 4, 5]. Here, MVCNN uses two dimensional [128D, 200D] and VDCNN uses three dimensional [128D, 200D, 300D] word embedding layers. We keep the dropout rate at 0.5 on each layer but we change kernel size when variable size of zero padding1D is added for performing spatial dimension to the output.

When we add a number of nodes in the layer in our model, the capacity increases that means accuracy, precision, and recall increase. As our training data is small, our model is pretty small; however, increasing model layers can drive to a more precise model. If more training data are given in the model, the larger the model should be. In our experiment, multilayer perception of CNN is applied in DCNN, VDCNN and MVCNN models; as there is no bound to use a specific number of layers so this stacked layer is susceptible to generalize our model better. By adding continuous layers of convolution and pooling operation in CNN, it might lose spatial information on classifying data.

The attention level in word and along with sentence increases the model accuracy with respect to other CNN, RNN type models. Calculation of attention vector which is co-related to word and sentence level that determines the less content for constructing the document representation. The main advantage of Capsule module is to resolve the max pooling level conversion for feature extractions that means the im- provement of CNN and RNN type models. Because losing the informa- tion in polling layers might cause in lesgs accuracy. Again, augmenting the compositional capsule network with k-clustering mechanism will improve the classification accuracy of our HAN-LSTM model.

= 1e-5) producing maximum accuracy of 84.18%, precision of 86.45% from other models. We add a LSTM layer on pre-trained BERT model, which amplifies the core advancement in classification accuracy over embedding models. That means BERT is in fact more able to represent semantic and syntactic features. This language model representation achieves substantial improvements over other models at a state of art result compared to Word2Vec model.

Deep Learning model complexity implies to investigate the gener- alization capability and limitation of neural networks on the training process. Our hybrid DL models are hyper-parameterized but it has very little effect on model complexity [55]. Model complexity indicates how the neural network model expresses its behavior on distribution function or activation function [56], prevents overfitting on by adding L1, L2 regularization to the loss function [57] and the amplification coefficient (wij) which is defined by the multilayer perception of hidden neurons. Selecting an activation function i.e., ReLU, Sigmoid, tanh in network hidden layers which is an active module for learning and computing complex task by taking piecewise nonlinear transforma- tion to the input. Pooling function such GlobalAverage Maxpooling1D, GlobalMaxpool1D with the use of variation of filter size on feature maps is needed to reduce fixed size vectors. However, the size of the DL model impacts on model complexity i.e., number of filters, kernel size,

In this paper, we investigated the most noteworthy work on sen- timent analysis on cricket reviews as a low resourced Bangla dataset using various deep learning architectures. This empirical study is an initial dive on lexicon dictionary-based approach on neural network mechanism. We measured the performance of our work based on accuracy, precision, recall and F1 score. Firstly, we developed a lexicon- based approach and used the BTSC algorithm to detect polarity from preprocessed text from our previous work. Then we implemented the popular basic learning models i.e., CNN, DCNN, MVCNN, VDCNN, RNN, RCNN, LSTM, Bi-LSTM, AC_Bi-LSTM, GRU, Bi-GRU, HAN-LSTM,

D-CAPSNET-Bi-LSTM, BERT-LSTM with setting as a customized and fine-tuned with hyperparameters on individual models. We found that LSTM had given better results over CNN and RNN type models. Then we used attention, capsule, transformer-based mechanism by adding LSTM layer, and the results showed significant improvement, which is indeed effective in the sentiment classification effect. By using atten- tion, capsule mechanism, we proposed some hybrid DL models, termed as HAN-LSTM and D-CAPSNET-Bi-LSTM along with semantic learning representations (word2vec) having excellent performance in terms of accuracy, precision, recall and F1-score. Finally, a hybrid model termed as BERT-LSTM was used for the classification task, and it surpasses others in terms of accuracy and precision.

This LDD dataset will be useful for future research. Bangla cricket review dataset is relatively small with respect to benchmark dataset and there is lack of enough large training corpus in Bangla domain, this is the main drawbacks in sentiment analysis tasks in Bangla. We have identified trinary polarity in cricket reviews since this dataset is not properly balanced data; so, using balanced data for each polar- ity in training process might enhance accuracy of prediction results. Because increasing the amount of training data can assist to promote the accuracy of prediction results. We could not use a well pretrained model due to lack of hardware resources, so in the future, we developed a large corpus and trained it with various parameters or layers with a tuned model and showing individual model with confusion matrix. Despite the fact that we gained satisfactory result, it still has a scope for further improvement. In the future, we will conduct our research using

memory augmented network. This will be done by combining external memory of neural network and other transformer models i.e., ALBERT, ELECTRA, RoBERT, DistilBERT along with multilayer, and also hybrid capsule and multi headed attention layer-based models with other word embedding learning representation mechanism such as Glove, fastText, etc. We will design a graph neural network to capture internal and external graph structures of NLP. Finally, this research work will contribute in the improvement of emotional sentiment analysis on Bangla domain corpus.

