degenerative disease [1]. More accurately, it is a progressive disease due to loss in structure and/or function of neurons in the substantia nigra, which might happen in elderlies. PD affects millions of people world- wide, and its diagnosis in the early stages is vital, as there is no cure for PD yet. The current solutions introduced to the disease mostly aim to slow down the progression process [2].

relatively old data set. Yet, they both have been subject of interest for many researchers worldwide. Another type of data recording is neuro- imaging, such as Magnetic Resonance Imaging (MRI), which provides information about brain structure differences in healthy and PD patients [9]. Applying ML techniques to these data sets can provide a more reliable prediction of the disease and aid the clinicians for a more ac- curate diagnosis.

Machine Learning approaches have been increasingly used in med- ical diagnostics in recent years. Since the clinical detection of PD in the early stages is difficult, these ML techniques have been developed to aid clinicians in PD detection [4]. Having acquired the data, we need to choose the ML procedure to apply. There are a varied number of clas- sifiers and preprocessing techniques to choose from. Support Vector Machines (SVM), Random Forests (RF), and Artificial Neural Networks

high prediction accuracy. Normalizing data, feature selection, and feature extraction are some tools to enhance model performance. In the case of neuroimaging data, such as MR images, some image processing techniques might also be applicable. Although several research works improve the automatic diagnosis process, enhancing the accuracy is still an open problem.

The contribution of the paper is finding a proper modeling for early diagnosis of PD. In this regard, we employ ML classifiers, namely SVM, XGBoost, and Multilayer Perceptron (MLP), to diagnose PD in the early stages from vocal characteristics. The SVM and MLP are older methods, while XGBoost has been offered in recent years. Yet, they all contribute to many applications in the data science community as they are powerful tools in the literature. An issue here could be the number of features compared to the number of samples, making the training procedure more difficult. Therefore, we then train an autoencoder to extract beneficial features to feed into a classifier, in this case, an SVM or a single sigmoid neuron. Later, we aggregate the predicted outputs and stack them for more precise predictions. We try simple averaging and Logistic Regression ensemble methods for this aim. Finally, we conclude that using a Logistic Regression to stack the outputs of SVM, XGBoost, MLP, and autoencoder-preceded SVM provides an accurate classification of PD patients. These results are validated by a 5-fold cross-validation method on min-max normalized data.

challenges mentioned above more effectively than deep learning methods. Although the autoencoder approach we used is an almost-deep learning approach, we still use an SVM to classify the data. Using a deep feed-forward neural network that extracts features based on class labels does not seem promising in such tasks.

As mentioned, PD is a central nervous system disorder resulting in a loss of motor function, increased slowness, and rigidity [20]. The most visible symptoms are related to motor functions. AI-based techniques can be useful to detect signs like tremor or bradykinesia (refers to slowness of movement). Unfortunately, these symptoms do not help us diagnose the disease early as they become apparent later.

Single-photon emission CT and PET scans are also known to be effective in disease classification [25]. In another recent study [26], shape features extracted from a single-photon emission CT scan on dopamine transporters are used for this aim. Besides, in Ref. [27], Jia- hang et al. extracted features from PET scan and used an SVM to classify PD patients. Deep belief networks are also accompanied by PET scans in another recent study [28].

orders, and vocal impairment [1,29]. However, more computational studies were made on vocal symptoms than on other non-motor symp- toms since gathering this data is easier. In the next subsection, we will review some works done on PD classification using vocal data.

large numbers of patients, he developed a method to identify patients suffering from both action and rest tremors. He observed patients with active tremor had symptoms like weakness, spasticity, and visual disturbance. In contrast, those with rest tremors differed in rigidity, slowed movements, and a very soft speech [30]. This was the very first time that speech symptoms took a severe role in determining PD occurrence.

Since then, some other studies have been devoted to applying ma- chine learning techniques on this data set to improve the prediction. In 2019, Polat applied a Synthetic Minority Over-Sampling Technique (SMOTE) to overcome the imbalance data samples problem and then used a Random Forest model to classify the samples [35]. In the same year, Nissar et al. tested a wide range of classifier methods, namely SVM, Naive Bayes, Logistic Regression, KNN, MLP, Random Forest, Decision Tree, and XGBoost, followed by Recursive Feature Elimination (RFE) and mRMR feature selection methods [36]. They reported a combina- tion of mRMR and XGBoost as their most efficient methods. However, in Ref. [38], an MLP also scored a high accuracy level.

In a different approach, we attach the encoder section of the autoencoder to a single sigmoid neuron. In some other trials, we append an SVM with an RBF kernel to this encoder. SVM classifier also performs accurately on the raw data, though it needs a more complicated struc- ture than the one following the feature extractor.

XGBoost: Generally, our experiments show that XGBoost performs better than MLP on this data set. If we want to get high accuracy on this data set with XGBoost, we have to avoid overfitting; because, for this data set, it is easy to end up with models that perform well on the training, but the test accuracy is much lower. Fortunately, in XGBoost classifiers, there are many parameters that allow us to avoid such a problem. Parameters such as colsample_bytree, which is the percentage of features used per tree, and subsample, that is the percentage of samples used per tree.

As explained in Ref. [8], these features are all informative, and an enhancement in predictions is achieved by adding the TQWT features. Yet, the total number of features is large compared to the number of samples. This yields more complicated models, as described in the pre- vious section. Therefore, we tried to extract fewer numbers of features by training an autoencoder. Having trained the autoencoder, we can pick the encoder part as a feature extractor and feed its output to

We used a single neuron with sigmoid activation function and Adam optimizer for the classification part in a couple of trials. We reached an accuracy of about 0.84 by coupling it with Autoencoder 1(explained in the previous section). The batch normalizer helps scaling the activations and hence affecting the learning rate. Training and validation loss of the autoencoder, in this case, is around 0.009 on average, which is the smallest value compared to other structures. Changing the number of layers and hidden neurons of each layer contributes to a high number of parameters to be learned or not enough parameters to generalize. As mentioned before, RMSprop optimizer is used on the model. RMSprop uses the momentum term, restricting vertical oscillations and speeding up convergence. Yet, decreasing the learning rate increments the oscil- lations and issues divergence.

To enhance the classification score, we fine-tune the weights of all layers. In other words, we retrain the encoder section of the autoen- coder, followed by the single neuron. This procedure is similar to training an MLP with some pre-trained weights. This process increments the classification score significantly, and an accuracy of 91% is obtained (Row 3). As the resulting classification structure is now a deep neural network with pre-trained weights, it can be interpreted that deep neural networks might also have comparable results to classic methods if there is an unsupervised learning mechanism in computing initial weights.

Moreover, we train a Logistic Regression stacking model (Row 12) with an L1 penalty on Row 4 to Row 7 outputs. We use 5-fold cross- validation to validate its generalization ability. The results are almost the same as what is resulted from the simple averaging method. The unweighted average stacking and the Logistic Regression model both achieve an accuracy of ~95% and an F1 score of ~97%.

development rate. The availability of data in this era has motivated scientists to use this data for their purposes, one of which to be medical purposes. A variety of data is published for the objective of studying PD, including gait, handwriting, neuroimaging, and voice records. Using machine-learning algorithms, scientists have devoted their time, studying these data to predict the disease. In this research, we tried to review some studies devoted to PD using data and developed our models using vocal data.

Processing vocal signals gives rise to applicable features. SVM is believed to be a practical model trained on this data. Our studies also support this belief. Furthermore, we try to introduce the application of autoencoders for the purpose. Training autoencoders and using the encoder section for extracting a nonlinear combination of features is shown to be useful. Stacking the developed models also resulted in predictions that are more accurate and precise.

As a result, we offer autoencoders as good feature extractors. Autoencoders are not widely used as a feature extractor but the evidence in this study suggests that they can be applicable in cases where there are few total number of samples compared to number of features, especially when the data is imbalanced. It not only reduces the complexity of a classifier, but also provides accurate classification. Note that in the era of deep learning, using classic classifiers which use lower resources but have near-similar or better results is more valuable. As the comparison of methods offers, artificial and deep neural networks did not contribute to much better results than what we achieved.

Ensemble learning also played a crucial role in improving classifi- cation accuracy. Ensemble learning has been of great importance recently and has been more widely used. Our experience also suggests it. Stacking results of classic classifiers, which themselves are time- efficient, has low cost and predicts more accurately. Using classifiers which are trained on a subset and/or combination of features in the ensemble can massively improve its result.

