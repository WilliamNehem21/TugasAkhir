Due to the emergence of digital data and communication networks, a huge collection of repositories has become available to the public. Many tools are available to extract information [1] from the various sources, but it is essential to extract data effectively and accurately. Some re- searchers came up with models and discussions where integration and indexing play important role for mining the information or querying. Integrating and indexing needs an accurate learning model to associate the data [2] and have an effective result. This task turns more challenging to develop efficient data integration and indexing for relevant informa- tion when the data is in unstructured form of distribution.

Data integration is a computationally efficient and accurate method in the field of data mining for the data categorization. The purpose of data integration is to serve trusted data from a variety of sources. It is a need to handle a large data set in terms of volume, dimensions, and complexity of data features. The core objective of data integration in real- time data from various domain sources is to generate valuable and meaningful information. The conventional data integration techniques

The source of information from many heterogeneous data source generates several challenges for integrating with the conventional data integration techniques. The heterogeneity can exist at the schema level, where different data sources often describe the same domain using different representations. It can also exist at the instance level, where different sources can represent the same real-world entity in different ways. For example, the bibliographical data of academic articles repre- sents diverse information but contextually they are related to a subject category. Many approaches for analyzing the heterogeneity data for integration are suggested in past through schema mapping [3], record linkage through object reference and entity matching. But the challenges are still lying for integrating multiple source and unstructured data ob- jects, due to its probability of information description and diversity in the features.

The structure of this paper is as follows: Background study is dis- cussed in Section 2. Section 3 represents the proposed integration and indexing method, Section 4 is a discussion of the experimental evaluation and results. Conclusion of the proposed work is summarized in Section 5.

As discussed in Ref. [6] of 2018, data integration system needs to handle uncertainty levels on the semantic mappings between the data sources and the mediated schema. It is essential for effective construction of indexing based on the keywords for data accessing queries. It is possible by tagging features for schema elements. It helps to discover mappings through understanding the meaning of tagging features. But as per work done in 2019, it has many challenges for understanding the dependable features [8] and its associations for accurate integration [10].

observations for a feature. In fact, all the features are often not important and distinctive because they are mostly found correlated or redundant to each other [11]. It might be very noisy for the selection [12] in some scenario. The function of these high dimensions may lead to low effi- ciency or poor performance in conventional learning models [13]. Therefore, it is challenging to learn the high-dimensional data features which will help to improve the accuracy and comprehensibility of results. It also has challenges to eliminate the unrelated and repeating features that involve in large volumes of data, which is a need to select from a subset of the data feature.

Integration is a computationally efficient and accurate method in the field of data mining for the data categorization. It coordinates to handle a large data set in terms of volume, dimensions, and complexity of data features. Various approaches are proposed for multi-domain [6] and uncertain data [8] for efficient and accurate integration. An uncertain data objects clustering based on the probability of similarity distribution is presented in recent paper [9] of 2019. It suggests that, the difficulties of integrating the uncertain data object is due to probability distribution which arises in many situations. The problem related distribution based feature similarity can be worked out effectively by means of the feature selection [11] and the feature extraction [14] methods. It will reduce the dimension of features finding a meaningful feature set group and selection.

Feature selection method is classified as a regulation method of feature selection for the mostly termed as "supervised" or "unsupervised" feature selection. Supervised feature selection methods uses the rela- tionship between characteristics and feature information [13] that leads to selection of the significant and relevant features [11]. But studying large volume of data in unsupervised feature selection leads to difficulty for analysis. It happens due to deficient of feature information to refer feature selection. Hence this article focuses on the issue of unsupervised feature selection.

As there is no enough information about the data features and small features, supervised algorithms may unintentionally remove many spe- cific features or fail with select features that are not relevant. Thus the semi-supervised feature selection can be developed to take advantage of the labelled and labelled data. But semi-supervised feature selection system has to guide to search for distinctive features in absence of labels. Hence it is seen as a much more difficult problem [13]. It evaluates the characteristic of interest for their ability to retain some element prop- erties. In many real-world applications, the lack of unlabeled data and rapid accumulation of high-dimensional resources creates the challenges for the labels identification. Hence its demands for a very promising and autonomic system for unsupervised feature selection technologies to meet the integration problem.

In unsupervised feature selection based integration, information identifies a subset of the features of holding a unique group as no label is available. These groups are in accordance with the specified standard for the more challenging grouping or clustering criterion [15]. The diffi- culties in the unsupervised feature selection are mostly being solved using the probability model methods [9,15]. Here the features are sorted into the group of labels utilizing the "feature selection based grouping" considering the latent features variable. Venkatesh, B, and J. Anuradhal

[13] proposed a visual feature and user separate class features for a generative model in 2019. The purpose is to the group the high dimen- sional relevant data. Similarly W. Fanet al [14] proposed a probability model in 2013 for the global integration capabilities and unsupervised feature selection.

Y. Guan et al. [22] suggested a framework of reasoning of the vari- ations of the "unsupervised non-Gaussian" method for feature selection. Unlike previous research, this work is based on the unsupervised learning model for data transformation and feature selection, the purpose to enhance the data integration task for better results. It will maintain se- mantic similarities along with selection of best features for distinguishing knowledge of the original data information in heterogeneous datasets. The feature Patterns is applied to each input key-value pair to generate an arbitrary number of intermediate key-value pairs. The reducer is applied to all values associated with the same intermediate key to generate output key-value pairs.

The biblometric study [7] is carried out in 2018 to analyze the inte- grated care literature and tests the usefulness of indexing the literature within PubMed. This idea boosted to experiment with Bibtex data on big data platform. Few Literatures [5,9] guide to know and analyze multiple techniques for indexing with pros and limitations.

In this paper [19], the modified Feature vector selection (FVS) method, easy tuning version of Support vector machine(SVM) selects a small number of data points for implementation. The FVR model is also solved analytically, as in least-squared SVM. Authors had worked with twenty six imbalanced dataset and comparison is done with several SVM based methods. Result show effectiveness of suggested method. Hence proposed method is also compared with SVM in integration performance analysis section.

This method gives better results for limited number and features of attribute. In addition to this, the limitation of the proposed algorithm is computationally slow because of the big database. It is clear from the recent literature review, that the Naive Bayes has wide scope to experi- ment with big data but is not sufficiently experimented over all with following combinations, such as.

So in this article the above identified gaps have be experimented over Bibtext data. The performances of PFP with Naive Bayes, SVM and TF approaches and presented after analysis and measure. Further perfor- mance of proposed F-LSA for indexing is justified by comparison analysis of PFP with F-LSA and TM with LSA. It is also justified for different datasets like CiteseerX, Bibtext and Cora at concluding point.

through counting their co-occurrences [24]. However, this might not be applicable for the domains where such kind of feature information is not available. In some cases, the fundamental dependency and correlations of feature are identified using association rules algorithm, but its fails to facilitate the multiple changing datasets and its features in different domains.

difficult to retrieve. In such cases, structured or unstructured data files delimited with comma or spaces are downloaded. It is easy to process and analyze if the files are limited but in case of high number, diverse and unstructured data makes it quite complex to process and analyze.

In this paper, data files of different technical article publishers are acquired from Information Extraction and Synthesis Laboratory (IESL). It aims to support the mining actionable knowledge from unstructured text. The gathered data containing a more than 4000 number of BibText files which have more than 6 million article records downloaded from the internet. The data records are extracted through processing each indi- vidual file and imply the data cleaning method to remove the data con- taining noise and filling up the missing data.

Features are extracted to perform the feature transformation and se- lection for integration. The selected features are semantically associated to classify using the pattern generated through SDRL. Each data records consists of few key data fields such as author, title, keywords and abstract known as metadata of the article. The terms are extracted from the keywords and abstract field to construct a set of terms which will use for the semantic classification to classify the article class for the integration. Semantic classification performs relevance association computation to relate the latent semantic relevancy [28]. Since, the article is technical information, the semantic-based classification [29] will compute the similarity between the set of terms information constructed to identify

[29] as per work done in 2018, where co-occurrence features can be acquired by the transfer relations between the records. The co-occurrence information of the terms can be captured when Singular Value Decomposition (SVD) is decomposed as proposed by Refs. [30] in the year 2019.

The degree of similarity in features reflects the correlation between the terms. The weight value not only reflects the correlation in the fea- tures but also embodies the co-occurrence information between the features in SVD space. The similarity degree in the documents mainly depends on the number of the co-occurrence of features. In generating latent semantic space, the latent relations will be excavated because of the transitivity between the features.

The Java program performs the integration through semantic classi- fication of data record with the help of knowledge pattern. After completion of integration, the indexing method F-LSA is executed to rank the integrated data group. To evaluate the improvisation in the indexing, a Data Access Visualization interface is built, where multiple queries are submitted to measure the accuracy of the outcome.

Many previous studies have only been used to analyze the purity metric and evaluate the clustering algorithm performance. However, when a larger number of clusters are available for the integration, it is easy to achieve purity measure. Particularly if each object has its own data clusters purity measured as 1. In addition, many of the partitions have the same purity as they are associated with each other. For example,

An experiment result analysis is provided for the integration and indexing through comparing the outcomes in their normal and with the proposed method. In the Normal analysis form, data integration is made simply based on it terms related to the class. In case of proposed inte- gration the integrated data clusters is analyzed against the class articles through measuring it purity and NMI.

Purpose of indexing the records is for faster access and quick retrieval. Accuracy depends on indexing as it depends on feature representation. Good/unique features results in better accuracy. Good features reveal data clearly and any method depends how quality features are extracted for indexing.

The selection of features and semantic analyzing is able to enhance the integration in big data. A process of semantic data relation learning (SDRL) method is discussed to learn k-features related to collection of data. These features are analyzed to predict a one-feature class of a data. Further effective grouping is done for Integration. The process of indexing utilizes the latent semantic analysis (LSA) method to under- stand the degree of similarity between features. The correlation between the terms of data records ranks appropriately for indexing.

Madhu Mahesh Nashipudimath would like to thank Principal and Management of Pillai College of Engineering, New Panvel, Navi Mumbai, India for their continue support and cooperation during this research work. Special gratitude to Dr. Satishkumar Varma for his constructive criticism and suggestions.

