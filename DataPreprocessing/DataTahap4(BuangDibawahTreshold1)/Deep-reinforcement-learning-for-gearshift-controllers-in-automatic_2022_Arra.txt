automated methods for gear shift control exist. Manual calibration is mostly done in tedious road tests by changing the parameter maps until the resulting shifts meet the intended characteristics of the ex- ecuting engineer. Many objective criteria exist to express subjective shift quality, e.g. the Vibration Dose Value (VDV) [9] or the amplitude and Root Mean Squared (RMS) value of longitudinal acceleration at the driver seat [10]. Using these objective evaluation criteria, different approaches of calibration automation have been developed and pub- lished, leveraging e.g. Fuzzy Logic and evolutionary algorithms [11,12] or gradient-based optimization over high-order polynomial functions mapping control parameters to comfort [13].

Sommer Obando [24] applies tabular Q-Learning and SARSA algo- rithms to control normal force on a friction clutch to reduce clutch judder. However, this work considerably simplifies the control task neglecting several aspects of realistic automotive transmissions such as nonlinear actuator dynamics and partial observability. Furthermore, substantial loss of performance is reported when transferring the agents from simulations to the experimental transmission.

Lampe et al. [29] use a DDPG agent to learn a closed-loop policy in a simplified simulation. They show that the resulting agent can control clutch engagement for vehicle start up in Automated Manual Transmissions (AMT) and Dual Clutch Transmissions (DCT). However, no validation on a physical transmission or car is carried out.

In the considered case, and in general for most gear shifts, the conventional actuation logic consists of an open loop control and a sub- ordinate PI-controller in combination with adaptive schemes to achieve the desired trajectory of the clutches slip speed. The shift process can be subdivided into two major phases: The filling of the piston until the piston touches the friction plates and the synchronization phase. During filling, no change of variables can be measured. Therefore the open loop controller follows a predefined current trajectory that uses parameter maps designed for a range of operation points like temperature and torque demand. This open loop control is adaptively improved based on initial changes of the speed from previous shifts. During synchroniza-

The deviation is used for feedback-control of the actuation current. The controller can increase or decrease the transferable clutch torque and therefore the slip speed and avoid undesirable jerks. However, this conventional control strategy has several drawbacks, e.g. the limited al- lowed actuation range, to ensure corrections with a small jerk, and the impossibility to predict and counteract future offsets, since PID-control can only react to errors already occurred.

The shifting control task introduced in Section 3 is aimed to be solved by DRL algorithms. In particular, two state of the art (SOTA) model-free DRL approaches are considered, namely Proximal Policy Optimization (PPO) [51] and Soft Actor Critic (SAC) [52] are applied to the control problem. A brief introduction to the general concept of reinforcement learning (RL) and the leveraged algorithms is given in Section 4.1.

SAC is an actor-critic Q-learning off-policy DRL algorithm. Hence, state transitions observed under any policy can be utilized for pol- icy learning. Besides other important concepts of continuous deep Q-learning, such as a replay buffer and target Q-networks [1], the actor-critic architecture [56] and clipped double Q-Learning [57], SAC incorporates the soft MDP formulation introduced in [58]. Conse- quently, SAC learns a stochastic policy that maximizes the expected return as well as the entropy of the policy. Our implementation utilizes the improved architecture of SAC proposed in [52].

normal force is applied to the friction plates resulting in a slipping torque close to zero. The threshold of 10% is an engineering decision based on expert knowledge, making the phase definition robust against sensory noise in the determination of the current relative rotational speed. The phase corresponding to a shifting progress from 10% up to 90% is referred to as power conversion phase. During this phase the piston applies a significant normal force on the clutch and the main portion of the synchronization is achieved. The harmonization phase corresponds to a shifting progress from 90% to 100%. During this phase, the last portion of relative rotational speed is synchronized. Empirical research shows, that the agent benefits from this subdivision of the synchronization phase, since the additional change of input state leads to a smoother final synchronization.

of the distributions are either expectations from expert knowledge or optimized to fit the dynamics of the real transmission. The variance is chosen as an engineering decision yielding sufficiently robust but not overly conservative agents. Additionally the policies have to be robust against noise from sensors. Therefore, Gaussian noise is added to the relative rotational speed of the simulator before calculating the shifting process for the state. For a full list of randomization parameters the reader is referred to Appendix A.2.

A major challenge in the hydraulic control path Appendix A.1 is the delayed system response during the filling phase, where no change in relative rotational speed is measurable until the filling is completed. If the chamber gets overfilled, the piston reaches the touchpoint with a high velocity, resulting in a high initial jerk. Due to the partial observability, the agent is guided by the fill state estimator, that has a high effect on its behavior during filling. In this test driven domain adaption approach (DA1), the policies are trained on the standard setting of the simulator, where the dynamic parameters are chosen as to be assumed on the test bench. DR is active during training to create robust agents, that can deal with small differences between simulation and test bench parameters. By transferring those trained policies to the test bench, multiple gear shifts are executed with the standard policy.

The authors would like to thank Bernd Frauenknecht for implement- ing the PPO algorithm and the DA2 approach and Thomas Doster and Katja Deuschl for their creativity and commitment to promote the novel techniques for transmission control. Specials thanks are forwarded to Emmanuel Chrisofakis and his colleagues to push the limits of the physics simulator. Moreover, the foresight of our management to sup- port this novel technique from the project start until today is highly appreciated.

