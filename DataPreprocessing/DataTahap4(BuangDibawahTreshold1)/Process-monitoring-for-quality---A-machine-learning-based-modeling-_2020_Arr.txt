Process Monitoring for Quality is a Big Data-driven quality philosophy aimed at defect detection through binary classification and empirical knowledge discovery. It is founded on Big Models, a predictive modeling paradigm that applies Machine Learning, statistics and optimization techniques to process data to create a manufacturing functional model. Functional refers to a parsimonious model with high detection ability that can be trusted by engineers, and deployed to control production. A parsimonious modeling scheme is presented aimed at rare quality event detection, parsimony is induced through feature selection and model selection. Its unique ability to deal with highly/ultra-unbalanced data structures and diverse learning algorithms is validated with four case studies, using the Support Vector Machine, Logistic Regression, Naive Bayes and k-Nearest Neighbors learning algo- rithms. And according to experimental results, the proposed learning scheme significantly outperformed typical learning approaches based on the l1-regularized logistic regression and Random Undersampling Boosting learning algorithms, with respect to parsimony and prediction.

Several researchers have worked on this problem. A framework for multiple release problems using a two-step fault detection procedure and fault removal process was proposed by Ref. [1]. A state-of-the-art report of the most important papers in this domain is presented in Refs. [2]. A similar project, but based on Bayesian Nets was suggested by Ref. [3]. A data mining approach for defect analysis and prevention of industrial products in Ref. [4].

over/under-sampling methods, (2) cost sensitive learning, (3) kernel-based learning, (4) active learning and (5) novelty detection. Where oftentimes the inherent ad hoc data manipulation (e.g., over/- under sampling, miss-classification costs, using small pools of data) approach lacks of theoretical foundation and principles to guide the development of systematic methods that can be efficiently generalized. In this context, learning from the original data set is identified as further research work [12]. Basically, the authors encourage the research com- munity to investigate the development of algorithms/methods that could learn from whatever highly/ultra unbalanced data is presented without

The proposed scheme combines the Hybrid Correlation and Ranking- based (HCR) and ReliefF filtering algorithms to select the most relevant features. To boost parsimony, a set of nested Candidate Models (CM) is developed and then, the Penalized Maximum Probability of Correct Decision (PMPCD) MS criterion is applied to select the final model. It can be virtually applied to any learning algorithm in which complexity is defined by the number of features in the model. Its universal applicability is demonstrated by analyzing four highly/ultra-unbalanced data sets using different learning algorithms. Empirical results demonstrate its capacity of finding a good quality solution after creating a few CM.

This paper is organized as follows: A review of the theoretical back- ground is in section 2. Section 3 describes the PMQ-L framework, fol- lowed by four binary classification empirical studies in section 4. A comparative analysis is given in Section 5. Finally, section 6 concludes the research.

Filter (preprocessing) methods are applied before the learning process to eliminate irrelevant/redundant features. Relevant features are selected using a predefined fitness function, which can be based on de- pendency, distance, consistency or discriminative capacity [16]. Computed scores are used to determine the fitness of each feature and are compared with a relevance threshold to select a subset of relevant fea- tures [10]. These methods select features independently of the learning algorithm.

Hybrid approaches have been proposed to take advantage of the particular characteristics of each method [21]. These approaches mainly focus on combining filter algorithms with either wrappers or regulariza- tion to solve the scalability problem, induce parsimony, and to achieve the best possible learning performance. The basic idea is to break down the FS problem into several stages, namely feature ranking, correlation-based feature elimination, and prediction optimization.

smaller score (and therefore never selected). It is designed to be applied to ML-based models in which their complexity can be defined by the number of features, e.g., SVM, LR, NB, KNN and FLD. Insights about the development and properties of this criterion can be found in Ref. [27].

In concordance with PMPCD, the application of the proposed learning scheme is limited to classifiers in which their complexity is mainly defined by the number of features in the model, e.g., SVM, LR, NB, KNN and FLD. Therefore, learning algorithms such as neural networks, random forest, etc. are out of the scope.

Four highly/ultra-unbalanced data sets were analyzed using the SVM, LR, NB, and KNN learning algorithms [35]. First, a full analysis is pre- sented using the NB algorithm on a manufacturing-derived data set. Then, the same procedure was applied to three different data sets. Due to space limitations, only results were reported.

Sensorless Drive Diagnosis [36], the data set contains 48 numerical features (plus the class label), which are extracted from motor current [37], the motor has good and defective components. This results in 11 different classes with different conditions. The goal of this study is to detect only class one. This data set is highly unbalanced (58509 instances

Occupancy Detection [38,39], the data set contains 5 features. To generate an unbalanced data structure, one out of 10 instances labeled as class 1 are included in the data set (index 1, 10, 20, etc.) and the remaining nine eliminated, all 0 class are included. The data set is split as follows: training set (6587 - including 173), validation set (1791 - 98),

comparative analyses are presented: (1) vs. the l1-regularized LR learning algorithm [19], this algorithm induces parsimony, therefore the goal of this analysis is to evaluate how PMQ-L solves the posed tradeoff between complexity and parsimony; (2) vs. the Random Undersampling Boosting (RUSBoost) learning algorithm [40], this algorithm is designed specif- ically to analyze highly/ultra unbalanced data structures, but it does not induce parsimony, therefore prediction analysis is the main goal of this comparative study.

A new Hybrid Feature Selection and Pattern Recognition method with the capacity to learn from the original data set was proposed, PMQ-L. It is aimed at detecting rare quality events through parsimonious modeling. Although the proposed approach does not guarantee to find the optimal solution (if it exists), it did promptly find a good quality solution. Its unique ability to deal with highly/ultra-unbalanced data structures and diverse learning algorithms to model linear and no-linear patterns was demonstrated in the 4 case studies, which also exhibited its capacity of selecting the driving features of the system.

