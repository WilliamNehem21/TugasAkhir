The recent migration to biometric technologies in most applications increases the interest in proposing solutions to monitor persons in an environment. Among these approaches are fingerprint-based control systems, iris-based control systems, face recognition and verification. However, the need to deploy autonomous systems focus our attention on face recognition because this category does not need interaction between the system and its users. One of the key conditions to achieve reliable face recognition is face representation. This is not always an easy task because of the variability of intra and inter-face charac- teristics. To illustrate how difficult it can be, let us consider a 3-D matrix representing a face. A single rotation of the given face gives a completely different matrix representation. So, the challenge in face representation is to find a description that will be invariant after rotation, translation, illumination, etc.

Many representations have arisen to this effect and one of the first was fractal representation [1]. This approach uses the theory that states an object can be represented by itself, that is, by auto similar elements. It was advantageous in the sense that it used fewer computational resources than the traditional principal component analysis (PCA) ap- proach [2], however, the insertion of a new sample would lead to a recalculation of feature vectors. Many other representations have

followed, and they are essentially based on the representation from a matrix (usually 3-D) to a 1-D feature vector. This is to reduce the use of computational resources. However, most of these representations assume that the image has a good resolution and do not handle spoofing attacks.

We propose, in this paper, a face representation to answer these challenges. It is a signature named FaSIVA which stands for /ace Signature for Identification, Verification and Authentication of persons. The idea is to propose a robust face representation that will not only handle the identification process but will take into consideration reinforcement processes such as image enhancement, verification, and authentication.

The rest of the paper is organized as follows; Section 2 presents previous work with an emphasis on outstanding work related to our study. In Section 3, the details of the FaSIVA signature are presented where all the formalization processes are explained. To validate the model, we present, in Section 4, the different implementations that have been carried out as well as the results obtained. The work is concluded in Section 5 with a summary and some thoughts for future work.

The principle used to represent a face is called sparsity which consists of finding minimum elements using the original matrix of the face image that can robustly and accurately represent the given face. The result obtained is called the features vector. A mathematical principle is usually used to find the given features vector. In this view, many approaches have been developed in the literature; some famous ones are local binary patterns (LBP), eigenfaces and fisher faces.

For LPB, the surrounding of each pixel is studied [3]. Here, a binary pattern is computed for each pixel using a mathematical formula and a threshold, and at the end, all the values computed are gathered to derive the image histogram. Since it uses a threshold mechanism, LPB is sensitive to local variations of illumination and does not perform well when there is partial occlusion.

For SIFT features [8], most of related algorithms are composed of four part which are scale-space extrema detection used to detect blob structures in an image. At this stage, a scale space is constructed where the interest points are detected. Then we have unreliable key-points removal where each candidate key-point is evaluated, and a decision is made depending on its value (below a threshold). This is followed by orientation assignment where one or more orientations are assigned to a key-point based on local image gradient directions. Finally, we have the key-point descriptor where image gradient are sampled around the key-point location using the scale of the key-point to select the level of the Gaussian blur for the image.

Concerning adversarial examples, they include situations in which an attack generates a patch that can be placed anywhere within the field of view of the classifier and causes the classifier to output a targeted class [24]. This leads the network to attain high confidence in the incorrect classification of an adversarial example and the capacity of fooling the system with imperceptibly little noise. To overcome this, a naive approach consists of pretending to be the attacker, generate sev- eral adversarial examples against your own network, and then explicitly train the model to not be fooled by them. Recently, more specific approaches have been proposed as an example defensive distillation method of Papernot et al. [25] which trains a distillation model of the same scale. An extensive study of such approaches is presented by Bingcai et al. in [26].

Finally to cope with coordinates in the frames CNN introduces coor- dinate independent features [27]. Since convolutional networks extract a hierarchy of feature fields from an input signal on a manifold, features are thereby computed via kernels, optimized to detect characteristic spatial patterns in lower level features. This give CNNs the capacity to easily handle coordinates when switching from on frame to another.

To design a complex algorithm, a top-down approach is taken and the analysis proceeds by successive refinements. By so doing, we stay away from any implementation, the concrete representation of data is not fixed; refers to the abstract data type. We give a notation that describes the data, the applicable operations to these data (primitives), and the properties of these operations (Semantic). According to [28], an abstract data type (ADT) is a mathematical specification of a set of data and a set of operations that can be performed on them. This type qualifies as abstract because it corresponds to a set of specifications that a data structure must then put in the work. It allows to define non-primitive data types which are not available (not already imple- mented) in current programming languages. However, the framework for formally defining the types and operations on objects of these types is that of algebraic specifications. More precisely, the semantics of an algebraic specification consists of the definition of one or many

which will be used to verify the characteristics of human presence (second part of the authentication) as described in Section 4.3. So, if all these checks are correct, one can indeed conclude that it is a real person and not a spoofed person that has been identified.

The usual approach entails using a dataset of random images to train the model in as much as the images are of good quality (resolution). However, this model performs poorly in the specific case of human faces, that is why we decided to train our model specifically on face images. To do so, we replace the dataset of objects with human faces. In our case, the faces have been retrieved in the label faces in the wild

testing, respectively. This gave us a set of 9924 images for training and that we have separated into two sets with a ratio of 3:1, for training and 3309 images for testing. The model was then trained on the dataset with 20 epochs.

The fundamental idea behind this network usage is to reconstitute an image of better resolution by the intuition developed by convo- lutional neural networks. The resulting process differs from a simple zoom because its algorithm adapts values of the image matrix on a new dimension which generally conduce to loss of certain feature elements. On the other hand, the super-resolution approach uses an initial image of little resolution and reconstitutes a similar image of better resolution. It, therefore, highlights hidden details enhancing in this case, features extraction.

This is the easiest step once the features have been extracted. It involves matching a features vector of an input image to a features vector belonging to a class in the knowledge base. The native approach used to this effect is the KNN (K-nearest neighbor) algorithm which computes the distance between each input image features vector and all images in the knowledge base and returns the class with the lowest distance. Another approach is the use of a support vector machine (SVM) trained to efficiently separate data for further use that can be either classification or regression. Even if its execution time is negligible once the model has been trained, its performance decreases considerably when the number of classes increases or when data are not linearly separable. We, therefore, choose to use this approach which is the use of a trained softmax layer that takes into consideration details of the features vector and uses less time for execution as well as SVM.

Formally, authentication comprises the verification that an indi- vidual is effectively the one he claims to be. In the frame of video surveillance, we will understand authentication to be any mechanism targeting the attestation of effective presence of the identified indi- vidual as well as any reinforcement decision-making principle. To do this, we first proceed to a verification of information. Concretely, we use features extracted by the FaceNet network, compute the distance (Euclidean) between this vector and the feature vector of the individual in the knowledge base. Note that a feature vector has previously been computed for everyone in the knowledge base. If the computed

Third step: The Casia dataset has in the training set, 160 fake videos and 80 real videos, and 240 fake videos and 120 real videos in the testing set. Proceeding the same way, we extracted 11 images per fake video and 21 per real video. This gave a total of 1760 fake images and 1680 real images. These images were later

identified by a diagnostic test. It suggests how good the test is at iden- tifying normal (negative) condition. Finally Accuracy is the proportion of true results, either true positive or true negative, in a population. It measures the degree of veracity of a diagnostic test on a condition.

Another name of TPR is Sensitivity. It is the proportion of true positives that are correctly identified by a diagnostic test. It shows how good the test is at detecting a disease. Also another name to TNR is Specificity which is the proportion of the true negatives correctly

studied related work on face representation. Later, the FaSIVA proposed method was detailed, where each of its parts was clearly illustrated. This considers three parameters namely the quality of the image, the pattern extracted from the image which is the combination of the pattern extracted from two popular models: the Resnet-50 and Facenet, and finally, a proposed authentication mechanism that takes into consideration two metrics; eye blinking and spoofing verification. A section dedicated to the implementation of FaSIVA and all the steps we went through to train our models which were followed by a testing phase were presented. It was proven that the proposed signature is valid, efficient and robust. This is because it no more focuses only on recognition but also handles operations that enhance image quality as well as authentication mechanisms to reinforce the recognition process and prevent spoofing attacks. From the results obtained, we are able to improve face recognition pipeline by enhancing image quality in case of low resolution images and prevent spoofing attacks of the system using liveness detection. For future work, we intend to investigate more on blinking detection, to accurately detect blinks even in non-frontal faces and in occluded faces. Now that the signature is well defined we are working inserting the ability of detecting adversarial attacks.

