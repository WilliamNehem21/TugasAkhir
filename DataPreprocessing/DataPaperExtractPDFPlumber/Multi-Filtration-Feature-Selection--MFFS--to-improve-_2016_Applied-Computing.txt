AppliedComputingandInformatics(2016)12,117–127
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)
www.ksu.edu.sa
www.sciencedirect.com
ORIGINAL ARTICLE
Multi Filtration Feature Selection (MFFS) to
improve discriminatory ability in clinical data set
S. Sasikala a,*, S. Appavu alias Balamurugan b, S. Geetha c
aAnnaUniversity, Tamil Nadu, India
bK.L.N. College ofInformation Technology, TamilNadu, India
cThiagarajar College ofEngineering, TamilNadu, India
Received 13June 2013; revised21March2014; accepted 29March 2014
Available online5 April2014
KEYWORDS Abstract Selection of optimal features is an important area of research in medical data mining
systems. Inthis paperwe introduce anefficientfour-stage procedure –feature extraction, feature
Medicaldatamining;
Biomedicalclassification; subset selection, feature ranking and classification, called as Multi-Filtration Feature Selection
Variancecoveragefactor; (MFFS),foraninvestigationontheimprovementofdetectionaccuracyandoptimalfeaturesubset
PrincipalComponent selection. The proposed method adjusts a parameter named ‘‘variance coverage’’ and builds the
Analysis; model with the value at which maximum classification accuracy is obtained. This facilitates the
MultiFiltrationFeature selectionofacompactsetofsuperiorfeatures,remarkablyataverylowcost.Anextensiveexper-
Selection imental comparison of the proposed method and other methods using four different classifiers
(Naı¨veBayes(NB),SupportVectorMachine(SVM),multilayerperceptron(MLP)andJ48deci-
sion tree) and 22 different medical data sets confirm that the proposed MFFS strategy yields
promising resultsonfeature selectionandclassification accuracyfor medicaldatamining fieldof
research.
ª2014KingSaudUniversity.ProductionandhostingbyElsevierB.V. Thisisanopenaccessarticle
undertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/3.0/).
1.Introduction prediction of usefulness of surgical procedures, clinical tests,
medication procedures, and the discovery of associations
Data mining application in medicine has proved to be a among clinical and diagnosis data [37]. The applicability of
successful strategy in the areas of medical services including dataminingforhealthcareapplicationsisincreasinglygaining
importance. The availability of diverse-natured medical data
fordiagnosisandprognosisandofpervasivedataminingtech-
* Correspondingauthor.Tel.:+919443831534. niquestoprocessthesedataoffersmedicaldataminingadis-
E-mailaddresses:nithilannsasikala@yahoo.co.in(S.Sasikala),app_s@ tinctiveplace totruly assistandimpact patient care.
yahoo.com(S.AppavualiasBalamurugan),sgeetha@tce.edu(S.Geetha).
Due to proliferation of synergized information from enor-
PeerreviewunderresponsibilityofKingSaudUniversity. mous patient repositories, there is a paradigm shift in the
insightofpatients,cliniciansandpayersfromqualitativeanal-
ysisofclinicaldatatodemandingabetterquantitativevisual-
Production and hosting by Elsevier ization of information based on all supporting medical data.
http://dx.doi.org/10.1016/j.aci.2014.03.002
2210-8327 ª2014KingSaudUniversity.ProductionandhostingbyElsevierB.V.
ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/3.0/).118 S. Sasikalaetal.
Forinstance,thephysicianscanevaluatethediagnosticinfor- previousmodels,andusesanindependentmeasuretoidentify
mationofmanypatientswithidenticalconditions.Inthesame the best subsets for a given cardinality and applies a mining
way,theycanverifytheirfindingstoo,withtheconformityof algorithm to select the best subset among all best subsets
peerphysiciansworkingonsimilar casesinotherparts ofthe acrossdifferentcardinalities.However,theensembleofafilter
world. The patterns that are discovered denote valuable based model with another filter based model, once for subset
knowledgethathelpsmedicaldiscoveries,forexamplediscov- selection and again for ranking proves to be a promising
eringthatacertaincombinationoffeaturesmayhelpinbetter, approach, for medical data mining. The ensemble is brought
andmoreaccuratediagnosisofa particulardisease. Accurate about in a fashion so as to reduce the number of features
diagnosis of diseases and subsequently, providing efficient andalso to enhancethe classification accuracy.
treatment,formanimportantpartofvaluablemedicalservices Theobjectiveofthisresearchworkisaimedatshowingthat
givenfor patientsin a health-care system. the selection of more significant features from the available
The unique characteristics of medical databases that pose rawmedicaldatasethelpsthephysiciantoarriveatanaccurate
challengesfordataminingaretheprivacy-sensitive,heteroge- diagnosis. The primary focus is on aggressive dimensionality
neous, and voluminous data. These data may have valuable reductionsoastoendupwithincreaseinthepredictionaccu-
informationwhichawaitsextraction.Therequiredknowledge racy.Thefeaturesaresubjectedtoadoublefiltrationprocess,
isfoundtobeencapsulatedin/asvariousregularitiesandpat- at the end of which, only the features that increase the accu-
terns that may not be apparent in the raw data. Extracting racy, and form the subset with the lowest cardinality, with
such knowledge has proved to be priceless for future medical their corresponding rank, are obtained. The method employs
decision making. Feature selection is crucial for analysing anefficientstrategyofensemblefeaturecorrelationwithrank-
various dimensional bio-medical data. It is difficult for the ing method. The empirical results show that the proposed
biologists or doctors to examine the whole feature-space MultiFiltrationFeatureSelection(MFFS)embeddedclassifier
obtainedthroughclinicallaboratoriesatonetime.Inmachine modelachievesremarkabledimensionalityreductioninthe22
learning, all the computational algorithms recommend only medical datasets obtained from the UCI Machine Learning
few significant features for disease diagnosis. Then these rec- repository[10]and Kentridgerepository[13].
ommended significant features may help doctors or experts
tounderstandthebiomedicalmechanismbetterwithadeeper
2. Relatedwork
knowledge about the cause of disease and provide the fastest
diagnosis for recovering the infected patients as early as
possible. Numerous works have been carried out in the field of dimen-
Featureselectionmethods[12]tendtoidentifythefeatures sionality reduction for medical diagnosis. The following
mostrelevantforclassificationandcanbebroadlycategorized section presents the summary of those works, highlighting
as either subset selection methods or ranking methods. The thestrengths and weaknessesof eachmethod.
former type returns a subset of the original set of features It could be observed that the naive Sequential Forward
which are considered to be the most important for classifica- Feature Selection (SFFS) (pure wrapper approach) [5] is
tion.Rankingmethodssortthefeaturesaccordingtotheiruse- impractical for feature subset selection from a large number
fulness in the classification task. Most of the classifiers, of samples of high-dimensional features. Hence Gan et al. [4]
irrespective of the application domain, uses the ranking proposed the Filter-Dominating Hybrid Sequential Forward
strategytoselectthefinalfeaturesubset,inanadhocmanner. FeatureSelection(FDHSFFS)algorithmforhighdimensional
Featureselection,asapre-processingsteptomachinelearning, feature subset selection. This method proved to be fast but
is prominent and effective in dimensionality reduction, by demanded huge computational complexity. Another variant
removing irrelevant and redundant data, increasing learning of the SFFS method called improved F-score and Sequential
accuracy, and improving result comprehensibility. Feature Forward Search (IFSFS) was proposed by Xie and Wang
selection algorithms generally fall into two broad categories, [36] for feature selection to diagnose erythemato-squamous
the filter model and the wrapper model [37].The filter model disease. This method was designed so as to improve the
dependsongeneralcharacteristicsofthetrainingdatatoselect F-score and measured the discrimination between more than
some features without involving any learning algorithm. The two sets of real numbers instead of measuring between only
filtermodelassessestherelevanceoffeaturesfromdataalone, two sets of real numbers. The method’s applicability to other
independent of classifiers, using measures like distance, infor- medical data sets was not reported and hence it was a very
mation, dependency (correlation), and consistency. The filter specific system targeted at the diagnosis of erythemato-
method is further classified into Feature Subset Selection squamousdiseaseonly.
(FSS) and Feature Ranking (FR) methods. The wrapper AnothercategoryoffeatureselectionmethodsusedMutual
model needs one predetermined learning algorithm in feature Information score. Vinh et al. [32] proposed a novel feature
selection and uses its performance to evaluate and determine selection method based on the normalization of this well-
whichfeaturesareselected.Foreachofthegeneratednewsub- known mutual information measurement and utilized the
set of features, the wrapper model is supposed to learn the information measurement to estimate the potential of the
hypothesis of a classifier. It has a propensity to find features features.Themethodcouldnoteclipsethestronglycorrelated
bettersuitedtothepredeterminedlearningalgorithmresulting features impact on the classification results. Correlated
in superior learning performance, but it also tends to take featuresmaybeaccountedforredundancyandhenceasingle
more computation time and is economically more expensive representative feature from that subset may be selected for
thanthefiltermodel[37].Wheneverdealingwithalargenum- furtherprocessing.
beroffeatures,thefiltermodelisusuallychosenduetoitshigh An incremental learning algorithm in which the most
accuracy[9].Thehybridmodeltakestheadvantagesofthetwo informative features are learnt at each step, is proposed byMFFS 119
Ruckstiebetal.[26]andiscalledasSequentialOnlineFeature This paper is organized as follows: Section 3 describes the
Selection (SOFS). Another Scatter Search-based approach proposed method with the suitable algorithm. Experimental
coupled with Decision Trees (SS+DT) is proposed by Lin results and discussions are presented in Section 4. The paper
and Chen [17]. The method acquired optimal parameter set- isconcludedwithamentiononthefuturescopeofthiswork.
tingsandselectedthebeneficialsubsetoffeaturesthatresulted
in better classification results. In [16] Koprinska empirically 3. Proposed system
evaluated feature selection methods for classification of
Brain–ComputerInterface(BCI)data.Anewfeatureselection
The proposed method involves four stages. The entire system
methodbasedonroughsettheoryhasbeenproposedbyPaul
flowoftheproposedmodelisshowninFig.1.Theindividual
and Maji [23]. The proposed method identified discriminative
stagesare described in the followingtext.
and significant genes from high-dimensional microarray gene
expression datasets.
3.1. System flowof the proposedmethod
Correlation Based Filter [3,18] is another strategy for fea-
ture selection. Ensemble methods have also been proposed.
3.1.1. Stage1 – RelevantFeature GenerationPhase(RFGP)
Raymer et al. [25] proposed a hybrid algorithm that coupled
A representative of unsupervised dimensionality reduction
a genetic algorithm with k-nearest-neighbour classifier and
methodisPrincipalComponentAnalysis(PCA)[14,39]which
applied it for protein–water binding from X-ray crystallo-
aims atidentifying a lower-dimensional space maximizing the
graphic protein structure data. MonirulKabi et al. [20] pre-
variance among data [38]. PCA is a very effective approach
sented a new Hybrid Genetic Algorithm (HGA) for Feature
of extractingfeatures [6,21].
Selection (FS), called as HGAFS. It employed a new local
Letusdenotethemulti-dimensionaldatasetintheformofa
search operation that is devised and embedded in HGA to
matrix,A.Thedimensionsactuallyrepresentdirectionsalong
fine-tune the search in feature selection process. The search
which the data vary. The feature generation process, which
process is guided in such a way that the less correlated (dis-
removestheirrelevantfeaturesandredundantfeatures,mainly
tinct)features consisting ofgeneraland special characteristics
findsanapproximate‘‘basis’’tothesetofdirections.Onlythe
of agiven dataset are generated insubsequent iterations.
crucial dimensions that serve as the corner stone upon which
A new approach called Redundancy Demoting (RD) has
other dimensions are dependent are generated from the given
been proposed by Osl et al. [22]. It takes an arbitrary feature
dataset.Theredundant-duplicatedimensionsarefinallyelimi-
ranking as input, and performs improvement in ranking by
natedwiththesensethattheycanbereconstructedeasilyfrom
identifyingredundantfeaturesanddemotingthemtopositions
theavailablesetofbasisdimensions.Thisisequivalenttofind-
in the ranking in which they are not redundant. Hybrid
ingthedimensionswithmaximalvariance,sincethepointsare
schemes that combine wrapper-based and filter-based
found to be constant approximately along other dimensions.
approachesarealsointheliterature[2,11,30]aresuchschemes
Variance factor is an important measure that denotes the
wherethe featuresareranked andthenselected soas tooffer
degree of data spread in a multi-dimensional dataset. Thus
superior classification accuracy. In the first stage, the filter
dimensionality reduction is effectively contributed from this
model is used to rank the features by the relief algorithm
first stepof filtration bychoosing appropriate variancefactor
andthenthehighestrelevantfeaturesarechosentotheclasses
at which the system yields the minimum number of features
with the help of the threshold. In the second stage, they used
withmaximumaccuracy.
shapely values to evaluate the contribution of features to the
classification task in the ranked feature subset. Tanwani
et al. [31] gave a study on comprehensive evaluation of a set
of diversemachine learningschemes ona number of biomed-
ical datasets. Sanchez-Monedero et al. [27] studied and pro- Medical
MFFS
posed the suitability of Extreme Learning Machines (ELM) Dataset
Engine
for resolving bio-informatics and biomedical classification
problems.
After reviewing the works on feature selection for medical Stage 1 Relevant Feature Generation
Phase (RFGP)
dataset [29] it is observed that most of the existing methods
sufferfromthefollowingproblems:(1)dependingonthecom-
plexityofthe searchmethod,theiterationsof evaluationsare Stage 2 Feature Ranking Phase (FRP)
too large; (2) they rely on a univariate ranking that does not
take into account interaction between the variables already
includedintheselectedsubsetandtheremainingones.More-
over,amethodthatproducesthebestaccuracyemploysmore Stage 3 Feature Re-Ranking Phase
(FRRP)
numberoffeaturesandhencemorerunningtimeisinvolvedin
the construction of the respective classifiers. Contrarily, a
Stage 4 Expert’s
method that outputs the fewest number of features produces Classifier Evaluation: NB, SVM,J48,MLP
Decision
inferior detection accuracy. A holistic and universal method
that achieves the best classification accuracy with fewest
featurespossibleisstillanopenresearchproblem.Thispaper
makes an attempt to design such a feature selection sequence # of Features Running Time Accuracy
and it is called as ‘‘Multi Filtration Feature Selection
(MFFS)’’. Fig.1 SystemflowoftheproposedMFFSmodel.120 S. Sasikalaetal.
ThebasictheoreticalideabehindPCAisfindingtheprinci- ThesuggestionusedbytheCFSisonthebasisthatalways
palcomponentsofthedatasetthatcorrespondtothecompo- features strongly correlated with the predicted class form the
nentsalongwhichthevariationisthemost.Thisisachievedby good feature subset than the features correlated with each
finding the covariance matrix, i.e., we find the principal com- other. The feature subset created by the CFS is computed by
ponents of the data, which correspond to the components the merit of the feature subset ‘S’ containing ‘k’ features as
alongwhichthereisthemostvariation.Thiscanbedoneusing inEq. (4).
thecovariancematrix,AATforourinputmatrixA,asfollows. The following equation provides the merit of the feature
Lettheeigenvaluesberepresentedask forthecovariance subset‘S’.
i
matrix. Then, the corresponding diagonal matrix is given in
kx
Eq.(1)as: Merit s¼ pffiffiffiffiffiffiffiffiffiffiffiffiffiffiffic ffiffif ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ð4Þ
kþkðk(cid:2)1Þx
ff
2k 0...0 3
1
whereMerit istheevaluatinghypothesisofafeaturesubset‘S’
6 0 k ...07 s
6 2 7 containing‘k’features,x istheaveragevalueoffeature–class
L¼6 . 7 ð1Þ fc
6 4 . . 7 5 correlation, and x ff is the average value of feature–feature
intercorrelation.
0 0...k
n
Thecorrelationbetweentwoentities‘i’and‘j’,x iscalcu-
ij
The eigenvectors v i of the matrix should satisfy AAT as given latedas in Eq.(5)
in Eq.(2)as
P ði(cid:2)iÞðj(cid:2)jÞ
AATv¼kv ð2Þ x ij ¼ qffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi: ð5Þ
i i i ½P ði(cid:2)iÞ2 (cid:3)½P ðj(cid:2)jÞ2
(cid:3)
On rewriting eigenvectors of the dataset as the rows of a
matrixP,the system becomes where‘i’istherecord’svalueoftheindependentvariableand
‘j’ is record’s value of the dependent variable which may be
AATP¼LP ð3Þ eitherfeatureorclasslabel.(cid:2)i and (cid:2)jarethemeansofthevalues
It is apparent from Eq. (3) that the columns of this matrix P ofthe independent anddependent variables,respectively.
represents the principal components of the original matrix
andhence confinesto the directions of most variance[39,38]. 3.1.3. Stage3 –Feature Re-Ranking Phase(FRRP)
PCA employs the entire features and it acquires a set of Inspiteoffeatureextractionandselection,aproblemispersis-
projectionvectorstoextractglobalfeaturefromgiventraining tentnamelytheclassifiermaybebiasedtowardstheattributes
samples. The approach mainly consists of three primary with more values. Hence this biased nature has to be elimi-
processes such distinction process, binary session and pattern nated for which we employ Symmetrical Uncertainty (SU). It
generation [29]. All these flavours make PCA [21] more suit- overcomes the problem of bias towards attributes with more
able for applying on medical datasets, which typically have values, bydividing information gainbythe sumof the entro-
these characteristics. The variance coverage factor is playing pies offeature subsets S andS.
i j
asignificantroleindecidingtheimportantfeaturesandhence Symmetry is a desired property for a measure of correla-
this parameter is tuned so as to capture the classifier model tions between features. However, information gain is biased
withthe best results. in favour of features with more values. Furthermore, the val-
ues have to be normalized to ensure they are comparable
3.1.2. Stage2 – FeatureRankingPhase (FRP) and have the same influence. Therefore, we choose symmetri-
The correlation between each feature and the class and cal uncertainty. It compensates for information gain’s bias
between two features can be measured and best-first search towards features with more values and normalizes its values
canbeexploitedinsearchingforafeaturesubsetofmaximum to the range [0; 1] with value 1 indicating that knowledge of
overall correlation to the class and minimum correlation the value of either one completely predicts the value of the
among selected features. This is realized in the Correlation- other and value 0 indicating that X and Y are independent.
based Feature Selection (CFS) method [7]. Correlation based In addition, it still treats a pair of features symmetrically.
Feature Selection is an algorithm that couples this evaluation Entropy-based measures require nominal features, but they
formulawithanappropriatecorrelationmeasureandaheuris- can be applied to measure correlations between continuous
tic search strategy. CFS quickly identifies and screens irrele- features as well, if the values are discretized properly in
vant, redundant, and noisy features, and identifies relevant advance. Therefore, we use symmetrical uncertainty in this
features as long as their relevance does not strongly depend work.
onotherfeatures.CFSisafullyautomaticalgorithm––itdoes AsCFSusesthebest-firststrategysearchmethodtocalcu-
notrequiretheusertospecifyanythresholdsorthenumberof latethemeritofthefeaturesubset,howeverthereisanecessity
featurestobeselected,althoughbotharesimpletoincorporate to fix the stopping criteria. Due to this strictly needed
ifdesired.CFSoperatesontheoriginal(albeitdiscretized)fea- constrain correlation between features is computed using
turespace,meaningthatanyknowledgeinducedbyalearning Symmetrical Uncertainty (SU)as specified in Eq.(6).
algorithm, using features selected by CFS, can be interpreted (cid:3)HðSÞþHðSÞ(cid:2)HðS;SÞ(cid:4)
intermsoftheoriginalfeatures,notintermsofatransformed SU¼2:0(cid:4) j i i j ð6Þ
HðSÞþHðSÞ
space.Mostimportantly,CFSisafilter,and,assuch,doesnot j i
incur the high computational cost associated with repeatedly where H(S) and H(S, S) are defined as in Eqs. (7) and
j i j
invokinga learningalgorithm. (8)as:MFFS 121
Input :Training set with 'n' features and 'm' samples - {(x ,x ,...,x y)......(x ,x ,...,x ,y )}
1,1 1,2 1,n, 1 m,1 m,2 m,n m
Output : Best Feature Subset,MaxAccuracy,MinError,respectiveVariance Coverage Factor(δ)
//Initialization
1.δ←0.45,BestFeatureSubset←{},S←∅
Phase I - Relevant Feature Generation Phase:
2.By forward feature selection strategy, populate features into S, where S is the feature subset.
3. For each FeatureSubset S,
4.ExtractedData = evaluatePCA(S,δ)
5.if (ExtractedData>BestFeatureSubset) then
6.BestFeatureSubset=ExtractedData
7.δ=δ+0.05
8.Repeat steps 2 to 7 until δ=0.95; Record the δ value and the respective BestFeatureSubset that gives maximum accuracy.
Phase II - Feature Ranking Phase:
9.Perform Correlation-based heuristic evaluation on BestFeatureSubset using
kω
Merit = cf
S ( )
k+k k-ω1
ff
10.Arrange the BestFeatureSubset in the decreasing order of the Merit score.
Phase III - Feature Re-Ranking Phase:
11.Rank[]=Create a Rank for BestFeatureSubset by usign Ranking-based heuristic of symmetrical uncertainty using
⎡H(S)+H(S)-H(S,S)⎤
SU=2.0×S⎢ j i i j ⎥
⎢⎣ H(S)+H(S) ⎥⎦
j i
12.Return the re-ranked BestFeatureSubset
Phase IV - Classifier Evaluation Phase:
13.Run a 10-fold CrossValidation on the Original feature set and BestFeaturesubset
using the Naive Bayes,SVM, J48,MLP classifier models.
14.Record the model that yields the maximum accuracy and minimum error.
15.Return BestFeatureSubset,MaxAccuracy,MinError,respectiveVariance Coverage Factor(δ)
Fig.2 AlgorithmoftheproposedMFFSmodel.
X
HðSÞ¼(cid:2) pðSÞlog ðpðSÞÞ ð7Þ be obtained. Hence the proposed scheme is designed with
j j 2 j
fseFSj MultiFiltrationFeatureSelection(MFFS)asthecentrallogic.
It consists ofthe following steps:
where a realistic model of a feature S can be formed by
j
evaluating the training data, considering the individual’s
4. Test resultsanddiscussion
probabilityvaluesofS.AnewfeaturesubsetS canbeworked
j i
out by partitioning the previously existing feature subset S,
j
4.1. Test scenario
thenthe relationship between subsets S andS isgivenby:
i j
X X
HðS;SÞ¼(cid:2) PðSÞ PðS=SÞlog PðS=SÞ ð8Þ
i j i i j 2 i j Empiricalstudywiththesyntheticdatasetshasbeenexecuted,
xeX yeY
to investigate the performance of the proposed algorithm in
Thealgorithm is betterexplained bythe Fig. 2. the followingperspectives:
3.1.4.Stage4 – ClassifierEvaluation 1. Classificationaccuracy
The proposed system is validated against standard successful 2. Number offeatures selected
classifiermodels[35].Classifiersareconstructedwiththefinal 3. Averagerunning time
subset of features obtained after subjecting the datasets to
RFGP,FRP andFRRPsteps sequentially. A detailed insight Datasets,testset-up,procedureandobjectivesforthetests
into variousclassifiers ispresented in Section4.4. necessaryfortheevaluationofthesegoalsaredescribedbelow.
3.2.Algorithm forthe proposedMFFS model 4.2. Datasets
Traditionalfilterapproachesusuallyselectthetoprankedfea- Theproposedapproachhasbeenevaluatedbyexperimentson
tures or eliminate the irrelevant features by using a threshold 22biomedicaldatasetsfromtheUCImachinelearningrepos-
criterion. Since prediction is made after the single filtering itory [10] and Kentridge repository [13]. The 22 biomedical
phenomenon,theyreportfeebleaccuracy.Alternatively,when datasets used to test the proposed approach are summarized
filtering is done, more than once, an improved accuracy may inTable1.ThelastcolumninTable1indicatesthe‘‘imbalance122 S. Sasikalaetal.
ratio’’ present in each dataset. It is the ratio between the 4.4.3. ModelM3– Decision Tree(DT)
cardinality of the class with the maximum instance to the A decision tree [1,33] is a predictive machine-learning model
cardinality ofthe class withthe minimum instance. that decides the target value (dependent variable) of a new
samplebasedonvariousattributevaluesoftheavailabledata.
4.3. Test set-up Decisiontree’sinternalnoderepresentsdifferentattributes;the
branches between the nodes tell us the possible values that
these attributes can have in the observed samples, while the
ThetestsarecarriedoutinasystemwithInteli5,8GBRAM,
endnodesarethetargetclasslabels.TheJ48decisiontreeclas-
DDR3, 500GB hard drive on a Windows XP operating
sifier [24] operates on the basis of constructing a tree and
system. The proposed algorithm is implemented using Weka
branching it based on the attribute with the highest informa-
[34].WEKAisacknowledgedasalandmarksysteminthefield
tion gain. The J48 tree with binary split allowed a confidence
of machine learning and data mining. It has attained wide-
factorof 0.25 andreducederrorpruning isemployed.
spread acceptance among the academia and industry spheres,
and has become a widely used tool for data mining research.
4.4.4. ModelM4– Multilayer perceptron(MLP)
Another flavour that is highly encouraging is its ‘‘Open
Source’’ nature. The free access given to the source code has AnMLP[15]canbeviewedasalogisticregression,wherethe
enabled us to develop and customize the modules matching inputisfirsttransformedusingalearntnon-lineartransforma-
our work. The stepwise approach is as follows. The input to tion.Thepurposeofthistransformationistoprojecttheinput
the system is given in the Attribute-Relation File Format dataintoalinearlyseparablespace.Thisintermediatelayeris
(ARFF).Theproposedalgorithmisexecutedandthefeatures referred to as a hidden layer. We have employed a back-
in the ranked order are obtained as the output. A result is propagation network with 0.3 as learning rate and 0.02 as
created in Weka using the name specified in n@relation’’. momentum. The attributes are normalized in the range of
The attributes specified under n@attribute’’ and instances (0.1,0.9).The trainingwas carriedoutfor 500epochs.
specified under n@data’’ are retrieved from the ARFF file
and then they are added to the created table. 10-fold cross 4.5. Test procedure
validation is performed for all classifiers [8]. Fifty runs were
done for each classification algorithm on each dataset with For realizing dimension reduction via PCA, the orthogonal
features selected by MFFS method. In each run, a dataset basecomponentsareobtainedoutofthedatasetsthroughlin-
was split into training and testing set, randomly. The results eartransformation.FortheevaluationthePCAvariancecov-
obtainedare shown inTables 2–9. erage parameter d is varied in the range of (0.45, 0.95). In
preliminary test d values outside this range did not lead to
4.4. Classification Models useful results for the analysis. So we tested in this range. The
proposed MFFS calculates the correlations of feature-class
4.4.1. ModelM1– Naı¨veBayes(NB) and feature–feature using CFS and the feature subset space
is searched. The subset with the highest merit is subjected to
Naı¨veBayesianClassifierisasimpleprobabilisticclassifier[35]
analysis.Thentheresultingsubsetisre-rankedusingsymmet-
with an assumption of conditional independence among the
ricaluncertaintyprinciple.Forperformanceanalysis,themod-
features, i.e., the presence (or absence) of a particular feature
els M1-M4, generated according to the earlier discussion are
ofaclassisunrelatedtothepresence(orabsence)ofanyother
applied to the optimal feature subset, returned after MFFS
feature. It only requires a small amount of training data to
steps. The best accuracy in percentage along with the respec-
estimate the parameters necessary for classification. Many
tive variance coverage and the number of features involved
experimentshavedemonstratedthatNBclassifierhasworked
toachieve itare returned.
quitewellinvariouscomplexreal-worldsituationsandoutper-
formsmanyotherclassifiers.Kernelestimationhasbeenused
4.6. Test objectives
incasesofdatasetswithnumericalattributes.Alsosupervised
discretization is done for converting numerical attributes to
nominalones. From the goals stated above, the following objectives are
established:
4.4.2. ModelM2– SupportVector Machine(SVM)
O1 – enhancing the detection accuracy for the classifier
SVM [13,19] finds the hyper plane with maximum margin in model,whichismeasuredusingthedetectionaccuracymet-
between two classes. The Support Vector Machine (SVM) is ric expressed in percentage.
actually based on learning with kernels some of which form O2 – reducing the number of features so as to achieve the
thesupportvectors.Agreatadvantageofthistechniqueisthat best accuracy in eachclassifier model.
itcanuselargeinputdataandfeaturesets.Thus,itiseasyto O3–reducingtherunningtimeformodelgenerationwhich
test the influence of the number of features on classification is measured in seconds.
accuracy. We implemented SVM classification [28] for two O4–determiningtheinfluenceofthevariancecoverageof
types of kernels: polynomial kernel and Gaussian kernel the PCA feature extraction model on the MFFS
(Radial Basis Function – RBF). The SVM model with com- performance.
plexity parameter C as 1.0, epsilon as 1.0E(cid:2)12, normalized
trainingdata,RBFkernelwithgammaas0.0.1,andtolerance TestobjectivesO1andO2aretheobvioustestgoalswithin
parameters as 0.0010producedthe best results. thefocusofthiswork.HighdetectionaccuracywiththeleastMFFS 123
numberoffeatures,whichareshowninTables2–9areproving running time in seconds taken by the proposed system.
the usefulness of applying the proposed MFFS scheme to Figs 3–11show the performance ofthe proposedsystem.
feature selection. ItcanbeseenfromTables2–8,thattheclassificationaccu-
Test objectives O3 is aimed at determining the overall racy based on the selected subsets by the proposed MFFS
quality of our feature selection approach. Test objective O4 scheme is better than that based on the original feature set.
is formulated to address the impact of the variance coverage Thisindicatesthat theselected feature subsetsare representa-
of the PCAon the MFFSperformance overeach model. tiveandinformativeand,thus,canbeusedinsteadofthecom-
To facilitate a logical sequence for the presentation of our plete data for pattern classification. The list of such selected
research results, the test objectives framed based on the goals features isshownin Table 7.
areorderedinawaytoglidefromthemostspecifictoamore From the empirical results obtained so far, it is worth
general case. Tables 2–9 summarize the evaluation of test noting that each method has its strengths and limitations. In
objectives O1–O4. particular, CFS obtains good classification accuracy in the
least amount of running time but at the expense of selecting
4.7.Test resultsanddiscussion manymorefeatures;PCAselectstheleastnumberoffeatures
butsuffersintermsofclassificationaccuracyandalsorequires
more running time than others; MFFS attains the best accu-
In Table 2, the average classification accuracy of the chosen
algorithms over unprocessed datasets is provided. Tables 3–8 racy and robustness in a reasonable time with lowest number
offeatures.Consideringallthesefactors,theproposedMFFS
show the best average classification accuracy with the four
schemeshowsoverallbetterperformancethanothermethods.
classifiers on each dataset and the best accuracy in each case
is highlighted in bold typeface. Table 9 shows the average Tables 10 and 11 summarize and compare characteristics of
Accuracy on Unprocessed Datasets
100
90
80
70
60
50
40
30
20
10
0
Naïve Bayes (NB)
Support Vector Machine (SVM)
Bio Medical Datasets J48
Multi layer Percetron (MLP)
Fig.3 Performancecomparisonofdifferentclassifiersonunprocessedbiomedicaldatasets.
8000
400
20
1
Bio-Medical Datasets
Fig.4 NumberoffeaturesselectedbytheexistingsystemsandproposedMFFS(inlog scaleforuniformscaling)withNaı¨veBayes
10
classifier.
ycaruccA
reifissalC
elacs
gol
ni
sierutaef
fo
.oN
01124 S. Sasikalaetal.
100
90
80
70
60
50
40
30
20
10
0
Bio-Medical Datasets
Fig.5 ClassificationaccuracyobtainedforexistingandtheproposedMFFSbyNaı¨veBayesclassifier.
8000
400
20
1
Bio-Medical datasets
Fig. 6 Number of features selected by the existing systems and the proposed MFFS (in log scale for uniform scaling) with SVM
10
classifier.
100
80
60
40
20
0
Bio-Medical Datasets
Fig.7 ClassificationaccuracyobtainedfortheexistingsystemsandtheproposedMFFSbySVMclassifier.
%
ni
ycaruccA
%
ni
ycaruccA
elacs
gol
ni
sierutaef
fo
.oN
01MFFS 125
8000
400
20
1
Bio-Medical Datasets
Fig.8 NumberoffeaturesselectedbytheexistingsystemsandtheproposedMFFS(inlog scaleforuniformscaling)withJ48classifier.
10
100
80
60
40
20
0
Bio-Medical Datasets
Fig.9 ClassificationaccuracyobtainedfortheexistingsystemsandtheproposedMFFSbyJ48classifier.
8000
400
20
1
Bio-Medical Datasets
Fig. 10 Number of features selected by the existing systems and the proposed MFFS (in log scale for uniform scaling) with MLP
10
classifier.
elacs
gol
ni
sierutaef
fo
.oN
01
%
ni
ycaruccA
elacs
01gol
ni
sierutaef
fo
.oN126 S. Sasikalaetal.
100
80
60
40
20
0
Bio-Medical Datasets
Fig.11 ClassificationaccuracyobtainedfortheexistingsystemsandtheproposedMFFSbyMLPclassifier.
our proposed method for selective 6 datasets with those of (c) EnsemblewithsomeoptimizationstrategieslikeParticle
otherpreviousworks in the literature. Swarm Optimization (PSO), Ant Colony Optimization
(ACO), andGenetic Algorithm (GA)etc.
5. Conclusion
Summarily,MFFScanbeexpectedtoserveasanexcellent
alternative for feature selection in the field of medical data
In this paper, we have proposed an efficient Multi Filtration
mining.
FeatureSelection(MFFS)method applicabletomedical data
mining. Empirical study on 6 synthetic medical datasets sug-
Acknowledgment
gests that MFFS gives better over-all performance than the
existing counterparts in terms of all three evaluation criteria,
ThisworkissupportedinpartbytheUniversityGrantCom-
i.e., number of selected features, classification accuracy, and
mission(UGC),NewDelhi,INDIA–MajorResearchProject
computational time. The comparison to other methods in the
underGrant No. F.No.:39-899/2010 (SR).
literature also suggests MFFS has competitive performance.
MFFSiscapableofeliminatingirrelevantandredundantfea-
Appendix A.Supplementary data
turesbasedonbothfeaturesubsetselectionandrankingmod-
elseffectively,thusprovidingasmallsetofreliablefeaturesfor
the physiciansto prescribefurther medications. Supplementarydataassociatedwiththisarticlecanbefound,in
For simplicity, several keypointsare collectedas follows. theonlineversion,athttp://dx.doi.org/10.1016/j.aci.2014.03.002.
(1) Itseemsthattheclassificationperformanceisnecessarily References
proportional to the removal of redundant features,
heavily dependent on the inclusion of relevant features [1] I.S.I. Abuhaiba, Efficient OCR using simple features and
and the ‘‘Accuracy’’ metric is observed maximum with decision trees with backtracking, Arabian J. Sci. Eng. 31 (2B)
minimum number offeatures. (2006)223–244.
(2) TheproposedMFFSalgorithmoperatesinvariablywell [2] P. Bermejo, L.D.L. Ossa, J.A. Gamez, J.M. Puerta, Fast
wrapper feature subset selection in high-dimensional datasets
onanytypeofclassifiermodel.Thisshowsthegeneral-
bymeansoffilterre-ranking,Knowl.-BasedSyst.25(1)(2012)
izationabilityandapplicabilityoftheproposedsystem.
35–44.
(3) Ourtraining andtestdatabasecollects the popular and
[3] Y. Chen, S. Yu, Selection of effective features for ECG beat
benchmark medical datasets. However, the proposed
recognitionbasedonnonlinearcorrelations,Artif.Intell.Med.
methodcanbetestedandappliedonreal-worlddataset 54(1)(2012)43–52.
too. [4] J.Q.Gan,B.A.S.Hasan,C.S.L.Tsui,Afilter-dominatinghybrid
(4) Thebestaccuracyrateachievedbyourproposedsystem sequential forward floating search method for feature subset
issuperiorto the existing schemes. selection in high-dimensional space, Int. J. Mach. Learn.
Cybern.3(4)(2012)1–8.
To make our system more practical, future work could [5] A.I. Guyon, A. Elisseeff, An introduction to variable and
featureselection,J.Mach.LearnRes.3(1)(2003)1157–1182.
includethe following.
[6] H.U. Guz, A hybrid system based on information gain and
principal component analysis for the classification of
(a) Fitting the proposed system to classify any other real-
transcranial Doppler signals, Comput. Methods Programs
worlddataset.
Biomed.107(3)(2011)598–609.
(b) Applying the proposed method for a multi-label [7] M.A. Hall, Correlation Based Feature Selection for Machine
dataset, where a record may belong to many classes Learning(PhDDissertation),Dept.ofComp.Science,Univ.of
simultaneously. Waikato,Hamilton,NewZealand,1998.
%
ni
ycaruccAMFFS 127
[8] J.W.Han,M.Kamber,DataMining:ConceptsandTechniques, [25] M.L. Raymer, T.E. Doom, L.A. Kuhn, W.F. Punch,
MorganKaufmannPublishers,2006. Knowledge discovery in medical and biological datasets using
[9] Y. Han, L. Yu, A variance reduction framework for stable a hybrid Bayes classifier/evolutionary algorithm, IEEE Trans.
feature selection, in: G.I. Webb, B. Liu, C. Zhang, D. Syst.ManCybern.33(5)(2003)802–813.
Gunopulos, X. Wu (Eds.), Data Mining, IEEE Computer [26] T. Ruckstieb, C. Osendorfer, P.V.D. Smagt, Minimizing data
Society,Sydney,Australia,2010,pp.206–215. consumption with sequential online feature selection, Int. J.
[10] S. Hettich, C. Blake, C. Merz, UCI Repository of Machine Mach.Learn.Cybern.4(3)(2012)235–243.
Learning Databases, 1998 <http://www.ics.uci.edu/mlearn/ [27] J.Sanchez-Monedero,M.Cruz-Ramrez,F.FernandezNavarro,
MLRepository.html>(accessed10June2013). J.C. Fernandez, P.A. Gutierrez, C. Hervas-Martnez, On the
[11] B.Hualonga,X.Jing,Hybridfeatureselectionmechanismbased suitability of extreme learning machine for gene classification
high dimensional datasets reduction, Energy Procedia (2011) using feature selection, in: A.E. Hassanien, A. Abraham, F.
30–38. Marcelloni,H.Hagras,M.Antonelli,T.Hong(Eds.),Intelligent
[12] M.M. Jazzar, G. Muhammad, Feature selection based SystemsDesignandApplications,2010,pp.507–512.
verification/identification system using fingerprints and palm [28] A. Shawkat, K.A. Smith Miles, A meta learning approach to
print,ArabianJ.Sci.Eng.38(4)(2013)849–857. automatic kernel selection for support vector machines,
[13] L. Jinyan, L. Huiqing, Kentridge Bio-Medical Data Set Neurocomputing70(1-3)(2006)173–186.
Repository, 2002 <http://datam.i2r.a-star.edu.sg/datasets/krbd/> [29] Q. Shen, R. Diao, P. Su, Feature selection ensemble, in: A.
(accessed10June2013). Voronkov (Ed.), Computing, Springer-Verlag, 2011, pp. 289–
[14] I.T. Jolliffe, Principal Component Analysis, Springer-Verlag, 306.
NewYork,NY,1986. [30] P. Smialowski, D. Frishman, S. Kramer, Pitfalls of supervised
[15] G. Kim, Y. Kim, H. Lim, H. Kim, An MLP-based feature featureselection,Bioinformatics26(3)(2010)440–443.
subset selection for HIV-1 protease cleavage site analysis, J. [31] A.K.Tanwani,J.M.Afridi,M.Z.Shafiq,M.Farooq,Guidelines
Artif.Intell.Med.48(2)(2010)83–89. to select machine learning scheme for classification of
[16] I. Koprinska, Feature selection for brain–computer interfaces, biomedical datasets, in: C. Pizzuti, M.D. Ritchie, M.
in:T.Theeramunkonget al.(Eds.),PacificAsiaConferenceon Giacobini (Eds.), European Conference on Evolutionary
KnowledgeDiscoveryandDataMiningPAKDD’09,Springer- Computation, Machine Learning and Data Mining in
Verlag,Berlin,Heidelberg,2010,pp.100–111. Bioinformatics, Springer-Verlag, Berlin Heidelberg, 2009, pp.
[17] S.Lin,S.Chen,Parameterdeterminationandfeatureselection 128–139.
for C4.5 algorithm using scatter search approach, Int. J. Soft [32] L.T.Vinh,S.Lee,Y.Park,B.J.Auriol,Anovelfeatureselection
Comput.16(1)(2011)63–75. methodbasedonnormalizedmutualinformation,Int.J.Appl.
[18] X. Lu, X. Peng, P. Liu, Y. Deng, B. Feng, B. Liao, A novel Intell37(1)(2011)100–120.
featureselectionmethodbasedonCFSincancerrecognition,in: [33] L.M. Wang, S.M. Yuan, L. Li, H.J. Li, Improving the
L.Chen,X.Zhang,L.Wu,Y.Wang(Eds.),SystemsBiology, Performance of Decision Tree: A Hybrid Approach.
IEEEComputerSociety,China,2012,pp.226–231. Conceptual modeling. Lecture Notes in Computer Science,
[19] S.A. Mahmoud, S.M. Awaida, Recognition of off-line vol.3288,Springer,2004,pp.327–335.
handwritten Arabic (Indian) Numerals using multi-scale [34] Weka 3: Machine Learning Software in Java, 2013. The
features and support vector machines vs. hidden Markov UniversityofWaikatoSoftwareDocumentation<http://www.
models,ArabianJ.Sci.Eng.34(2B)(2009)429–444. cs.waikato.ac.nz/_ml/weka>(accessed10June).
[20] M.d.MonirulKabi,M.d.Shahjahan,MuraseKazuyuki,Anew [35] H.I. Witten, E. Frank, Data Mining: Practical Machine
localsearchbasedhybridgeneticalgorithmforfeatureselection, Learning Tools and Techniques with Java Implementations,
Int.J.Neurocomput.74(17)(2011)2914–2928. MorganKaufmann,SanFrancisco,CA,USA,2000.
[21] K. Moutselos, I. Maglogiannis, A. Chatziioannou, Integration [36] J. Xie, C. Wang, Using support vector machines with a novel
of high-volume molecular and imaging data for composite hybrid feature selection method for diagnosis of erythemato-
biomarker discovery in the study of melanoma, Biomed. Res. squamousdiseases,ExpertSyst.Appl.38(5)(2011)5809–5815.
Int.(2014),http://dx.doi.org/10.1155/2014/145243. [37] E. Xing, M. Jordan, R. Karp, Feature selection for high-
[22] M. Osl, S. Dreiseit, F. Cerqueira, M. Netzer, B. Pfeifer, C. dimensional genomic microarray data, in: C.E. Brodely, A.P.
Baumgartner, Demoting redundant features to improve the Danyluk (Eds.), Machine Learning, Morgan Kaufmann
discriminatoryabilityincancerdata,J.Biomed.Inform.42(4) PublishersInc.,SanFrancisco,CA,USA,2001,pp.601–608.
(2009)721–725. [38] S. Yazdani, J. Shanbehzadeh, Mohammad Taghi Manzuri
[23] S.Paul,P.Maji,Roughsetbasedgeneselectionalgorithmfor Shalmani, RPCA: a novel pre-processing method for PCA,
microarray sample classification, in: Methods and Models in Adv.Artif.Intell.2012(1)(2012)1–7.
ComputerScience,IEEEComputerSociety,2010,pp.7–13. [39] M. Zahedi, A.G. Sorkhi, Improving text classification
[24] R. Quinlan, C4.5: Programs for Machine Learning, Morgan performance using PCA and recall-precision criteria, Arabian
KaufmannPublishers,SanMateo,CA,USA,1993. J.Sci.Eng.38(8)(2013)2095–2102.