Array7(2020)100034
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
—
Process-monitoring-for-quality A machine learning-based modeling for
rare event detection
Carlos A. Escobara,*, Ruben Morales-Menendezb, Daniela Maciasb
aGlobalResearch&Development,GeneralMotors,Warren,MI,USA
bTecnol(cid:1)ogicodeMonterrey,SchoolofEngineeringandSciences,MonterreyNL,Mexico
A R T I C L E I N F O A B S T R A C T
IndexTerms: ProcessMonitoringforQualityisaBigData-drivenqualityphilosophyaimedatdefectdetectionthroughbinary
Qualitycontrol classificationandempiricalknowledgediscovery.ItisfoundedonBigModels,apredictivemodelingparadigm
Manufacturingsystems thatappliesMachineLearning,statisticsandoptimizationtechniquestoprocessdatatocreateamanufacturing
Machinelearning
functionalmodel.Functionalreferstoaparsimoniousmodelwithhighdetectionabilitythatcanbetrustedby
Featureelimination
engineers, and deployed to control production. A parsimonious modeling scheme is presented aimed at rare
Modelselection
qualityeventdetection,parsimonyisinducedthroughfeatureselectionandmodelselection.Itsuniqueabilityto
Unbalancedbinarydata
Defectdetection deal withhighly/ultra-unbalanced datastructures anddiverselearning algorithmsisvalidated withfourcase
studies,usingtheSupportVectorMachine,LogisticRegression,NaiveBayesandk-NearestNeighborslearningalgo-
rithms.Andaccordingtoexperimentalresults,theproposedlearningschemesignificantlyoutperformedtypical
learningapproachesbasedonthel1-regularizedlogisticregressionandRandomUndersamplingBoostinglearning
algorithms,withrespecttoparsimonyandprediction.
1. Introduction [6]andpromotesexplainability[7];desiredcharacteristicsforaclassi-
fiertobedeployedintoproduction,Fig.2.PMQhasthepotentialtosolve
Becauseofeverincreasingcustomerdemands,manufacturersareina engineering intractable problems e.g., detecting defects that are not
constantcompetitionforimprovingqualityandreliabilityinproducts. detectedbyStatisticalProcessControlmethods.
Theseattributesareachievedbycorrectexecutionofthemanufacturing Constant product innovation forces manufacturing engineers to
processandbyaneffectiveprocessmonitoringsystem. launchproductionsystemsevenwithoutacomprehensiveunderstanding
Severalresearchershaveworkedonthisproblem.Aframeworkfor oftheprocess.Therefore,thehugeamountofprocessdata(e.g.,signals)
multiplereleaseproblemsusingatwo-stepfaultdetectionprocedureand is used to create hundreds or even thousands of features i.e., hyper-
faultremovalprocesswasproposedbyRef.[1].Astate-of-the-artreport dimensional feature spaces. Which frequently include irrelevant and
ofthemostimportantpapersinthisdomainispresentedinRefs.[2].A redundantones[8,9]thattendtohamperthelearningprocess[10].
similarproject,butbasedonBayesianNetswassuggestedbyRef.[3].A SincemostmanufacturingcompaniesgenerateonlyafewDefectsPer
data mining approach for defect analysis and prevention of industrial MillionofOpportunities(DPMO),rarequalityeventdetectionisoneofthe
productsinRef.[4]. modern intellectual challenges posed to this industry. From ML
Process Monitoring for Quality (PMQ) is a Big Data-driven quality perspective,manufacturing-deriveddatasetsforbinaryclassificationof
philosophy aimed at defect detection through binary classification qualitytendtobehighly/ultra-unbalanced(minorityclasscount<1%).
(good/bad)andempiricalknowledgediscoverythroughfeature/model Theproblemwiththesedatastructuresisthatthelearningalgorithms
interpretation[5].Itisablendofprocessmonitoringandqualitycontrol misclassifymostoftheminorityclassasthemajorityclass,e.g.,failto
foundedonBigModels(BM);apredictivemodelingparadigmthatuses detect.Sinceitisharderforthealgorithmtolearntheminorityclass.
MachineLearning(ML),statisticsandoptimizationtechniques,Fig.1,to Theunbalancedclassificationproblemistakingalotofattentionfrom
develop a manufacturing functional model: final model (classifier). the ML community [11,12]. Extensive efforts and significant progress
Functional,referstoaparsimoniousclassifierwithahighdetectionca- havebeenmadeinrecentyearstoaddressthisintellectualchallenge.The
pacity;parsimonyfacilitatesinformationextraction,inducesmodeltrust main research efforts are broken down in five categories: (1)
* Correspondingauthor.
E-mailaddress:carlos.1.escobar@gm.com(C.A.Escobar).
https://doi.org/10.1016/j.array.2020.100034
Received24January2020;Receivedinrevisedform31May2020;Accepted1July2020
Availableonline6August2020
2590-0056/©2020TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).C.A.Escobaretal. Array7(2020)100034
manipulations.Comprehensivereviewsofunbalancedlearningarepre-
sentedinRef.[12,13].
The feature explosion (hyperdimensional feature spaces) combined
with high conformance production rates (unbalanced binary data) are
two of the most important challenges of Big Data initiatives in
manufacturingthatinspiredthedevelopmentofPMQ-Learning(PMQ-L).
The Hybrid Feature Selection and Pattern Recognition (HFSPR) approach
proposed in this paper with the capacity to effectively learn from the
originaldatasetandtoidentifythedrivingfeatures.
Theproposalisanovelparsimoniousmodelingscheme,thatcanbe
appliedtotraintheNaiveBayes(NB),SupportVectorMachine(SVM),K-
Nearest Neighbor (KNN), Logistic Regression (LR) and Fisher Linear
Discriminant(FLD)learningalgorithms.Thegoalisararequalityevent
detectionthroughparsimoniousmodeling,whereparsimonyisinduced
throughFeatureSelection(FS)andModelSelection(MS).
TheproposedschemecombinestheHybridCorrelationandRanking-
based(HCR)andReliefFfilteringalgorithmstoselectthemostrelevant
features.Toboostparsimony,asetofnestedCandidateModels(CM)is
developedandthen,thePenalizedMaximumProbabilityofCorrectDecision
(PMPCD) MS criterion is applied to select the final model. It can be
virtually applied to any learning algorithm in which complexity is
definedbythenumberoffeaturesinthemodel.Itsuniversalapplicability
is demonstrated by analyzing four highly/ultra-unbalanced data sets
using different learning algorithms. Empirical results demonstrate its
capacityoffindingagoodqualitysolutionaftercreatingafewCM.
Thispaperisorganizedasfollows:Areviewofthetheoreticalback-
ground is in section 2. Section 3 describes the PMQ-L framework, fol-
lowed by four binary classification empirical studies in section 4. A
comparativeanalysisisgiveninSection5.Finally,section6concludes
theresearch.
Fig.1. Bigdata–bigmodels.
2. Theoreticalbackground
over/under-sampling methods, (2) cost sensitive learning, (3)
kernel-based learning, (4) active learning and (5) novelty detection. 2.1. FSmethods
Where oftentimes the inherent ad hoc data manipulation (e.g., over/-
under sampling, miss-classification costs, using small pools of data) FSistheprocedureofchoosingasubsetofgoodfeaturesbyelimi-
approach lacks of theoretical foundation and principles to guide the natingirrelevantandredundantones.Fromagivendataset,evaluating
developmentofsystematicmethodsthatcanbeefficientlygeneralized.In allpossiblecombinations(2n)becomesaNP-hardproblemasthenumber
thiscontext,learningfromtheoriginaldatasetisidentifiedasfurther offeaturesgrowup[14].TheadvantagesofFSmethodsare:(1)enable
researchwork[12].Basically,theauthorsencouragetheresearchcom- the learning algorithm to train faster, (2) reduce over-complexity of a
munitytoinvestigatethedevelopmentofalgorithms/methodsthatcould model making it easier to understand/interpret, (3) improve general-
learnfromwhateverhighly/ultraunbalanceddataispresentedwithout ization(predictiononunseendata)iftherightsubsetischosen,and(4)
Fig.2. PMQ-basedqualitycontrol.
2C.A.Escobaretal. Array7(2020)100034
Table1 Table2
Confusionmatrix. Confusionmatrix.
4). Solution,evaluationanddiscussion:Althoughthefeaturecombinationis
Predictedgood Predictedbad
subjecttocombinatorialexplosion,1:8(cid:5)1016ofcombinations,thePMQ-
Gooditem TrueNegative(TN) FalsePositive(FP) Lapproachonlyrequired83modelstofindasolution.Toevaluateits
Baditem FalseNegative(FN) TruePositive(TP)
relativequality,anexhaustivesearchwasperformedwithallthepossible
combinations–uptotwofeatures–andcomparedwiththefinalmodel.
preventover-fitting.TheFSmethodsbroadlyfallinto3classes:filters, SincenoMSisperformed,thetrainingsetisusedtodevelopthemodels
andthetestingsettoevaluatetheirgeneralizationability:(1)54ðC Þ1-
wrappers,andembedded[15]. 1
featuremodels,Fig.7(top);and(2)1431ðC Þ2-featuremodels,
Filter(preprocessing)methodsareappliedbeforethelearningprocess 2
Fig.7(bottom).
to eliminate irrelevant/redundant features. Relevant features are
selectedusingapredefinedfitnessfunction,whichcanbebasedonde- Predictedgood Predictedbad
pendency, distance, consistency or discriminative capacity [16]. Gooditem 9488 5
Computedscoresareusedtodeterminethefitnessofeachfeatureandare Baditem 0 7
comparedwitharelevancethresholdtoselectasubsetofrelevantfea-
tures[10].Thesemethodsselectfeaturesindependentlyofthelearning
defective(bad)item,whereasanegativelabelreferstoagoodquality
algorithm. item. The prediction performance of a classifier is summarized in a
Incontrast,wrappersmethodsusethelearningalgorithmasablack-
confusionmatrix[26].Thistableisusedtocontrastpredictedlabelswith
boxtoevaluatetherelativeperformanceofafeaturesubset[17,18].In
therealqualitycharacteristic,Table1,andtocomputerelevantmeasures
thisprocedure,asetofcandidatefeaturesareinputtothelearningal- ofclassificationperformance.
gorithm,andthepredictionperformanceisusedastheobjectivefunction Atype-Ierror(α)iscomparedwithaFPprediction;atype-II(β)error
to evaluate the feature subset. Although wrapper methods tend to be
iscomparedwithaFN[25]:
computationallyintensive,theyperformbetterthanfilters,duetothebias
inducedbythealgorithm[15]. FP FN
α¼ ; β¼ (1)
Inembeddedmethods,theFStaskisintegratedaspartofthelearning FPþTN FNþTP
process.TheLASSOwiththeL 1penaltyandRidgewiththeL 2penaltyare The MPCD is a probabilistic measure of classification performance
themostcommonapproachesinthiscategory.Theyshrinkirrelevantand
that is driven by detection. Since it is very sensitive to FN (missing
trivialfeaturestozerooralmostzerorespectively[19,20]. defectiveitems)inhighly/ultra-unbalancedclasses[5].Theαandβer-
Hybrid approaches have been proposed to take advantage of the
rorsarecombinedtoestimateitsscore:
particularcharacteristicsofeachmethod[21].Theseapproachesmainly
focusoncombiningfilteralgorithmswitheitherwrappersorregulariza- MPCD¼ð1(cid:3)αÞð1(cid:3)βÞ (2)
tiontosolvethescalabilityproblem,induceparsimony,andtoachieve
thebestpossiblelearningperformance.Thebasicideaistobreakdown wherehigherscoreindicatesbetterclassification,MPCD2½0;1(cid:4).
the FS problem into several stages, namely feature ranking,
correlation-basedfeatureelimination,andpredictionoptimization.
2.5. PenalizedMaximumProbabilityofCorrectDecision
2.2. ReliefF ItisaMScriterionbasedonMPCDthatefficientlysolvestheposed
tradeoffbetweenmodelcomplexityandpredictionability.Thebasicidea
ReliefFranksfeaturesaccordingtotheirdiscriminativecapacity[22].
ofthecriterionistoinduceparsimonybypenalizingforextrafeaturesin
Itsearchesforknumberofnearestneighborsofthesameclass(hits),as
themodel.Theformulationincludesarewardingtermbasedonpredic-
well as of the different class (misses) to evaluate the fitness of each tion/detectionð1(cid:3)αÞð1(cid:3)βÞandalightpenalizationterm(cid:3)lnðKÞ=34:55
feature. This procedure is repeated m times, which is the number of
basedonthenumberoffeatures(K)inthemodel.Andhence,CMwith
randomlyselectedinstances.Featuresareweightedandrankedbasedon
extra features with negligible contribution to prediction will have a
the average of the distances of all hits and all misses [23,24]. k is a
smallerscore(andthereforeneverselected).Itisdesignedtobeapplied
hyperparameter(user-specified)thatprovidesprotectiontotheeffectof
to ML-based models in which their complexity can be defined by the
noiseandcontrolsthelocalityoftheestimates.Onceallfeatureshave
numberoffeatures,e.g.,SVM,LR,NB,KNNandFLD.Insightsaboutthe
beenranked,theyareselectedbasedonτ,asignificancethresholdpro-
developmentandpropertiesofthiscriterioncanbefoundinRef.[27].
posed by Ref. [22]. Features with an estimated weight below τ are
consideredirrel pev ffiffiffia ffiffin ffiffitandthereforeeliminated.Theproposedlimitsforτ PMPCD¼ð1(cid:3)αÞð1(cid:3)βÞ(cid:3)lnðKÞ=34:55 (3)
are 0<τ(cid:1)1= αm [23]; where α is the probability of accepting an
Themodelwiththehighestestimatedvalueonthevalidationset[28,
irrelevant feature as relevant. ReliefF does not eliminate redundant
29]isthepreferredone.
features.
2.3. Correlation-basedredundancymeasure 2.6. HybridCorrelationandRanking-based(HCR)algorithm
meaT sh ue rePe oa frs ro en dp ur no dd au nc ct y-m bo em twen eet nco tr wre ola rti ao nn dc oo meffi vaci re ian bt lð er sxy [Þ 2i 5s ]u .se Itd ia ssa
a
PeaT rsh oe n’sH cC oR rrea ll ag to iori nth cm oef[ fi3 c0 ie] ne tslim ani dna tt he es Rr ee ld ieu fFn -d ba an st edfe ra at nu kr ie ns g.b Fa ese ad turo en
s
measureofstrengthoflinearrelationshipbetweentwovariablesðx;yÞ, areeliminatediftheircorrelationscoreisgreaterthanδ,auser-specified
anditcantakearangeofvalues½(cid:3)1;1(cid:4).Avalueof0indicatesthereisno threshold to determine if the discriminative information between the
linearrelationship,whileanabsolutevalueof1(orcloseto1)indicates compared features is redundant. The basic idea of the algorithm is to
stronglinearrelationship,andthereforeconsideredhighlyredundant.
keepthebestfeature–highestranked–fromasetoftwoormorehighly
correlatedvariables.
2.4. MaximumprobabilityofCorrectDecision—Ameasureofprediction
performance 2.7. PMQ-basedqualitycontrol
In the context of binary classification, a positive label refers to a RarequalityeventdetectionisoneofthemainapplicationsofPMQ.
3C.A.Escobaretal. Array7(2020)100034
1) FeatureSelection(FS):Theprimarypurposeofthisstep,Fig.3,isto
findasmallsubsetofrelevantfeatureswithhighpredictioncapacity.
Sincetheoptimalcombination–withrespecttoprediction–ofkand
δisnotknowninadvance,ahyperparameteroptimization [32],is
performed through a grid search [33,34]. Using the training set,
irrelevantandredundantfeaturesareeliminatedbyapplyingReliefF
andHCRalgorithms.FeaturesarerankedbasedonReliefFandirrel-
evant features are eliminated based on τ – significance threshold.
Fromtheselectedfeatures,highcorrelationsareeliminatedbasedon
δ.Thesetwostepsareperformedinafilter-typeapproach,wherethe
learningalgorithmisnotconsidered.
A CM is developed with the subset of features at each pairwise
combination,andthepredictivefitnessofeachmodelisevaluatedtofind
theincumbent(best)model–highestvalidationMPCD.Thefeaturesinthe
incumbent model are selected and their associated ReliefF ranking
recorded.
2) Model Selection (MS): Although a good feature subset has been ob-
tainedintheFSstep,Fig.3,theirindividualrelevanceinthemodelis
notknown.Toevaluatetheirprediction-contribution,asetofnnested
CMisdeveloped–nisthenumberofselectedfeatures–usingthetop
1featureinthefirstCM,thetop2featuresinthesecondone,andso
on. Finally, the PMPCD of each CM is estimated and used as a MS
criterion to induce parsimony – solve the tradeoff between model
complexityandpredictionability.Thefinalmodelistheonewiththe
highestPMPCDscore.
3) Generalizationevaluation:Toobtainanunbiasedestimation(orclosest
to)ofthegeneralizationabilityofthefinalmodel,thepredictionon
testingset(unseendata)shouldbereportedinaconfusionmatrix,last
stepFig.3.
The outcome of this method is a parsimonious classifier with high
detection ability, as the analytical tools used are aimed at analyzing
Fig.3. PMQ-Lframework.
highly/ultra-unbalanced data structures. Parsimony does not only
improvethelearningability,butalsohelpstoidentifythefewdriving
AsdepictedinFig.2,atypicalmanufacturingprocessgeneratesonlya
featuresofthesystem.
few DPMO. The BM learning paradigm is applied to process data to
design a classifier with high detection capacity to be deployed at the InconcordancewithPMPCD,theapplicationoftheproposedlearning
plant,e.g.,finalmodel.Sincepredictionisperformedunderuncertainty,a scheme is limited to classifiers in which their complexity is mainly
classifier cancommitFP andFN(i.e., failto detect)errors.Whereasa
definedbythenumberoffeaturesinthemodel,e.g.,SVM,LR,NB,KNN
and FLD. Therefore, learning algorithms such as neural networks,
gooditempredictedasbad(FP)wouldnotgenerateacriticalproblem,
randomforest,etc.areoutofthescope.
since in a second level inspection/revaluation would be back in the
value-adding process, a FN would become a warranty event. And if
this“miss” is a criticalcomponent/device, thenit couldhave aserious 4. Casestudies
economicimpactaswellasonthecompany’sreputation.
Fourhighly/ultra-unbalanceddatasetswereanalyzedusingtheSVM,
3. PMQ-learning—aHybridFeatureSelectionandPattern LR,NB,andKNNlearningalgorithms[35].First,afullanalysisispre-
sented using the NB algorithm on a manufacturing-derived data set.
Recognitionapproach
Then,thesameprocedurewasappliedtothreedifferentdatasets.Dueto
A new parsimonious modeling method is presented, it is a flexible spacelimitations,onlyresultswerereported.
approachwithauniqueabilitytodealwithhighly/ultra-unbalanceddata
4.1. Casestudy1
structuresanddiverselearningalgorithmstomodellinearandno-linear
patterns.
The data used for this analysis1 was collected from the Ultrasonic
ParsimoniousmodelingisinducedthroughFSandMS,Fig.3.Since
MetalWeldingofbatterytabsfortheChevroletVolt[5],awell-controlled
most manufacturing systems are time-dependent, cross-validation
processthatonlygeneratesafewDPMO.Itcontains54featureswitha
methods are not encouraged. Instead, time-ordered hold-out method
binary outcome (good vs bad quality), its data structure is
seemstobemoreappropriate.Thedatasetshouldbepartitionedinto
ultra-unbalanced–35badweldsoutof40,231examples(0.09%).Three
three subsets (i.e., training, validation, testing) [29]. And the search
spaceisdefinedbymanycandidatepairwisecombinations–basedon data sets are created following a time-ordered hold-out validation
differentvaluesofkforReliefFandδforHCR. Thevaluesofkcanbe approach:trainingset(18,495,including20bad),validationset(12,236-
8bad),testingset(9500-7bad).
determined by generating a logarithmically spaced vector [31] e.g., p
logarithmically spaced points between decades ½10a;10b(cid:4), where X¼
sumðbadÞinthetrainingset,a¼0andb ¼log 10ðXÞ.
1 PrivatelystoredintheManufacturingResearchLabofGeneralMotors.
4C.A.Escobaretal. Array7(2020)100034
Fig.6. CMusingthetop13features.
3). Generalizationevaluation:Thetestingset(9500-including7bad)was
usedtoestimatetheunbiasedgeneralizationabilityofthefinalmodel,
recognitionratesaresummarizedintheconfusionmatrix,Table2.This
modelincludesonlytwofeatures(25,5),anditcorrectlydetectedthe
sevendefectiveitemswithonlyfiveFPs–MPCD ¼0:9995.Itisclearthat
thesystemcanbejustifiedbyonlythesetwofeatures.
Table3
Topmodels(*PMQ-Lsolution).
Modelindex Features MPCD FN
1032 26,33 0.9998 2
1035 26,36 0.9998 2
Fig.4. CMinformation(denotedbylineintersections). 413 9,26 0.9997 3
1042 26,43 0.9997 3
1044 26,45 0.9997 3
1045 26,46 0.9996 4
Final 5,25 0.9995 5*
According to the grid search results, the incumbent model has an
estimated validation MPCD ¼ 0:8728, Fig. 4(top), and 13 features,
Fig. 4(bottom). This model was developed with these relevant hyper-
parameters:k ¼2;τ ¼0:0329;δ ¼0:50.AllCMfailedtodetectoneof
thedefectiveitems;therefore,theβ¼0:125inallmodels.Andtheyare
basicallycompetingovertheαerror.Asdisplayedbytheplots,asthe
number of low qualityfeatures includedin the model increases,the α
errorincreasestoo.Theproposedhyperparameteroptimizationallowed
tofindagoodsubsetoffeatures.
2) ModelSelection(MS):Toinduceparsimony,13CMwerecreated,and
PMPCDwasusedasaMScriteriontofindthefinalmodel.Thebasic
ideaistoevaluatetheindividualprediction-contributionofeachof
the13selectedfeatures,Fig.5showstheselectedfeaturesandtheir
associatedranking.CM-1containstop-1feature(25),CM-2contains
Fig.5. Featuresintheincumbentmodel. thetop-2features(25,5)andsoon.
1) FeatureSelection(FS):Thesearchspacecontains70pairwisecombi- According to the MS criterion, CM-2 should be selected, with an
nations; for ReliefF, 7 logarithmically spaced pointswere defined – estimatedPMPCD ¼0:8501,Fig.6.Thisanalysis,disclosesthatonlytwo
k¼f1;2;3;4;7;12;20g–andforδ,10evenspacedpoints–δ ¼ features are needed to approximate the pattern in the manufacturing
f0:50;0:55;…;0:95g. At each combination, feature relevance was system,sincethepredictionimprovementisnotsignificantifmorefea-
determinedbycomparingtheirweightswithτ¼0:0329–calculated turesareaddedtothefinalmodel.
withanαof0.05,andmof18,495.Fig.4showspredictionresults Based on exhaustive search, no single-feature model has better
(validationMPCD)andnumberoffeaturesofeachCM. generalization ability. Whereas six 2-feature models outperformed the
final model, Table 3 summarizes their relevant information. However,
5C.A.Escobaretal. Array7(2020)100034
themeaning/nameofeachfeature,theyarereferredtoasfeature1,2,…
48.
The KNN learning algorithm is applied with the same number of
neighbors,usedfortheReliefFalgorithm.Thesearchspacecontains70
pairwise combinations; for ReliefF and KNN, 7 logarithmically spaced
pointsaredefined–k¼f1;2;4;7;13;24;45g–andforδ,10evenspaced
points–δ ¼f0:50;0:55;…;0:95g.Featurerelevanceisdeterminedby
comparingtheirweightswithτ ¼0:0245.Theincumbentmodelwith3
features(9,11,21)isfoundwithk¼45andδ ¼0:85.Then,threeCMare
created (feature 9, features 9,11, features 9,11,21) and the PMPCD is
usedasaMScriteriontoselectthefinalmodel.Accordingtothecriterion,
the3rdmodelshouldbeselected(PMPCD ¼0:9573),finalmodelhasan
estimatedtestingMPCD ¼0:9857.
AlthoughthesearchspaceoftheapplicationoftheKNNinthisdata
set is subject to combinatorial explosion – 8:7(cid:5) 1017(248(cid:5) 3100) – a
goodqualitysolutionisfoundaftercreating73models.
4.3. Casestudy3
Statlog (Landsat Satellite) [38], the original data set contains 36
featureswith7classes.Onlyclass1isconsidered–class1vsall–,the
datasetissplitasfollows:trainingset(4435-including1072),validation
set(1000–293),andtestingset(1000–168).
The SVM learning algorithm is applied to the same search space
describedinCaseStudy2(70pairwisecombinations).Featurerelevance
isdeterminedbycomparingtheirweightswithτ ¼0:0672.Theincum-
bentmodelwith8features(33,21,25,13,5,14,2,30)isfoundwithk¼7
and.Then,8CMarecreatedandthePMPCDisusedasaMScriterionto
selectthefinalmodel.Accordingtothecriterion,the8thmodelshouldbe
selected (PMPCD ¼ 0:8748), final model has an estimated testing
MPCD ¼0:9976.
4.4. Casestudy4
Occupancy Detection [38,39], the data set contains 5 features. To
Fig.7. MPCDexhaustivesearchinthe1-featureand2-featurespaces. generateanunbalanceddatastructure,oneoutof10instanceslabeledas
class 1 are included in the data set (index 1, 10, 20, etc.) and the
remainingnineeliminated,all0classareincluded.Thedatasetissplitas
follows:trainingset(6587-including173),validationset(1791-98),
andtestingset(7908-205).
evaluatingallpossiblecombinationstofindanoptimalsolutionrapidly The LR learning algorithm is applied to the same search space
describedinCaseStudy2.Featurerelevanceisdeterminedbycomparing
becomesunfeasibleasthefeaturespacegrowsup.
The optimal solution could be defined as the model with the least
theirweightswithτ ¼0:0551.TheOptimalClassificationThresholdwith
respecttoMPCDalgorithmisusedtoobtaintheclassificationthresholdof
numberoffeaturesandthehighestpredictionability.Inthiscasestudy,if
thereisnoothermodelwithanestimatedMPCD>0:9998,theoptimal each CM [30]. The incumbent model with 2 features (feature 3 - CO 2,
feature1-Humidity)isfoundwithk¼1andδ ¼0:50.Then,2CMare
solutions would be model indexes 1032 and 1035 Table 3. However,
created,according to the criterion,the single-featuremodelshould be
sincethenumberofcombinationsishuge,amodelwithmorefeatures
selected (PMPCD ¼ 0:9681), final model has an estimated testing
mayhavegreaterMPCD.Oftentimesduetothetradeoffbetweenmodel
MPCD ¼0:9879.
complexityandpredictionability,thereisnostraightforwardoptimal
solution,thistradeoffshouldbesolved.
AlthoughthePMQ-Ldidnotfindtheoptimalsolution,itdidpromptly 4.5. Discussion
findagoodqualitysolution–amodelthatefficientlyaddressestheposed
tradeoff.Fig.7showstherelativelocationofthesolution–finalmodel. MostofBigDatainitiativesaresubjecttofeaturecombinatorialex-
plosion,asituationthatgetsaggravatedbythehyperparametertuning
processe.g.,CaseStudy2.Oneofthemainchallenges,istodetectwhich
4.2. Casestudy2 features actually contain discriminative information, since oftentimes
mostofthemareeitherirrelevantorredundant.Inthefourcasestudies,a
SensorlessDriveDiagnosis[36],thedatasetcontains48numerical goodqualitysolutionisfoundaftercreatingonlyafewmodels–with
features(plustheclasslabel),whichareextractedfrommotorcurrent respecttothefeaturecombinationspace.Allthesesolutionseffectively
[37],themotorhasgoodanddefectivecomponents.Thisresultsin11 selectedonlythemostrelevantfeaturestomodelthepattern,sinceeach
different classes with different conditions. The goal of this study is to finalmodelisvirtuallyseparatingthetestingdata(MPCD(cid:6)1).
detectonlyclassone.Thisdatasetishighlyunbalanced(58509instances
-including5319class1)anditissplitasfollows:trainingset(33,409- 5. Comparativeanalysis
including 3100), validation set (12,100 - 1319), and testing set (13,
00–900).Sincethedatasetdoesnotprovidespecificinformationabout To evaluate the performance of the PMQ-L modeling scheme, two
6C.A.Escobaretal. Array7(2020)100034
Table4
Datasetsinformation,positiveclasscountinparenthesis.
Data Description Features Trainingset Validationset Testset Ratio(overall)
1 UMW 54 18,495(20) 12,236(9) 9500(7) 0.09b
2 Statlog(class1) 36 4435(1072) 1000(293) 1000(168) 23.82a
3 CreditCardFraud 29 200,000(385) 40,000(52) 44,807(55) 0.17b
4 OccupancyDetection 5 6587(173) 1791(98) 7908(205) 2.92a
5 HTRU2 8 12,000(1484) 2000(91) 3898(64) 9.16a
a Highly-unbalanced.
b Ultra-unbalancedThepreprocessinginformationcanbefoundinRefs.[43].
Table5
Solutionsbydataset,comparativeanalysis1.
Dataset l1-regularizedLR PMQ-L
Features Hyperparameter(λ) TestingMPCD Features Hyperparameters(k,δ) TestingMPCD
1 42 7.168e-07 0.8567 2 12,0.65 0.9956
2 34 4.721e-05 0.9929 8 1,0.95 0.9976
3 21 2.738e-05 0.8268 10 1,0.50 0.8033
4 5 9.839e-06 0.6784 1 1,0.50 0.9879
5 7 5.520e-05 0.8727 4 1,0.90 0.8758
additiontothedatasetsofcasestudies1,3,4,twopubliclyavailabledata
setsarealsoincludedinthisanalysis.2First,foreachdataset70CMare
developed and the final model selected using the PMQ-L modeling
scheme. Then, following the learning approach in Ref. [30], the
l -regularizedLRlearningalgorithmis appliedtothesamedatasetto
1
develop100CMbyvaryingtheregularizationvalues(λ)[41].Finally,
theAkaikeinformationcriterion[42]isusedtoselectthefinalmodel.The
solutionsofthetwolearningapproachesareevaluatedintermsofthe
numberofCMdeveloped,thenumberoffeaturesinthefinalmodeland
theirgeneralizationability.ResultsarepresentedinTable5.Forrepro-
ducibility purposes the hyperparameters values of the final models are
included.
Accordingtoexperimentalresults,four(datasets1,2,4,5)outoffive
solutionsoftheproposedlearningschemeoutperformsthel -regularized
1
LR-based solutions, since they have a lesser number of features and
exhibit better generalization performance. In the third solution, the
tradeoff is solved differently. The final model developed by PMQ-L ex-
hibits slightly lower generalization ability (0.8033 vs 0.8268), but it
includes significantly smaller number of features (10 vs 21). This
comparativeanalysisisgraphicallypresentedinFig.8.
Asecondcomparativeanalysiswithawidelyusedlearningalgorithm
inthecategoryofover/under-samplingmethodispresented.TheRUS-
Boost is a combination of random undersampling and AdaBoost [44]
specificallydesignedtoanalyzehighly/ultraunbalanceddatastructures.
Randomundersampling is appliedto themajorityclass tobalancethe
ratiobetweenminorityandmajorityclasses,thenAdaBoostisappliedto
thebalanced-subsettobuildamodel.Forthisanalysis,theRUSBoostis
Fig.8. Comparativeanalysis,MPCDandnumberoffeaturesbydataset.
appliedtothe4datasets3ofthecasestudiespresentedinSection4.With
a search space of 10–150 trees, the testing results of each of the final
modelsaresummarizedinTable6.
Inthisanalysis,highly/ultraunbalanceddatastructuresthatexhibit
comparativeanalysesarepresented:(1)vs.thel 1-regularizedLRlearning both,linearandnon-linearpatternsareanalyzed.Accordingtoempirical
algorithm[19],thisalgorithminducesparsimony,thereforethegoalof results,thePMQ-LdevelopedbetterpredictivemodelsthanRUSBoostin
thisanalysisistoevaluatehowPMQ-Lsolvestheposedtradeoffbetween alldatasets,Table6.Sinceinmostofthecases,becauseofthehyper-
complexity and parsimony; (2) vs. the Random Undersampling Boosting dimensionalfeaturespaces,thepatternisnotknowninadvance,there-
(RUSBoost) learning algorithm [40], this algorithm is designed specif- foreitisrecommendedtoapplyallthelearningalgorithmsthatcanbe
icallytoanalyzehighly/ultraunbalanceddatastructures,butitdoesnot handledbyPMQ-Ltofindthebestone.
induceparsimony,thereforepredictionanalysisisthemaingoalofthis
comparativestudy.
Five highly/ultra-unbalanced data sets are analyzed, Table 4. In
2 Sincethedatasetofcasestudy2doesnotexhibitalinearpattern(classes 3 SincetheRUSBoostcanhandlelinearandnon-linearpatternsthefourdata
cannotbeseparatedbyalinearclassifier),itisnotincludedinthisanalysis. setsareanalyzed.
7C.A.Escobaretal. Array7(2020)100034
Table6
Solutionsbydataset,comparativeanalysis2.
Casestudy RUSBoost PMQ-L
Features Hyperparameter(trees,learning TestingMPCD Features Hyperparameters(k,δ) TestingMPCD
rate)
1 54 40,0.1 0.9980 2 2,0.50 0.9995
2 48 140,0.1 0.8838 3 45,0.85 0.9857
3 36 140,0.1 0.8377 8 7,0.95 0.9976
4 5 10,0.1 0.9883 1 1,0.50 0.9885
6. Conclusions [6] RibeiroMT,SinghS,GuestrinC.WhyshouldItrustyou?:explainingthepredictions
ofanyclassifier.In:Procofthe22ndACMSIGKDDintConfonknowledge
Discoveryanddatamining;2016.p.1135–44.
A newHybridFeatureSelectionandPatternRecognitionmethodwith [7] GunningD.Explainableartificialintelligence(XAI).DefenseAdvancedResearch
thecapacitytolearnfromtheoriginaldatasetwasproposed,PMQ-L.Itis ProjectsAgency;2017.
aimedatdetectingrarequalityeventsthroughparsimoniousmodeling. [8] ShaoC,PaynabarK,KimT,JinJ,HuS,SpicerJ,WangH,AbellJ.Featureselection
Althoughtheproposedapproachdoesnotguaranteetofindtheoptimal formanufacturingprocessmonitoringusingcross-validation.JManufSyst2013;10.
[9] WuestT,WeimerD,IrgensC,ThobenK-D.Machinelearninginmanufacturing:
solution (if it exists), it did promptly find a good quality solution. Its advantages,challenges,andapplications.ProdManufRes2016;4(1):23–45.
uniqueabilitytodealwithhighly/ultra-unbalanceddatastructuresand [10] YuL,LiuH.Featureselectionforhigh-dimensionaldata:afastcorrelation-based
filtersolution.In:ICML,vol.3;2003.p.856–63.
diverselearningalgorithmstomodellinearandno-linearpatternswas
[11] HaixiangG,YijingL,ShangJ,MingyunG,YuanyueH,BingG.Learningfromclass-
demonstratedinthe4casestudies,whichalsoexhibiteditscapacityof imbalanceddata:reviewofmethodsandapplications.ExpertSystAppl2017;73:
selectingthedrivingfeaturesofthesystem. 220–39.
[12] HeH,MaY.Imbalancedlearning:foundations,algorithms,andapplications.John
Accordingtoempiricalresults,theproposedmodelingschemeout-
Wiley&Sons;2013.
performedwidely-usedmodelingapproachesbasedonthel 1-regularized [13] ShuklaS,B.S.R.Onlinesequentialclass-specificextremelearningmachinefor
logistic regression and the Random Undersampling Boosting learning al- binaryimbalancedlearning.NeuralNetwork2019;119(235–248).
gorithmsintermsofparsimony,generalizationabilityandthenumberof [14] ChandrashekarG,SahinF.Asurveyonfeatureselectionmethods.ComputElectr
Eng2014;40(1):16–28.
candidatemodelsneededtodevelopagoodsolution. [15] NgA.Onfeatureselection:learningwithexponentiallymanyirrevelantfeaturesas
Sincerareeventdetectionandinformationextractionaretwoofthe trainingexamples.In:Procofthe15thintConfonmachinelearning.MIT,Dept.of
mainmodernchallengesintheapplicationofMLacrossindustries,the
ElectricalEngandComputerScience;1998.p.404–12.
[16] DeSilvaAM,LeongPH.Featureselection.In:Grammar-basedfeatureGeneration
proposedmodelingapproachcanbegeneralizedtootherdomains–as
fortime-seriesprediction.Springer;2015.p.13–24.
supportedbythecasestudies–facingthesamechallenges. [17] DengH,RungerG.Featureselectionviaregularizedtrees,.In:IntJConfonneural
Inthisresearch,hyperparameters(k,δ)optimizationwasperformed networks;2012.p.1–8.
[18] WestonJ,MukherjeeS,ChapelleO,PontilM,PoggioT,VapnikV.Featureselection
with respect to validation MPCD only. If it is considered that greater forSVMs.In:NIPS,vol.12;2000.p.668–74.
separability between classes is preferred for generalization purposes. [19] TibshiraniR.RegressionshrinkageandselectionviatheLASSO,.JRoyStatSocB
Future research along this path, can focus on formulating the model 1996:267–88.
[20] NgA.FeatureselectionL1vsL2regularizationandrotationalinvariance.In:Proc.
assessment task as a two objective optimization problem. In which Ofthe21stintConfonmachinelearning.ACM;2004.p.78.
separability would be the second fitness attribute to be considered to [21] WangF,YangY,LvX,XuJ,LiL.Featureselectionusingfeatureranking,correlation
furtherdiscriminatebetweentwoormorecompetingmodels. analysisandchaoticbinaryparticleswarmoptimization.In:5thintConfonsoftware
Engandservicescience;2014.p.305–9.
[22] KiraK,RendellL.Thefeatureselectionproblem:traditionalmethodsandanew
Creditauthorstatement algorithm.In:AAAI,vol.2;1992.p.129–34.
(cid:3)
[23] Robnik-SikonjaM,KononenkoI.TheoreticalandempiricalanalysisofReliefFand
RReliefF.MachLearn2003;53(1–2):23–69.
CarlosAlbertoEscobar:Developedthemethodandledallthesections
[24] KononenkoI.Estimatingattributes:analysisandextensionsofRELIEF.In:European
andanalysis.RubenMorales-Menendez:Helpedtorunthecomparative Confonmachinelearning.Springer;1994.p.171–82.
analyses and contrast the contribution of the method. Daniela Macias [25] DevoreJ.Probabilityandstatisticsforengineeringandthesciences.Cengage
Learning;2015.
Arregoyta:Collaboratedwiththeliteraturereviewofthepaper.
[26] FawcettT.AnintroductiontoROCanalysis.PatternRecognLett2006;27(8):
861–74.
[27] EscobarCA,Morales-MenendezR.Process-Monitoring-for-Quality—amodel
Declarationofcompetinginterest selectioncriterion.SMEManufLett2018;15:55–8.
[28] ArlotS,CelisseA.Asurveyofcross-validationproceduresformodelselection.Stat
Surv2010;4:40–79.
The authors declare that they have no known competing financial [29] FriedmanJ,HastieT,TibshiraniR.Theelementsofstatisticallearning,vol.1.
interestsorpersonalrelationshipsthatcouldhaveappearedtoinfluence Berlin:StatisticsSpringer;2001.
theworkreportedinthispaper. [30] EscobarCA,Morales-MenendezR.Machinelearningtechniquesforqualitycontrol
inhighconformancemanufacturingenvironment.AdvMechEng2018;10(2):1–12.
[31] TheMathWorksInc.Logspace.[Online].2017.Available:www.mathworks.com/h
References elp/matlab/ref/logspace.html.
[32] KuhnM,JohnsonK.Appliedpredictivemodeling,vol.26.Springer;2013.
[33] BergstraJ,BengioY.Randomsearchforhyper-parameteroptimization.JMach
[1] KumarV,MathurP,SahniR,AnandM.Two-dimensionalmulti-releasesoftware LearnRes2012;13(2):281–305.
reliabilitymodelingforfaultdetectionandfaultcorrectionprocesses.IntJReliab [34] ClaesenM,DeMoorB.“Hyperparametersearchinmachinelearning.In:TheXI
QualSafEng2016;23(3):164–78.
metaheuristicsintconf;2015.
[2] HeY,LiuF,CuiJ,HanX,ZhaoY,ChenD,ZhouZ,AZ.Reliability-orienteddesign [35] MurphyK.Machinelearning:aprobabilisticperspective.MITpress;2012.
ofintegratedmodelofpreventivemaintenanceandqualitycontrolpolicywithtime- [36] LichmanM.UCImachinelearningrepository.2013[Online].Available:http://arch
between-eventscontrolchart.ComputIndEng2019;(129):228–38.
ive.ics.uci.edu/ml.
[3] CaiB,ZhaoY,LiuH,XieM.Adata-drivenfaultDiagnosismethodologyinthree- [37] PaschkeF,BayerC,BatorM,Mo€nksU,DicksA,Enge-RosenblattO,LohwegV.
phaseinvertersforPMSMdrivesystems.IEEETransPowerElectron2016;32(7): Sensorlosezustandsüberwachungansynchronmotoren.In:Proc23th.Workshop
5590–600.
computationalintelligence,vol.5;2013.p.211.Dortmund,Germany.
[4] KangS,KimE,ShimJ,ChoS,ChangW,KimJ.Miningtherelationshipbetween [38] DheeruD,KarraTaniskidouE.UCImachinelearningrepository.2017[Online].
productionandcustomerservicedataforfailureanalysisofindustrialproducts. Available,http://archive.ics.uci.edu/ml.
ComputIndEng2017;106:137–46. [39] CandanedoLM,FeldheimV.Accurateoccupancydetectionofanofficeroomfrom
[5] A drb ie vl el nJ mA, aC nuh fa ak cr ta ub ro inrt gy —D, pE rosc co eb ssa -r mC oA n, itI om rinK gH -f, oW r-qe ug an le itr yD pM hi, loW soin pc he yk .AM SA M. EBi Jg Mda at na uf mlig oh dt e, lt se .m Ep ne er ra gt yur Be u, ilh dum 20id 1i 6t ;y 11a 2n :d 28C –O 32 9m . easurementsusingstatisticallearning
SciEngDataSciEnhancManuf2017;139(10).
8C.A.Escobaretal. Array7(2020)100034
[40] SeiffertC,KhoshgoftaarTM,VanHulseJ,NapolitanoA.Rusboost:ahybrid [43] EscobarCA,Morales-MenendezR.Process-monitoring-for-quality—amodel
approachtoalleviatingclassimbalance.IEEETransSystManCybernSystHum selectioncriterionforl1-regularizedlogisticregression.ProcediaManuf2019;34:
2009;40(1):185–97. 832–9.
[41] T.M.Inc.Lasso.2011[Online].Available:https://www.mathworks.com/help/ [44] FreundY,SchapireRE,etal.Experimentswithanewboostingalgorithm.In:icml,
stats/lasso.html. vol.96.Citeseer;1996.p.148–56.
[42] BurnhamKP,AndersonDR.Modelselectionandmultimodelinference:apractical
information-theoreticapproach.SpringerScience&BusinessMedia;2003.
9