Array15(2022)100212
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/locate/array
Spatio-temporalaggregationofskeletalmotionfeaturesforhumanmotion
prediction
ItsukiUedaâˆ—,HidehikoShishido,ItaruKitahara
UniversityofTsukuba,1-1-1,Tennodai,Tsukuba,Ibaraki305-8577,Japan
A R T I C L E I N F O A B S T R A C T
Keywords: Thisstudyproposesahumanbodymotionpredictionmodelthatcanadapttothedisordersinvarioushuman
Humanmotion motion patterns and represent the kinematic constraints. In human motion prediction, the acquisition of
Lie-algebra featuresthatcaptureinter-motionandinter-jointlinkagesisconsideredeffective.Togeneratelinksthatare
Temporalaggregation adaptivetothereferencetimeofdominantfeaturesandtheircrossing-overjoints,weconstructanattention-
Spatialaggregation
basednetworkthataggregatesmotionsequencestemporallyandspatially.Weevaluatedthemotionprediction
Attention
resultsusingtheHuman3.6Mdatasetwiththeindicesofmeanangleerrorandmeanperjointpositionerror
andshowedthatourmethodoutperformsotherstate-of-the-artmethods.
1. Introduction conventional networks by temporal and spatial aggregation and to
obtain high generalization performance by incorporating each aggre-
3D human motion prediction is a process that takes a numeric gation model by adding appropriate information to the framework of
human body pose sequence as input and predicts the future pose attention[18].Weproposeamethodtoachieveahighgeneralization
sequence as shown in Fig. 1. Motion prediction is an essential tech- performance within each aggregation model by adding appropriate
nology with many applications such as humanâ€“robot interaction [1], informationtotheframework.
automateddriving[2],pedestriantracking[3].Variousinputformats Temporal aggregation involves â€˜â€˜translation generalizationâ€™â€™ and
are possible for person motion prediction, including video [4], scene â€˜â€˜time scale generalizationâ€™â€™ as shown in Fig. 3. The generalization
mesh[5],objectinformation[6,7].Especiallymotioncaptureinforma- of translation means acquiring features that do not depend on at
tion about a single person is widely used due to its high practicality which time in the input sequence a given action occurs. The gener-
in terms of sensing cost and accuracy. These motion predictions deal
alization of time scales means acquiring features that do not depend
with numerical values of skeletal information obtained from motion
on how long a behavior occurs and how fast the same behavior is
captures such as joint positions and angles, as shown in Fig. 2. Since
performed.Previousmethodsadoptmainlyfourapproaches,including
humanmotioninvolvesmanyinterlockingjoints,thedescriptionofits
convolutional neural network (CNN) [11], recurrent neural network
characteristicsrequireshigh-dimensionalinformationinbothtimeand
(RNN) [10,13], discrete cosine transform (DCT) [12] and attention
space.Inaddition,thepredictiontaskishighlydynamicandnonlinear
mechanism (Attention) [15,19]. CNN has excellent generalizability of
and inherently involves high-dimensional uncertainty as time passes.
temporal translation, but it is not easy to generalize the time scale
Therefore, there is a limit to the analytical acquisition of features
becausethekernelsizefixesthereferencetimetime-length.RNNshave
requiredformotionprediction[8,9].Recently,data-drivenapproaches
thegeneralizabilityoftime-scaleintermsofthereferencetime-length
such as [10â€“16] has become the mainstream. For example, in the
stretchmodeling.However,itbelongstotheMarkovianmodels,which
case of walking, both hands and feet dominates in the same motion
isnotsuitablefortranslationalgeneralization,duetotheaccumulation
cycle, while the left and right hands and feet are linked in opposite
of errors when the dominant features of the motion appear in the
phases.Thus,thereisastrongbiasinthecontinuityandregularityof
forward part of the sequence. DCT can acquire features referring to
naturalhumanmotion.Data-drivenapproacheshaveimprovedmotion
prediction performance by separating pose information into temporal theoverallinputtimebytransferringthemtofrequencyspacebefore
and spatial axes and modeling feature aggregation with appropriate processing them. The pose information transferred to the frequency
network structures, respectively [17]. In this paper, we propose a space can be generalized for both translation and scale because the
method to classify the features that have been generalized by the timeshiftandfrequencyareseparated.However,DCThasdifficulties
âˆ— Correspondingauthor.
E-mailaddress: ueda.itsuki@image.iit.tsukuba.ac.jp(I.Ueda).
https://doi.org/10.1016/j.array.2022.100212
Received26January2022;Receivedinrevisedform18June2022;Accepted21June2022
Availableonline23June2022
2590-0056/Â©2022TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).I.Uedaetal. Array15(2022)100212
Fig.3. Generalizationoftemporalfeatures:(Left)Thefootduringwalkingisdominated
by short-time features, while the center of gravity during a change of direction is
dominatedbylong-timefeatures.(Right)Featurestobewatched,suchasachangein
motion,arenotalwayslocatedbehindtheinput.
Fig.1. 3Dmotionprediction:Themodeltakestheobservationsequenceontheleft
asinputandoutputsthepredictionresultsequenceontheright.Fromtoptobottom,
theground-truthvalue,theposition-basedandangle-basedmodelsofTraj-GCN[12], identical and share the rotation matrix applied between joints. They
andtheoutputoftheproposedmethodareshown.Theproposedmethodproducesa also provide the importance that each node attaches to other nodes
posesequenceclosertotheactualvalue. during aggregation, as adjacency matrices, to represent the spatial
dependenciesbetweenjoints.
The methods that use motion trees as adjacency matrices can de-
scribespatialproximityinlinkconnectionrelationships,buttheymake
itdifficulttoreferencedistantjoints.Weietal.[12]enablesreferencing
of distant joints by learning the adjacency matrix. Due to the lack of
informationontheoriginalconnectionrelationsofthemotiontree,it
was not sufficient for acquiring global features such as capturing the
entirenearjointsofarmsandlegs.Lietal.[13]givestheclosenessof
jointstoeachotherbyconstructingamulti-scalegraphthatintegrates
closejoints.Whilewide-areafeaturesareeasiertoacquire,thechoice
of which joints to integrate relies on heuristics. The generalization of
spatialdependenceisstillanunsolvedproblem.
We focus on the fact that the generalization issues are different
between the two data description methods, position-based and angle-
Fig.2. Examplesofhowtodescribeposeinformation.Thehumanskeletonismodeled based,whichhavebeenevaluatedseparatelyasindependentproblems.
with links and rotational joints. (Left) The offset ğ¥ ğ‘– of each joint is set based on the Concretely,theposition-baseddescriptioncancapturethefeaturesthat
ğ‘‡ pose.(Right)Therearetwowaystoquantifythepose:aposition-baseddescription,
suchasthe3Dpositionofjointğ‘–,andanangle-baseddescription,suchastherotation representawiderangeofmotions,suchastheinterlockingofdistant
vectorofjointangles,ğ« ğ‘–âˆˆR3.Therearetwotypesofangle-baseddescriptions. jointsinthekinematictreeduetotherelativepositionalrelationships
of the joints. On the other hand, angle-based description has an ad-
vantageincapturingfeaturesthatrepresentlocalactions,suchaslocal
generalizingthetranslationbecausethetimeshiftremainsabsolutedue actions at a terminal joint, regardless of the posture of the root side
totheboundaryconditioninacyclicoperation.Attentioncanprovide in the kinematic tree. Therefore, we employ an approach in which
feature descriptions referring to the entire time region by explaining the angle-based description gives the features held by each node of
theproximitybetweentimepointswithPositionalEncoding.Although thejointgraph,andtheposition-baseddescriptionprovidesthespatial
Positional Encoding provides generalization for both translation and dependenceduringfeatureaggregation.
scale,itmaybeinferiortoDCTforacquiringperiodicmotionfeatures AsshowninFig.6,weattempttogeneralizebothlocalandglobal
because it gives the proximity discontinuously for each time pair. In motion features by building a predictor that handles angle-based fea-
this method, we propose a new positional encoding in the attention tureswhileincorporatingposition-basedfeaturessuitableforrepresent-
architecture,whichreproducestheoperationsinfrequencyspacesuch ingspatialdependencies.Sincetherotationaxisofeachjointcanvary
asinDCT.Thismethodprovidesfrequencyandtime-shiftinformation frommomenttomomentasthehumanpose,InverseKinematics(IK),
toAttentionâ€™sQueryintheframeworkofrelativepositionembedding analgorithmthatcalculatestheangleofeachjointfromtheinputposi-
(RPE).Themodelcanacquiremotionfeatureswithseparatetranslation tion,requirestheasymptoticcomputationofrecursiveequations.Since
andtimescales,whichmeansitcaninterpretthesamemotionevents theinclusionofIKinthenetworkdegradesthepredictionaccuracyfor
with the same parameters when they appear at different times and actionsfarfromthereferencepose,addingalocation-baseddescription
speedsofoccurrence. to the input does not directly improve accuracy. Therefore, previous
Spatialaggregationcriticallyinvolvesgeneralizingmotionfeatures methodsuseonlyangle-basedorposition-baseddescriptionsforinput
per joint and spatial dependencies between joints. Generalization of andoutput,suchasFig.5(seeFig.4).
motion features per joint means acquiring independent features re- The contribution of this work is to improve the performance of
gardless of any joint. For example, the motion features observed in motionpredictionbyincreasingthegeneralizabilityofmotionfeature
the right knee are also applicable to the left knee. Generalization of acquisition, both in terms of temporal and spatial aggregation. For
spatial dependence among joints means the acquisition of reference temporal aggregation, we introduce operations in frequency space in
relationships among joints to provide each motion, for example, the the framework of RPE to Self-Attention to achieve generalization in
rotationlinkageoftheshoulder,elbow,andwristtoproduceastraight- both translations of occurrence time and time width scale for the
line trajectory of the fingertips. Previous methods have focused on reference time of dominant motion features. For spatial aggregation,
theexpressionofspatialdependencyusingnetworkstructuressuchas we derive position-based and angle-based pose descriptions that can
Graph Convolution and Graph Attention. Specifically, they generate be linearly mapped in the vicinity of the input final pose through
a joint graph with each joint as a node, then detach the joint-wise theparameterspaceofLiealgebra,andassignposition-basedfeatures
featuresandthespatialdependenciesacrossjoints.Forjoint-wisefea- to angle-based features by Cross-Attention, which allows us to both
tures, they consider the feature dimensions retained by each node as generalize the motion of individual joints and generalize the spatial
2I.Uedaetal. Array15(2022)100212
Fig.4. (Left)IKisingeneralanonlinearmap.(Right)TheLiealgebraassociatedwith
thetangentplaneofthefinalposecandefinealinearmapbetweenthepositionand
thetangentplane.
Fig. 6. Linking position and angle: (Top) Defining a mapping from joint angles to
positions in the neighborhood of ğ‘‹ ğ¿, and generating a position-based description
withthenecessaryelementsfortheinversemapping.(Middle)Featureextractionand
transformationtoangle-basedfeaturesarerepresentedbyTransformer.(Bottom)Future
poses are output by aggregating both position- and angle-derived features with the
TransformerDecoder.
Fig. 5. Example of a transformer that takes only angles as input and output: (Top)
Posedescribedbyangleisembeddedintofeaturespace.(Bottom)Outputofthefuture
poseğ‘‹ ğ‘¡ describedbyangle,aggregatedbytheTransformerDecoder. it cannot handle behaviors with different periods. Hence, regression-
based methods, such as LSTM, are widely used because they can
train a time-dependent range [13,22,23]. However, regression-based
methods assume Markovianity, which reduces the robustness in the
dependencebetweenjoints.Weperformedbenchmarktestsformotion
temporaldirection.DCTtransformsatime-discreteinputsequenceinto
prediction using the Human3.6M [20] and CMU-Mocap [21] datasets
a continuous sequence in the time axis and a discrete sequence in
and showed that our model has superior performance compared to
the frequency axis. DCT aggregates centrally in the temporal direc-
conventionalmethods.
tion and directly references distant times, which provides excellent
generalization performance of the feature scale. On the other hand,
2. Relatedworks
thetransformationfromdiscretevaluesrequiresextrapolation,suchas
signalfoldingtoformaperiodicfunction.Asaresult,boundarycondi-
2.1. Temporalaggregation
tionsremain,andgeneralizationoftranslationischallengingforacyclic
transitionbehavior.Therefore,severalworksproposedamethodusing
Modeling human motion prediction is difficult due to its high
DCT to predict frequency space instead of pose space [12,14]. DCT
dimensionality, nonlinear dynamics, and the uncertainty of human
aggregates centrally in the temporal direction and directly references
motion. Analytical methods for short time forecasting have been pro-
distant times, which provides excellent generalization performance of
posed such as Gaussian process latent variable model [8], hidden
thefeaturescale.Ontheotherhand,thetransformationfromdiscrete
Markov model [9], and random forest [9]. However, these methods valuesrequiresextrapolation,suchassignalfoldingtoformaperiodic
have short applicability due to the modelsâ€™ limited complexity. Thus, function. As a result, boundary conditions remain, and generalization
the mainstream methods in recent years have shifted to using deep of translation is challenging for acyclic transition behavior. Aksan
learning. et al. [15] proposed another approach that uses Attention to refer to
Previous research has mainly used four models, CNN, RNN, DCT, features at distant times without distinction. In Attention, Positional
and Attention, as efficient ways to acquire motion features in time- Encoding assigns time proximity. The generalization performance for
series data processing by deep learning. Li et al. [11] built a Con- timetranslationoffeaturesishigh,andthegeneralizationperformance
volutionalSequence-to-Sequencemodelusingtimeseriesconvolution. tofeaturescalesisalsomaintainedduetothepossibilityofwide-area
Sincetherangeofspatialandtemporaldependenciescapturedbythis referencing.However,itisinferiortothemethodusingDCTinperiodic
model is statically determined by the size of the convolutional filter, operationbecausetheproximityexistsforeachtimepair.
3I.Uedaetal. Array15(2022)100212
We attempt to reproduce the scale generalization performance of comparisonwithangle-baseddescriptions.Inaddition,itisdifficultto
DCTwithtemporalattentionbyfocusingonthefrequencycomponents applytheconstraintthatthelinklengthisinvariant,suchasthearm
dominanttothemotionintheframeworkofPositionalEncoding.There lengthdoesnotchangeduringthemotionwhentheoutputisdirectly
are two computational forms of Positional Encoding: absolute posi- representedasa3Dposition.
tionalencoding(APE)andrelativepositionalencoding(RPE).Benyou With angle-based description, it is easy to obtain local behavioral
etal.[24]hasreportedthattheperformanceofBERTcanbeimproved featuressuchasgazingfromtheelbowtothetipoftheelbow,regard-
byusingAPEandRPEtogether.Wuetal.[25]proposedamultiplica- lessoftrunkbehaviorsuchasproneorcrouching.Also,bydescribing
tivepositionalencodingascontextualRPE.Inthismethod,wepropose theoutputasangle-based,fixedlinklengthcanbeapplied.Thedisad-
anRPEbasedonthecontextualRPE[25]thatcanreproduceoperations vantageisthatthelinklengthinformationislostfromthedata,making
infrequencyspace,i.e.,operationswhenusingDCT,asaconvolution it difficult to describe the relationship between distant joints that are
inposespace. notdirectlyconnected.Thismakesitdifficulttoacquireglobalfeatures
andmodelspatialdependenciesbetweenjointsdynamically.Weietal.
2.2. Spatialaggregation compared the performance of Res-Sup, ConvSeq2Seq, Traj-GCN, and
Traj-GCN networks by using a position-based description for direct
There are two essential aspects of spatial aggregation: generaliza- motionpredictionandanangle-baseddescriptionfortransformationto
tionofthemotionbetweenindividualjoints,suchasthetypicalmotion eachjointpositionafterprediction.Itispointedoutthattheprediction
oftherightandleftknees,andgeneralizationofthespatialdependency performance of the angle-based description is inferior to that of the
between joints, such as the interlocking of the shoulder, elbow, and position-baseddescription.
wrist,suchasthesmoothtrajectoryofthefingertips,whichisindepen- Intheproposedmethod,featuresandspatialdependenciesacquired
dentofthepostureofthewholebody.Inordertogeneralizethesetwo fromtheposition-baseddescriptionareintroducedintotheintermedi-
characteristics, conventional methods focus on the representation of ate features of the motion prediction network using the angle-based
dependenciesbetweenjointsbynetworkstructures.GraphConvolution description using Cross-Attention. The transformation from positional
andGraphAttentionconsiderthemotiontreeagraphwitheachjoint informationtoangularinformation,calledinversekinematics,requires
as a node and represent spatial dependencies by adjacency matrices. recursive computation because the superposition of rotations cannot
Li et al. [11] use the connection relation of the motion tree as the bedecomposed.Sinceevenadeep-learning-basedpredictorhasasig-
adjacencymatrixofGraphConvolution.GraphConvolutioncanexpress nificanterror,simplyaddingaposition-baseddescriptiontotheinput
spatialmonotonicitysuchthatthecloserjointsgivemoreimportance doesnotimproveperformance.Wedecomposetheposesequenceinto
because it aggregates only the information of adjacent joints in each differencesfromthefinalinputposetoisolateroughmotioninforma-
layer. On the other hand, it is difficult to refer to distant or strongly tion, such as standing or walking, which facilitates the coordination
synchronized joints in the motion tree, such as the left and right ofpositionandangles.Inasimilarapproach,Liuetal.[16]proposed
wristsduringhand-holding,despitetheirhighspatialdependency.Wei a joint trajectory space that can efficiently analyze motion context
et al. [12] introduce the adjacency matrix as a trainable parameter. whilepreservinginformationonjointtrajectoriesbydecomposingthe
Theyconsidertheconnectionsbetweenjointsascompletegraphsand descriptionofjointpositionsintothefinalposeandthevelocityofeach
describe the spatial dependence in edge weights, which enables dis- frame. We assume that the input and output poses are in the vicinity
tancejointreferencing.Ontheotherhand,theoriginalinformationof of the final input pose and achieve the linkage between position and
theconnectionrelationislost,suchaswhichjointsareadjacenttoeach angleviatheLiealgebraaroundtherotationmatrixofthefinalpose.
other.Lietal.[13]constructsamulti-scalegraphthatintegratesclose
joints to obtain global features such as focusing on the movement of 3. Ourapproach
theentirefootinsteadofindependentjointssuchasthekneeandankle.
Whilethismethodmakesiteasiertoobtainglobalfeaturesbyomitting Fig.7showstheentireproposedmethod.Forinputandoutput,we
similarjoints,itreliesonheuristicstoselectwhichjointstointegrate. usetherotationofeachjointintheformofarotationmatrix.Asapre-
Emre et al. [15] uses Graph Attention to feed the adjacency matrix processingstep,weconverttheinputtoposition-basedandangle-based
dynamically.Incommonwitheachmethod,theproblemisgeneralizing representations based on the final pose. Full connection and graph
thespatialdependencebetweenjoints. convolution gives the pose information as a feature vector for Self-
We focus on generalizing spatial dependency by utilizing data Attention and provide the APE information. This network comprises
structures instead of network structures. In general, networks utilize a Transformer Encoder, which takes the position-based description
position-basedorangle-baseddescriptionsfordatastructuresdescrib- as input and outputs the feature values, and a Transformer Decoder,
inginput/outputposes.Theposition-baseddescriptiondirectlyusesthe whichtakestheangle-baseddescriptionasinputandperformsforward
3D position of each joint acquired by motion capture [26], etc. The prediction.
angle-baseddescriptiondeterminesaskeletalreferenceshape,suchas The transformer contains Temporal Attention for aggregation be-
T-poseorA-pose.ItusesEuleranglesandrotationvectorstodescribe tween time points, Spatial Attention for aggregation between joints,
therotationofeachjointfromthereferenceshape. Cross Attention for importing position-based features to the decoder
In the position-based description, it is easy to obtain spatial con- side, and a Feed Forward network. The decoder outputs the data in
straints on the end joints, such as the mutation of the ankle position rotation vectors in all coupling layers for each time and joint. The
into a zero vector when the axial foot stops during ground contact. post-processing obtains the output pose sequence by finding the rota-
Theposition-baseddescriptionisalsosuitablefordescribingtherela- tion matrix corresponding to the vector using the matrix exponential
tionshipbetweenjointsandselectingthejointstobegazedatfroma functionandapplyingittothefinalinputpose.
globalperspective.Itcanexplicitlydescribethepositionalrelationship
between joints distant from each other in the motion tree by using 3.1. Position-basedandangle-basedposedescriptions
relative positions. On the other hand, the link length strongly affects
themovementofthejointposition,andthescaleisdifferentbetween In this method, position-based and angle-based features are ob-
thebehaviorofthehipjointandthatofthetoe.Forexample,evenifthe tainedforspatialinformationaggregationtodescribethecoordination
motionfromtheelbowtothetoeisthesame,thedynamiccoordinate ofdistantjointsandgeneralizeeachjoint.Theinversekinematicsprob-
transformationdependsontheshapeoftherootside,makingitdifficult lemisconvertedfromjointpositionstoangles,anditisdifficulttosolve
toidentifythemotionfeatures.Therefore,itismoredifficulttoobtain theerrorevenwithrecentdeeplearningtechniques.Inthisstudy,we
generalizedfeaturesofindividualjointsandfeaturesoflocaljointsin defineahighlyaccuratemappingfromthepositiondescriptiontothe
4I.Uedaetal. Array15(2022)100212
Theğ‘Š canbedefinedtomaptoğ‘†ğ‘‚(3)bythematrixexponential
function.ThismeansthatwecancomputetherotationmatrixM for
ğ‘¡,ğ‘–
frameğ‘¡,jointğ‘–usingtherotationvectorM relativetotheinputfinal
ğ‘¡,ğ‘–
frameasfollows:
([ ] )
M =ğ‘’ğ‘¥ğ‘ ğ« M . (3)
ğ‘¡,ğ‘– ğ‘¡,ğ‘– âˆ§ ğ¿,ğ‘–
By using ğ‘… = [ğ« ,â€¦,ğ« ]ğ‘‡ as the angle description, we can
ğ‘¡ ğ‘¡,1 ğ‘¡,ğ‘
quickly obtain the derivative for the angle of the joint position given
by the forward kinematics. The Lie algebraic parameter of rotation
provides a stable property as the inputâ€“output space of the network
becauseitavoidssingularityanduniquenessproblemssuchasgimbal
lock,unlikeangledescriptionssuchasEulerangles.
Next, we focus on the position-based description that can collabo-
rate with the angle-based description. For simplicity, we assume that
thelinksareconnectedas1,2,â€¦,ğ‘–,â€¦,ğ‘—âˆ’1,ğ‘—,â€¦,ğ‘.Theoffsetoflink
ğ‘–atthereferenceposeisğ¥ âˆˆR3,therotationmatrixfromthereference
ğ‘–
pose is M âˆˆ R3Ã—3, the translation of the root is ğ¥ , and the rotation
ğ‘– 0
matrixM =I.Thehomogeneouscoordinatetransformationmatrixğ‘‡
0 ğ‘–
bylinkğ‘–isshowninEq.(4).
( )
M ğ¥
ğ‘‡ = ğ‘– ğ‘– (4)
ğ‘– ğ‘‚ 1
The angle of each joint provides the position ğ©(0) in the world
ğ‘—
coordinatesystemofjointğ‘— byforward-kinematicsinEq.(2).
â›0â
( )
ğ©(0) âœ0âŸ
1ğ‘— =T 0T 1â‹¯T ğ‘—âˆ’1âœ âœ0âŸ
âŸ
(5)
âœ1âŸ
â â 
Wederivethebehaviorwhenlinkğ‘–isfurtherrotatedbyğ« inrotation
ğ‘–
vectornotationbasedonM ,â€¦,M .Therotationmatrixofjointğ‘–after
1 ğ¾
applying ğ« can be written as Mâ€² = ğ‘’ğ‘¥ğ‘([ğ«] )M. The position ğ©(0)â€² of
ğ‘– ğ‘– ğ‘– âˆ§ ğ‘– ğ‘—
jointğ‘— afterthetransformationisasfollows.
â›0â
( ğ©(0)â€²) ( ğ‘’ğ‘¥ğ‘([ğ«] ) 0) âœ0âŸ
ğ‘—
1
=T 0T 1â‹¯ 0ğ‘– âˆ§
1
T ğ‘–â‹¯T ğ‘—âˆ’1âœ âœ0âŸ
âŸ
(6)
âœ1âŸ
â â 
Weconsiderajointcoordinatesystemğ‘–whosecoordinatetransforma-
tionfromtheworldcoordinatesystemisgivenbyTâˆ’1Tâˆ’1 â‹¯Tâˆ’1asthe
Fig.7. Overallprocessdiagram. ğ‘–âˆ’1 ğ‘–âˆ’2 0
coordinatesystembasedonlinkğ‘–.Forthepositionğ©(ğ‘–) oflinkğ‘— inthe
ğ‘—
jointcoordinatesystemğ‘–,thefollowingisestablishedfromEq.(4).
( ) ( )
angledescriptionandembedthenecessaryinformationinadvanceto ğ©(ğ‘–) ğ©(0)
avoidtheaccumulationoferrors.Specifically,forwardkinematicscan 0ğ‘— = Tâˆ’ ğ‘–âˆ’1 1Tâˆ’ ğ‘–âˆ’1 2â‹¯Tâˆ’ 01 0ğ‘— (7)
provide a differentiable mapping from joint angles to positions using
the offset and rotation matrices for each link in the reference pose. ( ) â›0â
T deh fe inr eo sta ati Lo in eam lga etr bi rx a,ta wk he is cha cta an ng be ent usp el dan te oa ct ret ah te er aef 3e -r De Onc Fe pp ao rase mea tn ed
r
ğ© 0( ğ‘—ğ‘–)
= T ğ‘–T ğ‘–+1â‹¯T
ğ‘—âˆ’1âœ
âœ
âœ0 0âŸ
âŸ
âŸ
(8)
representation for each joint. Furthermore, this parameter space can âœ â1âŸ â 
beinterconvertedbydefiningJacobianswithpositiondescriptions.In
ByassigningthemtoEq.(6),wegetthefollowing:
this section, we first introduce the parameter notation of Lie algebra
as the angle notation. Then, we show that the position change and ( ğ©( ğ‘—ğ‘–)â€²) =( ğ‘’ğ‘¥ğ‘([ğ« ğ‘–] âˆ§) 0)( ğ©( ğ‘—ğ‘–))
(9)
the transformation map of Lie algebra to the parameter space can be 1 0 1 1
describedintermsofthelocalcoordinatesystemofjointpositions.
In the neighborhood of M ,â€¦,M âˆˆ ğ‘†ğ‘‚(3), a good approximation is
We denote the angular description of the pose at time ğ‘¡ as ğ‘‹ = 1 ğ¾
ğ‘¡ givenbyğ‘’ğ‘¥ğ‘([ğ«] )âˆˆğ‘ ğ‘œ(3)usingthesource[ğ«] onthetangentplane.
[ğŒ ,â€¦,ğŒ ]. Each ğŒ âˆˆ R3Ã—3 is an orthogonal matrix whose ğ‘– âˆ§ ğ‘– âˆ§
ğ‘¡,1 ğ‘¡,ğ‘ ğ‘¡,ğ‘– The mapping between the source and the position description on the
determinant is 1, which is a member of the special orthogonal group
tangentplaneisasfollows.
ğ‘†ğ‘‚(3).WeconsidertheLiealgebraassociatedwiththetangentplanein
theinputfinalframeM ğ¿,ğ‘–.Thematrixğ‘Š âˆˆR3Ã—3 onthetangentplane ( ğ©( ğ‘—ğ‘–)â€²)
=
( I+[ğ« ğ‘–]
âˆ§
0)( ğ©( ğ‘—ğ‘–))
(10)
ismultipliedbythesourceoftheLiealgebra[ğ«] âˆ§,representedbythe 1 0 1 1
rotationvectorğ«=[ğ‘Ÿ ,ğ‘Ÿ ,ğ‘Ÿ ]ğ‘‡,asfollows.
1 2 3 ğ©(ğ‘–)â€²âˆ’ğ©(ğ‘–) = [ğ«] ğ©(ğ‘–) (11)
ğ‘— ğ‘— ğ‘– âˆ§ ğ‘—
B = M +[ğ«] (1)
ğ¿,ğ‘– âˆ§ Fromtheanti-commutativityoftheLiebracketproduct,theexpression
0 âˆ’ğ‘Ÿ ğ‘Ÿ
â› 3 2 â (11)istransformedasfollows:
[ğ«] = âœğ‘Ÿ 0 âˆ’ğ‘Ÿ âŸ (2)
âˆ§ âœ ââˆ’ğ‘Ÿ3 2 ğ‘Ÿ 1 01 âŸ â  ğ©( ğ‘—ğ‘–)â€²âˆ’ğ©( ğ‘—ğ‘–)=[ğ©( ğ‘—ğ‘–)] âˆ§ğ« ğ‘–. (12)
5I.Uedaetal. Array15(2022)100212
Similarly, when each joint is rotated by ğ« ,â€¦,ğ« , the amount of
1 ğ¾
change in position is related to the amount of change in angle as
follows:
ğâ€²âˆ’ğ=(BâŠ—J)ğ‘ (13)
â›ğ©(0)â
âœ 1 âŸ
ğ = â‹® (14)
âœ âŸ
âœğ©(0)âŸ
â ğ¾â 
â›M(0)[ğ©(0)] â‹¯ ğ‘€(0)[ğ©(0)] â
âœ 1 1 âˆ§ ğ¾ ğ¾ âˆ§ âŸ
J = â‹® â‹± â‹® (15)
âœ âŸ
âœğ‘€(0)[ğ©(ğ‘)] â‹¯ ğ‘€(0)[ğ©(ğ‘)] âŸ
â 1 1 âˆ§ ğ¾ ğ¾ âˆ§â 
ğ«
â› 1â
R = âœâ‹®âŸ. (16)
âœ âğ« ğ¾âŸ â 
Note that B is the matrix representing the parentâ€“child relationship
betweenthejoints,whichinthisexampleistheunitlowertriangular Fig. 8. (Top) Spatial Attention aggregates the information of different joints at the
sametime.(Bottom)TemporalAttentionaggregatestheinformationofthesamejoint
matrix.Theğ½ canbecomputedfromğ‘ƒ usingEq.(2).Thus,bysubsti-
atdifferenttimes.
tutingğ‘ƒ(0) forthepositiondescriptionattimeğ¿inthereferencepose
ğ¿
ğ‘ƒ,ğ‘ƒ(0)forthepositiondescriptionattimeğ‘¡inthedestinationposeğ‘ƒâ€²,
ğ‘¡
andğ‘ ğ‘¡ fortherotationğ‘,wecanassociatetheangledescriptionwith Usingtheweightstobelearnedforeachjoint[Wğº,â€¦,Wğº],Wğº âˆˆ
thepositiondescriptionğ‘ ğ‘¡P ğ‘¡âˆ’P ğ¿=(BâŠ—J ğ¿)ğ‘ ğ‘¡. Rğ·Ã—ğ¶, the middle layer output ğ¸0 âˆˆ Rğ‘Ã—ğ· with ro0 tation ağ‘ ppliedğ‘– is
ğ‘¡
â›ğ©(0)â â›ğ©(0)â calculatedasfollows.
P ğ‘¡=âœ âœ âœğ©(â‹®ğ‘¡ ğ‘,1 )âŸ âŸ âŸ,P ğ¿=âœ âœ âœğ©(ğ¿ â‹® ğ‘,1 )âŸ âŸ
âŸ
(17)
ğ¸
ğ‘¡0=â› âœWğº 0 â‹®G ğ‘¡,0â
âŸ (19)
â ğ‘¡,1 â  â ğ¿,1â  âœ âWğº ğ‘G ğ‘¡,ğ‘âŸ â 
SinceBisavaluethatcanbesetstaticallyforthesameskeletalmodel,
The output ğ¸ğ‘™ of the ğ‘™th layer of the graph convolution is calculated
and since the skeletal model is a tree structure with joint 1 as the ğ‘¡
using the adjacency matrix ğ´ âˆˆ Rğ‘Ã—ğ‘, the weights ğ‘Š âˆˆ Rğ·Ã—ğ·, and
root,wecantransformBintoalowertriangularmatrixbyexchanging ğ‘™ ğ‘™
theactivationfunctionğœ(â‹…)asfollows:
columns.Theinverseofthemappingbythelowertriangularmatrixcan
beeasilyrepresentedinthenetwork.ByaddingtheinformationofJto Eğ‘™=ğœ(AEğ‘™âˆ’1W) (20)
ğ‘¡ ğ‘™ ğ‘¡ ğ‘™
theencoderâ€™sinput,thetransformationtotheanglecanbeembedded
Note that we assumed that the adjacency matrix is a complete graph
in the network. This method uses Cross-Attention to combine angle-
withedgeweightsinsteadofakinematictree,andğ´ isthetrainable
basedfeatures,sotherotationmatrixisfurtherseparatedattheencoder ğ‘™
parameterforthereferenceofdistantjoints.
side,andthecombinationofjointpositionandjointcoordinatesystem
P âˆˆRğ‘Ã—3ğ‘ isusedastheinputposition-baseddescription.
ğ‘¡ 3.3. Attention
â›ğ©(0) â‹¯
ğ©(ğ‘)âğ‘‡
âœ ğ‘¡,1 ğ‘¡,1 âŸ Inthismethod,weusetheSelf-Attentionmechanismtoefficiently
P = â‹® â‹± â‹® (18)
ğ‘¡ âœ âœğ©(0) â‹¯ ğ©(ğ‘)âŸ âŸ aggregate information from distant joints and times. The
â ğ‘¡,ğ‘ ğ‘¡,ğ‘â  Cross-Attention mechanism incorporates position-based features into
angle-basedfeatures.AsshowninFig.8,weusetwomodelsforSelf-
3.2. Embeddingofhumanposeinformation Attention: Spatial Attention, which aggregates the information of a
differentjointateachtime,andTemporalAttention,whichaggregates
Pose-Embedding generates ğ·-dimensional feature vectors of poses the information of a different joint at each time. Attention, which
that can be aggregated by Self-Attention from pose sequences with aggregatesinformationforeachjointatdifferenttimes.
position-basedandangle-baseddescriptionstransformedinSection3.1. Spatial Attention is an independent process at each time. The in-
Let the input and output pose sequences be the tensors of G âˆˆ put/outputtothelayerattimeğ‘¡isğ¸ ğ‘¡=[ğ ğ‘¡,1,â€¦,ğ ğ‘¡,ğ‘],ğ¸ ğ‘¡â€²=[ğâ€² ğ‘¡,1,â€¦,ğâ€² ğ‘¡,ğ‘]
Rğ¿Ã—ğ‘Ã—ğ¶,EâˆˆRğ¿Ã—ğ‘Ã—ğ·,respectively.Cisthenumberofelementsineach âˆˆRğ‘Ã—ğ·.ToapplytheScaleDot-productAttention[18],wecalculated
description, e.g., 3ğ¾ for the position-based description in Section 3.1 Query Q ğ‘¡, Key K ğ‘¡, and Value V ğ‘¡ from the linear transformation of ğ¸ ğ‘¡
and3fortheangle-baseddescription.Gisthenumberofelementsin by the learnable weights Wğ‘„ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™,Wğ¾ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™,Wğ‘‰ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ âˆˆ Rğ·Ã—ğ· of the
each description, e.g., 3ğ¾ in the position-based description and 3 in trainingtargetasfollows:
the angle-based description. G is described by a common evaluation Q = EWğ‘„ , (21)
axis for time, specifically, the feature values ğ  ,ğ  at time ğ‘¡1,ğ‘¡2 ğ‘¡ ğ‘¡ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
ğ‘¡1,ğ‘– ğ‘¡2,ğ‘–
andjointğ‘–aredirectlycomparable.Ontheotherhand,forspace,the K ğ‘¡ = E ğ‘¡Wğ¾ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™, (22)
feature values ğ  ğ‘¡,ğ‘–1,ğ  ğ‘¡,ğ‘–2 for time ğ‘¡ and joints ğ‘–,ğ‘— are taken to different V = EWğ‘‰ . (23)
evaluationaxes.Forexample,theelbowjointandthekneejointhave ğ‘¡ ğ‘¡ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
two axes of motion: flexionâ€“extension in the offset vertical direction To ensure that the tight joints of ğ‘„,ğ¾ are strongly referenced, we
and adductionâ€“abduction in the offset direction. They have similar computetheAttentionmapA ğ‘¡âˆˆRğ‘Ã—ğ‘ asfollows:
rangesofmotion,butthedirectionsoftheaxesofmotionaredifferent.
A =ğœ(QKğ‘‡). (24)
Therefore,theinputğº âˆˆRğ‘Ã—ğ¶ ateachtimeisrotatedforeachjoint, ğ‘¡ ğ‘¡ ğ‘¡
ğ‘¡
and then static graph convolution is performed on the joint graph to Note that ğœ(â‹…) is the activation function, which generally uses a
outputthefeaturevectorğ¸ âˆˆRğ‘Ã—ğ·. normalizationsuchthatthesumofthedirectionsisoneafterapplying
ğ‘¡
6I.Uedaetal. Array15(2022)100212
the ReLU or exponential function. By aggregating ğ‘‰ according to
ğ‘¡
the Attention map and projecting it to the representation space with
weightsWğ‘‚ âˆˆRğ·Ã—ğ·,theoutputğ¸â€² iscalculatedasfollows:
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ ğ‘¡
Eâ€²=AVWğ‘‚ . (25)
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
Eq.(25)givestheinterpretationthattheadjacencymatrixisadynam-
icallyacquiredgraphconvolution.
Temporal Attention is similarly an independent process for each
joint.Theinput/outputtothelayeratjointğ‘–isasfollows:
E =[ğ ,â€¦,ğ ],Eâ€²=[ğâ€² ,â€¦,ğâ€² ]âˆˆRğ¿Ã—ğ·. (26)
ğ‘– ğ‘–,1 ğ‘–,ğ¿ ğ‘– ğ‘–,1 ğ‘–,ğ¿
UsingtheweightsWğ‘„ ,Wğ¾ ,Wğ‘‰ ,Wğ‘‚ âˆˆRğ·Ã—ğ·,thetempo-
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
ralaggregationatjointğ‘–iscalculatedasfollows:
Q = EWğ‘„ (27)
ğ‘– ğ‘– ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
K = EWğ¾ (28)
ğ‘– ğ‘– ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
V = EWğ‘‰ (29)
ğ‘– ğ‘– ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
A = ğœ(QKğ‘‡ +M) (30)
ğ‘– ğ‘– ğ‘–
Eâ€² = ğ´ğ‘‰Wğ‘‚ (31)
ğ‘– ğ‘– ğ‘– ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
Note that Q,K,V âˆˆ Rğ¿Ã—ğ·,M,A âˆˆ Rğ¿Ã—ğ¿. The ğ‘€ is a mask to avoid
ğ‘– ğ‘– ğ‘– ğ‘–
referringtothefutureduringinference,whichisğ‘‚fortheencoderand
âˆ’âˆfortheuppertriangularcomponentforthedecoder.
3.4. Embeddingtimeandjointproximity
TheAttentionmechanismaggregatesallofthetimeandjointinfor-
mationintheinputindistinctly.Forthispurpose,theproximityoftime
andjointisassignedtothefeaturevectorbyPositionalEncoding.Our
method uses Absolutely Positional Encoding (APE) to obtain features
thatdependonabsolutetimeandspecificjointsandRelativePositional Fig.9. RPEgivestheproductofqueryandcosinewhencomputingTemporalAttention.
Encoding(RPE)toobtaingeneralizedfeaturesthatdonotdistinguish
thetimingofoccurrence.
APE assigns proximity to the feature vectors generated in Sec- system calculated by forward kinematics is [ğ©(0),â€¦,ğ©(0)], and their
ğ‘¡,1 ğ‘¡,ğ‘
tion 3.2 just before the networkâ€™s input. Proximity on the time axis respective true values are [Mğºğ‘‡,â€¦,Mğºğ‘‡][ğ©ğºğ‘‡,â€¦,ğ©ğºğ‘‡]. The angular
ğ‘ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™=[ğ‘§ğ‘¡ ğ‘¡ğ‘’ ,ğ‘ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™]âˆˆR(ğ¿+ğ‘†)Ã—ğ·,proximityonthespatialaxisğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™=
errorğœƒ
ğ‘¡,ğ‘–forjointğ‘–isdefinedbyğ‘¡,1 therotatğ‘¡ i,ğ‘ onanğ‘¡,1 gleofthğ‘¡, eğ‘
rotationmatrix
[ğ‘§ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™] âˆˆ Rğ‘Ã—ğ·, the feature of the pose embedding at time ğ‘¡, joint Mğ‘‡Mğºğ‘‡,whichistheresidual.Therotationangleisgivenbyavariant
ğ‘–,ğ‘ ğ‘¡,ğ‘– ğ‘¡,ğ‘–
ğ‘–, and feature dimension ğ‘ is ğ‘’ğ‘™ ğ‘¡,ğ‘–,ğ‘, then the value ğ‘’ ğ‘¡,ğ‘–,ğ‘ after APE ofRodriguezâ€™sformulaasfollows
assignmentisasfollows: ( tr(Mğ‘‡Mğºğ‘‡)âˆ’1)
ğ‘¡,ğ‘– ğ‘¡,ğ‘–
ğ‘’ ğ‘¡,ğ‘–,ğ‘ =ğ‘’ğ‘™ ğ‘¡,ğ‘–,ğ‘+ğ‘§ğ‘¡ ğ‘¡ğ‘’ ,ğ‘ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™+ğ‘§ğ‘  ğ‘–,ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ (32) ğœƒ ğ‘¡,ğ‘–=ğ‘ğ‘ğ‘œğ‘  2 (36)
In this method, we use a fully learnable APE with the learnable Theoverallangleerrorğ¿ isasfollows.
ğ‘Ÿğ‘œğ‘¡
parametersğ‘ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™,ğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ toensuremonotonicityoverawidearea. âˆš
InRPE,whencomputingTemporalAttention,wedesignitinsuch ğ¿ âˆ‘+ğ‘† âˆš âˆšâˆ‘ğ‘
L = âˆš ğœƒ2 (37)
awaythatitassignsproximityonlytoqueries,asshowninFig.9.For ğ‘Ÿğ‘œğ‘¡ ğ‘¡,ğ‘–
featuredimensionğ‘,iftheproximitybetweentimesğ‘¡ andğ‘¡ isğ‘§ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’, ğ‘¡=ğ¿+1 ğ‘–=1
1 2 ğ‘¡1âˆ’ğ‘¡2,ğ‘
Thepositionerrorğ¿ summarizestheEuclideandistanceoftheposi-
thenEq.(24)canbereplacedbythefollowingequation ğ‘ğ‘œğ‘ 
tionforeachjointasfollows:
A = ğœ(QKğ‘‡ +QZğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’) (33)
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ¿+ğ‘† ğ‘
â›ğ‘§ ğ‘¡âˆ’1,1 â‹¯ ğ‘§ ğ‘¡âˆ’ğ¿,1â L ğ‘ğ‘œğ‘ = âˆ‘ âˆ‘ |ğ©( ğ‘¡,0 ğ‘) âˆ’ğ©ğº ğ‘¡,ğ‘ğ‘‡ | (38)
Zğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ = âœ â‹® â‹± â‹® âŸ (34) ğ‘¡=ğ¿+1ğ‘–=1
ğ‘¡
âœ âğ‘§ ğ‘¡âˆ’1,ğ· â‹¯ ğ‘§ ğ‘¡âˆ’ğ¿,ğ·âŸ â  4. Experiments
Our method uses a fixed cosine RPE in the time axis and sets the
proximityğ‘§ğ‘Ÿ ğ‘¡1ğ‘’ âˆ’ğ‘™ğ‘ ğ‘¡ğ‘¡ 2ğ‘– ,ğ‘£ ğ‘–ğ‘’ ,ğ‘ asfollows: 4.1. Datasets
ğ‘¡ âˆ’ğ‘¡
ğ‘§ğ‘Ÿ ğ‘¡1ğ‘’ âˆ’ğ‘™ğ‘ ğ‘¡ğ‘¡ 2ğ‘– ,ğ‘£ ğ‘–ğ‘’ ,ğ‘ =ğ‘ğ‘œğ‘ ( 1001 002ğ‘2 âˆ•ğ·) (35) We verify the performance of the model on the motion capture
dataset Human3.6M [20]. The Human3.6M dataset contains 15 dif-
3.5. Training ferent classes of motion captured by seven subjects. Each sequence
contains the Euler angles of each joint concerning the ğ‘‡ pose for 32
Inthetraining,weuseğ¿ +ğœ†ğ¿ astheobjectivefunction,which jointsat50fps.Weapplieddownsamplingmethodfrom50fpsto25fps
ğ‘Ÿğ‘œğ‘¡ ğ‘ğ‘œğ‘ 
isacompositeoftheangleerrorğ¿ andthepositionerrorğ¿ with forallsequencesforfairness.Weusedthedataofsixsubjectstotrain
ğ‘Ÿğ‘œğ‘¡ ğ‘ğ‘œğ‘ 
thehyperparameterğœ†âˆˆR+.Theoutputfortimeğ‘¡(ğ¿+1â‰¤ğ‘¡â‰¤ğ¿+ğ‘†)is themodelandtesteditonthemovementclassofanothersubject.The
[ ]
ğ‘Œ = M ,â€¦,M , and the joint position in the world coordinate testdatatraditionallyusedeightrandomlyselectedsequencesforeach
ğ‘¡âˆ’ğ¿ ğ‘¡,1 ğ‘¡,ğ‘
7I.Uedaetal. Array15(2022)100212
Table1
EvaluationresultsofpositionerrorinH3.6M:â†“MPJPE[mm].
Scenarios Walking Eating Smoking Discussion
ms 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 29.36 50.82 76.03 81.51 16.84 30.60 56.92 68.65 22.96 42.64 70.14 82.68 32.94 61.18 90.92 96.19
DMGNN 17.32 30.67 54.56 65.20 10.96 21.39 36.18 43.88 8.97 17.62 32.05 40.30 17.33 34.78 61.03 69.80
Traj-GCN 12.29 23.03 39.77 46.12 8.36 16.90 33.19 40.70 7.94 16.24 31.90 38.90 12.50 27.40 58.51 71.68
MSR-GCN 12.16 22.65 38.64 45.24 8.39 17.05 33.03 40.43 8.02 16.27 31.32 38.15 11.98 26.76 57.08 69.74
Oursw/oEnc 12.90 23.77 39.36 46.19 8.70 16.94 33.24 40.68 7.88 16.32 31.78 38.86 12.64 27.55 58.55 71.72
Oursw/oRPE 12.65 22.24 38.28 45.39 8.03 17.43 33.02 40.27 8.16 16.32 31.15 38.32 11.96 26.84 57.22 69.63
Oursw/oPos 10.72 20.71 36.41 42.50 7.40 15.87 33.11 41.20 7.23 15.29 31.62 39.14 11.30 27.38 61.79 75.91
Oursw/oRot 12.69 22.81 39.63 46.66 9.27 17.70 33.69 41.32 8.80 16.58 31.10 37.92 14.12 29.39 60.87 74.55
Ours 8.55 18.06 34.78 41.85 6.06 13.68 30.00 38.01 5.64 12.68 28.27 36.10 9.25 23.42 55.87 70.55
Scenarios Directions Greeting Phoning Posing
ms 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 35.36 57.27 76.30 87.67 34.46 63.36 124.60 142.50 37.96 69.32 115.00 126.73 36.10 69.12 130.46 157.08
DMGNN 13.14 24.62 64.68 81.86 23.30 50.32 107.30 132.10 12.47 25.77 48.08 58.29 15.27 29.27 71.54 96.65
Traj-GCN 8.97 19.87 43.35 53.74 18.65 38.68 77.74 93.39 10.24 21.02 42.54 52.30 13.66 29.89 66.62 84.05
MSR-GCN 8.61 19.65 43.28 53.82 16.48 36.95 77.32 93.38 10.10 20.74 41.51 51.26 12.79 29.38 66.95 85.01
Oursw/oEnc 8.88 19.30 43.67 53.46 18.36 38.60 77.50 93.96 10.32 21.20 42.73 52.41 13.96 29.27 66.89 84.32
Oursw/oRPE 8.28 19.82 43.31 53.42 16.84 36.78 77.41 93.46 10.12 20.85 41.59 51.30 12.85 29.45 66.78 85.21
Oursw/oPos 8.12 19.98 46.89 58.59 16.51 38.45 81.90 98.69 9.28 20.71 44.85 56.02 12.59 30.95 72.14 90.70
Oursw/oRot 10.52 22.05 46.81 58.13 19.54 40.14 80.19 96.54 11.69 23.10 45.30 55.67 16.75 34.64 72.31 90.36
Ours 6.62 16.75 41.27 53.01 13.67 33.23 74.42 92.18 7.66 17.60 39.45 50.15 9.74 24.64 60.85 78.83
Scenarios Purchases Sitting SittingDown TakingPhoto
ms 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 36.33 60.30 86.53 95.92 42.55 81.40 134.70 151.78 47.28 85.95 145.75 168.86 26.10 47.61 81.40 94.73
DMGNN 21.35 38.71 75.67 92.74 11.92 25.11 44.59 50.20 14.95 32.88 77.06 93.00 13.61 28.95 45.99 58.76
Traj-GCN 15.60 32.78 65.72 79.25 10.62 21.90 46.33 57.91 16.14 31.12 61.47 75.46 9.88 20.89 44.95 56.58
MSR-GCN 14.75 32.39 66.13 79.64 10.53 21.99 46.26 57.80 16.10 31.63 62.45 76.84 9.89 21.01 44.56 56.30
Oursw/oEnc 15.78 32.25 65.62 79.90 10.91 21.14 46.12 57.47 16.64 31.39 61.13 75.70 9.53 20.99 44.26 57.12
Oursw/oRPE 14.05 32.89 66.65 79.70 10.17 19.87 46.96 56.95 13.68 28.85 60.11 76.46 9.62 21.30 44.33 56.91
Oursw/oPos 13.99 32.45 68.59 83.03 10.02 21.92 49.39 62.45 15.57 32.03 67.27 83.24 9.51 21.46 48.81 61.47
Oursw/oRot 17.74 35.83 69.06 82.61 12.97 25.31 50.27 62.06 18.48 35.06 67.88 83.40 12.11 24.37 49.93 62.29
Ours 11.50 27.89 63.05 78.37 8.00 18.65 43.70 56.42 11.93 26.71 59.97 76.38 7.39 17.71 43.52 56.80
Scenarios Waiting WalkingDog WalkingTogether Average
ms 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 30.62 57.82 106.22 121.45 64.18 102.10 141.07 164.35 26.79 50.07 80.16 92.23 34.66 61.97 101.08 115.49
DMGNN 12.20 24.17 59.62 77.54 47.09 93.33 160.13 171.20 14.34 26.67 50.08 63.22 16.95 33.62 65.90 79.65
Traj-GCN 11.43 23.99 50.06 61.48 23.39 46.17 83.47 95.96 10.47 21.04 38.47 45.19 12.68 26.06 52.27 63.51
MSR-GCN 10.68 23.06 48.25 59.23 20.65 42.88 80.35 93.31 10.56 20.92 37.40 43.85 12.11 25.56 51.64 62.93
Oursw/oEnc 11.54 24.21 51.33 62.58 23.93 46.31 83.79 96.52 10.63 21.34 38.54 45.22 13.65 26.67 52.36 63.88
Oursw/oRPE 9.12 20.06 47.52 59.73 19.36 39.87 78.92 93.45 9.27 19.41 36.82 44.16 11.01 21.55 49.86 62.90
Oursw/oPos 9.97 22.84 50.73 63.27 19.53 41.55 78.79 91.81 9.80 20.43 37.44 44.04 11.44 25.47 53.98 66.14
Oursw/oRot 12.41 24.15 47.99 59.24 23.04 44.47 80.19 93.24 11.56 21.37 37.51 44.33 14.11 27.80 54.18 65.89
Ours 7.92 18.88 44.39 56.95 16.25 36.99 75.67 91.42 7.34 16.31 34.24 42.45 9.17 21.55 48.63 61.30
movement class. However, the number of evaluation sequences was errorof3Dpositions,onlytheresultsofangularerrorwerecompared.
small,andthechosensequenceswerebiasedtowardeasilypredictable For Human3.6M, we conducted comparative experiments on whether
ones.Lingweietal.[14]pointedouttheinaccuracyoftheevaluation it has Encoder and RPE modules and uses position-based and angle-
andevaluatedtheentiresequenceforeachactionclass.Weinheritthis based descriptions. We omitted the cross-attachment of the decoder
benchmarkandevaluateitusingtheentiretestdata. and used only the self-attachment for the angle descriptions for the
We also evaluate the same on the CMU-Mocap dataset [21]. For experiments without the encoder. For the experiments without RPE,
thesakeoffairness,weusethesamedataasinRes-sup[10]withthe we configured each Temporal Attention as a regular self-attention.
anthropometricdifferencesremovedandvalidateitoneightclassesof We replaced the decoderâ€™s initial input ğ‘… ,â€¦,ğ‘… with a single zero
1 ğ¿
testcases. vectorfortheonlyposition-baseddescriptionexperiment.Fortheonly
rotation-baseddescriptionexperiment,wereplacedtheencoderâ€™sinput
4.2. Evaluations with angle informations ğ‘… ,â€¦,ğ‘… instead of positions ğ‘ƒ ,â€¦,ğ‘ƒ . We
1 ğ¿ 1 ğ¿
implemented the architecture of our method using Pytorch [27], and
Following the standard evaluation metrics used in Res-sup [10], trainedthemodelusingAdaBound[28]forthegradientmethod.The
the Mean Per Joint Position Error (MPJPE), which describes the av- learning rate is ğ‘™ğ‘Ÿ = 0.0001, ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘Ÿ = 1.0, ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦ = 0.0005,
ğ‘™
erage distance from the ground-truth value of each joint position mini-batchistrainedwithrandomextractionofbatchsize256,input
in millimeters, is used to evaluate the results. The prediction times sequencelengthis32frames,andoutputsequencelengthis10frames.
were compared for 80, 160, 320, and 400 [ms] as in the previous
study. We measured the MPJPE for the output angle converted to 4.3. Results
the position by forward-kinematics. As a baseline, we compared the
resultsofourmethodwiththoseoffourrecentmethods:Res-sup,Traj- Table 1 shows the experimental results of the position error in
GCN, DMGNN, and MSR-GCN. We used the results reported in each Human3.6M[20].InHuman3.6M,allthemotionclassesoutperformed
paper for each error directly. Since DMGNN [13] did not report the the conventional method in prediction below 320 [ms]. In ablation,
8I.Uedaetal. Array15(2022)100212
Table2
EvaluationresultsofpositionerrorinCMU-Mocap:â†“MPJPE[mm].
Motion Basketball BasketballSignal DirectingTraffic Jumping
Milliseconds 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 15.45 26.88 43.51 49.23 20.17 32.98 42.75 44.65 20.52 40.58 75.38 90.36 26.85 48.07 93.50 108.90
DMGNN 15.57 28.72 59.01 73.05 5.03 9.28 20.21 26.23 10.21 20.90 41.55 52.28 31.97 54.32 96.66 119.92
Traj-GCN 11.68 21.26 40.99 50.78 3.33 6.25 13.58 17.98 6.92 13.69 30.30 39.97 17.18 32.37 60.12 72.55
MSR-GCN 10.28 18.94 37.68 47.03 3.03 5.68 12.35 16.26 5.92 12.09 28.36 38.04 14.99 28.66 55.86 69.05
Ours 9.62 16.55 36.48 46.89 2.98 5.37 12.16 16.18 5.25 11.76 27.49 38.02 12.68 27.55 54.92 68.58
Motion Running Soccer Walking WashingWindow
Milliseconds 80 160 320 400 80 160 320 400 80 160 320 400 80 160 320 400
Res-sup. 25.76 48.91 88.19 100.80 17.75 31.30 52.55 61.40 44.35 76.66 126.83 151.43 22.84 44.71 86.78 104.68
DMGNN 17.42 26.82 38.27 40.08 14.86 25.29 52.21 65.42 9.57 15.53 26.03 30.37 7.93 14.68 33.34 44.24
Traj-GCN 14.53 24.20 37.44 41.10 13.33 24.00 43.77 53.20 6.62 10.74 17.40 20.35 5.96 11.62 24.77 31.63
MSR-GCN 12.84 20.42 30.58 34.42 10.92 19.50 37.05 46.38 6.31 10.30 17.64 21.12 5.49 11.07 25.05 32.51
Ours 11.54 16.29 29.21 34.68 9.76 18.45 35.29 44.65 6.13 9.67 16.56 19.22 5.13 11.56 23.35 29.43
comparedtothecasewithoutencodersorposition-basedinput,there constraints such as connection relation and link length invariance by
is a remarkable improvement in the whole-body movement classes usingangleasoutput.Byselectingtheappropriatearchitectureforex-
such as Discussion, Sitting, SittingDown, and WalkingTogether. We tractingthefeaturesofpositionandangle,wecreateaninferencethat
consideredthattheposition-baseddescription,directlyreferringtothe canpredictwithhighaccuracyevenfornon-periodicanddifficult-to-
positionalrelationshipofdistantjoints,workedeffectively. generalizemotionclasses.Theresultsshowthatourmodeloutperforms
On the other hand, there is no significant improvement in the thestate-of-the-artmethodsinshort-termprediction.
long-time prediction of 400 [ms]. The association between position
and angle is assumed to be near the last input pose as the reference CRediTauthorshipcontributionstatement
pose. This may have made it challenging to integrate the position-
based features into the angle-based features when the movement is
Itsuki Ueda: Conceptualization, Methodology, Software, Val-
significant. In addition, the performance of short-time prediction is
idation, Formal analysis, Investigation, Data curation, Writing
particularly significant for motion classes that include many periodic
â€“ original draft, Writing â€“ review and editing, Visualization.
motions,suchasWalking,Directions,Greeting,andPhoning.Thecon-
Hidehiko Shishido: Supervision. Itaru Kitahara: Conceptualization,
siderable improvement compared to the experimental results without
Methodology, Resources, Writing â€“ review and editing, Supervision,
RPEsuggeststhatthedominantfrequencycomponentenhancementby
Projectadministration,Fundingacquisition.
RPE plays a role similar to that of DCT and improves the prediction
performanceforperiodicmotions.
Declarationofcompetinginterest
In the ablation of data description, the performance of long-term
prediction without position-based and short-term prediction without
Oneormoreoftheauthorsofthispaperhavedisclosedpotentialor
angle-basedarelowerthanouroriginal.Weinterpretthisresultthatthe
pertinent conflicts of interest, which may include receipt of payment,
position-baseddescriptionaffectslong-timepredictionsinceitdetects
either direct or indirect, institutional support, or association with an
globalfeaturesofthewholebody.Incontrast,theangle-basedinvolves
entityinthebiomedicalfieldwhichmaybeperceivedtohavepotential
short-timepredictionsinceitdetectslocalfeaturesofeachjointunit.
conflict of interest with this work. Itsuki Ueda reports a relationship
Table 2 shows the experimental results of position error in CMU-
withPreferredNetworksIncthatincludes:employment.
Mocap [21]. Similar to Human3.6M, we can find an improvement in
theperformanceofCMU-Mocapforpredictionsbelow320[ms].Inad-
dition,theimprovementismoresignificantinthemotionclasseswith Acknowledgments
complexmotionpatternsnearthefinalinputpose,suchasbasketball.
We believe that this method extracts effective motion characteristics This work was supported by JSPS KAKENHI Grant Number
becauseitcanrefertobothposition-basedandangle-basedfeatures. JP19H00806,JP21KK0070andJP22H01580.
5. Discussionandfutureworks References
This paper proposes a method for extensive temporal and spatial [1] Akhter I, Sheikh Y, Khan S, Kanade T. Nonrigid structure from motion in
information aggregation using Self-Attention. For temporal aggrega- trajectory space. In: Proceedings of the neural information processing systems
tion, the dominant frequencies are emphasized in the framework of (NeurIPS);2009.
[2] PadenB,ÄŒÃ¡pM,YongSZ,YershovD,FrazzoliE.Asurveyofmotionplanning
relativepositionalencodingandusedtogetherwithabsolutepositional
and control techniques for self-driving urban vehicles. IEEE Trans Intell Veh
encodingtoachieveaggregationthatincorporatesthecharacteristicsof 2016;1(1):33â€“55.
conventional models using DCT. For spatial aggregation, we focus on [3] GongH,SimJ,LikhachevM,ShiJ.Multi-hypothesismotionplanningforvisual
the fact that position-based and angle-based descriptions are suitable object tracking. In: Proceedings of the IEEE/CVF international conference on
for global and local feature extraction, respectively, and constructed computervision(ICCV).IEEE;2011,p.619â€“26.
[4] ZhangJY,FelsenP,KanazawaA,MalikJ.Predicting3DHumanDynamicsfrom
a structure that incorporates the position-based features extracted by
Video. In: Proceedings of the IEEE/CVF international conference on computer
the encoder into the angle features by Cross-Attention. We also show vision(ICCV);2019.
that a linear mapping can be defined between the parameter space [5] WangJ,XuH,XuJ,LiuS,WangX.Synthesizinglong-term3dhumanmotion
of Lie algebra and the position space of the joint coordinate system. and interaction in 3d scenes. In: Proceedings of the IEEE/CVF conference on
We introduce a position- and angle-based description in the neigh- computervisionandpatternrecognition(CVPR);2021,p.9401â€“9411.
[6] Adeli V, Ehsanpour M, Reid I, Niebles JC, Savarese S, Adeli E, Rezatofighi H.
borhood coordinates of the final input pose. At the same time, we
Tripod: Human trajectory and pose dynamics forecasting in the wild. In:
realized the expression of conditions such as fixation and contact
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision(ICCV);
of end joints by using the position as input and the expression of 2021,p.13390â€“13400.
9I.Uedaetal. Array15(2022)100212
[7] Corona E, Pumarola A, Alenya G, Moreno-Noguer F. Context-aware human [19] WangJ,XuH,NarasimhanM,WangX.Multi-person3Dmotionpredictionwith
motion prediction, In: Proceedings of the IEEE/CVF conference on computer multi-rangetransformers.In:Advancesinneuralinformationprocessingsystems
visionandpatternrecognition(CVPR);2020,p.6992â€“7001. (NeurIPS),vol.34.2021.
[8] Wang JM, Fleet DJ, Hertzmann A. Gaussian process dynamical models. In: [20] IonescuC,PapavaD,OlaruV,SminchisescuC.Human3.6m:Largescaledatasets
Proceedings of the neural information processing systems (NeurIPS), vol. 18.
and predictive methods for 3d human sensing in natural environments. IEEE
Citeseer;2005,p.3.
TransPatternAnalMachIntell2014.
[9] Lehrmann AM, Gehler PV, Nowozin S. Efficient nonlinear markov models for
[21] CMU.Carnegie-mellonmocapdatabase.2003,http://mocap.cs.cmu.edu.
humanmotion.In:ProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition(CVPR);2014,p.1314â€“1321. [22] FragkiadakiK,LevineS,FelsenP,MalikJ.Recurrentnetworkmodelsforhuman
[10] MartinezJ,BlackMJ,RomeroJ.Onhumanmotionpredictionusingrecurrent dynamics.In:ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
neuralnetworks.In:ProceedingsoftheIEEEconferenceoncomputervisionand vision(ICCV);2015,p.4346â€“4354.
patternrecognition(CVPR);2017,p.2891â€“2900. [23] JainA,ZamirAR,SavareseS,SaxenaA.Structural-rnn:Deeplearningonspatio-
[11] LiC,ZhangZ,LeeWS,LeeGH.Convolutionalsequencetosequencemodelfor temporalgraphs.In:ProceedingsoftheIEEEconferenceoncomputervisionand
humandynamics.In:ProceedingsoftheIEEEconferenceoncomputervisionand patternrecognition(CVPR);2016,p.5308â€“5317.
patternrecognition(CVPR);2018,p.5226â€“5234. [24] WangB,ShangL,LiomaC,JiangX,YangH,LiuQ,SimonsenJG.OnPosition
[12] MaoW,LiuM,SalzmannM,LiH.Learningtrajectorydependenciesforhuman EmbeddingsinBERT.In:Proceedingsoftheinternationalconferenceonlearning
motionprediction.In:ProceedingsoftheIEEE/CVFInternationalConferenceon
representations(ICLR);2021.
ComputerVision(ICCV);2019.
[25] Wu K, Peng H, Chen M, Fu J, Chao H. Rethinking and Improving Relative
[13] LiM,ChenS,ZhaoY,ZhangY,WangY,TiaQ.DynamicMultiscaleGraphNeural
Position Encoding for Vision Transformer. In: Proceedings of the IEEE/CVF
Networks for 3D Skeleton-Based Human Motion Prediction. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition (CVPR); internationalconferenceoncomputervision(ICCV);2021,p.10033â€“10041.
2020. [26] HoriuchiY,MakinoY,ShinodaH.ComputationalForesight:ForecastingHuman
[14] Dang L, Nie Y, Long C, Zhang Q, Li G. MSR-GCN: Multi-Scale Residual Body Motion in Real-time for Reducing Delays in Interactive System. In:
GraphConvolutionNetworksforHumanMotionPrediction.In:Proceedingsof Proceedings of the 2017 ACM international conference on interactive surfaces
the IEEE/CVF international conference on computer vision (ICCV); 2021, p. andspaces;2017.
11467â€“11476. [27] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z,
[15] AksanE,KaufmannM,CaoP,HilligesO.ASpatio-temporalTransformerfor3D Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M,
HumanMotionPrediction.In:Proceedingsoftheinternationalconferenceon3D Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. PyTorch:
vision(3DV);2021.
An imperative style, high-performance deep learning library. In: Advances in
[16] LiuZ,SuP,WuS,ShenX,ChenH,HaoY,WangM.MotionPredictionUsing
neuralinformationprocessingsystems(NeurIPS).CurranAssociates,Inc.;2019,
Trajectory Cues. In: Proceedings of the IEEE/CVF international conference on
p. 8024â€“35, URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-
computervision(ICCV);2021.
[17] AksanE,KaufmannM,HilligesO.StructuredPredictionHelps3DHumanMotion style-high-performance-deep-learning-library.pdf.
Modelling.In:ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer [28] LuoL,XiongY,LiuY,SunX.AdaptiveGradientMethodswithDynamicBound
vision(ICCV);2019. ofLearningRate.In:Proceedingsofthe7thInternationalConferenceonLearning
[18] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Representations(ICLR);2019,NewOrleans,Louisiana.
Polosukhin I. Attention is all you need. In: Guyon I, Luxburg UV, Bengio S,
Wallach H, Fergus R, Vishwanathan S, Garnett R, editors. Proceedings of the
neuralinformationprocessingsystems(NeurIPS),vol.30.CurranAssociates,Inc.;
2017.
10