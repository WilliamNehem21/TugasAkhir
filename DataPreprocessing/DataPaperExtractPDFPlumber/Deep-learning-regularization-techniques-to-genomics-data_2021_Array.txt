Array11(2021)100068
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
Deep learning regularization techniques to genomics data
Harouna Soumarea,b,*, Alia Benkahlab,1, Nabil Gmatic,1
aTheLaboratoryofMathematicalModellingandNumericinEngineeringSciences,NationalEngineeringSchoolofTunis,UniversityofTunisElManar,RueB(cid:1)echirSalem
BelkhiriaCampusUniversitaire,B.P.37,1002,TunisBelv(cid:1)ed(cid:3)ere,Tunisia
bLaboratoryofBioInformatics,BioMathematics,andBioStatistics,InstitutPasteurdeTunis,13PlacePasteur,B.P.741002,Tunis,Belv(cid:1)ed(cid:3)ere,Tunisia
cCollegeofSciences&BasicandAppliedScientificResearchCenter,ImamAbdulrahmanBinFaisalUniversity,P.O.Box1982,31441,Dammam,SaudiArabia
A R T I C L E I N F O A B S T R A C T
Keywords: DeepLearningalgorithmshaveachievedagreatsuccessinmanydomainswherelargescaledatasetsareused.
Deeplearning However, training these algorithms on high dimensional data requires the adjustment of many parameters.
Overfitting Avoidingoverfittingproblemisdifficult.RegularizationtechniquessuchasL1 andL2 areusedtopreventthe
Regularizationtechniques
parametersoftrainingmodelfrombeinglarge.AnothercommonlyusedregularizationmethodcalledDropout
Dropout
randomlyremovessomehiddenunitsduringthetrainingphase.Inthiswork,wedescribesomearchitecturesof
Genomics
DeepLearningalgorithms,weexplainoptimizationprocessfortrainingthemandattempttoestablishatheo-
retical relationship between L2-regularization and Dropout. We experimentally compare the effect of these
techniquesonthelearningmodelusinggenomicsdatasets.
1. Introduction is that the training model fits well the training dataset but looses its
predictioncapacityonunseendatasets.
In the last decade, Deep Learning (DL) algorithms have achieved Preventing overfitting problem is one major challenge in training
tremendoussuccessinmanydomainswherelargescaledatasetsareused thesealgorithms.However,therearemanytechniquesthatdealwiththe
suchasBioinformatics[2,13,50,62,88,94],NaturalLanguageProcessing problemofoverfittingcalled“regularizationtechniques”.Themostused
[5,15,28,47,71], Computer Vision and Speech Recognition [1,4,29,34, regularization techniques in Machine Learning (ML) community are L1
37,56,65]. and L2 regularization's [53]. The idea is to preventthe weights of the
Inthiswork,wereviewaclassofDLalgorithmscalledFeedforward model from being large by adding a supplementary term to the loss
Neural Network (FNN) [54,68,91], in which information moves in one function.Theeffectofthispenalizationistomakeitsothelearningal-
direction, from input to output through sequential operations called gorithmpreferstolearnsmallweights.Thismethodmakesmodelsless
“layers”. These models are the generalization of logistic regression complexandavoidtheriskofoverfitting.Anothercommonlyusedreg-
models,both(FNNandlogisticregression)arewidelyusedinBioinfor- ularization technique so-called “Dropout”, developed by Hinton et al.
matics and Biomedical science to perform classification and diagnosis [33]consiststorandomlyremovesomeneurons(inhiddenlayers)dur-
tasks [8,20,21,24,27,44,48,70,73]. In most cases, we look for a non ingthetrainingphase.Thisforcesthehiddenunitstoextractusefulin-
linearmappingy¼fðxÞbetweenavariableyandavectorofvariablesx. formation's from the input data and reduce co-adaptation between
Theformoff dependsonthecomplexityofthestudiedproblem. hiddenunits,thusmakingthemodellesssensitivetothespecificweights
Logisticregressiondefinesalowcomplexitymodelusingasimplenon ofneurons.TheDropouttechniqueallowstotrainanexponentialnumber
linear mapping from inputs to outputs. Whereas FNN defines a more of(thinned)Networksinareasonabletime[33].Duringthetestphase,
complexmappingbetweeninputsandtheircorrespondingoutputs,thus taking the mean prediction of the different (thinned) Networks is
resulting models have high complexity and flexibility and better pre- equivalenttotestonasingleNetworkwithallthehiddenneurons[6]
diction capacity. However, increasing the complexity of predictive (withoutdroppingoutanyunit).Tocompensatethefactthattheweights
modelsincreasesalsotheriskofoverfittingproblem,whichrepercussion are learned under Dropout, the outcome weights of neurons of each
* Correspondingauthor.TheLaboratoryofMathematicalModellingandNumericinEngineeringSciences,NationalEngineeringSchoolofTunis,UniversityofTunis
ElManar,RueB(cid:1)echirSalemBelkhiriaCampusUniversitaire,B.P.37,1002,TunisBelv(cid:1)ed(cid:3)ere,Tunisia.
E-mailaddresses:soumare.harouna@enit.utm.tn(H.Soumare),Alia.Benkahla@pasteur.tn(A.Benkahla),nmgmati@iau.edu.sa(N.Gmati).
1 Theseautorscontributedequally,theorderoftheirnamesisalphabetical.
https://doi.org/10.1016/j.array.2021.100068
Received28November2020;Receivedinrevisedform26March2021;Accepted10May2021
Availableonline24May2021
2590-0056/©2021PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).H.Soumareetal. Array11(2021)100068
hiddenlayeraremultipliedbytheDropoutrateofthatlayer,whichisa
classprobabilitiessumupto1,i.e.
Pnc
softmaxðzÞ ¼1.Fig.1describes
gain in terms of computation time. However, the quality of this i
i¼1
approximationremainslittleknown. the simplest possible Neural Network (NN), which contains a single
ManytheoreticalDropoutanalyseshavebeenexplored[6,23,31,49, neuroncorrespondingexactlytoaninput-outputmapping.Aneuronwith
55,58,75,79,81]. Baldi et al. [6] showed how the technique acts as sigmoid output function is equivalent to logistic regression. FNN is a
adaptative stochastic Gradient Descent. Wager et al. [79] analyzed nonlinear function, which is also composed of several simpler func-
Dropout as an adaptive regularizer for Generalized Linear Models tions(neurons,wheretheoutputofaneuroncanbeusedasaninputof
(GLMs).Maetal.[46]attemptedtoexplicitlyquantifythegapbetween another. Each of these functions provides a new representation of the
Dropout'strainingandinferencephasesandshowedthatthegapcanbe inputdata.Itiscomposedofaninputlayer,oneormorehiddenlayer(s)
usedtoregularizethestandardDropouttraininglossfunction. andanoutputlayer.
This paper explains the mathematics behind training DL algorithms
andattemptstofurtherestablishthetheoreticalrelationshipthatexists 2.1. SupervisedNeuralNetwork
between Dropout and other regularizations, mainly L2 norm. We
compare experimentally the effects of regularization techniques on Let's consider an L hidden layer Feedforward Neural Network, in
trainingmodelsusingtwodifferentgenomicclassificationdatasets. whichninputtrainingsamplesX¼x 1;x 2;…;x narelabeled,i.e.,givenan
ThehumanDNAisalongchainof3billionbasepairs,thefunctionofa inputx i,thecorrespondingoutputbythemodelisknownanddenotedy
i
largepartofit,isunknown.SomefragmentsofDNAcalledgenescodefor oryðx iÞ.Whereyisavectorcontaininglabels.AstandardNeuralNetwork
proteinsthatplayimportantrolesinchemicalprocessesessentialtolife. canbedescribedasfollows:
Somechangesinthegenescauseadysfunctionintheproductionofthe (cid:2) (cid:3)
corresponding proteins, which could cause genetic diseases. The most aðlÞ¼φ zl ; (1)
j j
common genetic changes are called Single Nucleotide Polymorphisms
(SNPS)andarecausedbyachangeofabasepairbyanotheroneata X
given position in the genome. It has been shown that some SNPS are zl j¼ wl ijað il(cid:3)1Þþbl j¼aðl(cid:3)1Þ(cid:2)wl jþbl j; (2)
i
involvedinseveralhumandiseasesandcanbeusedtopredicthuman
repo Inns oe ut ro exc per et ra ii mn ed nr tu s,g ws[ e2 s7 ta].
rtedbyusingLogisticRegressiononcancer
wherezl j,bl jandal j(a0
j
¼x j,forad-dimensionalinputx ¼ðx 1x 2…x dÞT)
are the jth hidden input, bias and activation function of the lth layer,
datasets,obtainedfromtheExpressionProjectforOncology(EXPO)[60].
respectively.wl istheweightconnectionfromtheithunitoftheðl(cid:3)1Þth
ThenwetrainedFNNonone1000GenomesProjectdatasetforindividual ij
ancestry prediction according to their genetic profile [59]). All in- layertothejthunitofthelthlayer.wl andaðl(cid:3)1Þ are,respectively,the
j
dividualsarerepresentedinbothdatasetsbytheirSNPSprofile[14]. incomingweightvectortothejthneuronoflayerlandtheoutputvector
Thisworkisorganizedasfollows:Section2describestheFNNarchi- of (l-1)th layer, φ is any activation function. FNNs can be seen as a
tecturesandthemathematicsbehindthem;inSection3Gradientdescent generalization of simple regression models. In fact, keeping only the
algorithmispresented;Section4describesthetraditionalregularization input layer with one linear output neuron in a Feedforward Network
techniquesandDropout,Section5describesthematerialsandmethods definesalinearregression,andwithasigmoidorsoftmaxfunctionatthe
and in Section6, we presentthe experimentalresults,where different output layer represents a logistic regression. Learning of a supervised
regularizationtechniquesareused. Network[26,63,87,90]consiststofindtheparametersw andb sothat
j j
outputaL fromthemodelapproximatesthedesiredoutputvectoryðxÞ,
2. DeepLearning:FeedforwardNeuralNetwork(FNN) foralltraininginputsx.Toachievethisgoal,wedefineameansquared
errorlossfunction
Inthiswork,wediscussFeedforwardNeuralNetwork(FNN)[42,54,
X
57,69,76,91]orMulti-LayerPerceptron(MLP).InsuchNetworks,thein- C¼1 Cx; (3)
formationmovesonlyfromtheinputtotheoutput(seeFig.2),without n x2X
anyloop.ThistypeofmodelismostlyusedforsupervisedMLtaskssuchas
P
regressionorclassificationtasks,wherethetargetfunctionisknown.The Cx ¼1 2kyðxÞ(cid:3)aLðxÞk2
2
¼1
2
n k¼c 1ðy k(cid:3)aL kÞ2.Wherey kandaL karekthoutput
basic supervised learning algorithm is linear regression [12,51,82], in activationanddesiredoutputrespectively,foragiveninputx.
thistaskthealgorithmlearnstomapaninputdatax2Rd tosomereal Thereisanotherchoiceoflossfunctionknownascrossentropy.To
valuey,byalineartransformation definethisfunction,let'sconsiderabinaryclassificationproblemwith
sigmoidoutputfunctionaLðxÞ ¼σðzÞ,foreachtraininginputsamplex,
f : Rd→R : wehavePðy¼0jxÞ¼1(cid:3)Pðy¼1jxÞ¼1(cid:3)aLðxÞandmoregenerally
x→z¼x(cid:2)wþb
(cid:4) (cid:5)
Wherewandbarerespectivelytheweightvectorandthebiasterm.The
Pðy¼y ijx iÞ¼aLðx iÞyi 1(cid:3)aLðx iÞ1(cid:3)yi :
symbol“(cid:2)”isthedotproductbetweentwovectors.Anothersimplesu-
pervised learning algorithm called logistic regression is used for the
classificationproblemswherethetargetfunctiontakesdiscretevalues.
Givenaninputdatax,thelogisticregression[18,19,39,74,86]appliesa
non linear function to its corresponding linear regression output z, to
produce classes membership probabilities. For example, in a binary
classification task, given x and it corresponding class C , logistic
1
regression algorithm outputs the conditional probability PðC 1jxÞ of x
givenC 1.ThisprobabilityisgivenbysigmoidfunctionσðzÞ ¼ 1þ1 e(cid:3)z.Inthe
casewheretherearemorethantwoclasses,theconditionalprobability
PðC ijxÞisgivenbythesoftmaxfunctionsoftmaxðzÞ
i
¼P ne czi ezk.Wherez¼
k¼1
ðz 1z 2…z ncÞ and z i ¼ x(cid:2)w iþ b i. w i and b i are respectively the weight
vectorandbiastermoftheithclassC.n isthenumberofclassesandthe
i c Fig.1. Logisticregression“neuron”.
2H.Soumareetal. Array11(2021)100068
hidden representation and decoder tries to reconstruct inputs from an
encoder, so it contains at least one hidden layer. In an Autoencoder
Network,thetargetyðxÞofeachinputsamplexistheinputitself,i.e.
yðxÞ ¼x,8x2Rd.Attheendtheoutputhasthesizeoftheinput.
ThemainobjectiveofanAutoencoderistoautomaticallycapturethe
most relevant features from input data. It is also used as a nonlinear
dimensionality reduction technique [32,66,80] to transform a high d
dimensional data to a lower dimensional data. Mathematically it is
definedbythefollowingapplication:
o: Rd→Rd
x i→φ0 W01∘φ W1ðx iÞ; 8x i2Rd
Fig.2. Classificationnetwork. Whereφ andφ0 aretheencodinganddecodingfunctionsparame-
W1 W01
trizedbyW12Rd(cid:5)handW012Rh(cid:5)drespectivelyanddefinedasfollow:
φ : Rd→Rh φ0 : Rh→Rd
W1 ; W01 :
x i→a hðx iÞ a hðx iÞ→oðx iÞ
Wherea andoare,respectively,thehiddenandoutputlayersoutput
h
vectors. The parameters ðW1;W01Þ are learned by minimizing the
reconstructionerrorbetweentheinputandtheoutputofNetwork
1
Xn
L¼ kx (cid:3)φ0 ∘φ ðxÞk2:
2n i W01 W1 i 2
i¼1
After Autoencoder training, the decoding layers are removed and the
encodinglayersareretainedandthelearnedmatrixW1isthenusedas
parametersofthefirstlayer(s)ofthesupervisedNetwork(seeFig.3).
Fig.3. Autencoder
Alternatively,thecouple(W1;W01
)canbelearnedjointly(seeFig.4)with
the classification Network [22,61,92]by minimizing C , the following
T
Supposing that the couples ðx i;y iÞ, i2f0;…;1g are independent, the lossfunction
likelihoodfunctionisgiven
! γ Xn
Yn C T¼Cþ
2n
kx i(cid:3)x^ ik2 2:
P yjXÞ¼(cid:3) Pðy¼y ijX (4) i¼1
i¼1
Wherex^
i
¼φ0 W01∘φ W1ðx iÞ,X^ isamatrixwhoserowsareformedbyx^ i’s
Yn (cid:4) (cid:5) andγisatuningparameter.
¼(cid:3) aLðx iÞyi 1(cid:3)aLðx iÞ1(cid:3)yi : (5)
i¼1
3. Gradientdescent
Training the NN consists to maximize the likelihood function which is
equivalenttominimizethecrossentropylossfunctiondefinedby
Oncethelossfunctionisdefined,gradientdescentstrategyistypically
1Xn usedtominimizeit.Gradientdescentisafirst-orderoptimizationstrat-
C¼(cid:3)
n
y ilogaLðx iÞþð1(cid:3)y iÞlogð1(cid:3)aLðx iÞÞ: (6) egy for nonlinear minimization problems [17]. The loss function C is
i¼1 minimizediterativelybyusingGradientdescentmethod[3]givenby
areC mo un ts uid ale lr yn eo xw cl, ua sivm eu .l It ni- tc hla isss cacl sa es ,s (i 6fi )ca tati ko en sp thro eb fl oe rm m,wherethelabels wl ij→wl ij(cid:3)α nX ∂∂ wC lx: (8)
x2X ij
1Xn Xnc
C¼(cid:3)
n
y kðx iÞlogaL kðx iÞ: (7) Whereαisthelearningrate.Forthesakeofsimplification,weassume
i¼1 k¼1
that there are no bias terms bl or simply consider it as an additional
P j
aL kðx iÞisthesoftmaxfunctionsatisfying0(cid:4)aL kðx iÞ(cid:4)1and n k¼c 1aL kðx iÞ ¼ component of wl j. At each iteration, we have to compute partial de-
1.Intherestofthiswork,Cdenotesthelossfunctiondefinedby(3). rivativesofCxforeachtraininginputx,andthenaveragethemtoupdate
weights wl. Unfortunately, this method can be very expensive and
ij
2.2. UnsupervisedNeuralNetwork(Autoencoder) learningoccursslowlywhenthenumberoftraininginputsislarge.This
problemoflearningslownesscanbeavoidedbytheStochasticGradient
Sofar,wehavedescribedFNNinthesupervisedlearningcase.Here,
wesupposethatinputsamplesX¼fx 1;x 2;…;x ngareunlabeled,where
x i2Rd.Autoencoderisonethemostusedunsupervisedlearningalgo-
rithms [41,52,77,83,93]. An Autoencoder is a NN designed to learn an
identityfunctioninawaythattheoriginalinputcanbereconstructfrom
acompressedversion.Suchanetworkwillallowthediscoveryofamore
efficientandcompressedrepresentationoftheinputdata.Itconsistsof
twoparts,anencoderandadecoder.Encodermapsinputsamplestoa Fig.4. Classificationnetwork&autoencoder.
3H.Soumareetal. Array11(2021)100068
Descent(SGD)method. means that errors are computed backwards, hence the name back-
propagation. By writing partial derivatives, ∂Cx with respect to δl, the
∂wl j
3.1. Stochasticgradientdescent ij
gradientdescentupdatingruleisrewritten
Theideaofstochasticgradientdescent[9,10,40,89]istoestimateat αXXnh
each iteration partial derivatives for only a small randomly chosen wl ij→wl ij(cid:3)
m
δl jðxÞa il(cid:3)1ðxÞ: (12)
sampleX m¼fx 1;x 2;…;x mgcalledmini-batchandtrainwithit. x2Xm j¼1
wl ij→wl ij(cid:3) mαX ∂∂ wC lx: (9) W goh rie tr he mn sh ,i ts heth Se Gn Dum alb goer rio thf mne iu sr co on ms bin int eh de wlt ih thla by ae cr k. pT ry op pi ac ga all ty i, onin ,wDL hea rl e-
x2Xm ij
wehavetocomputethegradientofalossfunction,tobeminimizedfora
largesetofdata.Theimplementationofthisalgorithmisdoneinafew
Wethentakeanotherrandomlychosenmini-batchandtheweightpa-
steps:
rametersareupdatedonit,untilthetraininginputsareexhausted,which
iscalledanepochoftraining.Atthispoint,westartagainwithanew
epoch.Tocomputethepartialderivatives,∂Cx
ateachlayer,weapplythe
1. Provideasetoftrainingexamples
∂wl 2. Foreachexamplex:givea1ðxÞ,andperformthefollowingsteps:
ij
chainrule: ● DoaFeedforward:Forl¼2;3;…;LcomputezlðxÞ¼Wlal(cid:3)1ðxÞþ
blwithalðxÞ ¼φðzlðxÞÞ.
∂∂ wC
l
ix j¼δl jðxÞ∂∂ wzl j
l
ij¼δl jðxÞa il(cid:3)1ðxÞ: ●● BO au ct kp pu rt oe pr aro gar tf eun tc ht eion erδ rL o:C r:om Fop rute l¼δL Lðx (cid:3)Þ 1¼ ;Lr (cid:3)Cx 2(cid:6) ;…φ ;0ð 2zL cðx oÞ mÞ.
pute
δlðxÞ ¼ððWlþ1ÞTδlþ1ðxÞÞ(cid:6)φ0ðzlðxÞÞ
Whereδl j¼∂ ∂C zl jxrepresenttheerrorfunctionofjneuroninthelthlayer,for 3. Gradientdescent:Forl¼L;L P(cid:3)1;…;2updatetheweightsaccording
aninputx.Forthesakeofsimplicity,wejustwriteδl jandal i(cid:3)1insteadof totheformulaWl→Wl(cid:3) mα x2XmδlðxÞðal(cid:3)1ðxÞÞT.Wecanalsoshow
δlðxÞ and al(cid:3)1ðxÞ. This expression tells us how a little change in the with small computations that the update formula for the vector bl
j i
weightedinputtothejthneuroninlayerlchangestheoverallbehaviorof co Pntaining the bias terms in any l layer is written:bl→bl(cid:3)
thelossfunction.Thebackpropagationalgorithmisusedtocomputeδl
j
mα x2XmδlðxÞ
foreachlayer.
Toimplementstochasticgradientdescentinpractice,anexternalloop
3.2. Backpropagation generating mini training example runs, and an external loop running
through several training epochs are required. However, these were
Backpropagation[30,84,85]isawidelyusedalgorithminminimizing omittedforsimplicity.
Feedforward Neural Network loss functions. It uses the chain rule to
compute iteratively the error of each neuron in a Network, from the 4. Regularizationtechniques
outputtotheinputlayer.
Errorsattheoutputlayer:Let'sbeginbycomputingδL;i2f1;…;cg, OneofthemostseriousproblemsintrainingMLmodels,particularly
errorsofneuronsinthelastlayerL.Byusingthechainruj
le,wehave
forNN,isoverfitting.Thisproblemoccurswhenatrainingmodelistoo
complex.
δL¼
∂Cx ∂aL
j
j ∂aL
j
∂zL
j (10) 4.1. L1 andL2 regularizationtechniques
(cid:2) (cid:3) (cid:2) (cid:3)
¼ y (cid:3)aL φ0 zL
j j j A widelyused technique to reduce amodelcomplexityis to adda
BecausethelossfunctiondependsonzL,throughaLonly. regularization term [26] to the loss function C. The new model loss
j j functionCλisdefinedasfollows:
Errorsatanyhiddenlayer:errorδl ofanyhiddenneuronjatany
j
layer l. The weighted input zl of a hidden layer l is linked to the loss Cλ¼CþλΩðWÞ
j
functionthroughallweightedinputsðzlþ1Þ tothenextlayer.
k k These,updatethegeneralcostfunctionbyaddinganothertermknownas
δl¼X∂Cx ∂z klþ1 pth ae rar meg eu tela rr si .zationterm,whereΩisL1orL2normandwistheNNweight
j ∂zlþ1 ∂zl
k k j
X ∂zlþ1 4.1.1. L2 regularization
¼ δl kþ1 ∂k
zl
: TheL2regularizationterm,commonlyknownasweightdecay.The
k j idea of this technique also known as ridge regression or Tikhonov
Usingthechainrule,wehave regularization[78],istoaddaL2termtothefunctiontobeminimized,in
this case ΩðWÞ ¼ 1kWk2. This added term in L2 norm imposes the
∂zlþ1 ∂zlþ1 ∂alþ1 2 2
k ¼ k k weightstoliveinasphereofradiusinverselyproportionaltotheregu-
∂zl
j
∂al
j
∂zl
j larizationparameter[[26],p.249]λ.Inthiscontext,theupdatingrule,
(cid:2) (cid:3)
usinggradientdescentstrategybecomes
¼wlþ1φ0 zl :
(cid:2) j (cid:3)k X j wl ij→(cid:2) 1(cid:3)α nλ(cid:3) wl ij(cid:3)α nXn ∂ ∂C wx li: (13)
δl¼φ0 zl wlþ1δlþ1: (11) i¼1 ij
j j jk k
k
this means that, after each iteration, the weights are multiplied by a
Theaboveexpressiontellsusthaterrorfunctionsatanyhiddenlayerare factor slightly smaller 1. It tends to force the model to prefer small
givenbytheweightedsumoftheerrorfunctionsatthenextlayer.Which weights.
4H.Soumareetal. Array11(2021)100068
4.1.2. L1 regularization In this work, without any assumption of input data, we quantify
L1regPularizationmodifiesthelossfunctionbyaddingaL1term,i.e. explicitlythegapandthenshowhowitrelatedtoL2regularization.
ΩðWÞ ¼ jwj.Theideabehindthistechniqueis toregularizethe
w2W
loss function by removing the irrelevant features from the training 4.2.1. Dropoutapplicationtolinearnetworks
model.Inthissituation,theupdatingruleiswritten To see more clearly the relationship between L2-regularization, we
startbystudyingtheprobleminaverysimplecase,whereallactivation
wl ij→wl ij(cid:3)α nλ sgn(cid:2) wl ij(cid:3) (cid:3)α nX i¼n
1
∂ ∂C wx
l
iji: (14) (fu i.n e.ct aio ln ¼si an l(cid:3)th 1Wem l,o wd hel ea rere al lin ae na dr. WCo ln as rid ee tr ha eN oN u, tw ph ue tr ve ea cl tl ou rn sit as na dre wli en ige har
t
matrixoflayerl2f1;…;Lgrespectively).TheDropoutNNlossfunctionis
Where sgnðwlÞ is the sign of wl. Both types of regularization try to
ij ij X(cid:6) (cid:6) (cid:6) (cid:6)
penalizethebigweightswhenit'snecessarybyshrinkingthemaftereach 1 (cid:6) (cid:6)yðxÞ(cid:3)aL(cid:3)1ðxÞW~L(cid:6) (cid:6)2 þ1(cid:3)pL(cid:3)1(cid:6) (cid:6)ΣL(cid:3)1W~L(cid:6) (cid:6)2 : (16)
updatingstep,butthewayofshrinkageisdifferent[26].WhenL2 reg- n x2X 2 pL(cid:3)1 2
ularizationisused,theweightsareshrunkbyanamountproportionalto
wl ij, whereas in L1 regularization, the weights are shrunk by constant y (cid:7)ðxÞ is the output vector (cid:8) 1given an input vector x. ΣL(cid:3)1 ¼
quantitytowardtozero.AsshowninFig.5(graphontheleft),inatwo 1diagðaL(cid:3)1ðXÞðaL(cid:3)1ðXÞÞTðXÞÞ 2 ,W~L ¼pL(cid:3)1WL.GivenamatrixA,we
dimensionalspace,theL1normdefinesaparameterspaceboundedbya n
parallelogramattheorigin.Inthiscase,thelossfunctionislikelytohit denote by diagðAÞ, a diagonalmatrix withthe samesize anddiagonal
theverticesoftheparallelogramratherthanitsedges.L1regularization elementsasA.
removes some of the parameters, thus L1 technique can be used as a
Ateachlayerl,wedefineamatrixalðXÞwhosecolumnscorrespondto
feature selection technique. On the other hand, the L2 regularization thevaluestakenbythevectoroftheactivationfunctional acrossinput
definesacirclewhoseradiussizeisinverselyproportionaltotheregu- data:alðXÞ ¼ðal iðx jÞÞ,1(cid:4)i(cid:4)m,1(cid:4)j(cid:4)n,whereal iðx jÞistheithoutput
larizationparameter(seeFig.6). neuronin(l)thlayerofthejthinputandmisthenumberofneuronsin
thelayer.
4.2. Dropouttechnique Proof. Training a standard nn without dropping neurons is done by
minimizingthefollowinglossfunction:
IntrainingaNN,Dropouttechniqueregularizeslearningbydropping
X(cid:6) (cid:6)
out some hidden units with certain probability. This is equivalent to 1 (cid:6)yðxÞ(cid:3)aL(cid:3)1ðxÞWL(cid:6)2
(17)
modifying [72] the NN by setting some hidden activation functions to n x2X 2
zero.UsingDropout,wecanformallydefinetheNNasfollows:
Dropout modifies the training process and the loss function in (17)
~al¼δlal; (15) becomes
j j j
Ateachneuronj,inahiddenlayerl,theoutputactivational jismultiplied 1 nX E δL(cid:3)1(cid:6) (cid:6)yðxÞ(cid:3)(cid:4) δL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞ(cid:5) WL(cid:6) (cid:6)2 2: (18)
byasampledvariableδl,toproducethinnedoutputactivationsa~l.These x2X
j j
thinnedfunctionsarethenusedasinputstothenextlayerandthesame Where δL(cid:3)1 is a random vector of the layer L(cid:3)1 with δL(cid:3)1↪
processisappliedateachlayer.Thisapplicationisequivalenttosampling i
BernoulliðpL(cid:3)1Þand(cid:6)denotestheHadamardproduct.Usingtheformula
a subNeural Networksfrom alarger network.Whereδl is a Bernoulli
j EðX2Þ¼ðEðXÞÞ2þVarðXÞforarandomvariableX,weshowthat(18)is
randomvariable(δl j↪Bernoulli(pl))ofparameterpl,i.e.aneuroninthe
equalto
lthlayeriskeptwithaprobabilityofplandremovedwithaprobability
1 nX(cid:6) (cid:6)yðxÞ(cid:3)E δL(cid:3)1(cid:4)(cid:4) δL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞ(cid:5) WL(cid:5)(cid:6) (cid:6)2 2þ1 nX Var(cid:4)(cid:4) δL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞ(cid:5) WL¼
x2X x2X
1X(cid:6) (cid:6)yðxÞ(cid:3)pL(cid:3)1aL(cid:3)1ðxÞWL(cid:6) (cid:6)2þ1X WLVar(cid:4) δL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞ(cid:5)
ðWLÞT:
n 2 n
x2X x2X
1(cid:3)pl.
Srivastavaetal.[72]suggestedthat,applyingDropouttoaNNwithn
unitscanbeseenassampling2nsubNetworkswithweightsharing.Inthe
testphase,asitisnotalwayspracticaltotakethemeanof2nmodels,an
approximateaveragingmethodisused.Theideaistoapproximatethe
exponentiallymanyNetworksbyasingleNNwithoutDropout.Tocorrect
the fact that training outgoing weights of a layer are obtained under
conditionthatneuronswereretainedwithaprobabilityp,theweights
aresimplymultipliedbyp.Thisapproximationhasbeenprovedforlo-
gistic and linear regression models [72,79]. But, for Deep Neural Net-
worksDNNs, there is an unknown gap between the expected output of
exponentialsubNetworksandtheoutputofasingledeterministicmodel.
Maetal.[46]showedthatundersomeassumptionsoninputdata,the
gapiscontrolledanditcanbeusedtoregularizethesingleNN.
Fig.5. TwodimensionalgraphicalinterpretationofL1andL2regularizations.
5H.Soumareetal. Array11(2021)100068
whichtheweightsarescaledbypL(cid:3)1tocompensatethefactthattheyare
learned under conditions in which 1(cid:3)pL(cid:3)1 of hidden units where
dropped out. In this case, Dropout training model can be seen as an
L2-regularizationwhere,theregularizerλdependson:Dropoutrate;the
varianceofeachinputandoutputlayer.
4.2.3. Dropoutwithothersregularizationtechniques
Fig.6. Dropout.
(cid:7) (cid:8) Dropoutisknowntoimprovetrainingmodelperformancewhenitis
As VarðδL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞÞ ¼ 1(cid:3)pL(cid:3)1 aL(cid:3)1ðxÞaL(cid:3)1ðxÞT, we obtain the combined with other regularization techniques. Batch normalization,
pL(cid:3)1 introducedbyRef.[35],isaregularizationtechniqueusedtospeedup
desired result. Under the assumption that input layers follow a thetrainingandimproveperformanceofDeepNNs.Inthetrainingofa
Gaussiandistributionwithstandarddeviationσ,Dropoutisequivalentin DNN,thedistributionofeachlayer'sinputschange,astheparametersofall
expectation to L2-regularization. The regularization parameter λ is a layersthatcomebeforeit,variate.Thiscanslowdownbyrequiringsmall
functionof1(cid:3)pL(cid:3)1σ2whichincreases(resp.decreases)withthevarianceof learning rates and careful parameter initialization. Given a batch of
pL(cid:3)1
inputlayersσ2(resp.withpL(cid:3)1).Thus,Dropoutregularizationconsistsof sampleusedtoupdateparameters,batchnormalizationnormalizesthe
inputsofeachlayerbyrecenteringandrescaling(subtractingthemean
detectingtheinputswithmorevarianceandshrinktheirweights.
anddividingbythebatchstandarddeviation).Thus,batchnormalization
prevents layers inputs to have large standard deviations [35]. show
4.2.2. Dropoutapplicationtononlinearnetworks experimentallythatbatchnormalizationwithlargelearningrate,speed
Here, we try to generalize the relationship between Dropout and upsignificantlytrainingasitcaneliminatetheneedforDropout.Infact,
L2-regularizationtoNetworkswithnonlinearunits.ConsideraNNwitha as discussed in Section ??, Dropout look for layer's inputs with more
non linear activation function, i.e., al ¼ φðal(cid:3)1WLÞ. Dropout training varitions and shrink their weights, this function of shrinking is then
expectedlossfunctionisgivenby largelyreducedbybatchnormalizationapplication.Combiningdropout
1 nX(cid:6) (cid:6) (cid:6) (cid:6)yðxÞ(cid:3)φ(cid:7)(cid:2) W~L(cid:3) T aL(cid:3)1ðxÞ(cid:8)(cid:6) (cid:6) (cid:6) (cid:6)2 þ 21 n(cid:7) 1(cid:3) pp(cid:8) X kφ00(cid:2) aL(cid:3)1ðxÞW~ L(cid:3) ΣL x(cid:3)1W~ Lk2 2: (19)
x2X 2 x2X
(cid:2) (cid:3)
WhereΣL x(cid:3)1¼ aL(cid:3)1ðxÞðaL(cid:3)1ðxÞÞT 1 2andW~L ¼pL(cid:3)1WL. rw ai ct yh .b Da rt oc ph on uo tr rm ega uli lz aa rit zio an tio[ n25 is,3 k5 n,4 o3 w] nc ta on gi im vepr ao sv ie gnD iN fiN cs ap nr te id mic pt ri oo vn ea mc ec nu t-
when it is combined with others regularization methods such as
Proof. We know that a non linear Dropout Network training loss is
definedas max-norm[72]andweightnormalization[67].Ratherthanconstraining
wholeweightmatrixofeachlayerasinL2regularization,theseconstrain
1 nX E δL(cid:3)1(cid:6) (cid:6)yðxÞ(cid:3)φ(cid:4)(cid:4) δL(cid:3)1ðxÞ(cid:6)aL(cid:3)1ðxÞ(cid:5) WL(cid:5)(cid:6) (cid:6)2 2: (20) e na ec uh roc no flu rom mn ho af vit nh ge vw ee ryigh lat rgm ea wtr eix ighto ts.pr Mev ae xn -nt os re mpa rr ea gt ue ll ay ria zn ay tioh nid cd oe nn
-
x2X
siststoconstraintheincomingweightvectorofeachhiddenneuronto
Usingtriangleinequality,(20)isboundedby liveinaballofradiusc,wherecisahyper-parameter.Weightnormal-
1 nX(cid:6) (cid:6)yðxÞ(cid:3)φ(cid:4) pL(cid:3)1aL(cid:3)1ðxÞWL(cid:5)(cid:6) (cid:6)2 2þ1 nX(cid:6) (cid:6)E δL(cid:3)1ðxÞ(cid:4) φ(cid:4)(cid:4) aL(cid:3)1ðxÞ(cid:6)δL(cid:3)1ðxÞ(cid:5) WL(cid:5)(cid:5) (cid:3)φ(cid:4) pL(cid:3)1aL(cid:3)1ðxÞWL(cid:5)(cid:6) (cid:6)2 2:
x2X x2X
(cid:4)(cid:4) Now,byapplyin (cid:5)gasecondorderTaylorexpansionofφaroundE δL(cid:3)1 izationconstrainsincomingweightvectorstohaveunitnorm.
aL(cid:3)1(cid:6)δL(cid:3)1ðxÞWLÞ ¼pL(cid:3)1aL(cid:3)1ðxÞWL and by posing Z ¼ ðaL(cid:3)1ðxÞ(cid:6)
δL(cid:3)1ðxÞÞWL(cid:3) pL(cid:3)1aL(cid:3)1ðxÞWL, we have φððaL(cid:3)1ðxÞ(cid:6)δL(cid:3)1ðxÞÞWLÞ ¼ 5. Materialsandmethods
φðpL(cid:3)1aL(cid:3)1ðxÞWLÞþφ0ðpL(cid:3)1aL(cid:3)1ðxÞWLÞZþ1φ00ðpL(cid:3)1aL(cid:3)1ðxÞWLÞZZT.
(cid:4) (cid:5)2 AllmodelsinthisworkareconstructedusingKerasandTensorflow
Then E δL(cid:3)1ðxÞ φððaL(cid:3)1ðxÞ(cid:6)δL(cid:3)1ðxÞÞWLÞ (cid:3) φðpL(cid:3)1aL(cid:3)1ðxÞWLÞ ¼ 1 2φ00 opensourcelibraries[38].LogisticregressionisbuiltusingaFeedfor-
ðpL(cid:3)1aL(cid:3)1ðxÞWLÞVarðZZTÞ. Because Z is centered i.e., E δL(cid:3)1ðxÞðZÞ ¼ 0. ward classification network without hidden layers, this model can be
Thus,anupperboundof(20)isgivenby extendedlatertoamorecomplexclassificationNetwork,dependingon
X(cid:6) (cid:2) (cid:3)(cid:6) (cid:7) (cid:8) X (cid:2) (cid:3)
1
n
x2X(cid:6) (cid:6)yðxÞ(cid:3)φ aL(cid:3)1ðxÞW~ L (cid:6) (cid:6)2 2þ 21
n
1(cid:3) pLp (cid:3)1L(cid:3)1 x2Xkφ00 aL(cid:3)1ðxÞW~ L Σ xW~ Lk2 2:
T Da ifb fele re1
ntdatasetsusedinthisstudy.
Hereagain,Dropoutcanbeseenasregularizer,wheretheregularizer
Dataset #ofsamples Class1 Class2 #ofSNPS
(cid:4) (cid:5)
represents the gap between E δL(cid:3)1 φððaL(cid:3)1(cid:6)δL(cid:3)1ÞWLÞ the expected Breast-Kidney 604 344 260 10937
Colon-Kidney 546 286 260 10937
outputofexponentialthinnedNetworksproducedbyapplyingDropout
Breast-Colon 630 344 286 10937
and φðpL(cid:3)1aL(cid:3)1WLÞ, the output of a single deterministic Network, in Colon-Prostate 286 69 355 10937
6H.Soumareetal. Array11(2021)100068
the problem complexity. Stochastic gradient descent is adopted in all Table2
experiments as an optimization strategy. Two types of datasets are Unregularizedlogisticreg.
included in our experiments, Expression Project for Oncology (expO)
Dataset Accuracy(in%)
cancerdatasetsand1000GenomesProjectethnicitydatasetsrespectively
Breast-Kidney 96.53
usedfortraininglogisticregressionandFFNmodels.
Colon-Kidney 97.82
Individualsinselecteddatasetsarehumansthatarerepresentedby Breast-Colon 94.13
thelistoftheirSNPS.EachSNPisrepresentedbyitsgenotype(i.e.genetic Colon-Prostate 97.46
information)ataspecificlocus.Inadiploidorganismateachlocus,there
aretwocopiesofalleles,onecomesfromthefatherandotherfromthe
mother.Consequently,agenotypetakesoneofthreevaluesforadiploid Table3
organism:0(homozygousreference),1(heterozygous)and2(homozy- Logisticreg.withL1norm.
gous alternate). The homozygous reference refers to the base that is Dataset RegularizationL1 Accuracy(in%)
foundinthereferencegenome,anhomozygousalternatereferstoany
Breast-Kidney λ¼10(cid:3)2 97.36
base,otherthanthereference,thatisfoundatthatlocusandgenotypeis
λ¼10(cid:3)3 99.01
saidheterozygousatagivenposition,whenthetwoallelesaredifferent. Colon-Kidney λ¼10(cid:3)2 95.82
TheinputofamodelisamatrixXofsizen(cid:5)d,wherenisthenumber λ¼10(cid:3)3 97.45
ofindividualsincludedinthestudyanddcorrespondtothenumberof Breast-Colon λ¼10(cid:3)2 94.44
features(SNPS).Theoutputytakesdiscretevalue(s)between0and1. 10(cid:3)3 93.17
Colon-Prostate λ¼10(cid:3)2 98.59
λ¼10(cid:3)3 98.03
5.1. ExpressionProjectforOncology(expO)cancerdatasets
Table4
Thedifferentcancersamplesincludedinthisstudy(seeTable1),are
Logisticreg.withL2norm.
downloadedfromRef.[11].Theoriginaldatasetscanbeobtainedfrom
theExpressionProjectforOncology(expO)thatwasdepositedatGene Dataset RegularizationL2 Accuracy(in%)
Expression Omnibus (GEO) repository [7], with accession number Breast-Kidney λ¼10(cid:3)2 98.18
GSE2109.TheobjectiveofexpOistoobtainandperformgeneexpression λ¼10(cid:3)3 98.02
analysisoncancertissuesamplesandassemblethepatient'slongterm Colon-Kidney λ¼10(cid:3)2 98.18
clinicalresults. λ¼10(cid:3)3 98.91
Breast-Colon λ¼10(cid:3)2 92.86
λ¼10(cid:3)3 99.44
Colon-Prostate λ¼10(cid:3)2 97.18
5.2. 1000GenomesProjectdataset
λ¼10(cid:3)3 96.62
The1000GenomesProject[16]tookadvantageofdevelopmentsin
Next-generationsequencing(NGS),whichallowstosequenceDNAandRNA Table5
much more quickly and cheaply. It's the first project to sequence the MLPaccuracyvsitssize.
genomesofalargenumberofpeopleinpopulationsfromdifferentre-
#ofunitsbyhiddenlayer Accuracy(in%)
gionsandcountries.Inthisstudy,n¼3450isthenumberofindividuals
[50] 81.33
sampledworldwidefrom26populationsandd¼315345isthenumber
[50-50] 81.68
of SNPS. The desired output of the model is a vector Y2Rc, whose
[100] 90.68
componentscorrespondtothe26classesofpopulations(i.e.c ¼26).The [100(cid:3)100] 92.70
modelconsistsofaninputlayer,anoutputlayerandtwohiddenlayersof [100-100-100] 90.49
equalsize.GiventheinputmatrixX,themodeloutputisavectora32Rc. [500-500-500] 90.46
A reluaction function is used in the two hidden layers followed by a
sotmaxlayertoperformancestryprediction.
6.2. Ancestrypredictionusingamultilayerperceptron(MLP)
6. Experiments
In this subsection, FNN is used on 1000 Genome Project ethnicity
Inthissection,wepresenttheeffectsofregularizationtechniqueson datasettopredictindividualsancestries.Asintheprecedingsubsection,
trainingmodelsfordifferentdatasets(seeTable1). westartwithasimplelogisticregressionmodel,whichgivesalowpre-
dictionaccuracyof54.64%.Toachievebetterpredictionresults,aMLP
with equal hidden units is constructed and the obtained results by
6.1. Cancerdatasetclassificationusinglogisticregression varyingthemodelcomplexityarereportedinTable5.Asexpected,the
modelpredictionaccuracystartsbyincreasingwithitscomplexityuntil
Un-regularized logistic regression results are reported in Table 2, somelevel(twohiddenlayerswith100unitsforeachone),thenitstarts
despiteitssimplicity,logisticregressionmodelgivesgoodclassification todrop.Becausebeyondthisstage,thetrainingmodelisconsideredtoo
accuracyonthesecancerdatasets.Toimprovepredictioncapacitiesof complexanditoverfits.
the present model, a penalty term is added and obtained results are
presentedinTable3andTable4forL1andL2regularizationaddedterm, 6.2.1. Classificationwithautoencoder
respectively.Wecanobservefromthesestablethatpenalizationwiththe WestartbyusingaclassificationNetworkwithonehiddenlayerof50
appropriate regularization parameter improves the classification accu- unitswithreconstructionpath.Thisgivesanaccuracyof84:85%.When
racy.Forexample,whenL1 regularizationisusedwithregularizerλ ¼ anotherhiddenlayerof50neuronsisaddedbetweenhiddentherepre-
10(cid:3)3, the classification accuracy on Breast-Kidney dataset goes from sentation a h and the output layer aL, as described in Fig. 4(where
96:53to99:01.Similarly,whenL2penalizationisapplied(λ ¼10(cid:3)3),the MPL¼½50(cid:7)).Thislastgivesanaccuracyof85:36%.Trainingtheclassifi-
prediction accuracy on Breast-Colon dataset increases from 94.44 in cation Network with a reconstruction path is difficult due to the high
unregularizedcaseto99:44. dimensionalityoftheinputdata.
7H.Soumareetal. Array11(2021)100068
Table6
PredictionaccuracyforDropout,batchnormalizationanddropoutcombinationwithbatchnormalization.
#ofuni.byhid.layer Drop.(p¼0.2) Drop.(p¼0.5) Bat.norm Drop.(p¼0.2)þBat.norm Drop.(p¼0.5)þBat.norm
(accuracyin%) (accuracyin%) (accuracyin%) (accuracyin%) (accuracyin%)
[50] 90.32 87.54 90.58 91.48 92.61
[50-50] 89.94 40.96 91.01 92.58 92.93
[100] 88.81 92.26 90.75 91.65 93.01
[100(cid:3)100] 90.32 64.12 89.19 92.46 93.00
Table7
PredictionaccuracyforL1,L2,DropoutregularizationtechniquesandDropoutcombinationwithothersregularizationtechniques.
#ofuni.byhid.layer L1(λ¼10(cid:3)4)reg. L2(λ¼10(cid:3)3)reg. Drop.(p¼0.5)þBat.norm Drop.(p¼0.5)þMa.norm Drop.(p¼0.2)þUn.norm
(accuracyin%) (accuracyin%) (accuracyin%) (accuracyin%) (accuracyin%)
[50] 92.13 92.61 92.61 92.35 93.25
[50-50] 91.77 90.75 92.93 83.68 90.32
[100] 92.70 92.70 93.01 93.83 94.43
[100(cid:3)100] 92.29 93.39 93.00 90.17 91.94
[100-100-100] 90.87 91.01 92.42 89.68 92.19
6.2.2. Classificationwithregularization that Dropout is less effective than L1 and L2 regularization techniques
WhenDropoutisusedalone,thechoiceofitsratepisveryimportant [58] when the training model is not complex (as for our model).
asinTable6,wehavetobemorecarefulaboutit.CombiningDropout CombiningDropoutwithtechniquessuchasBatchnormation,Unitnorm
withbatchnormalizationimprovestheperformancetrainingmodeland constraintorMaxnormenabledthetrainingmodeltoachieveitsbest
makesthechoiceofplessimportant.L1andL2regularization,givegood accuracy. The obtained results are compared to the results in Table 9
prediction accuracy and outperform Dropout as observed in Table 7. obtained by Ref. [64] on the same dataset. In Ref. [64], the authors
However, when Dropout is combined with batch normalization, max- proposedauxiliaryNNstopredicttheparametersofthefirsthiddenlayer
norm and unit-norm outperforms the traditional regularization tech- of the classification NNs and different features embedding techniques
niques.Wehaveobtainedourbestpredictionaccuracy94.43%,when suchasRandomprojection(RP),Perclasshistogram,andSNPtoVechave
Dropoutiscombinedwithunitnormconstraint. alsobeenproposed.Achievingsuchpredictionaccuracyobtainedwith
SNP data, these regularization techniques will allow us to face more
7. Discussion complicatedproblemsinmanydomainssuchaspreventivemedicine.
RegularizedLogisticRegressionhasachievedgoodresultscompared 8. Conclusion
to previous machine learning approaches tested on the same cancer
samples [70,73]. For instance, Stiglic et al. [73] combined different Inthiswork,wehaveexplainedstochasticgradientdescentoptimi-
feature selection methods such as Vector Machines Recursive Feature zation technique with back-propagation in training DL algorithms. To
Elimination (SVM-RFE) and ReliefF followed by SVM or k-nearest preventoverfittingproblem,regularizationtechniquesarestudiedand,
neighbors.Tothebestofourknowledge,thebestresultsonthesedata- theoretical relationship between Dropout and L2 regularization is
setswerereportedinRef.[70],wheretheauthorsusedStackedSparse established. Experimental results have shown that Dropout,when it is
Autoencoders (SSAE) to select most relevant features followed by a combined with techniques such as batch normalization, max-norm or
classification Neural Network to categorize the samples. Despite its unit-norm gives better performance than L1 and L2 regularization
simplicity,theproposedapproachoutperformsSSAEondatasetssuchas techniques.
Breast-Kidney and Breast-Colon (see Table 8). In Stacked Sparse For future work, we expect to further study these regularization
Autoencoders[36,45]manyAutoencoderlayersarestackedtogetherto techniquesinDNNandusethemtoanalyzegeneexpressionprofiledata
form an unsupervised learning algorithm, where the encoder layer withtheaimofpredictingrarediseases.
computed by an Autoencoder will be used as the input to another
Autoencoderlayer.Inpractice,alogisticregressionmodelmaybebetter Declarationofcompetinginterest
than a NN for relatively small data sets and simple classification tasks,
wheretheclassesaremoreorlesslinearlyseparable.Indeed,thelatter The authors declare that they have no known competing financial
aremoredifficulttotrain,requiremoretrainingsamplesandaremore interestsorpersonalrelationshipsthatcouldhaveappearedtoinfluence
pronetooverfittingthanlogisticregression. theworkreportedinthispaper.
Logisticregressionapplicationtoanscentrypredictiondatasetleadto
poorpredictionaccuracy,whichisduetothelargedimensionofinput Acknowledgments
featuresandhighnonlinearcorrelationbetweenthem,andtothegenetic
similaritybetweensomeethnicofgroupsofpopulations. ThisprojectwaspartlyfundedbyH3ABioNet,whichissupportedby
TrainingaNNwithanAutoencoderreconstructionpathimprovedthe the National Institutes of Health Common Fund under grant number
results.However,traininganAutoencoderinconjunctionwiththeclas- U41HG006941.
sification Network makes the high dimensional optimization problem
moredifficulttosolvethansimplytrainingtheclassificationNetwork, Appendix
yieldinginahigherclassificationerror.Toimprovetheresults,regula-
rizationtechniquesareused.Onecannoticethattraditionalregulariza- Inthissection,wereportresultsobtainedbySinghetal.[70](Table8)
tiontechnique'sapplicationhasmoreimprovedthepredictionaccuracy andthoseobtainedbyRomeroetal.[64](Table9).
ofthemodelcomparedtoDropout.Thiscouldbeattributedtothefact
8H.Soumareetal. Array11(2021)100068
Table8 [19] DreiseitlS,Ohno-MachadoL.Logisticregressionandartificialneuralnetwork
Logisticreg.vsSSAE.
classificationmodels:amethodologyreview.JBiomedInf2002;35:352–9.
[20] FakoorR,LadhakF,NaziZ,HuberM.Usingdeeplearningtoenhancecancer
Dataset StackedSparseAutoencoders Logisticregression diagnosisandclassification.In:Proceed.oftheinter.conf.onML.NewYork,USA:
ACM;2013.https://doi.org/10.1109/ICSCAN.2018.8541142.
Breast-Kidney 98.4 99.01 [21] FortG,Lambert-LacroixS.Classificationusingpartialleastsquareswithpenalized
Colon-Kidney 99.5 98.91 logisticregression.Bioinformatics2005;21:1104–11.https://doi.org/10.1093/
Breast-Colon 97.3 99.44 bioinformatics/bti114.
Colon-Prostate 99.7 98.59 [22] FuX,WeiY,XuF,WangT,LuY,LiJ,HuangJZ.Semi-supervisedaspect-level
sentimentclassificationmodelbasedonvariationalautoencoder.KnowlBaseSyst
2019;171:81–92.
[23] GalY,GhahramaniZ.Dropoutasabayesianapproximation:representingmodel
uncertaintyindeeplearning.In:Internationalconferenceonmachinelearning.
Table9 PMLR;2016.p.1050–9.
ReportedresultsinRef.[64]. [24] GanesanN,VenkateshK,RamaMA,PalaniAM.Applicationofneuralnetworksin
diagnosingcancerdiseaseusingdemographicdata.IntJChemAppl2010;1:76–85.
Model&Embedding MeanMisclassif.Error. #offree https://doi.org/10.5120/476-783.
(%) param. [25] GarbinC,ZhuX,MarquesO.Dropoutvs.batchnormalization:anempiricalstudyof
Basic 8:31(cid:8)1:83 31.5M theirimpacttodeeplearning.MultimedToolAppl2020:1–39.https://doi.org/
Rawend2end 8:88(cid:8)1:41 21.27K 10.1007/s11042-019-08453-9.
RandomProjection 9:03(cid:8)1:20 10.1K [26] GoodfellowI,BengioY,CourvilleA,BengioY.Deeplearning,vol.1.MITpress
Cambridge;2016.https://doi.org/10.1007/s10710-017-9314-z.
SNP2Vec 7:60(cid:8)1:28 10.1K
[27] Group,I.S.M.W.,etal..Amapofhumangenomesequencevariationcontaining1.42
Perclasshistograms 7:88(cid:8)1:40 7.9K millionsinglenucleotidepolymorphisms.Nature2001;409:928.https://doi.org/
Basicwithreconstruction 7:76(cid:8)1:38 63M 10.1038/35057149.
Rawend2endwithreconstruction 8:28(cid:8)1:92 227.3K [28] GuoJ,HeH,HeT,LausenL,LiM,LinH,ShiX,WangC,XieJ,ZhaS,etal.Gluoncv
RandomProjectionwith 8:03(cid:8)1:0:3 20.2K andgluonnlp:deeplearningincomputervisionandnaturallanguageprocessing.
reconstruction JMachLearnRes2020;21:1–7.1907.04433.
SNP2Vecwithreconstruction 7:88(cid:8)0:72 20.2K [29] HannunA,CaseC,CasperJ,CatanzaroB,DiamosG,ElsenE,PrengerR,SatheeshS,
Perclasshistogramswith 7:44(cid:8)0:45 15.8K SenguptaS,CoatesA,etal.Deepspeech:scalingupend-to-endspeechrecognition.
reconstruction arXivpreprintarXiv,1412.5567;2014.https://arxiv.org/abs/1412.5567.
[30] Hecht-NielsenR.Theoryofthebackpropagationneuralnetwork.In:N.netw.for
percep.Elsevier;1992.p.65–93.https://doi.org/10.1016/B978-0-12-741252-
8.50010-8.
References
[31] HelmboldDP,LongPM.Surprisingpropertiesofdropoutindeepnetworks.In:
Conferenceonlearningtheory.PMLR;2017.p.1123–46.
[1] AkhtarN,MianA.Threatofadversarialattacksondeeplearningincomputer [32] HintonGE,SalakhutdinovRR.Reducingthedimensionalityofdatawithneural
vision:asurvey.IEEEAccess2018;6:14410–30.https://doi.org/10.1109/ networks.science2006;313:504–7.https://doi.org/10.1126/science.1127647.
ACCESS.2018.2807385. [33] HintonGE,SrivastavaN,KrizhevskyA,SutskeverI,SalakhutdinovRR.Improving
[2] AlmagroArmenterosJJ,SønderbyCK,SønderbySK,NielsenH,WintherO.Deeploc: neuralnetworksbypreventingco-adaptationoffeaturedetectors.2012.arXiv
predictionofproteinsubcellularlocalizationusingdeeplearning.Bioinformatics preprintarXiv,1207.0580.http://arxiv.org/abs/1207.0580.
2017;33:3387–95.https://doi.org/10.1093/bioinformatics/btx431. [34] HuangK,HussainA,WangQF,ZhangR.Deeplearning:fundamentals,theoryand
[3] AmariS.Backpropagationandstochasticgradientdescentmethod. applications,vol.2.Springer;2019.
Neurocomputing1993;5:185–96.https://doi.org/10.1016/0925-2312(93)90006- [35] IoffeS,SzegedyC.Batchnormalization:acceleratingdeepnetworktrainingby
O. reducinginternalcovariateshift.arXivpreprintarXiv,1502.03167;2015.
[4] AmodeiD,AnanthanarayananS,AnubhaiR,BaiJ,BattenbergE,CaseC,CasperJ, [36] KatuwalR,SuganthanPN.Stackedautoencoderbaseddeeprandomvector
CatanzaroB,ChengQ,ChenG,etal.Deepspeech2:end-to-endspeechrecognition functionallinkneuralnetworkforclassification.ApplSoftComput2019;85:
inEnglishandMandarin.In:Inter.conf.onML;2016.p.173–82.https://arxiv 105854.
.org/abs/1512.02595. [37] KendallA,GalY.Whatuncertaintiesdoweneedinbayesiandeeplearningfor
[5] BacchiS,Oakden-RaynerL,ZernerT,KleinigT,PatelS,JannesJ.Deeplearning computervision?.In:NIPS;2017.p.5574–84.https://arxiv.org/pdf/1703.04977.
naturallanguageprocessingsuccessfullypredictsthecerebrovascularcauseof pdf.
transientischemicattack-likepresentations.Stroke2019;50:758–60.https:// [38] KerasT.Keras.2015.https://www.tensorflow.org/guide/keras/overview.
doi.org/10.1161/STROKEAHA.118.024124. [Accessed7September2020].
[6] BaldiP,SadowskiPJ.Understandingdropout.In:NIPS;2013.p.2814–22.https:// [39] KleinbaumDG,DietzK,GailM,KleinM,KleinM.Logisticregression.Springer;
doi.org/10.1016/j.artint.2014.02.004. 2002.
[7] BarrettT,EdgarR.[19]geneexpressionomnibus:microarraydatastorage, [40] Kone(cid:5)cnỳJ,Richt(cid:1)arikP.Semi-stochasticgradientdescentmethods.2013.arXiv
submission,retrieval,andanalysis.Meths.inenzy.2006;411:352–69.doi: preprintarXiv,1312.1666.
S0076687906110198. [41] LeL,PattersonA,WhiteM.Supervisedautoencoders:improvinggeneralization
[8] BilenM,Is¸ikAH,Yig(cid:4)itT.Ahybridartificialneuralnetwork-geneticalgorithm performancewithunsupervisedregularizers.AdvNeuralInfProcessSyst2018;31:
approachforclassificationofmicroarraydata.In:201523ndSPCAconf.(SIU), 107–17.
IEEE;2015.p.339–42.https://doi.org/10.1109/SIU.2015.7129828. [42] LiF,ZuradaJM,LiuY,WuW.Inputlayerregularizationofmultilayerfeedforward
[9] BottouL.Stochasticgradientlearninginneuralnetworks.ProceedingsofNeuro- neuralnetworks.IEEEAccess2017;5:10979–85.
Nımes1991;91:12. [43] LiX,ChenS,HuX,YangJ.Understandingthedisharmonybetweendropoutand
[10] BottouL.Stochasticgradientdescenttricks.In:Neuralnetworks:tricksofthetrade. batchnormalizationbyvarianceshift.In:Proceed.OftheIEEEconf.OnCVPR;
Springer;2012.p.421–36. 2019.p.2682–90.https://doi.org/10.1109/CVPR.2019.00279.
[11] CancerD.Openml;2015.https://www.openml.org/search?type¼data.[Accessed7 [44] LiaoJG,ChinKV.Logisticregressionfordiseaseclassificationusingmicroarray
September2020]. data:modelselectioninalargepandsmallncase.Bioinformatics2007;23:
[12] ChatterjeeS,HadiAS.Sensitivityanalysisinlinearregression,vol.327.JohnWiley 1945–51.https://doi.org/10.1093/bioinformatics/btm287.
&Sons;2009. [45] LiuG,BaoH,HanB.Astackedautoencoder-baseddeepneuralnetworkfor
[13] ChenY,LiY,NarayanR,SubramanianA,XieX.Geneexpressioninferencewith achievinggearboxfaultdiagnosis.MathematicalProblemsinEngineering2018;
deeplearning.Bioinformatics2016;32:1832–9.https://doi.org/10.1093/ 2018.
bioinformatics/btw074. [46] MaX,GaoY,HuZ,YuY,DengZ,HovyE.Dropoutwithexpectation-linear
[14] CollinsFS,BrooksLD,ChakravartiA.Adnapolymorphismdiscoveryresourcefor regularization.2016.arXivpreprintarXiv,1609.08017.https://arxiv.org/abs/1
researchonhumangeneticvariation.Geno.research1998;8:1229–31.https:// 609.08017.
doi.org/10.1101/gr.8.12.1229. [47] ManningCD,SurdeanuM,BauerJ,FinkelJR,BethardS,McCloskyD.Thestanford
[15] CollobertR,WestonJ.Aunifiedarchitecturefornaturallanguageprocessing:deep corenlpnaturallanguageprocessingtoolkit.In:Proceed.of52ndann.meet.ofACL:
neuralnetworkswithmultitasklearning.In:Proceed.ofthe25thinter.conf.onML; systemdemonstrations;2014.p.55–60.https://doi.org/10.3115/v1/p14-5010.
2008.p.160–7.https://doi.org/10.1145/1390156.1390177. [48] MauryaS,SinghV,DixitS,VermaNK,SalourA,LiuJ.Fusionoflow-levelfeatures
[16] ConsortiumGP,etal.Amapofhumangenomevariationfrompopulation-scale withstackedautoencoderforconditionbasedmonitoringofmachines.In:2018
sequencing.Nature2010;467:1061.https://doi.org/10.1038/nature09534. IEEEinternationalconferenceonprognosticsandhealthmanagement(ICPHM),
[17] CurryHB.Themethodofsteepestdescentfornon-linearminimizationproblems. IEEE;2018.p.1–8.
Quart.ofApp.Maths.1944;2:258–61.https://www.ams.org/journals/qam/1944-0 [49] MianjyP,AroraR.Ondropoutandnuclearnormregularization.In:International
2-03/S0033-569X-1944-10667-3/S0033-569X-1944-10667-3.pdf. conferenceonmachinelearning.PMLR;2019.p.4575–84.https://arxiv.org/abs/1
[18] DenoeuxT.Logisticregression,neuralnetworksanddempster–shafertheory:anew 905.11887.
perspective.KnowlBaseSyst2019;176:54–67. [50] MinS,LeeB,YoonS.Deeplearninginbioinformatics.Brief.inbioinfo.2017;18:
851–69.https://doi.org/10.1093/bib/bbw068.
9H.Soumareetal. Array11(2021)100068
[51] MontgomeryDC,PeckEA,ViningGG.Introductiontolinearregressionanalysis. [73] StiglicG,KokolP.Stabilityofrankedgenelistsinlargemicroarrayanalysisstudies.
JohnWiley&Sons;2021. BioMedResInt2010;2010.https://doi.org/10.1155/2010/616358.
[52] NgA,etal.Sparseautoencoder.CS294ALect.notes72.2011.p.1–19.http [74] SutradharR,BarberaL.Comparinganartificialneuralnetworktologistic
://ailab.chonbuk.ac.kr/seminar_board/pds1_files/sparseAutoencoder.pdf. regressionforpredictingedvisitriskamongpatientswithcancer:apopulation-
[53] OwenAB.Arobusthybridoflassoandridgeregression.ContempMath2007;443: basedcohortstudy.JPainSymptomManag2020;60:1–9.
59–72.https://doi.org/10.1090/conm/443/08555. [75] SuzukiT.Generalizationboundofgloballyoptimalnon-convexneuralnetwork
[54] OzanichE,GerstoftP,NiuH.Afeedforwardneuralnetworkfordirection-of-arrival training:transportationmapestimationbyinfinitedimensionalLangevindynamics.
estimation.JAcoustSocAm2020;147:2035–48. 2020.arXivpreprintarXiv,2007.05824.
[55] PalA,LaneC,VidalR,HaeffeleBD.Ontheregularizationpropertiesofstructured [76] SvozilD,KvasnickaV,PospichalJ.Introductiontomulti-layerfeed-forwardneural
dropout.In:ProceedingsoftheIEEE/CVFconferenceoncomputervisionand networks.ChemometrIntellLabSyst1997;39:43–62.
patternrecognition;2020.p.7671–9.https://arxiv.org/abs/1910.14186. [77] TianY,LuC,ZhangX,TanKC,JinY.Solvinglarge-scalemultiobjective
[56] PatelP,ThakkarA.Theupsurgeofdeeplearningforcomputervisionapplications. optimizationproblemswithsparseoptimalsolutionsviaunsupervisedneural
IntJElectrComputEng2020;10:538.https://doi.org/10.11591/ networks.IEEEtransactionsoncybernetics2020.
ijece.v10i1.pp538-548. [78] TikhonovAN.Onthestabilityofinverseproblems.In:Dokl.Akad.NaukSSSR;
[57] PeiJ,WangW,OsmanMK,GanX.Multiparameteroptimizationforthenonlinear 1943.p.195–8.https://doi.org/10.1007/978-3-642-81472-3_5.
performanceimprovementofcentrifugalpumpsusingamultilayerneuralnetwork. [79] WagerS,WangS,LiangSP.Dropouttrainingasadaptiveregularization.In:NIPS;
JMechSciTechnol2019;33:2681–91. 2013.p.351–9.https://arxiv.org/pdf/1307.1493.pdf.
[58] PhaisangittisagulE.Ananalysisoftheregularizationbetweenl2anddropoutin [80] WangY,YaoH,ZhaoS.Auto-encoderbaseddimensionalityreduction.
singlehiddenlayerneuralnetwork.In:20167thinternationalconferenceon Neurocomputing2016;184:232–42.https://doi.org/10.1016/
intelligentsystems,modellingandsimulation(ISMS).IEEE;2016.p.174–9. j.neucom.2015.08.104.
[59] Project,G.,.1000Genomeprojectdatasets. [81] WeiC,KakadeS,MaT.Theimplicitandexplicitregularizationeffectsofdropout.
[60] projectOE.Expressionprojectforoncology.2005.https://www.ncbi.nlm.nih In:Internationalconferenceonmachinelearning.PMLR;2020.p.10181–92.htt
.gov/geo/query/acc.cgi?acc¼GSE2109.[Accessed7September2020]. ps://arxiv.org/abs/2002.12915.
[61] QiGJ,ZhangL,LinF,WangX.Learninggeneralizedtransformationequivariant [82] WeisbergS.Appliedlinearregression,ume528.JohnWiley&Sons;2005.
representationsviaautoencodingtransformations.IEEETransactionsonPattern [83] WenT,ZhangZ.Deepconvolutionneuralnetworkandautoencoders-based
AnalysisandMachineIntelligence;2020. unsupervisedfeaturelearningofeegsignals.IEEEAccess2018;6:25399–410.
[62] RavìD,WongC,DeligianniF,BerthelotM,Andreu-PerezJ,LoB,YangG.Deep [84] WerbosPJ.Generalizationofbackpropagationwithapplicationtoarecurrentgas
learningforhealthinformatics.IEEEJBHI2016;21:4–21.https://doi.org/10.1109/ marketmodel.N.networ.1988;1:339–56.https://doi.org/10.1016/0893-6080(88)
JBHI.2016.2636665. 90007-X.
[63] ReedR,MarksIIRJ.Neuralsmithing:supervisedlearninginfeedforwardartificial [85] WerbosPJ.Backpropagationthroughtime:whatitdoesandhowtodoit.Procof
neuralnetworks.MitPress;1999. theIEEE1990;78:1550–60.https://doi.org/10.1109/5.58337.
[64] RomeroA,CarrierPL,ErraqabiA,SylvainT,AuvolatA,DejoieE,LegaultMA, [86] WrightRE.Logisticregression.1995.
Dub(cid:1)eMP,HussinJG,BengioY.Dietnetworks:thinparametersforfatgenomics. [87] XiaY,QinT,ChenW,BianJ,YuN,LiuTY.Dualsupervisedlearning.In:
2016.arXivpreprintarXiv,1611.09340. Internationalconferenceonmachinelearning.PMLR;2017.p.3789–98.
[65] RongD,XieL,YingY.Computervisiondetectionofforeignobjectsinwalnutsusing [88] XieF,ZhangJ,WangJ,ReubenA,XuW,YiX,VarnFS,YeY,ChengJ,YuM,etal.
deeplearning.ComputElectronAgric2019;162:1001–10.https://doi.org/ Multifactorialdeeplearningrevealspan-cancergenomictumorclusterswith
10.1016/j.compag.2019.05.019. distinctimmunogenomiclandscapeandresponsetoimmunotherapy.ClinCancRes
[66] SakuradaM,YairiT.Anomalydetectionusingautoencoderswithnonlinear 2020a;26:2908–20.https://doi.org/10.1158/1078-0432.CCR-19-1744.
dimensionalityreduction.In:Proceed.OftheMLSDA20142ndworkshopMLSdata [89] XieY,WuX,WardR.Linearconvergenceofadaptivestochasticgradientdescent.
analysis;2014.p.4–11.https://doi.org/10.1145/2689746.2689747. In:Internationalconferenceonartificialintelligenceandstatistics.PMLR;2020b.
[67] SalimansT,KingmaDP.Weightnormalization:asimplereparameterizationto p.1475–85.
acceleratetrainingofdeepneuralnetworks.In:NIPS;2016.p.901–9.https://arxiv. [90] XinJ,EmbrechtsMJ.Supervisedlearningwithspikingneuralnetworks.In:
org/pdf/1602.07868.pdf. IJCNN’01.Internationaljointconferenceonneuralnetworks.Proceedings(cat.
[68] SchmidhuberJ.Deeplearninginneuralnetworks:anoverview.Neur.networ. Noh01CH37222),IEEE;2001.p.1772–7.
2015;61:85–117.https://doi.org/10.1016/j.neunet.2014.09.003. [91] YangJ,MaJ.Feed-forwardneuralnetworktrainingusingsparserepresentation.
[69] SharkawyAN.Principleofneuralnetworkanditsmaintypes.J.Adv.Appl. ExpertSystAppl2019;116:255–64.
Comput.Math.2020;7:8–19. [92] YangX,DengC,ZhengF,YanJ,LiuW.Deepspectralclusteringusingdual
[70] SinghV,BaranwalN,SevakulaRK,VermaNK,CuiY.Layerwisefeatureselectionin autoencodernetwork.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
stackedsparseauto-encoderfortumortypeprediction.In:2016IEEEinternational visionandpatternrecognition;2019.p.4066–75.
conferenceonBioinformaticsandbiomedicine(BIBM).IEEE;2016.p.1542–8. [93] ZhengS,ZhaoJ.Anewunsuperviseddataminingmethodbasedonthestacked
[71] SitMA,KoyluC,DemirI.Identifyingdisaster-relatedtweetsandtheirsemantic, autoencoderforchemicalprocessfaultdiagnosis.ComputChemEng2020;135:
spatialandtemporalcontextusingdeeplearning,naturallanguageprocessingand 106755.
spatialanalysis:acasestudyofhurricaneirma.InternationalJournalofDigital [94] ZingarettiLM,GezanSA,Ferr~aoLFV,OsorioLF,MonfortA,Mun~ozPR,
Earth2019.https://doi.org/10.1080/17538947.2018.1563219. WhitakerVM,P(cid:1)erez-EncisoM.Exploringdeeplearningforcomplextraitgenomic
[72] SrivastavaN,HintonG,KrizhevskyA,SutskeverI,SalakhutdinovR.Dropout:a predictioninpolyploidoutcrossingspecies.FrontPlantSci2020;11:25.https://
simplewaytopreventneuralnetworksfromoverfitting.JMLR2014;15:1929–58. doi.org/10.3389/fpls.2020.00025.
https://dl.acm.org/doi/abs/10.5555/2627435.2670313.
10