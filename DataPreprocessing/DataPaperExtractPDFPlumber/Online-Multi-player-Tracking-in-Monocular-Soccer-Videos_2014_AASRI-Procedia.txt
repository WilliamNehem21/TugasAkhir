Available online at www.sciencedirect.com
ScienceDirect
AASRI Procedia 8 ( 2014 ) 30 – 37
2014 AASRI Conference on SportsEngineering and Computer Science (SECS 2014)
Online Multi-player Tracking in Monocular Soccer Videos
Michael Herrmanna,*, Martin Hoerniga, Bernd Radiga
aTechnischeUniversitätMünchen,ImageUnderstandingandKnowledge-BasedSystems,Boltzmannstr.3,D-85748Garching,Germany
Abstract
Thetrackingofplayers in monocular soccer videos isa challengingtaskbecause ofnumerous difficultiesthatcan occur
especiallyinTVbroadcasts,suchascameramotions,severeocclusionofplayers,orinhomogeneouslightningconditions.
Weproposeanewrobustmethodformulti-playertracking,whichisbasedonfindinglocalmaximaonaconfidencemap.
This map represents an ensemble of visual evidences, such as colors of the team outfits, responses of a HOG human
detector,andgrassregionsinimages.Thiscombinationoffeaturesallowsforarobustonlinetrackingprocedurethatdoes
not require any further information about the camera calibration or other user input. In the evaluation using four
representativedatasets,ouralgorithmshowsremarkableaccuracyandoutperformsastate-of-the-artpedestriantracker.
©© 2200114 4ThHe eArrumthoarns.n P,uHbloisehrendi gby, RElasedviige.r BP.u Vb.l iTshhies dis bany oEplesne avciceerssB a.rVtic.le under the CC BY-NC-ND license
S(hettlpe:c//tciroenatiavnecdo/mormponese.rorrge/lvicieenwseus/nbyd-enrc-rneds/p3.o0n/)s.ibilityofAmericanAppliedScienceResearch Institute
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
Keywords:computervision,socceranalysis,playertracking
1.Introduction
We aim to develop a system for an automaticanalysis of soccer matchesthat extractsmatch statistics and
performstacticalanalysisfrommonocularrecordings,suchasTVbroadcasts,which usuallyexhibitnumerous
difficulties. One important task of such a system is the 2D tracking of players in the video images. For this
purpose,weproposeanewrobustunsupervisedonlinemethodthatprovidesthreemaincontributions:
*Correspondingauthor.Tel.:+49-89-289-17779;fax:+49-89-289-17757.
E-mailaddress:michael.herrmann@tum.de
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.08.006Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37 31
Figure1:Fromlefttoright:originalimage,segmentationresults,confidencemap,anddetectionresults(withconfidencevalues).
 An observation model that is based on a combination of soccer-specific features (grass color), match-
specific features (colors of team outfits), and universal features (HOG detector) resulting in a robust
confidencemapforplayerpositions(seesections2and3andFigure1).
 An efficient measurement model that finds optimal locations on a confidence map starting at predicted
positionsofsingle-targetKalmanfilters[1](seesection4).
 Anefficientplayerdetection,avoidingtime-consumingtracking-by-detection(seesection4).
Inhumantracking,thetracking-by-detection approachiswidelyusedformulti-targettracking(seei.e.[2],
[3]).Humandetectors often showlowaccuracyandpoor computationalperformancefor smallobjects, while
in soccer videos, especiallyin low-resolution records and wide-angle scenes, player heights of 40 pixels and
lessarequitecommon.Incontrast,kernel-basedapproaches,suchasthemean-shifttracking[4],oftenrequire
aninitializationofthetemplatesandriskdriftingaway,duetoobject-specifictemplateadaption.
Zhang et al.[5] proposed an onlinemethod with a combination oftracking-by-detection and kernel-based
trackingandachievedstate-of-the-artresultsusingstandardpedestriantrackingdatasets.
In principle,ourapproachissimilar,butweusesoccer-specificknowledgetoautomaticallydetect players
without an exhaustive sliding-window approach. Our observation model is partially based on the ideas of
ASPOGAMO[6],butincorporatesmoreuniversalfeatures,suchastheHOGdetector,anddoesnotneedany
human-guided initialization. It is similar to the method proposed by [7], which is, however, a stand-alone
playerdetectionanddoesnotincorporatetrackinginformationovertime.
2.Preliminaries
Playerpositions.Inourapproach,thepositionofaplayer(cid:1868) ismodeledwithinanimage(cid:2165)(cid:2202) attime(cid:1872)using
(cid:3036)
an axis-aligned bounding box(cid:2158) ≔{((cid:1876),(cid:1877))∈ℕ(cid:2870)|((cid:1876) ≤(cid:1876) <(cid:1876) + (cid:1875))∧((cid:1877) ≤(cid:1877)<(cid:1877) +ℎ )}, where((cid:1876) ,(cid:1877))
(cid:2191) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036)
istheupper-leftcornerand(cid:1875) andℎ arewidthandheight,respectively.Aboundingboxdescribestheextent
(cid:3036) (cid:3036)
ofaplayerintheimageandinourmethodtheaspectratioisfixedto(cid:1875) ≔0.41ℎ (see[8]).
(cid:3036) (cid:3036)
Player segmentation. Our confidencemapisbased on segmentedplayer regionsintheimageandweuse
arobustgrasssegmentationmethodfollowing[9].ThisproceduretakesasinputaRGBimage(cid:2165)(cid:2202) attime(cid:1872)and
returns an image region(cid:2164)(cid:2202), describing the enclosing hull of the playing field, and a foreground region(cid:2162)(cid:2202),
which,underidealcircumstances,coverstheimageregionofallplayersinthevisiblepartoftheplayingfield32 Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37
(see Figure 1). Due tonoise and movement artifacts usuallynot all players are segmented properly, whereas
partsofotherforegroundobjects,suchasfieldlines,canbeincluded.
Unsupervised generation of color templates. Accordingtothe FIFArules [10], thecolors ofthejerseys
have tobe chosen sothat the players from different teams, thereferees, andthegoal keepers areall pairwise
distinguishable. We make use of this fact by initially determining color templates for each type of outfit,
basedoncolorhistograms.
Given the first frame(cid:2165)(cid:2777) of a video sequence, our automatic detection procedure (see section 5) returns
player positions represented by a set of(cid:1866) bounding boxes(cid:2158)(cid:2777) ≔(cid:3419)(cid:2158)(cid:2777),…,(cid:2158)(cid:2777) (cid:3423). We extract the set of RGB
(cid:2868) (cid:2778) (cid:2196)(cid:2777)
color vectors of(cid:2165)(cid:2777) for thepixelswithin(cid:3435)(cid:2158)(cid:2777)∪…∪(cid:2158)(cid:2777) (cid:3439)∩(cid:2162)(cid:2777) andcluster thisset byapplyingthe k-means++
(cid:2778) (cid:2196)(cid:2777)
algorithm [11] using the Euclidean distance. The cluster centers are the (cid:1863) dominant color vectors that
(cid:3004)
represent the(cid:1863) bins of our color histograms. A color vector is assignedto the bin that isrepresented bythe
(cid:3004)
nearest dominant color. We divide each bounding box(cid:2158)(cid:2777) to its equal-sized top, mid, and bottom parts(cid:2176)(cid:2158)(cid:2777),
(cid:2191) (cid:2191)
(cid:2169)(cid:2158)(cid:2777), and(cid:2158)(cid:2158)(cid:2777) andcalculatethreehistogramswith respect to(cid:2176)(cid:2158)(cid:2777)∩(cid:2162)(cid:2777),(cid:2169)(cid:2158)(cid:2777)∩(cid:2162)(cid:2777), and(cid:2158)(cid:2158)(cid:2777)∩(cid:2162)(cid:2777). Thistakes
(cid:2191) (cid:2191) (cid:2191) (cid:2191) (cid:2191)
into account that a player’s outfit changes from top to bottom, regarding jersey, shorts and socks. Stacking
andnormalizingresultinafeatureunitvectorwith3(cid:1863) entriesforeachboundingbox.
(cid:3004)
Tocalculatethedistanceoftwonormalizedhistogramvectors(cid:2190) and(cid:2190) , weusetheHellingerdistanceas
(cid:2778) (cid:2779)
proposedby[4]anddefine(cid:1856) ((cid:2190) ,(cid:2190) )≔(cid:3493)(cid:1856)(cid:2870)((cid:2190) ,(cid:2190) )with(cid:1856)(cid:2870)((cid:2190) ,(cid:2190) )∶=1−Σ(cid:2871)(cid:3038)(cid:3252)(cid:3493)(cid:2190) ((cid:1861))(cid:2190) ((cid:1861)).
(cid:3009) (cid:2778) (cid:2779) (cid:3009) (cid:2778) (cid:2779) (cid:3009) (cid:2778) (cid:2779) (cid:3036)(cid:2880)(cid:2869) (cid:2778) (cid:2779)
Wetrytoestimatethenumber ofdifferentoutfitsamongthegivenboundingboxes byclusteringtheset of
histogramvectorsusingk-means++ with(cid:1863)=1,…,5(therearefivedifferent possibleoutfits: 2xplayers, 2x
goalkeepers, 1xreferees)usingthedistance(cid:1856) . Wechoose(cid:1863) asthesmallest(cid:1863)for whichtheresult ofthe
(cid:3009) (cid:3040)(cid:3036)(cid:3041)
clustering fulfills our decision criterion: for a given set of(cid:1864)histogram vectors(cid:2190) ,…,(cid:2190) and their(cid:1863) cluster
(cid:2778) (cid:2194)
centers(cid:2197) ,…,(cid:2197) ,eachvector(cid:2190) isassignedtoitsclustercenter(cid:2197)((cid:2190) )andwedefinetheminimumdistanceas
(cid:2778) (cid:2193) (cid:2191) (cid:2191)
(cid:1856) ∶= min(cid:1856) ((cid:2190) ,(cid:2197)((cid:2190) ))and the maximum distance as(cid:1856) ∶=max(cid:1856) ((cid:2190) ,(cid:2197)((cid:2190) )). With the threshold
(cid:3040)(cid:3036)(cid:3041) (cid:2919) (cid:3009) (cid:2191) (cid:2191) (cid:3040)(cid:3028)(cid:3051) (cid:2919) (cid:3009) (cid:2191) (cid:2191)
parameters(cid:1872) and(cid:1872) , our decision criterion is((cid:1856) ≤(cid:1872) )∧((cid:1856) ≤(cid:1872) ). During tracking each
(cid:3040)(cid:3036)(cid:3041) (cid:3040)(cid:3028)(cid:3051) (cid:3040)(cid:3036)(cid:3041) (cid:3040)(cid:3036)(cid:3041) (cid:3040)(cid:3028)(cid:3051) (cid:3040)(cid:3028)(cid:3051)
trackedboundingboxisassignedtotheoutfitclasswithnearestdistance(cid:1856) atthemomentofdetection.
(cid:3003)
HOG-based human detection. We trained a human detector based on histograms of oriented gradients
(HOG) according to [12] with some slight modifications to their default detector. We use a 64×128 pixels
detection window with a human size of 41×100 pixels (see [8]). To allow for an efficient calculation, we do
not apply a Gaussian spatial window during the accumulation of the histograms. To avoid soccer-specific
overfitting,theclassifieristrainedusingtheINRIApedestriandataset[12].
3.Confidencemapforplayerpositions
We construct a function that describes a degree of quality of the image evidence(cid:2165)(cid:2202), given a set(cid:2158)(cid:2202) ≔
(cid:3419)(cid:2158)(cid:2202),…,(cid:2158)(cid:2202) (cid:3423)ofboundingboxesattime(cid:1872).Weassigneachboundingbox(cid:2158)(cid:2202) ∈(cid:2158)(cid:2202) totheconnectedcomponent
(cid:2778) (cid:2196)(cid:2202) (cid:2191)
of the foreground(cid:2162)(cid:2202) with the greatest overlap. If(cid:2158)(cid:2202)∩(cid:2162)(cid:2202) =∅, we assign(cid:2158)(cid:2202)to an empty region. Hence, the
(cid:2191) (cid:2191)
result isa set ofimageregions(cid:2175)(cid:2202) ≔{(cid:2174)(cid:2202),…,(cid:2174)(cid:2202) }, where each(cid:2174)(cid:2202) ∈(cid:2175)(cid:2202)has a set ofassigned boundingboxes
(cid:2778) (cid:2195)(cid:2202) (cid:2192)
(cid:2158)(cid:3561)(cid:3047) ≔(cid:3419) (cid:2158)(cid:3561)(cid:2202),…, (cid:2158)(cid:3561)(cid:2202) (cid:3423). In the following, fractions are only calculated if the denominator is not zero.
(cid:3037) (cid:2192) (cid:2778) (cid:2192) (cid:2196)(cid:2174)
Otherwise, theyare omitted. We perform all calculations for eachregion(cid:2174)(cid:2202) andits assigned boundingboxes
(cid:2192)Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37 33
independently. Forthepurposeofclearnessweomittheindex(cid:1862)andset(cid:1866) ≔(cid:1866)((cid:2174)(cid:2202))∈ℕ∖{0}.Inmost cases
(cid:3019) (cid:2192)
(cid:1866) =1.Caseswith(cid:1866) >1occurforexampleifoneplayerispartlyoccludedbyanotherplayerintheimage.
(cid:3019) (cid:3019)
Region-basedevidence. Ourregion-basedfeaturesareinspiredbythecompactnessconstraintandthesize
constraint of[6]andarebasedontheassumptionthatanimageregionofoneormoreplayersshouldbefully
covered by bounding boxes. The coverage feature (cid:1855) describes the degree of coverage of the connected
(cid:2869)
component(cid:2174)(cid:2202) bytheunionoftheboundingboxesandisdefinedas
(cid:1855) ((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))≔area(cid:3435)((cid:2158)(cid:3561)(cid:2202) ∪…∪(cid:2158)(cid:3561)(cid:2202) )∩(cid:2174)(cid:2202)(cid:3439)/area((cid:2174)(cid:2202)).
(cid:2869) (cid:2778) (cid:2196)(cid:2174) (cid:2778) (cid:2196)(cid:2174)
The over-coverage feature(cid:1855) penalizes regions inside the bounding boxes that are not covered by the
(cid:2870)
foregroundregion.Tofavor positionswhere(cid:2174)(cid:2202) isevenlyspreadalongthehorizontalaxis, eachboundingbox
(cid:2158)(cid:3561)(cid:2202) isdividedtotheupperhalf(cid:2203)(cid:2158)(cid:3561)(cid:2202)andthelowerhalf(cid:2194)(cid:2158)(cid:3561)(cid:2202) andwedefine
(cid:2191) (cid:2191) (cid:2191)
(cid:1855) ((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))≔1−max max area((cid:2197)(cid:2158)(cid:3561)(cid:2202)\(cid:2174)(cid:2202))/area((cid:2197)(cid:2158)(cid:3561)(cid:2202)).
(cid:2870) (cid:2778) (cid:2196)(cid:2174) (cid:2919) (cid:2925)∈{(cid:2931),(cid:2922)} (cid:2191) (cid:2191)
The overlap feature(cid:1855) takes into account that in general the bounding boxes of two players do not fully
(cid:2871)
overlap(exceptintherarecaseoffullocclusion).Itisdefinedas
(cid:1855) ((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))≔1−max area((cid:2158)(cid:3561)(cid:2202)∩(cid:2158)(cid:3561)(cid:2202))/area((cid:2158)(cid:3561)(cid:2202)∪(cid:2158)(cid:3561)(cid:2202)).
(cid:2871) (cid:2778) (cid:2196)(cid:2174) (cid:2919),(cid:2920) (cid:2191) (cid:2192) (cid:2191) (cid:2192)
Color-based evidence. The color-based confidence (cid:1855) is based on the dominant colors and the outfit
(cid:2872)
classes (see section 2). The normalized color histogram vector (cid:2190)((cid:2158)(cid:3561)(cid:2202))∈ℝ(cid:2871)(cid:3038)(cid:3252) is calculated with respect to
(cid:2191)
(cid:2158)(cid:3561)(cid:2202)∩(cid:2174)(cid:2202).Weset(cid:2174)(cid:2202) ∶=(cid:2174)(cid:2202),ifarea((cid:2174)(cid:2202))>0.Otherwise,(cid:2174)(cid:2202) isthelargestaxis-alignedinnerellipseof(cid:2158)(cid:3561)(cid:2202).Each
(cid:2191) (cid:2185) (cid:2185) (cid:2185) (cid:2191)
boundingboxhasanassignedoutfitclasswithhistogramvector(cid:2197)((cid:2158)(cid:3561)(cid:2202))∈ℝ(cid:2871)(cid:3038)(cid:3252) andwedefine
(cid:2191)
(cid:1855) ((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))≔1− (cid:3289)(cid:3267) (cid:3495)(cid:1998)(cid:3041)(cid:3267)(cid:1856) (cid:3435)(cid:2190)((cid:2158)(cid:3561)(cid:2202)),(cid:2197)((cid:2158)(cid:3561)(cid:2202))(cid:3439).
(cid:2872) (cid:2778) (cid:2196)(cid:2174) (cid:3036)(cid:2880)(cid:2869) (cid:3009) (cid:2191) (cid:2191)
HOG-based evidence. We use a human detector, trained as described in section 2. A resized sub-image
with size of 64×128 pixels is generated for each bounding box (cid:2158)(cid:3561)(cid:2202), so that the height of the bounding box
(cid:2191)
correspondsto100pixelsinthesub-image,thesub-imagehasthesamecenterpointastheboundingbox, and
the aspect ratio of the pixels remains unchanged. This sub-image is used to perform a classification of the
human detector. The result of the classifier is a decision value (cid:1856) ((cid:2158)(cid:3561)(cid:2202))∈ℝ, which is mapped using the
(cid:3005) (cid:2191)
parameters (cid:1873) and (cid:1874) to (cid:1856)(cid:4593) (cid:3435)(cid:2158)(cid:3561)(cid:2202)|(cid:1873) ,(cid:1874) (cid:3439)≔((cid:1856) ((cid:2158)(cid:3561)(cid:2202))−(cid:1873) )/((cid:1874) −(cid:1873) ). The confidence is defined by the
(cid:3005) (cid:3005) (cid:3005) (cid:2191) (cid:3005) (cid:3005) (cid:3005) (cid:2191) (cid:3005) (cid:3005) (cid:3005)
geometricmean
(cid:1855) (cid:3435)(cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202)(cid:3627)(cid:1873) ,(cid:1874) )≔ (cid:3289)(cid:3267) (cid:3495)(cid:1998)(cid:3041)(cid:3267)max(1,min(0,(cid:1856)′ (cid:3435)(cid:2158)(cid:3561)(cid:2202)|(cid:1873) ,(cid:1874) (cid:3439)).
(cid:2873) (cid:2778) (cid:2196)(cid:2174) (cid:3005) (cid:3005) (cid:3036)(cid:2880)(cid:2869) (cid:3005) (cid:2191) (cid:3005) (cid:3005)
Gating. During tracking we additionallyincorporate a gating term, based on the distance to the predicted
location of the tracker (if available). For each bounding box(cid:2158)(cid:3561)(cid:2202), we calculate the Euclidean distance of its
(cid:2191)
center point tothe center point ofthe correspondingpredicted bounding box(cid:2172)(cid:3561)(cid:2202), normalized bythe length of
(cid:2191)34 Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37
the diagonal of(cid:2172)(cid:3561)(cid:2202) andtrimmed toamaximum value of1. Thegating confidence(cid:1855) is defined byoneminus
(cid:2191) (cid:2874)
themeandistanceoftheboundingboxes(cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) andtheircorrespondingpredictions.
(cid:2778) (cid:2196)(cid:2174)
Ensemble averaging. The single features(cid:1855),(cid:1861) ∈{1,…6}are combined by weighted ensemble averaging
(cid:3036)
using the weights(cid:1875),(cid:1861) ∈{1,…6}, resulting in the overall confidence function(cid:1855)((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202)), which we
(cid:3036) (cid:2778) (cid:2196)(cid:2174)
defineasfollows:
(cid:1855)((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))≔(cid:2001)(cid:1875)(cid:1855)((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))/(cid:2001)(cid:1875).
(cid:2778) (cid:2196)(cid:2174) (cid:3036) (cid:3036) (cid:2778) (cid:2196)(cid:2174) (cid:3036)
Note that(cid:1855)((cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202))∈[0;1]because∀(cid:1861):(cid:1855) (cid:3435)(cid:2158)(cid:3561)(cid:2202),…,(cid:2158)(cid:3561)(cid:2202) ,(cid:2174)(cid:2202)(cid:3439)∈[0;1]. For(cid:2174)(cid:2202) =∅, we set(cid:1875) ≔0and
(cid:2778) (cid:2196)(cid:2174) (cid:3036) (cid:2778) (cid:2196)(cid:2174) (cid:2869)
(cid:1875) ≔0andifthereisonlyoneobject((cid:1866) =1),weomittheoverlapfeatureandset(cid:1875) ≔0.
(cid:2870) (cid:3019) (cid:2871)
Maximization of theconfidence. Foreverynewtimestep,wesearch for positionswithalocalmaximum
on the confidence map near the Kalman predictions. To keep thenumber of calculations low, we propose a
simplegreedyheuristic: we iterativelyoptimizethe confidence with respect tothe positions of thebounding
boxes,whiletheirsizesandtheregion(cid:2174)(cid:2202) remainfixed,thusfor(cid:1866) boundingboxes,wehave2(cid:1866) variables.In
(cid:3047) (cid:3047)
each iteration of the maximization procedure, we evaluate the confidence map at2(cid:1866) +1positions near the
(cid:3047)
currentposition anddeterminetheposition with thehighestconfidencevalue. Ifthis valueishigher than the
valueatthecurrentposition,theiterationcontinues,startingatthebestposition.Otherwise,theiterationstops
with the current position as the result. Usually, our efficient greedy approach stops after 2-3 iterations and
shows,however,satisfactoryresults.
The2(cid:1866) +1positionsin theneighborhood ariseasfollows: firstthegradientof(cid:1855)isapproximatedusinga
(cid:3047)
finite forward difference (with step sizeΔ(cid:1876) ≔8), which results in2(cid:1866) evaluations of the confidence map
(cid:3047)
(instead of4(cid:1866) using central difference). Then one step (likewise with step sizeΔ(cid:1876)) in the direction of the
(cid:3047)
gradientistakenfortheremainingevaluation.
Measurement of object size. Mainly due to perspective projection the size of a player in the image
depends on the image position. To enforce consistency, we normalize the sizes of the predicted bounding
boxes before the maximization step. For this purpose, we fit a linear model using a least-squares approach,
where theheightℎis the dependent and the coordinate(cid:1877)is the independent variable. Afterwards, theheight
ofeachboundingboxisadaptedaccordingtothismodelandthewidthisdeterminedby(cid:1875)≔0.41ℎ(see[8]).
After themaximization step,for each boundingbox(cid:2158)(cid:3561)(cid:2202) weiterateseveralstepsofscalewithrespecttothe
(cid:2191)
original sizeand takethesizewith thebest confidence value(cid:1855) asaresulting measurement, which results
(cid:3029)(cid:3032)(cid:3046)(cid:3047)
inthefinalmeasurementquality(cid:1869)(cid:3435)(cid:2158)(cid:3561)(cid:2202)(cid:3439)≔(cid:1855) .
(cid:2191) (cid:3029)(cid:3032)(cid:3046)(cid:3047)
4.Playertracking
We perform multi-target tracking byapplying a single-target Kalman filter [1] for each tracked bounding
box. A new measurement is generated byan optimization step with respect to theconfidence map using the
Kalmanpredictionasthestartingposition.Thedetectionofnewandlosttargetsisperformedwiththehelpof
deterministicheuristics,whichareguidedbytheconfidencemap.
State and measurement model. The state of player (cid:1868) is represented by a six-tuple
(cid:3036)
(cid:1867) ≔((cid:1876) (cid:1877) (cid:1876)̇ (cid:1877)̇ (cid:1875) ℎ )(cid:3021), where(cid:1876) ,(cid:1877),(cid:1875), andℎ are the parameters of the bounding box, and(cid:1876)̇ and(cid:1877)̇ are
(cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036) (cid:3036)
thevelocityin(cid:1876)and(cid:1877),respectively.Weapplyaconstantsizeandconstantvelocitymodel.Thus,thechangesMichael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37 35
in size and the acceleration of the objects are implicitly modeled by the process noise. Measurements are
performedforthepositionandthesizevariables.
Lossanddetectionofplayersduringtracking.Duringtrackingatrackedboundingbox(cid:2158)(cid:2202) isremovedif
(cid:2191)
its measurement quality(cid:1869)((cid:2158)(cid:2202))is below a threshold(cid:1872) in two frames in a row or if it has a significant
(cid:2191) (cid:3039)(cid:3042)(cid:3046)(cid:3046)
overlap with another tracked bounding box (with higher measurement quality) in morethan four frames in a
row.
In return, connected components that fulfill some size constraints according to the linear size model are
considered as single player candidates and for these positions the confidence map is evaluated and
maximized. Single player candidates that have a measurement quality (cid:1869)((cid:2158)(cid:2202)) greater than or equal to a
(cid:2191)
threshold(cid:1872) intwoframesinarowareaddedtothetrackedboundingboxes.
(cid:3031)(cid:3032)(cid:3047)(cid:3032)(cid:3030)(cid:3047)
Initial player detection. For the initial player detection, we select connected components of the
foreground region (cid:2162)(cid:2777) with an appropriate orientation and aspect ratio as single player candidates. In the
neighborhood of each candidate, we perform a sliding-window human detection with a scale space which
depends on thesizeofthecandidateregion.Thepositivedetectionsareused tofitour linear sizemodel.The
human detection is repeated in the neighborhood of all connected components of(cid:2162)(cid:2777) with a restricted scale
space depending on theposition of theregion in theimage. Again, the positive detections are used to fit the
sizemodel andtoestimatetheteamhistograms(seesection 2). Thesedetectionsarethestartingpointfor the
firstmaximizationstepresultingintheinitialplayerpositions.
5.Implementationdetailsandevaluationresults
In our implementation we set the number of dominant colors(cid:1863) ≔64as an estimated upper bound for
(cid:3004)
five outfits with each three four-colored parts (jersey, short, socks). The other parameters are determined
empirically and set as follows: (cid:1872) ≔0.1, (cid:1872) ≔0.25, (cid:1873) ≔−4, (cid:1874) ≔0.5,(cid:1875) ≔0.75, (cid:1875) ≔0.4,
(cid:3040)(cid:3036)(cid:3041) (cid:3040)(cid:3028)(cid:3051) (cid:3005) (cid:3005) (cid:2869) (cid:2870)
(cid:1875) ≔0.05,(cid:1875) ≔1.0,(cid:1875) ≔0.5,(cid:1875) ≔0.05,(cid:1872) ≔0.35, and(cid:1872) ∶=0.7. Weevaluateour system with
(cid:2871) (cid:2872) (cid:2873) (cid:2874) (cid:3039)(cid:3042)(cid:3046)(cid:3046) (cid:3031)(cid:3032)(cid:3047)(cid:3032)(cid:3030)(cid:3047)
the help of MOTA and MOTP [13] and use manually annotated ground truth of the following video
sequences:
 German–Holland,TVbroadcast,SD,37s,925frames(GH)
 BayernMunich–OSCLille,TVbroadcast,HD,15s,752frames(BL)
 ISSIA-CNRCamera3,staticcamera,Full-HD,120s,3000frames,onlineavailable[14](ISSIA)
 VS-PETSCamera3Test,staticcamera,720×576,100s,2500frames,onlineavailable[15] (VS)
Ground-truth and tracked bounding boxes are fixed to an aspect ratio 0.41:1 as proposed in [8]. Some
annotated targets that are outside the field (like coaches and linesman), as well as ground-truth bounding
boxes that are cropped by the image boundaries, are added to an ignore list, i.e. they don’t need to be
matched,butitisnotanerroriftheyarematched.
Our baseline is the publicly available tracker of Zhang et al. [5], which achieves state-of-the-art tracking
results on pedestrian tracking datasets. We used the standard parameter values which come with the source
code,excepttheHOG_DETECT_FRAME_RATIO,whichwesetto2forBLandISSIA,andto3forGHand
VSallowingtodetectsmallplayers.Asthedetectorofthistrackingproceduregeneratesalotoffalsepositive
detectionsintheaudiencearea,weignoreallitstrackedobjectsoutsideourfieldhull(cid:1834)(cid:3047) attime(cid:1872).36 Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37
Figure2:MOTA(toprow)andMOTP(bottomrow)results(independenceoftheoverlapthreshold)oftheproposedtrackingprocedure
incomparisonwiththeprocedureofZhangetal.[5].Resultsaregivenforeachdataset(GH,BL,ISSIA,VS)fromlefttoright.
Figure 2 shows the comparison of the MOTA / MOTP of our proposed tracking procedure and the
procedure of Zhang et al. depending on the overlap threshold for the assignment of the tracked bounding
boxes tothe groundtruth. Our system achieves satisfactoryMOTA scores for the standard overlapthreshold
of0.5.Withincreasingoverlapthreshold,theaccuracybecomesremarkable(upto0.9andmore).Inallcases,
our results outperform the results of the baseline. Our non-optimized implementation processes about 1-2
frames per second on an Intel Core2 Quad Q9650. Because of its highlyparallel structure, we believe that a
nearreal-timeperformancecouldbepossible. Incontrast,themethodofZhangetal.processes0.1framesper
second.
WeprovideapubliclyavailablevisualizationofourtrackingresultsforISSIAandVS(see[16]and[17]).
6.Conclusion
We proposed an unsupervised online 2D tracking procedure for players in monocular soccer videos that
applies an efficient determination of local maxima in a confidence map. This map is based on a robust
combination of soccer-specific (grass color), match-specific (team outfit colors) and general (HOG detector)
image features. Avoiding a time-consuming sliding-window approach our system allows for a fast player
tracking that in addition does not require any further input, such as user input or camera parameters. Our
trackingresultsachievehighaccuracyandoutperformastate-of-the-artpedestriantracker.
Acknowledgements
ThefundingforthisresearchwasprovidedbyGermanResearchFoundation(DFG)grantno.RA359/12-1.
References
[1] R.E.Kalman,“ANewApproachtoLinearFilteringandPredictionProblems.,”Transactionsofthe
ASME-JournalofBasicEngineering,vol.82,no.1,pp.35–45,Mar.1960.Michael Herrmann et al. / AASRI Procedia 8 ( 2014 ) 30 – 37 37
[2] M.D.Breitenstein,F.Reichlin,B.Leibe,E.Koller-Meier,andL.VanGool,“OnlineMultiperson
Tracking-by-DetectionfromaSingle,UncalibratedCamera,”IEEETransactionsonPatternAnalysisand
MachineIntelligence,vol.33,no.9,pp.1820–1833,Sep.2011.
[3] H.Izadinia,I.Saleemi,W.Li,andM.Shah,“(MP)2T:MultiplePeopleMultiplePartsTracker,”in
ComputerVision–ECCV2012,vol.7577,Berlin,Heidelberg:Springer,2012,pp.100–114.
[4] D.Comaniciu,V.Ramesh,andP.Meer,“Kernel-basedObjectTracking,”IEEETransactionson
PatternAnalysisandMachineIntelligence,vol.25,no.5,pp.564–577,May2003.
[5] J.Zhang,L.L.Presti,andS.Sclaroff,“OnlineMulti-personTrackingbyTrackerHierarchy,”inIEEE
9thInternationalConferenceonAdvancedVideoandSignal-BasedSurveillance,2012,pp.379–385.
[6] M.Beetz,S.Gedikli,J.Bandouch,B.Kirchlechner,N.vonHoyningen-Huene,andA.C.P.Perzylo,
“VisuallyTrackingFootballGamesBasedonTVBroadcasts,”in20thInternationalJointConferenceon
ArtificialIntelligence,2007,pp.2066–2071.
[7] S.Gerke,S.Singh,A.Linnemann,andP.Ndjiki-Nya,“UnsupervisedColorClassifierTrainingfor
SoccerPlayerDetection,”inVisualCommunicationsandImageProcessing,2013,pp.1–5.
[8] P.Dollar,C.Wojek,B.Schiele,andP.Perona,“PedestrianDetection:AnEvaluationoftheStateofthe
Art,”IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.34,no.4,pp.743–761,Apr.
2012.
[9] M.Hoernig,M.Herrmann,andB.Radig,“RealTimeSoccerFieldAnalysisfromMonocularTVVideo
Data,”in11thInternationalConferenceonPatternRecognitionandImageAnalysis(PRIA-11-2013),
Samara,2013,vol.2,pp.567–570.
[10] FIFA,“LawsoftheGame,”2014.[Online].Available:
http://www.fifa.com/aboutfifa/footballdevelopment/technicalsupport/refereeing/laws-of-the-game/.
[Accessed:30-Apr-2014].
[11] D.ArthurandS.Vassilvitskii,“k-means++:theAdvantagesofCarefulSeeding,”in18thannualACM-
SIAMsymposiumonDiscretealgorithms,Philadelphia,PA,USA,2007,pp.1027–1035.
[12] N.DalalandB.Triggs,“HistogramsofOrientedGradientsforHumanDetection,”inIEEEConference
onComputerVisionandPatternRecognition(CVPR2005),2005,vol.1,pp.886–893.
[13] K.BernardinandR.Stiefelhagen,“EvaluatingMultipleObjectTrackingPerformance:TheCLEAR
MOTMetrics,”EURASIPJournalonImageandVideoProcessing,vol.2008,pp.1–10,May2008.
[14] T.D’Orazio,M.Leo,N.Mosca,P.Spagnolo,andP.L.Mazzeo,“ASemi-automaticSystemfor
GroundTruthGenerationofSoccerVideoSequences,”in6thIEEEInternationalConferenceonAdvanced
VideoandSignalBasedSurveillance,2009,pp.559–564.
[15] UniversityofReading,“VS-PETSFootballDataset,”ThirdIEEEInternationalWorkshoponVisual
SurveillanceandPerformanceEvaluationofTrackingandSurveillance,2002.[Online].Available:
http://www.cvg.reading.ac.uk/VSPETS/vspets-db.html.[Accessed:30-Apr-2014].
[16] M.Herrmann,M.Hoernig,andB.Radig,“PlayerTrackingResultsISSIA-CNR,”2014.[Online].
Available:http://www.youtube.com/watch?v=L9t7ei6gAjk.[Accessed:30-Apr-2014].
[17] M.Herrmann,M.Hoernig,andB.Radig,“PlayerTrackingResultsVS-PETS,”2014.[Online].
Available:http://www.youtube.com/watch?v=SL1LjRAbPgI.[Accessed:30-Apr-2014].