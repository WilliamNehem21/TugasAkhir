Array15(2022)100207
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/locate/array
CE-Fed:Communicationefficientmulti-partycomputationenabled
federatedlearning
RenugaKanagavelua,∗,QingsongWeia,ZengxiangLib,HaibinZhanga,JuniartoSamsudina,
YechaoYanga,RickSiowMongGoha,ShangguangWangc
aInstituteofHighPerformanceComputing,A*STAR,Singapore
bDigitalResearchInstituteENNGroup,China
cBeijingUniversityofPostsandTelecommunications,Beijing,China
A R T I C L E I N F O A B S T R A C T
Keywords: Federatedlearning(FL)allowsanumberofpartiescollectivelytrainmodelswithoutrevealingprivatedatasets.
Federatedlearning Thereisapossibilityofextractingpersonalorconfidentialdatafromthesharedmodelseven-thoughsharingof
Edgecomputing rawdataispreventedbyfederatedlearning.SecureMultiPartyComputation(MPC)isleveragedtoaggregate
Multi-partycomputation the locally-trained models in a privacy preserving manner. However, it results in high communication cost
Committeeselection
andpoorscalabilityinadecentralizedenvironment.Wedesignanovelcommunication-efficientMPCenabled
federatedlearningcalledCE-Fed.Inparticular,theproposedCE-Fedisahierarchicalmechanismwhichforms
modelaggregationcommitteewithasmallnumberofmembersandaggregatestheglobalmodelonlyamong
committeemembers,insteadofallparticipants.Wedevelopaprototypeanddemonstratetheeffectivenessof
ourmechanismwithdifferentdatasets.OurproposedCE-Fedachieveshighaccuracy,communicationefficiency
andscalabilitywithoutcompromisingprivacy.
1. Introduction intersectionofdeeplearningandedgecomputing,wherethetraining
dataset remains in the hands of data owners and there is no need to
Artificial intelligence (AI) techniques find useful applications in pool data into a single location. Instead, model training is brought
various domains, such as healthcare, smart building, autonomous ve- to the edge of the network, so that data never leaves the network
hicles, remote monitoring and predictive maintenance of machines in and only the model is sent to the central coordinator for aggregation
manufacturing plant. The training of such AI models requires large
as shown in Fig. 1. It enables the clients to learn a model without
amount of data to achieve acceptable accuracy and throughput and
sharing the raw data and not relying on any trusted third party to
thus improving the user experience. The huge amount of data gener-
holdthedata.However,FLdependsonthecentralserver/coordinator
ated at different organizations are aggregated at a cloud-based server
for aggregating the model. All clients participating in the FL should
to produce more-effective inference model. Though this approach is
have a complete unanimity on the role of central server/coordinator
beneficial,thetransferofsheervolumeofdatatothecentralizedserver
as model aggregators. The central server/coordinator may encounter
burdens the back-bone network. It also introduces long latency [1],
whichisnotacceptableforapplicationswherereal-timedecisionsare singlepointoffailurewhichwouldcrashtheentiresystem.Wechoose
required,likeself-drivingcarsandautomatedcarassemblyplant[2]. the decentralized federated learning framework [6] to overcome this
Apartfromthose,thelegalrestrictionsandrisingconcernsaboutshar- issue.Itisaserver-less,decentralizedapproach,whereclientscommu-
ingprivateinformation[3]amongdataownersmakethemreluctantto nicatedirectlyamongthemselveswithoutacentralserver/coordinator
sendtheirdatatothedatacentreforthemodeltraining[4].Asaresult, asshowninFig.2.
hugevolumeofdatageneratedbyindividualorganizationsremainsin Though federated learning keeps the training data private to the
theformoffragmenteddatasilos. local user, still it is vulnerable to various privacy attacks during the
Toaddresstheabove-mentionedproblems,Federatedlearning(FL) trainingprocessandcausesprivacyleakage.Thereisalsoapossibility
[5]wasintroducedbyGoogle’sAIresearchteam.Ithasemergedasthe
∗ Correspondingauthor.
E-mailaddresses: renuga_k@ihpc.a-star.edu.sg(R.Kanagavelu),wei_qingsong@ihpc.a-star.edu.sg(Q.Wei),zengxiang_li@outlook.com(Z.Li),
zhang_haibin@ihpc.a-star.edu.sg(H.Zhang),juniarto_samsudin@ihpc.a-star.edu.sg(J.Samsudin),yang_yechao@ihpc.a-star.edu.sg(Y.Yang),
gohsm@ihpc.a-star.edu.sg(R.S.M.Goh),sgwang@bupt.edu.cn(S.Wang).
https://doi.org/10.1016/j.array.2022.100207
Received10March2022;Receivedinrevisedform10June2022;Accepted10June2022
Availableonline18June2022
2590-0056/©2022TheAuthor(s).PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).R.Kanagaveluetal. Array15(2022)100207
Fig.1. Centralfederatedlearning.
secretsharesoflocallytrainedmodelwithallotherclientsintheFLas
showninFig.3.Asaresult,whenthenumberofFLclientsincreases,
thecommunicationoverheadalsoincreasesinanexponentialmanner.
Theseobservationsmotivatedustoproposeanovelcommunication-
efficientMPCenabledfederatedlearningcalledCE-Fed,withtheobjec-
tiveofreducingthecommunicationoverheadandscalability.Thekey
approachofourproposedCE-Fedistoselectafewclientsasthecom-
mitteemembers,whoperformtheaggregationofthelocalmodelsofall
FLclientsinahierarchicalmannerusingtheMPCservice.Thereforeit
Fig.2. DecentralizedFL.
avoidssharingthemodelparameterfromeachclienttoeveryoneelsein
theFLlist.OurproposedCE-Fedisexecutedintwophases.Inthefirst
phase,wegrouptheFLclientsthatarelocatedclosetoeachother.The
localmodelsofallFLclientsinthesamegrouparesecurelyaggregated
using MPC to form an intra-group model. Based on the latency, one
clientiselectedfromeachgrouptoformtheaggregationcommittee.In
thesecondphase,thecommitteemembersworktogethertoaggregate
theinter-groupmodelsusingMPC,asshowninFig.4.
Themaincontributionsofourpaperaresummarizedasfollows:
• We quantitatively analysed the communication overhead when
Fig.3. MPCenabledFL. secure MPC is integrated into decentralized federated learning
withexperimentalresults.
• Weproposeahierarchicalmodelaggregationtoreducethecom-
municationcostincurredinMPCenabledFederatedLearning.
• Weproposelatency-basedcommitteeelectionalgorithmforMPC-
basedhierarchicalmodelaggregation.
• We demonstrate the effectiveness of our decentralized
communication-efficient MPC based federated learning through
extensiveexperimentsonvariousdatasets.
The remainder of this paper is organized as follows. Section 2
Fig.4. MPCenabledFL-hierarchicalaggregation. describes the background and related work. The CE-Fed framework
ispresentedinSection3.Theexperimentalanalysisandperformance
evaluations are presented in Section 4, followed by conclusions in
to extract the local user’s dataset by reverse engineering the commu- Section5.
nicatedmodelparameters[7–9].Toensureconfidentialityandprovide
data privacy guarantee of the model aggregation, privacy-preserving 2. Backgroundandrelatedwork
techniques, such as Differential Privacy (DP) [10], secure Multi-party
Computation(MPC)[11],andHomo-morphicEncryption(HE)[12,13] WepresenttheoverviewoffederatedlearningaswellastheMulti-
canbeusedtogetherwithFL[8]. party computation in this section. We also discuss the merits and
SecureMPCallowsforsecurecomputationamongmultipleparties limitationsofthevariousfederatedlearningresearchworks.
tocomputeafunctionjointlyovertheirsensitiveinputdata.Allpartici-
patingpartiesknowonlytheoutputwhilekeepingthoseinputsprivate. 2.1. Federatedlearning
Thepartiescanlearnonlythefinaloutput.
Though MPC has the advantage of secure data sharing, it tends Prior to the advent of Federated Learning (FL), data has to be
to have significant communication and computation overhead when uploaded and consolidated at a centralized data-centre or cloud. The
implemented on a large-scale decentralized federated learning sys- machine learning model will be generated from the centralized data
tem[14].InthetraditionalMPC-enabledFL,eachclientexchangesits andthenbedeployedforinferenceandrecommendation.Adataowner
2R.Kanagaveluetal. Array15(2022)100207
hasnocontroloverthedataaswellastheconstructedmachinelearning 2.3. Relatedwork
model, since both are no longer residing at the premise of the data
owner.Thereisapossibilitythatthecentralizedserviceproviderscan The growing demand for FL has resulted in the use of various
obtainextrarevenueonthedataaswellasonmodelandusethemfor techniquesandapproachestomakeitsdeploymentsuccessful.FLcan
someillegalpurposes.Thiscannotbepreventedbydataowners. be categorized as centralized FL or Decentralized FL based on the
networktopology.IncentralizedFL,acentralizedserverisresponsible
Federatedlearning(FL)hasemergedasaneffectiveapproachthat
for collecting the trained models from participating clients to build a
introduced by Google’s AI research team in 2017 [15]. The central
global model and send it back to clients. Google’s [30,31] research
server send the baseline ML model to clients. Then, each client train
team proposes federated model averaging algorithm for the model
its own sub-model using its local data and send the fine-tuned model
aggregation.ItusesadeeplearningbasedonTensorFlowandsupports
back to the central server. The central server collects all trained sub-
differential privacy method to enhance privacy guarantees to achieve
models and aggregates them. Then the aggregated models are sent
a local model with minimal communication rounds. Recent research
back to clients out over and over again until they have learnt all
works [32,33] are focused on multi-centre FL with the objective of
thereistolearn.Throughouttheentireprocess,rawdataarekepton
findingtheoptimalglobalmodelfrommultipleusergroups.Federated
clientdevicesandinsteadthelocalmodelsaretransferredandshared.
Stochastic Expectation Maximization (FedSEM) is proposed by [32]
Thefederatedmodelimprovesitsaccuracybyaggregatingmanylocal totrainmultipleglobalMLmodelsfromamulti-clusterenvironment.
models iteratively, taking advantage of complementary data/inputs Federatetransferlearning[34]usesdeeplearningtoaddressaspecific
fromalargenumberofdevices. situationwheretwoparticipantshaveapartofcommonsamplesand
Federated Learning (FL) has been extended to the collaborations onlyoneparticipanthasthelabelinformation.Toprotectmodelparam-
across multiple organizations. It is categorized to horizontal FL and eters,itusesadditivelyhomomorphicencryption.IndecentralizedFL,
vertical FL [16], based on the data distribution over the sample and thereisnoneedforthecentralizedserverformodelaggregation.Each
featurespaces.InhorizontalFL,datasetsofdifferentorganizationslike participant shares their local model with their neighbours. Research
differentindustrialorganizations,havethesamefeaturesbutdifferent works [35–37] focussed on peer-to-peer FL framework and address-
sample sizes. In vertical FL, different organizations like banks, or ing the communication delays [36]. Research work [25] focussed on
insurancecompaniesthatarelocatedwiththesamecity,havecollected privacy-aware access control mechanism, that enforce access control
datawithdifferentfeatures. toprivacysensitivedata.
Inourearlierwork[38,39],weproposedatwo-phaseMPC-enabled
Fewopen-sourceFLframeworksareavailable.Google’sTensorFlow
FLframeworktoimprovescalability.Wehavecarriedoutpreliminary
Federated(TFF)isalightweightopensourceframework,afirstattempt
work wherein the committee members are selected randomly. Ran-
inthecommunity.Itisdesignedforandroidusers,topredictkeyboard
dom selection may not be efficient as the parties are geographically
next word on their mobile phones [17] using TFF. FATE [18], is
distributedwhichcouldhaveimpactonlatency.Further,weconsider
the federated learning framework developed by Webank. It supports
MPCenabledFLwithhierarchicalmodelaggregationandcarryoutnew
various federated learning methods. Pysyft [19] is a python library
performanceexperimentsandanalysetheresultswithdifferentdatasets
thatonlysupportsFedAvgalgorithm.ItsupportsMPCanddifferential
inthiswork.
privacy.Itcaneitherrunonstand-aloneoronmultiplemachinesand
useswebsocketAPIforthecommunicationbetweendifferentclients. 3. Motivation
Clara[20]isaframeworkforbuildingAIacceleratedmedicalimaging
workflows.Facebook’sprivacypreservingmachinelearningframework MPCisusedforprivacy-preservingmodelaggregationinFLframe-
isCrypTen[21]. work [40]. With MPC protocol, each client splits its locally trained
modelSintonsecretshares,oneforeachoftheparticipatingclients.
These secret shares are exchanged among the peer clients, with each
2.2. Privacy-preservingtechnologies
one holding one share of the every other client’s secret. Then, each
client locally computes the sum of the one piece of the secret share
SecureMPCisaprivacypreservingtechniquethatallowforsecure ofthemodelsfromallclients.Thelocallyaggregatedsecretsharesat
computation over sensitive data. It was firstly introduced by Yao in each client are exchanged further and aggregated to reconstruct the
1986[22].Garbledcircuitsandsecretsharingarethetwodominated modelS.Thetotalnumberofmessagesexchangedbetweennclientsis
MPCtechniques[23]followedtoday.Thesecretsharingisacommonly proportional to (𝑛×(𝑛−1)×2) which is 𝑂(𝑛2). MPC results in higher
used MPC protocol. It splits the sensitive data into secret shares. The communication costs due to the communication and connectivity be-
originaldataisobtainedfromthecombinationofthesesecretshares. tween each client to every other clients in FL framework. The model
Theclientscannotlearnanythingotherthanfinaloutput. aggregation process in traditional MPC enabled FL is illustrated in
In federated learning, the model parameters and gradients are Fig.5.Weconsidersixclients(Node1,Node2,Node3,Node4,Node
5,andNode6)fortheillustration.Theindividualclient’slocalmodels
sharedwiththeserverformodelaggregation.Thereisapossibilitythat
(A,B,C,D, E and F) are privacy-sensitive. They are split into multiple
themalicioususercaninterceptthemodelparametersandcouldper-
secretshares[(A→𝐴 ,𝐴 ,𝐴 ,𝐴 ,𝐴 ,𝐴 ),(B→𝐵 ,𝐵 ,𝐵 ,𝐵 ,𝐵 ,
formreverseengineeringtoextractthesensitivedata,whileitisshared 1 2 3 4 5 6 1 2 3 4 5
𝐵 ),(C→𝐶 ,𝐶 ,𝐶 ,𝐶 ,𝐶 ,𝐶 ),(D→𝐷 ,𝐷 ,𝐷 ,𝐷 ,𝐷 ,𝐷 ),(E→
with the server on federated learning [24,25]. To address this issue, 6 1 2 3 4 5 6 1 2 3 4 5 6
𝐸 ,𝐸 ,𝐸 ,𝐸 ,𝐸 ,𝐸 ),and(F→𝐹 ,𝐹 ,𝐹 ,𝐹 ,𝐹 ,𝐹 )].Thesesecret
Multi party computation methods like additive secret sharing [26] or 1 2 3 4 5 6 1 2 3 4 5 6
sharesareexchangedamongotherclientsinsuchawaythateachclient
Shamirsecret[27]sharingcanbeusedtoencryptthegradients/model
holdsoneshareofeveryotherclient’ssecret.Forexample,Node1holds
updatesbeforeperformingtheaggregationsothatnoonewillbeable
𝐴 , 𝐵 , 𝐶 , 𝐷 , 𝐸 , 𝐹 . Consequently, each client (Node 1) conduct a
1 1 1 1 1 1
toseethegradients.
localaggregationofthesecretshares(G1→𝐴 +𝐵 +𝐶 +𝐷 +𝐸 +
1 1 1 1 1
DifferentialPrivacy(DP)isadoptedbyresearchersrecently[28]to 𝐹 ).TheselocallyaggregatedsecretsharesG1,G2,G3,G4,G5,andG6
1
ensure data security and confidentiality. Data privacy is protected by at corresponding client are exchanged and globally aggregated (S →
adding random noise to data. The introduction of noise results in a 𝐺 +𝐺 +𝐺 +𝐺 +𝐺 +𝐺 )tocancelouttherandomnessandthus
1 2 3 4 5 6
compromise between security and accuracy. It is not suitable for FL reconstructthelocalmodelsofallclients.Thetotalnumberofmessages
asaddingnoisetomodelparameterdatafromeachparticipatingparty exchangedbetweensix clientsisproportionalto(6×(6−1)×2)which
mayaffecttheaccuracyofglobalmodel[29]. is60.
3R.Kanagaveluetal. Array15(2022)100207
Fig.5. MPC-enabledmodelaggregation.
clients in the same group are securely aggregated using MPC to form
a intra-group model. Based on the latency, one client is elected from
eachgrouptoformtheaggregationcommittee.Inthesecondphase,the
committeemembersworktogethertoaggregatetheinter-groupmodels
usingMPC.Ithasthreemodules:
(1) Group leader election: This module selects the group leader
basedonthelatenciesfromtheleadertoalltheotherclientsin
thesamegroup.
(2) Modelaggregationcommitteeselection:Thismoduleselects
theaggregationcommitteemembersfromgroupleaders
(3) Hierarchical model aggregation module: This module uses
Fig.6. Communicationoverhead. committeebasedapproachforglobalmodelaggregation
Wefirstdiscussthegroupleaderselectionandpresentthetermsused
Wehavecarriedoutapreliminaryexperimentalstudytogetinsights in our implementation. Then, we present the aggregation committee
into how the consideration of the number of clients impacts on the membersselectionmethodandlastwedescribehowcommitteebased
communication overhead in privacy preserving FL using traditional modelaggregationhelpstoreducethecommunicationoverhead.Some
MPC. We evaluate the performance on MNIST dataset [41], using importantvariablesusedinourframeworkaredescribedinTable1.
the following parameters: total number of clients participated(N) =
varying from 4 to 128; Local epoch(E)=10; Learning rate(lr)=0.01; 4.1. Groupleaderelection
batch size=10; number of communication rounds(T)=50. The com-
munication overhead for two scenarios: FL with traditional MPC and
TherearelargenumberofclientsinvolveinFL.Thecommunication
FL without MPC is shown in Fig. 6. It is observed from Fig. 6 that
between the clients in the decentralized FL poses unique challenges
the number of communicated messages in MPC enabled FL increases
withanincreasingnumberofclients.Inafederatedlearningenviron-
withincreasingnumberofclients,aseachclientexchangetheirsecret
ment, participating clients can be either located in the same region
shares of model parameters with all the other clients during each
or geographically far apart in different locations(countries). In a de-
communicationround.Theresultsshowthat,thenumberofmessages
centralizedenvironment,thererequiresastrongco-ordinationamong
exchanged is 5×–10× more when compared with FL without MPC.
the clients to achieve better performance. If the participating clients
ThesefindingsmotivatedustoproposeanovelFLframeworktoreduce
are located at geographically distributed locations, they have higher
thecommunicationcostincurredintraditionalMPCenabledFederated
Learning. inter-node latencies, which in turn incurs long delay. It requires the
magnitude of hundreds of milliseconds or even seconds to transfer
4. CE-Fedframework the data across multi-geographic locations. The client with longest
delayisthebottleneckwhichforcesallotherclientstowaitformodel
In this section, we describe the proposed CE-Fed framework as aggregation. If this communication lag is not handled properly will
shown in Fig. 7, with the objective of reducing the traditional MPC affect the scalability of the learning algorithm. To alleviate this, we
communicationcost. propose a clustering approach that groups the clients based on their
The proposed CE-Fed selects a few clients as the committee mem- response latency and ensure that all clients in the same group have
bers who use MPC service to aggregate the local models of all FL minimuminter-nodelatency.
clientsinahierarchicalmanner.Thereforeitavoidssharingthemodel Aftergroupingtheclients,oneclientfromthegroupiselectedasa
parameterfromeachclienttoeveryoneelseintheFLlist.Ourproposed leader.Insteadoflettingeveryonetosharesecretsharestoeveryother
CE-Fedisexecutedintwophases.Inthefirstphase,wegrouptheFL one, we choose one representative from each group to communicate
clientsthatarelocatedclosetoeachother.ThelocalmodelsofallFL with other groups. This reduces communication overhead. The group
4R.Kanagaveluetal. Array15(2022)100207
Fig.7. CE-Fedframework.
Table1
Variablesinfederatedlearningframework.
Category Symbol Type Description
𝑛 Int No.ofparticipatingFLclients
𝑎 Int Globalmodeltrainingiterations
Federatedlearning
𝑏 Int Localmodeltrainingrounds
𝑏𝑆𝑖𝑧𝑒 Int BatchSizeofeachcommunicationround
𝑚 Int Electedcommitteemembers
Committee
𝐶 Array committeemembersList
𝑇 Tensor Localmodelparameters
CNNmodel 𝑊 Tensor Parameters/weightsofaggregatedmodel
𝑠 Integer Thesizeofparameters/weightsoflocal/aggregatedmodels
𝐺 Int NumberofmutuallyexclusiveGroups
𝑘 Int Numberofclientsingroup
Network 𝑆𝐺 Array GroupmembersList
𝐿 Int inter-nodelatency
𝑄 Array MutuallyexclusiveGroupLeadersList
leaderisselectedbasedontheaveragelatencywiththeotherclients. 4.2. Aggregationcommitteeselection
Each client is assumed to be at fixed position and the estimated la-
Tofurtherreducethecommunicationoverhead,thesubsetofgroup
tency between any pair of clients is known in advance. The elected
leaders from different groups will be elected to form the aggrega-
group leader may have a chance to participate in the global model tion committee. The committee members jointly working together to
aggregation. There is only one leader in the group and rest of them securely aggregate the different group local models using the MPC
are his followers. The leader sends Alive messages to its followers at service. There are several ways to form committee for secure model
aggregation. When all clients are placed in a close proximity to each
frequentintervals.IfthereisnoAlivemessagefromtheleaderwithin
other in a local area setting, the committee members can be selected
a particular time interval, the client with minimal inter-node latency based on the static rules [42,43]. This is not applicable when the
to the other followers will be elected as a new leader. The pseudo clientsarelocatedindifferentgeographicalsites,wheredifferentlinks
in the system have significantly difference latencies. Our committee
codeofgroupformationandleaderelectionisshowninAlgorithm1.
selectionisbasedonalatencywheresubsetofgroupleaders(moutof
Ouralgorithmconsidersmanymutuallyexclusivegroupswithvarying
ngroupleaders)arechosenasmemberswithlowlatencies(inter-node
numberofclients. latency)toothergroupsthatincreasestheefficiencyofthecommittee
Algorithm1 GroupLeaderElection selection. The elected leaders continuously communicate with other
1: Function{LeaderSelection(G,k,m,C)} group leaders and monitor their latencies with them. We consider
2: GroupInitialization the aggregation committee with m members ((m ≥ 3), based on the
3: Readthevalue assumptionthatthereisnocollusionamongthecommitteemembers.
4: for𝑖∈[1,𝐺]do
5: 𝑚𝑖𝑛←𝐿𝑎𝑡(𝑆𝐺[1]) 4.3. MPCenabledhierarchicalmodelaggregation
6: for𝑗∈[1,𝑘]do
7: if𝐿 <𝑚𝑖𝑛then
j The model aggregation is happening in two phases as shown in
8: 𝐿𝑒𝑎𝑑𝑒𝑟=𝑗
Fig.8:
9: endif
10: 𝑄←𝐿𝑒𝑎𝑑𝑒𝑟
• Intra-groupmodelaggregationand
11: endfor
12: endfor • Inter-groupaggregation.
13: EndFunction
5R.Kanagaveluetal. Array15(2022)100207
Fig.8. CE-FedHierarchicalModelAggregation.
4.3.1. Intra-groupmodelaggregation model to the model aggregation committee for global collaborative
Duringthetrainingprocess,alltheclientsinthesamegroupstart learning.Thepseudocodeofintra-groupmodelaggregationisshown
trainingtheirmodelslocallyusingtheirlocaldatasets.Eachclientina inAlgorithm2.
grouphastwomaintasks: The communication cost of intra-group model aggregation is es-
timated based on the number of messages exchanged between the
• Local training on its local data : After extracting the features clients. It can be calculated by adding the number of messages ex-
fromrawdata,theclient’sdataanalyticsmoduleperformslocal changed between clients in all groups. The variables used in our
trainingonitslocaldata. calculationsaredescribedinTable1.Thenumberofmessagesdenoted
• MPCEnabledintra-groupModelAggregation:Thelocallytrained as𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚 exchangedinthe𝑖thgroupduringintra-group
𝑖
modelparametersaresplitintosecretsharesandsecurelyaggre- modelaggregationiscalculatedasfollows:
gatedusingMPCprotocol.
𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚 𝑖=(𝑘 𝑖×(𝑘 𝑖−1))×2×𝑎 (1)
Initially,eachclientlocallytrainstheirlocaldatasetforfewitera-
Thesizeofmessagesdenotedas𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒 exchangedinthe
tions, till the convergence criterion is met. After completing the local 𝑖
𝑖thgroupduringintra-groupmodelaggregationisgivenby
training, the clients in the group collaboratively learn the model in a
peer-to-peerfashion.Asthereisnocentralserveravailabletoinitiate 𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒 =𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚 ×𝑠 (2)
𝑖 𝑖
the model aggregation process in the decentralized environment, any
random client 𝐴 in the group initiates that process. The tensor 𝐓 of The total number of messages denoted as 𝑇𝑜𝑡𝑎𝑙_𝐼𝑛𝑡𝑟𝑎_𝑀𝑠𝑔_𝑛𝑢𝑚
𝑖
exchanged in theintra-groupmodel aggregation among allgroups(G)
individual local model parameters/weights are privacy-sensitive. To
isgivenby
preserve privacy, MPC secret sharing technique is used to securely
aggregate the tensor 𝐓, without sacrificing accuracy. The tensor 𝐓 is 𝐺
∑
splitintomultipletensorsasthesecretsharesbasedonthenumberof 𝑇𝑜𝑡𝑎𝑙_𝐼𝑛𝑡𝑟𝑎_𝑀𝑠𝑔_𝑁𝑢𝑚= 𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚 𝑖 (3)
clients in the same group. Each client holds one share and exchanges 𝑖=1
theremainingshareswithalltheotherparticipatingclientsinthesame The size of messages denoted as 𝑇𝑜𝑡𝑎𝑙_𝐼𝑛𝑡𝑟𝑎_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒 exchanged in
group. Then, clients perform secure model aggregation of the secret theintra-groupmodelaggregationamongallgroupsisgivenby
sharesusingMPCtoobtaintheirintra-groupmodel. Afterlearninga 𝐺
∑
Algorithm2 Intra-groupModelAggregation
𝑇𝑜𝑡𝑎𝑙_𝐼𝑛𝑡𝑟𝑎_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒= 𝐼𝑛𝑡𝑟𝑎_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑠𝑖𝑧𝑒
𝑖
(4)
𝑖=1
1: Function{Intra-Group.Model.aggregation(n,u,v,m)}
2: for𝑛∈[1,𝑢]do 4.3.2. Inter-groupmodelaggregation
3: for𝑎∈[1,𝑣]do Theoptimizedintra-groupmodelsfromdifferentgroupsarefinally
4: for𝑏∈[1,𝑧]do aggregatedbythemodelaggregationcommitteememberstoformthe
5: 𝑇 ←𝑙𝑜𝑐𝑎𝑙𝑡𝑟𝑎𝑖𝑛𝑖𝑛𝑔 inter-group global model. Each aggregation committee member splits
6: endfor theintra-groupmodelreceivedfromdifferentgroupleadersintomse-
7: for𝑡∈[1,𝑢]do
cretsharesusingMPC.Itholdsoneshareandexchangestheremaining
8: Splitlocally-trainedmodelintousecretshares)
shareswithalltheothercommitteemembers.Thecommitteemembers
9: shareitwithmembersofaggregationcommittee
worktogethertoperforminter-groupglobalmodelsecureaggregation
10: endfor
usingMPCandbroadcasttheaggregatedglobalmodeltoallthegroup
11: endfor
leadersinvariousgroups.Then,thegroupleadersbroadcasttheinter-
12: 𝑆=𝑠𝑢𝑚𝑡ℎ𝑒𝑠𝑒𝑐𝑟𝑒𝑡𝑠ℎ𝑎𝑟𝑒𝑠
13: for𝑎∈[1,𝑢]&𝑎≠𝑥do group model to their followers in their group. The whole process of
14: Broadcast𝑆 topeerclients intra-groupmodelaggregationandinter-groupmodelaggregationare
15: Receive𝑆 frompeerclients repeated until the model converges. The pseudo code of inter-group
16: endfor modelaggregationisshowninAlgorithm3.
17: 𝑊 ←𝐼𝑛𝑡𝑟𝑎−𝑔𝑟𝑜𝑢𝑝𝐴𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑒𝑑𝑀𝑜𝑑𝑒𝑙
18: endfor The communication cost of inter-group model aggregation is esti-
goodintra-groupmodel,thegroupleadersenttheirgroup’sintra-group matedbasedonthenumberofmessagesexchangedbetweenthemodel
6R.Kanagaveluetal. Array15(2022)100207
Algorithm3Inter-groupModelAggregation 5.2. Communicationcost
1: Function{Inter-group.Model.aggregation(Q,m,n)}
2: for𝑄∈[1,𝑚]do The communication cost is evaluated in terms of the number of
3: 𝑇 ←𝐼𝑛𝑡𝑟𝑎−𝑔𝑟𝑜𝑢𝑝𝑀𝑜𝑑𝑒𝑙 the shares(messages) as well as the size of messages communicated
4: endfor betweentheclientswiththreedifferentdatasets:MNIST,CIFAR-10and
5: for𝑄∈[1,𝑚]do fashion-MNIST as shown in Fig. 9, Fig. 10 and Fig. 11. We compare
6: SplitIntra-groupmodelintomsecretshares theperformanceofourproposedCE-FedwithtraditionalMPCenabled
7: shareitwithmembersofaggregationcommittee FLframework(FL+traditionalMPC),traditionalFL(FL)andtwo-phase
8: endfor
MPCenabledFL(Two-phase)[38].
9: for𝑥∈[1,𝑚]do
10: S=sumthesecretshares
11: for𝑛∈[1,𝑚]&𝑎≠𝑥do 5.2.1. MNISTdataset
12: Broadcast𝑆 topeerclients We use two different data distributions of MNIST over clients. In
13: Receive𝑆 frompeerclients IndependentandIdenticallyDistributed(IID)datasettings,eachclient
14: endfor has balanced and identical data points. For IID data distribution, we
15: 𝑊 ←𝐺𝑙𝑜𝑏𝑎𝑙𝐴𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑒𝑑𝐼𝑛𝑡𝑒𝑟−𝐺𝑟𝑜𝑢𝑝𝑀𝑜𝑑𝑒𝑙 first shuffle the data and equally divide among the clients. However,
16: endfor in practical scenarios, the datasets at clients are more likely to be
17: return 𝑊 non-uniformandunbalanced(non-IID).Fornon-IIDdataheterogeneous
18: EndFunction
distribution, we distribute the data among the clients with different
proportions.
The accuracy over communication rounds for IID and non-IID are
aggregationcommitteemembers.Thenumberofmessagesdenotedas shown in Fig. 9(a) and Fig. 9(d). It takes significant more rounds
𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚 exchanged in the inter-group model aggregation (around10×)toreach98%testaccuracyinnon-IIDthaninIIDcases,
amongmaggregationcommitteememberscanbecalculatedasfollows: which indicates that non-IID data indeed can slow down the conver-
gence compared with IID cases. It also observed that the accuracy is
𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚=(𝑚×(𝑚−1))×2×𝑎 (5)
affectedbynon-IIDdataheterogeneity.
Thesizeofmessagesdenotedas𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒exchangedinthe InthetraditionalMPCenabledFL,eachclientsharesitssecretshare
intra-groupmodelaggregationis with every other client to reconstruct the global joint model. Conse-
quently,thenumberofsecretsharesexchangedisdirectlyproportional
𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑆𝑖𝑧𝑒=𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚×𝑏𝑆𝑖𝑧𝑒 (6) to the number of clients in the MPC enabled FL which is 0(𝑛2). It
The hierarchical model aggregation reduces the number of shares resultsinlotsofcommunicationoverheadasanalysedinSection2.1.In
exchanged across all participating clients, because secret shares are thetraditionalFL,eachclientsharesitsmodelparameterswithevery
shared only with the elected committee members m, where m ≪ n. otherclientforglobalmodelaggregation.Thenumberandthesizeof
Thetotalnumberofmessagesexchangedbetweenclientsisdenotedas messages increase with the increasing number of clients. In the two-
Total_Msg_Numandiscalculatedasfollows: phaseMPCenabledFL[38],alltheparticipatingclientsformasingle
groupandcommitteemembersareselectedrandomlyfromthatgroup
𝑇𝑜𝑡𝑎𝑙_𝑀𝑠𝑔_𝑁𝑢𝑚=𝑇𝑜𝑡𝑎𝑙_𝐼𝑛𝑡𝑟𝑎_𝑀𝑠𝑔_𝑁𝑢𝑚+
(7) forglobalmodelaggregation.Thenumberandthesizeofmessagesare
𝐼𝑛𝑡𝑒𝑟_𝐺𝑟𝑝_𝑀𝑠𝑔_𝑁𝑢𝑚
much lower than that in MPC enabled FL. However, in our proposed
Thetotalnumberofmessagesexchangedisproportionalto𝑂(𝑘2)+ FL with aggregation committee method(CE-Fed), multiple groups are
𝑂(𝑚2), is very few when compared with traditional MPC enabled FL formed based on the client’s geographical locations and one leader is
method[O(n2)]wherek,m≪n. electedfromeachgrouptoparticipateintheglobalmodelaggregation.
Allclientssharetheirsecretsharesonlywiththecommitteemembers.It
5. Experimentandresults reducesthenumberofmessagesexchangedsignificantlybyanaverage
of 90% in both IID and non-IID cases as shown in Fig. 9(b) and
5.1. Experimentalsetup Fig.9(e)respectively.Itthusdemonstratesthatourproposedmethod
hasmuchlowercommunicationcostandbetterscalability.Weobserve
ToimplementandevaluatetheeffectivenessofourproposedCE-Fed thatthemoremessages,1˜0×,areexchangedinthenon-IIDcasethan
framework,weusedPyTorch1.2.0andPython3.74asmachinelearn-
in the IID case as the non-IID takes more communication rounds to
ing library. We considered three public image datasets, MNIST [44],
achieve maximum test accuracy than IID cases as shown in Fig. 9(a)
CIFAR-10 [45] and Fashion-MNIST dataset [41]. We used the CNN
andFig.9(d).
modelthatconsistsoftwo5×5convolutionlayers.Thefirstlayerhas
Another metric we consider to evaluate the communication cost
32channelsandthesecondonehas64channels.Eachlayerisfollowed
is the size of messages exchanged between clients and committee
with 2 × 2 max pooling, with ReLu activation, and a final softmax
members. Large number of messages exchanged between clients and
output layer. The model has about 600,000 trainable parameters. In
committee members results in large amount of data transferred be-
ourexperiments,westudytheperformanceandeffectivenessinusing
tween them. We conduct the experiment by varying the number of
IID(Independentandidenticallydistributed)andnonIIDdatasets.In
clientsfrom4to128.IthasbeenobservedthatourproposedCE-Fed
theIIDdistribution,thedataisshuffledandpartitionedacrossallthe
significantlyreducesthecommunicationcostbyanaverageof90%in
clients,whereasinthenon-IIDdistribution,firstthedataissortedby
bothIIDandnon-IIDdistributionsasshowninFig.9(c)andFig.9(f).
the label and then partitioned across the clients in such a way that
eachpartyhasfixednumberoflabelsandthereisnooverlapbetween
samples of different clients. To eliminate the randomness caused by 5.2.2. CIFAR-10dataset
client sampling, all clients are participated in each and every round To further evaluate the effectiveness of our proposed FL, we run
oftraining.Thelearningrateissetto0.01,localepochnumberisset experiments on CIFAR-10 datasets. For IID setting, we partitioned
to5andbatchsizeissetto64.Thestochasticgradientdescent(SGD)is the 50,000 training examples equally among the clients. For non-IID
used as optimizer. We consider 4 groups and use FedAvg for model settings,wesharedthedatasetunevenlyandlimitthenumberofclasses
aggregation.WeuseAccuracyandcommunicationcostasperformance to 2 per client. The accuracy over communication rounds for IID and
metricsinourexperiments. non-IIDareshowninFig.10(a)andFig.10(d).Ittakessignificantmore
7R.Kanagaveluetal. Array15(2022)100207
Fig.9. MNISTdataset.
Fig.10. CIFAR-10dataset.
roundstoconvergewhencomparedwithMNISTdatasetbothinnon-IID distribute among the clients. For Non-IID data, the dataset is par-
case,whichindicatesthatnon-IIDhasstrongerdataheterogeneity. titioned into 200 shards of 300 samples, and each client randomly
There is a rapid increase in the number of shares exchanged by picks two shards. The accuracy for IID and Non-IID Fashion-MNIST
an average of 80%–90% with the increasing number of clients in the (FMNIST) datasets are shown in Fig. 11(a) and Fig. 11(d) and it is
MPCenabledFLwhencomparedwithourproposedFLwithcommittee closetoaccuracyofMNISTdataset.Thecommunicationcostinterms
selection as shown in Fig. 10(b), and Fig. 10(e). This shows that the of number of messages are shown in Fig. 11(b) and Fig. 11(e). Our
traditional MPC enabled FL is not scalable when more clients joined proposed CE-Fed method reduces the number of messages exchanged
the FL. It proves that our method is scalable to support more clients byanaverageof80%–90%inbothIIDandnon-IIDcases.Itsharesthe
participated in FL. Our proposed CE-Fed reduces the communication similarobservationswithMNISTdatasets.
The communication cost results in terms of size of messages are
costbyanaverageof80%–90%whencomparedwithTwo-phaseMPC
showninFigs.11(c)and11(f).Ithasbeenobservedthatintraditional
enabledFLmethod[38].
MPC enabled FL, the communication cost significantly increases with
CIFAR-10andMNISTdatasetshavethesimilarobservationswhen
increasing number of clients in both IID and non-IID distributions. It
consider the size of messages exchanged between clients. It has been
sharesthesameobservationswithMNISTandCIFAR-10datasets.
observed that our CE-Fed method reduces the communication cost
From the above experimental analysis, it has been observed that
significantlywithincreasingnumberofclientsbyanaverageof80%–
ourproposedCE-Fedreducesthecommunicationcostsignificantlywith
90%inbothIIDandnon-IIDdistributionsasshowninFig.10(c)and
increasingnumberofclients.
Fig.10(f).ItprovesthatourproposedCE-Fedmethodismoreefficient
inincurringlowcommunicationoverhead.
5.3. Accuracy
5.2.3. Fashion-MNISTdataset TostudytheeffectivenessofourproposedFL,theaccuracyscoreof
TheeffectivenessofourproposedFL isstudied onFashion-MNIST theproposedCE-Fedmethodiscomparedwithcentralizedtrainingand
dataset.ForIIDdatadistribution,wefirstshufflethedataandequally localtrainingasshowninFig.12.Inlocaltraining,modelsaretrained
8R.Kanagaveluetal. Array15(2022)100207
Fig.11. Fashion-MNISTdataset.
Fig.12. Accuracy.
Fig.13. Accuracy(Varyingnoofclients)-Comparison.
using local datasets of individual clients. Centralized training trains clients.IthasbeenobservedthattheproposedCE-Fedmethodreaches
the modeloncentralizeddata. Experimentresults showsthat thetest thestableaccuracyevenwhenmoreandmoreclientsjoinedthesystem.
accuracyoffederatedlearningiscomparablewithcentralizedlearning Thisshowsthatourproposedmethodisscalable.
and outperforms local training. It has been observed that FL training
achieves an improvement in accuracy of 22%, 33%, 17% on MNIST, 6. Conclusion
CIFAR-10andFashion-MNISTrespectivelywhencomparedwithlocal
training.Thekeyreasonforthelowaccuracyinlocaltrainingisthat Inthispaper,WeproposedandimplementedaCE-Fedframework
thesizeofthelocaldatasetisnotlargeenough. with hierarchical MPC based model aggregation. It supports multiple
Fig.13showstheaccuracyscoreofproposedCE-FedoverMNIST, partiestocollectivelylearnamachinelearningmodelinaprivacypre-
CIFAR-10 and Fashion MNIST datasets by varying the number of servingmanner.Ourproposedhierarchicalmodelaggregationreduces
9R.Kanagaveluetal. Array15(2022)100207
thecommunicationcostincurredinMPCprotocolsandalsoaggregates [15] McMahan B, Ramage D. Federated learning: Collaborative machine learning
themodelsinaprivacy-preservingmannerwithoutcompromisingthe without centralized training data. 2017, https://ai.googleblog.com/2017/04/
accuracy. The effectiveness of our proposed CE-Fed framework was federated-learning-collaborative.html.Google.AIBlog.
[16] Yang Q, Liu Y, Chen T, Tong Y. Federated machine learning: Concept and
demonstrated on various datasets. The above experiments indicates
applications.ACMTransIntellSystTechnol(TIST)2019.
that our proposed CE-Fed is able to reduce the communication cost
[17] Tensor flow federated. 2020, Retrieved from https://www.tensorflow.org/
significantlywhileachievethesimilaraccuracyandprivacy. federated/.Last[AccessedJuly2020].
[18] WebankFATE(FederatedAITechnologyenabler).2020,Retrievedfromhttps:
CRediTauthorshipcontributionstatement //github.com/FederatedAI/FATE.Last[AccessedAugust2020].
[19] OpenMined/PySyft-A library for encrypted, privacy preserving deep learning.
2020, Retrieved from https://github.com/OpenMined/PySyft. Last [Accessed
Renuga Kanagavelu: Conception and design of study, Writing –
August2020].
originaldraft.QingsongWei:Conceptionanddesignofstudy,Writing
[20] Softwareguard-extensions. 2020, Retrieved from https://en.wikipedia.org/wiki/
–originaldraft.ZengxiangLi:Conceptionanddesignofstudy.Haibin Software_Guard_Extensions.Last[AccessedAugust2020].
Zhang: Revising the manuscript critically for important intellectual [21] CrypTen documentation. 2020, https://crypten.readthedocs.io/en/latest/. Last
content. Juniarto Samsudin: Revising the manuscript critically for [AccessedAugust2020].
importantintellectualcontent.YechaoYang:Revisingthemanuscript [22] YaoAC.Howtogenerateandexchangesecrets.In:27thannualsymposiumon
foundationsofcomputerscience.IEEE;1986,p.162–7.
critically for important intellectual content. Rick Siow Mong Goh:
[23] Choi Joseph, Butler Kevin. Secure multiparty computation and trusted hard-
Conceptionanddesignofstudy.ShangguangWang:Conceptionand
ware Examining adoption challenges and opportunities. Secur Commun Netw
designofstudy. 2019;1–28.http://dx.doi.org/10.1155/2019/1368905.
[24] Fredrikson M, Jha S, Ristenpart T. Model inversion attacks that exploit con-
Declarationofcompetinginterest fidence information and basic counter measures. In: Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security. 2015, p.
1322–33,[Online].Available:http://dx.doi.org/10.1145/2810103.2813677.
The authors declare that they have no known competing finan-
[25] Ni Q, Bertino E, Lobo J, Brodie C, Karat C-M, Karat J, Trombetta A.
cial interests or personal relationships that could have appeared to Privacy-awarerole-basedaccesscontrol.ACMTransInfSystSecur2010;13(3).
influencetheworkreportedinthispaper. [26] Dahl Morten. Secret Sharing, Part 1 - Distributing trust and work, https://
mortendahl.github.io/2017/06/04/secret-sharing-part1/.
Acknowledgements [27] ShamirA.Howtoshareasecret.CommunACM1979;22(11):612–3.
[28] Choudhury livia, Gkoulalas-Divanis Aris, Salonidis Theodoros, Sylla Issa,
ParkYoonyoung,HsuGrace,DasAmar.Anonymizingdataforprivacypreserving
Allauthorsapprovedthefinalversionofthemanuscript.
federatedlearning.2020,arXivpreprintarXiv:2002.09096.
[29] Truex Stacey, Baracaldo Nathalie, Anwar Ali, Stein-ke Thomas, Ludwig Heiko,
References ZhangRui,ZhouYi.Ahybridapproachtoprivacy-preservingfederatedlearning.
In:Proceedingsofthe12thACMworkshoponartificialintelligenceandsecuri-ty,
[1] Ding C, Zhou A, Liu L, Ma X, Wang S. Resource-aware feature extraction in AISec@CCS.2019.
mobileedgecomputing.IEEETransMobComput2020. [30] KairouzP,McMahanHB,AventB,BelletA,BennisM,BhagojiAN,KBonawitz,
[2] Lian Xiangru, Zhang Ce, Zhang Huan, Hsieh Cho-Jui, Zhang Wei, Liu Ji. Can Charles Z, Cormode G, Cummings R, et al. Advances and open problems in
decentralized algorithms outperform centralized algorithms? A case study for federatedlearning.2019,arXivpreprintarXiv:1912.04977.
decentralizedparallelstochasticgradientdescentIn:NIPS.2017. [31] Mansour Y, Mohri M, Ro J, Suresh AT. Three approaches for personalization
[3] Ren H, Li H, Dai Y, Kan Y, Lin X. Querying in internet of things withapplicationstofederatedlearning.2020,arXivpreprint:2002.10619.
with privacy preserving: Challenges, solutions and opportunities. IEEE Netw [32] XieM,LongG,ShenT,ZhouT,WangX,JiangJ.Multi-centerfederatedlearning.
2018;32(6):144–51. 2020,arXiv:2005.01026.
[4] GarciaY,KassaM,CuevasA,CebrianM,MoroE,RahwanI,CuevasR.Analyzing [33] Xie Ming, Long Guodong, Shen Tao, Zhou Tianyi, Wang Xianzhi, Jiang Jing.
genderinequalitythroughlarge-scalefacebookadvertisingdata.ProcNatlAcad Multi-centerfederatedlearning,arXivpreprint:arXiv:2005.01026.
Sci2018;115(27):69586963. [34] LiuYang,ChenTianjian,YangQiang.Securefederatedtransferlearning.IEEE
[5] McMahan B, Ramage D. Federated learning: Collaborative machine learning IntellSyst2020;35(4):70–82.
without centralized training data. 2017, https://ai.googleblog.com/2017/04/
[35] Daily J, Vishnu A, Siegel C, Warfel T, Amatya V. GossipGraD: Scalable deep
federated-learning-collaborative.html.GoogleAIBlog.
learningusinggossipcommunicationbasedasynchronousgradientdescent.2018,
[6] Roy bhijit Guha, Siddiqui Shayan, Polsterl Sebastian, Navab Nassir,
arXiv:1803.05880.
WachingerChristian.BrainTorrent:Apeer-to-peerenvironmentfordecentralized
[36] WangJ,SahuAK,YangZ,JoshiG,KarS.MATCHA:Speedingupdecentralized
federatedlearning.2019,https://arxiv.org/pdf/1905.06731.pdf.
SGDviamatchingdecompositionsampling.2019,arXiv:1905.09435.
[7] ShokriR,StronatiM,SongC,ShmatikovV.Membershipinferenceattacksagainst
[37] LalithaA,KilincOC,JavidiT,KoushanfarF.Peer-to-peerfederatedlearningon
machine learning models. In: Proceedings of IEEE symposium on security and
graphs.2019,arXiv:1901.11173.
privacy.2017.
[8] Nasr M, Shokri R, Houmansadr A. Comprehensive privacy analysis of deep [38] KanagaveluRenuga,LiZengxiang,SamsudinJuniarto,YangYechao,YangFeng,
learning:stand-aloneandfederatedlearningunderpassiveandactivewhite-box Goh Rick Siow Mong, Cheah Mervyn, Wiwatphonthana Praewpiraya, Akkara-
inference attacks. In: Proceedings of IEEE ACM symposium on security and jitsakul Khajonpong, Wang Shangguang. Two-phase multi-party computation
privacy.2019. enabledprivacy-preservingfederatedlearning.In:IEEECCGRID2020.p.410–9.
[9] BonawitzK,IvanovV,KreuterB,MarcedoneA,McMahanHB,PatelS,RamageD, [39] Kanagavelu Renuga, Li Zengxiang, Samsudin Juniarto, Hussain Shaista,
Segal A, Seth K. Practical secure aggregation for privacypreserving machine Yang Feng, Yang Yechao, Mong Rick Siow, Cheah GohMervyn. Federated
learning.In:ProceedingsofACMcomputerandcommunicationssecurity.2017. learningforadvancedmanufacturingbasedonindustrialIoTdataanalytics,pp
[10] AlexandraW,AltmanM,BembenekA,BunM,GaboardiM,HonakerJ,NissimK, 143-176,ImplementingIndustry4.0.
OBrienR,SteinkeT,VadhanS.Differentialprivacy:Aprimerforanon-technical [40] ZyskindG,NathanO,PentlandA.Enigma:Decentralizedcomputationplatform
audience.VanderbiltJEntertainTechnolLaw2018;21(1):209–75. withguaranteedprivacy.https://arxiv.org/abs/1506.03471.
[11] EvansD,KolesnikovV,RosulekM.Apragmaticintroductiontosecuremultiparty [41] Fashion-MNISTDataset,https://github.com/zalandoresearch/fashion-mnist.
computation.NOWPublishers;2018. [42] AguileraMK,Delporte-GalletC,FauconnierH,TouegS.Stableleaderelection.
[12] Chillotti I, Georgieva M, Izabachne M. Faster packed homomorphic operations In:Proceedingsofthe15thinternationalconferenceondistributedcomputing.
andefficientcircuitbootstrappingforTFHE.In:Advancesincryptology,PartI. Springer-Verlag;2001,p.108–22.
Vol.10624.Lecturenotesincomputerscience,2017,p.377–408. [43] AguileraMK,Delporte-GalletC,FauconnierH,TouegS.Communication-efficient
[13] Phong LT, Aono Y, Hayashi T, Wang L, Moriai S. Privacy preserving deep leaderelectionandconsensuswithlimitedlinksynchrony.In:Proceedingofthe
learningviaadditivelyhomomorphicencryption.IEEETransInfForensicsSecur 23rdannualACMsymposiumonprinciplesofdistributedcomputing.St.John’s,
2018;13(5):1333–45. Newfoundland,Canada,ACMPress;2004,p.328–37.
[14] Ivan D, Ishai Y, Mikkel K. Perfectly secure multiparty computation and the
[44] MNISTDataset,https://csc.lsu.edu/saikat/n-mnist/.
computational overhead of cryptography. In: Proceedings of the 29th annual
[45] CIFAR-10Dataset,https://www.cs.toronto.edu/kriz/cifar.html.
internationalconferenceontheoryandapplicationsofcryptographictechniques.
2010.
10R.Kanagaveluetal. Array15(2022)100207
JuniartoSamsudinisaresearchengineerattheInstituteof
RenugaKanagavelureceivedthePh.D.degreeincomputer High-Performance Computing, A*STAR, Singapore. He got
sciencefromNanyangTechnologicalUniversity,Singapore, hismaster’sdegreeinSmartProductDesignfromNanyang
in 2015. She is a scientist at the Institute of High- Technological University, Singapore, 2007. His current
Performance Computing, A*STAR, Singapore. Her research projects include federated learning, HPC, IoT and cloud
interests include Federated Learning, Privacy-preserving infrastructureandobjectdetectionusingdeep-learning.
techniques,IndustrialIoTandEdgeAnalytics.
Yechao Yang received the bachelor’s degree in mechan-
Qingsong Wei received the Ph.D. degree in computer ical and electronic engineering from the University of
sciencefromtheUniversityofElectronicScienceandTech- Science and Technology of China, in 1998. He is a re-
nologiesofChina,in2004.HewaswithTongjiUniversity searchengineerinInstituteofHigh-PerformanceComputing
as an assistant professor from 2004 to 2005. He is a (IHPC), A*STAR, Singapore. His research interests include
Group Manager and senior research scientist at the Insti- Blockchain,IoT,andFederatedLearning.
tute of High-Performance Computing, A*STAR, Singapore.
Hisresearchinterestsincludedecentralizedcomputing,fed-
erated learning, high performance computing, emerging
non-volatile memory and storage system. He is a senior
memberoftheIEEE.
Rick Siow Mong Goh is the Director of the Computing
Science Department at the A*STAR Institute of High-
ZengxiangLiisExecutiveVicePresidentofDigitalResearch Performance Computing (IHPC). He received his Ph.D.
InstituteandDirectorofCollaborativeIntelligenceLab,ENN degree in electrical and computer engineering from the
Group, Beijing, China. His research group is focusing on National University of Singapore. At IHPC, he leads a
Industrial IoT, Blockchain and Federate Learning research teamofmorethan60scientistsinperformingworldleading
anddevelopmentfortrustedecosystemacrossmultiplein- scientific research, developing technology to commercial-
dustrydomains,includingsmartenergy,healthcare,andcity ization,andengagingandcollaboratingwithindustry.The
governanceapplicationdomains.HecompletedhisPh.D.in researchfocusareasincludeartificialintelligence(AI),high
NanyangTechnologicalUniversity,Singaporein2010. performancecomputing(HPC),distributedcomputing,ma-
chine & deep learning, complex systems, human–machine
interaction,andmodellingandsimulation.
Shangguang Wang is a Professor at the School of Com-
HaibinZhangreceivedthebachelor’sdegreesofComputer puting,BeijingUniversityofPostsandTelecommunications,
Science and Animal Science from Shandong Agriculture China.HeisaVice-DirectoroftheStateKeyLaboratoryof
University of China, in 2010. He was with NEC Corp and Networking and Switching Technology. He has published
Alibaba Group as solution engineers to lead the team for more than 150 papers, and his research interests include
researching and development from 2018 to 2020. He is a servicecomputing,cloudcomputing,andmobileedgecom-
researchengineerattheInstituteofHighPerformanceCom- puting.HeservedasGeneralChairsorTPCChairsofIEEE
puting, A*STAR Singapore. His research interests include EDGE 2020, IEEE CLOUD 2020, IEEE SAGC 2020, IEEE
federatedlearning,blockchain,high-performancecomputing EDGE2018,andIEEEICFCE2017,etc.,andVice-Chairof
anddistributedstoragesystem. IEEE Technical Committee on Services Computing (2015–
2018). He is currently serving as Executive Vice-Chair of
IEEETechnicalCommitteeonServicesComputing(2021–),
andVice-ChairofIEEETechnicalCommittee.
11