Array14(2022)100158
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/locate/array
Minute-wisefrostprediction:Anapproachofrecurrentneuralnetworks
IanZhoua,b,‚àó,JustinLipmana,b,MehranAbolhasana,NeginShariatia,b
aUniversityofTechnologySydney,Australia
bFoodAgilityCRCLtd,81Broadway,Ultimo,NSW,2007,Australia
A R T I C L E I N F O A B S T R A C T
Keywords: Frost events incur substantial economic losses to farmers. These events could induce damage to plants and
Frostprediction crops by damaging the cells. In this article, a recurrent neural network-based method, automating the frost
InternetofThings predictionprocess,isproposed.Therecurrentneuralnetwork-basedmodelsleveragedinthisarticleincludethe
Machinelearning standardrecurrentneuralnetwork,longshort-termmemory,andgatedrecurrentunit.Theproposedmethod
Recurrentneuralnetwork
aimstoincreasethepredictionfrequencyfromonceper12‚Äì24hforthenextdayornighteventstominute-
Temporalprediction
wise predictions for the next hour events. To achieve this goal, datasets from NSW and ACT of Australia
areobtained.TheexperimentsaredesignedconsideringthesceneofdeployingthemodeltotheInternetof
Thingssystems.Factorssuchasmodelprocessingspeed,long-termerroranddataavailabilityarereviewed.
Aftermodelconstruction,therearethreeexperiments.Thefirstexperimentteststheerrorsbetweendifferent
modeltypes.Thesecondandthirdexperimentstesttheeffectofsequencelengthonerrorandperformancefor
recurrent neural network-based models. All tests introduce artificial neural network models as the baseline.
Also,alltestsformodelerrorareconductedintworoundswithtestingdatasetsfromthecurrentyear(2016)
and next year (2017). As a result, recurrent neural network-based models are more suitable for short-term
deploymentwithasmallersequencelength.Incontrast,artificialneuralnetworkmodelsdemonstratealower
erroroverthelongtermwithfasterprocessingtime.Withtheresultspresented,thelimitationsoftheproposed
methodarediscussed.
1. Introduction and gradient vanishing [4]. To address these issues, Long Short-Term
Memory (LSTM) and Gated Recurrent Unit (GRU) are proposed as
Inthefieldofagriculture,frostsoccurwhenicecrystalsareformed variants of the RNN [5]. This article leverages RNN, LSTM, and GRU
within the plants and damage the cells [1]. As a result, frost could modelsforfrostprediction.
causesignificantlossesontheeconomyandecosystem[2].Currently, Inrecentyears,theInternetofThings(IoT)technologieshavebeen
therearemanyactiveandreal-timeprotectionmethodsagainstfrost, widely applied in the field of agriculture to provide real-time moni-
including heaters, sprinklers, artificial fog, and air disturbance tech- toringandactuationservices[6].TherearealsoafewIoT-basedfrost
nologies [3]. However, the frost prediction methods automating the protection systems. However, most of these frost protection systems
activationoftheseprotectionmethodscanstillbeimproved[3]. relyonthresholdsofreal-timesensorreadingstotriggerthefrostpro-
This article focuses on predicting the condition of future frost
tectionequipment[3].Theeffectofthesesimplemechanismsislimited
damagetoplants.ThepotentialofRecurrentNeuralNetworks(RNNs)
comparedtotheaccuracyofthepredictionalgorithms[3].Therefore,
infrostpredictionisexploredinthisarticle.RNNsareaspecialform
this article considers a few factors related to the future deployment
of artificial neural network (ANN) with recurrent connections, which
offrostpredictionalgorithms.Thesefactorsincludemodelprocessing
provide the capability to recognize sequential patterns [4]. RNNs are
speed,long-termaccuracyanddataavailability.Sincesystemresources
different from the basic ANN that only accepts one input at a time.
are limited for IoT systems, the model should require a faster pro-
RNNscanacceptseveralinputsinasequence.Intermsoftime-series
cessing speed [7]. Also, IoT systems should eliminate extra human
data, individual data points are processed at once in the sequence of
interventions[6].Therefore,thedeteriorationofmodelaccuracyover
time [4]. The output of the current time state is generated from the
time should be minimum to ensure manual updates to IoT nodes are
inputofthecurrenttimestateandtheoutputoftheprevioustimestate,
infrequent.Finally,asmostfrostpredictionmodelsdependonon-site
recursively[5].ThestandardRNNhasissuessuchasgradientexplosion
‚àó Correspondingauthorat:UniversityofTechnologySydney,Australia.
E-mailaddresses: ian.zhou@student.uts.edu.au(I.Zhou),justin.lipman@uts.edu.au(J.Lipman),mehran.abolhasan@uts.edu.au(M.Abolhasan),
negin.shariati@uts.edu.au(N.Shariati).
https://doi.org/10.1016/j.array.2022.100158
Received7October2021;Receivedinrevisedform4March2022;Accepted1April2022
Availableonline9April2022
2590-0056/¬©2022TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).I.Zhouetal. Array14(2022)100158
historical data [3], data availability is important when creating these 2. Methodology
models.Sincealargeamountofhistoricaldatacannotbeassumedto
beavailableatallsites,oursetsceneassumesthatonlyasmallamount This section explains the methodology and settings of the experi-
(threemonths)ofdataisavailabletominimizethedatacollectiontime ments. The study area is firstly explained, followed by the data pre-
fornewmodels. processingprocedures.Then,modelconstructionandtestingleveraging
thepreprocesseddataaredescribed.Finally,theexperimentprocesses
1.1. Relatedwork aresummarized.
In [3], frost prediction methods are categorized as ‚Äò‚Äòclassification
2.1. Studyarea
methods‚Äô‚Äôand‚Äò‚Äòregressionmethods‚Äô‚Äô.Classificationmethodspredictthe
occurrenceoffrostasapercentageinafuturetime,whereasregression
ThestudyareaislocatedinNewSouthWalesandAustralianCapital
methodspredicttheminimumtemperatureinafutureperiod[3].Both
Territory of Australia. In the study area, datasets from 30 different
methodsrelyonclimatedataasthemodelinput.Sincethefrostresis-
weatherstationsareobtained.Fig.1isamapofthestudyareawiththe
tancesfordifferentspeciesofplantsaredifferent[8],frostregression
weather station locations and IDs. Also, a list of location coordinates
predictionmethodsareproposedinthisarticletoprovidethefarmers
of the weather stations are presented by Tables A.7 and A.8 in the
futureenvironmentalinsightsandprovideamoregeneralizedsolution
Appendix[15].Alltheserawdatasetsusedinthisworkcanbeacquired
avoidingthedifferencesbetweenindividualplantspecies.
fromthepublicweatherstationdirectoryservicehostedbytheBureau
Thereareafewexistingfrostregressionpredictionmethods.In[9‚Äì
ofMeteorology(BOM)ofAustralia[15].
11], traditional machine learning methods are leveraged to predict
This study is focused on the months June, July, and August of
temperatureorminimumtemperatureinthenextdayornight.Random
winter [16,17]. Minute-wise climate data of these winter months in
forestmodelsareusedin[11]topredictnext-dayminimumtempera-
years2016and2017areextractedfromthe30weatherstationsinthe
ture with temperature and humidity inputs. Linear regression is used
study area. After extracting these data, they are further processed to
inboth[9]and[10].Environmentalparameterssuchastemperature,
bepreparedformodelconstructionandtesting.Theseproceduresare
dewpoint,andhumidityareinsertedasmodelinputsin[10].Onthe
otherhand,toconsidertheeffectofwindmachines,theauthorsof[9] discussedinthenextsubsection.
introduced the distance to wind machines along with elevation, time
oflocalsunset,andradiationreceivedduringthepreviousdayasinput 2.2. Datapreprocessing
parameters.
Apartfromthetraditionalmachinelearningmodels.ANNwithfully Therawdataneedstobepreprocessedbeforemodelconstruction.
connected layers, as a deep learning model, can also predict future After extracting the required columns (Timestamp, Air Temperature,
minimumtemperatures[12‚Äì14].Modelsin[12‚Äì14],predictminimum DewPoint,RelativeHumidity,WindSpeed,WindDirection)fromthe
temperature in the next 12‚Äì24 h as a numerical value. These three raw datasets, seven more steps of data preprocessing are performed.
works all implement prediction models with air temperature, relative These seven steps belong to two phases. In phase 1, the raw data is
humidity,precipitation,winddirectionandspeed.However,[12]also processed to transform useful features and handle empty values. The
includesdaytimelength,daytimemaximumandminimumtemperature outputsofphase1aresavedforreuseinthenextphase.Inphase2,the
to support night temperature predictions with a daytime baseline. outputsfromphase1areinputtedandmodifiedtofitaspecifictarget
In[13],precipitation,cloudcover,moisture,andpressureareincluded modelandmodelsetting.Asanexample,Fig.2outlinesthechangesof
as model inputs. The authors also considered humidity and wind ve- datastructureduringdatapreprocessing,usinganRNNwithasequence
locity at 19:00. The authors of [14] predicted next day minimum lengthof20asanexample.Theredfiguresindicateachangetothat
temperaturewithfewerinputparameters,butintroducedradiationto datainthatstep.
buildtheirpredictionANN. Phase 1 consists of five steps and starts with transforming wind
The above machine learning and deep learning models all predict speedanddirectionintofeaturesof‚Äò‚ÄòN-wind‚Äô‚Äôand‚Äò‚ÄòE-wind‚Äô‚Äô.Thesetwo
frost conditions in the next 12‚Äì24 h [9‚Äì14]. Therefore, in extreme new features represent wind speeds towards the north (N-wind) and
conditions, protection equipment might need to be switched on for east (E-wind) directions. This transformation is done to prevent the
12‚Äì24 h to ensure zero frost damage when solely considering model possible errors generated by the wind direction values (0‚Äì359) [18].
predictions. However, by constant manual observations, the opera- TheoriginalwinddirectiondatafromBOMisthebearingofthedirec-
tional time of protection equipment could be reduced [3]. Hence, to
tionthewindisoriginated[19].Therefore,tobegintheconversion,the
reducetheoperationaltimeautomatically,themajoraimofthisarticle
originalwinddirection(ùëöùëíùë°)ischangedtothewindblowingdirection
istoimplementminute-wisenexthourminimumtemperaturepredic-
(ùëëùëíùëî)byreversingthedirection(Eq.(1)).
tionfor frostprediction. Also,asmentionedintheaboveparagraphs,
{
the potential of RNN-based models (RNN, LSTM, GRU) are explored ùëöùëíùë°+180‚ó¶, ifùëöùëíùë°<180‚ó¶
ùëëùëíùëî= (1)
to solve this prediction problem. The performance of different RNN- ùëöùëíùë°‚àí180‚ó¶, ifùëöùëíùë°‚â•180‚ó¶
basedmodelsisalsocomparedinthisarticle.Inconclusion,themajor
contributionsofthisarticlearepresentedasfollows. Then, with the wind speed (ùë£) and the wind blowing direction
(ùëëùëíùëî),magnitudesofwindvectortowardsnorth(ùë£ )andeast(ùë£ )are
ùëÅ ùê∏
1. ApplicationofanRNN-basedfrostpredictionmethod. obtainedthroughEq.(2)[18].
2. Increasing the prediction frequency from once per 12‚Äì24 h for
thenextdayornighteventstominute-wisepredictionsforthe ùë£ ùê∏,ùë£ ùëÅ =ùë£√ósin(ùëëùëíùëî),ùë£√ócos(ùëëùëíùëî) (2)
nexthourevents.
Inthesecondstepofdatapreprocessing,theminimumtemperature
3. EvaluatedthelimitationsofRNN-basedfrostprediction.
is constructed as the prediction target. The minimum temperature is
Therestofthispaperisarrangedasfollows.Section2describesthe simply constructed by getting the minimum temperature value in the
methodology and experiment settings with the study area, data pro- next 60 min. As the recording of data is done once per minute, the
cessing procedures, and experiments. Experiments include comparing minimumtemperatureofthecurrentdataentryistheminimumvalue
temperaturepredictionmodels(ANN,RNN,LSTM,GRU),analyzingdif- ofthenext60temperaturevalues.
ferentRNNmodelsettings,andtheperformanceofpredictingminimum TofitthedatastructureforRNN,LSTM,andGRUmodels,sequences
temperaturewithotherfrost-relatedparameters.Then,theexperimen- of data are created in step 3. A sequence contains all the features
talresultsarediscussedinSection3andleadtowardslimitationswith (Air Temperature, Dew Point, Relative Humidity, Wind Speed, Wind
openchallenges.Intheend,Section4concludesthewholearticle. Direction). For every data entry, the current sequence is defined as
2I.Zhouetal. Array14(2022)100158
Fig.1. WeatherstationswithID.
sequenceùë°,thesequencefromoneminutebeforeisaddedtothecurrent 2.3. Modelconstructionandtesting
entry and defined as sequence ùë° ‚àí 1. Similarly, the sequence from
two minutes before is added and defined as sequence ùë°‚àí2. Since the Inthissubsection,informationofmodelconstructionandtestingis
maximum sequencelength of theexperiments is 120, sequences from provided. The computation environment is firstly described, followed
previousentriesareaddedtothecurrententryfromoneminutebefore bytheusageofdatasets.Next,themodelstructuresandhyperparam-
until 119 min before. Therefore, each entry includes 120 sequences eters are clarified. Finally, the two stages of model construction are
fromsequenceùë°‚àí119tosequenceùë°. revealed.
The timestamp column is a tool to help extract the data from the All models in this article are constructed on a desktop computer
desired time period. Now, as all of the features and sequences are using an Intel i7-8700K 3.70 GHz processor, equipped with 32 GB
generated, the timestamp column can be removed in step 4. Then, in RAM.ThegraphicsprocessingunitisaNvidiaRTX2080graphicscard.
thefinalstepofphase1,thelistwisedeletiondataimputationtechnique The deep learning framework for model construction and testing is
is applied to eliminate data entries with empty features [20]. This TensorFlow2.3.0.
also includes the removal of data sequences with missing features to Thedatapreprocessingprocessesgeneratetwodatasetsfrom2016
and 2017 for every weather station. In the experiment scene, models
preservethetimedifferencesbetweenobservations.Thefinalproduct
shouldbebuiltinthecurrentyearanddeployedinthenextyear.The
of phase 1 is output to hard disk to be used in phase 2. For every
year 2016 has been set as the ‚Äò‚Äòcurrent‚Äô‚Äô year and 2017 is the ‚Äò‚Äònext‚Äô‚Äô
weather station dataset, the steps in phase 1 are conducted for data
year. Therefore, datasets from 2016 are used for model construction
inyears2016and2017.
anddatasetsfrom2017areusedforfinaltesting.
Inphase2ofdatapreprocessing,theresultsfromphase1aretrans-
It is assumed that during the process of model construction only
formedtotherequiredformfordifferentmodelswithdifferentsettings.
the ‚Äò‚Äòcurrent‚Äô‚Äô year data is available. The datasets from year 2016 are
After reading an output from phase 1, step 6 of data preprocessing
splitformodeltraining,validation,andtesting.Foreverydatasetfrom
removes excess data sequences. For ANN models, only one sequence
differentweatherstations,80%ofthedataarerandomlyallocatedto
is required. Therefore, sequences ùë°‚àí119 to ùë°‚àí2 should be removed.
thetrainingdatasetandtheother20%tothetestingdataset.Fromthe
ForRNN,LSTM,andGRUmodels,sequencesareremovedaccordingto
trainingdataset,afurthersplitof20%ofthedataformsthevalidation
the sequence length of the target model. For example, with a desired
dataset.Thetrainingdataset,whichcontainsmostofthedata,isused
sequencelengthof20,sequencesùë°‚àí119toùë°‚àí20areremoved,leaving
to fit the parameters of the models. During the training process, the
only20datasequences(Fig.2).
validationdatasethelpstotunethehyperparameters.Afteralltraining,
Step 7 of data preprocessing is only executed to prepare the data
hyperparametertuning,andvalidationarecompleted,testpredictions
forRNNsandtheirvariants.Everyentryinthedatasetisconvertedto areconductedbythemodelwiththetestingdataset.Thisprovidesan
a2Dstructure.Eachrowofthis2Dstructurerepresentsasequenceof errormetricofthemodel.However,asthemodelshouldbedeployedin
aspecifictime.Therowsarestructuredtoptobottomfromanearlier the‚Äò‚Äònext‚Äô‚Äôyear,datasetsfrom2017areusedasextraandfinaltesting
timetoamorerecenttime.Atthisstage,thedatacanproceedtomodel datasets.Intheexperiments,theerrorsgeneratedfromthe2017testing
construction. datasetsarealsocompared.
Table1describesthefeaturesattheendofdatapreprocessing.Most Themodelstructureforallmodelsisdefinedasathree-layermodel.
featuresdemonstrateanormaldistribution.Therelativehumidityisthe Thefirstlayerhasfiveneurons.Thesecondhassevenandtheoutput
onlyskewed-leftfeature.Table2showsthePearsoncorrelationmatrix layer has one neuron. The first two layers change according to the
oftheprocessedfeatures.Dewpointandrelativehumidityshowstrong target model. For example, if it is an ANN model, the cells in these
correlationstotemperature.Otherfeaturesarerelativelyindependent twolayersareANNcellswithRectifiedLinearUnit(ReLU)activation
ofeachother. functions. For RNN and its variants, the cells use the tanh activation
3I.Zhouetal. Array14(2022)100158
Fig.2. Datapreprocessingsteps.(Forinterpretationofthereferencestocolourinthisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
functionswithrelevantcellstothemodels.Also,forthefirstlayerof ùõΩ as 0.9, ùõΩ as 0.999, and ùúñ as 10‚àí7. Another hyperparameter, batch
1 2
RNN,LSTM,andGRUmodels,thehiddenstateofthecellsisoutputas sizeisconfiguredto64.Withthesesettings,allmodelsaretrainedfor
sequentialinputsofthesecondlayer.Thethirdlayeronlyconsistsofa 100epochswithaMean-SquareError(MSE)lossfunction.
singlelinearcelltooutputtheresult. Modelsareconstructedintwogroups.Thegroupsareseparatedby
Adam is the optimizer used during training. Learning rate, ùõΩ , different model structures and settings. In the first group, one ANN
1
ùõΩ , and ùúñ, are the hyperparameters required for Adam [21]. In the modelisconstructedforeachofthe30weatherstations.Inthesecond
2
experiments,learningrateissettothe‚Äò‚Äògooddefaultsettings‚Äô‚Äôas0.001, group, RNN, LSTM, and GRU models with different sequence lengths
4I.Zhouetal. Array14(2022)100158
Table1
Descriptionoftheprocessedtrainingfeatures.
Feature
Item Temperature Dewpoint Relative N-wind E-wind
(‚ó¶C) (‚ó¶C) humidity(%) (km/h) (km/h)
Min ‚àí5.4000 ‚àí37.8000 2.0000 ‚àí86.2608 ‚àí78.0689
Mean 10.4629 6.3262 78.1665 ‚àí1.9071 4.8393
Max 31.0000 23.2000 100.0000 81.8329 95.3837
Std 5.0328 4.2037 18.0907 10.7936 11.3289
Distribution Normal Normal Skewed-left Normal Normal
Table2
Pearsoncorrelationmatrixoftheprocessedtrainingfeatures.
Temperature Dewpoint Relativehumidity N-wind E-wind
Temperature 1.0000 0.6391 ‚àí0.5744 ‚àí0.1330 0.0181
Dewpoint 0.6391 1.0000 0.2474 ‚àí0.0946 ‚àí0.1613
Relativehumidity ‚àí0.5744 0.2474 1.0000 0.0659 ‚àí0.2059
N-wind ‚àí0.1330 ‚àí0.0946 0.0659 1.0000 ‚àí0.1814
E-wind 0.0181 ‚àí0.1613 ‚àí0.2059 ‚àí0.1814 1.0000
aretrainedforall30weatherstations.Thereare6differentsequence nullhypothesisisrejectedandANNislikelytoproduceoutputswitha
lengthsettings(20,40,60,80,100,120).Altogether,thesecondgroup greaterlossthanLSTM.LSTMmodelshavethehighestaccuracyamong
outputs540models.Overall,thereare570modelsconstructedforthis otherRNN-basedmodelsduetotheextragatestomemorizesequence
article. The usage of these models in experiments is explained in the patterns[5].
nextsubsection. Anassumptionmadeforthemodelsofthispaperisthatthemodels
areconstructedusingdatafrom‚Äò‚Äòthis‚Äô‚Äôyearanddeployedinthe‚Äò‚Äònext‚Äô‚Äô
2.4. Experiments year. Therefore, models are also tested with testing datasets obtained
one year after the training datasets. The results are significantly dif-
Therearethreeexperimentsconductedtotestthemodelerrorand ferent compare to the results from the ‚Äò‚Äòcurrent‚Äô‚Äô year testing datasets
performanceofmodels.Inthefirstexperiment,theerrorsofdifferent (Fig.3).ANNshowsthelowestMSEloss,whichindicatesthehighest
model types (ANN, RNN, LSTM, GRU) are compared. ANN models accuracy.Ontheotherhand,LSTMmodelshavethelowestaccuracy.
from the first model group are the baseline of this experiment. For Theresultsoftheone-sidedpairedT-testsdemonstratethatLSTMand
each weather station, there are eight conducted tests to measure the GRUmodelshaveùëù-values(RNN:0.1350;LSTM:0432;GRU:0.0027)
losses from the current year and next year data for the four model less than the ùõº threshold. Therefore, it is likely that LSTM and GRU
types.RNNmodelsandtheirvariantsaretestedwithasequencelength models all have a significantly higher loss (less accurate) than ANN
of 120. Then, in the second experiment, the effect of the sequence models.
length of RNN, LSTM, and GRU models are compared. Additional to
Asallmodelsaretrainedwiththecurrentyeardata,LSTMmodels
theresultsobtainedinthefirstexperiment,30moretestsareconducted
with more parameters and gates [22] fit closer to the current year
for each weather station to obtain the results of the three RNN-based
testingdatasets.Ontheotherhand,RNN-basedmodelsareconstructed
model types with five sequence number settings, and tested with the
through learning the sequence patterns [4,5]. Thus, these models are
current year and next year testing datasets. Model errors of all RNN-
sensitive to the change of sequence patterns. In [23,24], the global
basedmodeltypesarecomparedwithdifferentsequencelengthsettings
climate change induces an increase of instability in weather patterns
againsttheANNbaseline.Similarly,inthefinalexperiment,thetrain-
overtime.Asaresult,theaccuracyofRNN-basedmodelsdeteriorates
ing time and inference time of different sequence length settings are
when tested with the next year testing datasets. Compared to the
comparedagainsttheANNbaseline.
baseline, LSTM and GRU models with more parameters [22] tend to
‚Äò‚Äòoverfit‚Äô‚Äô more to the current year pattern and are vulnerable to the
3. Resultsanddiscussions
changed next year pattern. However, the exact extent of accuracy
reductionisunknownasthechangeofclimatepatternsinthefutureis
To validate and compare different model settings for next hour
alsounknown.
frost prediction models, three experiments are conducted. The first
experiment compares the MSE between ANN and RNN-based model
3.2. Effectofsequencelengthonmodelerror
types.Then,inthesecondexperiment,MSEsofRNN-basedmodelswith
differentsequencelengthsareassessed.Finally,processingtimerelated
factorsarealsoanalyzedwithdifferentsequencelengths.Thisprovides Inthisexperiment,theeffectofsequencelengthonmodelerrorfor
anoverviewofdifferentmodels‚Äôreal-timecomputationabilities. RNN-basedmodelsisinspected.Fig.4showstheaverageMSEsofRNN-
basedmodels testedby currentyear datasets.Overall, theincrease of
3.1. Modelerror sequence length does not reduce the average loss. Only LSTM shows
a decreasing trend of losses with the increase of sequence length.
In this experiment, the model errors of RNN-based models with a However,thischangeisnotverysignificant.WhencomparedtoANN,
sequencelengthof120areevaluatedwithANNmodels.Fig.3shows LSTMandGRUwithsomesettings(sequencelength=20,40,80,100)
that when testing with testing datasets derived from the same year seem tohavea smallerloss thanANN (0.4550). This isconfirmedby
when the training datasets is collected, LSTM seems to perform with theone-sidedpairedT-testsresultsonTable3.Onlyùëù-valuesofLSTM,
thebestaccuracywiththelowestMSEloss.LSTMisalsotheonlyRNN- andGRUmodelswiththesequencelengthsof20,40,80,and100are
basedmodeltypetoexceedtheaccuracyofANNmodels.Thisresultis smallerthantheùõº.Thisrevealsthatwhentestingwiththecurrentyear
alsoconfirmedwithone-sidedpairedT-tests.Fromtheùëù-values(RNN: datasets.ThisdemonstratesthehigherlikelihoodthatLSTMandGRU
0.1544; LSTM: 6.1225e‚àí12; GRU: 0.2644), LSTM is the only model (sequence length = 20, 40, 80, 100) models are more accurate than
typethattheùëù-valueissmallerthanthe0.05ùõº value.Thismeansthe ANNmodelswhentestedwithcurrentyeartestingdatasets.
5I.Zhouetal. Array14(2022)100158
Fig.3. AverageMSEtestedwithcurrentandnextyeardatasets.
Fig.5. AverageMSEtestedwithnextyeardatasetsfordifferentsequencelengths.
Table4
ùëÉ-values of comparing average MSEs between ANN and RNN-based models with
differentmodelsequencelengths(nextyear).
Modeltype
Sequencelength RNN LSTM GRU
20 0.1466 0.0013 0.0109
40 0.1463 0.0040 0.0249
60 0.1538 0.0028 0.0156
80 0.1457 0.0072 0.0089
100 0.1497 0.0048 0.0294
120 0.1350 0.0433 0.0028
Fig.4. AverageMSEtestedwithcurrentyeardatasetsfordifferentsequencelengths.
Table3
ùëÉ-values of comparing average MSEs between ANN and RNN-based models with
differentmodelsequencelengths(currentyear).
Modeltype
Sequencelength RNN LSTM GRU
20 0.1603 3.4979e‚àí10 0.0032
40 0.1617 6.9911e‚àí11 1.3249e‚àí5
Fig.6. Averagetrainingtimeperepochfordifferentsequencelengths.
60 0.1628 1.0544e‚àí10 0.1591
80 0.1581 3.0000e‚àí11 4.2542e‚àí7
100 0.1576 2.1009e‚àí11 4.2930e‚àí7
120 0.1544 6.1225e‚àí12 0.2644 3.3. Effectofsequencelengthonprocessingtime
Inthefinalexperiment,thetrainingtimeperepochandinference
Fig. 5 is the average MSEs for RNN-based models with different timeperinputaretestedandcomparedbetweenANNmodelsandRNN-
sequence lengths tested by the next year testing datasets. Similar to basedmodelswithdifferentsequencelengths.Fig.6demonstratesthat
the reverse of results in Experiment 1, all RNN-based models have a thetrainingtimeperepochforRNN,LSTM,andGRUmodelsincreases
asthesequencelengthincreases.Thisisduetotheadditionalparam-
higherlossthantheANNMSE(0.7813).Table4istheùëù-valuesobtained
eters for training as the sequence length increases. LSTM and GRU
fromone-sidedpairedT-testswithanalternativehypothesisthateach
modelshaveoverlappingcurvesoftrainingtimes.RNNtrainingtimes
testedmodelhasagreaterMSEthantheANNbaseline.Thealternative
aregreaterthanthepriortwomodels.Thisisbecause,inTensorFlow
hypothesisisinfavorofLSTMandGRUmodelsastheirùëù-valuesareless
2.3.0,onlyLSTMandGRUmodeltrainingprocessesareoptimizedby
thanùõº.ThismeansLSTMandGRUmodelsarelikelytoperformwith
cuDNNforfastertrainingspeeds[25,26].However,whetheroptimized
highererrorsthanANNmodelsinthenextyear.Also,asexplainedin ornot,comparedtotheANNtrainingtimeof1.070s,astheùëù-values
Experiment1,thechangeofclimatepatternsinthefutureisunknown. aresmallerthanthe0.05ùõº,Table5indicatesthatthetrainingtimeof
ThiscouldbethereasonoftheadditionalnoiseinFig.5,comparedto ANNmodelsperepochismorelikelytobesmallerthanallsettingsof
Fig.4. RNN-basedmodels.
6I.Zhouetal. Array14(2022)100158
Table5 modeldeployedintheshortterm.InExperiment2,RNN-basedmodels
ùëÉ-valuesofcomparingaveragetrainingtimeperepochbetweenANNandRNN-based do not present a significant reduction in model error as the sequence
modelswithdifferentmodelsequencelengths.
lengthincreases.Therefore,consideringtheresultsfromExperiment3,
Modeltype
RNN-based models with a short sequence can be deployed for short
Sequencelength RNN LSTM GRU
timestoprovidepredictionswithhigheraccuracyandperformance.On
20 5.3068e‚àí32 1.4421e‚àí30 7.5427e‚àí31 the other hand, ANN models can be deployed over longer time spans
40 1.4240e‚àí31 1.5839e‚àí31 2.1966e‚àí31
withoutmuchaccuracydeterioration.Bothtrainingandinferencetime
60 1.5350e‚àí31 2.2799e‚àí32 1.3996e‚àí31
80 4.3655e‚àí32 5.7483e‚àí32 1.4185e‚àí31 issignificantlylowerthanRNN-basedmodels.Overall,ANNsmightstill
100 7.6250e‚àí32 2.8775e‚àí31 6.2908e‚àí31 bemoresuitablethanRNN-basedmodelsforfrostpredictioninalong-
120 1.2237e‚àí31 5.4545e‚àí32 2.6502e‚àí31 term scenario with minimal system maintenance and update because
of their higher accuracy and performance over the long term. After
constructingtheabovedifferentfrostpredictionmodelsandanalyzing
their performance, the limitations presented in this subsection are
discovered.Limitationscreatenewchallengesandleadtowardsfuture
directionstofrostpredictionresearch.
3.4.1. Modelaccuracyrequirementsarenotspecified
Different plants have different frost tolerance and sensitivity [8].
Therefore, model accuracy requirements may vary between different
plants.Therearemanystudiesonplantfrosttolerance.However,the
sensitivityofplantstoindividualfrostfactorsisnotfullyrevealed[28].
As a future direction, the sensitivity to different frost factors such
as temperature, humidity, dew point, cloud coverage, solar radiation
and wind speed should be tested with high precision for individual
speciesofplantsincontrolledenvironments.Withenoughplantspecies
studied, a model accuracy threshold can be set for frost prediction
models.
Fig.7. Averageinferencetimeperinputfordifferentsequencelengths.
Table6 3.4.2. Lackofstandarddatasets
ùëÉ-valuesofcomparingaverageinferencetimeperinputbetweenANNandRNN-based The models and experiment results are obtained from a public
modelswithdifferentmodelsequencelengths.
datasetthatrepresentslocalclimateconditionsofourstudysitesinAus-
Modeltype
tralia.Therefore,theaccuracyofthemodelsatothersiteswithdifferent
Sequencelength RNN LSTM GRU
climate patterns is questionable. To the best of our knowledge, prior
20 2.5574e‚àí45 1.0941e‚àí17 1.7544e‚àí35 research has been based upon locally obtained private datasets [9‚Äì
40 5.0297e‚àí49 2.1683e‚àí37 2.6969e‚àí42
14]. As a result, model results could be biased. There is a need of a
60 6.7674e‚àí62 3.7983e‚àí42 2.6171e‚àí42
80 2.1246e‚àí60 1.0625e‚àí44 1.5319e‚àí42 standarddatasetforfrostpredictionmodelsthatincludesdataentries
100 1.9803e‚àí54 2.7216e‚àí46 1.2132e‚àí37 fromdifferentlocations.
120 1.0382e‚àí58 5.9795e‚àí48 1.6906e‚àí46
3.4.3. TheRNNinputdataformataffectsthesystemenergyefficiency
TheRNN-basedmodelsinthisarticlerequireasequenceofclimate
Fig.7demonstratestheaverageinferencetimeperinputforRNN-
dataasmodelinputs.Eachsequenceofdatacontainssensorreadings
based models with different sequence lengths. There is a trend of
collectedeveryminuteoveralongtimespan.Thisplacesarestriction
increaseininferencetime,alongwiththeincreaseofsequencelength.
onnodedutycycles.Sensorreadingsmustbecollectedperminuteto
Similartotrainingtime,ahighersequencelengthofRNN-basedmodels
satisfy the model input. For common agricultural IoT systems, sensor
implies a larger input sequence and a model structure with more pa-
readings are transmitted through the radio from a node to a central
rameters.Thus,theinferencetimeincreaseswiththesequencelength.
processingnodeforfurtherprocessinganddataanalytics[29].In[30],
Also,theinferencetimeforallRNN-basedsettingsissignificantlylarger
radiopowerconsumptionincreasesasthetimeintervalofradiotrans-
than ANN inference time (4.5214e‚àí4 s). This statement is supported
missionreportsdecreases.Evenradiotransmissionsbetween10-minute
withtheone-sidedpairedT-testresultsasallùëù-valuesarelessthanthe
periods consume a significant amount of the system energy [30].
ùõº(Table6).
Therefore,ifsensorreadingsarereportedeveryminute,higherenergy
consumption will be placed on the whole system. This issue could
3.4. Limitationsandopenchallenges
be mitigated by aggregating a few minute-wise sensor readings into
one radio transmission. Also, inference on edge with edge computing
Experiment 1 shows that RNN-based models have lower model
errorsthanANNmodelswhentestedwithcurrentyeardatasets.LSTM can also reduce the number of radio transmissions. A model can be
models have the lowest errors and highest accuracy. However, RNN- deployed on edge devices and only transmit the predicted outcome
basedmodels‚ÄôaccuracydeclinesandisexceededbyANNwhentested whenittriggersapresetcondition.However,eventherearemitigation
withnextyeardatasets.Thisdeclineislikelytobecausedbyclimate plans, RNN-based models still limit the design of IoT systems with
pattern change over time [4,5,23,24]. Also, the models in this article potential high energy impact. On the other hand, each ANN model
are constructed with one year of data. From [27], RNN models often inference only requires a single set of climate data as input. ANN-
require data from more years to fully learn the seasonality patterns. basedfrostpredictionmodelscanbeappliedtosystemswithdifferent
However,thiswouldincreasethedependencyonhistoricaldata,which time intervals to obtain sensor data. Radio transmission intervals can
is a limitation mentioned in the next subsection. With both sources beadjustedaccordingtosystemrequirements.Therefore,ANNmodels
of accuracy deterioration, RNN-based models are only suitable for a maystillbemoresuitableforfrostpredictionsystems.
7I.Zhouetal. Array14(2022)100158
3.4.4. Modelsdependonpreviousyeardata differentmodels.Thirdly,theRNNinputdataformataffectsthesystem
All models constructed for this article depend on previous years energyefficiency.Thelimitationsonenergyefficiencycouldbeavoided
of data. Therefore, model accuracy is dependent on the quality of bytheapplicationofedgecomputingormessageaggregation.Onthe
historicaldata.Thisisalimitationonallmachinelearningmodels[31]. otherhand,ANNmodelsdonothavesuchlimitations.Thenextlimita-
As more recent researches are site-specific, sites without any record tionisrelatedtotheconstructionofmodels.Allofthesemodelsdepend
of historical climate data require an IoT data collection system to be on previous year data. For sites without data records, this limitation
deployed for data collection. To ensure similar performance to the requires extra development and deployment time. Therefore, models
resultsofthisarticle,thesystemmustbedeployedatleastayearprior thatdecouplefromhistoricaldatacouldbedevelopedtoeliminatethis
to produce a prediction model and provide frost prediction services. restriction completely. The final limitation is that prediction models
Thisincreasesthedeploymenttimeofthesystem.Apossiblesolution lackstoppingconditions.Asmostsystemsreadclimatedatafromone
is to explore the performance of models based on previous months sensor node, the activation of protection gear could contaminate the
of data. This solution could substantially reduce development time.
input data. Thus, it contaminates the prediction outcomes. A future
However, it cannot eliminate this excess system development time
modelcouldbedevelopedtoremovethiseffectofdatacontamination
for data collection. Methods eliminating the requirement of on-site
and provide a model-based automatic stopping mechanism to reduce
historical data need to be developed. Another possible direction is to
operationalcostsoffrostprotectionsystems.
explorethegeneralizationortransferofmodelstosimilarlocations.
3.4.5. Lackofstoppingconditions CRediTauthorshipcontributionstatement
Thefrostpredictionmodelsconstructedinthisarticleonlypredict
the start of a frost event with real-time sensor readings. This predic- IanZhou:Conceptualization,Methodology,Software,Formalanal-
tion could be the trigger of a frost protection mechanism. However, ysis, Investigation, Data curation, Writing ‚Äì original draft, Visualiza-
there are limited mentions of predicting the end of a frost event tion. Justin Lipman: Conceptualization, Supervision, Writing ‚Äì re-
to switch off any protection mechanisms. As most frost protection view&editing.MehranAbolhasan:Methodology,Validation,Writing
systems rely only on a single sensor node [3], activation of a nearby ‚Äì review & editing. Negin Shariati: Resources, Writing ‚Äì review &
protection mechanism might affect the sensor readings and contam- editing.
inate the prediction outcomes of frost prediction models. Therefore,
future prediction models could be developed to eliminate the effect
Declarationofcompetinginterest
offrostprotectionmechanisms.As apossiblebenefit,frostprotection
mechanisms could be switched off earlier to reduce the operational
The authors declare that they have no known competing finan-
cost.
cial interests or personal relationships that could have appeared to
4. Conclusion influencetheworkreportedinthispaper.
Theprimaryaimistoincreasethepredictionfrequencyfromonce Acknowledgment
per12‚Äì24hforthenextdayornighteventstominute-wisepredictions
for the next hour events. RNN-based models are selected to learn
We would like to acknowledge the support of Food Agility CRC
the sequence pattern of historical data. ANN models are used as a
Ltd, 81 Broadway, Ultimo, NSW, 2007, Australia, funded under the
baseline.DatasetsfromweatherstationsintheNSWandACTareasof
Commonwealth Government CRC Program. The CRC Program sup-
Australia are obtained. These datasets are recorded during the years
portsindustry-ledcollaborationsbetweenindustry,researchersandthe
2016 and 2017. With these datasets, it is assumed that our models
community.
are built during the year 2016 (current year) and deployed in year
The authors are with Radio Frequency and Communication Tech-
2017 (next year). Therefore, datasets from 2016 are used for model
nologies (RFCT) research laboratory, Faculty of Engineering and IT,
constructionandpreliminarytesting.Datasetsfrom2017areusedfor
UniversityofTechnologySydney,Ultimo,NSW2007,Australia.
finaltesting.Afterconstructingthemodels,therearethreeexperiments
testingthemodelerrors,alsotheeffectofsequencelengthsonerrors
and processing time for RNN-based models. The errors of models is Appendix. Weatherstationlocationcoordinates
testedwithboththecurrentandnextyeardatasets.LSTMseemstohave
thehighestaccuracywhentestedwiththecurrentyeartestingdatasets. SeeTablesA.7andA.8.
However,theaccuracyforallRNN-basedmodelsreduceswhentested
with the next year testing datasets. ANN models have the highest
accuracywiththenextyeartestingdatasets.WhentestingRNN-based TableA.7
modelswithdifferentsequencelengths,itseemsthatsequencelengths Weatherstationlocationcoordinates.
cannot affect the accuracy of models significantly. However, training StationID Latitude(degrees) Longitude(degrees)
andinferencetimeincreaseswiththesequencelength.Therefore,RNN- 69138 ‚àí35.3635 150.4828
basedmodelsshouldbeusedforshort-termdeploymentswithashorter 62100 ‚àí32.7244 150.2290
sequence length to ensure accuracy and performance. On the other 75019 ‚àí34.5412 144.8345
61363 ‚àí32.0335 150.8264
hand, ANN models demonstrate the lowest error when tested with
46012 ‚àí31.5194 143.3850
next year datasets. Also, the training and inference speeds of ANN
67119 ‚àí33.8510 150.8567
modelsarefasterthanRNN-basedmodels.Therefore,inthelongterm, 50017 ‚àí33.9382 147.1962
ANN models are more suitable than RNN-based models due to better 54038 ‚àí30.3154 149.8302
accuracyandperformance. 72162 ‚àí36.2304 148.1405
58208 ‚àí28.8824 153.0618
There are limitations determined. Firstly, the model accuracy re-
67108 ‚àí33.8969 150.7281
quirementsarenotspecifiedinthisarticleduetoalackofstudieson 61287 ‚àí32.1852 150.1737
precisefrostsensitivitiestoindividualfrostfactors.Secondly,thecur- 65070 ‚àí32.2206 148.5753
rentmodelandmostpreviousmodelsareconstructedwithlocaldata. 68242 ‚àí34.6532 150.8609
61392 ‚àí31.7416 150.7937
The lack of standard datasets limits unbiased comparisons between
8I.Zhouetal. Array14(2022)100158
TableA.8 [13] Zeng W, Zhang Z, Gao C. A Levenberg-Marquardt neural network model with
Weatherstationlocationcoordinatescontinue. rough set for protecting citrus from frost damage. In: Proc. 2012 eighth
StationID Latitude(degrees) Longitude(degrees) internationalconferenceonsemantics,knowledgeandgrids.2012.p.193‚Äì196.
Beijing,China.http://dx.doi.org/10.1109/SKG.2012.4.
72161 ‚àí35.9371 148.3779
[14] FuentesM,CamposC,Garc√≠a-LoyolaS.Applicationofartificialneuralnetworks
51161 ‚àí30.9776 148.3798
tofrostdetectionincentralChileusingthenextdayminimumairtemperature
70330 ‚àí34.8085 149.7311
forecast.ChilJAgricRes2018;78(3):327‚Äì38.http://dx.doi.org/10.4067/S0718-
61055 ‚àí32.9184 151.7985
58392018000300327.
66137 ‚àí33.9176 150.9837
[15] ofMeteorologyB.WeatherStationDirectory.BureauofMeteorology;2020,URL
64017 ‚àí31.3330 149.2699
http://www.bom.gov.au/climate/data/stations/.[Accessed04April202].
68239 ‚àí34.5253 150.4217
[16] ofMeteorologyB.Australiainwinter2016.2016,URLhttp://www.bom.gov.au/
66161 ‚àí33.9925 150.9489
climate/current/season/aus/archive/201608.summary.shtml.[Accessed04April
68241 ‚àí34.5638 150.7900
2020].
58077 ‚àí29.6224 152.9605
[17] ofMeteorologyB.Australiainwinter2017.BureauofMeteorology;2017,URL
69128 ‚àí35.1103 150.0826
http://www.bom.gov.au/climate/current/season/aus/archive/201708.summary.
65111 ‚àí33.8382 148.6540
shtml.[Accessed04April2020].
56238 ‚àí30.5273 151.6158
[18] LeeM,MoonS,KimY,MoonB.Correctingabnormalitiesinmeteorologicaldata
65103 ‚àí33.3627 147.9205
bymachinelearning.In:2014IEEEinternationalconferenceonsystems,man,
52088 ‚àí30.0372 148.1223
and cybernetics. San Diego, CA, USA; 2014, p. 888‚Äì93. http://dx.doi.org/10.
1109/SMC.2014.6974024.
[19] of Meteorology B. Wind. Bureau of Meteorology; 2020, URL http://www.
bom.gov.au/marine/knowledge-centre/reference/wind.shtml.[Accessed04April
References
2020].
[20] OsborneJW.Six:Dealingwithmissingorincompletedata:Debunkingthemyth
[1] Webb L, Snyder RL. Frost hazard. In: Bobrowsky PT, editor. Encyclopedia of ofemptiness.In:Bestpracticesindatacleaning:Acompleteguidetoeverything
naturalhazards.Dordrecht:SpringerNetherlands;2013,p.363‚Äì6.http://dx.doi. you need to do before and after collecting your data. SAGE Publications, Inc;
org/10.1007/978-1-4020-4399-4_148. 2013,p.105‚Äì38.http://dx.doi.org/10.4135/9781452269948.n6.
[2] Ma Q, Huang J-G, H√§nninen H, Berninger F. Divergent trends in the risk of [21] Kingma DP, Ba J. Adam: A method for stochastic optimization. 2017, arXiv
springfrostdamagetotreesinEuropewithrecentwarming.GlobalChangeBiol preprintarXiv:1412.6980.
2019;25(1):351‚Äì60.http://dx.doi.org/10.1111/gcb.14479. [22] ChungJ,GulcehreC,ChoK,BengioY.Empiricalevaluationofgatedrecurrent
[3] Zhou I, Lipman J, Abolhasan M, Shariati N, Lamb DW. Frost monitoring neuralnetworksonsequencemodeling.2014,arXivpreprintarXiv:1412.3555.
cyber‚Äìphysical system: A survey on prediction and active protection meth- [23] Augspurger CK. Reconstructing patterns of temperature, phenology, and
ods.IEEEInternetThingsJ2020;7(7):6514‚Äì27.http://dx.doi.org/10.1109/JIOT. frost damage over 124 years: Spring damage risk is increasing. Ecology
2020.2972936. 2013;94(1):41‚Äì50.http://dx.doi.org/10.1890/12-0200.1.
[4] GoodfellowI,BengioY,CourvilleA.Sequencemodeling:Recurrentandrecursive [24] H√§nninenH.Doesclimaticwarmingincreasetheriskoffrostdamageinnorthern
nets.In:Deeplearning.MITPress;2016,p.363‚Äì408. trees?PlantCellEnviron1991;14(5):449‚Äì54.http://dx.doi.org/10.1111/j.1365-
[5] Salehinejad H, Sankar S, Barfett J, Colak E, Valaee S. Recent advances in 3040.1991.tb01514.x.
recurrentneuralnetworks.2017,arXivpreprintarXiv:1801.01078. [25] Google. tf.keras.layers.LSTM. Google; 2020, URL https://www.tensorflow.org/
[6] Farooq MS, Riaz S, Abid A, Abid K, Naeem MA. A survey on the role api_docs/python/tf/keras/layers/LSTM.[Accessed12October2020].
of IoT in agriculture for the implementation of smart farming. IEEE Access [26] Google.tf.keras.layers.GRU.Google;2020,URLhttps://www.tensorflow.org/api_
2019;7:156237‚Äì71.http://dx.doi.org/10.1109/ACCESS.2019.2949703. docs/python/tf/keras/layers/GRU.[Accessed12October2020].
[7] SarkerVK,GiaTN,TenhunenH,WesterlundT.Lightweightsecurityalgorithms [27] Muzaffar S, Afshari A. Short-term load forecasts using LSTM networks. En-
for resource-constrained IoT-based sensor nodes. In: ICC 2020 - 2020 IEEE ergy Procedia 2019;158:2922‚Äì7. http://dx.doi.org/10.1016/j.egypro.2019.01.
international conference on communications. 2020. p. 1‚Äì7. Dublin, Ireland. 952,InnovativeSolutionsforEnergyTransitions.
http://dx.doi.org/10.1109/ICC40277.2020.9149359. [28] BarlowKM,ChristyBP,O‚ÄôLearyGJ,RiffkinPA,NuttallJG.Simulatingtheimpact
[8] diFrancescantonioD,VillagraM,GoldsteinG,CampanelloPI.Droughtandfrost ofextremeheatandfrosteventsonwheatcropproduction:Areview.FieldCrops
resistance vary between evergreen and deciduous Atlantic forest canopy trees. Res2015;171:109‚Äì19.http://dx.doi.org/10.1016/j.fcr.2014.11.010.
FunctPlantBiol2020.http://dx.doi.org/10.1071/FP19282. [29] Deepika G, Rajapirian P. Wireless sensor network in precision agriculture: A
[9] HalleyV,ErikssonM,NunezM.Frostpreventionandpredictionoftemperatures survey. In: 2016 international conference on emerging trends in engineering,
andcoolingratesusingGIS.AustGeogrStud2003;41(3):287‚Äì302.http://dx.doi. technologyandscience.2016.p.1‚Äì4.Pudukkottai,India.http://dx.doi.org/10.
org/10.1046/j.1467-8470.2003.00235.x. 1109/ICETETS.2016.7603070.
[10] Iacono LE, V√°zquez Poletti JL, Garc√≠a Garino C, Llorente IM. Performance [30] MartinezB,Mont√≥nM,VilajosanaI,PradesJD.Thepowerofmodels:Modeling
models for frost prediction in public cloud infrastructures. Comput Inform power consumption for IoT devices. IEEE Sens J 2015;15(10):5777‚Äì89. http:
2018;37(4):815‚Äì37. //dx.doi.org/10.1109/JSEN.2015.2445094.
[11] DiedrichsAL,BrombergF,DujovneD,Brun-LagunaK,WatteyneT.Prediction [31] Mohammadi M, Al-Fuqaha A, Sorour S, Guizani M. Deep learning for IoT
of frost events using machine learning and IoT sensing devices. IEEE Internet big data and streaming analytics: A survey. IEEE Commun Surv Tutor
ThingsJ2018;5(6):4589‚Äì97.http://dx.doi.org/10.1109/JIOT.2018.2867333. 2018;20(4):2923‚Äì60.http://dx.doi.org/10.1109/COMST.2018.2844341.
[12] Ghielmi L, Eccel E. Descriptive models and artificial neural networks for
springfrostpredictioninanagriculturalmountainarea.ComputElectronAgric
2006;54(2):101‚Äì14.http://dx.doi.org/10.1016/j.compag.2006.09.001.
9