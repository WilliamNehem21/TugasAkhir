Array18(2023)100292
Contents lists available at ScienceDirect
Array
journal homepage: www.sciencedirect.com/journal/array
Multiclass blood cancer classification using deep CNN with
optimized features
Wahidur Rahmana,c,*, Mohammad Gazi Golam Faruqueb, Kaniz Roksanac,
A H M Saifullah Sadic, Mohammad Motiur Rahmana, Mir Mohammad Azadd
aDepartment of Computer Science and Engineering, Mawlana Bhashani Science and Technology University, Tangail, 1902, Bangladesh
bDepartment of Computer Science and Engineering, Khwaja Yunus Ali University, Sirajganj, Bangladesh
cDepartment of Computer Science and Engineering, Uttara University, Dhaka, Bangladesh
dDepartment of Computer Science and Engineering, Hamdard University Bangladesh, Munshiganj, 1510, Bangladesh
A R T I C L E I N F O A B S T R A C T
Keywords: Breast cancer, lung cancer, skin cancer, and blood malignancies such as leukemia and lymphoma are just a few
Blood cancer instances of cancer, which is a collection of cells that proliferate uncontrollably within the body. Acute
Convolutional neural network lymphoblastic leukemia is of one the significant form of malignancy. The hematologists frequently makes an
Particle swarm optimization
oversight while determining a blood cancer diagnosis, which requires an excessive amount of time. Thus, this
Cat swarm optimization
research reflects on a novel method for the grouping of the leukemia with the aid of the modern technologies like
Machine learning
Machine Learning and Deep Learning. The proposed research pipeline is occupied into some interconnected parts
like dataset building, feature extraction with pre-trained Convolutional Neural Network (CNN) architectures
from each individual images of blood cells, and classification with the conventional classifiers. The dataset for
this study is divided into two identical categories, Benign and Malignant, and then reshaped into four significant
classes, each with three subtypes of malignant, namely, Benign, Early Pre-B, Pre-B, and Pro-B. The research first
extracts the features from the individual images with CNN models and then transfers the extracted features to the
features selections such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and SVC
Feature Selectors along with two nature inspired algorithms like Particle Swarm Optimization (PSO) and Cat
Swarm Optimization (CSO). After that, research has applied the seven Machine Learning classifiers to accomplish
the multi-class malignant classification. To assess the efficacy of the proposed architecture a set of experimental
data have been enumerated and interpreted accordingly. The study discovered a maximum accuracy of 98.43%
when solely using pre-trained CNN and classifiers. Nevertheless, after incorporating PSO and CSO, the proposed
model achieved the highest accuracy of 99.84% by integrating the ResNet50 CNN architecture, SVC feature
selector, and LR classifiers. Although the model has a higher accuracy rate, it does have some drawbacks.
However, the proposed model may also be helpful for real-world blood cancer classification.
Contributions of the paper • To apply the nature inspired algorithms to find the best features from
the extracted and apply the ML based classifiers and interprets the
• To extract the features with pre-trained CNN models from the indi- calculated experimental data accordingly.
vidual images of four significant stages of both the healthy and
malignant tissues. 1. Introduction
• To apply the feature selection algorithms to work with optimized
deep features and track out the performance. Cancer is a cluster of cells undergoing unchecked growth in the body
[1], and it can quickly spread to any organ. Cancer comes in various
forms; the most common are breast cancer [2], lung cancer, skin cancer,
* Corresponding author. Department of Computer Science and Engineering, Mawlana Bhashani Science and Technology University, Tangail, 1902, Bangladesh.
E-mail addresses: wahidtuhin0@gamil.com (W. Rahman), golam.faruq@gmail.com (M.G.G. Faruque), kanizroksana96@gmail.com (K. Roksana), saifullah.cse@
uttarauniversity.edu.bd (A.H.M.S. Sadi), motiurcse@mbstu.ac.bd (M.M. Rahman), csdrazad@hamdarduniversity.edu.bd (M.M. Azad).
https://doi.org/10.1016/j.array.2023.100292
Received 24 March 2023; Received in revised form 7 May 2023; Accepted 8 May 2023
Availableonline16May2023
2590-0056/©2023TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).W. Rahman et al. A r r a y18(2023)100292
and blood cancers like leukemia and lymphoma. There have been 9.2 Artificial Intelligence (AI), and Deep Learning (DL) has emerged as a
million fatalities from lung cancer, 1.7 million from skin cancer, and significant tool for aiding clinicians in determining treatment decisions.
627,000 from breast cancer [3,4], according to reports from the World Thus, many computer-aided diagnosis methods have been developed to
Health Organization (WHO) [5]. When it comes to cancers, leukemia has detect ALL blood pictures without human intervention [8,32].
a remarkably high mortality rate. It’s a malignant tumor that forms in Thus, this research reflects on the ML based method to classify the
the bone marrow when immature white blood cells are cloned in a type of Acute Lymphoblastic Leukemia (ALL) to classify the malignant
destructive way. With lung, colon, breast, and prostate cancers, leuke- tissues with optimized deep features. The contribution of this study is
mia [6,7] is among the most frequently diagnosed cancers in the United given as follows.
States. According to projections made by the US government’s cancer
data collector, the Surveillance, Epidemiology, and End Results (SEER) ⁃The proposed research has performed image preprocessing tech-
Program, there were 60,650 newly diagnosed cases of leukemia, and 24, niques and extracted the features from the individual images with
000 death occurred in the US in 2022. According to a review of the pre-trained CNN models of four significant stages of both the healthy
cancer database by the WHO, leukemia [8] incidence varies significantly and malignant tissues.
by region and subtype. More than 20,000 cases of pediatric blood cancer ⁃This study then applied feature selection algorithms to work with
are detected annually in India, with approximately 15,000 cases of optimized deep features and track out the performance for malign
leukemia only [9]. Around 61,780 instances of leukemia were diagnosed classification.
in the United States in 2019, with another 9900 cases being found in the ⁃ To achieve the optimum results in multi-class leukemia classifica-
United Kingdom. From 345,000 in 1990 to 518,000 in 2018, the number tion, the proposed research has applied the nature inspired algo-
of newly diagnosed cases of leukemia increased, lowering the Annual- rithms to find the best features from the extracted features and apply
ized Survival Insusceptibility Rate (ASIR) by 0.43% per year [10,11]. the ML based classifiers and interprets the calculated experimental
The subtypes of leukemia are Acute Leukemia (AL) and Chronic data accordingly.
Leukemia (CL). The progression of CL is usually gradual. On the other
hand, without specialized care, the average life expectancy for those The manuscript is classified into five sections. The section two rep-
with AL is only 3 months [6]. There are two subtypes of AL recognized resents a comprehensive study of previous works. Section three provides
by the French-American-British (FAB) classification system: Acute proposed method and working principles. The section four illustrates the
Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL) results with the relevant discussion. Finally, the section five represents
[12]. Additionally, Chronic Myeloid Leukemia (CML) and Chronic the conclusion of this manuscript.
Lymphocytic Leukemia (CLL) are the 2 different types of CL [12–15].
Acute Lymphocytic Leukemia (ALL) is a fatal malignancy that affects 2. Literature review
both adults and children, making up around 25% [16] of all malig-
nancies diagnosed in kids. Many machine learning and deep learning techniques were used to
Again, ALL is often referred to as acute lymphoblastic leukemia. The identify or classify the ALL (acute lymphoblastic leukemia) type. Some
term "acute" denotes that, if neglected, leukemia can spread swiftly and of the previous papers are described in this section.
be lethal within a matter of months. The term "lymphocytic" refers to the Researchers offer a novel Bayesian-based optimized CNN method for
fact that it originates from lymphocyte precursors, a subset of white identifying ALL in microscopic smeared images in study [8]. A hybrid
blood cells. Leukemia that begins in the bone marrow and spreads dataset was formed to be used in this study by combining two subsets of
rapidly is called Acute Lymphoblastic Leukemia (ALL) [17,18] or acute ALL-IDB datasets (ALL-IDB1 and ALL-IDB2). The hybrid dataset consists
lymphocytic leukemia (ALL) [19]. The rapid proliferation of leukemic of 368 blood smear photos. In the test set, the optimized CNN model for
cells in the blood and their subsequent spread to other organs and sys- ALL identification was found using the Bayesian optimization technique,
tems of the body [20], such as the spleen, lymph nodes, liver, brain, and and it achieved maximum accuracy of 100%.
neurological system, can produce a wide range of various symptoms The article [11] suggested an approach of convolutional neural
[21]. Some of these symptoms include bruising, bleeding from the gums network called SK U-Net to perform the task of nucleus segmentation for
and nose, fever, swollen lymph nodes, sore joints, and infections [22]. ALL. All 198 input photos come from the publicly available database
Bone marrow and blood are the primary organs affected by Acute ALL-IDB2. The SK U-Net achieved a higher Dice score of 0.916 than the
Lymphoblastic Leukemia [8,23,24]. Since it occurs more frequently in traditional U-Net, which only achieved a score of 0.320. A 98% accuracy
children than chronic or myeloid leukemia, the term "acute childhood is achieved with SVM, which is significantly higher than other methods.
leukemia" has been coined to describe this condition. It can be cured if The proposed method has a higher accuracy of 0.97% than prior
caught early enough, but if not, it can kill in a matter of months if left methods. Additionally, KNN and SVM achieved an accuracy of 0.85%
untreated [25–27]. L1, L2, and L3 are ALL subtypes recognized by the and 0.98%, respectively.
FAB categorization system. The nuclei of the L1-type cells are tiny in size The latest developments in ALL detection and categorization using
and have homogeneous chromatin, few nucleoli, and a modest amount deep and machine learning are presented in study [16] through a sys-
of basophilic cytoplasm. Nevertheless, L2 of enormous size exhibits tematic review. This article thoroughly examines the advantages and
uneven nuclear structure and clefting. L3 are very large or disadvantages of many different AI-based ALL detection methods.
medium-large, with prominent cytoplasmic vacuoles. The lifespan rate Lastly, a range of tough topics and potential future scopes are presented,
can be increased with appropriate therapy, but only if the cancer is which may inspire readers to develop their own research questions in
detected early and correctly [21,28]. ALL areas.
Hematologists perform blood smears or bone marrow examinations For the categorization of ALL using microscopic pictures of white
under the microscope to diagnose ALL and its subtypes. Nevertheless, blood cells, a powerful and effective hybrid InceptionV3 XGBoost
the accuracy of these tests depends on the expertise of the examining framework was developed in paper [19]. Both InceptionV3 and the
pathologists and may be compromised by the microscope’s extended use XGBoost model are used in the proposed approach, while Inception is
[21,29–31]. Besides, the hematologists frequently makes an oversight responsible for extracting image features, and XGBoost handles cate-
while determining a blood cancer diagnosis, which requires an excessive gorization. The proposed method used the transfer learning capability of
amount of time. Automatic diagnosis methods are urgently needed to pre-trained CNN architectures to train classifiers for the ISBI C-NMC
decrease the reliance on manual examination, speed up the procedure, 2019 dataset. The database contains white blood cell photos of 10,661.
and improve the precision of leukemia identification. In recent years, The proposed hybrid model obtains an F1 score of 0.986.
human-centric clinical diagnosis powered by Machine Learning (ML), In order to acquire reliable classification results from the model’s
2W. Rahman et al. A r r a y18(2023)100292
training on the bone marrow pictures, a robust segmentation method- accuracy). On the other hand, AlexNet +SVM, GoogLeNet +SVM, and
ology along with deep learning techniques and a CNN are utilized in ResNet50 +SVM hybrid models obtained 100%, 98.1% and 100% ac-
study [21]. Amreek Clinical Laboratory in Pakistan provided the dataset curacy, respectively.
for this study. The experimental results indicate that the suggested For leukemia diagnosis, research [38] offered an intelligent method
approach has a high accuracy of 97.78%. for automating the process of detecting lymphoblasts (blast cells) in the
Article [22] proposed a new approach to categorizing and identi- single-celled image. A CNN is used in the method’s implementation to
fying ALL using SVM and CNN. In the proposed research, lymphocyte distinguish malignant from healthy blood cells. The C NMC 2019 dataset
identification is followed by retrieving CNN features using the Alex-Net was used to train and evaluate the proprietary ALLNET model. The
Model. Finally, SVM is used to classify the discovered cell as either dataset consists of 10,661 pictures. The highest accuracy achieved by
normal or cancerous. In order to conduct this research, 4000 blood the custom deep learning ALL-NET classifier was 95.54.
smears lymphocyte samples were collected from the Hayatabad Medical The goal of study [39] is to develop methods for rapid and accurate
Complex in Peshawar, Pakistan. The accuracy of the proposed method is detection of ALL cells in order to stop cancer from spreading in children.
98%. CNN was utilized in this method to identify every cell that was present
The Acute lymphoblastic leukemia is diagnosed with the use of a ViT- on the blood smears sample slide. The main focus of this research is to
CNN ensemble model in article [33]. Vision transformer and Convolu- provide clinicians in hospital Hematology Laboratories with a better tool
tional Neural Network (CNN) models were combined together to for detecting ALL utilizing CNN. The accuracy of CNN in detecting ALL
generate the ensemble model. The study used a noisy, unbalanced ISBI cancer was determined to be 98.53%.
2019 dataset (consisting of 10,661 cell images). For the evaluation set, Study [40] proposes a method that uses CNNs (Convolutional Neural
the proposed ViT-CNN model achieved an accuracy of 99.03% in its Networks) to identify WBC and then investigate ALL illnesses auto-
classification. matically. Using the ALL IDB dataset, the efficiency of three different
To divide ALL into two groups, a convolutional network with 10 pre-trained CNN models (VGG, Alexnet, and GoogleNet) was compared.
layers and two-by-two max-pooling layers (with strides of 2) was pro- The results show that GoogleNet and VGG were superior to AlexNet in
posed, and 6 widely used machine learning approaches were con- terms of pre-trained models, with both obtaining 100% accuracy
structed in the research [34]. Both the ResNet50 and VGG16, which are throughout training. According to the results of the tests, VGG has the
both well-known deep learning networks, were utilized in this study. highest performance, with an accuracy of 99.13%.
The dataset was gathered from a CodaLab competition. In addition, the The purpose of study [41] is to develop a deep learning model for
validation accuracy of VGG16 is 84.62%, ResNet50 is 81.63%, and the detecting acute leukemia from images of lymphocytes and monocytes,
proposed convolutional network is 82.10%. Moreover, the accuracy of utilizing a customized framework. In order to classify pictures of acute
machine learning classifiers is as follows: 81.72% for RF, 79.88% for LR, leukemia, a CNN model was presented that combined the Tversky loss
79.28% for SVM, 77.89% for KNN, 68.91% for SGD, and 27.33% for function and the Adam optimizer with the four dense layers, six
MLP. convolution layers, and a Softmax activation function. The database was
In order to classify photos of ALL and healthy cells, study [35] pre- obtained from the Shahid Ghazi Tabatabai Cancer Center in Tabriz. The
sented an attention-based CNN. The suggested method consists of a suggested approach correctly identified 99% of cases of acute leukemia,
CNN-based model that employs a module named ECA (Efficient Channel such as ALL and AML (Acute Myeloid Leukemia).
Attention) in conjunction with the VGG16 to extract higher-quality Using the EfficientNet-B3 CNN framework, study [42] categorized
feature information from the image dataset. The study utilized the ALL as a model that automatically modifies its own learning rate. The
C-NMC dataset which consists of 10,661 single cell pictures. The suggested model was tested using the C-NMC Leukemia dataset which
experimental outcomes demonstrate the effectiveness of the suggested contains 27,558 pictures of RBCs. Recently developed classifiers were
CNN model in extracting deep features, with an accuracy of 91.1%. used to assess the proposed model. In general, the proposed model had
A quick and accurate diagnosis of ALL can be aided by the instance an accuracy of 98.31%, and the Disc similarity coefficient (DSC) of
segmentation proposed in article [36], accomplished by applying Mask 98.05%. The proposed methodology was also used to separate healthy
R–CNN on microscope pictures of white blood cells. The present and parasitized microscopic pictures with an average accuracy of
research used the transfer learning method to build Mask R–CNN in 97.68%, proving its usefulness beyond detecting ALL.
order to fit the instance segmentation problem on microscopy white The article [43] proposes an autonomous approach that gives users
blood cell images. To fix the issue of poor lighting in stained white blood access to a widely-used classifier for improved facial expression recog-
cell microscope images, the proposed method applied a contrast nition. The system is broken down into two primary machine-learning
enhancement method to the dataset. An actual dataset from Dr. Soetomo phases: feature selection and feature categorization. The Active Shape
Hospital in Surabaya was used by the proposed method. The system Model (ASM), which is made up of landmarks, is used to perform feature
achieved 83.72% accuracy. selection, and the classification of features has been evaluated using
Research [37] includes the following 3 proposed systems: the first seven different well-known classifiers, namely KNN, NB, DT, Quadratic
consists of a feed forward neural network (FFNN), an ANN (artificial classifier, RF, MLP, and SVM. The experimental findings showed that the
neural network), and an SVM. These three components are all based on Quadratic classifier gives excellent performance and has the best accu-
hybrid features that were retrieved with the LBP (Local Binary Pattern), racy of any classifier tested (92.42%).
Gray Level Co-occurrence Matrix (GLCM), and FCH (Fuzzy Color His- Studies [44] have provided an overview of tools for detecting cancer
togram) methods, respectively. CNN models AlexNet, GoogleNet, and and techniques for treating it. The study aimed to develop accurate
ResNet-18, trained with the transfer learning approach, are the basis of colon cancer survival prediction models by analyzing data from the
the second suggested system. These architectures were used to suc- SEER program. The authors also evaluated different classification sys-
cessfully extract and classify deep feature maps. The third proposed tems to estimate mortality rates within five years of diagnosis. Results
method combines CNN and SVM algorithms to extract and categorize showed that the deep autoencoder model yielded the best prediction
feature maps. The dataset of ALL IDB1 and ALL IDB2 were used in the performance (97%) and AUC-ROC (95%), respectively.
study. A total of 108 pictures are in the ALL IDB1 dataset and the In the study [45], a unique hybrid AlexNet-gated recurrent unit
ALL-IDB2 dataset consists 260 images. The accuracy of the ANN and (AlexNet-GRU) model was used to detect and classify breast cancer in
FFNN were 100%, where the SVM had an accuracy of 98.11. Addition- lymph nodes (LNs). Three models, including the suggested
ally, for the ALL IDB1 and ALL IDB2 dataset, the AlexNet, GoogleNet and AlexNet-GRU, the convolutional neural network GRU (CNN-GRU), and
ResNet achieved an accuracy of 100%. ResNet-18 +SVM outperformed the CNN long short-term memory (CNN-LSTM), are evaluated and
the competition on the ALL IDB1 and ALL IDB2 datasets (100% contrasted in the present study. The experimental results showed that
3W. Rahman et al. A r r a y18(2023)100292
Fig. 1. Overall system illustration of the proposed system.
the proposed AlexNet-GRU model outperformed the CNN-GRU and classification with the conventional Machine Learning (ML) classifiers.
CNN-LSTM models on all performance metrics, with an accuracy of Fig. 1 shows the overall proposed system illustration with existing
99.50%, respectively. components.
The research [46] introduces the Histogram of Directional Gradient In this figure, first, the dataset is collected from the secondary
(HDG) and the Histogram of Directional Gradient Generalized (HDGG), sources. After that, image pre-processing has been applied to enrich the
two revolutionary new descriptors for collecting discriminant facial dataset. After that the images has been provided to the pre-trained CNN
expression features that outperform existing classifiers in terms of ac- models to extract the significant features. Then, the research has applied
curacy and efficiency in feature extraction. The proposed descriptors are the feature optimization techniques to find the best features from the
grounded in linear classification using SVM and directional local gra- collected image features. After that, the dataset is spitted into two
dients. Low-dimensional characteristics are employed to improve clas- identical parts training data and test data where train data is provided to
sification performance, allowing for more accurate face and expression the ML models with existing traditional models to find the experimental
recognition to be developed. Compared to other works already pub- results. Also, test data has applied to evaluate performance analysis
lished, the experiment’s findings demonstrate an accuracy of 92.12%. matrices. After finding the experimental results, a set of comparison
Many previous work [36,38–41] only used DL techniques and on the have been performed (see Table 1).
other hand, some work [22,34,37] used both ML and DL techniques to
detect ALL cases from the image dataset. Optimizer algorithm is used in 3.1. Dataset
the model to reduce the loss and improve the accuracy level of the
model. Only paper [36–38,41] used optimizer algorithm. In this pro- This research has been taken the dataset from the secondary sources
posed framework both ML & DL techniques are used with 2 optimizer more specifically from Kaggle [47]. The dataset is comprised with 3262
algorithm PSO & CSO which improvised the accuracy level of the pro- images of actual peripheral blood smear images. The images were
posed work. Additionally, some previous work [34,36,37,40,41] dataset included from the 89 patients where 25 patients were suspected as
size was very poor and it effects the model’s performance. Besides, most healthy individuals and rest of the 64 patients were suspected as Acute
of the previous studies worked on the binary classification of the Lymphoblastic Leukemia (ALL). The dataset is classified into two iden-
infected blood cell to detect malignant. Multiclass classification was tical classes such as Benign and malignant categories and further
merely used in these studies with less significances. Thus, the proposed reshaped the dataset into four significant classes with three subtype of
model used a large dataset that mainly focuses on the multiclass clas- malignants namely, Benign, Early Pre-B, Pre-B and Pro-B. All of the
sification of the infected cells of blood tissues along with its noteworthy images were captured with a Zeiss camera in a microscope at 100×
stages. Further, seven traditional ML algorithms with five DL methods magnification and stored the images as JPG format in the storage. The
are utilized to classify the Acute Lymphoblastic Leukemia (All). More- types and subtypes of these images were carefully and conclusively
over, PCA, LDA and SVC feature selector algorithms were used in the determined by a specialist using flow cytometry. Table 2 shows the
present study with PSO and CSO optimization algorithms. corresponding sample images for this research.
3. Materials and methodology
3.2. Feature extraction
In this section, methodology of the research is described. The section
In this sub-section, a mechanism of feature extraction is described.
is classified into four interconnected subsections such as the research
Firstly, the pipeline of feature extraction will be presented with algo-
dataset, feature extraction with pre-trained Convolutional Neural
rithmic annotations. Then, the mechanism of feature selection will be
Network (CNN) models, the extraction of the feature vectors and
illustrated. After, this subsection will present the working principles of
4W. Rahman et al. A r r a y18(2023)100292
Table 1
Comparison between the previous work and proposed framework.
Ref Algorithms/Models Techniques Optimizer Accuracy
[22] SVM, CNN, Alex-Net Model ML +DL – 98%
[41] CNN, Tversky loss function DL Binary cross-entropy optimizer, 99%
Adaptive movement estimation (Adam)
[40] CNN, VGG, Alexnet, and GoogleNet DL – 99.13%
[39] CNN DL – 98.53%
[38] CNN, cross-entropy loss function DL Adaptive movement estimation (Adam) 95.54%
[37] Feed forward neural network (FFNN), ANN, SVM, AlexNet, GoogleNet, ResNet-18, CNN ML +DL Adaptive movement estimation (Adam) 100%
[36] Mask R–CNN DL Stochastic gradient descent (SGD) with a 83.72
momentum coefficien
[34] CNN, ResNet-50, VGG-16, RF, LR, SVM, KNN, SGD, MLP ML +DL Adaptive movement estimation (Adam) 84.62%,
Proposed CNN, VGG19, ResNet50, InceptionV3, Xception, Support Vector Machine (SVM), Random ML +DL Particle Swarm Optimization (PSO), Cat 99.84%
Method Forest (RF), Decision Tree (DT), Naive Bayes (NB), Extreme Gradient Boosting (XGB), K- Swarm Optimization (CSO)
Nearest Neighbor (KNN), Logistic Regression (LR)
Particle Swarm Optimization (PSO) and Cat Swarm Optimization (CSO) Algorithm 01
along with the algorithmic annotations and interpretations. This sub- Working mechanism of proposed pipeline to extract feature vectors
section will present the feature vectors and the working mechanism of
conventional classifiers. Input: 2D Images
Output: Feature Vectors
Initialization :
3.2.1. Feature Extraction with Pre-trained CNN
1. n =2N-1, Where N =1, 2,3, 4 … … …
In this research, four conventional pre-trained Convolutional Neural ….n
Network (CNN) models have been applied to extract the features from 2. X ← Input Image
the single images. Fig. 2 shows the corresponding diagram of respective 3. Yn ← Apply the median filter on the input image X using the karnel
pipeline of feature extraction mechanism of this study. In this figure, size n ×n
initially, the system extracts the images from the dataset to feed the pre-
4. Fv ← Respective Feature Vector
Start :
trained models to extract the images. Four traditional pre-trained CNN 1. for each N:
architecture namely, VGG19, ResNet50, InceptionV3 and Xception have 2. Find Yn
been applied sequentially to extract feature vectors from the images. 3. Use (X, Yn) to get Fn |Fn {P0, P1, …,
After extracting the features, seven conventional classifiers have been
P14}
4. Fv ← Fn
implemented to classify the images. Due to work with best features, two 5. End for
nature inspired algorithms have been implemented and performed the 6. Show Fv
feature selection method. After that the system measures the perfor- End :
mance based on their significant classes. The whole pipeline follows the
Algorithm 01 to extract the feature vectors from a particular image.
Table 2
Sample images of each classes of the dataset.
Class Name Benign Early Pre-B Pre-B Pro-B
Samples
5W. Rahman et al. A r r a y18(2023)100292
Fig. 2. Pipeline of the proposed Deep Learning method for feature extraction.
Fig. 3. The architecture of VGG19.
The Algorithm 01 shows efficient view of feature extraction from a The research also utilizes the pre-trained ResNet50 CNN architecture
particular image. In this procedure, the system first initialized the [48] in the proposed pipeline to extract the features from a particular
dataset based on the number of images. Where X represent the input image. ResNet50 identically consists of 50 layers with having approxi-
image and Y is the output after applying the image filtering and resizing. mately 2M parameters. The ResNet50 architecture has several parts to
Then, the pseudocode represents a loop structure to enumerate the constitute the model. The first part contains 64 kernels with a
feature vectors. max-pooling layer, convolution layer and fully connected layer. The
Fig. 3 shows the architecture of VGG19 pre-trained CNN architecture augmentation layer permits dilapidation problems and eliminates the
that has been included in the pipeline in Fig. 2. VGG19 is fine-tuned with disappearing problem. Besides, the skip connection act like a
some of the layers to ignore the overfitting issues for a small dataset. In super-pathway. The research predominantly includes the ResNet50 for
this architecture, the pre-trained model is comprised of a series of feature extraction and excludes the classifier part. The proposed model
Convolutional Layers (CL) and single or multiple Fully Connected (FC) with ResNet50 CNN architecture accepts the Blood cell images of 224 ×
layers. The model is identically classified into two interconnected parts. 224 ×3 and assembles 2048 features from the out of the last layer of
The first part denotes the feature extraction part from the input layer to feature extraction part for each image. Fig. 4 shows the architecture of
the last max-pooling layer. The second part represents the residual the corresponding ResNet50 model. After that, the research utilizes the
network of the model, which is mainly responsible for the classification. Inception V3 architecture. InceptionV3 is known as GoogleNet, and the
The proposed solution mainly focuses the VGG19 model on feature model itself is a pre-trained network model. The Inception model has 22
extraction; thus, the classification part is declined in this study. The layers having 5 M parameters with a filter size of 1 ×1, 3 ×3 and 5 ×5
proposed model with VGG19 accepts the Blood cell images of 224 ×224 to extract features at various scales through max pooling. After intro-
×3 and assembles 4096 features from the out of the last layer of feature ducing InceptionV3, the 5 ×5 convolutional filters are replaced with
extraction part for each image [48]. two 3 ×3 filters to reduce computation discarding the performance of
6W. Rahman et al. A r r a y18(2023)100292
Fig. 4. The architecture of ResNet50.
Fig. 5. The architecture of Inception V3.
networks. The InceptionV3 consists of 48 layers and fine-tuned structure Linear SVC. The main objective of the SVC is to fit the data and return
to avoid overfitting. In the proposed pipeline, InceptionV3 model takes the "best feature" hyperplane that has the ability to categorize the data
the Blood cell images of 299 ×299 ×3 and extracts 2048 features from from a certain dataset [50]. In the proposed model, the SVC feature
the out of the last layer of feature extraction part for each image [48]. selector works as In Algorithm 02.
Fig. 5 illustrates the respective diagram of proposed InceptionV3
architecture. Algorithm 02
Working mechanism of the proposed SVC Feature Selector in feature selection
3.2.2. Feature Selection Models
This subsection provides the working principles of the feature se- Input: Images Features
lection models in our proposed system. In the research, the model Output: Best Feature Vectors
Initialization :
mainly develop with three significant feature selection models namely, 1. X =N-1, Where N =Number of features of a particular CNN
Principal Component Analysis (PCA), Linear Discriminant Analysis model.
(LDA), and Support Vector Classifier (SVC) feature selector. PCA and 2. Y ← No. of classes
LDA [49,50] are favorable algorithms to diminish the feature vectors 3. Xn ← No. of training data
[50]. PCA is an unsupervised learning algorithm and mainly focuses on
4. Yn ← No. of testing data
5. Fv ← Respective Best Feature Vectors
enhancing the variation in a particular dataset. On the contrary, LDA 6. Sv ← No. of Selected Best Feature Vectors
and SVC feature sector are the supervised learning method that focuses Start :
on a feature vector subspace that improves the separability between the 1. feature =SVMFeatureSelection (Xn, Yn)
groups. In the research, PCA, LDA, and utilize with some sort of 2. if (No. of feature>0.5)
:
following mathematical expression.
3. Sv =features
PCA can effectively work with the construction of covariance matrix. 4. Fv =add(all the Sv)
A Symmetric d ×d-dimensional covariance is prerequisite to build PCA 6. End if
where d denotes the number of dimensions in a certain dataset and holds 5. Show Fv
the pairwise covariance’s between diverse noteworthy features [50]. For End :
instance, assume Xj and Xk denotes the two features of the targeted
population. Then, the covariance can be calculated by the following
Fig. 6 depicts the block diagram of the feature selection method used
Equation (1).
by these feature selectors to identify the "Best Feature." In this figure, the
σ jk=1 n∑ n i=1( x j(i)(cid:0) μ j)( x( ki)(cid:0) μ k) (1) s fey es dte sm
th
o eb st ea i vn es
c
t te os rt
s
d ta ot a t,
h
c eo an lv ge or rt is
t
hth me
s
d ia nt a
o
i rn dt eo
r
f te oa t fiu nre
d
v te hc et o mrs o, sa tn ad
p
t ph re on
-
priate features for dealing with conventional classifiers. Lastly, the
But, LDA works with the five interconnected steps. Initially, LDA
model computes the level of precision with the assistance of ML
calculates the respective d-dimensions of the mean vectors. Then LDA
classifiers.
creates scatter matrices and computes eigenvectors. Then, LDA sorts the
eigen vectors in decreasing order. Then, a matrix multiplication results
in the corresponding features reduction or feature selection.
On the other hand, SVC works with the particular problem with
7W. Rahman et al. A r r a y18(2023)100292
Fig. 6. The proposed architecture of the feature selector algorithms.
3.2.3. Working Principles of Proposed Particle Swarm Optimization (PSO) Algorithm 03
and Cat Swarm Optimization (CSO) PSO algorithm for best particle calculation
This subsection represents the two significant inspired algorithms for
the selection of the best feature that can optimize the level of accuracy. Input: Set of random particles
In the research pipeline, the research includes PSO and CSO algorithm to Output: Best fitted particles
Initialization :
find best features also work with the minimal number of best fittest 1. Initialize random particle P ={P1, P2, P3, …Pi }.
features. 2. t⟵ Time
Particle Swarm Optimization (PSO) is the most prominent and 3. cl⟵ cognitive factor
influential meta-heuristic-based optimization model. This algorithm is 4. c2⟵ Social factors
mainly inspired by nature’s significant behaviors, specifically in fish and 5. r1, r2 ⟵ the value in the interval [0, 1]
6. w⟵ inertia weight
bird schooling. The algorithm is called a heuristic solution because the 7. pbesti⟵ best position of Pi
model always tends toward the global optimal. In nature, any of the 8. gbest ⟵ global best position of the particle
birds in a particular swarm has minimal observable proximity to the Start :
observer. But, more than one bird out of these birds allows all the swarm 1. while (best solution)
2. for each Pi
birds to be conscious of the more excellent surface of a fitness function
[51].
3. U i(cid:0)pd Xa it (e
t
)t )he
+
v ce 2lo
r
2c (it gy bV ei s( tt + (cid:0)1 X) i= (t)w )Vi(t)+c 1r 1(pbest
Assume, P is the number of particles and i denotes the position of 4. Update position Xi(t+1)=Xi(t)+Vi(t+1)
each iteration t as Xi(t). Let’s consider the Xi(t) as a coordinate like Xi(t) 5. Use objective function f to evaluate the fitness value of Pi
= (xi(t), yi(t)). Also, consider the velocity of the each particle will be 5. U{pdate pbesti (t)|pbesti (t+1)=
d lie kn eo Ete qd u aa ts i oV ni( st () 2 = )–( (v 4i x )( .t ), vi y(t)). Then the position of the particle will be 6. Upp ( dpb i ae ( ts t et +i ( gt b1) e)i sf i tf (f t( f )p ( |b p ge b bs e et s si( t tit ( () t t) ) +)≤ 1> )f( f =p (i p( mit ( a+ t x+ {1 f1) () )) pbesti (t)),f(ggest(t))}
Xi(t+1)=Xi(t)+Vi(t+1) (2) 7. End for
End while
End :
xi(t+1)=xi(t)+vi x(t+1) (3)
yi(t+1)=yi(t)+vi y(t+1) (4) Chu and Tsai first developed cat swarm optimization (CSO) in 2007,
and it functions in both Seeking Mode (SM) and Tracking Mode (TM).
Then, the following equation can be written like Equation (5)
However, in TM, cats move to their next location with some velocities,
(cid:0) ) (cid:0) )
Vi(t+1)=wVi(t)+c 1r 1 pbesti(cid:0) Xi(t) +c 2r 2 gbest(cid:0) Xi(t) (5) showing how cats pursue their target. In SM, cats do not move and
remain in a specific position and feel for the next best move. Fig. 8 shows
In Equation (5), r1, r2 represents the number in a range [0, 1]. Also, the working flow chart of the CSO algorithm [52].
w, c1, c2 are the one of the significant parameters of PSO algorithm To work with the CSO, some of the parameters need to be noticed like
and pbesti is the position that gives calculated best fit F(x) value for Number of Dimension of Variations (CDC), Mutative ration for the
the particle i. Finally, gbest is the measured value explored by all selected dimensions (SRD), Number of Duplicate Cats (SMP), and Passed
the particles in the swarm. Algorithm 03 shows the corresponding position as one of the candidates (SPC). The position change of the
steps of the algorithm and Fig. 7 show the summary of the algo- duplicate cats can be enumerated with following Equation (6):
rithm with the flow chart. X mn=1+SRD×R×X m (6)
8W. Rahman et al. A r r a y18(2023)100292
Fig. 7. The flowchart of PSO algorithm.
9W. Rahman et al. A r r a y18(2023)100292
range of 0–1. Xbest,i = i-th best position of the cat. Xl,i = The current
Position R =[0, 1].
Then the update position of each cat will represented by:
X l,i=X l,i+v l,i (9)
Algorithm 05. CSO algorithm in Tracking Mode (TM)
Input: Set of random particles
Output: Best fitted particles
Initialization :
1. Initialize random particle.
2. i ⟵ Cat number M =1, 2, 3, 4 ….
3 r ⟵ the value in a range 0–1
4. Mmax ⟵ Max Speed
5. v =velocity of the cat
Start :
1. Update the current velocity by using Equation (8)
2. for (v ≤r)
4. Execute the subsequent part with v
5. Else
6. v =Mmax
7. End if
8. Update the position of the each cat by using Equation (9)
End :
Fig. 8. The flowchart of CSO algorithmCat Swarm Optimization (CSO). 3.3. Classification with the conventional classifiers
In this study, ML methods are applied to the features data extracted
Where,Xmn =The current PositionR =[0, 1].
from cases of acute lymphoblastic leukemia. To achieve this, the study
The algorithm will set the calculated probability of each candidate
makes use of established classifiers found in current ML practice. A total
points if the Fitness Function (FS) is considered with Equation (7). Other
of seven different classifiers, including Support Vector Machine (SVM),
the algorithm will set the value to 1.
Random Forest (RF), Decision Tree (DT), Naive Bayes (NB), Extreme
P m= F|F SS mm ax(cid:0)
(cid:0)
F FS Sm min in| (7) sG ir oa nd (ie Ln Rt
)
B uo seo s tt oin cg
o
n(X stG ruB c) t, tK h- eN fiea nr ae ls mt N ode eig
l
h [b 5o 3r
]
.( KNN), Logistic Regres-
Where, 0 < m < j.Algorithm 04 3.3.1. Support Vector Machine (SVM)
CSO algorithm Seeking Mode (SM) SVM is a classifier and regression algorithm that can classify any
object based on a given set of data. It generates the optimal decision
Input: Set of random particles boundary, which divides n-dimensional spaces into distinct classes,
Output: Best fitted particles thereby facilitating future data insertion. The classification was con-
Initialization
structed using the kernel trick mechanism of SVM, and its corresponding
1. Initialize random particle the parameter SPC, SMP, SRD, SPC, and
CDC. equation is shown in Equation (2) for the kernel trick of SVM. For a two-
2. j⟵ Duplicate cat position dimensional non-linearly separable dataset, kernel trick transforms the
3. m⟵ having value 0 <m <j two-dimensional data to a higher dimension, such as three, four, or ten
Start: dimensions. This method is known as the kernel trick.
1. If (SPC! =0) (cid:0) )
2. J =SMP -1 Kerneltrick.k x i,x j =x i.x j (10)
3. Else
4. J =SMP
5. End if 3.3.2. Random Forest (RF)
6 Make J duplicate cat positions The machine learning technique known as Random Forest is used for
7. for (m =1 to SMP) both classification and regression. An ensemble classifier is another
8. Compute the value of Xmn randomly using SRD & CDC using Equation
name for this algorithm, which incorporates several decision tree
(6)
9. Replace the old value models. A classifier like this one takes the results of many decision trees
10. Calculate the probability Pm using Equation (7) applied to various subsets of a dataset. It averages them in order to
11. End for improve the accuracy of the dataset’s overall predictions. A more sig-
End :
nificant number of trees can be accurately assessed using this method.
3.3.3. Decisoin Tree (DT)
The algorithm mainly works with two type of search strategies such
An example of a supervised learning algorithm is the decision tree
as SM and TM. The working procedures of SM is depicted in Algorithm
(DT). A tree-based data structure represents the algorithm. The Decision
04 and The TM is illustrated in the Algorithm 05. In TM, the velocity of
Tree is a graphical representation of a collection of rules for making
each individual cat is represented by the following Equation (8).
decisions based on a dataset’s features. The tree’s internal nodes reflect
v l,i=v l,i+r×q×X best,i(cid:0) X l,i (8) the features of the dataset, while the tree’s branches represent the rules.
The DT is a helpful tool for visually representing any Boolean value (true
Where, i =1, 2, 3, 4 …...M, here M =cat number. r =random value in a or false). The equation for DT can be seen in Equation (10).
10W. Rahman et al. A r r a y18(2023)100292
( ) ( ) ( ) ( )
1 1 0 0 Table 3
InformationGain,S=(cid:0) P log P (cid:0) P log P
true 2 true false 2 false Description of performance evaluation matrices.
(11) Metrics Description
Accuracy Displays the overall right prediction percentage.
3.3. T4 h. eN pa rï ov be aB ba ily ie tys ( oN
f
B a)
h ypothesis can be calculated using Bayes’ the-
(A) A= FP+TT PP ++ FT NN +TN×100
orem, often known as Bayes’ Rule or Bayes’ law. Bayes’s theorem can be Precision Describes as a way to assess a model’s quality.
expressed as the following Equation (11). The NB in data classification
(P) P= TPT +P FP×100
works with this equation to find the results. Recall (R) Defines as a measurement of model quantity.
P(A|B)=P(B P|A (B) )P(A)
(12) F1-score
DR e= moT nP sT t+ rP aF tN es× ho1 w00
r eliable and accurate a model is.
(F1) F1=2×R R× +P P×100
3.3.5. XGBoost Algorithm (XGB) NCM Gives a tabular representation of the rates of correct and incorrect
The powerful XGBoost machine-learning algorithm can aid in data detection for various classifications.
analysis and decision-making. To be precise, XGBoost is a program that
uses decision trees that are boosted by a gradient. Several researchers
and data scientists around the world have utilized it to fine-tune their
machine learning models.
3.3.6. K-Nearest Neighbor (KNN)
The Supervised Learning method ML techniques use K-Nearest
Neighbor. This algorithm assumes that the new data and existing data
are similar. When placing new data into the class most comparable to the
available categories. The Manhattan equation is utilized for distance
calculation in this approach for KNN. Equation (12) gives the Manhattan
distance formula for the KNN classifier.
Manhattan distance
∑
k i=1|x i(cid:0) y i| (13)
3.3.7. Logistic Regression (LR)
A supervised classification approach is logistic regression. This is Fig. 9. The structure of the confusion matrix of the results.
used to compute the target variable’s possibility. Because the target
variable is binary, there are only two potential classes (1/success/yes) or certain image. The gathered data was then fed into ML-based models to
(0/failure/no). It is classified into three types: binary or binomial, assess performance. We divided the dataset into 80% training data and
multivariable, and ordinal. Hence equation for LR can be represented as 20% testing data for our study. Table 4 shows a summary of the models
Equation (13). after extracting the features from each individual images before per-
y=b 0+b 1x 1+b 2x 2+b 3x 3+…+b nx n (14) forming the classification.
Initially, we have chosen the CNN models with Support Vector Ma-
chine (SVM) classifier. To find the better result we have also combined
4. Results and discussion
the features to perform the feature fusion of InceptionV3 and Xception
models. Table 5 shows the summary of the results achieved from the
This section provides the experimental data analysis with Machine
pertained CNN model and SVM classifiers. We have achieved the highest
Learning (ML) based models. Firstly, this section presents the perfor-
accuracy of 98.43% with ResNet50 architecture where the closest
mance analysis with only pre-trained Convolutional Neural Network
(CNN) architecture along with ML based classifiers. Secondly, this sec-
tion illustrates experimental data analysis with the natured inspired
Table 4
algorithms as well as the feature selection algorithms like Linear The overall summary of memory consumption of each model and time.
Discriminant Analysis (LDA), Principal Component Analysis (PCA) and
Model Name Memory Usage For No. of Time Required to
Support Vector Feature Selector. Finally, this section provides the per-
Extracted Features Features complete the extraction
formance analysis with different classifiers. (MB) (Apo. Hour)
To evaluate the efficacy of the proposed model a set of performance
VGG19 63.2260 4096 3.50
evaluation matrices have been enumerated such as Accuracy (A), Pre- ResNet50 47.1710 2048 2.10
cision (P), Recall (R) and F1-Score. Table 3 shows the descriptions of the InceptionV3 61.6590 4048 2.30
performance evaluation matrices with some mathematical annotations. InceptionV3 + 91.2230 4096 4.00
On the other hand, Fig. 9 shows the orientation of the confusion matrix Xception
for Blood cancer cell classification.
4.1. Performance analysis without feature selection and PSO Table 5
The overall performance of different Pre-trained CNN models.
This subsection provides the results analysis with ML based classi- CNN Model A P R F1
fiers only with the pre-trained CNN architecture. This study ran all of the VGG19 97.18 96.64 96.87 96.75
programs on the Google Colab, which has 53 GB of RAM and a dedicated ResNet50 98.43 98.28 98.32 98.30
Graphics Processor Unit (GPU). This setup’s subscription was ’pro sub- Inception V3 79.15 77.82 75.68 75.76
scription’. The research initially retrieved the important elements from a Inception +Xception 87.62 87.71 84.27 85.06
11W. Rahman et al. A r r a y18(2023)100292
Table 6 Table 7
The fold-wise performance measurement of the different Pre-trained CNN The overall performance of different classifier with VGG19.
models.
Classifiers A P R F1
CNN Model Fold 1 Fold 2 Fold 3 Fold 4 Fold 5
SVC 94.98 95.17 93.73 94.35
VGG19 95.61 96.08 96.07 96.08 96.23 RF 93.57 93.74 91.89 92.64
ResNet50 97.95 97.80 97.64 97.95 97.80 DT 84.17 82.78 83.52 83.09
Inception V3 77.89 80.41 80.72 81.03 80.87 NB 78.21 78.51 78.70 78.08
Inception +Xception 87.77 88.71 90.12 90.12 90.28 XGB 93.42 93.03 92.57 92.78
KNC 93.89 94.14 92.40 93.12
LR 95.61 95.44 94.98 95.20
accuracy was 97.18% with VGG19 model. We also ran 5-fold cross
validation on each model to see whether it could detect overfitting
Table 8
concerns. Table 6 shows the summary of cross-validation scores.
The fold-wise performance of different classifiers with VGG19.
On the contrary, Fig. 10 shows a comparison on accuracy of different
pre-trained CNN architectures. This figure clearly indicates that Classifiers Fold 1 Fold 2 Fold 3 Fold 4 Fold 5
ResNet50 provides the nearly the optimum results with the SVM clas- SVC 89.91 90.59 91.22 91.22 91.53
sifiers where InceptionV3 gives very poor performances than the feature RF 89.65 89.65 89.81 89.96 89.96
fusion and the other models. DT 77.27 78.84 79.62 80.25 78.37
NB 75.86 76.95 76.17 76.01 70.00
Besides, we have performed the seven conventional ML based clas-
XGB 89.18 89.18 89.65 90.59 89.65
sifier such as Support Vector Machine (SVM), Random Forest (RF), De- KNC 89.49 91.22 90.59 90.27 90.74
cision Tree (DT), Naive Bayes (NB), Extreme Gradient Boosting (XGB), LR 92.16 93.26 93.10 93.57 92.94
K-Nearest Neighbor (KNN), and Logistic Regression (LR). We have
interpreted the results the sequentially based on the performance
Table 9
matrices. In these experiments, we have found the satisfactory result
The overall performance of different classifier with ResNet50.
from VGG19 with LR classifier but the accuracy was not very
convincing. Because, we have tracked highest accuracy of 99.53% ac- Classifiers A P R F1
curacy with ResNet50 architecture with LR classifier. The corresponding SVC 98.43 98.28 98.32 98.30
results are shown in Tables 7–10. Table 11 shows the summary of the RF 96.86 97.04 96.57 96.78
result analysis with InceptionV3 and Xception models. The all the ex- DT 91.68 90.95 91.40 91.15
NB 90.25 90.58 90.32 90.23
periments are performed without explicitly using the nature inspired
XGB 98.27 98.15 98.31 98.23
algorithms. KNC 98.27 98.46 97.49 97.93
LR 99.53 99.39 99.57 99.48
4.2. Performance analysis with feature selection and PSO and CSO
Table 10
This subsection presents the experiment data analysis with nature The fold-wise performance of different classifiers with ResNet50.
inspired algorithm like Particle Swarm Optimization (PSO) and Cat
Classifiers Fold 1 Fold 2 Fold 3 Fold 4 Fold 5
Swarm Optimization (CSO). Initially, we have applied the PSO on the
extracted features with the pre-trained CNN architectures. After we have SVC 95.44 96.39 95.91 96.23 96.38
RF 94.82 95.60 94.81 95.29 95.60
applied the CSO.
DT 86.50 86.34 89.01 86.65 86.82
NB 70.79 75.04 78.65 78.90 78.96
1) Data Analysis with PSO XGB 95.60 96.70 97.01 97.48 96.85
KNC 95.60 95.29 95.76 95.61 95.92
LR 97.95 97.80 97.64 97.95 97.80
The summary of data analysis with PSO are illustrated in Table 10.
This table, clearly shows the research achieved the highest accuracy of
99.68% in Acute Lymphoblastic Leukemia (All) while working with ROC Curve. Table 12 shows the summary of the corresponding results
ResNet50 architecture and SVM classifier along with the PSO algorithm. with the PSO and pre-trained CNN architectures. Figs. 11–14 shows the
We have also enumerated the results with the cross validation scores learning curves, the calculated confusion matrix and AUC-ROC curve.
along with the learning curves and confusion matrix and Area Under
Fig. 10. Comparison on Accuracy of different CNN models.
12W. Rahman et al. A r r a y18(2023)100292
Table 11 Table 14 shows an overview of data analysis with CSO. This table
The overall performance of different classifier with Inception V3 +Xception. plainly demonstrates that the study got the preeminent accuracy of
Classifiers A P R F1 99.68% in Acute Lymphoblastic Leukemia (All) while using the
ResNet50 architecture, SVM classifier, and CSO algorithm. We also lis-
SVC 87.62 87.71 84.27 85.06
ted the findings with the cross validation ratings, learning curves,
RF 87.93 88.11 84.65 85.46
DT 79.62 78.38 76.80 77.37 confusion matrix, and Area Under ROC Curve. Table 14 summarize the
NB 72.88 73.17 69.25 69.51 equivalent findings with the CSO and pre-trained CNN architectures.
XGB 92.95 92.43 91.29 91.74 Figs. 15–18 depict the learning curves, the computed confusion matrix,
KNC 88.40 88.69 85.33 85.83
and the AUC-ROC curve.
LR 92.48 92.05 91.66 91.84
We have also shown how the outcomes obtained with and without
CSO can be compared. Table 15 displays the relevant results from the
Also, we have illustrated the comparison between the achieved re- various methods. This chart made it abundantly obvious that the model
sults from with PSO and without PSO. Table 13 shows the respective performed better than earlier methods after the CSO was embedded.
findings from the different techniques. This table clearly represented Additionally, we have improved our performance in matrices used to
that after embedding the PSO the model has achieved the better per- evaluate measured performance.
formance than previous techniques. Also, we have accomplished the
better performance in measured performance evaluation matrices. 4.3. Performance analysis of different feature selection techniques
2) Data Analysis with CSO This subsection presents the experimental data analysis with several
feature selector or feature reduction techniques such as PCA, LDA and
Table 12
The overall performance of different Pre-trained CNN models with PSO.
CNN Model Subset Accuracy A P R F1 Fold 1 Fold 2 Fold 3 Fold 4 Fold 5
VGG19 97.64 97.81 97.69 97.43 97.55 92.63 92.94 92.95 93.57 93.26
ResNet50 99.68 99.84 99.75 99.87 99.81 98.11 98.43 98.11 97.96 98.27
Inception V3 80.87 79.93 77.31 75.98 76.08 71.47 72.57 72.73 73.67 73.66
Inception +Xception 88.55 87.93 87.80 83.65 84.48 76.80 78.83 79.63 80.24 80.24
Fig. 11. The performance measurement of VGG19 and PSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve with PSO.
13W. Rahman et al. A r r a y18(2023)100292
Fig. 12. The performance measurement of ResNet50 and PSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
14W. Rahman et al. A r r a y18(2023)100292
Fig. 13. The performance measurement of Inception V3 and PSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
15W. Rahman et al. A r r a y18(2023)100292
Fig. 14. The performance measurement of Inception V3 +Xception and PSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
After that, we have executed the operations with ResNet50 features
Table 13 and with these feature selection techniques. With the SVC, we have
Comparison of the performance of ResNet50 with and without PSO. found immense improvement in accuracy. Table 17 shows the corre-
Techniques A P R F1 sponding results achieved from the different feature selector techniques.
Without Extracted Feature Vectors + 99.53 99.39 99.57 99.48 This table shows the highest accuracy of 98.84% in Acute Lymphoblastic
PSO ResNet50 +LR Leukemia (All) while dealing with ResNet50 architecture along with
With PSO Extracted Feature Vectors + 99.84 99.75 99.87 99.81 SVC Feature Selectors and LR classifiers.
ResNet50 +PSO +LR Further, we have applied the feature selections techniques on the
fusion features from InceptionV3 and Xception models. This orientation
SVC Feature Selector. We have accomplished the data analysis with PCA provides better results than previous where the highest accuracy in
with seven conventional ML classifiers on different pre-trained archi- blood cancer cell classification was 94.50%. Table 18 shows the corre-
tectures. Initially, we have performed the operations with VGG19 fea- sponding data for Inception V3+Xception and different feature
tures and with these feature selection techniques. After applying the selectors.
SVC, we have found massive improvement in accuracy. Table 16 shows To assess the efficacy of the proposed model we have given a com-
the corresponding results achieved from the different feature selector parison on their performances and interpreted the results accordingly.
techniques. This table shows the highest accuracy of 98.59% in Acute Table 19 provides a short overview of the performances found from
Lymphoblastic Leukemia (All) with SVC Feature Selectors and LR ResNet50 and LR classifier. In this table, we have taken the ResNet50
classifiers. architecture with LR because this model provides much better results
Table 14
The overall performance of different Pre-trained CNN models with CSO.
CNN Model Subset Accuracy A P R F1 Fold 1 Fold 2 Fold 3 Fold 4 Fold 5
VGG19 97.80 97.81 97.69 97.43 97.55 92.63 92.94 92.95 93.57 92.26
ResNet50 99.68 99.84 99.75 99.87 99.81 98.11 98.43 98.11 97.96 98.27
Inception V3 88.55 88.56 87.85 87.51 87.67 81.91 81.50 81.50 82.29 81.44
Inception +Xception 88.87 87.93 87.93 83.65 84.48 76.80 78.83 79.63 80.24 84.24
16W. Rahman et al. A r r a y18(2023)100292
Fig. 15. The performance measurement of VGG19 and CSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
17W. Rahman et al. A r r a y18(2023)100292
Fig. 16. The performance measurement of ResNet50 and CSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
18W. Rahman et al. A r r a y18(2023)100292
Fig. 17. The performance measurement of InceptionV3 and CSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
19W. Rahman et al. A r r a y18(2023)100292
Fig. 18. The performance measurement of Inception V3 +Xception and CSO with (a) Learning curve (b) Confusion Matrix (c) AUC-ROC curve.
accuracy was slightly boosted and was 99.375. But when we included
Table 15 the nature inspired algorithm with SVC feature selector, the accuracy
Comparison of the performance of ResNet50 with and without CSO.
was optimized to 99.84%. This the highest accuracy we have achieved
Techniques A P R F1 from both PSO and CSO. Fig. 19 shows a bar chart of the comparison
Without Extracted Feature Vectors + 99.53 99.39 99.57 99.48 among different techniques.
PSO ResNet50 +LR This research also reflects a comparison on the performances be-
With CSO Extracted Feature Vectors + 99.84 99.75 99.87 99.81 tween two nature inspired algorithms. Though these two algorithm
ResNet50 +CSO +LR
provides same results, there is a difference in number of feature selec-
tion. To work with these algorithm, we have occupied same number of
than any other CNN model in this research. When we applied the PCA iterations, population size as well as number of seeds. The PSO algo-
algorithm the accuracy was 98.90%. When we worked with the LDA the rithm found the number of 1019 best selected features. Whereas the CSO
Table 16
Classifiers with the Feature Selection techniques for VGG19.
Classifiers Feature Selector
PCA LDA SVC Feature Selector
A P R F1 A P R F1 A P R F1
SVC 95.30 94.50 94.67 94.57 93.73 92.81 93.85 93.10 97.81 97.69 97.43 97.55
RF 85.74 87.26 80.69 81.77 94.98 95.87 93.77 94.57 95.77 96.42 94.36 95.17
DT 73.82 72.06 72.64 72.29 85.11 83.95 83.87 83.88 86.52 85.56 85.20 85.33
NB 39.18 49.52 32.22 25.61 76.02 75.86 76.52 75.18 87.30 85.69 86.23 85.57
XGB 93.73 93.69 92.71 93.14 96.08 96.29 95.57 95.87 96.55 96.96 95.48 96.10
KNC 93.26 94.71 90.99 92.12 92.01 93.69 89.37 90.53 93.73 94.63 91.77 92.77
LR 97.02 96.93 96.53 96.72 97.49 97.30 97.18 97.23 98.59 98.43 98.45 98.44
20W. Rahman et al. A r r a y18(2023)100292
Table 17
Classifiers with the Feature Selection techniques for ResNet50.
Classifiers Feature Selector
PCA LDA SVC Feature Selector
A P R F1 A P R F1 A P R F1
SVC 97.33 96.03 97.55 96.67 97.33 96.55 97.64 96.97 98.90 98.85 98.71 98.78
RF 86.34 86.21 78.71 79.26 97.96 98.33 97.63 97.94 97.33 97.80 96.41 97.00
DT 85.56 84.18 80.08 81.09 88.54 87.33 87.63 87.47 88.54 87.63 86.84 87.17
NB 44.27 30.15 39.62 31.81 89.01 88.32 88.95 88.37 91.05 89.86 90.96 90.11
XGB 97.96 97.47 97.48 97.47 98.12 97.91 98.14 98.01 98.90 98.80 98.56 98.68
KNC 96.08 96.62 94.14 95.10 94.66 95.42 93.03 93.91 97.80 98.10 96.93 97.44
LR 99.84 99.87 99.71 99.79 99.69 99.63 99.74 99.68 99.84 99.75 99.87 99.81
Table 18
Classifiers with the Feature Selection techniques for InceptionV3+Xception.
Classifiers Feature Selector
PCA LDA SVC Feature Selector
A P R F1 A P R F1 A P R F1
SVC 90.28 90.12 87.91 88.89 87.46 87.56 84.81 85.63 87.93 87.80 83.65 84.48
RF 70.69 70.97 65.49 62.49 87.62 87.59 84.28 85.16 89.50 89.25 89.26 87.10
DT 60.42 63.68 62.89 62.92 78.53 76.88 76.48 76.66 76.33 73.04 72.91 72.91
NB 42.16 34.69 38.05 32.01 65.20 65.46 61.99 62.63 74.92 74.27 70.77 71.26
XGB 86.52 86.73 83.50 84.33 92.48 92.03 90.74 91.24 93.83 93.68 92.00 92.67
KNC 80.72 80.45 77.38 77.45 81.03 79.99 76.79 76.90 89.97 90.11 86.77 87.64
LR 91.85 91.12 90.92 90.99 93.10 92.18 91.78 91.95 94.04 93.63 92.95 93.26
Table 19
Comparison of different Feature Selection Models with ResNet50’s features and LR.
Techniques A P R F1
ResNet50 +LR +PCA 98.90 98.64 98.91 98.77
ResNet50 +LR +LDA 99.37 99.46 99.11 99.28
ResNet50 þLR +SVC Feature Selector þPSO 99.84 99.75 99.87 99.81
ResNet50 þLR +SVC Feature Selector þCSO 99.84 99.75 99.87 99.81
Fig. 19. Bar chart of comparison among the existing best techniques.
Table 20
Among the performance of PSO and CSO.
Techniques No. of Iterations Population Size No. of Seeds A P R F1 No. Best Selected Features
ResNet50 +LR +SVC Feature Selector +PSO 05 10 1234 99.84 99.75 99.87 99.81 1019
ResNet50 +LR +SVC Feature Selector +CSO 99.84 99.75 99.87 99.81 909
21W. Rahman et al. A r r a y18(2023)100292
algorithm tracked out the number of features of 909. Table 20 shows the Data availability
corresponding comparison between these two algorithms.
Data will be made available on request.
5. Conclusion
References
The many different varieties of cancer, which are the collection of
[1] Ou F-S, et al. Biomarker discovery and validation: statistical considerations 2021;
cells that are developing uncontrollably within the body, include breast
16(4):537–45.
cancer, lung cancer, skin cancer, and blood cancers like leukemia and [2] Chtihrakkannan R, et al. Breast cancer detection using machine learning 2019;8
lymphoma. One of the most important types of cancer is Acute (11):3123–6.
Lymphoblastic Leukemia (ALL). This study examines the application of a [3] Chaurasia V, et al. Prediction of benign and malignant breast cancer using data
mining techniques 2018;12(2):119–26.
novel technique for categorizing Acute Lymphoblastic Leukemia using [4] Solanki YS, et al. A hybrid supervised machine learning classifier system for breast
cutting-edge technologies like Machine Learning (ML) and Deep cancer prognosis using feature selection and data imbalance handling approaches
Learning (DL). The major components of the proposed research pipeline 2021;10(6):699.
[5] Ahmad S, et al. A novel hybrid deep learning model for metastatic cancer
include dataset construction, feature extraction using Convolutional
detection. 2022. p. 2022.
Neural Network (CNN) architectures that have been pre-trained from [6] Fujita TC, et al. Acute lymphoid leukemia etiopathogenesis 2021;48:817–22.
each individual image of a blood cell, and classification using traditional [7] Leukemia—Cancer Stat Facts. [cited 2023 14 March]; Available from: https://seer.
cancer.gov/statfacts/html/leuks.html.
ML-based classifiers. The dataset is split into two similar catego-
[8] Atteia G, Alhussan AA, Samee NAJS, Bo-Allcnn. Bayesian-Based Optimized CNN for
ries—benign and malignant—and then reconfigured into four signifi- Acute Lymphoblastic Leukemia Detection in Microscopic Blood Smear Images
cant classes, each of which has three subtypes of malignant, namely 2022;22(15):5520.
[9] Cancer Research Uk [cited 2023 17 March]; Available from: http://www.cancerr
benign, early pre-B, pre-B, and pro-B. The research first extracts the
esearchuk.org/.
features from CNN models, and then feeds the extracted features to [10] Dong Y, et al. Leukemia incidence trends at the global, regional, and national level
feature selectors such as Principal Component Analysis (PCA), Linear between 1990;9:1–11. 2017. 2020.
[11] Cranaf, R., G. Kavitha, and S. Alagu, A decision Support system for the
Discriminant Analysis (LDA), and SVC Feature Selectors, along with two
identification of acute lymphoblastic leukemia in microscopic blood smear images.
nature-inspired algorithms such as Particle Swarm Optimization (PSO) [12] Mishra S, et al. Texture feature based classification on microscopic blood smear for
and Cat Swarm Optimization (CSO). The seven ML classifiers have acute lymphoblastic leukemia detection 2019;47:303–11.
thereafter been used in research. A collection of experimental data has [13] Das PK, Jadoun P, Meher S. Detection and classification of acute lymphocytic
leukemia. IEEE-HYDCON; 2020. 2020. IEEE.
been compiled and analyzed in order to evaluate the effectiveness of the [14] Patel N, Mishra AJPCS. Automated leukaemia detection using microscopic images
suggested architecture. The research first worked with pre-trained CNN 2015;58:635–42.
models with conventional ML classifiers and found the highest accuracy [15] Bennett JM, et al. Proposals for the classification of the acute leukaemias French-
American-British (FAB) co-operative group 1976;33(4):451–8.
of 98.43% accuracy without explicitly using the feature selection algo- [16] Das PK, et al. A systematic review on recent advancements in deep and machine
rithms and nature inspired algorithms. The research has executed the learning based detection and classification of acute lymphoblastic leukemia. 2022.
proposed model with ResNet50 architecture with feature selection al- [17] Abbas N, et al. Nuclei segmentation of leukocytes in blood smear digital images
gorithms and PSO & CSO. Then, we have tracked out the highest ac- 2015;28(5).
[18] Abbas N, et al. Machine aided malaria parasitemia detection in Giemsa-stained thin
curacy up to 99.84%. This is very remarkable improvement in multi- blood smears 2018;29:803–18.
class classification in malignant with the feature fusion and nature [19] Ramaneswaran S, et al. Hybrid inception v3 XGBoost model for acute
lymphoblastic leukemia classification, vol. 2021; 2021. p. 1–10.
inspired algorithms.
[20] Brownlee J. Deep learning with Python: develop deep learning models on Theano
The model has certain shortcomings even though the outcomes are and TensorFlow using Keras. Machine Learning Mastery; 2016.
optimum. Firstly, the proposed is not applied in real-time malignant [21] Rehman A, et al. Classification of acute lymphoblastic leukemia using deep
learning 2018;81(11):1310–7.
classification. Secondly, we have only applied the PSO and CSO on the
[22] Arbab Q, Khan MQ, Ali HJTS. Automatic Detection and Classification of Acute
extracted features. If these algorithms are applied in the deep layers of Lymphoblastic Leukemia Using Convolution Neural Network 2022;3. 4.
customized CNN, then the model can be embedded in the small IoT [23] Mughal B, et al. Removal of pectoral muscle based on topographic map and shape-
based devices like smart watch or smartphone. In future, this research shifting silhouette 2018;18(1):1–14.
[24] Norouzi A, et al. Medical image segmentation methods, algorithms, and
will overcome these issues to create the proposed system more reliable applications 2014;31(3):199–213.
in real-life applications. Firstly, this study will develop and android [25] Mughal B, et al. Extraction of breast border and removal of pectoral muscle in
application. Then, the proposed model will be applied to android wavelet domain. 2017. p. 5041–3. 28(11.
[26] Mughal B, Sharif M, N.J.T.E.P.J.P. Muhammad. Bi-model processing for early
application in order to compile with phone camera for real-time classi- detection of breast tumor in CAD system 2017;132:1–14.
fication. Secondly, this study will be implemented in small IoT device to [27] Mughal B, et al. A novel classification scheme to decline the mortality rate among
aid hematologist in the leukemia classification. However, the proposed women due to breast tumor. 2018. p. 171–80. 81(2.
[28] Mohapatra S, et al. An ensemble classifier system for early diagnosis of acute
model may also be helpful for real-world Acute Lymphoblastic Leukemia lymphoblastic leukemia in blood microscopic images 2014;24:1887–904.
(All) classification. [29] Rehman A, et al. Rouleaux red blood cells splitting in microscopic thin blood smear
images via local maxima, circles drawing, and mapping with original RBCs. 2018.
p. 737–44. 81(7).
Contribution of the authors [30] Saba TJBR. Halal food identification with neural assisted enhanced RFID antenna.
2017. p. 7760–2. 28(18.
Wahidur Rahman: Conceptualization, Methodology, Software [31] Waheed SR, et al. Multifocus watermarking approach based on discrete cosine
transform. 2016. p. 431–7. 79(5).
Formal analysis, Mohammad Gazi Golam Faruque: Data Collection, [32] Ahmed N, et al. Identification of leukemia subtypes from microscopic images using
Data Optimization, Kaniz Roksana: Visualization, Investigation, A H M convolutional neural network. 2019. p. 104. 9(3.
Saifullah Sadi: Investigation, Data Analysis and Figure Drawing, [33] Jiang Z, et al. Method for diagnosis of acute lymphoblastic leukemia based on ViT-
CNN ensemble model. 2021. p. 2021.
Mohammad Motiur Rahman: Supervision, and Writing- Reviewing
[34] Rezayi S, et al. Timely diagnosis of acute lymphoblastic leukemia using artificial
and Editing, Mir Mohammad Azad: Supervision and Reviewing intelligence-oriented deep learning methods. 2021. p. 2021.
[35] Zakir Ullah M, et al. An attention-based convolutional neural network for acute
lymphoblastic leukemia classification. 2021, 10662. 11(22.
[36] Revanda AR, et al. Classification of acute lymphoblastic leukemia on white blood
Declaration of competing interest
cell microscopy images based on instance segmentation using Mask R-CNN. 2022.
15(5).
The authors declare that they have no known competing financial [37] Abunadi I, E.M.J.S. Senan. Multi-method diagnosis of blood microscopic sample for
early detection of acute lymphoblastic leukemia based on deep learning and hybrid
interests or personal relationships that could have appeared to influence
techniques. 2022. p. 1629. 22(4).
the work reported in this paper.
22W. Rahman et al. A r r a y18(2023)100292
[38] Sampathila N, et al. Customized deep learning classifier for detection of acute [47] Aria M. Acute lymphoblastic leukemia (ALL) image dataset. 2021 [cited 2023
lymphoblastic leukemia using blood smear images. Healthcare; 2022 [MDPI]. March, 10]; Available from: https://www.kaggle.com/datasets/mehradaria/
[39] Pallegama R, et al. Acute lymphoblastic leukemia detection using convolutional leukemia.
neural network. 2020, 26529. 10(6). [48] Khan HA, et al. Brain tumor classification in MRI image using convolutional neural
[40] Safuan SNM, et al. Investigation of white blood cell biomaker model for acute network. Math Biosci Eng 2020;17(5):6203–16.
lymphoblastic leukemia detection based on convolutional neural network. 2020. [49] Dobilas SLDA. Linear discriminant analysis — how to improve your models with
p. 611–8. 9(2). supervised dimensionality reduction. 2021 [cited 2023 March, 10]; Available from:
[41] Ansari S, et al. A customized efficient deep learning model for the diagnosis of https://towardsdatascience.com/lda-linear-discriminant-analysis-how-to-improve-
acute leukemia cells based on lymphocyte and monocyte images. 2023. p. 322. 12 your-models-with-supervised-dimensionality-52464e73930f.
(2. [50] Tao Z, et al. GA-SVM based feature selection and parameter optimization in
[42] Abd El-Ghany S, Elmogy M, El-Aziz AJD. Computer-Aided Diagnosis System for hospitalization expense modeling. Appl Soft Comput 2019;75:323–32.
Blood Diseases Using EfficientNet-B3 Based on a Dynamic Learning Algorithm [51] Sharif M, et al. An integrated design of particle swarm optimization (PSO) with
2023;13(3):404. fusion of features for detection of brain tumor. Pattern Recogn Lett 2020;129:
[43] Ayache F, Alti A. Performance evaluation of machine learning for recognizing 150–7.
human facial emotions. Rev. d’Intelligence Artif. 2020;34(3):267–75. [52] Sikkandar H, Thiyagarajan R. Deep learning based facial expression recognition
[44] Gupta S, et al. Prediction performance of deep learning for colon cancer survival using improved Cat Swarm Optimization. J Ambient Intell Hum Comput 2021;12:
prediction on SEER data. BioMed Res Int 2022:2022. 3037–53.
[45] Ahmad S, et al. A novel hybrid deep learning model for metastatic cancer [53] Kurban H, Kurban M. Building Machine Learning systems for multi-atoms
detection. Comput Intell Neurosci 2022:2022. structures: CH3NH3PbI3 perovskite nanoparticles. Comput Mater Sci 2021;195:
[46] Ayeche F, Alti A. HDG and HDGG: an extensible feature extraction descriptor for 110490.
effective face and facial expressions recognition. Pattern Anal Appl 2021;24:
1095–110.
23