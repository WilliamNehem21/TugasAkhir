Array14(2022)100152
Contents lists available at ScienceDirect
Array
journal homepage: www.sciencedirect.com/journal/array
Joint Multiple Fine-grained feature for Vehicle Re-Identification
Yan Xu*, Leilei Rong**, Xiaolei Zhou, Xuguang Pan, Xianglan Liu
College of Electronic and Information Engineering, Shandong University of Science & Technology, Qingdao 266590, China
A R T I C L E I N F O A B S T R A C T
Keywords: The process of recognizing the same vehicle in different scenes is called vehicle re-identification. However, due to
Image retrieval the different locations of the surveillance cameras, there may be obstacles in the captured vehicle pictures and
Deep learning multiple viewpoints may make the same vehicle look different. In order to effectively reduce the interference of
Vehicle re-identification
obstacle occlusion, multiple viewpoints, and other factors on vehicle re-identification, in this paper, we propose a
Fine-grained feature
multi-fine-grained feature extraction network. While retaining the global information of vehicles, we extract the
Feature map segmentation
finegrained features of vehicles precisely by segmenting the vehicle feature map. In addition, we introduce a new
mINP
evaluation metric mean Inverse Negative Penalty (mINP) to evaluate the vehicle re-identification model more
comprehensively. Our method achieves superior accuracy over the state-of-the-art methods on the challenging
vehicle datasets: VeRi-776, VehicleID, and VRIC.
1. Introduction differences, strengthens the network’s learning of local features, and
further expands the differences between similar instances. Khorram-
Vehicle re-identification (re-id) aims to identify the same vehicle shahi et al. [14] adopted the idea of feature extraction from rough to
from different scenes, and belongs to the subtask of image retrieval. fine. In the first stage, the proposed adaptive key point selection module
With the promotion of deep learning technology, vehicle re- was used to select the key points with the largest amount of information
identification has become a hot topic in the field of computer vision. from the initial layer of the global feature extraction network to roughly
However, due to the production of vehicle database from different sur- extract local features around the selected key points. In the second stage,
veillance cameras in the real world, the captured vehicle images have the vehicle features of the first stage are refined through the two-layer
interference factors such as motion blur, dark background, low resolu- hourglass network with jump connections, and the relatively fine
tion, obstacle occlusion, multi-viewpoint, and so on. Hence, how to vehicle features obtained are used for vehicle feature matching. There-
weaken the adverse factors and improve the accuracy of vehicle re- fore, using local feature information for vehicle re-identification can
identification has become a key research direction in this field. make the network model extract more refined vehicle features, and
In recent years, researchers have mainly designed new network combine with the global features of vehicles, can significantly improve
structures [1–7] based on convolutional neural networks (CNN) to learn the model’s ability to distinguish vehicles with similar appearance but
more discriminative vehicle features or introduced additional informa- different identities, and improve the accuracy of vehicle
tion [8–10] to improve the performance of vehicle re-identification re-identification. Later, scholars [15] combined global and local infor-
models. Early research [11] focused on using the global information of mation together. Although it effectively improved the accuracy of
the vehicle to complete the re-identification task, while ignoring the vehicle re-identification, they failed to make full use of the vehicle’s
local information of the vehicle. The local information contains key local information.
features that distinguish different vehicles. In order to obtain local To solve this problem, we divide the vehicle feature map horizontally
feature information of vehicles, Liu et al. [12] proposed a region-aware and keep the global information of the vehicle at the same time. The
deep Model (RAM), which can not only extract global features of vehi- advantage of convolutional neural networks lies in the perception of
cles but also learn discriminant features of different local regions. He local information in the image. By feature map segmentation, on the one
et al. [13] proposed a simple and effective local regularization method, hand, it can make the feature extraction network pay more attention to
which improves the network’s ability to perceive subtle feature the fine-grained features of the vehicle (such as vehicle logo, lights,
* Corresponding author.
** Corresponding author.
E-mail addresses: x1y5@163.com (Y. Xu), rong15305385721@163.com (L. Rong).
https://doi.org/10.1016/j.array.2022.100152
Received 14 October 2021; Received in revised form 3 February 2022; Accepted 1 April 2022
Availableonline15April2022
2590-0056/©2022TheAuthor(s).PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).Y. Xu et al. A r r a y14(2022)100152
Fig. 1. The overall framework of the algorithm model for vehicle re-identification. (a) CAM represents a channel attention mechanism; (b) Illustration of multi-fine-
grained feature extraction network. (Best view in color).
Fig. 2. Visualization of the feature maps of deep residual networks.
annual inspection signs, body decoration, etc.). On the other hand, it can obtained. Then the model is put into the testing set (Query set and
effectively reduce the influence of obstacles and other adverse factors in Gallery set) to calculate the Euclidean distance between the target
the original image on vehicle feature learning. In addition, we also vehicle in Query set and the vehicle to be retrieved in Gallery set, and
introduce channel attention mechanism to further strengthen the net- the similarity between the vehicles is calculated and ranked by the
work’s recognition of vehicle fine-grained features after feature map distance. The higher the ranking, the higher the similarity. Finally, the
segmentation. Experiments on the mainstream open datasets (VeRi-776 retrieval results are printed by line. The green border shows a correct
[16], VehicleID [17], and VRIC [18]) show that the best result is ach- retrieval and the red border shows an incorrect retrieval.
ieved in the feature map quartering, as shown in Fig. 1(b).
The rest of this paper is organized as follows: Section 2 introduces the
2.2. Multi-fine-grained feature extraction network
specific process of vehicle re-identification and provides an overview of
multi-fine-grained feature extraction network and loss function. Section
In order to fully extract the fine-grained features of the vehicle and
3 introduces the new evaluation metric: mINP. Section 4 presents the
improve the accuracy of vehicle re-identification, we propose a multi-
experimental results and analysis, and Section 5 draws concluding
fine-grained feature extraction network (MFG-Net), as shown in Fig. 1
remarks.
(b). It can be seen from Fig. 2(b) that compared with other deep residual
networks [19], ResNet50 network is more targeted in extracting vehicle
2. Proposed method
features. At the same time, it can be seen from Fig. 2(a) that Conv_5 layer
of the network focuses on the features of the vehicle area, so we take
2.1. The specific process of vehicle re-id
ResNet50 as our backbone network, and two feature extraction branches
are built after the Conv_5 layer: global branch and local branch.
Firstly, the image of the training set is imported into the multi-fine-
The 1*1 convolution module is added before and after the global
grained feature network. After several iterations, the optimal model is
average pooling (GAP) in the global branch. The purpose of adding 1*1
2Y. Xu et al. A r r a y14(2022)100152
Table 1 3. mINP: a new evaluation metric for vehicle Re-ID
The parameter and meaning of loss function.
Parameter Meaning Since the previous vehicle re-id model evaluation metrics mAP and
Rank-n cannot objectively evaluate the retrieval ability of the model, we
Ni the number of vehicle images per batch
introduce a new model evaluation metric: mINP (mean Inverse Negative
Nid the number of vehicle identities
xj the output of fully connected layer for j th identity Penalty) [21], which is used to characterize the most difficult and cor-
y the ground truth identity of input vehicle image rect retrieval ability of the network model. So far, we are the only team
Ai Anchor that uses the ability of the sample retrieval as an indicator in the field of
Pi Positive
vehicle re-identification.
Nj Negative
αδ m Wi en ii gm hta l margin NP i=X i(cid:0) XG i (i=1,2,…,Q) (4)
β Weight i
G
INP i=1(cid:0) NP i= Xi (5)
convolution module before GAP is to greatly increase the non-linear i
s 1c oh
*
a 1tr h ca a oc tt
n
te vhr oi es
l
t uni tc e is t ow nw o mh rki ol e dc a uk n le
e
e g p ii svi n e ag df dut elh l
d
e p als a fc tya
e
l rte o
G
o t Ahf Peth ,a e td
h
vf ee a a fn et t au a tr g ue e
r
s em soa f ap fd tu e ep rn tc Ghh A.a W Pn g whe e id ln l, mINP= Q1∑ Q 11(cid:0) NP i= Q1∑ Q 11(cid:0) X i(cid:0)
X
iG i (6)
not go through the classification layer directly, but are fused first, and In vehicle re-identification, G and X are the number of target vehicles
the fused features are classified. In this way, the accuracy can be greatly in vehicle retrieval results and the number of times to retrieve the last
improved without affecting the model inference speed. target vehicle, respectively; X-G is the number of interfering vehicles; NP
In the local branch, the vehicle feature map (h*w*c) is divided into and INP are the occupations of interfering vehicles and target vehicles in
four blocks according to its height, and the size of each block is (h/ retrieval results in turn. mINP represents the mean occupancy rate of Q
4*w*c), which can effectively reduce the influence of such adverse target vehicles in the search results.
factors as occlusion on feature extraction. At the same time, we embed Therefore, mINP can evaluate the performance of the model more
the channel attention module in the global branch and the local branch objectively and effectively avoid the dominance of simple matching in
respectively to make the network pay more attention to the personalized mAP/CMC evaluation. It can not only reflect the relative performance of
characteristics of the vehicle. The working principle of this module is the vehicle re-id model but also provide a supplement to the widely used
shown in Fig. 1(a). mAP and CMC metrics.
During the training stage, global branch and local branch do not
share the weight and train separately. But when testing, all branch in- 4. Experiment
formation will be assembled into a comprehensive feature to increase
network performance. 4.1. Datasets and settings
We conduct extensive experiments on three public benchmarks for
2.3. Loss function vehicle re-id, namely, VeRi-776, VehicleID, and VRIC. The details of
the datasets are shown in Table 2.
In the global branch of the MFG-Net network, we introduce two loss In addition, the software tools are PyTorch, CUDA11.1, and CUDNN
functions: hard mining triplet loss [20] and softmax cross-entropy loss; V8.0.4.30. The hardware device is a workstation equipped with AMD
meanwhile, in the local branch, we only use softmax cross-entropy loss, Ryzen 5 3600X CPU 32G, NVIDIA GeForce RTX 3080 and 256 GB+2 TB
and finally get a total loss by weighting. memory.
( (cid:0) ) )
L Softmax= (cid:0)
∑Ni
log
∑exp x
y( ) (1) 4.2. Evaluation protocols and implementation details
i=1 N j=id 1exp x j
During the training stage, the vehicle image is resized to 384 ×128,
and then enhanced by random erasure and horizontal flipping. At the
same time, the Amsgrad optimizer is used to optimize our model. The
⎡ ⎤
⎢ ⎥
⎢ ⎥
L hardminingtriplet=∑ Q i=1∑ K A=1⎢ ⎢ ⎢
⎢
⎢
⎣P⏞ =̅ m̅̅ 1̅ ,a̅ …̅̅ x̅ ,̅ K̅̅ ‖̅̅⏟ A⏟ i̅̅ (cid:0)̅̅̅̅̅ P̅̅̅ i̅ ‖̅̅⏞ 2hardestpositive (cid:0) ⏞
N j i=
∕=̅ =̅ m
1
j̅̅
1
,̅ …,̅ i …̅ n̅ ,̅ Q,̅ K⃦̅̅̅ ⃦̅̅̅ A̅̅⏟ i⏟ (cid:0)̅̅̅̅ N̅̅̅ j⃦̅ ⃦̅̅̅ 2̅̅̅ +̅̅̅̅ δ⏞hardestnegative⎥ ⎥ ⎥
⎥
⎥ ⎦
(2)
+
initial learning rate is 0.0003. α, β, and δ are set to 1, 0.1, and 0.3.
During the testing stage, the protocol proposed in Refs. [8,16] is
L total=α*L Softmax+β*L hardminingtriplet (3)
followed. We compute the Cumulative Matching Characteristic (CMC)
curves for three datasets, and further compute the mean Average Pre-
where the meanings of the parameters of (1), (2) and (3) are listed in
cision (mAP) and Rank1. Moreover, we also calculate a new evaluation
Table 1.
metric mINP on the three datasets. In this way, we can evaluate our
model more objectively.
3Y. Xu et al. A r r a y14(2022)100152
Table 2 of the vehicle in MFG-Net and can effectively improve the performance
The details of the datasets. of vehicle re-id.
Dataset VeRi-776 VRIC VehicleID In addition, we choose ResNet50 with softmax cross-entropy loss as
the baseline, and four vehicle re-id network frameworks are then
Images 51,035 60,430 221,763
designed based on the baseline:
IDs 776 5,622 26,267
Training 37,778/ 54,808/ 110,178/13,134
Set/IDs 576 2,811 1) +global branch (w/o Lhardminingtriplet);
Query/IDs 1,678/ 2,811/ 6,532/ 11,395/ 17,638/ 2) +global branch (w/ Lhardminingtriplet);
200 2,811 800 1,600 2,400 3) +local branch;
Gallery/IDs 11,579/ 2,811/ 800/800 1,600/ 2,400/
200 2,811 1,600 2,400 4) +CAM.
The ablation studies result of the MFG-Net on VeRi-776 dataset are
Table 3 shown in Table 4.
The result of the horizontal division times of feature map on VeRi-776 dataset. From Table 4, we can see that compared with “baseline”, “baseline +
Horizontal division times mAP mINP R1 global branch (w/o Lhardminingtriplet)” improves the 6.12% mAP, 4.91%
mINP, and 1.47% Rank1 on VeRi-776 dataset. The results show that
1 71.11 25.58 93.68
2 72.05 28.45 92.60 global branch effectively improve the accuracy of vehicle re-id. After
3 (Ours) 77.15 36.82 96.72 applying hard mining triplet loss, “baseline + global branch (w/
4 73.10 30.01 94.82 Lhardminingtriplet)” outperforms “baseline + global branch (w/o
5 70.88 25.32 90.98
Lhardminingtriplet)” by a large margin (2.13% mAP, 5.38% mINP, and
2.61% Rank1). This result validates the effectiveness of the hard mining
triplet loss to optimize the distance between positive and negative
sample pairs. After adding local branch, the network improves the
4.42% mAP, 2.54% mINP, and 1.10% Rank1 on VeRi-776 dataset. We
can clearly see that the feature map segmentation can reduce the
interference of background information and improve the accuracy of
vehicle re-id. By adding CAM, the network achieves 2.05%, 1.28% and
1.84% improvement in mAP, mINP and Rank1 on VeRi-776. This result
show that CAM can enhance the feature extraction capability of
network.
4.4. VeRi-776
During the train stage on VeRi-776, the learning rate is decreased by
a factor of 0.1 after the 10th and 20th epoch, till the end 30th epoch. And
the batch-size of training and testing are both 48. The training loss of
MFG-Net on VeRi-776 is shown in Fig. 4.
Our study presents performance comparisons between the proposed
MFG-Net and representative state-of-the-art methods (i.e., DAVR [1],
VRSDNet [22], VAMI +ST [2], RAM [12], GRF +GGL [15], BS [23],
CCA [3], MRM [4], SPAN w/CPDM [24], TCL +SL [25], UMTS [26],
Fig. 3. The CMC curves of ablation study on VeRi-776 dataset. and MsDeep [5]) on VeRi-776. According to the experimental results in
Table 5, our method improves by 1.25% and 1.11% on mAP and Rank1,
respectively, compared with the second-best method UMTS without
Table 4 using any auxiliary information (such as VAMI +ST using the vehicle’s
The ablation studies result of the MFG-Net on VeRi-776 dataset.
spatio-temporal information). The new metric mINP is applied to the
Method mAP mINP R1 field of vehicle re-identification for the first time and reaches 36.82% on
Baseline 62.43 22.71 89.70 VeRi-776.
+global branch (w/o Lhardminingtriplet) 68.55 27.62 91.17
+global branch (w/ Lhardminingtriplet) 70.68 33.00 93.78
+local branch 75.10 35.54 94.88 4.5. VehicleID
+CAM 77.15 36.82 96.72
During the training stage on VehicleID, the learning rate is decreased
by a factor of 0.1 after the 15th and 30th epoch, till the end 40th epoch.
4.3. Ablation study
And the batch-size of training and testing are 24 and 32, respectively.
The training loss of MFG-Net on VehicleID is shown in Fig. 5.
To investigate the effectiveness and contribution of the feature map
VehicleID has three test sets, namely Test800, Test1600 and
horizontally quartered in our MFG-Net, we conducted ablation study on Test2400. VAMI [2], DAVR [1], RAM [12], CCA [3], MRM [4], TCL +SL
VeRi-776 dataset. The evaluation results are shown in Table 3. The CMC [25], GRF + GGL [15], MSA [27], BS [23] and MsDeep [5] are also
curve of the ablation study on this dataset is shown in Fig. 3, in which
included in our comparison list. Table 6 shows the comparison results on
the effectiveness of the horizontal division times of the feature map is
VehicleID three test sets. Compared with the current best results, our
visually compared from Rank 1 to 20. model MFG-Net has improved by approximately 2.18%–3.01% on mAP.
From Table 3 and Fig. 3 we can see, the feature map horizontally
At the same time, the mINP metric reaches 68.32%, 64.38%, and
quartered (the number of horizontal divisions is equal to 3) surpasses the
60.03% respectively. This demonstrates that MFG-Net requires less
others. The comparison results show that the feature map horizontally
effort to find all the correct vehicle matches, verifying the ability of
quartered is more suitable for fully extracting the fine-grained features
mINP.
4Y. Xu et al. A r r a y14(2022)100152
Fig. 4. The training loss of MFG-Net on VeRi-776.(a) Epoch loss. (b) Iteration loss.
MFG-Net on VRIC is shown in Fig. 6.
Table 5 To further validate our proposed method, we carry out experiments
Comparison with state-of-the-art methods on VeRi-776. on the VRIC dataset, which contains more challenging training
examples.
Methods mAP mINP Rank1
It can be seen from Table 7 that the performance of our method MFG-
DAVR [1] 52.36 – 83.25
Net is better than all other methods listed in Table 7 including MSVF
VRSDNet [22] 53.45 – 83.49
VAMI +ST [2] 61.32 – 85.92 [18], GLAMOR [6], BS [23] and PGAN [7] on mAP and Rank1 metric. In
RAM [12] 61.50 – 88.60 addition, mINP reaches 55.81% and shows that our model has better
GRF +GGL [15] 61.7 – 89.4 retrieval capabilities.
BS [23] 67.55 – 90.23
CCA [3] 68.05 – 91.71
MRM [4] 68.55 – 91.77 5. Discussion
SPAN w/CPDM [24] 68.9 – 94.0
TCL +SL [25] 68.97 – 93.92 In our work, we mainly use feature map segmentation combined
MsDeep [5] 74.50 – 95.10 with channel attention mechanism to extract multiple fine-grained
UMTS [26] 75.9 – 95.61
features of vehicles to improve the accuracy of vehicle re-id. Differ
MFG-Net 77.15 36.82 96.72
from what we think [24,28,29], adopt feature alignment to adjust the
image to the same scale, which is conducive to similar feature matching.
4.6. VRIC Finally, these methods enhance the performance of re-id model.
In order to prove the robustness and generalization of the MFG-Net,
During the training stage on VRIC, the learning rate is decreased by a our retrieval results are visualized on VeRi-776, VRIC and VehicleID, as
factor of 0.1 after the 15th and 30th epoch, till the end 40th epoch. And shown in Fig. 7. We can see that the MFG-Net is more robust and
the batch-size of training and testing are both 32. The training loss of generalized to vehicles in different poses.
Fig. 5. The training loss of MFG-Net on VehicleID. (a) Epoch loss, (b) Iteration loss.
5Y. Xu et al. A r r a y14(2022)100152
Table 6
Comparison with state-of-the-art methods on VehicleID.
Methods Test800 Test1600 Test2400
mAP mINP Rank1 mAP mINP Rank1 mAP mINP Rank1
VAMI [2] – – 63.12 – – 52.87 – – 47.34
DAVR [1] 72.40 – 68.04 70.11 – 66.48 67.97 – 64.07
RAM [12] – – 75.20 – – 72.30 – – 67.70
CCA [3] 78.89 – 75.51 76.53 – 73.60 73.11 – 70.08
MRM [4] 80.02 – 76.64 77.32 – 74.20 74.02 – 70.86
TCL +SL [25] 80.13 – 74.97 77.26 – 72.84 75.25 – 71.20
GRF +GGL [15] – – 77.1 – – 72.7 – – 70.0
MSA [27] 80.31 – 77.55 77.11 – 74.41 75.55 – 72.91
BS [23] 86.19 – 78.80 81.69 – 73.41 78.16 – 69.33
MsDeep [5] 84.30 – 81.20 81.00 – 78.00 78.60 – 75.60
MFG-Net 88.37 68.32 82.02 84.70 64.38 78.69 81.01 60.03 75.16
Fig. 6. The training loss of MFG-Net on VRIC. (a) Epoch loss. (b) Iteration loss.
try to combine pedestrians and vehicles for re-identification. This idea
Table 7 will provide solid technical support for the construction of smart cities.
Comparison with state-of-the-art methods on VRIC.
Methods mAP mINP Rank1 Author contribution statement
MSVF [18] 47.50 – 46.61
GLAMOR [6] 76.48 – 75.58 Yan Xu: Writing – review & editing.; Leilei Rong: Methodology,
BS [23] 78.55 – 69.09 Software, Formal analysis, Writing – original draft, Writing – review &
PGAN [7] 84.80 – 78.00 editing.; Xiaolei Zhou: Investigation, Writing – review & editing.;
MFG-Net 84.86 55.81 79.56
Xuguang Pan: Validation, Formal analysis, Visualization, Supervision.;
Xianglan Liu: Validation, Resources, Visualization, Supervision.
6. Conclusion
Declaration of competing interest
This paper presents an effective multi-fine-grained feature extraction
network for vehicle re-identification. Using the proposed MFG-Net, the
The authors declare that they have no known competing financial
fine-grained features of the vehicle can be fully utilized, which provides
interests or personal relationships that could have appeared to influence
robustness against vehicle obstructions. Our experiments show that the
the work reported in this paper.
proposed MFG-Net is superior to multiple state-of-the-art vehicle re-
This work was supported by the Natural Science Foundation of China
identification methods on VeRi-776, VehicleID and VRIC datasets. In
(11547037, 11604181), Shandong Province Postgraduate Education
addition, we introduce a new evaluation metric: mINP. The experi-
Quality Curriculum Project (SDYKC19083), Shandong Province Post-
mental verification not only confirms the retrieval ability of our model,
graduate Education Joint Training Base Project (SDYJD18027), Hisense
but also verifies the effectiveness of the new metric.
Group research and development Center Project, and the Scholarship
In the future, we will not only enrich the types of vehicles, but also
Fund of SDUST.
6Y. Xu et al. A r r a y14(2022)100152
Fig. 7. Visualization of MFG-Net retrieval results on VeRi-776, VRIC and VehicleID. The green and red boxes represent correct matching vehicles and wrong
matching vehicles, respectively.
References [10] Jin W. Multi-camera vehicle tracking from end-to-end based on spatial-temporal
information and visual features. ACM Int Conf Proc Ser 2019:227–32. https://doi.
org/10.1145/3374587.3374629.
[1] Peng J, Wang H, Xu F, Fu X. Cross domain knowledge learning with dual-branch
[11] Gu J, Jiang W, Luo H, Yu H. An efficient global representation constrained by
adversarial network for vehicle re-identification. Neurocomputing 2020;401:
133–44. https://doi.org/10.1016/j.neucom.2020.02.112. Angular Triplet loss for vehicle re-identification. Pattern Anal Appl 2021;24(1):
367–79. https://doi.org/10.1007/s10044-020-00900-w.
[2] Zhou Y, Shao L. Viewpoint-aware attentive multi-view inference for vehicle Re-
[12] Liu X, Zhang S, Huang Q, Gao W. Ram: a region-aware deep model for vehicle re-
identification. In: IEEE comput soc conf comput vision pattern recognit; 2018.
p. 6489–98. https://doi.org/10.1109/CVPR.2018.00679. identification. In: IEEE int conf on multimedia and expo (ICME); 2018. p. 1–6.
https://doi.org/10.1109/ICME.2018.8486589.
[3] Peng J, Jiang G, Chen D, Zhao T, Wang H. Eliminating cross-camera bias for vehicle
re-identification. Multimed Tool Appl 2020:1–17. https://doi.org/10.1007/ [13] He B, Li J, Zhao Y, Tian Y. Part-regularized near-duplicate vehicle re-identification.
In: Proceedings of the IEEE/CVF conference on computer vision and pattern
s11042-020-09987-z.
recognition; 2019. p. 3997–4005. https://doi.org/10.1109/CVPR.2019.00412.
[4] Peng J, Wang H, Zhao T, Fu X. Learning multi-region features for vehicle re-
[14] Khorramshahi P, Kumar A, Peri N, Rambhatla SS, Chen JC, Chellappa R. A dual-
identification with context-based ranking method. Neurocomputing 2019;359:
427–37. https://doi.org/10.1016/j.neucom.2019.06.013. path model with adaptive attention for vehicle re-identification. In: Proceedings of
the IEEE/CVF international conference on computer vision; 2019. p. 6132–41.
[5] Cheng Y, Zhang C, Gu K, Qi L, Gan Z. Multi-scale deep feature fusion for vehicle Re-
https://doi.org/10.1109/ICCV.2019.00623.
identification. In: IEEE int conf acoust speech signal process proc; 2020.
p. 1928–32. https://doi.org/10.1109/ICASSP40776.2020.9053328. [15] Liu X, Zhang S, Wang X, Hong R. Group-group loss-based global-regional feature
learning for vehicle re-identification. IEEE Trans Image Process 2019;29:2638–52.
[6] Suprem A, Pu C. Looking GLAMORous: vehicle Re-id in heterogeneous cameras
https://doi.org/10.1109/TIP.2019.2950796.
networks with global and local attention. arXiv preprint arXiv:2002.02256, htt
[16] Liu X, Liu W, Mei T, Ma H. A deep learning-based approach to progressive vehicle
ps://arxiv.org/abs/2002.02256; 2020.
re-identification for urban surveillance. In: European conference on computer
[7] Zhang X, Zhang R, Cao J, Gong D, You M. Part-guided attention learning for vehicle
vision; 2016. p. 869–84. https://doi.org/10.1007/978-3-319-46475-6_53.
re-identification. 2019. arXiv preprint arXiv:1909.06023.
[17] Liu H, Tian Y, Yang Y, Pang L, Huang T. Deep relative distance learning: tell the
[8] Shen Y, Xiao T, Li H, Yi S. Learning deep neural networks for vehicle re-id with
visual-spatio-temporal path proposals. IEEE Int Conf Comput Vision 2017:1900–9. difference between similar vehicles. In: Proceedings of the IEEE conference on
computer vision and pattern recognition; 2016. p. 2167–75. https://doi.org/
https://doi.org/10.1109/ICCV.2017.210.
10.1109/CVPR.2016.238.
[9] Jiang N, Xu Y, Zhou Z, Wu W. Multi-attribute driven vehicle re-identification with
spatial-temporal re-ranking. In: IEEE int conf image process; 2018. p. 858–62. [18] Kanacı A, Zhu X, Gong S. Vehicle re-identification in context. In: German
conference on pattern recognition; 2018. p. 377–90. https://doi.org/10.1007/978-
https://doi.org/10.1109/ICIP.2018.8451776.
3-030-12939-2_26.
7Y. Xu et al. A r r a y14(2022)100152
[19] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: conference on computer vision and pattern recognition; 2020. p. 7103–12. https://
Proceedings of the IEEE conference on computer vision and pattern recognition; doi.org/10.1109/CVPR42600.2020.00713.
2016. p. 770–8. https://doi.org/10.1109/CVPR.2016.90. [25] He X, Zhou Y, Zhou Z, Bai S, Bai X. Triplet-center loss for multi-view 3d object
[20] Hermans A, Beyer L, Leibe B. In defense of the triplet loss for person re- retrieval. In: Proceedings of the IEEE conference on computer vision and pattern
identification. arXiv preprint arXiv:1703.07737. 2017, https://arxiv.org/abs/1 recognition; 2018. p. 1945–54. https://doi.org/10.1109/CVPR.2018.00208.
703.07737. [26] Jin X, Lan C, Zeng W, Chen Z. Uncertainty-aware multi-shot knowledge distillation
[21] Ye M, Shen J, Lin G, Xiang T, Shao L, Hoi SC. Deep learning for person re- for image-based object re-identification. Proc AAAI Conf Artif Intell 2020;34(7):
identification: a survey and outlook. IEEE Transactions on Pattern Analysis and 11165–72. https://doi.org/10.1609/aaai.v34i07.6774.
Machine Intelligence; 2021. https://doi.org/10.1109/TPAMI.2021.3054775. [27] Zheng A, Lin X, Dong J, Wang W, Tang J, Luo B. Multi-scale attention vehicle re-
[22] Zhu J, Du Y, Hu Y, Zheng L, Cai C. VRSDNet: vehicle re-identification with a identification. Neural Comput Appl 2020;32(23):17489–503. https://doi.org/
shortly and densely connected convolutional neural network. Multimed Tool Appl 10.1007/s00521-020-05108-x.
2019;78(20):29043–57. https://doi.org/10.1007/s11042-018-6270-4. [28] Zheng Z, Zheng L, Yang Y. Pedestrian alignment network for large-scale person re-
[23] Kumar R, Weill E, Aghdasi F, Sriram P. A strong and efficient baseline for vehicle identification. IEEE Trans Circ Syst Video Technol 2018;29(10):3037–45. https://
re-identification using deep triplet embedding. J Artif Intell Soft Comput Res 2020; doi.org/10.1109/TCSVT.2018.2873599.
10(1):27–45. https://doi.org/10.2478/jaiscr-2020-0003. [29] Liu X, Liu W, Zheng J, Yan C, Mei T. Beyond the parts: learning multi-view cross-
[24] Meng D, Li L, Liu X, Li Y, Yang S, Zha ZJ, Huang Q. Parsing-based view-aware part correlation for vehicle re-identification. In: Proceedings of the 28th ACM
embedding network for vehicle re-identification. In: Proceedings of the IEEE/CVF international conference on multimedia; 2020. p. 907–15. https://doi.org/
10.1145/3394171.3413578.
8