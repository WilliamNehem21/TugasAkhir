Available online at www.sciencedirect.com
AASRI
Procedia
AASRI Procedia 1 ( 2012 ) 468 – 473
www.elsevier.com/locate/procedia
2012AASRI Conference on Computational Intelligence and Bioinformatics
A Sentence Alignment Model Based on Combined Clues and
Kernel Extensional Matrix Matching Method
Wu Honglina, Liu Yiyanga,*, Liu Shaominga,b
aNortheastern University, Shenyang, 110004, China;
bFuji Xerox Co., Ltd., Kanagawa, 2208668, Japan
Abstract
A sentence alignment model based on combined clues and Kernel Extensional Matrix Matching (KEMM) method is
proposed. In this model, a similarity matrix for sentence aligning is formed by the similarities of bilingual sentences
calculated by the combined clues, such as lexicon, morphology, length and special symbols, etc.; then this similarity
matrix is used to construct a select matrix for sentence aligning; finally, obtains the sentence alignments by KEMM.
Experimental results illustrated that our model outperforms over the Gale’s system on handling any types of sentence
alignments, with 30% total sentence alignment error rate decreasing.
2012 Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied
Science Research Institute Open access under CC BY-NC-ND license.
Key Words(cid:726)Sentence Alignment(cid:712) Combined Clues(cid:712)Kernel Extensional Matrix Matching
1.Introduction
In various fields of research on natural language processing, the importance of bilingual corpus is more and
more obvious. Different applications call for aligned bilingual corpus of different granularities and
corresponding technology, which includes article, paragraph, sentence, phrase and word level. Sentence level
aligned bilingual corpus is indispensable to example based machine translation.
*Liu Yiyang. Tel.: +86-24-83672480.
E-mail address: lyy880315@yahoo.cn.
2212-6716 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi: 10.1016/j.aasri.2012.06.073Wu Honglin et al. / AASRI Procedia 1 ( 2012 ) 468 – 473 469
A sentence alignment model based on combined clues and KEMM method is proposed. In this model, a
similarity matrix for sentence aligning is formed by the similarities of bilingual sentences calculated by the
combined clues; then this similarity matrix is used to construct a select matrix for sentence aligning; finally,
obtains the sentence alignments by KEMM.
2. Similarities of Bilingual Sentences Calculated By the Combined Clues
To the translations needed to have sentence alignment for Chinese text C and Japanese text J, we assume
that the number of C is m and to be expressed as C=CS CS …CS , the number of J is n and to be expressed
1 2 m
as J=JS JS …JS . The similarities of all bilingual sentences calculated by the combined clues included lexicon,
1 2 n
morphology and length. Results by each approach are summed up after multiplied by different weights and
then plus special symbols similarity to be the final similarity between Chinese and Japanese sentences. The
formula to calculate similarity of Chinese sentence CS and Japanese sentence JS :
h k
Sim(CS ,JS )(cid:32) (cid:68)(cid:117)SimDict(CS ,JS )(cid:14)(cid:69)(cid:117)SimMorph(CS ,JS )(cid:14)
h k h k h k
(1)
(cid:74)(cid:117)SimLength(CS ,JS )(cid:14)SValue(CS ,JS )
h k h k
(cid:162),(cid:163)and(cid:164) are the weights, and(cid:162)+(cid:163)+(cid:164)=0.8; 0(cid:304)SValue(CS ,JS )(cid:304)0.2.
h k
2.1.Based on Bilingual Dictionary
In this calculating method, sentences are expressed as a set of words. Chinese and Japanese sentences are
expressed as CS (cid:32){c,c ,(cid:34),c }and JS (cid:32){j,j ,(cid:34),j }.Similarity calculatingbased on bilingual dictionary
h 1 2 m k 1 2 n
(SimDict) is:
TransSetC (cid:14)TransSetJ
SimDict(CS ,JS )(cid:32) (2)
h k
CS (cid:14) JS
h k
|CS | and |JS | are the number of content words in Chinese and Japanese, |TransSetC|and |TransSetC| are
h k
the number of content words that have translations.
2.2. Based on Font
Sentence is expressed as a set of characters in calculating. Chinese and Japanese sentences are expressed as
CS (cid:32){cc,cc ,(cid:34),cc }andJS (cid:32){jc,jc ,(cid:34),jc }. The calculating method based on font:
h 1 2 r k 1 2 s
MorphSetC (cid:14) MorphSetJ
SimMorph(CS ,JS )(cid:32) (3)
h k
CS (cid:14) JS
h k
|CS | and |JS | are the number of characters in sentences, |MorphSetC| is the number of the same Chinese
h k
characters CS have inJS . |MorphSetJ| is the number of the same Chinese characters JS have in CS .
h k k h
2.3.Based on Sentence Length
Block=<C,J> is proposed as a translation block that have been aligned. Then alignment A=PP P …P . P(1
1 2 3 k i
(cid:304)i(cid:304)k) is a couple of translation and P=<CS,JS>; CS is a set of Chinese sentences, it expresses from zero to
i i i i
more sentences in C; JS is a set of Japanese sentences, it expresses from zero to more sentences in J.
i
In a couple of translation P=(cid:728)CS,JS>, to propose that l =length(CS) is the length of CS and l =length(JS)
1 2
is the length of JS.In a word, l (cid:451)l are the number of characters in CS and JS. To the translation block
1 2470 Wu Honglin et al. / AASRI Procedia 1 ( 2012 ) 468 – 473
Block=<C,J>, sentence alignment based on length statistic is to look for the one has the largest probability in
all probable alignment A. Condition below must be met:
A(cid:32)argmaxPr(A|Block)(cid:32)argmaxPr(A|(cid:31)C,J (cid:33)) (4)
To propose that each couple of translation is absolutely, and Pr(P|<C,J>) is not depend on translation block,
it’s only decided by P=<CS,JS>. It can be thought based on statistic that Pr(P|<CS,JS>) is only decided by
matching pattern match(P) of P and the length of CS and JS.
Since the Bayesian theory:
A(cid:32)argmax (cid:150)Pr((cid:31)l,l (cid:33)|match(P))(cid:152)Pr(match(P)) (5)
1 2
A P(cid:143)A
Pr(match(P)) is the matching probability. To propose that (cid:71)(l,l ) is a function satisfied Standard normal
1 2
distribution, and the key of model above is the design of the evaluate function (cid:71)(l,l ). It’s given in Gale:
1 2
l (cid:16)c(cid:152)l
(cid:71)(l ,l )(cid:32) 2 1 (6)
1 2
l (cid:152)s2
1
c(cid:32) (cid:166)l 2/ (cid:166)l 1, s2is the slope of the line that the result of all points (l 1,(l 2(cid:16)l 1)2) in training corpus being
P(cid:143)TC P(cid:143)TC
linear regression analyzed. s2 is a normalization factor to make sure (cid:71)(l,l ) is a normal distribution. To the
1 2
Chinese and Japanese sentence alignment, c=1.06 and S2=6.8 are from the statistic of the corpus.
In Gale test, Pr(match(P)) in formula(6) is a penalty factor to multi-alignment. Since the sentence
alignment model in this paper is a method of combining some different approaches, the final alignment
similarity needs a penalty calculating. Then formula(6) becomes:
SimLen(CS ,JS )(cid:32)argmax (cid:150)Pr((cid:71)(l,l )|match(P)) (7)
h k 1 2
A P(cid:143)A
2.4.Based on Special Character
Numbers (such as 1978, 03, 24, etc.), English characters (such as China, Henry, etc.), quotation marks and
brackets are used to calculate the similarity of Chinese and Japanese special characters. The maximum value
of special characters’ similarity (SValue) is set as 0.2. When CS and JS have the same special characters,
h k
SValue(CS ,JS ) is added 0.05, and it’s added to 0.2 for max. It is a supplement for the similarity calculating
h k
of SimDict,SimMorph and SimLength.
2.5.Multi-alignment Penalty Factor
Multi-alignment penalty factor is to deal with alignment conflict. For an instance, when aligned conflicts
happen and judge if the style is 1-2 or the two aligned 1-1 and 0-1, we need to calculate the similarity of these
three kinds and then multiply the corresponding penalty factor. Then we can compare their similarity modified.
We stat the proportion of different alignment in Chinese and Japanese corpus including 9679 aligned
sentences. Combining researches to set the penalty factor ((cid:543)) as the table 1 shows.
Table 1 Penaltyfactor for multi-alignment
Alignment style Penalty factor((cid:543))
1-2;2-1 0.95
2-2 0.60
1-0;0-1 0.55Wu Honglin et al. / AASRI Procedia 1 ( 2012 ) 468 – 473 471
3.SentenceMatching Based on Kernel Extensional Matrix Method
3.1.Alignment Similarity Matrix and Alignment Selecting Matrix
An alignment similarity two-dimensional matrix (SimMatrix) is constructed by Chinese and Japanese
sentences that the number of them is m and n. The element in the matrix is the similarity.
SimMatrix[h][k](cid:32)Sim(CS ,JS ) (8)
h k
Then we construct a sentence alignment selecting matrix (SelMatrix). Each element is the sorting
information of the alignment similarity.
3.2.Sentence Matching
Sentence matching is to choose the alignment results from the sentence alignment selecting matrix.
1) Select the Alignment in “1/1” Row
Six steps are included in this process.
Step1: According to the SelMatrix to select the location where "1/1" is. And then construct a 3(cid:104)3 matrix
to calculate similarity and check the probability if there is some multi-alignment.
Step2:To calculate the probability of the multi-alignment.
If SelMatrix[i-1][j-1] = “1/1” and
SelMatrix[i+1][j+1] = “1/1”;
NCA = (CSi)-(JSj);
Else If SelMatrix[i-1][j-1] or SelMatrix[i+1][j+1] is
not "1/1"
NCA(cid:32)argmaxSim(P)
P(cid:143)A
MultiSim= Cal(min(SimAdd, Selmatrix[i][j]));
Compare MutiSim with NCA;
If MutiSim is larger than NCA, we put the multi-alignment into CandSet. Otherwise turn to step5.
Step3:To judge the style of new member of CandSet. Processing is according to its style.
Step4:The new multi-alignment processing can be considered as 10 categories, and one of them can be
made formal to :
If
((cid:7)k:(1(cid:100)k(cid:100)m)(cid:154)(k(cid:122)i):(CS k)(cid:16)(JS j(cid:16)1JS j)(cid:143)CA)
If Sim(NCA)(cid:33)Sim((CS )(cid:16)(JS JS ))
k j(cid:16)1 j
CA(cid:32)CA/{(CS )(cid:16)(JS JS )}
k j(cid:16)1 j
CA(cid:32)CA(cid:137){NCA}
return true;
return false;
Step5:Add the ensured alignment in the aligned queue and record the aligning status of sentences.
Step6:To judge whether all "1/1" locations are dealt with. If not, turn to step1.
2) Select the Alignment not in “1/1” Row
This processing is to look for the locations without "1/1" and look for the minimum sum of similarity in
its row. And then to judge whether cross-aligned will be happened in this location.
And the processing in column is similar with the processing above.
3) Spatial Alignment Processing
Step1:To look for unaligned row.472 Wu Honglin et al. / AASRI Procedia 1 ( 2012 ) 468 – 473
Step2:To select the point "1/1" in the previous row and back row. And then to calculate the similarity of
the unaligned row with previous row and back row.
Step3:To compare the modified multi-alignment similarity with the point.
Step4:Accoring to the comparing results to deal with them further.
Step5:To store the results and record the alignment status. And to look for if there is any unaligned row.
4.Expriments and Results Analysis
The test set used in sentence alignment expriment is randomly selected from the Chinese and Japanese
chapter-aligned corpus and they are artifically marked standard answers of stence alignment.The test set
includes 558 alignments, the style distribution shown as table 2.
To verify the effectiveness of the model put forward in this paper, we achieve the sentence alignment
system with the method given in this paper. We test the error rate of the system we construct on the test set
and compare the results with Gale system which is famous in sentence alignment field.
Table 2 Alignment types distribution of Testing set
Style Distribution Distribution Rate(%)
1-0;0-1 6 1
1-1 505 90.5
1-2;2-1 47 8.5
Table 3 shows the distributionof all kinds of alignment style in our test set and corpus. We can see that the
distribution of our test set is approaching the distribution of our corpus. So our test set is representative, it can
represent the test result of our corpus.
Table 3 Comparison of alignment types distribution
Style Test Set(%) Bilingual Corpus(%) Gale(%)
1-0;0-1 1 0.2 1
1-1 90.5 93.8 89
1-2;2-1 8.5 5.8 9
We achieve the align method in Gale as beseline system. But the test results of the system have
differences from reference (it’s worse than the results in reference). So the data is not faith to Gale system.
We use the better experimental results from Gale shown in reference. As shown in table 3, the distribution of
our test set is approaching the test set in Gale. So this comparison is faithful.
Test set in Gale includes some rare alignment and they don’t appeared in our test set. Since the experiental
results in Gale show that the error rate of processing the styles is 100%, the difference cannot make effects.
So the comparison is sloped to Baseline.
In summary, we try our best to ensure the faith and accurate in experiment. Table 4 shows the experiental
results of sentence alignment.
From the table 4, we can know that our system has a lower error rate. We analysis the test results and find
that the reson for lower error rate is to use the KEMM to avoid error spreading phenomenon.
On all kinds of alignment styles our system always has a lower error rate than Gale system. The error rate
of our system is 2.3% and Gale is 3.5%. The exprimental results and analysis above fully prove the
effectiveness of our calculating model.Wu Honglin et al. / AASRI Procedia 1 ( 2012 ) 468 – 473 473
Table 4 Experiment result: error rate
Align Style Test Set Align Num Our Error Num Our Error rate (%) Gale Error rate (%)
1-0;0-1 6 2 33.3 100
1-1 505 7 1.4 2
1-2;2-1 47 4 8.5 9
Sum 558 13 2.3 3.5
5.Conclusion
A sentence alignment model based on combined clues and Kernel Extensional Matrix Matchingmethod is
proposed in this paper.
In this model, a similarity matrix for sentence aligning is formed by the similarities of bilingual sentences
calculated by the combined clues, such as lexicon, morphology, length and special symbols, etc.; then this
similarity matrix is used to construct a select matrix for sentence aligning; finally, obtains the sentence
alignments by KEMM.
Experimental results illustrated that our model outperforms over the Gale’s system on handling any types
of sentence alignments, with 30% total sentence alignment error rate decreasing. These can prove the
effectiveness of our model proposed in this paper.
Acknowledgements
This work was supported in part by the Fundamental Research Funds for the Central Universities of
Northeastern University(N100304006).
References
[1] Gale W, Church K. A program for aligning sentences in bilingual corpora. Proceedings of the 29th
Annual Meeting of the Association for Computational Linguistics. Berkeley 1991;177-184.
[2] Elithorn A., Banerji R.Nagao. M.A Framework of a Mechanical Translation Between Japanese and
English by Analogy Principle. Artifical and Human Inteligence. New York: Elsevier Science Publishers
Corporation; 1984; 173-180.
[3] Brown, R.D. Automated Generalization of Translation Examples. Proceedings of the Eighteenth
International Conference on Computational Linguistics 2000;125-131.
[4] Halil Altay Guvenir, Ilyas Cicekli. Learning Translation Templates from Examples. Information
Systems 1998; 23(6):353--363.
[5] Arnold D., Balkan L., Humphreys R. Lee, Meijer S., Sadler L.. Machine Translation. 1994.