Array15(2022)100189
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/locate/array
Usingtextualbugreportstopredictthefaultcategoryofsoftwarebugs
ThomasHirsch,BirgitHofer∗
InstituteofSoftwareTechnology,GrazUniversityofTechnology,Austria
A R T I C L E I N F O A B S T R A C T
Keywords: Debuggingisatime-consumingandexpensiveprocess.Developershavetoselectappropriatetools,methods
Bugreport and approaches in order to efficiently reproduce, localize and fix bugs. These choices are based on the
Bugbenchmark developers’assessmentofthetypeoffaultforagivenbugreport.Thispaperproposesamachinelearning(ML)
Faulttypeprediction basedapproachthatpredictsthefaulttypeforagiventextualbugreport.Webuiltadatasetfrom70+projects
for training and evaluation of our approach. Further, we performed a user study to establish a baseline for
non-experthumanperformanceonthistask.Ourmodels,incorporatingourcustompreprocessingapproaches,
reachupto0.69%macroaverageF1scoreonthisbugclassificationproblem.Wedemonstrateinter-project
transferability of our approach. Further, we identify and discuss issues and limitations of ML classification
approachesappliedontextualbugreports.Ourmodelscansupportresearchersindatacollectionefforts,as
forexamplebugbenchmarkcreation.Infuture,suchmodelscouldaidinexperienceddevelopersindebugging
toolselection,helpingsavetimeandresources.
1. Introduction trytoprovideanabstractionthatencapsulatesthedifferentchallenges
developers face when dealing with a specific bug type including the
Softwarebugsconsumeasignificantamountofsoftwaredeveloper different tools and approaches for reproduction and localization. To
resources, resulting in financial losses [1]. For these reasons, sizeable leverage on such a classification schema for practical debugging sup-
research fields developed around bugs in software, providing novel port,e.g.,fortoolrecommendation,aprioriknowledgeaboutabug’s
and advanced tools and approaches. In real world software develop- typeisrequired.
ment,thisprogressismostprevalentintheapplicationofante-mortem Inthispaper,weproposeamachinelearning(ML)basedclassifier
approaches that aim at preventing the introduction of bugs, as for that predicts the fault type of a bug based on its textual bug report.
examplebugprediction,staticchecking,andautomatedtesting. This ML based approach tries to encapsulate a small aspect of what
However, once a bug has manifested itself and been reported, would commonly be considered developer knowledge and experience
developers mostly utilize classic debugging tools and techniques to
in debugging. A priori information about the underlying fault type
reproduce,localizeandfixbugs[2–4].Awidevarietyofsuchdebug-
can support inexperienced developers in their choice of debugging
gingtoolsandtechniquesareavailabletodevelopers,e.g.,breakpoints,
approachesandtools.
conditional breakpoints, memory profilers, and leak detection tools.
Thisarticleextendspreviouswork[5]presentedatthe4thInterna-
Whilesomeofthesetoolsarehighlyspecializedtowardsspecificfault
tionalWorkshoponSoftwareFaultsinthefollowingways:(1)Weextend
types and steps of the debugging process, other approaches cover a
the employed classification schema by the Other category. (2) We
broader spectrum of possible applications in the debugging process.
enlargethetrainingsetby127bugticketsandwequantifythequality
However,thereisnoonefitsallsolution.
of the training set with internal and external verification methods.
The developers’ choices of debugging approaches and tools there-
(3) We perform a user survey to establish a baseline of human clas-
forehaveagreatinfluenceontheirsuccessandpaceinfixingthebug.
sifier performance on this task. (4) We introduce preprocessing steps
In most cases, this choice depends on the information obtained from
tailored specifically for bug reports and we apply ensemble learning
thetextualbugreports,andthedevelopers’experienceandknowledge.
In order to encapsulate and discretize some of this knowledge, we methodologiesontheclassificationproblem.(5)Weevaluatetheclassi-
have created a bug classification schema on a high abstraction level. ficationperformanceofvariousclassicalMLalgorithms,preprocessing
We categorize bugs according to their underlying fault into Concur- steps,andensembleapproaches.(6)Weevaluatetheperformancefor
rency, Memory, Semantic, and Other bugs. With these categories, we inter-projectapplication.OurmainfindingsofthisJournalversionare:
∗ Correspondingauthor.
E-mailaddresses: thirsch@ist.tugraz.at(T.Hirsch),bhofer@ist.tugraz.at(B.Hofer).
https://doi.org/10.1016/j.array.2022.100189
Received8March2022;Receivedinrevisedform11May2022;Accepted13May2022
Availableonline26May2022
2590-0056/©2022TheAuthor(s).PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).T.HirschandB.Hofer Array15(2022)100189
• Non-expert human classifier performance averages at 0.62 ResourceLeakExceptioninCR7Duringourtestswithourbenchmark,we
weightedaverageF1. sawaLeakwhichaccordingtousandnormandidnotmakesense.
• DifferentMLalgorithmsshowsignificantlydifferentperformance
These issues make bug reports a challenging target for NLP ap-
onspecificclasses(e.g.,MemorybugsonMultinomialNaiveBayes
proaches. Our task of fault type classification is further complicated
andRandomForrestclassifiers,with0.72and0.79macroaverage
by lack of information or misleading information in such bug tickets.
F1respectively.)
• Ensemble approaches perform (0.69 macro average F1) better Wethereforedonotexpecttoseethehighclassificationperformance
scoresknownfromclassicNLPshowcaseproblems.
than the best performing single classifier model (0.66 macro
averageF1forLogisticRegressionclassifier),withensemblesize
3. Relatedwork
playingonlyaminorrole.
The remainder of this paper is structured as follows: Section 2 Multiple researchers have investigated the application of ML and
describes the problem of fault type prediction using bug reports. In NLPmethodologiesontextualbugreportsforclassificationproblems.
Section3,wediscussrelatedworkandsimilarclassificationendeavors. Lopes et al. [6] automatically classified more than 4000 bug reports
In Section 4, we discuss the background of this work including exist- collected from three open-source database applications according to
ing bug classification schemata, ML based classification, and natural the Orthogonal Defect Classification (ODC) schema. They performed
languageprocessing(NLP).InSection5,wepresentourexperimental undersampling to avoid imbalanced datasets. Thung et al. [7] per-
setupandapproach,followedbyourresearchquestionsandresultsin formedautomatedODCdefecttypeclassificationusingsemi-supervised
Section6.InSection7,wediscusstheinternalandexternalthreatsto learning.OurworkdistinguishesfromLopesetal.’sandThungetal.’s
validity.Section8concludesourworkanddiscussesfutureresearch.All worksintheusedschema.WhiletheODCschemaaimsattheanalysis
datasetsandimplementationsarepubliclyavailable(seeSection8). and optimization of the software development process, our schema
aims to assist developers in choosing the best debugging tool for the
2. Problem bugathand.
Tan et al. [8] categorized bugs w.r.t the dimensions root cause,
The classic NLP example of sentiment analysis is based on the impact,andaffectedcomponent.TheyappliedMLclassifiersassupport
assumption that ‘sentiment’ information is inherent to human written intheirdataminingefforts.WhileTanetal.categorizebugsintocon-
text, and the assumption that the inputs contain only human written currency,memoryandsemanticbugs,weintroduceafourthcategory
text. Bug reports are very different from such showcase NLP prob- (Other) which covers documentation, build system, configuration and
lems. The information required for correct classification may not be UIresourcefaults.Sinceonlyasmallfractionoftheirmanuallylabeled
contained in a bug report: Bug reports are textual descriptions of dataset contains concurrency bugs, they performed a keyword search
complicatedbehaviors,states,andoutcomestocommunicateaproblem usingkeywordssuchas‘race’and‘lock’onthetextualbugreports.In
in a complex system. Such reports are authored by people occupying contrasttotheirwork,weperformedthekeywordsearchonthecommit
differentroles,functions,andpositionsinrelationtotheproject.These messagesinsteadofthebugreportstoreduceinformationleakage.
roles range from end-users unfamiliar with software development, to Lietal.[9]usedMLclassifierstostudybugcharacteristicsinopen-
highly experienced developers with years of experience within the sourcesoftware.SimilartoTanetal.,theyclassifiedtherootcauseof
project. Because of this, the authors’ technical expertise and project bugsintoMemory,Concurrency,andSemanticbugs.
specific expertise can vary significantly. Further, different roles have Ray et al. [10] used ML classifiers to investigate programming
inherently different viewpoints and scopes of the bug reports. For languageandcodequalitymetricsinopen-sourceprojects.Theyapply
example, bug reports may describe only the bugs’ impact in non- fivedifferentrootcausecategories:Algorithmic,Concurrency,Memory,
technical terms, while other bug reports may describe a problem in generic Programming, and Unknown. Since their approach is focused
technicaldetailwithoutmentioningpossibleimpacts. on the analysis of fixed bugs, they train their ML approach on com-
Inaddition,bugreportscantakemanyshapesduetoawidevariety mit messages (a posteriori approach). However, we are interested to
of templates and formatting rules, or lack thereof, as well as the provideinformationtothedevelopersbeforetheyhavefixedthebug.
noncompliance to such rules. This results in noisy,1 and sometimes Therefore, we train our classifier on the textual bug report instead of
incomplete bug reports, that widely vary in length,2 content, and thecommitmessages(aprioriapproach).
vocabulary. Nietal.[11]predictedtherootcausetypefromthecodechanges
In some cases, deriving aforementioned fault type from such bug by converting the code changes into abstract syntax trees (ASTs) and
reportscanbetrivial,asforthismemory bug3: thenusingaTree-basedConvolutionalNeuralNetwork(TBCNN).They
distinguishedsixmainrootcausecategories(function,interface,logic,
Native(java)processmemoryleakAninternalmemoryleakwhenusing
computation, assignment, and others) and 21 sub-categories. In con-
GarbageCollectorMXBean#getLastGcInfointheJVM.Disableusingit...
trasttoourwork,thisclassificationwasperformedpostmortemonthe
bugfix.
However, more often than not, it can be challenging the readers’
Thefollowingapproachesfocusonthepredictionofcategoriesother
domain and project specific knowledge and expertise, as for example
than the root cause: Goseva et al. [12] used supervised and unsuper-
thissemanticbugoriginatingfromSpringBoot4:
visedlearningalgorithmsforlabelingsecurityandnon-securityissues.
Gradle plugin still includes *Launcher classes with Layout.NONE No Wuetal.[13]combinedinteractiveMLandactivelearningtopredict
descriptionprovided. high-impact bugs. Xia et al. [14] proposed a novel feature selection
algorithmformachinelearningbasedclassificationofbugreportsinto
In some cases, it is even impossible, e.g., this bug report, arising
mandelbugs and bohrbugs. Later on, Du et al. [15] proposed a cross-
fromadocumentationerror5:
projecttransferlearningframeworkforthesamepurpose.CaPBug[16]
predictsthecategory(e.g.ProgramAnomaly,GUI,Performance,Test-
Code) and priority of bug reports using different classifiers, but also
1 seee.g.,orientdbissue2121.
2 seee.g.,elasticsearchissue4960. provides a good overview of papers for categorizing and prioritizing
3 elasticsearchissue1118. bugs.
4 spring-bootissue139. ML has been used to automatically distinguish bug reports from
5 nettyissue1508. feature requests [17], to predict the severity [18] or priority [19] of
2T.HirschandB.Hofer Array15(2022)100189
agivenbugreportandtoextractpatternsfromlargecodebases[20]. 4.2.1. Classifiers
A recent survey [21] provides a good overview on the application of MultinomialNaiveBayes(MNB)areprobabilisticclassifiersbased
deep learning approaches in software engineering research, including onBayestheorem.Naiveinthiscontextstandsfortheassumptionthat
defectandvulnerabilitypredictionandbuglocalization. probabilitiesoffeaturesareindependent.MNBclassifiersarefastand
Particularly interesting in the context of fault localization is the easytouseandcanprovideaperformancebaselinetocompareother
approach proposed by Fang et al. [22] that classifies bug reports as classifiersagainst.
informativeoruninformative.Thisapproachcanbeusedasaprepro-
SupportVectorMachines(SVM)arenon-probabilisticapproaches
cessingstepininformationretrieval(IR)approachestofilteroutthose
that can be used for regression and binary classification. SVMs con-
bugreportswhereIRpromiseslittleinsights.
struct a hyperplane in the feature space separating the classes. For
Huangetal.[23]manuallylabeledtheintentionofmorethan5,400
multi-classclassificationproblems,multipleSVMsaretrainedsimulta-
sentences from issue reports into seven categories, e.g. ‘Information
neouslyinaone-vs-allorone-vs-onesetup.
Giving’and‘ProblemDiscovery’.Afterwards,theytrainedadeepneural
RandomForrest(RF)areanensemblelearningapproachthatcan
networktopredicttheseintentions.
beusedforregressionandclassification.RFclassifiersconstructaseries
4. Background of diverse decision trees that are then combined as an ensemble to
performclassification.
First,weprovideanoverviewofexistingbugclassificationschemata LogisticRegression(LR)arelinearclassifiersforbinaryclassifica-
(Section 4.1). Afterwards, we briefly explain the used classifiers, and tion.Logisticregressionaswellaslinearregressionarebothbasedon
statistical methods (Section 4.2) and the most important terms w.r.t. linearmodels.LRusesthelogistic(sigmoid)functiontodiscretizetheir
natural language processing (Section 4.3). Finally, we provide the output—hence thename logisticregression.For multi-class problems,
formaldefinitionsoftheusedperformancemetrics(Section4.4). multipleLRsarecombinedinaone-vs-allorone-vs-onesetup.
4.1. Bugclassificationschemata
4.2.2. Evaluationstrategiesandbalancing
K-fold Cross Validation (K-fold CV) is a statistical method for
Aplethoraofbug,fault,anddefectclassificationschematahasbeen
selectingparametersandevaluatingMLmodels.Thedataisslicedinto
proposed in the past by researchers and practitioners alike. Such a
𝑘 equally sized parts. Training of the model is performed with 𝑘−1
classification schema can cover one or more dimensions of a defect,
e.g. severity, impact, and root cause. All classification schemata are partsandtheresultingmodelisvalidatedontheremainingpart.This
of course intended to fulfill a certain purpose, and their dimensions, is repeated 𝑘 times so that every slice is used as validation set once.
depth,anddetailareselectedtoachievethesetgoal.Thesepurposes Stratified k-fold CV creates slices in a manner that the original class
range from investigations into process optimization to enable auto- distributionispreserved.
matedtriageandprioritization,toresearchintodifferentareasofthe Bootstrap for ML classification is performed by repeatedly gen-
software development process, to the support of techniques such as erating training and test splits using uniform random sampling with
automatedrepair. replacement.Anitemcanthereforeoccurmultipletimesinasplit,but
Polskietal.[24]discussedtheapplicationofexistingfaultclassifi- it cannot occur in both test and training. The resulting performance
cationschemataandbugdistributionsforfaultinjection,andprovided scoresareusedtocalculateconfidenceintervalsfortheconfidencelevel
an overview on fault classification schemata. Endres [25] performed 𝛼asthe(1−𝛼)∕2and𝛼+(1−𝛼)∕2percentiles.
one of the earliest attempts at bug classification to investigate higher Imbalanceddatainthecontextofaclassificationproblemmeans
levelcauses(e.g.,technological,organizational,historic). the amount of items per class is not equal for all classes. While there
Gray[26]introducedthecategoriesBohrbugsandHeisenbugs.Grot-
are ML algorithms that are insensitive to imbalanced data, all of the
tke and Trivedi [27] extended upon Gray’s categories by introducing
above described classifiers are sensitive to such imbalances. There
Mandelbugsandaging-relatedbugs.
are basically two strategies to balance a training set, up-sampling
Chillarege et al. [28] devised the Orthogonal Defect Classification
and down-sampling. Up-sampling creates synthetic instances for the
(ODC)schematoformthebasisforanalysisandoptimizationofasoft-
minorityclasstoscaleituptomatchthemajorityclassessize.Down-
waredevelopmentprocess.TheIEEEStandardClassificationforSoftware
samplingremovesinstancesfromthemajorityclasstoscaleitdownto
Anomalies(IEEEStd1044–2009)[29]hasestablishedavocabularyfor
matchtheminorityclassessize.
softwareanomaliesaswellasaclassificationschemaandattributesfor
defectsandfailures.
Only a few classification schemata target the debugging process 4.3. NaturalLanguageProcessing(NLP)
with the purpose of supporting software developers. Li et al. [9] and
Tanetal.[8]studiedthecharacteristicsofbugsinopensourcesoftware Inthefollowing,weprovideanoverviewofthepreprocessingand
to enable more effective debugging tool design and better under- preparation steps for applying machine learning algorithms on text
standing of bugs occurring in the real world. Given this focus on documents.
debugging tools and debugging processes, they classified bugs along In theTokenization step, theinput document issplit intotokens.
threeaxes:impact,softwarecomponent,androotcause.Impactconsists Themodelsdescribedinthisworktokenizebywords.
of six categories (e.g., incorrect function, crash, or hang). Their root
Vectorizationcreatesafeaturevectorfromatokenizeddocument.
cause dimension comprises three categories: Memory bugs arise from
The models in this work utilize simple Count Vectorizers, where the
impropermemoryhandling,concurrency bugsoccurinmulti-threaded
feature vector represents the count of occurrences of the individual
programsduetosynchronizationissues,includingraceconditionsand
tokensinthedocument.
deadlocks,andsemanticbugsareinconsistenciesbetweenrequirements
Bagofwordsisacommonapproachinwhichthedocumentissplit
orprogrammers’intentionsandtheactualsoftwarefunction.
into word-tokens followed by vectorization. The order of words and
4.2. Machinelearning theirconnectionstoeachotherarelost;adocumentisrepresentedas
anunorderedcollectionofwords.
In this work, supervised machine learning approaches are applied Termfrequency-inversedocumentfrequency(Tf-idf)isatrans-
to perform automated classification. The problem at hand is a multi- formation method that applies weights depending on how common a
classclassificationproblem,whereeachinputdatainstancebelongsto termisinthewholedocumentcorpus.Tf-idftransformationofagiven
aspecificclass. term 𝑡 and document 𝑑 is the term frequency 𝑡𝑓, as the number of
3T.HirschandB.Hofer Array15(2022)100189
occurrencesoftheterminthisdocument,timestheinversedocument WeightedaverageF1istheaverageofallclasses’F1scoreswith
frequency𝑖𝑑𝑓 oftheterm. thenumberofinstancesofeachclassasweightsforthisclass.
𝑇𝑓−𝑖𝑑𝑓(𝑡,𝑑)=𝑡𝑓(𝑡,𝑑)∗𝑖𝑑𝑓(𝑡) (1) 𝑤𝑎𝐹1= ∑𝑐𝑙𝑎𝑠𝑠𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒𝑠 𝑐𝑙𝑎𝑠𝑠∗𝐹1 𝑐𝑙𝑎𝑠𝑠 (3)
∑𝑐𝑙𝑎𝑠𝑠𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒𝑠
( ) 𝑐𝑙𝑎𝑠𝑠
𝑛+1
𝑖𝑑𝑓(𝑡)=𝑙𝑜𝑔 +1 (2) Macro average F1 in a multi-class problem is the unweighted
𝑑𝑓(𝑡)+1
meanofallclasses’F1scores.Givenabalancedinput,themacroand
Thedocumentfrequency𝑑𝑓(𝑡)isthenumberofdocumentsinthecor- weightedaverageF1scoresproducethesameresult.
pusthatcontaintheterm𝑡.TheresultingTf-idfweightedvectorsareof
∑𝑐𝑙𝑎𝑠𝑠𝐹1
Euclideannorm.Theformulafor𝑖𝑑𝑓 abovedepictstheimplementation 𝑚𝑎𝐹1= 𝑐𝑙𝑎𝑠𝑠 (4)
intheMLsoftwarelibraryusedinthiswork,whichdiffersslightlyfrom
|𝑐𝑙𝑎𝑠𝑠|
itstextbookdefinition[30](see[31]foradetailedexplanation). Cohenskappameasuresinter-rateragreementcorrectedforagree-
Stemmingreplaceswordswiththeirwordstem.Forexample,the mentbychance.Itiscalculatedbasedontheproportionofitemswhere
words ‘crash’ and ‘crashing’ have the stem ‘crash’. The resulting stem bothratersagree(𝑝 0),andtheproportionoftimeswhereagreementis
doesnotnecessarilyhavetobeaworditself. Withoutpriorstemming, expected by chance (𝑝 𝑐) [34]. 𝜅 values between 0.41 < 𝜅 < 0.60 are
‘crash’ and ‘crashing’ are considered different tokens and therefore consideredasmoderate,0.61<𝜅<0.80assubstantial,and0.81<𝜅<
resultinseparatefeaturesinabagofwordsapproach;withstemming 1.00asalmostperfectagreement[35].
bothwordsarerepresentedinthesamefeature. 𝜅= 𝑝 0−𝑝 𝑐 (5)
Casefoldingisperformedtoprovideadocumentwithalllettersin 1−𝑝
𝑐
thesamecase.Vectorizersarebasedonstringcomparison,occurrences
5. Experimentalsetup
of the same word starting with an upper case letter would be repre-
sentedasadifferentfeaturethanthesamewordstartingwithalower
First, we explain the used classification schema (Section 5.1). Af-
caseletter.
terwards, we describe the underlying dataset (Section 5.2) and the
Decamelcasing splits camelcased words into several words that
creation process of the training set (Section 5.3). Finally, we present
allowsfortokenizationandstemmingofthosewords.
theMLmodelingapproach(Section5.4).
Stop word removal erases very common words (e.g. ‘the’, ‘and’,
‘is’)thatdonotaddvalueforthetaskathandfromtheinputtext.
5.1. Classificationschema
Artifact removal discards non-human language artifacts such as
stack traces, code snippets, config files, file listings, log outputs and
Our classification schema should aid developers in the debugging
thread dumps from bug reports. Some bug reports in our dataset are
process. We base it on the root cause dimension of Tan et al.’s clas-
asbigas80kboftextbecauseofsuchartifacts. Amongstthebiggest
sification schema [8]. This root cause classification is intended to
artifactsarestacktraces.Whilethosetracessupportdevelopersintheir
encapsulate the most promising debugging approaches and specific
debuggingefforts,thecontainedinformationforclassificationpurposes
debugging tools for each category. The top-level of our classification
ismostlylimitedtonameandtypeoftheoccurredexceptions.
schema is composed of four main categories, Concurrency, Memory,
Thewidevarietyofdifferentformatsofartifactsposesasignificant
Other, and Semantic. The group of useful debugging tools for Concur-
problem for automated removal using regular expressions [32]. We
rency bugs will be disjoint from the group of tools useful for bugs in
thereforeemployourcustomMLbasedartifactremovalprocess[33].
theOther category.
TheunderlyingMLmodelistrainedonsoftwareprojects’documenta-
Table 1 provides an overview of our classification schema. A de-
tion files as well as issue tickets and leverages GitHub markdown for
tailed documentation including examples and for the increasingly de-
automatedtrainingsetcreation.Theresultingclassifiermodeloperates
tailed subgroups can be found in our online appendix. The purpose
on a line-by-line basis and keeps exception names that occur in the
of these detailed levels of our classification schema is two-fold: to
artifact.
serve as documentation and education tool for manual classification,
andtoprovideadditionalinformationthatinfersfaultandfixpatterns
4.4. Performancemetrics providing a clearer picture to the reader. The top-level categories
Concurrency,Memory,Other,andSemanticareusedintheexperiments
We use Precision (P), Recall (R), F1-score (F1) in single class ex- inthenextpartsofthispaper.
aminations to measure and rank our classifiers’ performance, and to
enableinter-classifiercomparison,andtheirweightedaverageF1-score 5.2. Dataset
(waF1) and macro average F1-score (maF1) to compare multi-class
performance. These metrics can be calculated from the classifiers’ Tocollectareasonablenumberofissuetickets,wemined101open-
confusion matrices. True Positives (TP) is the number of instances in source Java projects hosted on GitHub. These projects cover a wide
thepredictedclassthatmatchtheactualclass.FalsePositives(FP)ex- variety of different software domains, ranging from server side appli-
pressesthenumberofinstancesinpredictedclassthatdonotmatchthe cations,databaseapplications,MLframeworks,testingframeworks,to
actualclass.TrueNegatives(TN)isthenumberofinstancescorrectly mobileapplications.Weaddedallclosedissueticketsoftheseprojects
identifiedasnotbelongingtotheclass,andFalseNegatives(FN)isthe whoselabelscontainanyofthestrings‘bug’,‘defect’,‘breaking’,or‘re-
numberofinstancesincorrectlyidentifiedasnotbelongingtotheclass. gression’toourdataset,i.e.,54755issuetickets.Theresultingdataset
PrecisionforaclassisTPdividedbythetotalnumberofinstances is rather noisy due to quirks in GitHub API, bug triaging performed
classifiedtobelongtothisclass(𝑃 = 𝑇𝑃 ).Aprecisionof1.0means manually by project maintainers, and varying workflows in different
𝑇𝑃+𝐹𝑃
thatallinstanceslabeledasclassXarecorrect; projects. Since GitHub API does not differentiate between issues and
pull requests, the initial dataset contains both. Further, issues can be
RecallforasingleclassisTPdividedbythetotalnumberofinstances
closedduetoavarietyofreasonsincludingduplication,reluctanceto
actually belonging to this class (𝑅 = 𝑇𝑃 ). A recall of 1.0 means
𝑇𝑃+𝐹𝑁 fix,orrejectedas‘notabug’despitethelabel.Tocleanthedataset,we
thatallinstancesofactualclassXarecorrectlylabeledasbelongingto
removedallissueticketsthat
classX.
F1forasingleclassistheharmonicmeanoftheclass’precisionand • arepullrequests,
recall(𝐹1=2∗ 𝑃∗𝑅). • havezeroormorethan10commitslinked,
𝑃+𝑅
4T.HirschandB.Hofer Array15(2022)100189
Table1
Faulttypeclassificationschemawithtop-levelcategoriesinboldfontand2nd-levelcategoriesinnormalfont.
Faulttype Description
Concurrency Improperorincorrectsynchronizationandwrongassumptionsabout
atomicity.
Orderviolation Incorrectordercausedbymissingorincorrectsynchronizationorthread
handling.
Racecondition Twoormorethreadssimultaneouslyaccessthesameresourcewithat
leastonebeingawriteaccess.
Atomicviolation Atomicityofaoperationwaswronglyassumed,orimproperuseofnot
threadsafedatatypesandstructures.
Deadlock Twoormorethreadsstuckinwaitingfortheothertoreleasearesource.
Parallelization Missingparallelizationresultinginlagging,orremovalofparallelizationto
meetconstraints.
Other Concurrency-relatedfaultsthatdonotfallinanyoftheabovecategories.
Memoryandresources Improperorincorrectmemoryandresourcehandling.
Overflow Bufferoverflows,excludingoverflowingnumerictypes.
Nullpointerdereference Nullpointerdereferences.
Uninitializedmemoryread Uninitializedmemoryreadsexceptnullpointerdereference.
Leak Memoryandresourceleaks.
Danglingpointer Pointersreferringtoinvaliddata.
Doublefree Freeingthesamememorymorethanonce.
Other Memory-relatedfaultsthatdonotfallinanyoftheabovecategories.
Other BugsthatcannotberesolvedbychangingJavacode.
Documentation Incorrectormissingdocumentation.
Buildsystem Missingorincorrectbuildconfigurationandbugsinbuildscripts.
UIresources MissingorincorrectUIresources.
Configuration Missingorincorrectconfigfiles,excludingbuildconfigandUIconfig.
Semantic Inconsistencyofrequirements,programmersintentions,andactual
implementationthatdonotfallintheabovecategories.
Exceptionhandling Incorrect,missing,orovershootingexceptionhandling.
Missingcase Codeismissing.Missingimplementation,missingprogramflow,or
missingothercodepartsduetounawarenessofacertaincase.
Processing Codeisincorrect.Incorrectimplementations,rangingfrommiscalculations
toincorrectlibraryusage.
Typo Simpletypographicerrors.
Dependency Bugsintroducedbychangesinaforeignlibraryorsystemthatlead
softwarethatcanbebuilt,butbehavesunexpectedorincorrect.
Other Faultsthatdonotfallintoanyoftheabovecategories.
• changedmorethan250LOCormorethan20filespercommit, carryanunderstandingoftheunderlyingproblem,whilealsocarryinga
• linktoatleastonecommitthathasbecomeunavailable, certainleveloftechnicalexpertiseandassociatedvocabulary.Further,
• havecommitswithmorethanoneparent,or the corpus of commit messages is distinct from the corpus of textual
• have commit messages mentioning more than one issue ticket bug reports that constitutes the inputs of our models. This reduces
id or containing phrases that indicate multiple fixes (e.g., ‘also unwanted biases towards certain keywords in our machine learning
fixes’). experiments.
Selectionofbugreportsandmanualclassification.Wesplitourdataset
Thisreduceddatasetconsistsof11621bugreportsandformsthebasis into two groups: issues containing Java code changes (10146 issues)
forourexperiments. andissuesthatdonot(1475issues).Onthisfirstgroup,weappliedour
keywordsearch,yielding756results,ofwhichwemanuallyanalyzed
5.3. Trainingsetcreation 514. Further, we randomly selected and analyzed 158 issues from
this group to adequately sample the majority class (Semantic). From
Managing imbalance. This raw collection of issues tickets is ex- the second group, we randomly selected and analyzed 508 issues to
pectedtobehighlyimbalancedw.r.t.ourclassificationtarget.Otherre- adequatelysampleourOther category.Intotal,wemanuallyanalyzed
searchersidentifiedMemory andConcurrency bugsasminorityclasses, 1180issuesfrom86softwareprojects.Duringthismanualexamination
making up only 2%-16% of all bugs [8–10]. Since our selected ML efforts, we removed 418 issues from the dataset for the following
algorithms are sensitive to such imbalance, we will employ down- reasons:
sampling to balance our dataset. However, a certain number of data
• Reportsthatwerenotconsideredabug.
pointsfortheminorityclassesarerequired,asthesizeoftheseminority
• Corresponding fixes were in programming languages other than
classes dictate the resulting training set size. We estimate that our
Java,iftheydidnotfallintotheOther category.
dataset of 11621 bug tickets contains only a few hundred Memory
• Non-Englishbugreports
and Concurrency bugs. Since exhaustive examination of all issues is
• The fault type was indeterminable within reasonable amount of
deemed infeasible, we need to filter and preselect issues for manual
time.
examination.
We therefore performed a keyword search on commit messages Themostcommonreasonforanindeterminablefaulttypewererefac-
to identify candidates for manual classification. We used a modified torings,enhancements,andothermajorcodechangeswithinthelinked
version of the keywords used by Ray et al. [10]. Our keyword set commitsthatmakeitimpossibletoisolatethecoderesponsibleforthe
contains29keywordsandregularexpressions,e.g.,‘overflow’,‘\sleak’, bugwithoutin-depthprojectknowledge.
‘deadlock’, ‘\shangs\s’, ‘\sstarves\s’. The complete list of keywords is The resulting data set has a total size of 762 bug reports from 75
availableintheonlineappendix(seeSection8).Commitmessagesare different software projects. It contains 155 concurrency, 132 memory,
authored by the developers performing the bug fix. These developers 142other,and333semanticbugs.
5T.HirschandB.Hofer Array15(2022)100189
Datasetqualityandverification.Researcher1performedthemanual
classificationdescribedabove.Sixmonthslater,Researcher1reclassi-
fied100randomlysampledandblindedissuesfromthesetforinternal
verification.Inthisinternalverificationstep,Researcher1scored0.95
for Cohens Kappa, that suggests almost perfect agreement [35] and a
weightedaverageF1of0.96.
For external verification, Researcher 2 classified 246 randomly
sampled and blinded issues. This external verification resulted in a
CohensKappascoreof0.69,suggestingsubstantialagreement [35],and
aweightedaverageF1of0.80.
Finaldatasets.InpreliminaryMLexperiments,weidentifiedthree
projects that are not suitable for our approach of fault type classifi-
cation.Theseprojectsare:LeakCanary,aJavamemoryleakdetection
library, Bazel, a build automation framework, and JHipster, a web
application generator. Their domains make it impossible to correctly
identify certain bug types using an NLP approach based only on bug
reports without knowledge on the projects’ domain and purpose. For
example, bug reports from a memory leak detection library utilize
a vocabulary otherwise directly connected to memory leaks for all
classesofbugs,furtherreinforcedbyclassnamesandfunctionnames Fig.1. SurveyF1Bootstrapconfidenceintervals.
withinthesesoftwareprojects.Analogtothis,bugreportsfromabuild
frameworkhaveavocabularyotherwiseassociatedwithOther bugsin
anygeneralpurposesoftwareproject.Wethereforeremovedallitems
Approach:Weperformedanonlineusersurveytaskingparticipants
fromtheseprojectsfromthetraining/testsetsandsurveyanswers. to classify textual bug reports according to their fault type in four
We used the initial manual classifications of Researcher 1 as the categories.Thesurveywaspromotedviaemailanddirectmessagesto
basis for the experiments and balanced the resulting data set by un- professional developers at resident software companies and to master
dersamplingthemajorityclasses,resultinginequallysizedcategories students in the field of computer science. It can be assumed that the
containingeach124bugreports.Thefinaldatasetusedinourexper- majorityofsurveyparticipantswerestudents.Anonymousparticipation
iments has a total size of 496 bug reports from 71 different software was allowed, while participants disclosing a contact e-mail address
projects. couldwinvouchersofawell-knownonlinestore.Multiplesubmissions
fromthesamepersonwereallowed.Foreachsubmission,wetracked
5.4. MLclassifier IPaddress,emailaddress(optional),timespentoneachbugreport,and
thecorrespondinganswersforeachbugreport.
MLalgorithms.WehaveselectedLogisticRegression(LR),Multino- Abalancedsetof500bugreportswasrandomlysampledfromthe
mial Naive Bayes (MNB), Random Forrest (RF), and Support Vector 762bugreportsalreadyinvestigatedbyResearcher1(see5.3).Partic-
Machines (SVM) based on other researchers’ work in similar endeav- ipantswereprovidedtenbugreportsrandomlyselectedfromapoolof
ors[6,8,9,17,36],andtheireaseofuse.Further,weemployaLR-based 500.Thebugreportswereformattedthesamewayastheoriginalson
stacking classifier ensemble learning approach to combine the best GitHub,andsinglechoiceanswersforeachbugreportwereavailable.
performingmodels. WhilewedidnotprovidealinktotheoriginalbugticketonGitHub,
Experimentsetup.Ourmodels’pipelinesconsistofartifactremoval, inlineimagesasforexample,screenshots,andlinksinthebugreport
and decamelcasing, followed by stemming and count vectorization, textswerepreserved.Participantsweregivenashortintroductioninto
throughTf-idf,tofinallyaclassifieralgorithm.WeperformnestedCV ourclassificationschemaonthesurvey’sentrypage.Shortexplanations
with five folds each for hyperparameter tuning and model selection. of the four fault types were available during participation inline of
Theinner𝑘-foldCVisusedforhyperparametertuningofthemodels. eachpage.Further,alinktodetailedexplanationsandexamplesofour
Theselectedhyperparameterspaceonlyconcernspreprocessingsteps, classificationschemawereprovided.Therewasnotimelimit.
and consists of enabling or disabling stop word removal, decamelcas- Results:Wereceived51submissions.Wecarefullyinvestigatedthe
ing, stemming, artifact removal, and usage of Tf-idf. Parameters of timesspentandIPaddressesanddidnotfindanysubmissionsindica-
theclassifieralgorithmsarenottunedandkeptattheirdefaultvalues tiveofbeingmadewithmaliciousintent.Thesubmissionsprovideus
(scikit-learn0.24.2).Theouter𝑛-foldisusedformodelselection.Allk- with a total of 510 manually classified bug reports for our analysis.
foldCVsplitsusedinourexperimentsweredoneinastratifiedmanner Removal of bug reports originating from three projects as discussed
toconservethebalancednatureoftheinputdataset. in , leaves us with 483 classified bugs as the basis for the following
To evaluate the resulting models, we use Bootstrap with stratified discussion.
80–20 training-test splits, 𝑛 = 100 repetitions and a confidence level Fig. 1 shows the Bootstrap intervals from survey submissions (𝑛 =
of 𝛼 = 0.95. We calculate the mean performance scores from all 1000, 𝛼 = 0.95). The mean performance in terms of weighted average
Bootstrap repetitions. For comparing two models, we perform one- F1 over all classified bug reports is 0.62. Participants performed best
sidedT-testsonthescoresfrombothmodels’Bootstraprepetitionswith onMemory bugs,withameanF1scoreof0.63,andworstforSemantic
𝐻 0=𝑀𝑜𝑑𝑒𝑙𝐴𝑖𝑠𝑛𝑜𝑡𝑏𝑒𝑡𝑡𝑒𝑟𝑡ℎ𝑎𝑛𝑀𝑜𝑑𝑒𝑙𝐵.Further,webuildconfusion bugswithameanF1scoreof0.47.
matrices through aggregation of the Bootstrap iterations’ confusion Fig. 2 shows the normalized confusion matrix of all received an-
matrices. swers.OtherbugsaremostfrequentlymisclassifiedasSemanticbugsby
participants. Recall is lowest for Concurrency bugs (0.56) and highest
6. Experimentalresults for Memory bugs (0.68). Precision is lowest for Semantic bugs (0.48)
andhighestforOther bugs(0.70).
6.1. RQ1:Whatistheperformanceofhumansclassifyingbugreports? Discussion:Developersforeigntoasoftwareprojectperformwith
a weighted average F1 score of 0.62, and can correctly identify the
Motivation:ByansweringRQ1,weestablishabaselinetocompare faulttypebasedonabugreportin62%ofthecases.Aswelackthe
MLapproachesagainst. samplesizetoinvestigatereasonsformisclassificationonasinglebug
6T.HirschandB.Hofer Array15(2022)100189
Results:Fig.4(a)showsthemacroaverageF1Bootstrapconfidence
intervals and mean performance of the resulting models. The mean
macro average F1 scores of the classifiers are closely clustered, 0.64
forMNB,0.65forSVM,0.65forRF,and0.66forLR.OnlyLRscores
arestatisticallysignificant(𝑝<0.025)differentfromtheothermodels’
scores(𝑝=0.003).
However,investigatingthemodels’performanceonspecificclasses
shows that despite their similar macro average F1 scores, there are
differencesintheircapabilities.Fig.4(b)showstheF1Bootstrapcon-
fidence intervals and mean performance of the modelsforeach class.
The most apparent difference can be observed in the performance of
theLRandRFmodelsonMemory andOther bugclasses.ThemeanF1
performanceoftheLRandRFmodelsare0.69and0.65fortheOther
class, and 0.74 and 0.79 for the Memory class. One sided T-tests on
themodels’performancescoresshowthattheRFmodelisstatistically
significantbetterthanLRmodelforMemory bugs(𝑝=2∗10−10),and
viceversaforOther bugs(𝑝=2∗10−10).
EXP3:ToleveragetheinsightsfromEXP1andEXP2,weuseensem-
Fig.2. Confusionmatrixofallreceivedanswers. ble methods. To train and identify the best models for each class, we
performnestedcrossvalidationforeachclasswhileaddingweightsto
thisclassandatthesametimeusingtheclassspecificF1measureas
reportlevel,wecanonlyspeculateoverthereasonsforthisratherlow metricforinnerCVhyperparametertuning.Weperformthisstepforall
overallperformance.However,issuessuchasincomplete,ambiguous, classifieralgorithms.Giventhe𝑛foldouterCV,thisprovidesuswith𝑛∗
orevenmisleadingbugreportsaresuspectedtoplayasignificantrole 𝑛𝑢𝑚𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑒𝑟𝐴𝑙𝑔𝑜𝑟𝑖𝑡ℎ𝑚𝑠modelsforeachclass.Fromthese,weselect
in this. Netty issue 15086 for example, was classified as memory bug the best 𝑚 models for each class and ensemble them using a stacking
byourparticipants,whilethesubsequentdiscussionandfixingcommit classifier ensemble with LR as base classifier. The base LR classifiers’
on GitHub clearly show it to be a documentation error and therefore parametersarekeptatthedefaultvalues.Theresultingensemblesize
belongingintotheother category. is 𝑚 ∗ 𝑛𝑢𝑚𝐶𝑙𝑎𝑠𝑠𝑒𝑠. We trained and evaluated the resulting ensembles
Answer: Based on these findings, we establish a human (non- with𝑚=1,2,5usingBootstrap.
expert)performancebaselineof0.62weightedaverageF1. Results: The models contained in the ensembles and their corre-
spondinghyperparametersareshowninTable2,Ensemble1consisting
6.2. RQ2:WhatperformanceisachievableusingMLalgorithmsforclassi- of the topmost four classifiers, Ensemble 2 of the topmost eight clas-
fyingbugreports? sifiers, and Ensemble 3 of all classifiers listed in the table. Due to
aforementionedselectionprocessweobservecertainhyperparameters
Motivation: We investigate the performance of classic ML ap- tobeconnectedtospecifictargetclasses,mostnotablyartifactremoval
proaches for fault type classification. Despite this narrow focus, cap- asalreadydiscussedinEXP1.
turing only a small but essential aspect of a bug, the obtained results Fig.5(a)showsthemacroaverageF1Bootstrapconfidenceintervals
canserveasanindicatoroftheapplicabilityofclassicMLapproaches andmeanperformanceoftheresultingensemblesandtheonaverage
ontextualbugreportsfortechnicaldebuggingsupport. bestperformingmodelsfromEXP2.ThemeanmacroaverageF1per-
Approach:Weperformthreeconsecutiveexperiments,measurethe formances of the ensembles are closely clustered, ranging from 0.68
performanceofclassifiermodels,andidentifybenefitsanddrawbacks forEnsemble1to0.69forEnsemble3.T-testontheensembles’scores
of preprocessing steps and classifier models. Further, we investigate showsthatwecannotrejectthenullhypothesisthatthedistributions
misclassificationsandtheirreasons. of scores differ (𝑝 = 0.118 for Ensemble 1 scores to be different from
EXP1:WeperformednestedCVwithanSVMclassifieralgorithmto
Ensemble2scores,and𝑝=0.078forEnsemble1scorestobedifferent
establishaperformancebaseline.Thebestscoringmodeloftheouter
fromEnsemble5scores).
CVwasthenevaluatedusingBootstrap.Weperformedthisexperiment
However,Ensemble1withameanmacroF1scoreof0.68performs
twice,withandwithoututilizingartifactremoval.
significantlybetterthanthesingleclassifiermodelsfromEXP2.Results
Results:Fig.3(a)showsthemacroaverageF1Bootstrapconfidence
for one sided T-test for Ensemble 1 scores being greater than the LR
intervals and mean performance of the two models. Artifact removal
classifier from Experiment 2 are 𝑝 = 4 ∗ 10−11 (Ensemble 1 > SVM:
increases the classifier’s mean macro average F1 from 0.62 to 0.65.
𝑝=4∗10−19).
A one sided T-test of the models’ performance scores confirms that
Fig. 5(b) shows the ensembles’ performance across different bug
themodelwithartifactremovalisbetterthanthemodelwithoutthis
classes. One sided T-testing Ensemble 1 scores against LR show that
preprocessingstep(𝑝=4∗10−10).
it performs significantly better for the class of Other (𝑝 = 0.008)
However,thisincreaseinoverallperformanceisnotreflectedinin-
and Semantic (𝑝 = 1 ∗ 10−10) bugs. The same test for the best
creasedperformanceforeachbugclassascanbeobservedinFig.3(b).
classifiers for Memory and Concurrency bug classes from EXP2 (RF
While for example, the model with artifact removal performs signifi-
and LR respectively) does not allow to reject the null hypothesis that
cantlybetterthanthemodelwithoutartifactremovalforMemory bugs
(𝑝=2∗10−30)withmeanF1scoresof0.74vs.0.66,itistheotherway Ensemble1performsbetter(RFMemory 𝑝=0.060andLRConcurrency
round for Semantic bugs (𝑝 = 0.001) with mean F1 scores of 0.48 and
𝑝=0.069).However,theseresultssuggestthatEnsemble1performsat
0.66. leastequallywellforthesebugclasses.
EXP2: We performed nested CV with MNB, SVM, RF, and LR Fig. 6 shows the cumulative confusion matrix of Ensemble 1 col-
classifier algorithms. We again selected the best scoring model from lected from the 100 Bootstrap iterations. Semantic bugs are the ones
theouterCVandevaluatedthemusingBootstrap. mostoftenconfusedwithothercategories,mostnotablytheOther cat-
egory.Toinvestigatethereasonsofmisclassifications,wecollectedall
testsetsandtheirpredictionsfromallBootstrapiterationsofEnsemble
6 nettyissue1508. 1, yielding 22286 bug reports and their predictions. 7072 of which
7T.HirschandB.Hofer Array15(2022)100189
Fig.3. EXP1:BootstrapconfidenceintervalsforSVMandSVMwithartifactremoval.
Fig.4. EXP2:Bootstrapconfidenceintervalsforbestperformingmodelfromeachclassifieralgorithm.
Fig.5. EXP3:BootstrapconfidenceintervalsoftheensemblesandofbestperformingclassifiermodelsfromExperiment2.
wereincorrectpredictions,andtheremaining15214predictionswere artifact portions in the always correctly predicted group, the sample
correct. sizeistoosmallandtoonoisytoallowanyconclusionstobedrawn.
Wecomparedthelengthincharactersandlengthinlines,forboth Therefore, we investigated the content of these bug tickets rather
theoriginalbugreports,andthebugreportsafterartifactremoval,as than statistical measures on their size. We created a vocabulary from
wellasthelengthsincharactersandlinesofremovedartifacts.Further, all words included in these bug reports, using case folding and stop
wecomparedthenumberofoccurrencesofexceptionnamesinthebug wordremoval,aswellasremovalofwordscontainingdigitsandspecial
tickets,aswellasthenumberofbugticketsthatcontainsuchexception characters. We counted in how many bug reports each word occurs.
names.However,noneofthesemetricsshowanysignificantdifferences Fig. 7 shows the top twenty words for both groups. For the group of
betweenthemisclassificationsandcorrectclassifications,whichcanbe always correctly identified items, the corresponding bug reports are
explainedbysignificantoverlapofcontainedbugticketsinbothcorrect explicitespeciallyregardingMemory andConcurrency issues,whilethe
andincorrectsetsbecauseofthesmalloriginaldatasetsizeof496bug alwaysincorrectlyidentifieditemsarelackingsuchexpressiveness.
tickets. We further investigated the fault types of the always correct and
We therefore investigated bug tickets that were always predicted always incorrect groups of bug tickets. The latter group is headed
correctly(141),oralwayspredictedincorrectly(33).Whileabovelisted by Semantic bugs (11) while the always correctly identified group is
metrics hint towards shorter tickets with shorter non-human written stronglydominatedbyMemory bugs(56).
8T.HirschandB.Hofer Array15(2022)100189
Table2
Hyperparametersusedinensembleclassifiers.
Classifier Target Artifact Class Decamel- Tf-idf Stemm- Stopword
removal weight case ing removal
SVM Concurrency true 0_4 false true true false
RF Memory true 1_4 true true false English
MNB Other false – false true false English
RF Semantic false 3_4 true true true false
RF Concurrency false 0_4 true true true English
MNB Memory true – false false true English
MNB Other true – false true true English
MNB Semantic false – false false true false
MNB Concurrency true – false false true English
MNB Concurrency true – false false true English
MNB Concurrency true – false false true English
SVM Memory true 1_4 false true true false
RF Memory true 1_4 true true false English
RF Memory true 1_4 true true true English
MNB Other true – true true true false
SVM Other false 2_4 true true true false
SVM Other false 2_4 true true true false
SVM Semantic false 3_4 true true true false
RF Semantic false 3_4 true true true false
RF Semantic false 3_4 true true true English
effectoncertaincategories.Further,wefoundthatdifferentclassifier
algorithmsdiffersignificantlyintheircapabilitiestocorrectlyidentify
certainclasses.
Weshowthatbothfindingscanbeexploitedbyensemblemethods
to deliver better overall classification performance without the draw-
backsofseparateclassifiersandasingularpreprocessingpipeline.Our
results suggest that ensemble size is not a critical factor, and an en-
sembleconsistingofoneclassifierfinetunedtoaspecificclassalready
provides significantly better performance than any single classifier
approach.
Our investigation into this smallest ensemble’s classification capa-
bilitiesshowsthatSemanticbugsarethehardesttoidentifycorrectly,
andthattheyaremostoftenconfusedwithOther bugs.
Comparingmisclassifiedbugreportsagainstcorrectlyclassifiedones
doesnotshowanystatisticalrelevantdifferenceintermsofdocument
lengthorcontentsintermsofexceptionnamesorartifacts.However,
investigationintothesmallersubgroupsofalwaysincorrectlyoralways
Fig.6. EXP3:Cumulativeconfusionmatrixfrom100BootstrapiterationsforEnsemble
correctly identified items shows the latter group’s higher expressive-
1.
ness in terms of vocabulary that can directly point towards the bug’s
faulttype.Asexpected,Semanticbugsdominatethealwaysincorrectly
identifieditems,whileMemory bugsarethemostfrequenttypeinthe
alwayscorrectlyidentifieditems.
Answer:WeachieveameanmacroaverageF1scoreof0.68,and
a Bootstrap confidence interval (𝛼 = 0.95) ranging from 0.63 to 0.73
when using ensemble methods. The ensembles perform significantly
betterthansingleclassifiermodels(MNB,LR,RF,SVM,scoring0.66,
0.65, 0.64, 0.65 mean macro average F1 respectively). The ensemble
sizedoesnotsignificantlyincreaseperformanceinourexperiments.
6.3. RQ3:CantheproposedMLapproachbeusedforinter-projectclassifi-
cation?
Motivation: Here, we consider the practicability, portability, and
limitationsoftheapproach.
Approach: We measure the performance of our machine learning
approach in the scenario that training sets and validation sets are
buildfromdifferentsoftwareprojects.Wegenerated100validationand
trainingsplitsbyrandomselectionoforiginsoftwareprojectsfromour
Fig. 7. EXP3: Words contained in always incorrectly and always correctly predicted dataset.Thesesplitswerechosensothatvalidationsplitsizeisbetween
bugreportswithEnsemble1.
18% and 20% (90–98 bug reports) of the full dataset and validation
splitsareroughlystratified(numberofbugreportsforeachbugclass
within0.5ofthemeanofallfourclasses).Theresultingtesttraining
Discussion: Our results show that preprocessing steps as artifact splitsarecomprisedofissueticketsoriginatingfromdifferentsoftware
removal,althoughincreasingoverallperformance,haveadetrimental projects.
9
3elbmesnE
2elbmesnE
1.mesnET.HirschandB.Hofer Array15(2022)100189
Analog to our analysis in RQ2, we investigated the bug tickets
thatwereeitheralwayscorrectlyclassified(99bugs),oralwaysincor-
rectly classified (24 bugs). The always correct classified bug reports
are headed by the Other class (36 bugs), followed by Memory (30
bugs),Concurrency(29bugs),andSemantic(4bugs)classes.Thealways
incorrectlyclassifiedbugreportsareheadedbytheSemanticclass(10
bugs),followedbyMemory andConcurrency classes(each5bugs),and
Other class (4 bugs). Manual examination of the always misclassified
bugs revealed 6 bugs that we consider hard to classify and 6 that
we consider impossible to classify with the given information; the
remainderprovideenoughinformationandcontext.Theimpossibleto
classify bugs are e.g., reports on improper function that arose from
wrong or improper documentation and were fixed as documentation
Fig.8. 95%percentilesofF1macroaveragescoresfor100projectsplits. corrections or enhancements, or reports on incorrect function where
therootcausewasmissingUIresourcefiles.
Discussion:AcomparisonwithourfindingsfromRQ2showsthat
Analog to our approach in RQ2, we perform nested CV for each themeanclassifiersperformanceishigherforcrossprojectapplication
classifier algorithm (SVM, LR, RF, MNB) and for each category while of our model. To investigate this effect, we evaluate the classifier
performanceonasingleprojectleveltostudytheirimpactonoverall
adding weights to the respective category using only the training set.
Wetakethebest𝑚modelsfromeachcategoryandcombinetheselected performanceresults.
ThehypothesisthattheoccurrenceofelasticSearchinsomanysplits
models by ensembling them using a stacking classifier ensemble with
inflatesoverallperformancecanberefutedgiventhelowperformance
a LR base classifier. Finally, we train the resulting ensemble on the
onitsbugreports.Giventhewiderangeofclassificationperformances
trainingsetandevaluateitsperformanceonthevalidationsets.
and occurrences of projects in our splits we cannot identify a single
Results: Due to the great class imbalance found within most
cause in form of a specific software project that would overly inflate
projects,someprojectsappearmoreoftenthanothersintheseproject
ourmodel’sperformancemeasures.
splits, while some projects do not appear at all. Netty with a total of
However, besides the discussed test splits, there is another signif-
84 bug tickets does not occur in any validation split due to its high
icant difference to our RQ2 experiments regarding the training splits:
number of memory bugs that collides with our balancing and size
bugreportsfromnettyarealwaysinthetrainingsets.Itispossiblethat
criteria discussed above, and elasticSearch occurs in 78 project splits
netty bugreportsareimportanttothetrainingprocess.Unfortunately,
duetoitsbigsize(41bugreports)andbeingalmostbalanceditself.
we cannot remove netty from the training set while maintaining bal-
Fig. 8 shows the 95% percentiles and mean of macro average F1
ance,becauseofitssignificantsizeof84items(including40memory
scoresasboxplots.Again,themeanmacroaverageF1performancesof
bugs).
theensemblesarecloselyclustered,rangingfrom0.71forEnsemble1
Answer:Examinationoftheclassifiers’performanceonindividual
to0.72forEnsemble3.OnesidedT-testontheensembles’scoresshows
software projects shows a wide spread of performance scores, with
thatwecannotclaimanincreaseofperformanceduetoensemblesize
certainprojectsperformingextremelywell(e.g.,spring-framework),and
(𝑝 = 0.06 for Ensemble 2 scores to be not greater than Ensemble 1
others rather poor (e.g., n4js). However, our analysis and the small
scores, and 𝑝 = 0.09 for Ensemble 5 scores to be not greater than
availablesamplesizesdonotallowanaprioripredictionofaproject’s
Ensemble2scores).However,Ensemble1withameanmacroF1score
suitabilityforourapproach.
of 0.71 performs significantly better than the single classifier models
The mean performance on 100 validation splits shows that the
LR(0.68)andSVM(0.65).ResultsforonesidedT-testforEnsemble1
modelsareportableandrobust,andthattheperformanceresultsfrom
scoresbeinggreaterthanLRandSVMclassifiersare𝑝=1∗10−6 and
RQ2 can be maintained in a cross project application. Our smallest
𝑝=2∗10−18.
ensemble models mean macro average F1 performance from all 100
We calculate the project specific F1 score from the collective test validation splits is 0.71, our biggest models score is 0.72. Analog to
sets and their predictions from all iterations where this project oc- our results from RQ2, these ensembles perform uniformly better than
curred. Since the issues from separate projects are imbalanced w.r.t single classifier models (MNB, LR, RF, SVM, scoring 0.68, 0.65, 0.62,
the amount of bugs for each fault type, we use the weighted average 0.65meanmacroaverageF1respectively).
F1 score as metric. Again, we only consider projects with at least ten
issue tickets contained in the training set. Table 3 shows the project 7. Threatstovalidity
specificscoresfortheseprojects,theentryALLOTHERScontainsallthe
remainingprojectswithlessthan10bugreports.Thehighestweighted Thebiggestthreattovalidityisthesmallsamplesize.Toattenuate
average F1 score of 0.93 can be found for spring-framework (11 bug effectsarisingfromsuchasmalldataset,werepeatedlyranourexper-
reports);thelowestscoreof0.55occursforn4js(12bugreports).We imentsinarandomizedfashionandperformedstatisticaltestsonour
analyzed bug reports meta data, as document lengths, distribution of results.
contained fault types, occurrence of artifacts of the projects listed in Further, hidden biases may reside in bug reports from any single
Table 3, however, we did not find any correlations to the projects’ softwareproject.Tocounterthisthreat,wesourcedthisdatasetfrom
performancescores.Further,theprojects’bugticketssamplesizesare 71differentopensourceJavaprojectscoveringvariousorganizations,
toosmalltodrawanyconclusions.However,wemanuallyinvestigated softwaredomains,anddeploymenttargets,rangingfromsearchengines
the bug reports of the best (spring-framework) and the worst (n4js) and database applications to small Java libraries and mobile applica-
performing projects. We found that spring-framework bug reports are tions.Tovalidateourresultsandtodemonstratethetransferabilityof
easiertoclassifybyhumansastheycontaintheinformationrequired our approach, we performed our experiments on test/training splits
incontrasttomoreambiguousbugreportsinn4js.7 along software projects, ensuring that bug reports in the test set are
fromdifferentsoftwareprojectsthanthebugreportsusedintraining.
AthreattoexternalvalidityisthatourdatasetcontainsonlyJava
7 https://github.com/spring-projects/spring-framework/issues/24265 vs. bugs. In addition, only open source projects hosted on GitHub were
https://github.com/eclipse/n4js/issues/444. consideredinthiswork.Wecanthereforenotgeneralizeourfindings
10T.HirschandB.Hofer Array15(2022)100189
Table3
Ensemble1performanceonspecificprojectsbugsincrossprojectevaluation.
Project Bug Bugreports Splitscontain- Precision Recall F1
reports occurred ingproject (w.a.) (w.a.) (w.a.)
spring-framework 11 165 15 0.94 0.93 0.93
spring-security 20 180 9 0.86 0.85 0.85
redisson 25 1325 53 0.88 0.83 0.85
junit5 12 456 38 0.93 0.82 0.85
ExoPlayer 12 324 27 0.84 0.83 0.84
spring-session 15 680 40 0.79 0.78 0.78
hazelcast 16 448 28 0.76 0.71 0.72
async-http-client 14 448 32 0.79 0.69 0.71
ALL_OTHERS 185 1349 100 0.68 0.66 0.67
elasticsearch 41 3198 78 0.67 0.65 0.65
checkstyle 14 196 14 0.72 0.67 0.63
spring-boot 35 455 13 0.72 0.58 0.58
n4js 12 264 22 0.60 0.59 0.55
to other programming languages, issue trackers, and closed source Weinvestigatedtransferabilityofourmodelsbyapplyingthemon
software. bugreportsoriginatingfromsoftwareprojectsotherthanthesoftware
WeutilizedlabelsonGitHubissuetrackerstoselectcandidatesfor projects used in the training set. Transfer worked extremely well on
our dataset. Labeling is performed manually by the software project some projects (e.g., spring framework with a macro average F1 score
maintainers,andisthereforesubjecttomisclassifications.Wecounter of 0.93), while performing poorly on others (e.g., N4js with a macro
thisthreatbyexcludingbugreportsthatwedeemmislabeled,e.g.,fea- average F1 score of 0.55). However, mean performance scores drawn
turerequestswronglylabeledasbugs. from100ofsuchtransferexperimentswereconsistentwithourearlier
Themanualclassificationperformedbyresearcher1isalsosubject findings (e.g., mean macro average F1 score of 0.71 for our smallest
to misclassifications and therefore a threat to internal validity. To ensemble),supportthatourapproachprovidestransferablemodels.
counter this threat, a second researcher independently classified a Our results show that predicting the correct bug type solely on
blinded random sample of the dataset, and researcher 1 re-classified
an initial bug report is difficult. The reasons for this are 1. the noisy
ablindedrandomsamplesixmonthsaftertheinitialclassification.We
natureofbugreports,containingvarioustypesofnon-humanlanguage
usedtheseadditionalsamplestocalculateinter-rateragreementscores,
artifacts, 2. the extreme variety in resulting length and structures, or
toquantifythequalityofourdataset.
lackthereof,3.thedifferentviewpointsandscopesoftheirreporters,
The majority of participants for our survey are master degree stu-
e.g., an end user describing the impact of the bug in non-technical
dents in computer science. These participants are to be considered
terms, or a senior developer delegating work to other developers—
non-experts, as they are not involved in the development of the soft-
describingtheproblemintechnicaldetailincludingthebug’slocation
ware projects sourcing our datasets. Further, participants performed
andwhatactionshavetobetakentofixthebug.
classification without provision of the original project context. The
Our classification schema aims to provide a high level abstraction
resultingscoresarethereforeonthelowerendofhumanclassification
ofdebuggingapproachesandtoolsnecessaryforeffectivedebuggingof
performanceforthegiventask.
thesefaulttypes.Usingthisschema,weattempttoencapsulateasmall
portion of senior developer’s knowledge in machine learning models
8. Conclusion
through learning from historical bug reports and their corresponding
fixes.Inthiswork,weidentifytheissuesarisinginsuchendeavor,and
We have investigated the application of NLP and classical ML ap-
proposeanddemonstratepossiblesolutionsforsomeoftheseissues.
proachesontextualbugreportstopredictthebugtypeintermsoffour
While our results are promising when compared to our human
classes,Concurrency,Memory,Other,andSemanticbugs.
classifierperformancebaseline,applicationofourmodelsasdebugging
We have investigated human classifier performance on this task,
support in a production environment scarcely warranted at this stage
followed by experiments using classical ML algorithms on this multi-
andfutureresearchinthisdirectionisneeded.However,ourapproach
class classification problem. The mean classification performance of
in its current form can support researchers in their effort of creating
non-expert human classifiers was rather low, with a mean weighted
average F1 score of 0.62. The best single classifier model (Logistic bug benchmarks of specific bug types for experiments on specialized
Regression)has0.66macroaverageF1. debuggingtools.Ourdatasetsandimplementationsaremadepublicly
Our investigation into ML approaches highlighted advantages and available on GitHub.8 and Zenodo9 We hope that other researchers
disadvantages of certain NLP preprocessing steps and different ML benefitfromourdetailedinvestigationandtheprovidedartifacts.
algorithms.Toexploitthegainedinsights,weusedensemblemethods
that combine multiple classifier models and preprocessing pipelines.
CRediTauthorshipcontributionstatement
Using such ensemble methods, we achieved mean macro average F1
scoresof0.69.
Thomas Hirsch: Conceptualization, Methodology, Software, Vali-
Not all types of bugs are equally hard to predict, and our models
dation,Formalanalysis,Investigation,Resources,Datacuration,Writ-
parallel the strengths and weaknesses of human classifiers: Memory
ing – original draft, Writing – review & editing, Visualization. Birgit
bugs are the class with the highest classification performance for hu-
Hofer:Conceptualization,Validation,Writing–originaldraft,Writing
mans (0.63 mean weighted average F1 score), as well as standalone
–review&editing,Visualization,Supervision,Projectadministration,
classifiermodels(0.79meanmacroaverageF1forRandomForrestclas-
sifiers),andensemblemodels(0.77meanmacroaverageF1).Semantic Fundingacquisition.
bugsconstitutetheclasswiththelowestclassificationperformancefor
humans (0.47 mean weighted average F1 score), standalone classifier
models (0.55 mean macro average F1 for Multinomial Naive Bayes 8 https://github.com/AmadeusBugProject/fault_type_prediction.
classifiers),andourensemblemodels(0.55meanmacroaverageF1). 9 https://doi.org/10.5281/zenodo.6539173.
11T.HirschandB.Hofer Array15(2022)100189
Declarationofcompetinginterest [14] Xia X, Lo D, Wang X, Zhou B. Automatic defect categorization based on fault
triggering conditions. In: Int. conference on engineering of complex computer
The authors declare that they have no known competing finan- systems.2014,p.39–48.http://dx.doi.org/10.1109/ICECCS.2014.14.
[15] DuX,ZhouZ,YinB,XiaoG.Cross-projectbugtypepredictionbasedontransfer
cial interests or personal relationships that could have appeared to
learning. Softw Qual J 2020;28(1):39–57. http://dx.doi.org/10.1007/S11219-
influencetheworkreportedinthispaper. 019-09467-0/FIGURES/5.
[16] AhmedHA,BawanyNZ,ShamsiJA.Capbug-Aframeworkforautomaticbugcat-
Acknowledgment egorizationandprioritizationusingNLPandmachinelearningalgorithms.IEEE
Access2021;9:50496–512.http://dx.doi.org/10.1109/ACCESS.2021.3069248.
[17] Pandey N, Sanyal DK, Hudait A, Sen A. Automated classification of software
TheworkdescribedinthispaperhasbeenfundedbytheAustrian
issuereportsusingmachinelearningtechniques:anempiricalstudy.InnovSyst
ScienceFund(FWF):P32653(AutomatedDebugginginUse). SoftwEng2017;13(4):279–97.http://dx.doi.org/10.1007/s11334-017-0294-1.
[18] AlharthiZSM,RastogiR.Anefficientclassificationofsecureandnon-securebug
References reportmaterialusingmachinelearningmethodforcybersecurity.MaterToday
2021;37(Part2):2507–12.http://dx.doi.org/10.1016/J.MATPR.2020.08.311.
[19] Ortu M, Destefanis G, Swift S, Marchesi M. Measuring high and low priority
[1] Ang A, Perez A, Van Deursen A, Abreu R. Revisiting the practical use of defects on traditional and mobile open source software. In: 7th Int. workshop
automatedsoftwarefaultlocalizationtechniques.In:Int.symposiumonsoftware onemergingtrendsinsoftwaremetrics.2016,p.1–7.http://dx.doi.org/10.1145/
reliabilityengineeringworkshops.2017,p.175–82.http://dx.doi.org/10.1109/ 2897695.2897696.
ISSREW.2017.68. [20] AllamanisM,BarrET,DevanbuP,SuttonC.Asurveyofmachinelearningfor
[2] HirschT,HoferB.Whatwecanlearnfromhowprogrammersdebugtheircode. bigcodeandnaturalness.ACMComputSurv2018;51(4).http://dx.doi.org/10.
In:8thInt.workshoponsoftw.eng.researchandind.practice.2021,p.37–40. 1145/3212695.
http://dx.doi.org/10.1109/SER-IP52554.2021.00014. [21] Yang Y, Xia X, Lo D, Grundy J. A survey on deep learning for software
[3] Wong WE, Gao R, Li Y, Abreu R, Wotawa F. A survey on software fault engineering.2020,arXiv.arXiv:2011.14597.
localization. IEEE Trans Softw Eng 2016;42(8):707–40. http://dx.doi.org/10. [22] Fang F, Wu J, Li Y, Ye X, Aljedaani W, Mkaouer MW. On the classification
1109/TSE.2016.2521368. ofbugreportstoimprovebuglocalization.SoftComput2021;25(11):7307–23.
[4] GazzolaL,MicucciD,MarianiL.Automaticsoftwarerepair:Asurvey.IEEETrans http://dx.doi.org/10.1007/S00500-021-05689-2/TABLES/6.
SoftwEng2019;45(1):34–67.http://dx.doi.org/10.1109/TSE.2017.2755013. [23] Huang Q, Xia X, Lo D, Murphy GC. Automating intention mining. IEEE
[5] Hirsch T, Hofer B. Root cause prediction based on bug reports. In: 4th Int. Trans Softw Eng 2020;46(10):1098–119. http://dx.doi.org/10.1109/TSE.2018.
workshop on software faults (IWSF), ISSRE workshop proceedings. 2020, p. 2876340.
171–6.http://dx.doi.org/10.1109/ISSREW51248.2020.00067. [24] PloskiJ,RohrM,SchwenkenbergP,HasselbringW.Researchissuesinsoftware
[6] LopesF,AgneloJ,TeixeiraCA,LaranjeiroN,BernardinoJ.Automatingorthogo- faultcategorization.ACMSIGSOFTSoftwEngNotes2007;32(6):6.http://dx.doi.
naldefectclassificationusingmachinelearningalgorithms.FutureGenerComput org/10.1145/1317471.1317478.
Syst2020;102:932–47.http://dx.doi.org/10.1016/j.future.2019.09.009. [25] Endres A. An analysis of errors and their causes in system programs. In: Int.
[7] ThungF,LeX-BD,LoD.Activesemi-superviseddefectcategorization.In:23rd conference on reliable software. 1975, p. 327–36. http://dx.doi.org/10.1145/
Int.conferenceonprogramcomprehension.2015,p.60–70.http://dx.doi.org/ 800027.808455.
10.1109/ICPC.2015.15. [26] GrayJ.Whydocomputersstopandwhatcanbedoneaboutit?Technicalreport
[8] Tan L, Liu C, Li Z, Wang X, Zhou Y, Zhai C. Bug characteristics in open 85.7,1985,URLhttp://www.hpl.hp.com/techreports/tandem/TR-85.7.pdf.
source software. Empir Softw Eng 2014;19(6):1665–705. http://dx.doi.org/10. [27] GrottkeM,TrivediK.Aclassificationofsoftwarefaults.In:16thInt.symp.on
1007/s10664-013-9258-8. softwarereliabilityengineering(ISSRE’05)-sup.proceedings.2005,p.19–20.
[9] Li Z, Tan L, Wang X, Lu S, Zhou Y, Zhai C. Have things changed now?: [28] Chillarege R, Bhand ari IS, Chaar JK, Halliday MJ, Ray BK, Moebus DS.
An empirical study of bug characteristics in modern open source software. Orthogonal defect classification: A concept for in-process measurements. IEEE
In: 1st Workshop on architectural and system support for improving software TransSoftwEng1992;18(11):943–56.http://dx.doi.org/10.1109/32.177364.
dependability.2006,p.25–33.http://dx.doi.org/10.1145/1181309.1181314. [29] IEEE.Std1044-2009-IEEEstandardclassificationforsoftwareanomalies.2010.
[10] Ray B, Posnett D, Filkov V, Devanbu P. A large scale study of programming [30] Baeza-YatesBerthierRibeiro-NetoR.Moderninformationretrievaltheconcepts
languages and code quality in GitHub. In: ACM SIGSOFT symposium on the andtechnologybehindsearch.seconded..AddisonWesley;2011,p.68–74.
foundations of software engineering. 2014, p. 155–65. http://dx.doi.org/10. [31] Scikit-learn documentation. URL https://scikit-learn.org/stable/modules/
1145/2635868.2635922. generated/sklearn.feature{_}extraction.text.TfidfTransformer.html.
[11] NiZ,LiB,SunX,ChenT,TangB,ShiX.Analyzingbugfixforautomaticbug [32] CalefatoF,LanubileF,VasilescuB.Alarge-scale,in-depthanalysisofdevelopers’
causeclassification.JSystSoftw2020;163:110538.http://dx.doi.org/10.1016/ personalities in the apache ecosystem. Inf Softw Technol 2019;114:1–20. http:
j.jss.2020.110538. //dx.doi.org/10.1016/J.INFSOF.2019.05.012.
[12] Goseva-PopstojanovaK,TyoJ.Identificationofsecurityrelatedbugreportsvia [33] Hirsch T, Hofer B. Identifying non-natural language artifacts in bug reports.
text mining using supervised and unsupervised classification. In: Int. conf. on In: 2021 36th IEEE/ACM International Conference on Automated Software
softwarequality,reliabilityandsecurity.2018,p.344–55.http://dx.doi.org/10. Engineering Workshops (ASEW). 2021, p. 191–7. http://dx.doi.org/10.1109/
1109/QRS.2018.00047. ASEW52652.2021.00046.
[13] WuX,ZhengW,ChenX,ZhaoY,YuT,MuD.Improvinghigh-impactbugreport [34] Cohen J. A coefficient of agreement for nominal scales. Educ Psychol Meas
predictionwithcombinationofinteractivemachinelearningandactivelearning. 1960;20(1):37–46.http://dx.doi.org/10.1177/001316446002000104.
InfSoftwTechnol2021;133:106530.http://dx.doi.org/10.1016/J.INFSOF.2021. [35] Landis JR, Koch GG. The measurement of observer agreement for categorical
106530. data.Biometrics1977;33(1):159.http://dx.doi.org/10.2307/2529310.
[36] Thung F, Lo D, Jiang L. Automatic defect categorization. In: Work. conf. on
reverseengineering.2012,p.205–14.http://dx.doi.org/10.1109/WCRE.2012.30.
12