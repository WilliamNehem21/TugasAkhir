Artificial Intelligence in the Life Sciences 3 (2023) 100064
Contents lists available at ScienceDirect
Artificial Intelligence in the Life Sciences
journal homepage: www.elsevier.com/locate/ailsci
Perspective
Exploring chemical space — Generative models and their evaluation
Martin Vogt
Department of Life Science Informatics, b-it, LIMES Program Unit Chemical Biology and Medicinal Chemistry, Rheinische Friedrich Wilhelms-Universität,
Friedrich-Hirzebruch-Allee, 5-6, Bonn 53115, Federal Republic of Germany
a r t i c l e i n f o a b s t r a c t
Keywords: Recent advances in the field of artificial intelligence, specifically regarding deep learning methods, have invig-
Artificial intelligence orated research into novel ways for the exploration of chemical space. Compared to more traditional methods
Chemical space that rely on chemical fragments and combinatorial recombination deep generative models generate molecules
Chemical space exploration
in a non-transparent way that defies easy rationalization. However, this opaque nature also promises to explore
Deep neural networks
uncharted chemical space in novel ways that do not rely on structural similarity directly. These aspects and the
Generative models
Inverse QSAR/QSPR
complexity of training such models makes model assessment regarding novelty, uniqueness, and distribution of
generated molecules a central aspect. This perspective gives an overview of current methodologies for chemical
space exploration with an emphasis on deep neural network approaches. Key aspects of generative models in-
clude choice of molecular representation, the targeted chemical space, and the methodology for assessing and
validating chemical space coverage.
1. Introduction be replaced by exploratory efforts that will only be able to sample a tiny
fraction of CS under consideration.
The chemical space (CS) of small molecules refers to the universe The creation of arbitrary chemical structures exploring CS is, in prin-
of all conceivable chemically stable small molecules [1] . Combinatorial ciple at least, “easy ”as any random chemical graph structure that fol-
estimates put its size in excess of 10 60 molecules [2] . This estimate can lows basic valence rules can be considered a legal molecule. As such,
easily vary by arbitrary orders of magnitude depending on the definition a simple brute-force approach to sampling CS can be envisioned that
of what constitutes a “small ”molecule. However, the main significance (1) generates random mathematical graphs (of limited size) (2) labels
of this number is that it is so large that (a) only an almost infinitesimal vertices and edges randomly with atom types and bond orders, and (3)
fraction of CS can ever be realized, synthesized, and explored in vitro or filters the resulting chemical graph structures to remove chemically non-
in vivo and likewise (b) only a “slightly ”larger infinitesimal fraction can sensical structures. Alternatively, and more closely reflecting the ap-
be explored in silico regardless of the computational resources available proach taken by many current deep learning approaches, the random
now or in the future. Generative models (GMs) aim to explore CS not graph generation process can be substituted by the generation of ran-
by exhaustive enumeration but by randomly sampling and exploring dom strings to be interpreted as linear representations of molecules like
its properties. “Generative ”refers to the central aspect of such models SMILES [6] . Such a simplistic approach has its analogy in the figura-
aiming to learn probability distributions from training data, which are tive monkey randomly hitting letters on the keyboard of a typewriter
then used to guide the generation of novel molecules. as popularized by the infinite monkey theorem [7] . This theorem, when
Small molecule CS can be described algorithmically by using com- applied to CS guarantees that it is in principle possible to exhaustively
binatorial approaches that systematically explore chemical structural explore CS this way because any molecule has a certain (although min-
graphs of increasing size. While exhaustive enumeration in this way is imal) probability of being generated. At the same time, the process out-
infeasible in general, it has been realized for molecules of limited size lined above is not practical because the probability of generating valid
(up to 11, 13, and 17 heavy atoms) and limited atom types (C, N, O, molecular representations is extremely low. However, if the random
S, and halogens) yielding CSs of 26.4 million, 977 million, and 166 bil- generation process can be directed to mainly generate valid molecular
lion molecules, respectively [3–5] . The numbers resulting from these representations, and probabilities can be adjusted to focus on reason-
impressive efforts demonstrate their exponential growth and the ulti- able chemical structures, the outlined workflow is not only feasible, but
mate futility of extending this approach to even just moderately larger is also the principle on which most current GMs rely.
molecules. Thus, beyond a certain size such exhaustive efforts have to While the ability to “randomly generate valid chemical structures ”
is a prerequisite for a successful GM it is not a meaningful criterion to
E-mail address: martin.vogt@bit.uni-bonn.de
https://doi.org/10.1016/j.ailsci.2023.100064
Received 30 December 2022; Received in revised form 1 February 2023; Accepted 2 February 2023
Available online 4 February 2023
2667-3185/©2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ )M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
judge its quality per se . For instance, for generic CS exploration it would between more traditional fragment-based methods and deep learning
be preferable that every molecule of the relevant CS has roughly the methods can be observed. In fragment-based methods, fragments repre-
same chance of being generated or the bias towards certain molecules is sent chemically sound substructures and recombinations and modifica-
at least reasonable (e.g., larger molecules are less likely to be generated tions are chemically meaningful. In this case, the molecular represen-
than smaller ones). On the other hand, often CS is explored with certain tation itself (SMILES-based or graph-based) plays a minor role because
goals in mind. Just as in vitro and in vivo exploration of CS in lead opti- the ways these representations are manipulated are based on the un-
mization campaigns aims to optimize endpoints like potency or ADMET derlying chemistry of the molecules. Deep learning models are agnostic
properties [8] , computational CS exploration can be conducted with re- of the semantics of the chosen representation and manipulation occurs
spect to certain goals focusing on molecules with favorable properties on a purely syntactical level. A central step in the training of DNNs
for endpoints of interest [9] . is to learn the grammar and rules of the representation so that valid
Lead optimization campaigns usually focus on exploring CS around representations can be generated. Furthermore, typical patterns in rep-
one or a few scaffolds. However, computational approaches are not lim- resentations can be learned, which form syntactical “building blocks ”
ited in this way and can emphasize exploration based on relevant physic- of newly generated molecules and thus characterize the generated CS.
ochemical and biological properties. However, if a DNN only learns the syntactical rules of a representation,
Thus, to assess the usefulness of GMs the following aspects need to generation of molecules will be dominated by the similarity of the repre-
be considered: sentation, which only imperfectly correlates with molecular similarity.
A recent study has shown indeed, that while deep GMs can successfully
• Validity of generated representations.
learn the grammar of a representation from relatively small training sets,
• Novelty of generated molecules.
larger sets might be required to more properly reflect physicochemical
• Synthetic accessibility and drug-likeness of generated molecules.
properties of the reference training set [22] .
• Extent to which the covered CS is representative and relevant for the
These considerations are most clearly reflected in models based on
intended purpose.
linear character-based representations. Although several deep GMs uti-
Some of these aspects like validity and novelty can be easily quan- lizing molecular graphs directly for input and output have been devel-
tified using statistical metrics. Furthermore, computational metrics like oped in recent years [23–26] , language-based representations form the
the synthetic accessibility score [10] and the quantitative estimate of basis of most popular models. This is unsurprising as the rapid develop-
drug-likeness [11] can give indications how reasonable and practical ment of DNN architectures in recent years has been especially successful
proposed structures are. The degree to which a CS is representative can in signal processing and natural language processing [27] .
be assessed by determining distributions of structural and physicochem- The SMILES notation [6] is the most popular linear notation for
ical quantities, while the biological relevance of a CS is characterized by chemical structures. It possesses a relatively simple grammar where
the extent to which it is enriched with molecules possessing favorable parenthesized expressions are used to represent acyclic compounds. Ad-
properties like a target-specific activity. For GMs targeting a focused CS, ditionally, matching numbers associated with pairs of atoms indicate
e.g., CS containing molecules with specific biological activities or syn- bonds completing rings (see Fig. 1 ). It is the representation of choice for
thetically accessible CS, traditional benchmark approaches are not ap- most generative DNN models to date. In one of the early publications
plicable as the CS to be covered by GMs can be extremely large. Thus, using generative DNNs, Gómez-Bombarelli et al. [15] showed that DNNs
it should not be expected that a GM is able to recreate molecules from a can accurately learn the grammar of SMILES and can generate a high
hold-out test set. On the contrary, recreation of known molecules might percentage of valid SMILES. However, training of the model essentially
be an indication that the CS covered is relatively small and overpopu- failed, when the grammatically much more complex InChI line notation
lated with molecules from already explored CS. [28] was used.
GMs that predate the deep learning revolution of the last years A single molecule can have many different SMILES notations. The no-
mainly used fragment-based approaches and by recombining fragments tation is determined by the starting atom corresponding to the first token
ensured that the randomly generated structures (after filtering) repre- of the SMILES string and the order in which the atoms of the molecules
sent reasonable molecules [12–14] . By design, the CS covered by such are traversed in a depth-first traversal. Thus, the same molecule can
GMs is clearly defined (and, at the same time, limited) by the fragment have widely varying SMILES representations (see Fig. 1 ). While canoni-
libraries used. This might guarantee that the CS covered is representa- calization algorithms, which assign priorities to the atoms of a molecule
tive regarding structural composition, for instance if a fragment library based on the molecular topology, can be used to obtain a unique rep-
was derived from a reference set of relevant molecules, but this does not resentation for a molecule it is still unavoidable that some structurally
(necessarily) extend to biological properties. similar molecules have significantly different canonical SMILES repre-
One of the expected advantages of CS exploration using deep neural sentations. Generally, it can be stated that similar SMILES represent sim-
networks (DNNs) in so-called deep GMs is their potential for abstracting ilar molecules while the converse might only be true to a limited extent.
from a given structural context and generate and explore CS not based This issue can be addressed during training of the models by augment-
on structural similarity alone but instead based on multiple parameters ing the training set by randomizing or enumerating all possible SMILES
that are relevant for endpoints to be optimized [ 15 , 16 ]. In essence, this representations of a molecule. A number of studies have shown that this
is also the central question of the inverse QSAR/QSPR problem [17– has a positive effect on the performance [ 22 , 29 , 30 ].
20] and is one of the drawing powers of applying advanced artificial in- Given the reasonable assumption that the more complex the repre-
telligence (AI) methods to the exploration of CS. Architectures of DNNs sentation, the harder it will be for a deep GM to generate valid rep-
do not incorporate any chemical knowledge, all of which has to be ac- resentation, variations and alternatives to the SMILES notation have
quired during training and is encoded in the learnable parameters of the been developed aimed at making it easier to generate valid representa-
network. However, this aspect also makes it harder to assess whether a tions. The DeepSMILES notation [31] is a grammatical simplification of
trained DNN fulfills these expectations. This perspective extends a pre- SMILES that does not require matching parentheses or correctly paired
vious review on CS exploration methodologies [21] by additionally fo- numbers for cyclic bonds (see Fig. 1 ). While this notation has not been
cusing on the problem of assessing and validating CS covered by GMs. widely adopted, studies have shown that the use of DeepSMILES can
make it easier for a GM to generate valid representations although im-
2. Molecular graph representations for generative models provements were not considered major [22] .
As an alternative linear representation, SELFIES [32] were designed
The representation of molecules and their processing in GMs play an with the aim that any combination of symbols can be interpreted as a
important role in how CS is covered. In this regard, a crucial distinction valid chemical structure fulfilling basic topological and valence rules
2M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
Fig. 1. Molecular representations. For thalidomide, several linear representations are shown. Canonical and three randomized SMILES were generated using RDKit
( https://www.rdkit.org ). DeepSMILES and SELFIES were determined on the basis of the SMILES representations. Due to their length only one randomized SELFIES
is shown.
(see Fig. 1 ). This ensures that any GM generating SELFIES, will, by de- of a context-free grammar from which SMILES [6] representations of
sign only produce valid molecules. In SELFIES, symbols have to be in- molecules are generated, and STONED [45] uses random mutations of
terpreted in a context-dependent manner, e.g., the same symbol can in- non-redundant SEFLIES [32] for goal-directed CS exploration.
dicate a specific element, the size of a sidechain, or the size of a ring.
The complex semantics of SELFIES has the effect that small changes in
3.2. Deep neural networks
the syntax can lead to structurally highly dissimilar molecules. Thus,
while a model based on SELFIES would generate only valid represen-
By design, fragment-based methods (and the grammar-based
tations it might be much harder to learn structural properties from a
ChemGE method) incorporate basic chemical knowledge so that the
reference CS with the goal of expanding it. This observation has been
generated output consists of correct chemical structures following ba-
confirmed showing that indeed SELFIES-based GMs will generate 100%
sic valence rules and is directed toward chemically reasonable CS. In
valid molecular representations, however the CS covered tends to differ
DNNs however, these aspects need to be learned during training. First,
more strongly from a reference training set regarding metrics character-
a model needs to learn to generate syntactically valid molecular repre-
izing structural and physicochemical properties [22] .
sentations, and, second, the molecules generated should properly reflect
the reference CS on which the model was trained. These goals are not
3. Architectures of generative models
strictly separable during training. Initial training of DNNs requires rel-
atively large data sets. Retraining techniques like transfer learning or
3.1. Conventional approaches based on probabilistic methods
reinforcement learning can be used to direct the CS of a DNN towards
a specific region of interest, e.g., molecules showing favorable proper-
Approaches from machine learning and utilization of optimization
ties or specific bioactivities, for which only small amounts of data are
algorithms have been incorporated in GMs for CS exploration that pre-
available.
date the rise of deep learning in chemoinformatics [12] . Most common
GMs for CS exploration have mainly focused on three types of archi-
among these are fragment-based methods relying on fragment recom-
tectures: recurrent neural networks (RNNs), variational autoencoders
binations [33–38] that can be either explored combinatorially or using
(VAEs), and generative adversarial neural networks (GANs).
single or multi-parameter optimization strategies [ 39 , 40 ]. These meth-
ods typically employ probabilistic methods with the aim to generate
chemically reasonable molecules and optimize these with respect to one 3.2.1. Recurrent neural networks
or more endpoints. RNNs have their origin in signal and natural language processing
Popular probabilistic methods used in fragment-based GMs include [27] and can thus be very well adapted for exploring CS using linear
genetic or evolutionary optimization algorithms (GAs) [38–43] , ant representations like SMILES [ 46 –51 ]. RNNs process an input sequence
colony optimization [44] , Markov chains [35] , or Monte-Carlo tree symbol by symbol, where its behavior at any given point t depends on
search [43] . By design, all these methods are well suited for focused or the current input symbol s
t
and an internal state at t − 1, which in turn
goal directed CS exploration campaigns because their iterative proba- depends on the initial part of the sequence up to t − 1 (see Fig. 2 a). Mem-
bilistic nature can guide exploration towards CS with favorable prop- ory units like long short-term memory (LSTM) [52] or gated recurrent
erties as part of a single or multi-objective optimization strategy. units (GRU) [53] can selectively keep track of and memorize parts of the
Fragment-based de novo design methods are not limited to exploration of input. RNNs resemble Markov chains in that they learn conditional prob-
local CS and can also be adopted for generic sampling of CS [38] putting abilities for each symbol depending on the history of previous symbols.
the emphasis on exploring reasonable synthetically accessible CS where However, whereas the distributions of Markov chains are only depen-
the distribution of generated molecules mimics that of a reference set to dent on the immediately preceding symbol, RNNs can encode long-term
which the models are adapted. dependencies and are thus suited to learn complex grammars, which
Probabilistic methods are not limited to fragment-based approaches. make them predestined for natural language processing and the genera-
E.g., ChemGE [42] uses a pool of chromosomes representing rules tion of SMILES by encoding grammar rules like correctly parenthesized
3M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
Fig. 2. DNN architectures for generative models. Four different
architectures for deep generative models are displayed schemati-
cally. (a) Recurrent neural network, (b) variational autoencoder,
(c) generative adversarial network, (d) adversarial autoencoder.
expressions or matching ring closure symbols in their probability distri- two DNNs (See Fig. 2 b), one to encode the representation as a latent vec-
butions. tor and one to decode the latent vector, recreating the original represen-
Trained RNNs can generate molecular representations by sampling tation (or an alternative representation for the same molecule [29] ). The
sequences of symbols s
1
,...,s
t −1
,s
t
,... where the probability of each sym- latent space is a relatively low-dimensional mostly non-redundant vec-
bol is determined by the learned distributions. Several molecular gen- tor space. Because the projection mapping of molecules into this space
erative networks have been suggested based on this principle [46–51] . has to be learned and is not predefined by the architecture it is possible
Frequently, however, RNNs are used as modules in other DNN architec- to train autoencoders in such a way that neighborhoods in latent space
tures. E.g., the learned hidden states of RNNs can be used as the basis reflect similarities of properties or biological activities of interest. Thus,
for encoding molecules in a so-called latent space as part of variational sampling from neighborhoods of molecules in latent space can be used
autoencoders (VAEs) [ 15 , 29 ]. They have also been used as generators to sample novel molecules with similar properties thus expanding the
in GANs [54] or autoencoder-based GANs [55–57] instead of more con- local CS [ 15 , 16 , 29 , 58 , 59 ].
ventional convolutional neural networks. While basic autoencoders might be able to recreate inputs without er-
ror they are not sufficient to guarantee that latent space representations
have meaningful similarity relationships and thus are not well suited
3.2.2. Variational autoencoders
for generative purposes. VAEs can be seen as Bayesian equivalents of
The idea of autoencoders is to take a molecular representation and
autoencoders and instead of learning a fixed representation, VAEs learn
map it onto a latent continuous vector space in such a way that the origi-
a multivariate Gaussian distribution in latent space. This helps to en-
nal molecule can be reconstructed. To this end, an autoencoder requires
4M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
sure continuity, so that neighborhoods in latent space reflect molecular 4. Validation of generative models
similarity [15] .
4.1. Validation metrics
3.2.3. Generative adversarial networks
A GAN consists of two components, a generator and a discriminator
The nature of DNNs precludes easy interpretation, which makes it
[27] . Based on a trainable probability distribution, the generator will
hard to rationalize how CS is covered by deep GMs. Generative DNN
generate molecular representations while the discriminator will classify
models like VAEs, AAEs, or GANs sample from a continuous latent
a representation as either “real ”or “fake ”aiming to distinguish “real ”
space. They are trained with the goal that neighborhoods in latent space
molecular representations from a reference training set from “fake ”ones
represent meaningful neighborhoods in generated CS characterized by
produced by the generator (see Fig. 2 c). The two networks have op-
continuity with respect to structural changes and/or with respect to
posite goals, hence the name “adversarial ”; while the discriminator’s
physicochemical or biological properties. Contrast this with fragment-
task is to accurately distinguish real from generated samples, the gen-
based methods using randomization to compose, combine, and recom-
erator’s goal is to make this task as hard as possible for the discrimi-
bine fragments. By design, such methods can, for instance, mimic the
nator. The generator and discriminator network are trained simultane-
frequency of fragments or functional groups from a reference set and
ously, and learning is achieved by the competitive nature of the pro-
ensure that generated molecules share similar structural features. The
cess. While different GAN architectures have been proposed for CS ex-
obvious limitation here is that fragment-based models might be too con-
ploration [ 54 , 60 ], for instance using RNNs as generators [54] , they are
servative in their approach and explore CS based on structural similarity
also often combined with autoencoders in what are known as adversarial
alone. While deep GMs are not restricted in this way, they have to ac-
autoencoders (AAEs). Here, a generator samples from the latent space
quire all their chemical knowledge by training, which is an additional
of the autoencoder while a discriminator distinguishes real from fake
challenge in generating CS that reflects structural and physicochemical
latent vectors. Independent of the discriminator, the decoder is used to
properties of a reference set. This aspect, combined with their opaque
generate the molecular representation from the latent vector [ 55–57 , 61 ]
nature makes validation and assessment of generated CS a central aspect
(see Fig. 2 d).
of building successful models. Several recent publications have focused
on assessing and benchmarking the CSs generated by fragment-based
and deep GMs [ 22 , 67 , 68 ].
3.3. Training and goal-directed chemical space exploration for deep
Basic aspects of the quality of GMs include validity, novelty, and
generative models
uniqueness of generated molecules. These quantities can be easily cal-
culated from a reference set and a sample of generated molecules. The
Although it has been recently shown that it is possible to successfully
validity criterion assesses to what extent a GM generates valid repre-
train DNNs on relatively small training sets of around 10,000 molecules
sentations of chemically sound structures. In some sense, validity by
[22] , i.e., such models are able to generate valid molecular represen-
itself can be considered the least crucial of these aspects. The validity of
tations at an acceptable rate, these models might not be good enough
a molecule can be easily checked with respect to syntactic correctness
to accurately reflect the CS of reference sets. However, the focused ex-
of the chosen representation and the adherence to basic chemical rules
ploration of CS and the optimization of sampled molecules regarding
regarding atomic valencies and bond orders. Thus, as long as a GM pro-
specific (biological) properties like activity is a central task in chemoin-
duces a significant fraction of valid molecules, invalid representations
formatics. Typically, the data available for these tasks is often at best
can be easily filtered out, which could still result in a model that reason-
one order of magnitude less than required to train a deep GM. For ex-
ably samples CS albeit in a less efficient manner. Nevertheless, validity
ample, even for the most prominent targets, only a few thousand active
is an important indicator of successful training, as it shows that a model
molecules are reported in databases like ChEMBL [62] . Two popular ap-
can learn the syntax and grammar of a representation, which can be con-
proaches to address this issue are transfer learning and reinforcement
sidered a prerequisite for learning the chemistry and properties of the
learning [27] . They rely on first training a model on a large generic data
CS to be represented by the model. Validity has been demonstrated to
set covering a more general CS, e.g., bioactive molecules, and then tun-
correlate well with improved performance while training on increasing
ing the model towards a region of interest using a much smaller training
data set sizes [22] .
set relevant for the task at hand. In the case of transfer learning this is
Novelty characterizes the property of a GM to generate molecules
done by retraining a pre-trained model using a focused reference data
unseen in the training set while uniqueness reflects its ability to avoid
set of interest [ 46 , 47 , 50 , 51 ], while in reinforcement learning, the prob-
recreating the same molecule multiple times. These quantities charac-
abilistic distribution underlying the generative process is tuned towards
terize the size of the CS covered by a GM and can be close to 100%
a target CS [ 49 , 55 , 58 ]. This approach is well suited for GANs [ 54 , 60 , 63 ]
if the covered CS is multiple orders of magnitude larger than either
and AAEs [ 55–57 , 61 ] but has also been applied to RNNs [ 64 , 65 ].
the size of the training set or the number of molecules sampled from
Considering that neighborhood relationships in latent space repre-
the GM. To put this in perspective, randomly sampling from a (the-
sentations can be modified during training, they can also be tuned to
oretical) space of N molecules, one ca√n expect to start seeing dupli-
reflect similarities of chemical properties of interest. Gómez-Bombarelli
cate samples for samples sizes of ca. 𝑁 or more molecules, while
et al. [15] and Colby et al. [16] modified the loss function of the VAE to
in a sample of N molecules one can expect to see ca. 37% duplicates.
include regression errors with respect to molecular properties in order
The training set size usually represents a tiny fraction of the targeted
to control latent space representations and allow optimization of such
CS so that ideally the fraction of novel molecules exceeds 99%. Low
properties in latent space. In AAEs, the distribution of molecules in la-
novelty can be a sign of overfitting, and low uniqueness can indicate
tent representation is controlled directly by the discriminator, which has
a mode collapse. Mode collapse describes a lack of diversity of gener-
been shown to improve the continuity of latent space representations
ated molecules and can be ascribed to a concentration of the probability
[66] . Polykovskiy et al. [56] and Hong et al. [61] combined molecular
distribution around a few molecules. While a GM should aim to maxi-
representations with molecular properties resulting in latent space rep-
mize these metrics, its potential to do so might be limited if the size of
resentations that allow conditional generation of molecules with desired
the targeted CS is small; for instance when exploring CS around specific
properties. Here, latent space of molecular representation is trained to
scaffolds [69] .
be independent of specific molecular properties while the decoder is
Zhang and co-workers [70] trained multiple GMs on a small subset
augmented with a property vector to generate molecules with desired
of GDB-13 [4] comprising roughly 0.1% of a completely enumerated CS
properties.
of ca. one billion molecules with sizes of up to 13 heavy atoms. The
5M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
GMs were used to sample one billion compounds each and the best re- 4.2. Model interpretation
sulting models achieved a coverage of 39%. Following the argument
above, a coverage of about 63% would be the theoretically best perfor- Beyond the characterization of generated CS using the discussed met-
mance that could be expected assuming perfect coverage and identical rics the ability to rationalize the CS based on an understanding of the GM
probabilities for each molecule. Given that the GMs also generated a is an important aspect of assessing its quality. Given that the black-box
significant portion of molecules not covered by GDB-13 this indicated character of DNNs makes it hard to explain outcomes, model interpre-
that the GMs are indeed able to cover a significant part of the whole tation has not been a central aspect of deep GM research in chemoinfor-
GDB-13. matics and few publications have considered it.
While validity, novelty, and uniqueness can confirm that a GM will Analyzing latent space representations can yield insights into how
indeed explore novel CS, these metrics are not sufficient to characterize similarity is perceived by a deep GM. For instance, in Ref. [55] the rela-
it regarding relevant biological and physicochemical properties. Beyond tionship between neighborhoods in latent space and structural similar-
purely random sampling of CS, the envisioned utility of GMs lies in their ity of exemplary compounds was investigated. A more recent approach
ability to generate novel compounds having properties that are charac- used generative topographic mapping to map the latent space of an au-
terized by a reference set used for training either directly or as part of re- toencoder onto two dimensions [75] , which the authors then used to
training to focus the covered CS. The ability of GMs to adequately cover construct an activity map of adenosine A2a receptors visualizing how
the targeted CS can be assessed by comparing the property distributions these active compounds were distributed in latent space.
of the training set molecules and the sampled molecules. For practical In Ref. [76] , input saliency maps of SMILES were used to aid the
applications this approach is limited to properties that can be easily com- interpretability of the generative process. Input saliency maps assign a
puted. These include simple descriptors like molecular weight or atom score to each token indicating its relevance for the next token to be
composition, topological descriptors like the Bertz topological complex- generated. Mapping the tokens to molecular structures then makes it
ity [71] , or physicochemical properties like the octanol-water partition possible to interpret them in a chemical context.
coefficient logP(O/W) for which computational models exist [ 22 , 67 , 68 ]
but also extends to more complex descriptors like the synthetic acces- 5. Conclusions
sibility score [10] and the quantitative estimate of drug-likeness [11] .
The similarity between reference and sample distributions is quantified Although conventional fragment-based approaches have been avail-
using metrics like the Kullback–Leibler divergence, the Jensen–Shannon able for CS exploration for a long time the deep learning revolution
distance or the Wasserstein distance [22] . Structural similarity can be of the last years has invigorated research in GMs using DNNs. Lin-
assessed based on scaffold and fragment distributions [68] and on Tan- ear molecular representations have facilitated the adoption of many
imoto similarities of structural fingerprints [ 22 , 67 , 68 ] and can also be DNN architectures developed for signal and natural language process-
used to detect issues like mode collapse or strong biases in the trained ing. Training and optimizing the architectures of DNNs is still a very
models. time-consuming and demanding task. However, the increasing afford-
While these distribution-based descriptors can be used to determine ability of GPU-clusters and/or the availability of pre-trained models
how representative a CS is with respect to structure and physicochem- make their usage more attractive and widely available. The SMILES no-
ical properties, they do not address biological properties. The Fréchet tation remains the most popular linear representation for DNNs. While
ChemNet Distance (FCD) [72] has been developed with this in mind. alternative notations like DeepSMILES or SELFIES have been developed
The concept was inspired by the Fréchet inception distance [73] . How- with the purpose of simplifying the training of DNNs they have not been
ever, instead of relying on the Inception v3 [74] network for image clas- adopted widely and have yet to show a significant advantage. The at-
sification, it relies on a DNN, termed ChemNet, that has been trained to tractiveness of DNNs for GMs lie for one in their abstraction from a
predict bioactivities of molecules for more than 6000 assays. The acti- strictly structural context and their flexibility, for instance by augment-
vations of the neurons of the final hidden layer of the network encode ing representations with additional properties [ 69 , 77 ] or by tuning their
the relevant features for the predictions of the output layer. For a set of behaviour using transfer or reinforcement learning. The idea that a DNN
molecules the distribution of these activations are modeled as a multi- can project molecules onto a latent space, explicitly or implicitly, where
variate Gaussian distribution. The distributions of a reference set of real neighborhood relationships are characterized by similarity of biologi-
molecules and a sample of generated molecules are compared using the cal properties and activities is a major advantage over fragment-based
Fréchet distance. While the quality of this distance will be limited by the methods and has the potential to lead to the discovery of more diverse
accuracy of ChemNet and the quality and generality of the assay data, structures that nevertheless maintain required properties and bioactiv-
the concept is very elegant for the assessment of the biological relevance ities. It is very hard to assess to what extent current GMs are already
of a CS. able to perceive CS this way. A few prospective applications of deep
Under the assumption that larger training data sets should be able GMs have been reported [ 58 , 78–80 ]. However, whether these models
to improve the performance of GMs, Skinnider et al. [22] studied the still mostly rely on structural similarity or more advanced parameteri-
correlation between the metrics discussed here and training set size. zation of CS is hard to rationalize. Current benchmark systems have only
The found a very high correlation between validity and training set size very limited capabilities in this regard. Their metrics are restricted to de-
(using SMILES). FCD also showed high correlation, while most struc- scriptors immediately derivable from the structural representation. FCD
tural and physicochemical descriptors showed good to satisfactory cor- represents a promising strategy to address this issue by utilizing a latent
relations. While smaller training sets were sufficient to generate valid representation aimed at classifying biological activities. - Ultimately, a
representations at a reasonable rate, other metrics improved signifi- better understanding of deep GMs is highly desirable. Visualizations of
cantly with larger training sets. This indicated that it was easier for GMs latent space and investigating local neighborhoods are first steps in the
to learn the syntax of the representations, but more information was understanding of their structure and input saliency maps can pinpoint
required to accurately reflect the CS of the reference sets. Somewhat key dependencies in the generation of linear representations that can be
counter-intuitively, training set sizes were only slightly correlated with interpreted on a structural level.
uniqueness and negatively correlated with novelty. A possible explana-
tion might be that a GM that only learned the grammar and is not (yet) Declaration of Competing Interest
restricted by the properties of the reference space might be less likely
to reproduce training molecules while generating unique molecules at a The authors declare that they have no known competing financial
similar level. interests or personal relationships that could have appeared to influence
the work reported in this paper.
6M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
Data availability for the de novo generation of molecules occupying druglike chemical space. J Chem
Inf Model 2009;49:1630–42 .
No data was used for the research described in the article.
[36] White D, Wilson RC. Generative models for chemical structures. J Chem Inf Model
2010;50:1257–74 .
[37] Rodrigues T, Hauser N, Reker D, et al. Multidimensional de novo design reveals
5-HT2breceptor-selective ligands. Angew Chem Int Ed 2014;54:1551–5 .
References [38] Polishchuk P. CReM: chemically reasonable mutations framework for structure gen-
eration. J Cheminf 2020;12:28 .
[1] Kirkpatrick P, Ellis C. Chemical space. Nature 2004;432:823 823 . [39] Brown N, McKay B, Gasteiger J. A novel workflow for the inverse QSPR problem
[2] Bohacek RS, McMartin C, Guida WC. The art and practice of structure-based drug using multiobjective optimization. J Comput Aided Mol Des 2006;20:333–41 .
design: a molecular modeling perspective. Med Res Rev 1996;16:3–50 . [40] Nicolaou CA, Apostolakis J, Pattichis CS. De novo drug design using multiobjective
[3] Fink T, Bruggesser H, Reymond JL. Virtual exploration of the small-molecule chem- evolutionary graphs. J Chem Inf Model 2009;49:295–307 .
ical universe below 160 Daltons. Angew Chem Int Ed 2005;44:1504–8 . [41] Brown N, McKay B, Gilardoni F, et al. A graph-based genetic algorithm and its ap-
[4] Blum LC, Reymond JL. 970 million druglike small molecules for virtual screening in plication to the multiobjective evolution of median molecules. J Chem Inf Comput
the chemical universe database GDB-13. J Am Chem Soc 2009;131:8732–3 . Sci 2004;44:1079–87 .
[5] Ruddigkeit L, van Deursen R, Blum LC, et al. Enumeration of 166 billion organic [42] Yoshikawa N, Terayama K, Sumita M, et al. Population-based de novo molecule
small molecules in the chemical universe database GDB-17. J Chem Inf Model generation, using grammatical evolution. Chem Lett 2018;47:1431–4 .
2012;52:2864–75 . [43] Jensen JH. A graph-based genetic algorithm and generative model/Monte Carlo tree
[6] Weininger D. SMILES, a chemical language and information system. 1. Introduction search for the exploration of chemical space. Chem Sci 2019;10:3567–72 .
to methodology and encoding rules. J Chem Inf Model 1988;28:31–6 . [44] Reutlinger M, Rodrigues T, Schneider P, et al. Multi-objective molecular de novo
[7] Borel E. La mécanique statique et l’irréversibilité. J de Physique Théorique et Ap- design by adaptive fragment prioritization. Angew Chem Int Ed 2014;53:4244–8 .
pliquée 1913;3:189–96 . [45] Nigam A, Pollice R, Krenn M, et al. Beyond generative models: superfast traversal,
[8] Wermuth CG, Aldous D, Raboisson P, et al. The practice of medicinal chemistry. optimization, novelty, exploration and discovery (STONED) algorithm for molecules
Academic Press; 2015 . using SELFIES. ChemRxiv 2021. doi: 10.26434/chemrxiv.13383266.v2 .
[9] Vogt M. How do we optimize chemical space navigation? Expert Opin Drug Discov [46] Segler MHS, Kogej T, Tyrchan C, et al. Generating focused molecule libraries for
2020;15:523–5 . drug discovery with recurrent neural networks. ACS Central Sci 2017;4:120–31 .
[10] Ertl P, Schuffenhauer A. Estimation of synthetic accessibility score of drug-like [47] Gupta A, Müller AT, Huisman BJH, et al. Generative recurrent networks for de novo
molecules based on molecular complexity and fragment contributions. J Cheminf drug design. Mol Inform 2017;37:1700111 .
2009;1:8 . [48] Ertl P., Lewis R., Martin E., et al. In silico generation of novel, drug-
[11] Bickerton GR, Paolini GV, Besnard J, et al. Quantifying the chemical beauty of drugs. like chemical matter using the LSTM neural network. arXiv 2017.
Nat Chem 2012;4:90–8 . doi: 10.48550/arXiv.1712.07449 .
[12] Schneider G, Fechner U. Computer-based de novo design of drug-like molecules. Nat [49] Olivecrona M, Blaschke T, Engkvist O, et al. Molecular de-novo design through deep
Rev Drug Discov 2005;4:649–63 . reinforcement learning. J Cheminf 2017;9:48 .
[13] Hartenfeller M, Zettl H, Walter M, et al. DOGS: Reaction-driven de novo design of [50] Amabilino S, Pogány P, Pickett SD, et al. Guidelines for recurrent neural network
bioactive compounds. PLoS Comput Biol 2012;8:e1002380 . transfer learning-based molecular generation of focused libraries. J Chem Inf Model
[14] Yonchev D, Bajorath J. Integrating computational lead optimization diagnostics with 2020;60:5699–713 .
analog design and candidate selection. Future Sci OA 2020;6:FSO451 . [51] Yonchev DDeepCOMO B.J.. From structure-activity relationship diagnostics to gen-
[15] Gómez-Bombarelli R, Wei JN, Duvenaud D, et al. Automatic chemical design using a erative molecular design using the compound optimization monitor methodology. J
data-driven continuous representation of molecules. ACS Central Sci 2018;4:268–76. Comput Aided Mol Des 2020;34:1207–18 .
[16] Colby SM, Nuñez JR, Hodas NO, et al. Deep learning to generate in silico chemi- [52] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput
cal property libraries and candidate molecules for small molecule identification in 1997;9:1735–80 .
complex samples. Anal Chem 2019;92:1720–9 . [53] Cho K, van Merrienboer B, Gulcehre C, et al. Learning phrase representations us-
[17] Baskin I, Gordeeva E, Devdariani R, et al. Methodology of the inverse problem so- ing RNN encoder–decoder for statistical machine translation. In: Proceedings of the
lution for the structure property relation in case of topological indices. Dokl Akad 2014 conference on empirical methods in natural language processing (EMNLP).
Nauk SSSR 1989;307:613–17 . Association for Computational Linguistics; 2014. p. 1724–34 .
[18] Brüggemann R, Pudenz S, Carlsen L, et al. The use of Hasse diagrams as a potential [54] Sanchez-Lengeling B, Outeiral C, Guimaraes GL, et al. Optimizing distributions
approach for inverse QSAR. SAR QSAR Environ Res 2001;11:473–87 . over molecular space. an objective-reinforced generative adversarial network
[19] Miyao T, Funatsu K. Finding chemical structures corresponding to a set of coordi- for inverse-design chemistry (ORGANIC). ChemRxiv 2017. doi: 10.26434/chem-
nates in chemical descriptor space. Mol Inform 2017;36:1700030 . rxiv.5309668.v3 .
[20] Sanchez-Lengeling B, Aspuru-Guzik A. Inverse molecular design using machine [55] Blaschke T, Olivecrona M, Engkvist O, et al. Application of generative autoencoder
learning: generative models for matter engineering. Science 2018;361:360–5 . in de novo molecular design. Mol Inform 2017;37:1700123 .
[21] Vogt M. Using deep neural networks to explore chemical space. Expert Opin Drug [56] Polykovskiy D, Zhebrak A, Vetrov D, et al. Entangled conditional adversarial autoen-
Discov 2021;17:297–304 . coder for de novo drug discovery. Mol Pharm 2018;15:4398–405 .
[22] Skinnider MA, Stacey RG, Wishart DS, et al. Chemical language models enable nav- [57] Prykhodko O, Johansson SV, Kotsias PC, et al. A de novo molecular genera-
igation in sparsely populated chemical space. Nat Mach Intel 2021;3:759–70 . tion method using latent vector based generative adversarial network. J Cheminf
[23] Jin W, Barzilay R, Jaakkola T, Dy J, Krause A. Junction tree variational autoencoder 2019;11:74 .
for molecular graph generation. In: Proceedings of the 35th international conference [58] Zhavoronkov A, Ivanenkov YA, Aliper A, et al. Deep learning enables rapid identifi-
on machine learning, 80. PMLR; 2018. p. 2323–32. Proceedings of Machine Learning cation of potent DDR1 kinase inhibitors. Nat Biotechnol 2019;37:1038–40 .
Research 10–15 Jul . [59] Iovanac NC, Savoie BM. Simpler is better: how linear prediction tasks improve trans-
[24] You J, Liu B, Ying R, et al. Graph convolutional policy network for goal-directed fer learning in chemical autoencoders. J Phys Chem A 2020;124:3679–85 .
molecular graph generation. In: NIPS’18: Proceedings of the 32nd international con- [60] Putin E, Asadulaev A, Ivanenkov Y, et al. Reinforced adversarial neural computer
ference on neural information processing systems; 2018. p. 6412–22 . for de novo molecular design. J Chem Inf Model 2018;58:1194–204 .
[25] Li Y, Zhang L, Liu Z. Multi-objective de novo drug design with conditional graph [61] Hong SH, Ryu S, Lim J, et al. Molecular generative model based on an adversarially
generative model. J Cheminf 2018;10:33 . regularized autoencoder. J Chem Inf Model 2019;60:29–36 .
[26] Mercado R, Rastemo T, Lindelöf E, et al. Graph networks for molecular design. Mach [62] Gaulton A, Hersey A, Nowotka M, et al. The ChEMBL database in 2017. Nucleic
Learn Sci Technol 2021;2:025023 . Acids Res 2016;45:D945–54 .
[27] Goodfellow I, Bengio Y, Courville A. Deep learning. Cambridge, MA: The MIT Press; [63] Putin E, Asadulaev A, Vanhaelen Q, et al. Adversarial threshold neural computer for
2017 . molecular de novo design. Mol Pharm 2018;15:4386–97 .
[28] Heller SR, McNaught A, Pletnev I, et al. InChI, the IUPAC international chemical [64] Popova M, Isayev O, Tropsha A. Deep reinforcement learning for de novo drug de-
identifier. J Cheminf 2015;7:23 . sign. Sci Adv 2018;4:eaap7885 .
[29] Bjerrum E, Sattarov B. Improving chemical autoencoder latent space and molecular [65] Blaschke T, Arús-Pous J, Chen H, et al. REINVENT 2.0: an AI tool for de novo drug
de novo generation diversity with heteroencoders. Biomolecules 2018;8:131 . design. J Chem Inf Model 2020;60:5918–22 .
[30] Arús-Pous J, Johansson S, Prykhodko O, et al. Randomized SMILES strings improve [66] Makhzani A., Shlens J., Jaitly N., et al. Adversarial autoencoders. arXiv 2015.
the quality of molecular generative models. ChemRxiv 2019. doi: 10.26434/chem- doi: 10.48550/arXiv.1511.05644 .
rxiv.8639942.v2 . [67] Brown N, Fiscato M, Segler MH, et al. GuacaMol: Benchmarking models for de novo
[31] O’Boyle N, Dalke A. DeepSMILES: an adaptation of SMILES for use in molecular design. J Chem Inf Model 2019;59:1096–108 .
machine-learning of chemical structures. ChemRxiv 2018. doi: 10.26434/chem- [68] Polykovskiy D, Zhebrak A, Sanchez-Lengeling B, et al. Molecular sets (MOSES): a
rxiv.7097960.v1 . benchmarking platform for molecular generation models. Front Pharmacol 2020;11.
[32] Krenn M, Häse F, Nigam A, et al. Self-referencing embedded strings (SELF- doi: 10.3389/fphar.2020.565644 .
IES): a 100% robust molecular string representation. Mach Learn Sci Technol [69] Chen H, Vogt M, Bajorath J. DeepAC –conditional transformer-based chemical lan-
2020;1:045024 . guage model for the prediction of activity cliffs formed by bioactive compounds.
[33] Wang R, Gao Y, Lai L. LigBuilder: a multi-purpose program for structure-based drug Digital Discov 2022;1:898–909 .
design. J Mol Model 2000;6:498–516 . [70] Zhang J, Mercado R, Engkvist O, et al. Comparative study of deep generative models
[34] Chéron N, Jasty N, Shakhnovich EI. OpenGrowth: an automated and rational algo- on chemical space coverage. J Chem Inf Model 2021;61:2572–81 .
rithm for finding new protein ligands. J Med Chem 2015;59:4171–88 . [71] Bertz SH. The first general index of molecular complexity. J Am Chem Soc
[35] Kutchukian PS, Lou D, Shakhnovich EI. FOG: Fragment optimized growth algorithm 1981;103:3599–601 .
7M. Vogt Artificial Intelligence in the Life Sciences 3 (2023) 100064
[72] Preuer K, Renz P, Unterthiner T, et al. Fréchet ChemNet distance: A metric for gener- [76] Bagal V, Aggarwal R, Vinod PK, et al. MolGPT: Molecular generation using a trans-
ative models for molecules in drug discovery. J Chem Inf Model 2018;58:1736–41. former-decoder model. J Chem Inf Model 2021;62:2064–76 .
[73] Heusel M, Ramsauer H, Unterthiner T, et al. GANs trained by a two time-scale [77] He J, You H, Sandström E, et al. Molecular optimization by capturing chemist’s
update rule converge to a local Nash equilibrium. Adv Neural Inf Process Syst intuition using deep neural networks. J Cheminf 2021;13 .
2017;30:6627–38 . [78] Yuan W, Jiang D, Nambiar DK, et al. Chemical space mimicry for drug discovery. J
[74] Salimans T, Goodfellow I, Zaremba W, Lee D, Sugiyama M, Luxburg U, et al. Im- Chem Inf Model 2017;57:875–82 .
proved techniques for training GANs. Adv Neural Inf Process Syst 2016;29:2234–42. [79] Merk D, Friedrich L, Grisoni F, et al. De novo design of bioactive small molecules by
[75] Sattarov B, Baskin II, Horvath D, et al. De novo molecular design by combining artificial intelligence. Mol Inform 2018;37:1700153 .
deep autoencoder recurrent neural networks with generative topographic mapping. [80] Grisoni F, Neuhaus CS, Gabernet G, et al. Designing anticancer peptides by construc-
J Chem Inf Model 2019;59:1182–96 . tive machine learning. ChemMedChem 2018;13:1300–2 .
8