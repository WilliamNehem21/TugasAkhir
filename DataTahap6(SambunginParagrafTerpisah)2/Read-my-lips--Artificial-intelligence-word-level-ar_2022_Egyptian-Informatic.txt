Deep learning techniques extract useful features and patterns from data using the optimization of neural networks to solve real-world problems. Many fields such as natural language pro- cessing, computer vision, and speech recognition leverage deep learning to achieve tasks with performance better than human experts. The vast amount of data available in the big data era is one of the main causes behind the recent advances in deep learn- ing techniques. As more data is available for training, validation, and testing purposes, deep learning is capable of accomplishing recognition tasks with much higher accuracy. Another important factor of the current deep learning breakthrough is the growth of the computing power with efficient resources and supercomputers.

Moreover, VSR is used to aid audio speech recognition (ASR) in noisy environments [6,7]. This combination is referred to as audio- visual speech recognition (AVSR) and can be leveraged to identify speech in multi-talker environments (e.g. recorded videos used in criminal investigations) and identify instructions given to systems or machines from humans in noisy environments such as under- water and space activities. Finally, VSR has a wide range of multi-

The rest of this paper is organized as follows: Section 2 presents the most recent related work. Section 3 describes the design com- ponents and the implementation details of the RML system. The experimental setup and results are discussed in Section 4. Finally, conclusions and future work are presented in Section 5.

The authors in [9] presented a very recent survey on automated lipreading mechanisms and provided a comparison between the previously proposed solutions for the main stages of lipreading (i.e. feature extraction and classification networks). In this section, we focus on the most related work to our paper. Faubel et al. [10] combined audio-visual voice activity detection (VAD) with micro- phone array processing techniques to improve the speaker local- ization. Siatras et al. [11] utilized the increased deviation and values of the number of low intensity pixels in the mouth region of a speaking person as visual cues to aid speech detection. Pingx- ian et al. [12] proposed a lip detection system based on the com- puter vision library; Opencv.

More recently, Kumar et al. targeted mitigating the problem of low prediction accuracies in VSR classification models when unseen data is used for testing [13]. This was achieved by deploy- ing Generative Adversarial Networks (GANs) to generate new unseen data and use it to perform zero-shot learning which improved the accuracy of VSR systems by 27%. In [14], the authors modeled how visual speech is perceived and showed its use case on video compression. In addition, a view-temporal attention mechanism was proposed to model VSR while taking into consid- eration both the view dependence and the visemic importance. The experimental results showed absolute improvement of 5% in the viseme error rate.

Shah et al. investigated the linguistic and acoustic features of two state-of-the-art audio transformer models (i.e. wav2vec2.0 and Mockingjay) by probing each of their layers [15]. The experi- mental results showed that these models can significantly capture audio, fluency, suprasegmental pronunciation, syntactic and semantic text-based characteristics. For each category of charac- teristics, the authors identified a learning pattern and the best layer within a model to choose for feature extraction for down- stream tasks. The audio-visual speech recognition models in [16] were modified in [17] to predict visemes given an input video. The updated models were used to perform experiments in bilin-

Eldirawy and Ashour [18] built a system to lip-read the num- bers from one to ten when spoken in the Arabic language. Three recognition methods were used: K-mean, fuzzy k-mean, and k- nearest neighbors (K-NN) classifiers with maximum recognition accuracy of 55.8%. Morade and Patnaik [19] proposed a lipreading algorithm based on localized active counter model (ACM) and a hidden Markov model (HMM). The algorithm was tested by record- ing videos of English digits from zero to nine from 16 speakers (8 male and 8 female) and the Cuave database. The recognition accu- racy varied between 77.8% and 79.6%.

Alzahraa et al. [20] proposed a model that relies on lip move- ments to identify the words for the numbers in the range from zero to nine in the Arabic language. The model utilizes speeded up robust features (SURF), histogram of oriented gradient (HoG), and Haar techniques and the reported recognition accuracy is 96.2%. In [21], Wand et al. merged feed-forward and recurrent neural net- work layers into a single structure that was trained using back- propagating error gradients to recognize speech from soundless video recordings. The best reported word recognition accuracy is 79.6% using videos of 19 speakers and 51 words from the GRID cor- pus [22].

Kumar et al. presented a regression model to perform multi- view lipreading and generate an audio output [26]. The model con- sists of a view classifier followed by a Spatio-Temporal Convolu- tional Neural Network (STCNN) and BGRUs network. The OuluVS2 database was used to train and test the model and the Perceptual Evaluation of Speech Quality (PESQ) was used as the evaluation metric. The highest score of 2.315 was achieved with triple-view at 0, 45, and 60 degrees. In addition, the proposed model provided substantial delay improvements over previous work. Uttam et al. designed an encoder-decoder speaker- independent architecture which takes silent videos as input and outputs an audio spectrogram of the reconstructed speech [27]. The model consists of four components: pose classifier, decision network to select the right encoder, audio autoencoder, and video encoder that consists of seven three-dimensional convolutional layers followed by an LSTM layer. The proposed model was tested

Weng and Kitani proposed a word-level visual lipreading method based on deep three-dimensional CNN with two streams (i.e. grayscale video and optical flow streams) and BiLSTM [28]. The method was tested on LRW dataset with accuracy of 84.11%. In [29], Shrivastava et al. proposed a novel end-to-end deep neural network model for word-level VSR in resource constrained envi- ronments such as mobile devices. Depthwise 3D convolution and channel shuffling were deployed to achieve 70% recognition accu- racy using the LRW dataset with 6 times fewer parameters and 20 times smaller memory foot print.

with 81.67% accuracy. It is not clear from the paper how the data- set is divided into training and testing in order to guarantee that the testing part of the dataset is unseen. On the other hand, our proposed approach achieves 82.84% accuracy on unseen testing set using a dataset of 1828 recordings.

The design process of RML consists of three steps: dataset col- lection, dataset preprocessing and splitting, and prediction models implementation. The details of the three steps are described in the following three subsections, respectively. After the third step is completed, the preprocessed labeled dataset is used to train, vali- date, and test the implemented models as discussed in the subSec- tion  4.2.  The  implemented  deep  learning  models  perform

Most of the 73 speakers uttered each one of the 10 words once (i.e. 730 videos); however, few of them repeated the 10 words between two and four times (i.e. This accounts for the extra 321 videos). This ensures the inclusiveness of the dataset because there are many diversities between people that need to be accounted for, such as: speediness of speaking, mouth shape and movements, lips geometric features, amount of tongue determined by the redness of the taken mouth frame, the alveolar ridge, teeth, braces, mus- tache, beard, and makeup.

Two versions of the dataset are used in the experimental work: the colored version and the grayscale version. In the colored ver- sion, the video frames are represented by three channels: Red, Green, and Blue (i.e. RGB). The colored video frames are converted to grayscale frames to generate the second version of the dataset; where each pixel is represented by a single channel that reflects the brightness of the pixel.

After the last preprocessing step (i.e. normalization), the image frames are split into three sets:training, validation, and testing. The training set is used to train and fit the used deep neural network model and the validation set is used to tune the model hyper- parameters and improve its performance. In addition, the valida- tion set is used to avoid overfitting. The testing set is unseen data to evaluate the recognition accuracy of the used model. The dataset is split as follows: 652 videos in the training set, 125 videos in the validation set, and 274 videos in the testing set. A data augmenta-

During the implementation phase of the RML, three deep learn- ing models were investigated in order to choose the one with the highest recognition accuracy for the 10 Arabic words using the dataset described in subSection 3.1. In this subsection, we describe the details of the three models.

One of the main deep learning models used for recognition tasks is the Convolutional Neural Network (ConvNet or CNN) because it is capable of capturing spatial and temporal dependen- cies. CNN is an artificial neural network (ANN) that consists of an input layer followed by multiple hidden layers and then an output layer. The hidden layers contain one or more convolution layers normally followed by pooling, normalization, flatten, and fully- connected layers [45].

along n-dimensions on the input data according to a specific stride. According to the value of n, a convolution layer can be classified to: 1-dimensional (Conv1D), 2-dimensional (Conv2D), and 3- dimensional (Conv3D). Notice that in order for the kernel to slide along n-dimensions of the input data, the shape of the input data should consist of at least n + 1 dimensions.

The output of the last MaxPooling layer is unrolled to a 1- dimensional vector using a flatten layer. The vector is processed by fully connected layers (i.e. Dense layers) with dropout layers added between them. The dropout layers are used for regulariza- tion and to avoid overfitting. During training time, a dropout layer eliminates some of its inputs according to a specific rate and scales up the remaining inputs by 1/(1-rate) such that the network never relies on a feature to be present.

The numbers,sizes, and parameters of the layers in the CNN are chosen to achieve highest recognition accuracy. When the grays- cale version of the dataset is used, the Conv3D and the 3- dimensional MaxPooling layers in the CNN model are replaced with Conv2D and 2-dimensional MaxPooling layers respectively. This is due to the fact that the shape of the grayscale version of the dataset consists of three dimensions (i.e. frame number, frame height, and frame width) and the kernels slide along two dimen- sions of the input data.

When compiling the proposed prediction models, the Cross- Entropy (CE) Loss function and the Adam optimizer with learning rate (lr) of 0.0001 are used. The combination of the Softmax activa- tion function utilized by the output layers of the proposed models and the CE Loss function is called Categorical CE Loss or Softmax Loss. The Categorical CE Loss function is commonly used when dealing with multi-class classification problems (i.e. more than two classes), where the classes are labeled using one-hot code and only one class can be correct. The objective is to minimize the Loss function of the prediction model. This happens when class with the highest prediction is the correct class.

Given that the dataset splitting method described in subSec- tion 3.2 is performed randomly, the simulation results presented in the following three subsections are the average values of five simulation runs with five different random seeds for dataset split- ting. In addition, the standard deviation for the testing accuracy of each model is reported.

is used (i.e. 0.792 > 0.766). This indicates that when the image frames of the videos contain colors such as red for the tongue and white for the teeth, the CNN model is capable of extracting precise mouth movement features from the training set. These fea- tures are then used to categorize the testing set with high accuracy.

Based on the observations in the previous three paragraphs, a voting model which simultaneously runs six prediction models (i.e. two instances of each model, one instance with the grayscale dataset and one instance with the RGB dataset) is proposed. Each input video, after being preprocessed, is fed to the six instances and the final prediction of each instance is considered as a vote. When there is a majority vote, it is chosen as the final output of the voting model. There are three scenarios where a majority vote exists: four or more identical votes, three identical votes and the

In this paper, we introduce Read My Lips (RML); a word-level Arabic lipreading system. Three deep learning models were inves- tigated during and implementation phase of RML: CNN, TD-CNN- LSTM, and TD-CNN-BiLSTM. In order to train, validate, and test the system, two datasets which contain grayscale and RGB video frames of 73 speakers uttering ten common Arabic words was col- lected. The experimental results show that the CNN model with RGB dataset has the highest overall prediction accuracy of 79.2% on unseen test data. In addition, a voting model is proposed to implement the RML system which improves the overall prediction accuracy to 82.84%.

We have two future directions in mind: first, we plan to increase the size of the dataset by including more Arabic words and more videos of the same word. The purpose of this extension is to validate our current results for a larger dataset and improve the prediction accuracy of the models. Second, we plan to extend RML to become a sentence-level Arabic lipreading system so that it can be used in real-time applications.

