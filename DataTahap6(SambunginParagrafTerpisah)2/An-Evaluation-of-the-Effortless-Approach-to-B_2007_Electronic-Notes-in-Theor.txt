To ways of algorithm construction are considered in the definition of the con- structing engagement level by Naps et al. [7]: direct generation and hand construc- tion. Our approach has some of both ways. First, visualizations of the different execution stages are automatically generated by the environment (direct genera-

The rest of the paper is structured as follows. Section two describes the evalu- ation: participants, variables studied, method and procedure. Then results of the evaluation are shown in section three, and discussed in section four. Finally, in section five, conclusions and future work are described.

Participants were randomly divided in two groups: the viewers group and the builders group (VG and BG respectively for the rest of the paper). Both groups were asked about their previous knowledge about the algorithm, only one student having previous knowledge. Therefore both groups belong to the same population and further results can be compared.

The evaluation was divided into two sessions: a training session where the IDE was shown to the students, and the experimental session where knowledge about the algorithm was evaluated. Participation was ten and thirteen students respectively. The training session was two hours long. The instructor demonstrated the tool, he generated two web-based animations with WinHIPE as an example, and stu- dents generated two more animations. The animations used were unrelated to the algorithm that would be used in the experimental session. None of the students ap- peared to have problems using the tool. At the end of this session a questionnaire about the tool was completed by the students thus, we got their first impression about the tool.

The experimental session also was two hours long, and two weeks after the training session. First, we explained to the students that we were carrying out the evaluation, and that their participation would be voluntary. Next, we randomly formed the VG (n=7) and BG (n=6), and we checked that all students in the BG had attended the previous training session. Students of both groups were asked about their previous knowledge about the algorithm. Then, we gave the students all the materials they were allowed to use to study the algorithm, which was a textual description of the algorithm for both groups, and: (3) was only identified by 14% of students (1/7) in the VG, but by 83% of students (5/6) in the BG (U = 7.000,p < .05). We did not find significant differences in performance in the second, third and fourth questions. Thus, the average grades for the understanding level were 0.88 for the BG and 0.73 for the VG, a 16% of learning improvement.

We found significant differences (U = 7.000,p < .05) in the answers to the question related to the application level. The VG obtained an average grade of 0.33, while the BG obtained an average grade of 0.77, a 60% of learning improvement.

