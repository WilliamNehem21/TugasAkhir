Siddique et al. [21] analyzed five machine learning algorithms for Combined Cycle Power plant (CCPP) net power prediction using a six-year data set. The dataset evaluates output power based on input parameters such as temperature, humidity, and pressure. Gradient boosted Regression Tree performs better than all others, with 450 trees having the lowest RMSE and AE. Tufekci et al. [22] demonstrated a feasible model for predicting a base load operated power output. The CCPP is utilizing the Bagging technique with the REPTree predictor to predict the following day's hourly energy production. Wang et al.

[23] introduced a new Machine Learning-based method for predicting and optimizing ORC performance. BPNN and SVR models provided accurate predictions, proving research feasibility and effectiveness. SVR improves cycle operating parameter and reverse prediction. The approach has replaced the thermodynamic optimization, making it more rapid, precise, and efficient.

The key contribution of this study involves thorough examination of the optimal MCIT, TIT, and climate conditions for sCO2 Brayton cycles and optimization using advanced optimization framework to attain optimal thermal performance of the system. This analysis is conducted by employing machine learning algorithms and a creative approach to optimization using the Genetic Algorithm (GA) method. In addition to the comprehensive analysis and parametric optimization, this study also underscores the critical role of hyperparameter optimization for the machine learning algorithms, specifically the Artificial Neural Network (ANN), ensuring their optimal performance in modeling and optimization tasks. This study examines the optimization technique of sCO2BC, which incorporates concentrated solar power (CSP) and utilizes solar irradiance as the major source of heat. This study examines the impact of several factors on net power output, cycle thermal efficiency, and net energy utilization. The system design and performance are significantly influenced by climate conditions. Furthermore, an investigation has been performed for potential application of a Pre-compressor inside a recompression cycle, specifically in the context of partial cooling and main compression intercooling sCO2BC. This investigation also explores the inclusion of an additional cooler to enhance cooling at intermediate pressure, hence improving overall system performance. The scope of this work is outlined in the section below: interactions. Additionally, ANNs enable adaptability in accommodating diverse situations and complexities within a given system. They have exceptional proficiency in managing complex systems, enabling quick adjustments in response to changing circumstances, leveraging data-driven analysis to inform decision-making, and expediting the process of software development. ANNs have the potential to significantly improve energy efficiency, rendering them highly valuable in various domains, including power generation. Nevertheless, it is imperative to utilize these tools along with thermodynamic models in order to optimally utilize their capabilities while effectively addressing their limits. To optimize design parameters, multi-objective GA is integrated with the best machine learning model. MATLAB software is used for setting up neural network & GA- gamultiobj was set as solver for multi-obj optimization. The following subsections provide insights regarding the adopted framework for this study.

In the current work, six machine learning algorithms, specifically XGBoost, Random forest, LightGBM, AdaBoost, KNN, and ANN are used to train the data. On the basis of prediction accuracy, the best model is chosen for next stage of optimization. The following subsections present details about the theoretical framework used in the study.

The supervised learning algorithm XGBoost, accurately predicts a target variable by combining the values of simpler variables. It implements regression trees as weak learners and minimizes the objective function by means of a convex loss function and penalty clause. The training procedure is iterative, with the addition of new trees for forecasting residues and errors and the execution of gradient boosting to minimize loss. Instead of selecting the best model based on the data, XGBoost parallelizes the dataset's boosting by training hundreds of models on diverse subsets[38].

The RF approach, which combines multiple independent decision trees to precisely anticipate new input data, is a key part of machine learning's supervised learning strategy. Ensemble learning solves classification and regression problems. Using each tree's projections and predictions, Optuna hyperparameter tweaking minimizes overfitting and maintains accuracy [39]. Least squares create the decision tree, x(n) was separated into R1 and R2 after choosing the separation point (sp). Calculating the minimal output y value by calculating c1 and c2. This means that (n) is this ap's optimal separation point from x.

KNN is a popular regression technique that calculates distances between neighbors using weighting techniques. It provides more accuracy but is time-consuming. However, it necessitates adaptive neighbors and weighting to accommodate dataset properties, and traditional KNN may not be accurate due to dataset density variations.[43]. Let {N j}k represent the target values of an instance's k nearest neighbors. After that, the conventional KNN prediction Yi for the i-th instance is calculated as shown below: 20. Ideally, the number of neurons for the analysis should be set to 10. The feedforward neural network is a versatile tool that can be employed for various types of input-output mappings. The performance metric employed in this study is the Mean Squared Error (MSE), whereas the training method utilized is Bayesian regularization (trainbr). Although the trainbr approach normally takes longer, it may provide a high degree of generalization for difficult, constrained, or noisy datasets and adaptive weight minimization determines when training ends.

