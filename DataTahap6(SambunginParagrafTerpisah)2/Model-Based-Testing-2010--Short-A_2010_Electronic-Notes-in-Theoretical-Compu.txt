Quality of software is an issue of increasing importance and growing concern. Sys- tematic testing is one of the most important and widely used techniques to check the quality of software. Testing, however, is often a manual and laborious process with- out effective automation, which makes it error-prone, time consuming, and very costly. One of the new technologies to meet the challenges imposed on software testing is model-based testing.

From an industrial perspective, model-based testing is a promising technique to improve the quality and effectiveness of testing, and to reduce its cost. The current state of practice is that test automation mainly concentrates on the automatic exe- cution of tests for which a multitude of commercial test execution tools is available. These tools, however, do not address the problem of test generation. Model-based testing aims at automatically generating high-quality test suites from models, thus complementing automatic test execution.

From an academic perspective, model-based testing is a natural extension of formal methods and verification techniques, where many of the formal techniques can be reused. Formal verification and model-based testing serve complementary goals. Formal verification intends to show that a system has some desired proper- ties by proving that a model of that system satisfies these properties. Thus, any verification is only as good as the validity of the model on which it is based. Model- based testing starts with a (verified) model, and then intends to show that the real, physical implementation of the system behaves in compliance with this model. Due to the inherent limitations of testing, such as the limited number of tests that can be performed, testing can never be complete: testing can only show the presence of errors, not their absence.

An important benefit of model-based testing is the automatic generation of large numbers of test cases from a model. Model-based testing, however, is more than just generating an amount of test cases from a model. To prevent that just any artifact generated from a model could be called a test case, test generation must be based on a sound and well-defined underlying theory of model-based testing. Such a theory must support precise reasoning about the objects of model-based testing, such as models, IUTs, test cases, test generation, and verdicts. Even more important, such a theory must establish relations between these objects: it defines precisely when an IUT is correct with respect to a model, what it means for a test case to be valid, and what a correctness proof for a test generation algorithm encompasses.

Two important ingredients of such a theory of model-based testing are a testing hypothesis and an implementation relation. A testing hypothesis, or test assumption [1], establishes the link between the black-box, real IUT, which consists of software, hardware, physical components, or a combination of these, and the world of models. The assumption is made that any real IUT can be modelled by some object in a domain of models. The testing hypothesis presupposes that such a domain of models is known apriori, and that a valid model of the IUT exists in this domain, but not that this particular model is apriori known. In this way, the testing hypothesis allows reasoning about IUTs as if they were models in this (formal) domain.

Building on the testing hypothesis, an implementation relation, also called con- formance relation or refinement relation, is a formal relation between models of IUTs and specification models [3]. It defines when an IUT is correct with respect to a specification model. This implies that an implementation relation is at the core of a theory of model-based testing, and that validity of test cases and cor- rectness of a test generation algorithm must always be assessed with respect to an implementation relation.

Model-checking and model-based testing serve complementary goals. Model- checking aims at showing that a model is valid and has particular properties. Model-based testing starts with a valid model to show that the IUT behaves in compliance with this model. Only when model-based testing and model-checking are based on compatible theories, we can ensure that model-checked properties are preserved in the IUT.

The implementation relation ioco is based on the testing hypothesis that im- plementations behave as input-enabled labelled transition systems. Over time, a number of variants of ioco have been defined which differ in, e.g., specification models, input-enabledness, and the treatment of partial specifications. The relation uioco is more specific with respect to partial specifications, wioco does not assume input-enabledness of implementations, tioco and rtioco add real-time properties to models, and hioco extends this to hybrid transition systems. The relation sioco treats data in a symbolic way, which facilitates transposing pre/post-conditions to the realm of labelled transition systems, and stiocoD extends this to the combi- nation of real-time and data. The relation mioco does it with multiple input and output channels, iocor considers refinement of actions, qioco adds quantification of imprecision, poco does it with partial observability, and eco takes the environment into account.

Model based testing has been around for years. It is based on the idea to derive test cases from the model of the system under test (SuT) that was created for develop- ment purposes. This concept has a drawback: using development information alone only provides verification but not validation. A development model of a software component can be used to generate test cases which can obtain c0 to c2 coverage of the model but do not cover actual user workflows or exceptional and error pro- voking scenarios. Validation is about answering the question if the correct system was realized. The test cases necessary for this need to include information from the different stakeholders, designers, tester etc. Within this paper the methodology of

Automotive OEMs need to integration-test the various control devices, which are supplied by different component manufacturers and the coordination of the func- tions between different suppliers. Consistent representation of the functionalities of the overall systems and their dependencies poses a great challenge in this field. The practical suitability of the .mzT method and the .getmore test case generator has been demonstrated in several projects in the automotive industry [2].

In a cooperation project the extension of the TTCN-3-based test and simulation system TTsuite MOST with the model-centric test design approach was evaluated [5]. MOST (Media Oriented Systems Transport) is the leading network standard for car infotainment. The objective of this project is to systematically test the remote operation possibilities while retaining the greatest possible test coverage. The functions to be tested include play, pause, forward, scan, shuffle, and list, which can be executed in any order. Based on the description of all available functions, a test model (.mzT) was created using the model-centric testing methodology. The basis of the test setup is TTsuite MOST with existing TTCN-3 libraries for the MOST connection of the test system, the hierarchical .mzT test model and the infotainment system of a real vehicle. After the test cases have been generated from the test model, they are compiled with TTsuite MOST and are immediately available for execution. Unlike with intuitive, manual test case preparation, with this approach it is possible to systematically identify all test cases and to cut down the number of test cases.

Model-centric testing shows that MBT can be used in practice successfully. Based on this .mzT grew from an idea to a real methodology including guidelines, training and tooling. .mzT helps the test managers to organize the test not according to a new tool chain, but to use existing tool chains and extend those by MBT/.mzT.

Testing effort grows exponentially with the system size and traditional testing meth- ods seem not to be able to keep pace with the trend in engineering towards more complex and bigger systems. On the construction side a current trend in European industry is to apply model-based systems engineering techniques to deal with the increased complexity of systems and with the intention to produce high quality sys- tems at reduced costs. Unfortunately, the quality assurance side has not kept step with this development.

Intensive research on model-based testing (MBT) and analysis has been con- ducted in recent years, and the feasibility of the approach has been successfully demonstrated, e.g. in [2,7]. Yet, Boberg [1] shows that most studies apply model- based testing on the component level, or to a limited part of the system while only few studies focus on the application of the technique on the system or even aircraft level. The main difference being that the goal is not to produce code but to provide a high quality specification.

A number of studies have demonstrated that the cost of fixing problems increases as the lifecycle of the system under development progresses, e.g. [3]. Testing thus needs to be applied as early as possible in the lifecycle to keep the relative cost of repair for fixing a discovered problem to a minimum. This means that testing

Descriptive modelling of system test cases: Descriptive modelling allows a description of the test cases using a combination of natural plain text language and the formal SysML Activity diagram graphical notation. The combination provides a semi-formal notation that is easily used and quickly adapted by system designers and test engineers. Furthermore, the notation is understood easily and the created diagrams can be used for communication purposes with non-engineers as well.

Innovative cross-domain tool framework: Another way forward for model based testing is the integration of the different technologies and tools for analy- sis and testing into an innovative generic, viz. domain independent, model-based analysis and testing tool solution, a so-called testing framework. Embracing com- plete separation between tools and its data, and by offering a standard interface for storing and retrieving model elements to and from the platform, tool owners can easily work with their data and integrate with data from other tools in a common, standard way.

