This paper will explore some of the more advanced areas of information re- trieval. We will focus on cross-lingual information retrieval, multimedia informa- tion retrieval and semantic-based information retrieval. Cross-lingual information retrieval deals with asking questions in one language and retrieving documents in one or more different languages. With an increasingly globalized economy, the ability to find information in other languages is becoming a necessity. Multimedia information retrieval deals with finding media other than text, i.e. music and pic- tures. As computers are now used for storing video and audio collections, methods for quick and accurate retrieval are needed. Finally, semantic based information retrieval goes beyond classical information retrieval and uses semantic information to understand the documents and queries in order to aid retrieval.

This paper will continue as follows. First, in section 2 we will look at at cross- lingual information retrieval. Then, in section 3 we will look at information retrieval of multimedia content. Next, in section 4 semantic-based information retrieval will be examined. Finally, in section 5 concluding remarks will be made.

Recently, a number of tracks and workshops have sprung up to support research in this area. TREC (Text Retrieval Conference) had a Cross-Language IR track for a few years until 2002. CLEF (Cross Language Evaluation Forum) has been running since 2000 and deals with European languages. The NTCIR (NII Test Collection for IR Systems) Project is a yearly competition in Japan that covers many topics including CLIR dealing with languages such as Japanese, English, Chinese and

Most systems in CLIR use some type of translation. While there exist non- translation methods, such as cognate matching [3], latent semantic indexing [19], and relevance models [35], the predominate method is still translation. As such one of the main problems in CLIR is dealing with language translation. What should be translated, how should it be translated, and how to eliminate bad translations are some of the major areas of research in CLIR. In addition how to acquire large enough amounts of translation data is also an active topic for research. Because these, even now, are the foremost problems, this section is devoted to advanced research done to alleviate them.

This section will continue as follows. First, we will look at what to translate. Then, we look at the methods used for translation. Next, we will look at methods researchers have come up with to automatically acquire resources for translation. Finally, we look at what the future holds for CLIR.

The big three choices in what to translate are the query, the document or both. Query translation involves translating the query to the target language. Document translation translates the document into the source language (i.e. the language used for the query).

Document translation is typically done using a machine translation system, such as SYSTRAN [64]. McCarley [42] points out several possible advantages to document translation. The most appealing being that by translating the document there are more chances for translating a word correctly or into a synonymous form that is used in the query.

Much research done in comparing document translation to query translation has found that document translation is typically better. Oard found that in some cases document translation gave better results than query translation on TREC-6 data [48]. Chen and Gey found that document translation gave slightly better results for the CLEF 2003 test collection [12].

However, there are some problems with document translation. The main one being that machine translation is computationally expensive and in some instances impractical [8]. However, with modern computers this is becoming less of a problem, especially for smaller document collections. Other problems include the cost of machine translation systems and the lack of availability of translation systems for a wide range of language pairs.

The simplest approach is to do word-by-word translation and use co-occurrence information for disambiguation. Fukui et al. found this type of technique to work well for patent retrieval [21]. However, this approach does not typically give the best results. Gao et al. found substantial improvements using a decaying co-occurrence approach that also utilized syntactic dependency [23]. Gao and Nie found that more specialized translation models, such as NP translation gave better results on the TREC collections [22].

In addition to standard query translation there is query expansion. Query ex- pansion extends the query words to include similar concepts to allow for better retrieval. In monolingual IR this is typically accomplished with a thesaurus. For CLIR there are two types of expansion: pre-translation and post-translation. Pre- translation expansion adds new query words from the language the query was writ- ten in. Post-translation, takes the translated query and then extends it by some means. McNamee and Mayfield showed that even with degradation in the quality of translation (i.e. poor dictionary or error prone parallel corpus) using both pre and post translation expansion greatly improve results [43].

The final option is to translate both the document and the query. While this is the most expensive it also seems to yield the best results. The reason is that document translation involves translating from the target language to the source language and query translation is the opposite, from source to target. Even when training with the same data the translation quality can be vastly different as [42] found.

Doing both translations allows for systems to take advantages of the strengths of translation in both direction. McCarley found that it gave the best results [42] for French and English. Chen and Gey also found that it gave better overall results [12].

The Machine translation method simply uses a machine translation system to trans- late either the document or query. The main drawback, as mentioned earlier, is that it is computational expensive. In situations where there is a large collection of documents or when searching for documents on the web, machine translation is impractical.

Between dictionary based and corpus based translation, corpus based translation typically gives much better performance, as [43] found. However, the creation of parallel corpora is complicated and quite expensive. It can be extremely difficult to find parallel corpora for certain languages or that are large enough to be of use.

Rogati and Yang used a parallel corpus and GIZA++ [49] to determine transla- tion probabilities that can be used for query translation [55]. Their goal was to show that degradation in performance between black box commercial machine translation systems and free material used to create a transparent system is so small that it is more preferable to use the transparent system since it allows the researcher greater control. They were able to achieve good results, an average precision greater than 0.3, on most of their tests even when using a pivot language for translation.

Nie et al. introduced a probabilistic model for CLIR that incorporates parallel corpora [47]. They tested on French and English and showed comparable results to machine translation approaches. They also introduced a simple way of gathering a parallel corpus using the web, causing them to believe their approach is more flexible than machine translation.

Because of the cost of machine translation and the difficulties of parallel corpora, bilingual dictionaries are widely used. A bilingual dictionary is a list of words in the source language and their translation(s) in the target language. Optionally, these dictionaries have translation probabilities assigned that allow for disambiguation and weighting.

Levow et al. looked at dictionary based CLIR in great detail [36]. They made the conclusion that CLIR is more complicated than just translation and retrieval. They also believe that studies on dictionary based CLIR can help improve corpus based CLIR.

Hedlund et al. built a dictionary based system entitled UTACLIR that works on a variety of language pairs [29]. Because they deal with many European lan- guages, UTACLIR pays special attention to compound words that are abundant in languages like Finish and German. To deal with words that could not be translated they used N-grams for partial string matching.

The main problems with both corpus based and dictionary based translation are coverage and quality. Poor quality corpora and dictionaries can greatly decrease the performance of a system [43]. Coverage relates to out of vocabulary words, or words that are not present in the dictionary or corpus. These words will have no translation, while in some languages that are related this is no problem in other language pairs such as Chinese and English this is a big problem [75]. Because of this there has been considerable research done on automatically or semi-automatically acquiring parallel corpora or bilingual lexicons.

Some of the most prominent research done in this area is by Resnik and Smith [54]. They looked at using structural information from HTML to determine pairs of bitext web pages. Their method used search engines and a web spider to determine possible document pairs. Then, they used structural information and a content based similarity measure, for when two pages have different structure, to determine correct pairs.

While CLIR has made great advances in recent years it is still a little behind mono- lingual retrieval. Typically, the results are not as a good as monolingual results. In addition, acquiring lexicons and parallel corpora still remain a bit of a stumbling block especially for minority languages. In the future, we can expect to see even more research exploiting the world wide web. Finally, after CLIR has reached the level of monolingual IR, there is still the problem of how to present the informa- tion to the user. Not all users will have the ability to read the documents they retrieve. Because of this, we expect to see an increased amount of research on fast and reliable machine translation.

There are numerous conferences and workshops on MIR. Some of the more prominent conferences include ACM SIGMM and the International Conference on Image and Video Retrieval. In addition there are typically special tracks in multi- media conferences, computer vision conferences, etc. dealing with MIR.

With the two needs for MIR systems in mind, this section will continue as follows. First, we will present a look at the current research being done in music retrieval. Next, we will look at the research done on image retrieval. Then, we will look at research done on video retrieval. Finally, we will talk about the future of MIR.

In the past 5 years there has been an explosion of music made available through services such as iTunes, Napster, eMusic, etc. Even the most casual user is quickly acquiring gigabytes of music data on their computers. And there is easily petabytes of available data on the Internet. Because of this, music retrieval is a hot topic.

One noticable approach to music IR is to borrow from research in text IR. The previously mentioned research by Pickens et al. used the standard text IR approach of language modeling [51]. Uitdenbogerd and Zobel built an architecture using n-grams and approximate string matching [67]. They found that using melody information was enough for practical systems and that each of the methods, n-grams and approximate string matching, worked well for certain types of music data.

Image retrieval really started in the 1970s with research done by researchers in computer vision and database management [56]. In these early days and up until the last 15 years or so, the predominant method for searching was to first annotate each image in the collection with text and then use standard text IR methods, such as [11]. Recently, as with the other areas in multimedia IR, content based retrieval is being heavily researched.

Corridoni et al. looked at retrieving images based on color semantics, such as warmth, accordance, contrast etc. [14]. The system allowed users to give certain color semantics and find images that match. Kato et al. developed a system that takes a sketch done by the user and finds that image and others similar [31]. Bujis and Lew developed the imagescape application that also allows for users to sketch in images and find images similar to it [4].

Natsev et al. used multiple signatures per image to help in computing the similarity between the given image and the images in the database [46]. They found that this approach found more semantically accurate results than traditional methods. Chang et al. showed that statistical learning methods help improve the performance of visual information retrieval systems [10]. They found that they needed to introduce new algorithms to deal with sparse training data and imbalance in the type of training data. Rui et al. added relevance feedback to their MARS system to allow the user to guide the system in order to improve the search results [57]. Tieu and Viola created a framework that uses many features and a boosting algorithm to learn queries in an online manner [66]. They were able to achieve good results with only a small amount of training data, because they used selective features.

Recently, television shows, movies, documentaries, etc. have become available for download from a varying number of sites. In addition digital video and home editing is becoming the norm. Video retrieval aims to help aid the user in finding the video they seek, whether it be a full video or just a scene.

Like image retrieval some of the earliest approaches where to annotate video data and use standard IR techniques. This is still being used in modern day online video systems, such as YouTube and Google. However, with growing collections that are automatically collected from broadcast or other means annotation is impossible. As such, automatic techniques are needed. Wactlar et al. created a terabyte sized video library [70]. They used automatically acquired descriptors for indexing and segmentation.

Researchers have also tried to mimic text IR techniques in the video domain. Sivic and Zisserman made analogies between text IR and video IR [60]. Their goal was to create a fast system that works as well on video as Google does on text. They used the analogy in every facet by doing such things as building a visual vocabulary and using stop list removal. They found that while there are still some problems the analogy to text IR worked well and leaves them with future research possibilities.

Video retrieval involves such tasks as content analysis and feature extraction [1]. Aslandogan and Yu also point out that one of the most important parts of video retrieval is segmentation or partitioning [1]. Zhang et al. used multiple thresholds on the same histogram to detect gradual transitions and camera breaks [74]. Gunsel et al. looked at using syntactic and semantic features for unsupervised content-based video segmentation [28].

Naphide and Huang used a probabilistic framework to map low level features into semantic representations [45]. The semantic representations were then used for indexing, searching and retrieval. Snoek et al. developed a semantic value chain that extracts concepts from videos [62]. They used a 32 concept lexicon and were able to achieve very good performance in the 2004 TREC Video Track.

Browne and Smeaton incorporated various relevance feedback methods and used object-based interaction and ranking [2]. Yan et al. used negative pseudo-relevance feedback for the 2002 TREC Video Track [72]. They found that this approach increased performance over standard retrieval. Yan and Hauptman introduced a boosting algorithm called Co-Retrieval for determining the most useful features [71].

Gaughan et al. built a system that incorporates speech recognition and tested in an interactive environment [24]. Girgensohn et al built a system focused on the user interface and used story segmentation with both text and visual search [26]. Their system was one the best at TRECVID.

As the amount of available multimedia data continues to grow, the need for precise MIR systems will grow also. Currently, the main push across all areas of MIR is on content based retrieval, which uses the semantics of the image, video, or audio. As the underlying algorithms improve and the semantics of audio, images, and video are better understood, the precision and usefulness of the systems will also greatly improve. In addition to improving precision, user satisfaction must be taken into account. To this end, the future of MIR will be rely greatly on strides made in affective computing.

Semantic information retrieval tries to go beyond traditional methods by defining the concepts in documents and in queries to improve retrieval. In the previous section on multimedia information retrieval, we saw that there is a current trend toward content based, or semantic, retrieval. In a similar manner semantic based information retrieval is the next evolution of text IR.

Some of the earliest work on semantic based IR was done by Raphael in 1964 [53]. He built the SIR system which broke down different queries/questions into different subroutines for processing. In a similar vein to Raphael, Li et al. looked at using semantic information for learning question classifiers [38].

Researchers have been bridging research done in semantic based IR and tradi- tional natural language processing research fields. Li et al. used multiple informa- tion resources to help measure the semantic similarity between words [39]. Varelas et al. looked at semantic similarity methods based on WordNet and how they have applications to web based information retrieval [69].

The main methods for accomplishing semantic based IR are ontologies, seman- tic networks, and the semantic web. Ontologies and semantic networks can bring domain specific knowledge that allows for better performance. The semantic web, which has been a big buzz word for the past years, promises to bring semantic information in the form of standardized metadata.

This section will continue as follows. First, we will take a look at how ontologies are being used in IR. Next, we will look at research that has used semantic maps or networks. Then, we will look at the semantic web. Finally, we will talk about the future of semantic based information retrieval.

One common form of semantic information used in information retrieval is ontolo- gies. Ontologies represent knowledge by linking concepts together and typically results in hierarchical classification. Khan et al. used an ontology model to gen- erate metadata for audio and found an increase in performance over traditional keyword approaches [32]. Gomez-Perez et al. used an ontology for a legal oriented information retrieval system [27]. They found that the ontology helped guide the user in selecting better query terms. Soo et al. used an ontology as domain specfic information to increase the performance of an image retrieval system [63]. Cesarano et al. used an ontology to help categorize web pages on the fly in their semantic IR system [9].

The semantic web opens a realm of new possibilities for web oriented information retrieval. Shah et al. described an approach for retrieval using the semantic web [59]. They developed a prototype system that allows for users to annotate their queries with semantic information from a couple of ontologies. Using this extra information they were able to significantly increase the precision over standard text based methods. As with other semantic information, semantic web technology can help describe domain specific information that can help improve results. Mukherjea et al. used a semantic web for biomedical patents for an information retrieval and knowledge discovery system [44]. Yu et al. looked at bringing the power of the semantic web to personal information retrieval using web services [73].

One of the main problems with the semantic web is the need for annotation. However, research such as [33], [16] and [17] is working on automatic annotation methods. Dingli et al. looked at unsupervised information extraction techniques to create seed documents which are then used to bootstrap the learning process [17]. Dill et al. built the SemTag system that was designed to automatically tag large corpora with semantic information [16].

There are a few problems facing semantic based IR. The first is the availability of semantic information sources. In English, this is not so much of a problem, but in other languages like Chinese, semantic resources are still scarce. The second prob- lem is that, typically, algorithms dealing with semantics are much slower than the standard IR algorithms. In the future, as researchers in natural language processing progress in their own research on semantics these problems may not be so big. If the

This paper presented a survey of some of the areas of advanced information retrieval. We focused on cross-lingual information retrieval, multimedia information retrieval and semantic-based information retrieval. These three represent some of the most active areas of research in information retrieval. All of the presented areas have made great progress and are important for the future.

However, currently IR systems are designed to achieve high recall and precision, which is of course desired, but neglect user satisfaction. As the researchers in the multimedia information retrieval field have come to find out, future systems must make user satisfaction one of their top priorities. To this end, in the future we believe that affective computing will be a necessity for all areas of information retrieval.

Researchers in both information retrieval and affective computing see this need. Picard, one of the more important people in affective computing, gave many uses for affective computing including information retrieval [50]. Dalrymple and Zweizig performed an evaluation of information retrieval systems with respect to user sat- isfaction [15]. This type of research and future integration of ideas from affective computing are needed to help make IR systems human-centered.

