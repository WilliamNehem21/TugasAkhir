Abstract While dealing with real world data for classification using ANNs, it is often difficult to determine the optimal ANN classification model with fast convergence. Also, it is laborious to adjust the set of weights of ANNs by using appropriate learning algorithm to obtain better classification accuracy. In this paper, a variant of Harmony Search (HS), called Global-best Harmony Search along with Gradient Descent Learning is used with Functional Link Artificial Neural Network (FLANN) for classification task in data mining. The Global-best Harmony Search (GbHS) uses the concepts of Particle Swarm Optimization from Swarm Intelligence to improve the qualities of harmonies. The problem solving strategies of Global-best Harmony Search along with searching capabilities of Gradient Descent Search are used to obtain optimal set of weight for FLANN. The proposed method (GbHS-GDL-FLANN) is implemented in MATLAB and compared with other alternatives (FLANN, GA based FLANN, PSO based FLANN, HS based FLANN, Improved HS based FLANN, Self Adaptive HS based FLANN, MLP, SVM and FSN). The GbHS-GDL-FLANN is tested on benchmark datasets from UCI Machine Learning repository by using 5-fold cross validation technique. The proposed method is analyzed under null-hypothesis by using Friedman Test, Holm and Hochberg Procedure and Post-Hoc ANOVA Statistical Analysis (Tukey Test & Dunnett Test) for statistical analysis and validity of results. Simulation results reveal that the performance of the proposed GbHS-GDL-FLANN is better and statistically significant from other alternatives.

Data mining is the process of identifying novel, understand- able and previously unknown patterns in data which helps in decision making. Most tricky and challenging decision making processes in day to day human life is classification, which helps to make decision from past experience. In data mining, the Classification is defined as a variety of data analysis process that can be used to assign important classes to unknown patterns. Classification task predicts definite class labels and constructs a model based on the training dataset which is used to classify anonymous patterns.

A hybrid functional link artificial neural network (HFLANN) based on genetic algorithm (GA) for optimal input feature selection by using functionally expanded selected features is proposed by Dehuri et al. [40] which address nonlin- ear nature of classification problems. Through experimental results, the HFLANN is proven to be better in optimal set feature selection as compared to RBFN and FLANN with back propagation learning.

A comprehensive survey on FLANN is made and an efficient PSO based back propagation learning is proposed by Dehuri and Cho [41]. In this paper, the basic concept of FLANN, associated basis functions, learning schemes and development of FLANNs over time are discussed. Also the authors have used PSO based back propagation learning scheme on Chebyshev-FLANN for classification and the pro- posed method is proved to be better as compared to FLANN by testing with benchmark datasets.

An efficient FLANN for stock price prediction of the clos- ing price of US stocks is suggested by Patra et al. [42] and found to be better in performance in terms of more accurate predictions of stock. In this paper, a FLANN with trigonometric functional expansion (Trigonometric-FLANN) is used and shown to be better result as compared to MLP-based prediction model.

A FLANN based prediction model for prediction of causing genes in gene diseases is proposed by Sun et al. [43]. In this study, three classifiers (i.e. MLP, SVM, FLANN) have been implemented and compared. The performance of the FLANN classifier is found to be better over MLP and SVM. For better prediction of the stock market indices, Chakravarty and Das [44] have proposed a Functional Link Neural Fuzzy (FLNF) Model and compared with FLANN based prediction model in terms of root mean square error. The simulation results show that the FLNF performs better over FLANN. Also the authors have addressed the issue of falling in local minima in case of back propagation learning

A classification method based on FLANN is achieved by Majhi et al. [45] for classification of online Indian customer behavior and the proposed FLANN model found to be superior in classification accuracy than other statistical approach (discriminant analysis). Also authors have suggested to use psychographic and cultural information for further improvement of the proposed method.

An accurate hybrid FLANN classifier (HFLNN) is pro- posed by Dehuri and Cho [46] by selecting an optimal subset of favorable input features. This is achieved by eliminating fea- tures with fewer or no predictive information. The proposed method is found to be better as compared to FLANN and RBFN.

Forecasting of stock exchange rates is achieved with Genetic algorithm (GA) based FLANN model by Nayak et al. [47] and proposed method is compared with MLP, GA based MLP and GA based FLANN models. The authors have claimed that the FLANN-GA is found better in almost all cases.

Bebarta et al. [48] have implemented few variants of FLANN model (Power FLANN, Legendre FLANN, Cheby- shev FLANN and Laguerre FLANN) for forecasting stock price index and performances are measured in terms of stan- dard deviation error, squared error, etc. All the four proposed methods are implemented and found to be simple and efficient to predict the various Indian stock data.

A Bat inspired optimization based FLANN classification method is proposed by Mishra et al. [49]. The method is com- pared with FLANN and hybrid PSO based FLANN classifica- tion method. In this paper, bat algorithm is used to adjust the weights of the FLANN efficiently which results in high accu- racy for classification. The simulation results show that the proposed method outperforms FLANN and hybrid PSO based FLANN classifiers.

Mishra et al. [51] have developed MLP, FLANN and PSO- FLANN classification models for classification of biomedical data. In this paper, to extract important input features, an effi- cient dynamic classifier fusion (DCF) is proposed along with principal component analysis (PCA) scheme. After extraction of optimal input features, LMS classifier is performed along with PSO based Back propagation learning algorithm. Although MLP is a traditional ANN, surprisingly, in this study, PSO based Back propagation learning-MLP is found to be better as compared to FLANN and PSO-FLANN.

compared with MLP, support vector machine (SVM), RBFN, FLANN with gradient descent learning and Fuzzy Swarm Net (FSN) model. Initially, IPSO is used to optimize the weight value of Functional link ANN and finally, functionally expanded (using trigonometric basis functions) input patterns are supplied to FLANN for classification. The proposed method is found to be simple and better as compared to MLP, SVM, FLANN with gradient decent learning and FSN. Mili and Hamdi [53] have developed a good number of FLANN based classifier such as PSO based FLANN, GA based FLANN and Differential Evolution (DE) based FLANN for classification task. These classifiers are compared and tested with various expansion functions. In their study, the authors have concluded that the proposed methods are performing better in terms of accuracy and convergence as

Naik et al. [55] have designed a Honey Bee Mating Optimization (HBMO) based learning scheme for FLANN classifier and compared with FLANN, GA based FLANN and PSO based FLANN classifiers. The proposed method mimics the iterative mating process of honey bees and strate- gies to select eligible drones for mating process, for selection of best weights for FLANN classifiers.

learning methods which learns from past data in Classification tasks in Data mining. Almost all the higher order ANNs (HONNs) including functional link higher order ANN (FLANN) are sensitive to random initialization of weight and rely on the learning algorithm adopted. Although a selection of efficient learning algorithm for HONNs helps to improve the performance, initialization of weights with optimized weights rather than random weights also plays important roles in efficiency of HONNs.

Vasebi et al. [141], Coelho and Mariani [142], Ceylan and Ceylan [143], Geem [144], Sinsupan et al. [145], Gao et al. [146], Ceylan et al. [147], Coelho et al. [148], Sui et al. [149], Sivasubramani and Swarup [150], Geem [151], Khorram and Jaberipour [152], Pandi and Panigrahi [153], Sivasubramani and Swarup [154], Chatterjee et al. [155], Afshari et al. [156], Sirjani et al. [157], Sirjani and Mohamed [158], Sirjani et al. [159], Javaheri and Goldoost-Soloot [160], Mukherjee [161]

Basically, a better learning algorithm helps the ANN model for fast convergence. Further, a use of competitive optimiza- tion technique can, not only improve the convergence of a learning algorithm, but also enhance accuracy of an ANN based classifier. In the next subsection, a new meta-heuristic optimization technique, known as Harmony Search technique and its variants have been described.

The Harmony Search (HS) [69] is a meta-heuristic algorithm inspired by musical process of searching for a perfect shape of harmony. The algorithm is based on natural musical processes in which a musician searches for a better state of harmony by tuning pitch of each musical instrument, such as jazz improvisa- tion. The music improvisation by pitch adjustment in the Harmony Search is analogous to local and global search process to find better solution in any optimization techniques.

Basically, the harmony memory (HM) is a group of pre- defined number of solution vectors similar to a population of particle in PSO or chromosome in GA. Initially HM is initial- ized with random solution vectors and gradually, solution vectors in HM are improved by using Step-3 of harmony search procedure known as HM improvisation step. This step is entirely controlled by the parameters: Harmony Memory Consideration Rate (HMCR), Pitch Adjustment Rate (PAR) and Bandwidth (bw).

Where  u = (u1; u2 .. . uL ),   e = (e1; e2 .. . eL)  and d = (d1; d2 ... dL) are the vector which represent sets of functional expansion, set of error and set of error

In GbHS, it eliminates the difficulties of selecting appropri- ate bandwidth (bw) by directly adopting the current best pitch (Global best) from the harmony memory and adjusting other solution vectors to improve their qualities in the HM without pitch adjustment step. This process of HM improvisation is analogous to selection of local best (LBest) and global best (GBest) particle (In PSO) from population based on which, changing of position of particles is obtained. The performance of GbHS is found to be significantly better than HS and IHS in terms of quality of solution and convergence rate.

In this section, we have considered four FLANN classifiers with Gradient descent learning based on four variants of Harmony Search algorithm. In this paper, a deep experimental analysis on Harmony Search algorithm and its different vari- ants (i.e. Improved HS, Global-best HS and Self Adaptive HS) has been done and an attempt has been made to use the problem solving strategies of these variants to improve perfor- mance of FLANN classifiers. Here the objective is to select the best set of weight (Weight-set) from a set of randomly selected weight-sets (Population) for FLANN model for classification task. This paper mainly focused on Global-best HS based Gradient Descent Learning-FLANN model (GbHS-GDL- FLANN) for classification and the objective is to investigate the performances of Global-best HS (GbHS) to enhance clas- sification accuracy of FLANN classifier as compared to basic HS (HS), Improved HS (IHS) and Self Adaptive HS (SAHS). Also, the performance of GbHS-GDL-FLANN is compared with other meta-heuristic algorithm (GA based FLANN and PSO based FLANN) to get generalized performance. The pseudo codes developed during implementation of proposed GbHS based Gradient descent learning FLANN (GbHS- GDL-FLANN) are presented in Section 5.1. The simulation results and the comparisons of performance of these hybrid FLANN classifiers (FLANN, GA-GDL-FLANN, PSO-GDL-FLANN, HS-GDL-FLANN, IHS-GDL-FLANN, GbHS-GDL-FLANN and SAHS-GDL-FLANN), MLP, SVM

After evaluation of fitness values for each weight-set in HM, the HM goes through HM improvisation process based on Global-best Harmony Search (GbHS). During this, the parameters: HMS (Harmony Memory Size), HMCR (Har- mony Memory Consideration Rate), PAR (Pitch Adjustment Rate) and bw (Bandwidth) are set and based on which MCP (Memory Consideration Probability), PAP (Pitch Adjustment Probability) & RP (Random Probability) are computed (Algo- rithm 1). Basically, the Harmony Search procedure is governed by these parameters.

Algorithm 2 represents pseudo-codes for Harmony Memory improvisation in which, initially, among all weight-sets (har- monies) in HM, some are randomly selected with a probability of MCP (Memory Consideration Probability) and included into New Harmony Memory (NHM). Here the objective is to migrate some weight-sets (harmonies) from HM into NHM without any changes on them, which serve as new harmonies. For the improvement of weight-sets through pitch adjustment, some weight-sets are selected randomly from HM with a prob- ability of PAP and then they are adjusted based on the variable distance bandwidth (bw) which is similar to the local search method with a step size bw. Similarly, with a probability of Random Probability (RP), some weight-sets are selected ran- domly and added to NHM by suitably adding or subtracting a random value on it. Although Global-best Harmony Search is suggested to bypass the pitch adjustment step, better result also can be obtained through pitch adjustment of harmonies.

After the rejection of the null-hypothesis from Friedman test in Section 8.1 and Holm procedure in Section 8.2, in this section, the Post-Hoc ANOVA Statistical Analysis has been carried out by using Tukey Test [270] & Dunnett Test [271] to get generalized statistic on the performance of all classifiers.

Landa-Torres I, Manjarres D, Gil-Lopez S, DelSer J, Salcedo- Sanz S. A preliminary approach to near optimal multi-hop capacitated network design using grouping-dandelion encoded heuristics. In: IEEE international workshop on computer-aided modeling analysis and design of communication links and networks; 2012a.

Manjarres D, Landa-Torres I, Gil-Lopez S, DelSer J, Salcedo- Sanz S. A heuristically-driven multi-criteria tool for the design of efficient open WiFi access networks. In: IEEE international workshop on computer-aided modeling analysis and design of communication links and networks (CAMAD); 2012b.

