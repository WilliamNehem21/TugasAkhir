We consider the model checking problem for FLC, a modal fixpoint logic capable of defining non- regular properties. This paper presents a refinement of a symbolic model checker and discusses how to parallelise this algorithm. It reports on a prototype implementation of the algorithm in Glasgow Parallel Haskell (GpH) and its performance on a cluster of workstations.

Nowadays, model checking for (temporal) logics is commonly accepted as one of the key methods in verification. A major limitation to the usefulness of model checking for verification purposes is the state space explosion problem. To tackle this problem one employs symbolic methods [3] that work on process descriptions directly and usually achieves better performance there.

The implementation of GpH models a virtual shared heap, i.e. all program variables are accessible as if residing on the same processor. The runtime system arranges for automatic data transfer between processors if this is not the case. A virtual shared heap facilitates the development of architecture independent programs that are able to exploit large numbers of processors. In contrast, the number of processors in physical shared memory machines is bounded by hardware constraints, usually to a few dozens. For applications wanting to exploit only small amounts of parallelism such a simpler shared

One important language feature for this application is the FFI interface provided in Haskell. This enables us to use existing, tuned C-code for basic BDD operations. However, since the Long BDD library [8] that we use, man- ages its own heap, interaction between the Haskell heap and the C heap is necessary. On Haskell side this means that a BDD is represented as a For- eign Object, i.e. a data-structure that is constructed outside the Haskell heap. This data structure is represented by a pointer into the C heap and a finaliser routine that is executed once it is not used from the Haskell heap anymore.

The main advantage of this source of parallelism is its coarse granular- ity, turning the entire test set generation into a parallel thread. The form of parallelism exploited here is producer-consumer (or pipeline) parallelism, where producer and consumer work in parallel on the same data structure. This parallelism could be combined with the massive amount of divide-and- conquer parallelism in version PMC1. Additionally, we could also compute the test set itself in parallel. This is possible since there are finitely many functions under which the closure of a set of BDDs needs to be computed.

One important goal in our design of the parallel algorithm is to achieve scalability, i.e. to ensure that a larger number of processors than those cur- rently used can be exploited without changes to the code. Unfortunately, this property also triggers more data transfer between heaps than necessary. An implementation that uses laziness to avoid these operations unless parallelism is definitely exploited should improve the one processor performance, but is tricky to implement. Hardwiring such explicit order of evaluation into the code is the main reason for its complexity in an otherwise simple model of parallelism. We therefore now work on an approach where the structure of a Foreign Object is extended with a marshalling function, that will be auto- matically started as part of the graph packing algorithm used by the runtime- system to transfer computations between processors. This ensures that data transfer between heaps is only done when needed for parallel execution, and eliminates this complexity from the parallel GpH code.

