This paper is part of a comprehensive approach to debugging for functional logic languages. The basic idea of the whole project is to trace the execution of functional logic programs by side effects and then give different views on the recorded data. In this way well known debugging techniques like declarative debugging, expression observation, redex trailing but also step-by-step debuggers and cost center oriented symbolic profiling can be implemented as special views on the recorded data. In addition, creating new views for special debugging purposes should be easy to implement. This is where the contribution of this work sets in. We describe how the recorded data is interpreted and preprocessed in order to yield an extremely simple yet versatile interface to base the different views on. Using this interface, formulating the basic functionality of declarative debugging, for example, is a matter of a few lines.

It is the basic credo of declarative programming that abstracting from certain as- pects of program executions greatly improves the quality of the written code: Typ- ical sources of errors are principally omitted, like issues of memory management, type errors and multiple allocation of variables. The program is much nearer to the logic of the implemented algorithm than to its execution. This makes code much more readable, comprehensive and maintainable.

Visualization of Computation A straightforward approach to search bugs is to represent the actual program execution in a human readable form and to pro- vide tools to comfortably browse this representation. Such tools, beginning with step-by-step debuggers, have been developed for many languages, imperative and declarative alike. These tools normally depend on a specific backend of the sup- ported language and seldom aim at portability. Some very elaborated examples for declarative languages include ViMer [11] for the logic language Mercury [28], Ozcar [21] for the Mozart system 3 , a backend for the language Oz [27] and TeaBag

Value Oriented Debugging approaches based on analyzing what values have been computed by evaluating a given expression within the program are for instance declarative debugging (cf. [26] for logic, [22,23] for functional, [10] for functional logic programming), observations for lazy languages (cf. [15] for functional [5,19] for functional logic languages 4 ), backward stepping and redex trailing (for func- tional languages only, cf. [4] resp. [29]).

7 The other main thread of formal reasoning about functional logic languages is based on [16]. It is still a desideratum to give the missing prove link between the CLN calculus of [16] or one of its further developed successors with the big-step semantics of [1]. A good place to start might be the DN calculus of [13].

In addition to the points above, we also extended the approach of [12] by record- ing information about the actual pattern matching performed while executing the program. This information turns out to be crucial when integrating more of the tools described in Section 1.2. The HAT system of [12] was able to emulate, among others, redex trailing, observations and declarative debugging because these are all value oriented techniques. These techniques are concerned with the denotational mapping between expressions and their values. The other tools mentioned in Sec- tion 1.2 are concerned with operational aspects of the program execution. As it is not possible to reconstruct the state transformation induced by executing the program from the data recorded by HAT, it is not possible to integrate such more operational tools. In our approach, in contrast, the operational behavior of the program can be reconstructed and, thus, at least conceptually, the whole range of tools mentioned can be emulated as special views on the recorded data.

The present paper is concerned with how to provide a simple yet versatile inter- face to the traces of program executions in the functional logic language Curry. It describes on one hand how the traced data is represented in Curry (Section 2) and proposes a much simpler data structure to represent general computations (Sec- tion 3.1). In addition, techniques of how to elegantly obtain and process this data structure are described (Section 3.2). Using this structure it should be easy to im- plement tools like the ones mentioned in Section 1.2, at least as far as the access to run-time data about program executions is concerned.

The basic idea is that representing computations in the framework of functional logic languages can be as simple as categorizing computation steps into a) single step b) subcomputation c) branching. A single step might be further distinguished to be an unfolding, the binding of a variable, the suspending of a computation or perhaps some representation of a side effect. Value oriented techniques are then characterized by subcomputation to the most evaluated form whereas operation oriented tools feature subcomputations to head normal form only. These different kinds of subcomputations can be seen as interpreting the program trace in the light of different evaluation strategies. Computing the most evaluated form is like employing a strict strategy while stopping at head normal form is lazy evaluation.

As mentioned above, this paper is based on two preliminary works. In [9], we have extended a semantics for functional logic languages by the construction of a trace graph. In [7] we have presented a program transformation which writes by side effects information into a file from which a graph in the sense of [9] can be produced.

Argument arrows have a dot at their origin. A node referred to by an argument arrow represents an expression which was an argument of the function represented by the node from whence the arrow came. In the example, the node labeled add is the origin of two argument arrows both pointing to the node _5, which represents a free variable. This corresponds to the expression (add x x) of the example program.

There are application nodes like the ones labeled with main, add or True. These nodes represent the unfolding of the function (resp. application of a constructor) corresponding to the label. Each application node has one position for each argument of the corresponding function (or constructor).

In lazy functional (logic) languages there are possibilities to construct graphs in an elegant way. First, sharing already introduces directed acyclic graphs. For in- stance, both arguments of the tuple introduced by (let x=e in (x,x)) physically refer to the same memory address at run time. But also cyclic graphs can be constructed where recursive let expressions are allowed. For instance, the expres- sion (let ones=1:ones in ones) introduces at run time a structure with a cyclic reference in the heap. Representing graphs in this way has some advantages:

TNode r p par info The node with number r belongs to the computation of path p and has the node with number par as parent. The kind of the node (application, failure, free variable or case, cf. above) is then given in the info part which will not be considered in the following.

Then the building of the graph as a cyclic data structure can be implemented as a function manipulating three of these search structures: 1) a mapping of node refer- ences to the list of their successor references 2) a mapping of argument references to the list of their corresponding node references and their paths 3) one mapping of node references to trace nodes. (The Structure of trace nodes was defined in Section 2.1).

The rules for Successor and RedirectArg only add information to the maps. The last rule contains the recursive let which adds the information of the current trace node to the node map. The elements of these trace nodes depend on the call to cycle on the thus updated map. This ties the loop and makes sure that the result of cycle is a cyclic structure in the heap which directly resembles the trace graph. An elegant definition in this way is only possible in lazy languages.

There are, however, drawbacks to this technique: This definition can only work efficiently if the whole trace fits into memory. This is not to be expected for all applications we would like to be able to debug. Therefore there is an alternative im- plementation to build the trace graph. This alternative implementation represents the graph as a potentially infinite term. Each node is upon demand retrieved from the trace file by side effects. This is comparable to lazy file access by the Curry standard function readFile. The access to the parents, successors or arguments is not possible in constant time as it involves some kind of binary search on the file for each access. But as there are no cycles in the graph, the degree of heap referencing is much lower and therefore trace nodes can become garbage much more often. This ensures that the program will only have parts of the trace graph in memory at each moment.

The definition of computations above allows to generate, combine and process com- putations in a monadic programming style. Computations are a combination of list and state monads. As is well known, list monads are very expressive for non- determinism and a state monad is useful to abstract from information which has to be updated regularly during computations. In our case, this information includes for instance the path for which a given subgraph has to be interpreted. (Cf. the discussion of the path concept above.)

After describing the representation of computations we gave an account of how these representations can easily be generated and processed in a monadic program- ming style. Overall we have shown how different advanced features of Curry can be used to lift the low level information contained in the execution trace to a higher abstraction. Advanced programming techniques work together in order to create a framework in which interpretation for traces can elegantly be formulated. Future work includes giving an overview of the different strategies and views we realized using this framework.

