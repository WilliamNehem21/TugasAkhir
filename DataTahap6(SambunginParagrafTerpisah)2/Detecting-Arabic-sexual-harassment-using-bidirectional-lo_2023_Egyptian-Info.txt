important in the Arab world. The dataset contains 56,245 Arabic tweets between 2019 and 2020 [3]. After many experiments to find the optimal solution to this problem, two methods are combined to extract more features: During the word representation training phase, features are extracted from BiLSTM (Bidirectional Long- Term-Memory) using embedding of words, and the features are extracted from TCN (Temporal Convolutional Network) using the pre-trained FastText.

The framework for this paper can be defined as follows: part 2 is an overview of relates work, and part 3 provides theoretical back- ground information on techniques for classifying sexual harass- ment. Part 4 discusses the approach used in this paper, while part 5 assesses and explains the models used. Finally, part 6 shows the conclusions drawn of implementing the proposed methods.

Extreme Gradient Boosting (XGBoost) is a distributed gradient- boosted decision tree machine learning library that is scalable. It is an open-source software package that implements optimally dis- tributed gradient-boosting machine learning methods under the Gradient Boosting framework. It is just an improvised version of the gradient enhancement algorithm, and its working procedures are nearly identical. XGBoost implements parallel processing at the node level, making it more powerful and faster than the gradi- ent boosting approach. XGBoost prevents overfitting and increases

True-positive (TP) is the number of sexual harassment samples correctly classified, and True-negative (TN) is the number of non- sexual harassment samples correctly classified. False-positive (FP) represents the number of non-sexual harassment samples identified as sexual harassment. A false-negative (FN) result is the number of sexual harassment samples incorrectly classified as non-sexual harassment. The ROC curve shows the actual posi- tive rate (TPR) versus the false-positive rate (FPR) for different thresholds for each binary classifier.

Future work recommends applying stem and stops words extracted from the datasets, not depending on the NLTK library default that uses fixed stop words because each dataset has its own stop words and stemming. Combine another optimization method, e.g., B. Particle-Swarm Optimization with CNN to improve classification accuracy in Arabic text. Discover more deep learning techniques to classify Arabic text.

