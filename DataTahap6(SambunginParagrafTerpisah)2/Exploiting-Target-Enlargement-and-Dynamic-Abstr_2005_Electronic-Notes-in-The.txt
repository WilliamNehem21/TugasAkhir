analysis. Section 3 is dedicated to the related work. Section 4 introduces the outline of our approach justifying and describing algorithms for target en- largements within SAT. Section 5 describes our dynamic under-approximate reachability analysis to compute target enlargements. Section 6 presents our top-level algorithm with some more implementation-level details. Section 7 presents our experimental evidence. Finally, Section 8 concludes the paper with a brief summary, and some possible future works.

In order to decide if a Boolean formula f is satisfied, most solvers adopt variants of the basic Davis-Putnam recursive algorithm. SAT solvers generally operate on problems for which the propositional formula f is specified in Conjunctive Normal Form (CNF). This form is a two-level decomposition: The logical AND of one or more clauses, each of which consists of the logical OR of one or more literals.

With the advent of SAT-based BMC tools a lot of researchers compared SAT-based methods with more traditional BDD-based ones. As different re- searchers agree that the two approaches are essentially complementary, a lot of recent works concentrate on dovetailing the two approaches in a loose or strict fashion. In this section, we review, among these works, the ones more strictly related to our approach.

Gupta et al. [16,17] perform BDD-based reachability analysis by using a SAT procedure within symbolic image computation. They call their approach BDDs at SAT Leaves. More specifically, they use BDDs to represent state sets and a CNF formula to represent the transition relation. Symbolic image of a state set is computed by exhaustive SAT search of all solutions within the space of primary input, present and next state variables. However, rather than using SAT to enumerate each solution all the way down to a leaf, image switches to BDD-based computations at certain intermediate points within the SAT decision tree. This is done as a trade off between space complexity of BDDs and time complexity of full SAT enumeration. In a sense, this approach can be regarded as SAT providing a disjunctive decomposition for image computation into many sub-problems, each of which is handled symbolically using BDDs. As far as SAT performance is improved by BDD learning, Cabodi et al. [9] propose BDD pre-processing by means of over-approximate reachability. The authors show how to translate over-approximate state sets from BDDs to CNF

Another work by Gupta et al. [15] may be considered as a variation of [9] for the BMC case, as they also use over-approximate reachability analysis to constrain the BMC search. A novel idea in their work is the extension to induction-based unbounded verification, where the authors exploit over- approximate information as an additional (non redundant) constraint.

As a way to partition a verification task between a BDD and a SAT engine. We perform a preliminary effort with BDDs, we conclude the task through a SAT solver, working on the solution space left uncovered by BDD pre- processing. In other words, counterexamples are computed (or refuted) partially within the BDD domain and partially within the SAT one.

In the rest of this section we first show how BDD-based target enlargement can be considered in terms of task partitioning between the BDD and SAT tools. Then we introduce some specific optimizations for SAT searches. BDD preprocessing for target enlargement and the overall verification algorithm will be described in the following sections.

The new enlarged start (target) state set is a set of states for which a path from S (to T) exists. BDD-based computation of Se and Te will be discussed in the next section. Let us just consider here the straightforward case of under- approximation by bounded exact reachability, i.e., a few traversal iterations, with Se = FRd and Te = BRd . In this case dF and dB are the depths of the bounded traversals in the forward and backward directions respectively. The enlarged set Se replaces S and the first dF time frames in the combinational unrolling. Dually for Te. More formally:

Let us examine the substitution on the particular case of BMC. We choose it for sake of simplicity, and we show that a given BMC problem of bound k can be solved, with target enlargement, as a BMC problem with shorter bound. Similar formulations can be done with unbounded model checking. The following proposition holds.

Over-approximate reachability has been proposed in several works as an ab- straction technique, with the aim of improving capacity and scalability. On the contrary, under-approximate techniques have received less attention in formal verification in recent years. Under-approximation was specifically ad- dressed in the BDD sub-setting work by Somenzi et al. [21]. Many works then followed the partitioning and guided search paradigms [8,22,12] where a difficult reachability task could be faced by case splitting or focusing on a

The former strategy is a very straightforward option, particularly suited for symbolic traversals characterized by affordable initial iterations. With the latter one we tackle BDD explosion more aggressively. As BDD blow up is often related to the number of support variables, i.e., the variables BDDs depend on, we aim at reducing the support of state sets, with a possibly minimal impact on the number of represented state sets.

In general, forward reachable state sets depend on all variables since it is the first iteration, whereas backward state sets follow the so called Cone-Of- Influence of the property. In practice the number of support variables (and the BDD size) of Te grows for growing values dB. The good choice is a trade- off between BDD size and number of state variable we are able to further constrain in successive SAT processing.

We work within the inner loop of a BDD-based traversal. Whenever a state set violates a predefined threshold (BDD size and/or support size), we dynamically operate sub-setting. High density as introduced in [21] aimed at clipping a BDD so that a minimal number of minterms (i.e., states) was re- moved from it. Density was defined as the ratio number between the minterms in the subset and in the original BDD. Pruning was done recursively, so no particular care was taken at reducing the amount of variables in the support. The sub-setting technique we propose is a compromise between support reduction and high-density. It can be used either for BDD super-setting or sub-setting, as the basic step is variable quantification. If sub-setting is our goal, we universally quantify a variable so that a minimal number of minterms are removed from the original BDD. Super-setting would be achieved in a dual way, by adopting existential quantification and minimizing the newly

Due to the above mentioned dynamic scheme, time and memory efficiency of variable selection is a key issue, as the introduced overhead should by neg- ligible within the overall traversal process. A naive approach consists of first computing variable abstraction for all variables, then selecting the one with best weighted size-density benefit. This can be very expensive, as it means to compute a new (possibly larger) BDD for each variable.

As a result, a best density abstraction variable can be computed in linear time without generating any new BDD. This does not take into account the BDD size of the result. So we add an extra step where we actually compute the abstraction for the topmost variables, after ordering them by estimated minterm density. Possible BDD blow up is avoided by a size threshold con- trolling universal abstraction as a try-and-abort operator.

Initial BDD-based traversals compute enlarged sets Se and Te. SAT verifi- cation iterates over BMC and inductive steps until either a counterexample is found by BMC or an inductive step is unsat. If a counterexample is found, it is extended by a prefix and a suffix, computed within Seand Te. If verification passes, the procedure returns the depth of termination.

Industrial designs: The IBM Formal Verification Benchmark Library [18]. This suite includes 75 circuits in Blif format. They contain from 95 to 917 memory elements. For each of them, there is a unique output (formula 1) which also indicates the property to check.

