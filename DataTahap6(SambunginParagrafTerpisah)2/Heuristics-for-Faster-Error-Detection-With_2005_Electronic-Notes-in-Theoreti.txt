In black box testing we do not have information on the internal structure of the SUT. In this paper we show that several simple coverage metrics can still be helpful to reveal errors. Since the coverage of the states of the system cannot be measured, the coverage of the specified behaviour (that is, the interaction with the environment) of the system is measured.

Exploration testing [9], as we call our testing method, is automated. When the expected behaviour of the system (in the form of a deterministic LTS) is given to a test engine, it explores the LTS beginning from its initial state. In each state the engine decides whether to send an input, listen to an output, or reset the system and restart the testing. Only those inputs and outputs that are possible in the current state of the LTS may be sent to or accepted from the SUT.

The main objective and contribution of this paper is to introduce and compare heuristic algorithms which aim revealing errors quickly; and show how the problem of designing a specification coverage aided test selection algorithm can be split in parts. The introduced algorithms are compared by running tests against conference protocol systems consisting of two and three conference protocol entities. (The conference protocol basically provides a chat service with multiple chat rooms.) The number of test steps needed for finding an error is measured and compared. Errors are infiltrated to the systems by replacing one correct protocol peer by a mutated one.

Correct and mutated protocol entities were implemented at the University of Twente [4]. The implementations have been tested before in [1,14], but with a different test setup. While in the mentioned publications a single conference protocol entity is tested, we test the service provided by systems of two and three clients. With this setup we aim to capture the nature of a truly concurrent and reactive system: the system is often able to read inputs

The transition tour method [13] tries to execute every transition in the specification at least once. This approach is more practical (for example, incomplete specifications are allowed) than the methods mentioned earlier, although it tests less. Our approach resembles this method, but in addition to covering transitions we can use simultaneously coarser coverage criteria to guide test runs. In [10] Lee et.al. introduced transition tours for communi- cating finite state machines. They presented a guidance method that tries to execute every transition in each state machine separately, instead of the full state space of the specification. This is an interesting and justified cover- age criteria which, unfortunately, can not be handled by the coverage classes presented in this paper.

In Section 2 the LTSs and coverage criteria are formalised. The test engine architecture is presented in Section 3 to show the context in which the test guidance algorithms operate. Section 4 contains the main issue of the paper. Firstly, the concepts of step evaluation, state evaluation and evaluation order, which are the basic building blocks of our heuristic algorithms, are presented. The rest of the section is dedicated to the description of the heuristic algo- rithms to be compared. The test setup for the comparison and the results are presented in Section 5.

In this paper we use three coverage classes: 111, 001 and 010. Intuitively, to cover an LTS according to the coverage 111 every transition of the LTS must be executed. The coverage 001 demands that every state be arrived in, and 010 that every action appearing in the transitions of the LTS appears also in the executed transitions.

If the input action is accepted, which in our setup means that sending the command to the stdin pipe of the UI process was successful, the explorer executes the corresponding transition in the specification. Executions change the current state of the specification, coverage values, and get recorded in the executed trace. If the input action is refused, the explorer executes the refused input action in the specification, if possible. The ability to detect the refusal of

When an output action is received from the adapter module, the explorer verifies from the specification module that the received action can be executed in the current state. If not, it announces an illegal response error and stops testing. Otherwise, the action is executed similarly to an input transition execution.

beginning from the current state is calculated. The names of the algorithms are inspired by the game-like approach to a test run. In every turn the test engine decides if it makes a move (sends an input to the SUT) or lets the SUT make a move (listens to the response of the SUT). The move is the execution of a transition: each move increases the score by the value of the transition (given by the step evaluation function). The player algorithms are greedy: they try to gather as big a score as possible during the steps they calculate.

This is a slightly enhanced random heuristic. In each state a decision about the next step is made according to the following rules. Output is listened to if there are no inputs or if a correct output certainly causes a new transition to be covered. Otherwise, if a new transition is covered by executing an input action, then the input is sent. If not, then if it is possible that the SUT gives output that covers a new transition, then output is listened to in a state s

In the pessimistic player algorithm the probability estimation returns a map P where P[t] = 1 for the output transition t which has the minimal desirability. For the other output transitions t', P[t'] = 0. Thus the minimal desirability of the available output transitions is stored in ovalue.

Similarly to the pessimistic player, there are two versions of the adaptive player algorithm. One uses the step evaluation function (2) and the other the function (3). The only difference between the adaptive and the pessimistic player is the way they estimate probabilities of outputs that are available at the same time.

This algorithm resembles the player algorithms: both are greedy and use a bounded lookahead to the LTS to make decisions. There are still remarkable differences. This algorithm looks ahead only when there are no uncovered transitions in the current state and the look ahead is no deeper than the number of steps to the nearest desired state. The players look ahead before every step and always to the same depth. Furthermore, this algorithm is optimistic: when there is an uncovered output in the current state, the output is always listened to if there are no uncovered input transitions. The players are optimistic only when input actions are considered. During the test run, the adaptive player becomes more and more pessimistic about the output actions which have not been executed. The pessimistic player always expects the least desirable output to be received. Lastly, this algorithm is nondeterministic, whereas the players are deterministic.

For simplicity it is specified that messages sent to conferences are never lost. That is, everyone present in the conference, except for the sender itself, receives the sent messages. Therefore, sending messages is limited so that we can be sure whether or not each user should receive them. For example, when a user starts joining a conference, nobody can send messages to the conference before the user has received his prompt. Otherwise, we would not know if the user should receive the messages sent during the joining or not.

We used nine different mutants of the client processes in the test setups. Every mutant was tested 32 times with each nondeterministic and once with each deterministic heuristic algorithm. The mutants are identified by the numbers explained in [5]. The mutants used in the results are 14, 17, 19, 21, 22, 24, 27, 28 and 60. Without going into the details of the mutated behaviours, their errors ranged from inability to receive certain PDUs to errors in updating internal data structures and errors in sending. The mutants were chosen so that they produce erroneous service. Some other mutants satisfied the specification.

Algorithms with step evaluation functions that included the coverage of action names (010) performed better than algorithms with simpler functions. Player algorithms with the function (3) were always better than with the function (2). The performance of the state space evaluation algorithm was also enhanced when its step evaluation function was switched from (5) to (6). But even with the enhancement, the state space evaluation algorithm performed poorly.

