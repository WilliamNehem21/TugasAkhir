In recent years, software, system and enterprise architecture have become estab- lished disciplines in both industry and academia. Architecture models may aid the communication between various stakeholders. Architecture models may also aid the understanding of the complex systems they represent. One important part of understanding is to be able to infer new knowledge from a model, i.e. to be able to analyze the models. As an example, by considering an architecture model over a set of enterprise services and their relations, an observer may infer that these services will be unable to interoperate (perhaps due to a protocol mismatch). The model does not explicitly state that the services cannot interoperate, but by using a set

This paper builds on previous research within the field of architecture analysis where architecture models are analyzed using a formalism based on Bayesian statis- tics [9] [10]. This approach allows the analysis of various system properties, such as the interoperability, information security and the availability of software systems. In this paper we will use enterprise service interoperability analysis as a running example.

In order to specify the joint distribution, the respective conditional probabilities that appear in the product form must be defined. The second component P de- scribes distributions for each possible value xi of Xi, and pa(Xi) of Pa(Xi), where pa(Xi) is the set of values of Pa(xi). These conditional probabilities are represented in matrices, here forth called Conditional Probability Matrices (CPMs). Using a Bayesian network, it is possible to answer questions such as what is the probability of variable X being in state x1 given that Y = y2 and Z = z1.

The abstract model tells us what information we need to find in order to conduct analyses of different variables. Once this information is collected it is specified in the model, thus creating an instantiation of the abstract model. These instantiations are called concrete models.

Services are independent building blocks that collectively represent an appli- cation environment, much like components of a software system. However, services possess a number of qualities that components lack, e.g. the complete autonomy from other services. This allows a service to be responsible for its own domain. Fur- thermore, services are typically limited in their scope to support a specific business function or a group of related functions [3] Each service at an enterprise need to be of high quality, i.e. every service needs the foundation to be able to interoperate with other services. Two aspects affecting service quality are the correctness and availability of a service.

Services have service descriptions. These are used for advertising and de- scribing the service capabilities, behavior, quality, and its interface [16]. The service descriptions have four attributes: understandability, completeness, correctness, and existence in service repository. These attributes all affect the quality of the service being described.

Services use a service bus, often referred to as an enterprise service bus (ESB), as a communication medium. The service bus is a middleware-like solu- tion to manage message and transaction traffic [13]. The service orchestration description is the specification that details and controls the orchestration of ser- vices to interact [17] These descriptions are written in a service orchestration language, where BPEL (Business Process Execution Language) is considered an industry standard. The services orchestration description must be service compati- ble in order for the orchestrated services to be of high quality. Services interoperate in service clusters, e.g. three fine-grained services A1, A2, and A3 may be orchestrated to provide a more coarse-grained service B. Clusters thus appear on many different levels of abstraction, with the most coarse-grained enterprise service consisting of several other, more fine-grained, clusters.

All clusters, independent of abstraction level, can be analyzed with respect to interoperability; i.e. how well the services within the cluster interoperate. The service cluster interoperability is measured in terms of the quality of each service within the cluster, the service protocol compatibility, service bus compatibility, and service orchestration language compatibility

Each object of the concrete model has a number of attributes; questions that need to be answered for the interoperability assessment. There are several ways for us to collect evidence for each attribute; we can therefore find several pieces of evi- dence about a single attribute. Viable ways to collect information are, for instance, to interview people, perform manual tests, study documents, or a combination of these. In the current example, we introduce three sources of evidence. Max the consultant, who is in charge of developing the service cluster. George, the system owner, is responsible for the system that implements the services. Finally, the in- vestigator can collect information by performing manual inspections of the various attributes in the concrete model.

In this simple example, the number of attributes that needs to be assigned values is already quite substantial, twenty-six to be exact. For each attribute there is a possibility to collect three pieces of evidence; in our example this means that there are some seventy-eight pieces of evidence that can be collected. This number will grow rapidly with the addition of new services. The addition of each new

In this paper, we focus on the second practical problem associated with large concrete models and many pieces of evidence. This problem is due to the cost of evidence collection. If every piece of evidence in the service interoperability example would take one minute to collect, it would take a man-month to collect all pieces of evidence, assuming one hundred services and ten sources of evidence. A better option is normally to refrain from collecting all evidence and instead accept a level of uncertainty of the assessment results. When this strategy is employed, it is desirable to collect those pieces of evidence that have the biggest impact on the assessment results to the lowest evidence collection cost. An algorithm to determine which piece of evidence to collect next is described in the next section.

Diagnosis involves two types of tasks: (i) determining of the (combination of) causes of the observed symptoms, and (ii) increasing the credibility of the diagnosis through the collection of additional, initially unobserved, data. Since information seldom comes for free, the second task by necessity involves the formulation of a strategy to gather information as cleverly as possible, i.e. to gain the most diagnostic value at the least cost. We now proceed to make this more precise.

An example, which was implemented in GeNIe, featured the analysis the inter- operability of a very small cluster of services. In the example it was demonstrated that the use of the diagnosis algorithm reduced the cost of data collection by almost as compared to collecting data without the use of any particular strategy. From this we conclude that the diagnosis algorithm offers a possibility of lowering the cost of performing system quality analyses quite significantly.

