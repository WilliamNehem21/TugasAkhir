model significantly reduced the complexity against the previ- ous techniques. In this method, the Limited-memory Broyden- Fletcher-Goldfarb-Shanno (L-BFGS) technique has been utilized in order to find the optimal point, which is the alternative goal of this model to determine certain factors at the training stage to extend the conditional probability of the training data. So, in order to calculate the conditional probability, we only employed the forward and backward methods that further used for computing the gradients. Due to which the computational time has thoroughly condensed.

state normalization, and additionally, HCRF maintains hidden states to be capable to absorb unknown construction of successive records. As result, CRF and HCRF can work with weighted scores making the set of parameters used comparatively larger as com- pared to MEMM and HMM. We refer the reader for a comprehen- sive and detailed analysis of HCRF and its limitations to [34].

To address the limitations of HCRF and other learning models for emotion recognition on speech data, in the following section, we present our novel approach based on HCRF method, which has the ability to overtly exploit combination of full covariance Gaussian dissemination. Our method gets the benefits from exist- ing HCRF. We apply and test our model on speech data to recognize emotions and compare its results with those obtained by HMM and HCRF with diagonal covariance Gaussian functions.

interpretations. To address the limitations of HMM, a technique called maximum entropy Markov model was proposed. This model shows better results for certain operations/tasks including part-of- speech tagging (POS) [30], evidence abstraction [31], and the recognition of automatic speeches [32]. However, maximum

The Emo-DB dataset consists of expressive exclamations of 10 actors and actresses from German. Of which 50% are male and 50% are female. The utterances (or the spoken sentences) are a set of pre- defined sentences in one of the 7 defined emotional states: 1) neu- tral, 2) boredom, 3) disgust, 4) fear, 5) sadness, 6) Joy and 7) anger. Each successful attempt by the actors and actresses have been eval- uated by a group of 20 judges and the final concluding utterance is only selected if 80% of the listeners have correctly recognized.

In this section, we briefly discuss the computational complexity of the proposed approach as compared to others. The existing HCRF algorithm computes the gradients by a series of forward and back- ward algorithms. We however, execute the forward and backward algorithm once and cache the results in-memory for later use. For-

Audio-based emotion recognition has received lots of attention over the past decade. Several audio-based emotion recognition sys- tems have been proposed; however, still it is major issue for most of the systems to correctly classifying the emotions. There are some attributes which may degrade the accuracy, e.g extraction of the prominent features, and high similarity among different emotions that occurs in the presence of low between-class vari- ance in the feature space.

Accordingly, we have presented a new version of the HCRF algo- rithm that uses full covariance Gaussian density functions. Then, we proved it theoretically and experimentally that the recognition rates of the proposed approach is comparatively precise than exist- ing algorithms. We also proved that these improvements are statis- tically correct by using p-values for testing and comparisons. Moreover, our algorithm does not only add to the accuracy of recognition of emotions, it also has less theoretical complexity as compared to others in training the HCRFs model. As shown in pre- vious section, our proposed approach has a linear complexity while the exiting methods are of quadratic complexity. This extends the functionality of HCRF and enables it to be used in more practical and scalable applications. Although the scope of this paper is restricted to audio-based emotion recognition, however, it is com- pletely possible to extend it to other related ares of recognition including speech recognition, acoustic based context awareness, and gesture recognition among others.

