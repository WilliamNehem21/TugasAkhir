In this paper, we discuss the gesture language, as described in [2,3], for interaction in 3D virtual space. However, the form of analysis we present can be applied to a more general class of issues, and is also independent of the gesture recognition technology used. In fact, the problems encountered in the analysis of the system can be dealt with at a level of abstraction that effectively hides the details of any specific technology that might be employed for the interaction.

The design of a gesture language forms an interesting example for the application of formal modelling because of its limited (but easy to extend) complexity and new aspects. We do not intend to criticise the language pro- posed in [3], but rather show an approach that may be of help in this area of interface design. There are a number of issues that have to be dealt with in the design of a proper dynamic gesture language. Some of them are purely language related issues, others have their roots in human factors.

Ambiguity. One of the problems of the gesture language is that when using only informal reasoning it is not so easy to make sure that there is no ambiguity in gesture recognition. In [7] it has been shown that in one of the proposed gesture languages [3] such an ambiguity exits, i.e. there are possible series of postures that may lead to the recognition of more than one gesture at a time. Detecting ambiguities may not be that easy in general, however. A formal model and automatic verification tools may be helpful in finding critical situations.

Overlap. Another problem is that there might be some overlap between the gestures. This could lead to the partial recognition of one gesture during the recognition of another gesture. This way the recognition of a gesture following another may occur much sooner than expected by the user, leading to confusion.

This multitude of factors can make it rather hard to develop gesture lan- guages that are a pleasure to use. Moreover, often the usability of a gesture based interface is validated a posteriori, i.e. after all implementation work has been done and a prototype is available. Finding the exact cause of usability problems at that stage may turn out to be very difficult. First of all because there are so many factors that may have contributed to the problem. Secondly, because statistical approaches are used to recognise poses and gestures and therefore repeated experiments may produce different results for every test. Thirdly because there is a natural variation in human performance.

The above problem could be handled better if a way could be found to analyse the factors separately. One way in which this could be done is to per- form analysis on models of the gesture interface, focusing on different aspects in isolation. Two of these aspects are ambiguity and overlap. This is the part we will focus on in this paper.

The modelling technique we use is that of (hybrid) automata using varying degrees of the capabilities of the model checking tool HyTech [11,12]. This technique allows us to describe a relatively simple automata model of the gesture recognition process and perform automatic and systematic verification of ambiguities and overlap in the gesture language. We show how the model can be helpful for the improvement of the gesture language. As a next step we show how a timed extension of the model can be used to examine the effect on the recognition performance of the system when allowing for intermediate, non-specified, postures between specified postures of a gesture.

In the next section we describe in more detail the gesture language as orig- inally proposed. Section 3 shows an automaton model and analysis of a subset of the language illustrating the detection of ambiguity and overlap by means of reachability analysis. Section 4 discusses improvements to the language and gives a model and analysis of the complete language. In Section 5 and 6 hu- man factors related issues and further research are discussed. Section 7 draws a number of conclusions.

Typical features that are used to characterise postures are the orientation of the hand and bending values of finger joints. For some of the dynamic poses, the trajectory of the movement is also specified. When the gesture language has been specified, recognition is performed by a Gesture Recognition Machine. Its algorithm operates by means of a number of parallel processes, each one specialised for recognising one of the gestures, that independently use the incoming data. When a gesture is recognised by one of the processes, this is notified to the application so that the system can react appropriately.

The first model we will consider models a subset of the gesture language. We model the zoom, the grip and the exit gesture recognition processes as separate automata using a graphical version of the language HyTech. For details on the syntax and semantics of this language and its associated tool for reachability checking we refer to [11,12]. In this section we will explain the semantics informally while developing the models for the gesture language.

All three considered gestures consist of series of hand-poses of two kinds; flat and fist. All other hand-poses are recognised as being of the kind other. This simple model allows us to formalise the problem of ambiguity and over- lap that may occur in the gesture language and how these problems can be detected automatically by means of model checking techniques.

This needs to be checked for each gesture separately. For example, it can be verified whether whenever Zoom is in location z3 the other processes, grip and exit, can be in a location different from their initial ones g1 and e0. The HyTech reachability analysis expression below checks whether zooming can be activated while both the Grip and the Exit gestures are on their way of being recognised at the same time, i.e. Zoom is in z3 and Grip or Exit are not in their initial locations.

The first improvement is to remove the ambiguity between Zoom and Exit. In the simple version the system would start zooming whenever a flat pose is recognised. The original gesture recogniser is also able to discriminate the orientation of the hand-position to a certain extent. In fact, in one version of the gesture language [2] the Zoom gesture is made with the back of the hand towards the user and the Exit gesture starts with a flat pose with the back of the hand away from the user or to the right of the user.

One approach to deal with this problem is to make the language more resilient to intermediate poses. But in doing so, we would like to know the effect of such a change on the reliability of the language recognition. This is particularly useful to obtain a first indication about the performance of different recognition strategies.

In this paper we have dealt with only a small subset of the above problems as a first step towards the more ambitious goal of informing design decisions based on theoretical models of different aspects of gestural interaction. In particular we showed how a rather simple graphical timed automaton specifi- cation can be used to analyse automatically ambiguity and overlap in a gesture language. Further a timed extension of the model was developed to analyse a time dependent technique to make the language more robust. The analyses have been performed by means of reachability analysis provided by the tool Hytech. This tool provides an exhaustive search through the state space of the language specification.

The language specification can be considered as a first prototype of the gesture language on which a number of essential properties can be verified. This is a great advantage over an approach that relies only on experimental, a posteriori, validation of the interface for various reasons. First of all, as we have remarked previously, human users have difficulty repeating gestures in exactly the same way. This makes it hard to test the recognition system in order to find flaws in the language itself. The automata model provides a simple way to check for language related problems such as ambiguities or mode-surprises. Secondly, the reachability analysis tool gives a trace as a result in case a problem has been encountered. This is extremely helpful for improving the language where necessary. Further, we have shown that an automaton model can be used to investigate the effect of the introduction of techniques to cope with human factor related problems, such as making the language resilient to intermediate poses.

The relative ease of modelling and its complementarity to a posteriori validation of gesture based interfaces makes it worth considering formal mod- elling whenever possible. We do not claim that formal models allow us to find all problems and possible flaws. Rather we hope to have shown that formal modelling can be very helpful in finding some problems in an early stage of development in an area rather different from protocol design in the context of which most formal methods have been developed.

