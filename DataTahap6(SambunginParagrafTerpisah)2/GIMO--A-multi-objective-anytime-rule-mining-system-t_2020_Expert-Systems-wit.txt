Some mining approaches create opaque models that cannot be analyzed by domain experts at all. Some types of models (e.g., support vector machines or neural networks) are nearly always hard to understand, while for others (e.g., decision rules or decision trees) understandability depends on complex- ity (Barredo Arrieta et al., 2020; Dam, Tran, & Ghose, 2018).

One type of human input concerns the cost of misclassification. Many approaches are not cost-sensitive at all or need a cost matrix to be specified at the start. In reality, problems are cost- sensitive, but the exact cost matrix is often not known. More- over, costs are usually assumed to be independent of each other for each input in an algorithm. However, costs often depend on each other, e.g., due to synergy effects.

Multi-objective optimization, i.e., the system can find rules with varying trade-offs for objectives like reduction of false positives, reduction of false negatives, but also custom cost functions that take relationships between instances into account or estimate the understandability of a model. This avoids the need to spec- ify a cost matrix at the start.

Anytime, i.e., the user can explore the data and the current re- sults and interact with the system at any time. The user rarely needs to wait for the system, and neither does the system need to wait for the user.

In a case study in a medium-sized software company, we extracted data from software repositories on which code change parts led to which remarks (Baum et al., 2018a). The rule mining system de- scribed in the current article was then used to derive rules that characterize change parts that do not need to be reviewed.

For domain-specific use cases, there is a cost associated both with false positive predictions and false negative predictions. These costs can be modeled in a cost function cost(h, X, Y) that estimates the costs using labeled data. For the remainder of this paper, we assume without loss of generality that cost functions should be minimized. Thus, a learning algorithm should optimize the cost function, i.e.,

For our use case, we have three types of cost functions: (1) cost functions for the estimation of the effort spent by domain experts to act on predictions of the decision support system, (2) cost func- tions for the number of actions not covered, and (3) costs for the cognitive complexity of understanding the rules. The exact defini- tions of our cost functions will be given in Section 6.3.

Another property of RIPPER is that the label of an instance only depends on the instance itself, not on other instances. When the instances are independent given the class label, this is not a prob- lem. However, this is not the case for our type of problem. In- stead, as stated at the start of this section, our labels can be ex- plained by multiple instances at the same time: They form groups

This interaction should be iterative and allow users to modify the manually defined conditions and get feedback about the re- sults at any time. Please note that these interactions are different from active learning (Settles, 2009): In active learning, domain ex- perts may also interact with the algorithm, but only by labeling in- stances on-the-fly. In our case, labeled instances are available and the domain experts can directly modify the generated rules.

However, unlike in standard GRASP-PR, the steps are exe- cuted by one or several concurrently working processes (mining threads).1 Each mining thread runs in an infinite loop in which it processes work packages from a queue of work items. Such a work package is a combination of a ruleset and a task that needs to be done with that ruleset, e.g., a newly created ruleset that now needs to be optimized by local search. When there is no open work, a mining thread generates new items, or it tries to improve exist- ing solutions further. A second difference to standard GRASP-PR is that our algorithm works on a Pareto front of solutions and takes the multi-objective nature of the problem into account. This Pareto front is kept as a shared data structure, which is updated every time a new ruleset is evaluated.

Except for the preprocessing step during rule generation, we did not yet mention how to solve our problem that there is a relation- ship between instances, i.e., that the instances are not indepen- dent given the class label (Section 4.2). Due to the meta-heuristic design, the algorithm can be easily enabled to take these relation- ships into account by adding appropriate cost functions: The value of the cost function needs to decrease if and only if one instance that is the cause of an action is covered and if no other instance that is related to that action is already covered. We achieve this by counting the actions that are covered by the rules

Changing the target function In several parts of the algorithm, the multi-dimensional cost vector needs to be transformed into a single numeric target value. This single number is used to se- lect the best element when hill-climbing during the local search, as well as during path relinking. It is also used to pick a decent rule as a foundation for further search and generation. Initially, this target function returns the precision of the ruleset. The user can change it to another function to try out other optimization biases at any time. This can be useful to try out different cost factors. Ad- ditionally, results from MSOPS (Wagner et al., 2007) indicate that switching between different target functions leads to a better ap- proximation of the Pareto front.

We evaluated GIMO on a real-world data set in the context of review remark prediction. The details of the review remark predic- tion study are out of the scope of this article and available sep- arately (Baum et al., 2018a). Within this article, we focus on the comparison between GIMO and existing approaches for the mining of decision rules: the popular machine learning algorithms C4.5 decision trees (Quinlan, 1993), RIPPER (Cohen, 1995), and Meta- Cost + RIPPER (Domingos, 1999).

We compare GIMO with C4.5, RIPPER, and MetaCost + RIPPER. We use the implementations provided by Weka (Eibe, Hall, & Wit- ten, 2016). We optimize the confidence factor for the pruning of the C4.5 tree with five-fold cross-validation and sampled values between 0.05 and 0.25 in intervals of 0.05. The optimal value is 0.25. We set the minimal number of objects per leaf node to 143, i.e., at least 1% of the instances of the minority class MUST_BE. As stated in the introduction, one of the benefits of GIMO compared to classic cost-sensitive algorithms like MetaCost is that the cost matrix does not need to be specified explicitly. For MetaCost, we chose the cost matrix so that a misclassification that leads to a missed review remark is 100 times as costly as a misclassification that leads to unnecessarily reviewing an instance.

ticket tgain, we observe by far the largest relative difference to gain for GMO, which drops from 23.2% gain to only 3.5% tgain. This in- dicates that GMO mostly saves effort by ignoring changes for a few very large tickets, but does not save effort in most cases. This limits the usefulness of GMO in a real-world scenario, as savings would only occur sporadically. A likely cause is overfitting of GMO, which only uses precision as the target function. Overfitting is countered by the manual interactions with GIMO.

Overall, GIMO is among the best algorithms for most perfor- mance metrics. We further observe that the interactive feedback provided by domain experts was very helpful for the overall result, i.e., at the cost of very few missed actions, there was a big gain in the saved effort. We also observe that standard rule mining sys-

For the missed actions, GIMO performs second best with 1.3% missed actions and is only outperformed by GMO which misses 0.9% of the actions. Thus, both the interactive and non-interactive variants of our approach are very good at identifying the actions. RIPPER and C4.5 both perform worse regarding this metric with performance values between 19.2% and 34.1%.

our study. With 17 features and roughly 40 conditions, one might ask whether the GIMO ruleset is still too complex. On the other hand, due to the multi-objective nature of our approach, the do- main experts could have easily chosen a simpler ruleset. Because they did not do so, we consider this threat under control. The highly structured form of the rulesets and the interactive features of the UI might have contributed to ease understanding.

The explainability of opaque models, like deep neural networks, is heavily researched at the moment. One approach is to build a simplified human-understandable model that approximates the original model. As the right balance between understandability and approximation quality is of prime importance here, our multi- objective user-informed approach could be useful.

Tobias Baum: Conceptualization, Methodology, Software, Inves- tigation, Resources, Data curation, Writing - original draft, Project administration, Funding acquisition. Steffen Herbold: Software, Validation, Investigation, Resources, Writing - original draft, Fund- ing acquisition. Kurt Schneider: Resources, Writing - review & editing, Supervision, Funding acquisition.

