Static program analysis [1,2] can alleviate some of the aforementioned disadvan- tages. In contrast to traditional testing, static program analysis does not execute the implementation, but analyzes the source code for known dangerous program- ming constructs, for combinations of those and their causal relationships, and the impact of potentially tainted input. Typical examples in C/C++ are null pointer dereferences, accessing freed memory, memory leaks, or creating exploits through buffer overruns. These types of bugs are only found to the extend in that they affect the functional behavior, and since traditional testing is focussed on checking the functional behavior of the system, finding these types of bugs is more often a welcomed side-effect, rather than intentional. Because static program analysis can pinpoint those software deficiencies directly, and because it is scalable to large code bases and can be run fully automatically, it is a way to complement traditional testing.

by modern static analysis tools. We briefly describe the underlying technology of our own analyzer Goanna in Section 3. The focus of this work is Section 4 where we discuss the dimensions for classifying bugs, give example deficiencies, as well as measures for improvement. Section 5 presents empirical data based on analyzing Firefox, before Section 6 concludes the paper.

Naturally, the more information that is available, the better the analysis results become. However, from a practical point of view collecting and computing semantic information can result in excessive computation, and slow down the analysis to a point where it is not scalable to larger programs. In most circumstances a static analysis tool is only regarded as useful if the analysis time is roughly in the same order of magnitude as the compilation process and not several orders of magnitude higher.

While may-analysis can turn up significantly more bugs, the detected bugs may not exist in the actual program behavior due to infeasible paths or infeasible data dependencies. In this context these warnings are called false positives (or false alarms). Must-analysis, however, might miss bugs due to the nature of under- approximation. We call these false negatives.

To complicate matters further, modern static program analyzers often mix over- and under-approximations within the same analysis. For instance, the semantics of pointer arithmetic might be under-approximated and the semantics of a loops over- approximated. While not sound, those frameworks have proven to be most effective in turning up many bugs without generating many false alarms at the same time [3]. Another complication is that the term false alarm is often used in a different way, motivated from the point of view point of a developer, or from the point of view of a tester. Section 4 discusses these alternative uses.

is only violated if the resource p is used on all paths after being freed. While this relaxed property does not pick up as many bugs as the previous one, it also does not create as many false alarms. This is one way to tune a static program analyzers.

Tuning the strictness of different checks can be one way to improve the bug/false- alarm ratio, another option is to add more semantic information and reflect the fact that it is semantically impossible to access p after a free operation. We implemented a false path elimination strategy based on interval constraint solving that does exactly this. Hence, our implementation can use the first check to find all possible bugs and still rule out many false positives automatically.

We implemented the whole framework in our tool Goanna. Goanna is able to handle full C/C++ including compiler dependent switches for the GNU gcc compiler and uses the open source model checker NuSMV [10] as its generic analysis engine. The run-times are typically in the order of the compilation, i.e., we experience an

With local path-sensitive analysis a warning is a true positive if there exist input to the function, such that the error path becomes possible. With a context-sensitive analysis a warning is only a true positive, if the rest of the system is actually able to provide such an input. If the rest of the system can guarantee that such an input is impossible, the subsystem can use this as an assumption, and guarantee correct behavior under this assumption.

In the remainder we will discuss each of these categories in detail and provide details to illustrate that the categories are not necessarily correlated, e.g that sever- ity can be independent from incidence. All examples are taken from the Firefox code base [15].

The severity of a warning can be either high, medium or low, and this is typically how developers categorize bugs. Security flaws, even if they have benign causes or only occur under very specific circumstances, can be more severe than errors that have an immediate impact on functionality. A warning might have medium or high severity, even if close manual inspection cannot establish conclusively, whether there is an actual execution producing the bug. Just the chance for the deficiency to create a run-time bug is sufficient reason to change the code.

the case in this example. Variable numtoks is an unsigned integer, which suggests the index should be bounded to positive integers, and it is unclear if the predecessor array for tokens is defined in any meaningful way. For this reason this warning was classified to have medium severity.

Catching bugs depending on their incidence can best be tuned by the analysis algorithms used. Must-analysis is good at picking up bugs that will always occur, while may-analysis picks up potential bugs on single executions. Moreover, a path- sensitive analysis can filter out those combinations of conditions that are infeasible in the execution. To get a better understanding of inputs from other parts of the program and to increase the precision of the analysis a full context-sensitive approach should be used.

Goanna supports the fine tuning of properties by reasoning about some program path or all paths as described in Section 3. Moreover, Goanna can automatically eliminate infeasible paths that are caused by a set of excluding conditions. Goanna does not perform full context-sensitive analysis, yet. However, one can argue that every function should be implemented defensively, i.e., inputs should be checked before being used to avoid unexpected crashes.

Reducing false alarms resulting from intra- or inter-procedural over-approximations can best be addressed by finer abstractions and specialized analysis algorithms. In the example above creating a fine grained CFG for the analysis of short circuit operators can help. Moreover, a specialized inter-procedural pointer analysis taking aliasing into account can also aid in reducing context-sensitive over-approximations. Often, incorrect warnings are caused by one of the many exceptions, and corner cases that exist in C and C++. In this case it is usually sufficient to refine the syntactic description of the properties on an intra-procedural level. This technique is also effective to cover coding practices that are not according to standard, but nevertheless common.

Our implementation is written in OCaml and we use as the back-end model checker NuSMV 2.3.1. The current implementation is an early version consisting mostly of intra-procedural analyses for full C/C++. At the moment we have implemented 18 different classes of checks. These checks cover, among others, the correct usage of malloc/free operations, use and initialization of variables, potential null-pointer dereferences, memory leaks and dead code. The CTL property is typically one to two lines in the program and the description query for each atomic proposition is around five lines because of the need to cover many exceptional cases.

We evaluate our tool with respect to run-time performance, memory usage, and scalability, by running it on a regular basis over nightly builds of the Mozilla code base for Firefox. The code base has 1.43 million lines of of pure C/C++ code, which becomes 2.45 million non-empty lines after preprocessing. The analysis time including the build process for Firefox is 234 minutes on a DELL PowerEdge SC1425 server, with an Intel Xeon processor running at 3.4 GHz, 2 MiB L2 cache and 1.5 GiB DDR-2 400 MHz ECC memory.

Firefox. This was obviously an accepted coding practice, to achieve consistency throughout the code base, rather than being accidental omissions or errors. It is interesting to note that all of these warnings were semantically correct, pointing to behavior that occurs always, and thus are true positives from a language semantics point of view. However, from a user perspective these warnings are of a very low severity.

While a virtual function call in a destructor does not necessarily cause a problem, it might result in memory leaks, and is considered bad coding practice, regardless of the effects. Our analysis of the Firefox code base found 114 instances of such usages. From those warnings, 105 refereed to the same macro that was used over and over again, and thus caused as many warnings after preprocessing. This points to another way to improve the usability of static analysis tools, namely to present to the user with a concise set of warnings. Except for this duplication of warnings, the check itself was kept unchanged, as all of the warnings are considered valuable.

Overall, we achieved an 83% reduction in false alarms and very low severity warnings by mostly changing the precision of our rules. This was achieved by tak- ing care of particular programming styles and strengthening our CTL properties. Encoding checks as CTL properties and switching from a may- to a must-analysis easily by changing the path quantifier proved crucial to quickly adjust the granu- larity where needed.

Currently, our reported defect density is around 3.2 warnings per 1000 LoC for the Firefox code base. The best state-of-the art checkers provide a defect density of around 0.35 for Firefox. While there appears to be a significant gap between those two numbers, when neglecting low impact properties such as unused values and taking into account must-properties only, we achieve a defect density of 0.36. However, those numbers can only serve as a rough comparison, because we do not know about the exact checks commercial tools are implementing, their reporting strategy or the exact techniques used. Nonetheless, understanding programming styles, severity of bugs and the importance of scalability are in a first analysis step more important than deep semantic analysis techniques.

In this work we presented our experiences in tuning static program analysis for large software systems. In particular we gave a classification of properties and bugs based on our practical experiences in keeping the warning rate down to report relevant issues. We advocate that a crucial first step is to get familiar with programming styles and the oddities in real code and to fine tune syntactic checks accordingly instead of applying an expensive semantic analysis right from the beginning. Typ- ically static program analysis returns a manageable set of warnings. In a future step, only this set should be subjected to full context-sensitive analysis to keep the overall analysis scalable.

Related to our work is an evaluation and tuning of static analysis for null pointer exceptions in Java as described in [16]. The authors show that many null pointer bugs can be found on a syntactic level without sophisticated semantic analysis. They show that fine tuning syntactic checks is the key for good analysis results. A similar conclusion has been drawn in [17] where the authors studied coding patterns and

More information about prominent commercial static analyzers can be found in [19]. The authors compare a number of tools and point out their strength and weaknesses as well as their experiences of using them within Ericsson. A more general comparison of some static program analyzers including a discussion on bugs can be found in [21,20,22].

