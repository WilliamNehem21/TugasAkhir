Enforcing adherence to standards in software development in order to produce high quality software artefacts has long been recognised as best practice in traditional software engineering. In a distributed heterogeneous development environment such those found within the Open Source paradigm, coding standards are infor- mally shared and adhered to by communities of loosely coupled developers. Following these standards could potentially lead to higher quality software.

This paper reports on the empirical analysis of two major forges where OSS projects are hosted. The first one, the KDE forge, provides a set of guidelines and coding standards in the form of a coding style that developers may conform to when producing the code source artefacts. The second studied forge, SourceForge, imposes no formal coding standards on developers. A sample of projects from these two forges has been analysed to detect whether the SourceForge sample, where no coding standards are reinforced, has a lower quality than the sample from KDE.

This paper is structured as follows:Section 2 provides a description of past and current research works which can be related to the present paper, and describes how it completes or enhances previous approaches. Section 3 presents the definitions and the attributes used in this work. It also introduces and instantiates the GQM approach [2], tailoring a research question for the current study.Section 4 produces an empirical hypothesis based on the presented research question, illustrating the null and the alternative hypotheses, as well as the complexity and coupling metrics used to evaluate them. In the same section, an overview of the samples from the two forges is also given. Section 5 presents the results of the statistical tests linked to the hypothesis, and evaluates whether the null hypothesis should be rejected or not.Section 6 illustrates both internal and external threats to validity to this empirical study, whileSections 7 and 8 conclude this paper.

software literature dealing with the measurement of software quality and associated characteristics such as complexity, comprehensibility, reusability and maintainabil- ity. Measurements have been traditionally divided into development and design quality metrics (early in the product lifecycle), e.g. [20], and post-release metrics which can be applied to a finished software product. One of the most famous ef- fort in the later area presented the results of the impact of coupling, cohesion and complexity on the development cost of object-oriented systems [4].

[1] defines the characteristics of software quality as: functionality, reliability, usabil- ity, efficiency, maintainability and portability. In the past, there have been other attempts to determine quality numbers based on source code metrics: the most well- known is the Maintainability Index [27], which is a composite metric to assess, at the system level, the relative maintainability. The Halstead Effort [15], the McCabe cyclomatic complexity, the lines of code and comments are averaged into a single number to represent a global index of maintainability and quality. As reported in other studies [6], the McCabe values typically follow a Power-law distribution for methods or procedures in a system: using the average for such distributions will hinder some of the characteristics of the distribution, such as its skewness. The Halstead Effort metric has been evaluated as highly correlated with the McCabe index [17], but also heavily criticized as an unreliable metric [19]. In this work, the McCabe indexes are evaluated both as distribution in each project, and considering the fraction of highly complex functions (i.e., whose index is larger than 10 [23]). The Halstead Effort as a metric is not considered in this work.

Each KDE project has the freedom to develop its own specific Coding Style; projects are recommended to follow the kdelibs coding style 4 . There is a cod- ing style for kdelibs that is supported by a vim script in kdesdk/scripts/kde-devel- vim.vim that helps developers to keep the coding style correct. In addition to defaulting to the kdelibs coding style it will automatically use the correct style for Solid and kdepim code. Developers can add rules for other projects via the SetCodingStyle function.

In addition to these quality-related policies in KDE, there is also an automated quality assurance tool called the English Breakfast Network (EBN) 5 . The EBN is a tool for detecting and measuring aspects of quality within KDE as a whole. For example, the EBN measures code defects and errors in source documentation, such as spelling errors.

Complexity: within the software engineering literature, this is a very broad term. To help in scoping it down, this paper consider complexity only from the point of view of the source functions. The attribute that will be used to empirically detect the complexity of source functions is the McCabe cyclomatic complexity. This is a measure of structural complexity, and it can be calculated

Question: The purpose of this study is to establish differences between samples from KDE and SourceForge. Their complexity will be evaluated and a compar- ative research question will be evaluated via a direct comparison between the projects composing the two samples. Based on the complexity and coupling at- tributes defined above, the research question asks: in the presence of coding stan- dards, will projects be less complex, and of higher quality, than of the counterpart forges not reinforcing any standards.

Metrics: Every project from both samples will have their functions evaluated in terms of McCabe cyclomatic complexity (to assess structural complexity) and the fan-in and fan-out metrics (to assess the coupling). As a compounded metrics, the value of instability will also be evaluated based on the definition given above. Once completed, these measurements will be summarised per project for both metrics, specifically into a median value, a maximum value and a variance.

attributes introduced above (mc, c i, I). The directional tests will be also consid- ered, in order to establish whether one forge achieves statistically higher quality results than the other: this means that a test will be run to check whether the KDE sample achieved a smaller level of complexity, or whether the KDE level of fan-in is larger than SourceForge. Based on common design principles, in fact, software designers and programmers should aim for low fan-out and high fan-in [11].

This section summarizes the findings that were collected evaluating the research hypotheses. The attributes presented above were evaluated at the latest available change recorded in the CVS or SVN repositories: the couplings (fan-in and fan- out), instability and complexity of all the functions of the projects from KDE and SourceForge were calculated. For every project in the samples, the median and variance values of all the functions were used to compare the two forges. The maximum value was also used where appropriate.

quality software by openly sharing documents and guidelines on programming styles and coding standards. In order to assess this, two OSS forges (collections of software projects) were selected: the first (KDE) is a large container of applications which share the same graphical interface objectives, and guidelines are shared among de- velopers to comply with existing coding standards. The second (SourceForge) is a wider-spectrum collection of projects, and no explicit effort is attempted towards a common framework of coding rules.

