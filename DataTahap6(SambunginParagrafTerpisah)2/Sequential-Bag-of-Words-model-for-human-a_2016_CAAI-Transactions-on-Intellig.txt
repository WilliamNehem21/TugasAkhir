Zhao et al. [16,22] developed the Local Binary Pattern on Three Orthogonal Planes (LBP-TOP). Delightedly, LBP-TOP had been applied for dynamic texture description and recogni- tion successfully and had obtained good results on facial expression analysis. The proposed algorithm extracted the LBP the other hand, the spatial temporal co-occurrence information were encoded in XT and YT planes. Shao et al. [13] proposed to extend the computation of LBP and computed histogramming LBP [7] named Extended LBP-TOP and Extended CSLBP- TOP, respectively. The presented methods could extract more dynamic information inside the spatio-temporal cuboids and be applied on the gradient cuboids.

The rest of the paper is organized as follows. In Section 2, we introduce the reason for using sub-actions and illustrate the framework of our approach. Section 3 and Section 4 describe the segmentation and classification approach respectively. In Section 5, we conduct experiments on UT-Interaction and Rochester datasets and compare our approach with other BoWs based approaches. Finally, conclusions are drawn in Section 6.

To segment actions efficiently, we should achieve two goals. First, ensure the sub-actions in the same sub-section of the same action class are of the same type, ignoring the speed differences between actors. Second, all sub-actions should capture enough motion information for classification. The two-stage segmentation can reach the above goals nicely, which is detailed below.

Here we use point density to chop clips for further seg- mentation instead of using number of frames to ensure there is enough motion information in all clips. Generally, dense feature points infer strenuous action. The intensity of the ac- tion may be changed over the entire action process. Some of the action primitives maybe moderate, so there could be few interesting points in these frames and result in insufficient information. Moreover, it also balances the speed difference between different actors or actions. We compute a histogram of spatial-temporal word occurrence for each video clip, the kth clip is described as: (shake hands), the shaking parts of the action are more salient. But to the fourth column (point), the whole process is quite different from other actions, so its salience is evenly distrib- uted. Although the environment of the two scenes is different, their salience maps are quite similar. The salience ranges from 0 to 1, and the average difference between them is 0.1777. Except the red grid on the right top of (c) caused by overact in scene-2, most difference is less than 0.2. This means our segmentation and salience calculation approach is robust to the inner-class variation and environmental change.

datasets, the cuboid size is w = h = 1 pixels, t = 2 frames, and descriptors can also be used to acquire features. For both using 3D-SIFT, w = h = 2 pixels, t = 3 frames, the threshold threshold is 0.0002 when using gradient descriptor. When is 0.0001. After extracting features from videos, k-means

descriptor [4] shows better results on UT-Interaction scene-2 and Rochester than 3D-SIFT because it extracts more feature points to describe the sub-actions more sufficiently. On UT- Interaction scene-1, our result is comparable to [11,17]; but manifest a faster computational speed. Noting that [11] fo- cuses on the spatial structure between visual words, hence it can be combined with our approach. Approach in [17] aims at action prediction, and it conducts a iterate segment matching between classes and is inefficient to action classification. On both UT-Interaction scene-2 and Rochester datasets, our approach shows the best performance.

The authors acknowledge the financial support by the National Natural Science Foundation of China (NSFC, No. 61340046), the National High Technology Research and Development Program of China (863 Program, No. 2006AA04Z247), the Scientific and Technical Innovation Commission of Shenzhen Municipality (No. JCYJ20120614152234873, JCYJ20130331144716089)

Award on Artificial Intelligence, Excellence Teaching Award, and Candidates of Top Ten Outstanding Professors in PKU. He is an IEEE member, vice president of Chinese Association for Artificial Intelligent (CAAI), and vice chair of Intelligent Robotics Society of CAAI. He has served as keynote speakers, co-chairs, session chairs, or PC members of many important international conferences, such as IEEE/RSJ IROS, IEEE ROBIO, IEEE SMC and IIHMSP, recently also serves as reviewers for many international journals such as Pattern Recognition, IEEE Trans. on Signal Processing, and IEEE Trans. on PAMI.

Hao Tang received the B.E. degree in electronics and information engineering in 2013 and is working to- ward the Master degree at the School of Electronics and Computer Engineering, Peking University, China. His current research interests are image classification, hand gesture recognition, gender recognition, image retrieval, action recognition and deep learning. He has published several articles in ACM Multimedia Con- ference (MM), IEEE International Conference on Image Processing (ICIP) and International Joint Con- ference on Artificial Intelligence (IJCAI).

Wei Xiao received his Ph.D in computer science and technology in Tsinghua University, China. He is currently a postdoctoral researcher on Human-Robot Interaction (HRI) in Peking University, China. His research interests include computer vision and HRI. He has published several articles in IEEE International Conference on Multi-sensor Fusion and Integration for Intelligent Systems and ACM Multimedia Conference (MM).

Ziyi Guo received her B.E. degree in digital media technology in 2014, and is working toward the Master Degree in the School of software and microelec- tronics, Peking University, China. Her research in- terests include interactive media technologies, interaction design and human action recognition.

Lu Tian received her master degree in computer sci- ence and technology in Human-Robot Interaction (HRI) Lab, Peking University Shenzhen Graduate School, China. Her research interest is mainly about human action recognition. She has published articles in IEEE International Conference on Image Process- ing (ICIP).

Yuan Gao received the B.E. degree in intelligent science and technology from Xidian University in 2012. Then he obtained the M.S. degree in computer applied technology from Peking University in 2015. Currently, he is working toward the Doctor degree in Christian-Albrechts-University of Kiel, Germany. His research interests include object detection, 3D recon- struction, facial expression and gender recognition. He has published articles in IEEE International Confer- ence on Image Processing (ICIP).

