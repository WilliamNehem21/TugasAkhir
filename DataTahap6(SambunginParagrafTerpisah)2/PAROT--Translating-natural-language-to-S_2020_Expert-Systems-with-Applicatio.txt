Compound sentences which female actor played in Casablanca and is married to a writer born in Rome ?. (Zhao et al., 2017) es- timates that 9% of the total errors in queries generated by their gAnswer tool is due to the fact that it does not handle queries with unions or filters.

SPARQL query is composed of a set of triple patterns known as graphs patterns. The graphs patterns are placed directly after the WHERE key word or after the target variables in the SPARQL query. The triple patterns are of the form of < subject > < predi- cate > < object > where the subject, predicate and object may be variables (SPARQL Working Group, 2013). The idea therefore in this section is to process a user submitted query to identify potential triples that will be used to construct the SPARQL graphs. Triples identified from the user query are referred here as user triples. To identify user triples from the submitted query, we categorize it into either: :hasRiver river}. To capture both these possibilities, for each user triple we extract, we create a second one where the concepts in the subject and object position are interchanged. Therefore, for the user triple {rivers traverse Mississippi} we create another {Misssippi traverse river} where the concepts Mississippi and rivers are inter- changed. In this example we generate the following user triples.

A non-relational query is a query (sentence) which has no rela- tional phrase linking any of its nominals. For instance, the sentence What is the area of the most populated state? is a non-relation based query. To identify triples that exist in this category of queries, we use Algorithm 2.

the Nobel prize ?, since it is a relation based query, it is first pro- cessed based on the discussion in 3.3.1, the GenerateTriple(s) will extract the triples {(Chemist won Nobel)or (Nobel won Chemist)}. Adding this to the triple generated based on adjective, the final user triples for the query is shown in Listing 6.

To develop this kind of a lexicon, we adopted lemon (Lexi- cal Model for Ontologies) (McCrae, Spohr, & Cimiano, 2011) which is a model for lexicons that are machine readable. It allows in- formation to be represented relative to the underlying ontology. Lemon was a natural choice since it is RDF based and uses the principles of Linked Data. It can also be extended easily to cap- ture the information needed. To reduce the work of generat- ing the lexicon manually, we adopted the technique proposed in (Walter, Unger, & Cimiano, 2013). We exploited the technique to generate the lexicon in lemon model semi-automatically. We de- signed the lexicon such that it preserved the structure of the un- derlying ontology. For each lexical entry in the lexicon we specify the following information:

an ontology triple, the position a term occupies in the user triple should match the position of the term it is mapped to in the on- tology triple i.e. a term in the user triple that occupies the subject position must be mapped to a term in the ontology triple that oc- cupies subject position. By doing this, we narrow the search space hence reducing the mapping time significantly. A user triple can be in any of this forms

The non-scalar adjectives need no special handling apart from those discussed in Section 3.3.2. However, scalar adjectives help to further narrow down the query hence need addition processing on top of those discussed in section 3.4.1. The syntactic constrains of scalar adjectives are defined in section 3.3.1. To process the scalar adjectives, we execute three steps

To process the negation, we first remove the negation part and extract triples contained in the positive query. For example, in the query Which river does not traverse Alaska or Mississippi ? its pos- itive form is Which river traverses Alaska or Mississippi ?. The user query is then processed normally to generate initial SPARQL query as ahown in Listing 23.

For complex questions, we used two datasets, the first dataset was from the 9th challenge on question answering over linked data (QALD-9)1. We specifically evaluated the PAROT system on the test dataset. The questions contained in the dataset are of different complexity, including questions with counts, superlatives compar- atives and temporal aggregators. The second type of dataset used was that provided by Mooney2 which has been used previously by PANTO for similar evaluation. We specifically used the dataset that is composed of geography data in the United States. The dataset is accompanied with 880 queries where each query has its ex- pected response in prolog format. From this dataset, we converted the prolog format into OWL ontology. We then selected queries that were compound in nature and contained negation. To evaluate the performance of PAROT on simple questions, we used 200 ques- tions and their corresponding answers from the dataset proposed by (Bordes et al., 2015).

We then computed the macro and micro F-measure of PAROT over all test questions. To compute micro-F-measure, we summed up all true and false positives and negatives and calculated the pre- cision, recall and F-measure at the end. For the macro-measures, we calculated precision, recall and F-measure per question and averaged the values at the end. The results were compared with those of gAnswer tool (Zhao et al., 2017), which was the top per- forming tool in QALD-9 challenge (Usbeck et al., 2018a).

ber of variety of questions i.e. the wide coverage of the syntac- tic based heuristics. PAROT performs 18% better that gAnswer in this task. Its high coverage is depicted by its comparatively higher recall value. Its high precision shows that the heuristics are able

to resolve user questions into correct SPARQL queries. However, an optimum performance of PAROT was inhibited by its inability to answer questions that start with When. It also can only partially handle aggregation. When it comes to query processing time qAn- swer has a significant lower response time as compared to PAROT. The significantly slow query response time of PAROT is attributed to its elaborate query analysis and categorization step which takes

All persons who have made substantial contributions to the work reported in the manuscript (e.g., technical help, writing and editing assistance, general support), but who do not meet the cri- teria for authorship, are named in the Acknowledgements and have given us their written permission to be named. If we have not in- cluded an Acknowledgements, then that indicates that we have not received substantial contributions from non-authors.

