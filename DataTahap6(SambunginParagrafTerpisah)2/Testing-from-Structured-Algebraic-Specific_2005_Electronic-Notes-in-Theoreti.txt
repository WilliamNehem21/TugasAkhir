Specification-based testing (SBT) is concerned with deriving test suites from formal specifications of programs. More recently, several works in this area have been developed [3,13,2], promoting the combined use of formal methods and testing to produce high integrity systems in a cost-effective way [7,4]. In the algebraic field, SBT consists in checking whether specification axioms are satisfied by an implementation under test (IUT). From a selected test case (usually an axiom), tests are run to exercise referred operations and an oracle evaluate the output criteria according to the results produced by the tests [15,21]. In other words, oracles check satisfaction of specification axioms, for a finite test data set, by the IUT.

In order to establish testing as an effective verification technique, it is es- sential to develop well-founded methods and strategies that support automa- tion of testing activities [19]. A great effort is still needed to have testing as a standard activity in formal frameworks. Accurate interpretation of test results regarding correctness and how to properly select finite test sets along with au- tomation and technology transfer are crucial points. Exploratory attempts to apply formal SBT in industrial settings can already be found [2].

a model checker for an object-oriented Petri nets modelling language called RPOO [16]. Veritas uses CTL temporal logic [11] for properties specification. The tool exploits the object-oriented view of RPOO models so that we do not need to deal with Petri nets syntax for specifying atomic propositions. Veritas is implemented in the SML language.

Even if the signature of the IUT matches the signature of the specification, the structure of the IUT does not necessarily reflect the structure of the speci- fication. On the other hand, when testing from a structured specification, it is necessary to think of its structure. This is due to the fact that the semantics of specifications is given in a compositional way [5], i.e., the signature and class of models of a specification are determined according to the result of applying specification-building operations to its constituent specifications. In other words, the structure of the specification must be considered in order to make sense of axioms. Accordingly, among obstacles that can be encountered when testing from structured specifications are [22]:

It is reasonable to think that particular test sets for a given sort should be defined separately for some signatures, groups of axioms or individual axioms. In other words, it may be beneficial to handle the quantifier problem differently according to the signature/axiom under consideration at the point where the quantifier appears. Even if a test set is finite, it may be impractical to test certain functions based on this test set, particularly the more complex and time-consuming ones.

The set of operations chosen above corresponds to a small set of prim- itive operations which enable individual problems found when testing from structured specifications to be analysed in isolation. These operations can be combined in order to define more complex and interesting ones found in the literature [31,18], like enrichment (then in Casl) and arbitrary union or sum of specifications (and in Casl). Instantiation of generic specifications can be defined in terms of union and translate in the usual way. For example, see [29].

The Evaluation module is the main part of the tool. It is responsible for verifying the described properties in the specification against the provided state space. In general, if a universally quantified formula is evaluated as false, the evaluation module shows a counter-example trace which proves that the property is not true in the system. In a similar way, when an existentially quantified formula is true, the module shows a witness trace which proves that the property is satisfied in the system. The implemented algorithms are based

Finally, the results produced by the evaluation module can be viewed as a textual report or as a graphical representation. In the former mode, the report contains the truth-value of the property, the possible trace proving such value, and the CPU time spent in the evaluation process. In the latter, we get a visualisation of the traces, which allows us to analyse them step- by-step in a graphical mode. This is done by the Object System Simulator module.

Veritas has been implemented in the SML language according to algo- rithms of CTL formula checking proposed in [27]. Both algorithms and im- plementation were validated, prior to the experiment presented in this paper, based on static analysis and ad-hoc/manual testing. However, the complex- ity of the implementation of such applications naturally makes static analysis hard or even impossible to be fully performed.

To handle point 2 mentioned above, we focused on specification generated values only, but also considered limit unreachable values (e.g. negative inte- gers) to assess IUT robustness. To handle 3, we opted to work with simple formulas, covering possible basic combinations of constructors with at most 2 constructors per formula. Finally, to handle 5, we analysed subdomains se- lected at specification level and compared to code level subdomains and added relevant data to the test sets.

According to test planning, we constructed 3 SML structures to act as test interface for the test oracles to be run, one for each of the specifications presented in Section 3: FacadeKripke, FacadePath and FacadeCTL. Gener- ally, these structures contain direct function calls demanded by the need to translate names from Casl libraries to SML library names. For instance, Set in Casl is translated to Binaryset in SML. The translation has been done ac- cording to rules presented in [21] and reviews have been performed to check them. The test interfaces also include functions that generate the test data sets in SML, according to the specification of these data. For instance, the list of state identifiers for a given kripke structure (see Kripke specification). Again, these are mostly direct function calls to constructors and predefined functions in SML.

A further obstacle is to check the path construction procedure used by Veritas according to axioms of the Path specification. The problem is that it is impossible to compute a path from function calls. There is not such a function as paths are constructed as needed when EG and EU operators are considered. In this case, we decided to have path test data sets composed of paths generated as example and counterexample by Veritas. These data has been collected by running model checking experiments using the tool and the kripke structured being considered. In this way, we managed to check indirectly the paths constructed by the tool.

Furthermore, the process of building the test interfaces has been beneficial to improve the Veritas documentation. It has promoted a thorough investi- gation of interfaces and internal algorithms to guarantee the right functions are called with the right parameters, specially, functionality related to path construction. The final test code produced has 3.233 lines, where 2.895 was automatically generated by the CaslTest tool (test oracles). The version of Veritas considered has 840 lines of code, excluding predefined Moscow ML libraries used 16 , giving the expected rate of the test code at about 3 times bigger than the IUT [23]. The test code can be reused to run regression tests

Furthermore, tests that could not be implemented often reviewed func- tionality misconception or a huge gap between specification and the IUT. For instance, we could opt to focus on either abstract CTL semantics or model checking of properties expressed in CTL, having atomic propositions as labels of a state. The more abstract is the specification the hardest is harness con- struction. The less abstract the biggest the risk of overspecifying and loose sight of the test perspective.

All observable sorts were mapped to predefined types or exported libraries. Test harness has been mostly automatically generated and the manually pro- duced part has been rigorously verified. Moreover, subdomains of data in the code have been analysed and taken into account for data selection.

tion, test cases are selected and test oracles automatically generated. Special care is taken on data selection and test harness construction. This paper also discusses advantages and limitations of the approach followed, including benefits of specification-based testing on validating code and specification by providing a bridge between them.

The tests produced can be applied to check further versions of Veritas and even other similar model checkers w.r.t. to the Casl specification. This is particularly important for this application since a process of algorithms optimisation is under development. Also, they can be applied to the new tools that have been developed by the local research team to cooperate with Veritas, for instance, a space state generator. Finally, test oracles can be easily generated again to cope with changes in the specification.

