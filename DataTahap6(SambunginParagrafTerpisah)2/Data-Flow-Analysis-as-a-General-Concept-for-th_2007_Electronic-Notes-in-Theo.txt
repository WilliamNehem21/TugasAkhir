Program annotations have been suggested to improve the code generation or verification process of a JIT compiler. The term program annotation is used as a synonym for code information added to the mobile code during its generation. This information can be used by the consumer side of a mobile system to speed-up optimizations or increase security of a given program. The main challenge after transferring mobile code to the runtime environment is the verification of the trans- mitted annotations. Since annotations are additional information which are derived from the program and do not belong to the underlying mobile code the verification of program annotations is complicated. Therefore, in most projects program anno- tations are assumed to be sound [7,13] and will not come under further scrutiny. However, if the code consumer is relying on the annotations but cannot prove their correctness, semantically incorrect transformation of the program code can occur and harmful behavior could be the result.

In this paper a verifiable program annotation technique is presented which con- ceptually is based on data flow analysis and that can be used for the transmission of program information which can be modelled through a data flow framework. Our technique derives information of a program on the producer side making use of a well-known general iterative data flow algorithm. Upon completion of an analysis, our algorithm adds parts of the derived data flow information called annotation points to the transmitted code format. On the consumer side the full data flow information is reconstructed from the transmitted annotation points by using a modified version of the same general data flow algorithm that has been used on the producer side. Non-accurate transport of program annotations like manipulation can be detected from this algorithm by the fact that a fixpoint of the considered data flow problem cannot be reconstructed from its annotation points.

The paper is structured as follows: Section 2 gives a brief introduction into monotone data flow analysis, and Section 3 describes the conceptual functioning of our program annotation technique. Implementation details and results are described in section 4. In Section 5, we discuss related work and Section 6 concludes with a summary.

correctness are called monotone data flow frameworks (MDF). In the initial phase of the algorithm each instruction other than the start node is assigned to an outgoing data flow information that corresponds to the one element of the semi-lattice. The start node s of a method is assigned a special element NULL that stands for the in- formation that arrives at the start node from the different call points of the method. For intraprocedural analysis, that is what we are interested in, NULL stands for no incoming information and therefore can be represented depending on the considered data problem by the one or zero element of the semi-lattice. In the iteration phase the algorithm derives for each instruction successively the outgoing data flow infor- mation from its direct predecessor nodes. The algorithm terminates and yields as a

For the restoration of data flow information items first the start node of the control flow graph is set to NULL. Afterwards, comparable with the functioning of the general algorithm, the data flow information for each instruction will be iteratively calculated form its predecessor nodes. For assuring that the restoration of data flow information can be performed in one pass during the recovering process the nodes of the control flow graph will be traversed in reversal postorder. The use of this traversal order guarantees that always when an entry node of a loop is the runtime of the general iterative algorithm would increase because of higher complexity of semantic functions, meet- and compare operators. On the other hand, we expect a higher workload for loading and decoding the annotations, because annotations of complicated data flow problems are more complex. However, for the expected case that the time overhead introduced through loading and decoding will comprise further a small part of the overall compilation time, our algorithm also should outperform an ordinary data flow analysis when using more complicated applications.

