Biometrics is one of the most important branches of pattern recognition [1e3]. Face recognition is one of the most attractive biometric techniques. Nevertheless, face recognition in real appli- cations is still a challenging task [4]. The main reason is that the face is a non-rigid object, and it often has different appearance owing to various facial expression, different ages, different angles and more importantly, different illumination intensity. In recent years, deep learning has become more and more prevalent in computer vision. AlexNet [5] which is designed by Alex Krizhevsky got the champion of ILSVRC-2012 competition and outperformed the second place nearly 10 percents.

For deep learning algorithms in engineering applications, massive training data is necessary. Baidu researchers proposed to demonstrate the importance of enough training data for deep networks [10]. They used mobile phone cameras to collect a number of face images, and used a trained deep learning model to test the verification task, reaching an accuracy of 85% when the false alarm rate is 0.0001. After adding new training data (Chinese celebrities collected from web sites) to train the model they ach- ieved an accuracy of 92.5% when the false alarm rate is still 0.0001. This example tells us that more training data is necessary and helpful for deep networks and can lead to better performance.

If we want to solve the illumination change problem using deep networks, there must be enormous training data that have various illumination intensity. However, it is very tough to collect such kind of dataset in real word. In this paper, we design a deep network system using both visible light image and near-infrared image. Besides conventional recognition tasks of visible light face images, recognition of near-infrared face images also attracts the attention of many researchers. As we know, near-infrared face images are usually less sensitive to illumination change. Compared to near- infrared images, visible light face images could reflect more de- tails of the faces. Therefore, we apply both visible light image and near-infrared image in order to solve the illumination change problem and at the same time, preserve the advantage of visible light images. First of all, we use public visible light face data re- sources from the internet to train a deep network model, which is referred to as the first model. Then we use a number of near- infrared face images to re-train the obtained deep network model. After re-training is completed, we use the ultimate deep network model as feature extractor of near-infrared face images and refer to it as the second model. After that, we exploit the cosine distance between the test sample and training samples to get the class score and apply an adaptive score fusion strategy and nearest neighbor algorithm to conduct the final classification.

Here, we need to point out that we use two different devices to obtain face images: a visible light camera and a near-infrared camera. Therefore, our system can simultaneously collects two kinds of face images, near-infrared face images and visible light face images. The first model and second model are respectively applied to visible light face images and near-infrared face images to extract features. Then the cosine distance will be calculated to get the class score. Here the score could be considered as the correlation in- tensity that between the test sample and training samples. Because these two kinds of images are respectively beneficial to capture light-invariant features and texture features of face, we finally combine the score to conduct the final classification. The score fusion strategy combines the two separate models into a union one. According to the result of the experimental analysis, our model has a good performance in practical application scenes.

Face recognition has been a prevalent research for many years. There are many classical algorithms for face recognition. Here we can simply classify these methods into three categories: 1) Local feature method, subspace-based methods and more sparse repre- sentation methods. Local feature methods are mainly proposed to solve varying facial expression problem. As we know, the face is a non-rigid object, and the change of facial expressions and other factors will lead to changes in facial features. But researchers have found that some local features of face images do not change severely. Therefore, local features have been exploited for face recognition. For example, Gabor [11], LBP [12], SIFT [13] features all show promising performance in face recognition. 2) The basic idea of subspace-based methods is to exploit a transform factor to map the high dimensional face image to a low-dimensional feature space. PCA [14] is one of the most classical subspace-based for improving the robustness of SRC by eliminating the variations in face recognition, such as disguise, occlusion, and expression. The quintessence of sparse representation methods is the sparsity. It means most coefficients of the training samples are 0, which is advantageous for preserving useful training samples.

We can see that except for the fully connected layer, there are 5 blocks in the VGG Net model. Each block consists of several con- volutional layers followed by additional nonlinear operations, such as ReLU operation and local response normalization. The kernel size and channel number in one block are identical. The last layer of each block is always a pooling layer. So the feature map size of

The SunWin Face database contains 4000 face images from 100 identities. It has two parts: 1) 2000 visible light pictures from the 100 identities. For each person, 10 pictures are collected under normal light, the other 10 pictures per person are captured under abnormal light. 2) 2000 near-infrared pictures from the 100 iden- tities. For each person, 10 pictures are also obtained under normal light and the other 10 pictures are captured under abnormal light. The collected database contains different facial expressions, lights and other changes. A visible light camera and a near-infrared camera were used to collect data at the same time.

There are three fusion strategies: pixel-based fusion, feature level fusion and score-level-based fusion. Simultaneously using visible light and near-infrared face images can make the extracted face feature more comprehensive. Empirically, for face recognition, the score based fusion strategy is better than the feature based fusion strategy. The reason is that the feature-based fusion will cause information loss. The strategy based on score fusion avoids this defect, so the score fusion strategy can always obtain better experimental results [29]. Therefore, this paper applies the score fusion strategy to conduct the final classification.

In our face recognition system, the first model and the second model will respectively process the visible light image and near- infrared image and extract feature from both images. Then we apply the cosine distance to calculate the score of both features between test sample and training samples, Here the score could be considered as the correlation intensity that between the test sample and training sample. After that, we use the weighted combination strategy to perform score fusion [30], as shown in (1).

used face recognition algorithms, such as LBP, the accuracy is only 85.17%, far below than that of the deep network model. For the verification set YTF [31], we can get the similarity conclusion. However, the performance of both Local Binary Pattern (LBP) [32] method and Fisher Vector Faces (FVF) [33,34] method decline greatly comparing to the LFW database. The reason is that the images in the YTF database are taken from the video, which are more complex in attitudes, expressions and other factors.

In this paper, we proposed a CNN-based model which could apply both visible light image and near-infrared image to perform face recognition. Besides, we also design an adaptive score fusion strategy which is significantly helpful to improve the performance. Compared with the traditional deep learning algorithm, our pro- posed method can construct a robust face feature extraction model. In practical it is robust to illumination variation. We validate our model via several data sets. The experimental results show that the new model achieves better performance.

