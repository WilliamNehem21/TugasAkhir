Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 346 (2019) 65–76
www.elsevier.com/locate/entcs

The Geodesic Classification Problem on Graphs
Paulo Henrique Macˆedo de Arau´jo 1,3
Departamento de Computac˜ao and Campus de Quixad´a Universidade Federal do Cear´a
Fortaleza and Quixad´a, CE, Brazil

Manoel Campˆelo2,4
Departamento de Matem´atica e Estat´ıstica Aplicada Universidade Federal do Cear´a
Fortaleza, CE, Brazil

Ricardo C. Corrˆea2,5
Departamento de Ciˆencia da Computa¸c˜ao Universidade Federal Rural do Rio de Janeiro Nova Iguacu, RJ, Brazil

Martine Labb´e 6
Departement D’Informatique Universit´e Libre de Bruxelles Bruxelles, Belgium


Abstract
Motivated by the significant advances in integer optimization in the past decade, Bertsimas and Shioda developed an integer optimization method to the classical statistical problem of classification in a multi- dimensional space, delivering a software package called CRIO (Classification and Regression via Integer Optimization). Following those ideas, we define a new classification problem, exploring its combinatorial
aspects. That problem is defined on graphs using the geodesic convexity as an analogy of the Euclidean convexity in the multidimensional space. We denote such a problem by Geodesic Classification (GC) prob- lem. We propose an integer programming formulation for the GC problem along with a branch-and-cut algorithm to solve it. Finally, we show computational experiments in order to evaluate the combinatorial optimization efficiency and classification accuracy of the proposed approach.
Keywords: Classification, Geodesic Convexity, Integer Linear Programming.


https://doi.org/10.1016/j.entcs.2019.08.007 1571-0661/© 2019 Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Supervised learning denotes the automatic prediction of the behavior of unknown data based on a set of samples. It is a tool widely used in many everyday situations of the information society in which we live. In general terms, it can be described as the following two-phase procedure: in the initial phase, or training phase, the sample set is analyzed. Each sample consists of an array of encoded attributes that characterize an object of a certain type together with a label that associates a class to the corresponding object. Most commonly, only two classes are considered. A tacit assumption made at this phase is that there is an underlying pattern associated with the samples of each class that sets them apart from the samples of the other classes. Thus, the purpose of the training phase is to determine a mapping from all possible objects into the set of possible classes as an extension of an underlying patterns of the samples. Then, in the second phase, the mapping determined in the training phase is used to respond to queries for the class of objects that do not belong to the sample set.
An optimization problem is usually associated with the training phase. Re- ferred to as classiﬁcation problem, it consists in grouping similar samples so as to get clusters as internally homogeneous as possible. A wide range of solution meth- ods is available, each depending on the coding of the samples and the criterion adopted to express homogeneity. A very popular approach is to encode the sam- ples as vectors in an Euclidean space and to assume that the class patterns can be appropriately characterized by convex sets. In this vein, continuous optimization methods, including linear and quadratic programming, have been developed in the last 40 years [1,6,8,9,14]. More recently, integer linear programming tools started to be used in conjunction with continuous methods [3,13,16,17,18].
Strongly inspired by the version of the classification problem based on Euclidean convexity concepts discussed in [7], the object of study of this paper is the use of integer linear programming formulations for the resolution of a new variation of the classification problem stated in terms of notions of convexity in graphs. The statement of the new classification problem assumes the following hypotheses:
The objects are not encoded numerically. Instead, each object is characterized by its similarities with other objects. The configuration of the objects is thus represented by a similarity graph G = (V, E), connected, where V is the set of all objects and E gives the pairs of similar objects. The objects associated with the sample set constitute a proper subset of V . In addition, it is assumed that there is an underlying samples pattern that can be expressed, or at least

1 Partially supported by the Coordena¸c˜ao de Aperfei¸coamento de Pessoal de N´ıvel Superior - Brasil (CAPES) - Finance Code 001.
2 Partially supported by the Brazilian agencies CAPES, CNPq (Proc. 305264/2016-8, Proc. 443747/2014- 8) and FUNCAP (PNE-0112-00061.01.00/16).
3 Email: phmacedoaraujo@lia.ufc.br
4 Email: mcampelo@lia.ufc.br
5 Email: correa@ufrrj.br
6 Email: mlabbe@ulb.ac.be

approximated, by the notion of geodesic convexity in graphs, defined with re- spect to the shortest paths in G (analogously to the definition of Euclidean convexity with respect to the Euclidean distances between points) [15].
The sample set may contain an arbitrary number of misclassified objects, called outliers, resulting from possible sampling errors or due to inherent characteris- tics of the phenomenon being modeled. From the mathematical point of view, an outlier is that object that causes the underlying pattern of the samples in its class to deviate from the convexity definition. The possible occurrence of outliers poses an additional challenge to any method used to solve the classi- fication problem since they need to be detected and disregarded of accurate solutions.
Considering the hypotheses above, the Geodesic Classiﬁcation (GC) problem tackled in this paper becomes purely combinatorial. However, as detailed later, a solution is neither a covering nor a partitioning of G in convex sets in the sense studied in [2,5]. Thus, the study of the GC problem is of theoretical interest since it refers to the possibility of establishing new problems on graphs which may bring subsidies to resolution methods, including those based on Euclidean convexity. An- other motivation with a more practical bias is to allow to encode objects similarities through some binary relation over the sample set. Many applications such as text mining, communities detection in social networks, historic files similarity predic- tion, recommendation systems, and spam filtering are potential beneficiaries of this approach [10].
To the best of our knowledge, the GC problem was not yet studied in the liter- ature. The main purpose of this work is to propose and study algorithmic methods to solve it. To this end, we state an integer programming formulation and derive a family of facet-defining inequalities. A branch and cut method is then presented. Some computational experiments are described, showing that the solution method proposed is efficient for small and medium size instances. A particular characteristic of the implementation performed was that a lazy constraints scheme and cutting plane algorithm were fundamental to reduce its running time. The accuracy of the geodesic convexity approach is validated by comparing the prediction provided by the algorithm proposed with the one obtained for similar instances with SVM and MLP (these are two of the most used approaches for the Euclidean convexity classification problem).
This text is organized as follows. After presenting some basic concepts and notation on graphs in Section 2, we formalize the Geodesic Classification Problem. An integer linear formulation is presented in Section 3 together with a family of facet-defining inequalities. In Section 4, we present the developed algorithms and computational experiments. Finally, in Section 5, we give some concluding remarks and future work directions.

Problem Statement
Let V be a finite set of objects that are related through a binary relation described by the connected undirected graph G = (V, E), |V | = n. Two distinct objects are similar if the corresponding vertices are adjacent in G (analogously, two vertices v, w ∈ V , v /= w, are called similar if vw ∈ E). We also refer to G as similarity graph, and we adopt the standard terminology used in Bondy and Murty’s book [4]. In particular, a path between v, w ∈ V is the sequence ⟨v⟩, if v = w, or a sequence of distinct vertices P = ⟨v = v1, v2,..., vl = w⟩ such that vivi+1 ∈ E for i = 1,...,l − 1. The length of a path is given by its number of edges, i.e. the length of
⟨v⟩ is zero and that of P is l − 1. A geodesic between v, w ∈ V is a shortest path between v and w and its length is denoted by δ(v, w). The closed interval I[v, w] is the set of all vertices lying on a geodesic between v and w. More generally, given S ⊆ V , I[S]= u,v∈S I[u, v]. In this case, if I[S]= S, then S is a convex set. The convex hull of S, denoted by H[S], is the smallest convex set containing S. We also define Dvw = I[v, w] \ {v, w}.
For the definition of the 2-class classification problem considered in this paper, we are given two nonempty subsets VB, VR ⊆ V , VB ∩VR = ∅, so that VBR = VB ∪VR represents the sample set and VN = V \VBR. The sets VB and VR define the blue class and red class vertices, respectively. The remaining vertices are called unclassiﬁed vertices. An example of a similarity graph and its sample set is depicted in Fig. 1. In analogy to the Euclidean version of the problem, we define that G is linearly separable with respect to AB ⊆ VB and AR ⊆ VR if
H[AB] ∩ AR = ∅,
H[AR] ∩ AB = ∅, and
H[AB] ∩ H[AR] ∩ VN = ∅,
and linearly inseparable otherwise. If G is linearly inseparable with respect to some sample set, it can be turned linearly separable if some vertices are disregarded (i.e., considered as outliers). Such a situation occurs in the example of Fig. 1. Indeed, G is linearly inseparable because v ∈ H[VB] ∩H[VR], but it becomes linearly separable if either v1, v2, v3 or v4 is considered as an outlier. The classification problem associated with G, VB, and VR asks for the smallest number of disregarded vertices, as follows.

Fig. 1. Example of a similarity graph with blue class (as solid circles) and red class (as solid squares) vertices.

Definition 2.1 Given G, VB, and VR, the Geodesic Classiﬁcation (GC) problem asks for groups AB ⊆ VB and AR ⊆ VR that maximizes |AB ∪ AR| and makes G linearly separable with respect to AB and AR.
It is worth noting that AB and AR define a mapping from V onto {blue, red} classes which classifies all unclassified in H[AB] as blue class vertices and all unclas- sified vertices in H[AR] as red class vertices. In addition, this mapping arbitrarily classifies the unclassified vertices in V \ (H[AB] ∪ H[AR]). Note that the vertices in (H[AB] ∩ VR) ∪ (H[AR] ∩ VB) are considered as outliers and are not reclassified (that is, moved to the other class). Due to the outliers and the vertices arbitrarily classified, each class does not necessarily define a convex set.
Integer Formulation for the GC Problem
To describe an integer formulation, let us define some notation. We denote by TK(v) the set {S ⊆ VK | v ∈ H[S] \ S}, for K ∈ {B, R}. By the definition of geodesic convexity, |S|≥ 2 for all S ∈ TB(v) ∪ TR(v).
Variables and valid inequalities
Two types of binary variables are defined. For every v ∈ VBR, av = 1 means that vertex v ∈ VB (v ∈ VR) belongs to group AB (AR), and av =0 if v is disregarded. On the other hand, for every v ∈ VN , pv indicates the class of v as follows: if v ∈ H[AB] (resp. v ∈ H[AR]), then pv = 0 (resp. pv = 1); otherwise, if v does not belong to the convex hulls of both groups, then pv can be set arbitrarily. Thus, the

formulation aims at maximizing
v∈VBR
av subject to

av + Σ aw ≤ |S|,	(v ∈ VB,S ∈ TR(v)) or (v ∈ VR,S ∈ TB(v))	(1)
w∈S
pv +	aw ≤ |S|,	v ∈ VN ,S ∈ TB(v)	(2)
w∈S
— pv +	aw ≤ |S|− 1,	v ∈ VN ,S ∈ TR(v)	(3)
w∈S
av, pu ∈ {0, 1},	v ∈ VBR,u ∈ VN	(4)
Constraint (1) is the analogous of the convex-inclusion inequality defined in [7] in the single-group case. It ensures that v ∈ VBR turns to be an outlier whenever it belongs to the convex hull of a set S of samples in the opposite class or at least one point in S must be an outlier. The covering constraints are defined by (2) and (3). If v ∈ VN ∩ H[AB] (resp. v ∈ VN ∩ H[AR]), then pv must get 0 (resp. 1). Hence, v cannot lie in the convex hull of both groups of opposite classes simultaneously.
Let P be the convex hull of the points satisfying (1)-(4). We denote by ev the n-sized binary vector with 1 at the entry v and 0 otherwise. The entry refers to the value of the variable av, if v ∈ VBR, or pv, if v ∈ VN . Since the n points {ev | v ∈ V } clearly represent feasible solutions for P and are all linearly independent points, P

is full-dimensional. The following rank-1 Chv´atal-Gomory inequalities result from a configuration of geodesic convex combinations that cannot occur in a Euclidean space.
Proposition 3.1 If v, vj ∈ VB and w, wj ∈ VR are distinct vertices such that
{v, vj}⊆ Dww′ and {w, wj}⊆ Dvv′ , then
av + av′ + aw + aw′ ≤ 2	(5)
is a facet-deﬁning inequality for P.
Proof. The validity of (5) stems from the fact that it is a {0, 1 }-Chv´atal-Gomory cut with multiplier 1/3 for each constraint of type (1) defined for (v, {w, wj}), (vj, {w, wj}), (w, {v, vj}), and (wj, {v, vj}). To show that it defines a facet of P, we consider feasible solutions satisfying (5) at equality. Each one of these solutions is associated with a vertex u ∈ V and is represented by a n-sized binary vector au. The n feasible solutions we consider are the following:


⎪ e + e ′ ,	if u = vj
v	v
⎪
ev′ + ew,	if u = w
au = ⎪⎨ ev′ + ew′ ,	if u = wj


ew + ew
⎪
′ + eu +	Σ
ez, if u ∈ VR
′
\ {w, wj}

z∈VN ∩H[{u,w,w }]
⎪
⎩ ev + ew + eu,	if u ∈ VN
To show that they are affinely independent, let αu ∈ R, for all u ∈ V , be multipliers such that  u∈V αuau = 0.  First note that the variable au, for all u ∈ VBR \
{v, vj, w, wj}, is set to 1 only in au. Thus, αu = 0 in these cases and, consequently,
αvav + αv′ av′ + αwaw + αw′ aw′ + u∈VN αuau = 0. Among the solutions associated with the vertices in VN , the variable pu gets 1 only in au. Hence, αu = 0 in these cases as well. For the remaining multipliers, we have
αv + αv′ =0 
αv′ + αw + αw′ =0 
αv + αw =0 
αw′ =0 
which implies that αv = αv′ = αw = αw′ = 0. Therefore, all n solutions are affinely independent.	2
Checking for Feasibility
Since the number of convex-inclusion and covering constraints may be very large, the following lazy constraint scheme is useful for checking feasibility of integer solutions.

It comprises two steps. First, we look for a pair (v, S) that violates a corresponding convex-inclusion or covering constraint. To this end, it is sufficient to explicitly compute the convex hull of both groups, AB and AR, of the solution (a, p) at hand, and to check whether there is:
v ∈ VN ∩ H[AB] ∩ H[AR]: in this case, we define S = AB if pv = 1 or S = AR if
pv = 0.
v ∈ AB ∩ H[AR]: S is defined to be AR. v ∈ AR ∩ H[AB]: S is defined to be AB.
The second step consists of determining a W ⊆ S that is minimal (i.e., v /∈ H[W\u], for any u ∈ W ) and such that the corresponding convex-inclusion or covering con- straint for (v, W ) is still violated. Then, the violated constraint is added to the model as a lazy constraint. The minimal subset W can be determined with the following O(n4) algorithm. We start with W = S and, as long as there exists a vertex u ∈ W such that the convex hull of W\{u} still reaches v, then we remove u from W . This procedure finishes when W becomes minimal.
To calculate the convex hull S of AB (or AR), we start with Sj = AB (or Sj = AR) and iteratively update it. At each iteration, we add to Sj all vertices in Duv, for every pair u, v ∈ Sj (with at least one of them added to Sj in the previous iteration). This step is repeated until Sj does not change. At this point, we get S = Sj. Sets Duv can be determined a priori with the BFS-like algorithm described in the next section.
Algorithms and Computational Experiments
In this section, we describe the method used to solve the problem and how it was implemented. We also present some results of computational experiments carried out with an Intel i7-7700 processor with 3.6 GHz, 8 cores and 32 GB RAM memory, running a 64 bits Linux OS. The algorithms were coded in C++, and CPLEX 12.8 was the optimization solver used. Essentially, we embedded separation procedures in the default branch-and-cut method provided by CPLEX.
Solution Method
The main steps of our solution method are the following:
Computation of the shortest paths (Complexity O(n(n + m))): For each v ∈ V , we determine sets Dvw, for all w ∈ V \ {v}, by applying a breadth-first search algorithm from v. Recall that Dvw comprises all internal vertices in all shortest paths from v to w.
Initial cutoff: Since a trivial solution is obtained by taking all vertices of a class as outliers, min{|VB|, |VR|} is provided as a cutoff.
Initial model configuration: All inequalities (5) are included in the initial model by exhaustive enumeration of all sets computed in Step (i) (none of the con- straints (1)-(3) are used initially).

Partial linear relaxation resolution: At the root node of the branch-and-cut tree, we solve the linear relaxation of the initial model together with constraints (1)-(3), for each pair (v, S) with |S| = 2, separated as cuts. The separation procedure is implemented as a Cut Callback macro of CPLEX.
Exact model resolution: Starting from the model obtained in Step (iv), we add the integrality constraints (4) and solve the integer formulation by adding (1)-(3) as lazy constraints (with a Lazy Callback procedure).

Testbed
To test the developed algorithm, two sets of instances were used in the experi- ments, namely a set of random and a set of real world inspired instances. The ran- dom instances were categorized by number of vertices (n ∈ {50, 100, 150, 200, 250}), graph density percentage (d ∈ {5, 10, 20, 30, 50, 70}) and initially classified vertices percentage (p ∈ {20, 40, 60, 80}), where n = |V |, d = 100|E|/(n(n − 1)/2), and p = 100|VBR|/n. For each combination (n, d, p), we generated 10 random instances. Therefore, there are 1200 random instances in total.
The real world inspired instances were derived from two datasets for the Eu- clidean version of the classification problem, namely Parkinson’s disease [12] (195 points with 22 attributes) and cardiac Single Proton Emission Computed Tomog- raphy (SPECT) images [11] (267 points with 44 attributes), both available at https://archive.ics.uci.edu/ml/datasets.html. Each point in these datasets gives an information of a patient to be used to predict new diagnostics. We derived 10 instances for the GC problem for each instance in the datasets by constructing a graph G = (V, E) and the sets VB and VR. The vertex set V represents the samples of the dataset and the edge set is defined using the transformation described in [19]. The vertices in VB and VR are randomly chosen such that 20% of the vertices are left unclassified in the instance generated. The predefined classes of the vertices chosen as unclassified are available in the dataset and were used to test the efficiency and accuracy of the algorithms.

Results for Random Instances
The experiments with random instances were used to analyze the combinatorial aspects of the problem, notably the effect of the constraints added to the model at steps (iii) and (iv). For the sake of comparison, we tested two other versions of the algorithm, each consisting of the elimination of Step (iii) or Step (iv). The results observed are summarized in Tab. 1, showing the performance of the three versions tested with respect to a standard implementation with no constraints added (i.e.., eliminating both steps (iii) and (iv)). We could note that the inequalities (5) were extremely effective: on average, there were 2.8|V | inequalities added and they reduced 81% of the running time. The constraints (1)-(3) added when solving the root node at Step (iv) were very effective as well. It was much better than setting all of them in the initial model. On average, there were 18|V | constraints added and they reduced 75% of the running time. However, using the separator for constraints


Table 1
Performance of the separation algorithms.

(1)-(3), for each pair (v, S) with |S| = 3 did not give better results. For |S| > 3, it showed to be not practical to use a separation algorithm for them.
Regarding the lazy constraints scheme presented in Section 3.2, its application at Step (v) was fundamental to reduce the running time (with respect to an im- plementation with all constraints (1)-(3) added to the initial model). Actually, it is impractical to solve the problem without the lazy constraints scheme since the number of constraints (1)-(3) is potentially exponential. Considering all random in- stances, the average running time of algorithm was about a few seconds, so it shows to be very good even for medium size instances. In general, random instances with density between 10% and 30% are harder to solve, although the worst running time was of 52 seconds for an instance with 250 vertices and 5% of density. Moreover, it seems that, for p ≤ 40 and for sufficient large n, the running time decreases expo- nentially as the density increases. This is an evidence that the proposed algorithm works extremely well, especially for dense, medium sized instances.

(a) p = 20	(b) p = 40
 
(c) p = 60	(d) p = 80
Fig. 2. Average time versus density.


Realistic Instances
In order to evaluate the accuracy of our classification method (GC), we tested it with the Parkinson’s disease and SPECT instances. Besides, we compared its per-


Table 2
B&C Algorithm, SVM and MLP comparison for Parkinson’s instances.

Table 3
B&C Algorithm, SVM and MLP comparison for SPECT instances.

formance with those obtained by the classic Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP) methods. These are approaches for the classification problem in the Euclidean space. We used the implementations of SVM and MLP available at http://scikit-learn.org/stable/modules.html. It is worth observing that SVM and MLP were applied to the original Parkinson’s desease and SPECT in- stances (before the transformation to graph instances).
Tables 2 and 3 summarizes the computational results for for the Parkinson’s dis- ease and SPECT instances, respectively. The following information is displayed for each instance: (i) instance identification in the format <dataset>-n<n>-d<d>- p<p >-<sequential-number>, (ii) the diameter of the similarity graph (Diam),
(iii) minimum and maximum degree of a vertex (Dgmin and Dgmax), (iv) mini- mum number of outliers (OPT), (v) running time of our B&C algorithm in seconds (TGC(s)), (vi) percentage of the unclassified vertices/points correctly classified by GC (AcuGC(%)), SVM (AcuSV M (%)) and MLP (AcuMLP (%)).

A first remark concerns the high number of outliers in the optimal solution. Re- member that we are using linear separation and these real world inspired instances are more unlikely to have such a property.
Regarding the Parkinson’s disease instances, we can see that GC obtained the best accuracy in 4 of them, while 5 and 6 were the corresponding scores for SVM and MLP, respectively. However, on average, SVM got the worst accuracy, due to the poor performance in the ninth instance. GC and MLP obtained similar average accuracy with a slight advantage to the latter. For these instances, we could observe higher running times in comparison with the random instances. This was possibly due to the larger diameters and lower maximum degrees of the input graphs.
The results for the SPECT instances were similar to those for the random in- stances. We can observe that the running times of our B&C algorithm are much smaller than those for the Parkinson’s instances. In the SPECT graphs, the diam- eters are lower and the maximum degree are higher. The GC approach presented the best accuracy in 7 SPECT instances, while SVM and MLP did it in 2 and 6 instances, respectively. On average, GC got also the best accuracy, similar to the one by MLP.
Concluding Remarks
In this work, we defined the Geodesic Classification (GC) problem as the analogous, on graphs, of the Euclidean Classification problem. In addition, we proposed an in- teger programming formulation for this new combinatorial optimization problem, as well as a family of facet-defining inequalities and a branch and cut method to solve it. An interesting point of these results is that the family of facet-defining inequal- ities is related to a structure of mutual convex combinations that are not possible in the Euclidean space. Results with some computational experiments show that the solution method proposed is very promising since the algorithm for the integer formulation proved to be very efficient, even for medium size instances. A particular characteristic of the implementation performed was that a lazy constraints scheme and cutting plane algorithm were fundamental to reduce its running time. We vali- dated the accuracy of the geodesic convexity approach by comparing the prediction provided by the algorithm proposed with two of the most used approaches for the Euclidean convexity classification problem, namely, SVM and MLP. The results are very promising, since the prediction accuracy of the GC approach showed to be stable and as good as such classic linear separation algorithms for the multidimen- sional space. As future works, we intend to carry on a deeper polyhedral study in order to improve the performance of the algorithm.

References
Arthanari, T. S. and Y. Dodge, “Mathematical Programming in Statistics,” Wiley-Interscience, New York, 1993.
Artigas, D., S. Dantas, M. C. Dourado and J. L. Szwarcfiter, Partitioning a graph into convex sets, Discrete Mathematics 311 (2011), pp. 1968–1977.

Bertsimas, D. and R. Shioda, Classification and regression via integer optimization, Operations Research 55 (2007), pp. 252–271.
Bondy, J. and U. Murty, “Graph Theory,” Springer, 2008.
Buzatu, R. and S. Cataranciuc, Convex graph covers, The Computer Science Journal of Moldova 23
(2015), pp. 251–269.
Carrizosa, E. and D. R. Morales, A mixed integer optimisation model for data classification, Computers & Operations Research 40 (2013), pp. 150–165.
Corrˆea, R. C., D. D. Donne and J. Marenco, On the combinatorics of the 2-class classification problem, Discrete Optimization 31 (2019), pp. 40–55.
Cortes, C. and V. Vapnik, Support-vector networks, in: Machine Learning, 1995, pp. 273–297.
Freed, N. and F. Glover, Evaluating alternative linear programming models to solve the two group discriminant problem, Decision Sciences 17 (2007), pp. 151–162.
Hong, L. and B. D. Davison, Empirical study of topic modeling in twitter, in: Proceedings of the First Workshop on Social Media Analytics, SOMA ’10 (2010), pp. 80–88.
Kurgan, L. A., K. J. Cios, R. Tadeusiewicz, M. Ogiela and L. S. Goodenday, Knowledge discovery approach to automated cardiac spect diagnosis, Artificial intelligence in medicine 23 (2001), pp. 149– 169.
Little, M. A., P. E. McSharry, S. J. Roberts, D. A. Costello and I. M. Moroz, Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection, BioMedical Engineering OnLine 6 (2007), p. 23.
Maskooki, A., Improving the efficiency of a mixed integer linear programming based approach for multi- class classification problem, Comput. Ind. Eng. 66 (2013), pp. 383–388.
Pardalos, P. M. and P. Hansen, 45, American Mathematical Society, Providence, RI, 2008.
Pelayo, I. M., “Geodesic Convexity in Graphs,” Springer-Verlag, New York, 2013.
Sun, M., A mixed integer programming model for multiple-class discriminant analysis, International Journal of Information Technology and Decision Making 10 (2011), pp. 589–612.
Uney, F. and M. Turkay, A mixed-integer programming approach to multi-class data classification problem, European Journal of Operational Research 173 (2006), pp. 910–920.
Xu, G. and L. G. Papageorgiou, A mixed integer optimisation model for data classification, Comput. Ind. Eng. 56 (2009), pp. 1205–1215.
Zaki, M. J. and W. M. Jr, “Data Mining and Analysis: Fundamental Concepts and Algorithms,” Cambridge University Press, New York, NY, USA, 2014.
