Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 288 (2012) 75–86
www.elsevier.com/locate/entcs

Summarized Dimensions Revisited
Holger Siegel1 and Axel Simon1
Technische Universität München 85748 Garching, Germany

Abstract
The task of summarizing memory regions becomes increasingly important in the analysis of data structures and the analysis of recursive functions. Calculating summaries of memory regions containing several abstract variables becomes quite subtle when using relational abstract domains. We address this challenge using two principal operations, fold and expand , that calculate such summaries for any numeric relational domain and argue optimality. We thereby generalize an earlier approach in the literature that was geared towards memory cells modelled by a single abstract variable.
Keywords: abstract interpretation, numeric domains, summarization


Introduction
Abstract interpretation is increasingly applied in the context of software verification. For this task, expressive abstract domains are required that are able to express relational information, that is, how the value of one variable is restricted by the value of another variable. For instance, when x = y is known, restricting x by a test x ≤ 5 will also restrict y such that y ≤ 5. Also, as the number of memory cells allocated in a program cannot be bounded a priori, it is crucial to summarize the information contained in different memory cells into one abstract memory cell to ensure tractability.
Suppose that x1 and x2 represent the content of two memory cells that are to be summarized into xs where x1 ∈ [0, 5] and x2 ∈ [5, 10] are kept in the standard interval domain [4]. Here, it seems obvious to merge the information on x1 and x2 by using the join operation of the interval domain, leading to xs ∈ [0, 10]. Indeed, this approach was proposed by Gopan et al. [3] who addressed the task of summa- rizing the cells of arrays. However, the task of summarizing memory cells becomes more intricate when using a relational domain, for instance, linear inequalities [2].

1 Email: firstname.lastname@in.tum.de. Supported by DFG EN SI 1579/1.

1571-0661 © 2012 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2012.10.009

Suppose now that x1, y1 represent the first memory cell and x2, y2 represent the second memory cell that are to be summarized into the cell xs, ys where xi = yi +5 and y1 ∈ [0, 5], y2 ∈ [5, 10]. One way forward is to extract the intervals of x1 and of x2 and to set xs to their join [5, 15] (analogously for y1, y2 and ys). However, the resulting values for xs, ys lack the relational property xs = ys +5 that held for both, x1, y1 and x2, y2. In this paper we tackle the challenge of defining a summary operation that retains relational information between all components of a memory cell. Moreover, we discuss different options of summarizing information in memory cells and their precision trade-offs.
Overall, this paper makes the following three contributions:
We present different ways of summarizing relational numeric content of memory cells and their impact on the precision of the analysis,
we detail an operation fold for summarizing memory cells and its dual, expand, using standard numeric domain operations,
we show optimality of these operations, assuming that the domain operations join, meet, projection and the swapping of two variables are optimal.
The remainder of the paper is organized as follows: After illustrating our tech- niques on a few examples, Sect. 2 formalizes the summarization of relational infor- mation using a generic concrete numeric domain. The herein developed operations are then lifted to abstract domains in Sect. 3 where their optimality is proven. We conclude the paper in Sect. 4.

Principles of Summarizing Memory Cells
Static analysis for finding faults in software amounts to a reachability analysis of nu- meric properties, which is done by calculating the set of possible values the program variables can take at each program point.
For tractability, these states are approximated by so-called abstract domains that map a set of variables to some abstract properties. While it seems plausible that the value of each program variable is represented by exactly one variable in the abstract domain, this no longer holds true for more sophisticated analyses. A C variable that holds a struct may be represented by several abstract variables, namely one per field. Also, the elements of an array or several dynamically allocated heap regions may be represented by the same abstract variable. In the latter case, the abstract variable over-approximates the content of each concrete memory cell. For this case, Gopan et al. [3] discuss how to implement transfer functions that manipulate these summarizing variables. In order to illustrate their approach, consider the following example.
Example 2.1 The following C code fills the array a with increasing integers: i n t a [100]; 
f o r ( i n t  i =0; i <100; i ++) a [ i ]= i ;

In the first iteration, the write access to the array will create a memory cell represented by the variable x in the abstract domain. This cell is initialized to zero, since i, the right hand side, is zero. In the next iteration, the write access to a [1] creates a second memory cell whose value 1 is represented by, say, xj. For tractability, the new memory cell is merged with the existing one by an operation called fold . The net effect is that afterwards xj no longer exists while x contains a summary of the previous values of x and xj, for example, the interval [0, 1].

The work of Gopan et al. addresses the analysis of arrays over single scalar values which is why their expand and fold operations only manipulate a single variable at a time. In particular, their method can lead to a significant loss of precision when summarizing compound memory cells. While arrays occasionally range over com- pound memory cells, they are ubiquitous in the analysis of heap allocated structures. This is illustrated in the next example.

Example 2.2 Consider the nodes of a B-Tree in memory. Each node contains an ordered list of n indices together with n +1 pointers to subtrees. For the sake of presentation, we ignore the pointers between the different nodes and concentrate on the numeric properties of each node. A simple B-Tree can thus be represented as follows:


A crucial invariant of every node i is that the right index ri is always greater than the left index li. One aspect of analyzing operations on B-Trees is to verify that this invariant is maintained.
The fold operation of Gopan et al. is too imprecise for this purpose, as it would summarize the left indices into ls ∈ {1, 3, 4, 8} and right indices into rs ∈ {2, 5, 6, 9}, so that ls < rs does not necessarily hold. We now present a way to summarize these nodes without losing relational information between their components.


Preliminaries
Before we consider actual abstract domains, we illustrate the idea of summarizing numeric information by operations on sets of concrete values. Specifically, let V denote a fixed set of numeric values, such as R, Z or some finite interval, that represents the values a single memory cell can hold. Let Cn := ℘(Vn) denote the numeric domain of dimension n, ordered by set inclusion. A vector c ∈ Cn then represents the state of the n variables that are in scope at a program point.
For every dimension n we define functions dropn : Cn → Cn—1, functions addn :
i	i

Cn → Cn+1 and functions swapn : Cn → Cn, such that

dropn(c)= {(x1,..., xi—1, xi+1,..., xn) | (x1,..., xn) ∈ c}
addn(c)= {(x1,..., xi—1, y, xi,..., xn) | (x1,..., xn) ∈ c, y ∈ V}
swapn (c)= {(..., xj,..., xi,.. .) | (..., xi,..., xj,.. .) ∈ c}

for every state c ∈ Cn. In the following, we omit the superscript n when it is clear from the context. Intuitively, function dropi removes dimension i thereby discarding all information pertaining to variable xi. Function addi adds a new dimension i that corresponds to a new, unrestricted variable xi. Function swapi,j exchanges the information stored in dimensions i and j.
In addition, we abbreviate sequences of operations as follows, assuming that ab . . .
as well as ajbj ... are ascending lists of dimensions that have no common elements:
dropab... = dropa ◦ dropb...
addab... = addb... ◦ adda
swapab...,a′b′... = swapa,a′ ◦ swapb...,b′...

These operations form the basis of summarizing the information of several variables which is the topic of the next section.

Folding Memory Cells Element-Wise
With the help of the previously defined functions, we can now define functions

foldn
: Cn → Cn—1 that summarize two dimensions i and j into dimension i:
foldn (c) := dropj (c ∪ swapi,j (c))

and their counterparts expandn  : Cn → Cn+1 by
expandn (c) := addj(c) ∩ swapi,j (addj(c))
These functions are equivalent to the functions foldi,j and expandi,j presented by Gopan et al. [3]. Function foldi,j summarizes the information contained in the variables xi and xj into the variable xi. Function expandi,j restores dimensions i and j from the summarized dimension i, so that xi and xj contain an approximation of their previous content. For instance, in Example 2.1 we have variables (x, xj, i) ∈ c = {(0, 1, 1)} after two loop iterations. The dimensions x and xj are summarized by calculating foldx,x′ (c)= {(0, 1), (1, 1)}.
The element-wise folding of memory cells has limitations when it comes to sum- marizing compound memory cells, like nodes of the B-Tree in Example 2.2. Con- sider the task of summarizing the second and third child node of the B-Tree into
one summarized node. We first introduce variables a,...,h to represent the con- tents of the nodes which store the information that B := {(3, 6, 1, 2, 4, 5, 8, 9)} e 

(a, b, c, d, e, f, g, h):
The 8-dimensional numeric domain now has to be summarized into a 6-dimensional numeric domain by merging dimensions g and h into dimensions e and f , respec- tively. This is done by calculating folde,g (foldf,h(c)):




fold




e,g



(fold




f,h



(B)) = fold

⎛⎧(3, 6, 1, 2, 4, 5, 8),⎫⎞
e,g	=
(3, 6, 1, 2, 4, 9, 8)
⎧⎪(3, 6, 1, 2, 4, 5),⎫⎪
⎪⎨(3, 6, 1, 2, 8, 5),⎪⎬
(3, 6, 1, 2, 4, 9),
⎪⎩(3, 6, 1, 2, 8, 9) ⎪⎭


In order to regain information about the concrete cells, we re-expand this set by applying expandf,h ◦ expande,g to it, obtaining the following 16-element set:
{(3, 6, 1, 2, e, f, g, h) | e, g ∈ {4, 8}∧ f, h ∈ {5, 9}}
Notice that for e =8 and f =9 we have lost the information that e < f , which makes this summarization unsuitable to verify the B-Tree invariant of increasing indices. Indeed, this summarization method is rather weak, in particular when relational information is of interest. The next section investigates a novel summarization method that honors relational information within a node.

Folding Compound Memory Cells en bloc
We now illustrate how to summarize complete nodes at once. To this end, define the following generalisations of the fold and expand operations:
fold ab...,a′b′...(c)= dropa′b′...(c ∪ swapab...,a′b′...(c))
expandab...,a′b′...(c)= adda′b′...(c) ∩ swapab...,a′b′...(adda′b′...(c))

Here, the intention is that ab . . . represent all variables of a compound memory cell, and ajbj ... those of the memory cell to be merged into the former.
The intuition is that relational information that holds between ab . . . and ajbj ... is retained during the fold operation. We illustrate this behavior in the B-Tree example by, again, folding the second and third child node into one summary node:


fold


ef,gh
({(3, 6, 1, 2, 4, 5, 8, 9)})= ⎧⎨(3, 6, 1, 2, 4, 5),⎫⎬
⎩(3, 6, 1, 2, 8, 9) ⎭

Note that all vectors in the resulting set obey the invariant of the B-Tree in that e < f . This invariant is maintained even when expanding the summarized node into two again:




expand
⎧(3, 6, 1, 2, 4, 5, 4, 5),⎫
⎨(3, 6, 1, 2, 4, 5),⎬	= ⎪⎨(3, 6, 1, 2, 4, 5, 8, 9),⎪⎬

ef,gh ⎝⎩(3, 6, 1, 2, 8, 9) ⎭⎠
(3, 6, 1, 2, 8, 9, 4, 5),
⎪⎩ (3, 6, 1, 2, 8, 9, 8, 9) ⎪⎭

Although the two nodes are summarized, the important relational information be- tween the components of each node has been preserved. Indeed, not only does our summarization method yield more precise results, it is also cheaper, as only one join and meet operation is required to calculate fold and expand , respectively. In contrast, folding element-wise requires one such operation for each dimension that is summarized.

Discussion
One question that arises is whether folding memory en bloc is always more precise than folding element-wise. Surprisingly, this is not the case. This is illustrated in the following example that exhibits a 4-dimensional state on which folding and expanding two dimensions is exact using the element-wise method but approximate when using the en bloc method.
Example 2.3 Consider two compound memory cells, consisting of two elements each, labelled a, b and c, d. Let their values be given by:
(a, b, c, d) ∈ {(0, 0, 0, 0), (1, 1, 1, 1))}
First, consider summarizing dimensions element-wise, starting with indices b and d, followed by indices a and c:


fold

fold
⎛⎧⎨(0, 0, 0, 0),⎫⎬⎞⎞ = fold
⎛⎧⎨(0, 0, 0),⎫⎬⎞ = ⎧⎨(0, 0),⎫⎬

a,c ⎝
b,d ⎝⎩(1, 1, 1, 1) ⎭⎠⎠
a,c ⎝⎩(1, 1, 1) ⎭⎠	⎩(1, 1) ⎭

Unfolding the summarized dimensions by the reverse operation expandb,d ◦expanda,c
reconstructs the original 4-tuple {(0, 0, 0, 0), (1, 1, 1, 1))}.
Now, consider summarizing dimensions en bloc:


fold
⎛⎧⎨(0, 0, 0, 0),⎫⎬⎞ = ⎧⎨(0, 0),⎫⎬

ab,cd ⎝⎩(1, 1, 1, 1) ⎭⎠	⎩(1, 1) ⎭

Seemingly, we have obtained the same abstraction. But when it is expanded by the corresponding operation expandab,cd, the original set is not restored. Instead, it is

approximated by a greater set:


expand



ab,cd
{(0, 0, 1, 1)})= B := ⎧⎨(0, 0, 0, 0), (0, 0, 1, 1),⎫⎬ .
⎩(1, 1, 0, 0), (1, 1, 1, 1) ⎭


Thus, in this example, summarizing element-wise turns out to be more precise. In general, a folda,c operation retains the relations that a as well as c has with other dimensions. In the context above, both a and c are equal to b so that this information is retained in the summary.
In order to address the question which method is more suitable for the purpose of program analysis, we consider a second example in which summarization en bloc is more precise: Consider the states (a, b, c, d) ∈ B from above. Here, applying the element-wise summary first on b, d then on a, c yields:

folda,c(foldb,d(B)) = Bj := {(0, 0), (0, 1), (1, 0), (1, 1)}

The element-wise expansion of this most-general state results in a state that is greater than B, since expandb,d(expanda,c(Bj)) = {0, 1}4. In contrast, the summa- rization en bloc is able to re-construct the original set B:
expandab,cd(foldab,cd(B)) = expandab,cd({(0, 0), (1, 1)})= B
The en bloc summarization is more precise in this example since it can express that the dimensions that are not summarized are equal, that is, the fact that a = b and c = d is retained. Intuitively, by summarizing two sets of dimensions, our en bloc method will retain all relational information that exists amongst the dimensions in each set. In contrast, the element-wise method looses this relation and rather sporadically recovers relational information with variables in the environment. Thus, the en bloc summarization reliably retains information that holds within a compound memory cell while it is difficult to find use cases for the relational information the element-wise method infers.
The existence of two summarization approaches begs the question whether there are others and, in particular, if there is a best method. Indeed, given two lists of indices ab . . . and ajbj .. ., any partitioning P of these lists can be used to apply fold and expand |P | times, once for each partition. Each choice of partitioning leads to a different abstraction and, thus, precision.
In the following, it is shown that there exists no best partitioning; indeed, while each choice of a partitioning leads to a different abstraction, each of these abstrac- tions has the same precision.
We commence by pointing out that a Galois insertion consists of two functions, of which one is injective and one is surjective. Then the following proposition helps in reasoning about the cardinality of summarizations:
Proposition 2.4 Each pair foldab...,a′b′... and expandab...,a′b′... is a Galois insertion.

Proof. Being composed of monotone functions, fold ab...,a′b′... and expandab...,a′b′...
are also monotone. Also, there is
fold ab...,a′b′...(expandab...,a′b′...(c))
= dropa′b′...(adda′b′...(c) ∩ swapab...,a′b′...(adda′b′...(c))) = c

expandab...,a′b′...(foldab...,a′b′...(c))
= adda′b′...(dropa′b′...(h)) ∩ swapab...,a′b′...(adda′b′...(dropa′b′...(h)))
⊇ h ∩ swapab...,a′b′...(h)= h ⊇ c
with h = c ∪swapab...,a′b′...(c). The first inequality holds because adda′b′... ◦ dropa′b′... is extensive and the second holds because h = swapab...,a′b′...(h). Thus, pair fold ab...,a′b′... and expandab...,a′b′... form a Galois insertion.	2
The next proposition states the result for finite value sets V. It will then be generalized to arbitrary value sets.
Proposition 2.5 If V is finite, then no Galois insertion between Cn and Cn—k is better than another.
Proof. Let α, γ and αj, γj be Galois insertions between Cn and Cn—k. Then
|img(γ ◦ α)| = |img(γ)| = |V|n—k = |img(γj)| = |img(γj ◦ αj)|.
Thus, while every choice of α, γ may lead to different abstractions, each abstraction is equal in terms of precision.	2
The next proposition states that the result of applying fold and expand consists of the same elements of V as its argument.
Proposition 2.6 Let S ⊆ V. Then for every a ∈ Cn—k there is
A(c) :=expandab...,a′b′	(c ∩ S) ⊆ S
B(c) :=expandab...,a′b′	(c \ S) ⊆ Cn \ S
and expandab...,a′b′	(c)= A(c)  B(c).
We now formally conclude that the partitioning should be chosen based on the kind of information that the analysis aims to infer.
Proposition 2.7 No partitioning P1 ... Pk of ab	leads to a better summarization

fold P1,P′ ··· fold Pk,P′ , expand Pk,P′ ··· expand P1,P′
than another.

1	k	k	1
Proof. From Prop. 2.4 it follows that such a summarization forms a Galois insertion. Thus, from Prop. 2.6 it follows that two such summarizations are incomparable if their restriction to any finite S ⊆ V is incomparable. But then from Prop. 2.5 it follows that all summarizations are equal or incomparable.	2

From this proposition it follows that there is no best abstraction: For every choice of partitioning there are program states where one summarization delivers a better approximation than another. As in the example, where the choice of summarizing all indices en bloc is motivated by the invariant that has to be verified, the appropriate summarization method must be chosen depending on the application.

Applying Summarization in Abstract Domains
Up to now, we have only considered operations on sets of states Cn that are, in general, not computer representable. Indeed, in the abstract interpretation frame- work [1] the partial order ⟨Cn, ⊆⟩ can be seen as a concrete domain over which the semantics of the program is expressed. This section shows how summarization can be applied to abstract domains that approximate a set of concrete states c ∈ Cn with a machine representable abstract state. After some preliminary definitions, we discuss the fold and expand operations in the context of abstract domains.

Preliminaries
For the sake of the presentation, we use the following, slightly restrictive definition of an abstract domain.
Definition 3.1 An abstract domain of dimension n is a complete lattice ⟨An, ±n
, Hn, Hn⟩ with join Hn and meet Hn together with monotone functions αn : Cn → An and γn : An → Cn such that for each c ∈ Cn there is c ⊆ γn(αn(c)) and for each a ∈ An there is αn(γn(a)) = a.
We will omit the dimension of each operation when it is clear from the context. With this definition, every pair α and γ is a Galois insertion, for which the following laws are known to hold:
Proposition 3.2 For all x, y ∈ C and p, q ∈ A there is
α(x) H α(y)= α(x ∪ y)	(1)
γ(p) ∩ γ(q)= γ(p H q)	(2)

The next section lifts fold and expand to their abstract counterparts. The alge- braic laws above are then used to argue optimality in Sect. 3.3.

Domain Operations
We have defined operations fold , expand in terms of three operations drop, add and swap that work on sets of concrete values. In order to define the corresponding ab- stract operations expand # and fold # on numeric abstract domains, we observe that their constituent abstract operations drop#, add # and swap# are already present in common abstract domains:

A new dimension has to be inserted whenever a, say, local variable or a new memory cell is introduced. Hence, a numeric domain must provide an operation add # that introduces a new, unbounded variable.
When a variable goes out of scope, the information for that variable becomes obso- lete: The dimension associated with this variable should be removed for tractabil- ity via an operation drop, thereby obtaining a more compact representation of the program state.
All numeric domains provide operations for assigning numeric values, such that swapping two variables x and y can be expressed by assignments to and from an auxiliary variable [6].
Based on these functions, the definition of the abstract counterparts to fold and
expand are simply their liftings to the abstract domain:


fold #	′ ′ (a)= drop#
(a H swap#	′ ′  (a))

ab...,a b ...
a′b′...
ab...,a b ...

expand #	′ ′ (a)= add #
(a) H swap#	′ ′ (add #
(a))

ab...,a b ...
a′b′...
ab...,a b ...
a′b′...

The merit of defining expand # and fold # in terms the three simple operations drop#, add # and swap# is that interesting theoretic results can be derived more easily. For instance, the following section derives the optimality of fold # and expand # for a range of existing abstract domains.

Completeness of expand # and fold #
One of the attractive properties of the element-wise summarization of Gopan et al. [3] is the simplicity of implementing the fold # operation. When summarizing several dimensions, one may wonder if another abstract fold operation exists that
gives a better approximation of fold ab...,a′b′... than fold #	′ ′ . In this section, we
ab...,a b ...
show that this is not the case, that is, we show that fold # and expand # are optimal whenever the constituent functions drop#, add # and swap# are optimal. To this end, we introduce two notions of completeness and a notion of optimality [5]:
Definition 3.3 Let f : Cn → Cn be a concrete domain operation and f # : An →
An its corresponding abstract domain operation. Then
f # is forward complete iff f ◦ γ = γ ◦ f #.
f # is backwards complete iff α ◦ f = f # ◦ α.
f # is a best approximation of f iff f # = α ◦ f ◦ γ
Proposition 3.4 If the abstract domain provides a backwards complete operation

#
a′b′...
, a forward complete operation add #
and a forward and backwards com-

plete operation swap#
′ ′ , then fold #
′ ′	and expand #
′ ′	are the best ap-

ab...,a b ...
ab...,a b ...
ab...,a b ...

proximations of fold ab...,a′b′... and expandab...,a′b′... .
Proof. Completeness follows from Prop. 3.2.  From that it follows directly that
fold #	′ ′	and expand #	′ ′	are the best approximations of fold ab...,a′b′... and
ab...,a b ...	ab...,a b ...

expandab...,a′b′... .	2
The following diagram shows the completeness properties for functions fold and
expand :


foldab...,a′b′...
Cn	n—1
α	α

Cn,—1,
γ

expandab...,a′b′...
,n ,
γ

J  fold #	′ ′	J 
expand #	′ ′

A 	ab...,a b ...	/A¸n—1
An—1
	ab...,a b ...	/A¸

The next proposition states that fold #	′ ′	and expand #	′ ′	form a Galois
ab...,a b ...	ab...,a b ...
insertion. Namely, function fold #	′ ′	is surjective, so that the whole expressivity

ab...,a b ...
of the summarized abstract numeric domain is exploited.
Proposition 3.5 Given the preconditions of Prop. 3.4, function fold #

′ ′	to-

gether with function expand #	′ ′	forms a Galois insertion.
ab...,a b ...

ab...,a b ...
Proof. From Prop. 3.4 it follows that fold #	′ ′	= α ◦ fold ab...,a′b′... ◦ γ, which
ab...,a b ...
is a composition of monotone functions. Therefore fold #	′ ′	is also monotone.
ab...,a b ...
Similarly, expand #	′ ′	is monotone. Also, for all c ∈ Cn and a ∈ Cn—k the

following holds:
ab...,a b ...



fold #
′ ′ (expand #
′ ′ (a)) = α(foldab...,a′b′...(expandab...,a′b′...(γ(a))))

ab...,a b ...
ab...,a b ...
= α(γ(a)) = a

expand #
′ ′ (fold #
′ ′ (c)) = α(expandab...,a′b′...(γ(α(foldab...,a′b′...(γ(c))))))

ab...,a b ...
ab...,a b ...
⊇ α(expandab...,a′b′...(foldab...,a′b′...(γ(c)))) ⊇ c

Thus, fold #	′ ′	and expand #	′ ′	form a Galois insertion.	2
ab...,a b ...	ab...,a b ...
Prop. 3.4 and Prop. 3.5 show that, given appropriate functions drop#, add # and swap#, it is not only easy to implement summarization for an abstract numeric domain, but this implementation is also the best approximation of the concrete operations for that domain.

Conclusion and Future Work
In this work we have observed that summarizing relational information is non-trivial. Using generic fold and expand operations we proposed a family of abstractions that feature the same precision but retain different relational information. It remains for future work to identify the right family member for a given application. No matter which member is chosen, the summarization is optimally precise given com- plete concrete and abstract lattices. An interesting open question is whether this optimality result can be generalized to incomplete abstract lattices. This question is particularly relevant as common relational numeric abstract domains such as convex polyhedra [2] do not form complete lattices.

References
P. Cousot and R. Cousot. Static Determination of Dynamic Properties of Programs. In B. Robinet, editor, International Symposium on Programming, pages 106–130, Paris, France, April 1976.
P. Cousot and N. Halbwachs. Automatic Discovery of Linear Constraints among Variables of a Program. In Principles of Programming Languages, pages 84–97, Tucson, January 1978. ACM.
Denis Gopan, Frank DiMaio, Nurit Dor, Thomas Reps, and Mooly Sagiv. Numeric domains with summarized dimensions. In Kurt Jensen and Andreas Podelski, editors, Tools and Algorithms for the Construction and Analysis of Systems, volume 2988 of Lecture Notes in Computer Science, pages 512–
529. Springer Berlin / Heidelberg, 2004.
W. H. Harrison. Compiler Analysis of the Value Ranges for Variables. Transactions on Software Engineering, 3(3):243–250, May 1977.
David Schmidt. Comparing completeness properties of static analyses and their logics. In Naoki Kobayashi, editor, Programming Languages and Systems, volume 4279 of Lecture Notes in Computer Science, pages 183–199. Springer Berlin / Heidelberg, 2006.
A. Simon and A. King. Exploiting Sparsity in Polyhedral Analysis. In C. Hankin and I. Siveroni, editors,
Static Analysis Symposium, volume 3672 of LNCS, pages 336–351, London, September 2005. Springer.
