Egyptian Informatics Journal 19 (2018) 191–198








Full length article
A contemporary feature selection and classification framework for imbalanced biomedical datasets
Thulasi Bikku a,⇑, Sambasiva Rao Nandam b, Ananda Rao Akepogu c
a Department of CSE, Vignan’s Nirula Institute of Technology and Science for Women, Palakaluru, A.P, India
b Principal, RITW, Hyderabad, Telangana, India
c Director of Academics & Planning, JNTUCEA, Ananthapuramu, India



a r t i c l e  i n f o 

Article history:
Received 21 March 2017
Revised 16 October 2017
Accepted 22 March 2018
Available online 29 March 2018

Keywords: Biomedical data Document clustering
Document classification Bioinformatics
User recommended system
a b s t r a c t 

Due to the availability of a large number of biomedical documents in the PubMed and Medline reposito- ries, it is difficult to analyze, predict and interpret the document’s information using the traditional doc- ument clustering and classification models. Traditional document clustering and classification models were failed to analyze the document sets based on the user’s keyword and MESH terms. Due to the large number of feature sets, conventional models, such as SVM, Neural Networks, Multi-nominal naïve bayes have been used as feature classification, where additional text filtering measures are typically used as feature selection process. Also, as the size of the document’s increases, it becomes difficult to find the out- liers using the document’s features and MESH terms. Biomedical document clustering and classification is one of the essential machine learning models for the knowledge extraction process of the real-time user recommended systems. In this paper, we developed a novel biomedical document feature clustering and classification model as a user recommended system for large document sets using the Hadoop frame- work. In this model, a novel gene feature clustering with ensemble document classification was imple- mented on biomedical repositories (PubMed and Medline) using the MapReduce framework. Experimental results show that the proposed model has a high computational cluster quality rate and true positive classification rate compared to traditional document clustering and classification models.
© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

Clustering can be defined as the phenomenon of managing data objects into group of classes, which are disjoint in nature. Objects in a particular cluster share the same properties, whereas, object in different clusters share dissimilar properties. Document clustering are responsible for auto-grouping of documents into groups. Clus- tering is a type of unsupervised classification. Unsupervised classi- fication is the process of classification which does not interact with previously defined classes and training sets. Bioinformatics, pat-

* Corresponding author.
E-mail address: thulasi.jntua@gmail.com (T. Bikku).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.


tern recognition, image processing and data mining are some most common applications of clustering [1]. Data mining has an impor- tant tool for data analysis which is named as Cluster analyst.
The whole process of clustering can be split into two categories, these are: Hierarchical algorithm and Partition algorithm. Hierar- chical algorithm decomposes the dataset into clusters gives rise to a child clusters in a hierarchical pattern, which is responsible for the generation of clusters named as a dendrogram. Each cluster gives rise to a child cluster. Partition algorithm is responsible for decomposition of datasets into smaller units in a step. Hierarchical algorithm can be further divided into two, those are: Agglomera- tive and Divisive clustering. Agglomerative clustering involves generation of singleton cluster and later it combines more clusters recursively [2]. Divisive clusters initiate with a single cluster and recursively divided into more clusters. This loop aborted when ter- mination condition is satisfied.
There are two major demerits of clustering process, those are: 1. Calculating optimal cluster numbers. 2. Calculating relative good- ness among two different clusters. Criterion and validation func- tions are essential to achieve all the advantages of clustering. In


https://doi.org/10.1016/j.eij.2018.03.003
1110-8665/© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



the traditional clustering schemes, instances with similar proper- ties are grouped under same cluster, whereas; instances having dissimilar properties are grouped under different category of clus- ters. These types of clustering schemes are known as hard cluster- ing schemes. In case of soft clustering schemes, instances may be grouped in different clusters simultaneously.
Different clustering schemes are:- hierarchical schemes, vector quantization schemes, mixture density-based schemes, graph theory-based schemes, combinational search techniques-based schemes, fuzzy schemes, kernel-based schemes, and so on.
The most frequent applications of document classifications are noticed in the field of biomedicine and research domain. The pro- cess of classification is broadly categorized into two parts, those are: training and testing phase. Classification algorithm is respon- sible to create a classification model with the help of the training set. Later the model performance is evaluated in the testing phase. Many research works have been proposed since years in order to develop a classification algorithm with optimized performance. Some of the popular classification models [3] are briefly described below.
The Naive Bayes model is more efficient than that of the logistic regression scheme, nearest neighbor, decision tree and neural net- work, according to Receiver Operating Characteristic (ROC) curve, which is more significantly implemented in the research domain. The model is a simple, less parameterized and efficient one in terms of performance.
KNN is a special type of instance-based classification model, in which approximation function is computed locally and it continues until classification occurs. It is also termed as lazy learning because there is no need of training phase like other conventional approaches. These training data are tested in the testing phase. For large datasets, training data are split into smaller partial data- sets. The most common application of KNN is implemented in den- sity estimation process and parametric estimation process. Hidden Markov Model (HMM) is a type of statistical classification model, in which the whole model is taken as a Markov process and the Markov process is having unobserved or hidden states. This model is represented as a dynamic Bayesian model. Some applications of this model are: Face recognition and Detection process. The only flaw of this approach is it ignores state structures in super states. Support Vector Machine is a construction-based classification model. It also follows the idea of statistical learning [4]. This algo- rithm achieves enhanced performance as compared to other exist- ing approaches. With the help of hyperplanes, SVM defines decision boundaries. It also distinguishes data points of different classes, which can solve both linear and nonlinear classification problems. A mapping function is invoked for mapping of the orig- inal data points from the input space to a high-dimensional or infinite-dimensional feature space through a kernel function. Rele- vance Vector Machine (RVM) is an enhanced version of SVM. This model provides better performance rate when compared with other models including SVM. It emphasizes on sparsity and com- pressed sensing. This approach uses subsets of the training data
of SVM which are fewer than that of support vectors.
In decision trees classification models, divide-and-conquer is followed to create a tree, in which instances are combined with other attributes. Decision tree contains nodes and a leaf node with the output of this algorithm is either true or false. Both the path and nodes are considered to form pre-conditions so constraints are formed by the path from root to leaf. Tree pruning helps to dis- card unwanted preconditions and redundancy. One of the best among classification algorithms is Random forest algorithm classi- fies the huge amount of data with high accuracy. In decision trees the models are generated by creating a large number of decision trees. The result contains modal class which is predicted by indi-
vidual trees. It uses a concept of merging weak learners to form strong learners [5].
Genetic Algorithms (GA) are categorized as evolutionary and stochastic algorithm which gives an optimal solution. Crossover and mutation operations are performed to encode candidate solu- tions. To create offspring, solutions are selected according to the fitness function. The initial population is randomly constructed; every candidate solution is evaluated to gain a fitness score at each generation. In hierarchical clusters, decomposition occurs and smaller datasets are formed with respect to the different hierarchi- cal pattern. Clusters form child clusters or leaf clusters are con- structed which are known as Dendograms. The process of document classification plays a vital role in recent days. For dec- ades the vast amount of research has been carried out to propose a new integrated approach combing document clustering and clas- sification to achieve optimized performance rate.
Hadoop is a reliable, distributed and scalable open source soft- ware framework that is suitable for scalable and parallel program- ming on a large amount of data. This framework is designed and implemented to automatically divide large data into small seg- ments, each of which can be executed on any cluster node and to handle node failures efficiently. The Hadoop framework contains two essential components: Hadoop Distributed File system and MapReduce. HDFS can store large data and partition these data into smaller blocks of 64 MB each. MapReduce mechanism consists of two sub-phases: Map and Reduce. The input partitioned chunks are given to mapper phase for parallel processing. The output of the mapper phase is given to reduce phase to generate the output. Since a single individual machine has restricted an amount of pro- cessing power and enhancing its processing power will be more expensive. To improve the performance of the single machine pro- cessor, an efficient Map-Reduce framework is used to develop large-scale applications, which are independent of hardware and distributed environment.
The main contribution of the proposed model is included, the gene based features clustering and classification, which is respon- sible for construction of predictive classification models. In this model, feature based relational gene clusters are used to find the probability of a document belonging to a specific gene related clus- ters. If a document has a low similarity index in one cluster, then it is grouped in another cluster which is having a high similarity index. There is an effective index which helps in extraction of related documents from previous index. The proposed method reduces the estimated cost of clustering and classification.

Related works

Rojcek [1] developed a new technique to achieve uncontrolled fuzzy clustering and fast fuzzy classification model on limited dataset. They implemented the concept of KMART neural network for document classification, which is an innovative approach used to integrate clustering with fuzzy classification. Clustering as well as classification algorithms share common initial weights. This uncontrolled system is based on two basic methods, those are: 1. involves plasticity, 2. involves stability. This approach can be implemented on classification of labelled ones with limited feature set. These methods are formed without affecting the pre-defined structure (stability) of the training set. This model was tested on small datasets and the algorithm shows better performance than that of existing conventional approaches for document clustering and classifications with limited instances and dimensions. The main limitations in this model include high misclassification rate and less true positive rate on the high dimensional datasets.
Aïtelhadj et al. [2] proposed a new technique in order to cluster XML documents. The main objective is to share common structures



to multiple Hadoop structures. The proposed model was executed in two steps: In the first step, XML documents are classified using the features automatically and these structured documents are pre-processed as a model for classification. XML documents with common structure are more frequently cluster the structure of the same query. They evaluated on the real and synthetic data and implemented on both XML and semi-structured documents. As the size of the documents and clusters are increasing, mean cluster error rate and the classification rate decreasing. Additional large feature set documents are added in order to increase con- tents, so that search engine processing will be improved for query processing.
Chan and Chong [3] gave emphasize on non-text based classifi- cation scheme and identified the most common issues in the clas- sification model. They classified hypertext-based non-textual information for the purpose of classifying unknown documents. In the traditional approaches of classification, many search engines processes only text-based documents and unable to retrieve the graphic or diagrammatic documents. In this paper, a new algo- rithm for non-textual diagrammatic document classification was implemented on document sets to analyze the similarity index of the documents. Various feature vectors for classification model are presented and tested. This method analyzes and compared with the other methods (like HAC) and showed that the proposed approach is far better than that of others in terms of performance. The main issue in the non-textual based classification scheme is, this model failed to process large number of document sets for clustering and classification.
Curtis et al. [4] proposed a novel unsupervised model to handle huge volume of documents, also it is quite hard for the algorithms to retrieve and classify massive documents. This model is based on supervised learning, but the idea of unsupervised algorithm is more feasible and effective one for large scale data. A large number of clustering schemes have been developed in the last few years using unsupervised learning. In this model, a novel two- threshold method for hierarchical decomposition of features was implemented on un-structured datasets with missing values and class labels. It discards all the clusters that do not belong to the same cluster. The main problems in this model include the updat- ing of static threshold and parameter initialization in each iteration.
Dai et al. [5] analyzed various real-world applications and iden- tified limitations of uncertainty and imbalanced nature. Heteroge- neous data extraction and classification from different sources is very costly and time taking process. The conventional machine learning approaches do not perform effectively due to imbalance and uncertainty. In this work, they categorized the labelled data in a separate domain and termed it as in-domain data, whereas, the unlabeled data are categorized under out-of-domain data. Their main objective was to extract the knowledge from in- domain and implement it on out-of-domain. They proposed a Co- Clustering-based Classification scheme (CoCC) to improve the clus- tering and the classification rate in between in-domain and out-of- domain. They performed empirical analysis and evaluated their work and analyzed that this model can’t handle massive docu- ments with dynamic parameter pruning.
Diaz-Valenzuela et al. [6] identified the major issue of semi- supervised approaches and proposed a novel auto-supervised model. In order to overcome the limitations of semi-supervised approaches, they proposed a new method with auto-generation of data structure. Partition based clustering is used to detect the relationships features, among various instances. In the document clustering model, this approach is validated and evaluated in two stages. The first stage of this algorithm contains a k-means algo- rithm which helps to cluster must-link or cannot-link constraints, and the subsequent stage uses these constraints with input data in
semi-supervised clustering. The major problem identified in this work includes dynamic clustering measures need to be used to enhance the fuzzy constraints and to improve the performance rate.
Hachenberg and Gottron [7] developed a new approach to achieve structural classification and clustering by an innovative template based feature extraction technique. This technique is used to find the locality-based hashing and compression. Compres- sion schema depends on locality-based hashing and document’s tag; this approach is tested on 13,000 documents with limited dimensions. In this model, runtime complexity does not depend on the size of the training set. Traditionally, Medical document classification algorithms based on the Medical Subject Heading (MeSH) are used to classify document indexing and making a filter based search engine for biomedical repositories. MeSH, a ‘‘con- trolled vocabulary corpus ‘‘, was developed by the National Library of Medicine (NLM) to organize document key terms in a hierarchi- cal MeSH tree structure that cover the various domains such as medicine, dentistry, nursing, pre-clinical terms and health-care system [8]. Each MeSH tree structure facilitates document term searching from the root level to the leaf level. As the size of the MeSH term descriptors increases, corresponding hierarchical tree structures are also growing exponentially and it is difficult to dis- cover important patterns using traditional document classification models.
Ke [9] proposed a new technique for automated text classifica- tion with least document feature representation. They imple- mented a Least Information Theory (LIT) for document feature extraction. Shannon entropy is enhanced for non-linear relation between information and uncertainty. LIT is a type of information-based method to weight probabilistic distribution. There are two weight measures used for classification, those are: LI Binary and LI Frequency, which are evaluated independently in the early stage. The researcher experimented and evaluated this method with three benchmark collections, which provides signifi- cant performance (LIB ⁄ LIF) as compared to other methods (TF ⁄ IDF).
Lin and Chen [10] developed a new genre classification scheme for musical documents with different melodic patterns consisting of keywords, statistical and low-level features. The proposed clas- sification depends on correlation analysis of musical patterns grouped by appropriate clustering is implemented. Performance is increased remarkably by smoothing methods. They considered five different types of musical patterns (such as jazz, lyric, rock, classical) are transformed into symbolic patterns and achieved 70.67% of accuracy. The patterns are transformed into symbolic patterns. In biomedical document classification, K-nearest neigh- bor technique is a special type of machine learning model based on the document feature extraction and key phrase extraction; it does not build a training model until the new instances are classi- fied. The basic idea of the KNN model is to classify a new document based on the similarity measure between the document feature vector and the training document set. After training the model, it gets K documents as majority voting based on the highest similar- ity measure.
Nam and Quoc [11] proposed a hybrid approach by merging cluster-centric filter feature selection with frequency-centric filter feature as a Frequency Cluster Feature Selection (FCFS). Due to this integration, FCFS chooses MeSH terms, so that it is frequently appears inside each category. They chose datasets from two distin- guished domains, which are: news and medicine databases. The main limitations of this work include an enhancement of the intra-category feature of FCFS, is gained by optimization of close- ness of documents.
Shruti and Shalini [12] proposed a new algorithm known as Fuzzy Relational Eigenvector Centrality-based Clustering Algorithm



(FRECCA) for identifying interrelated sentence clusters, which uses fuzzy clustering for this method. It compares the chosen sentences and calculates a similarity index. Further works may be done in this algorithm by extending it to gain better performance than that of FRECCA. Genetic algorithm takes input data sets according to random classes. After processing the resulting solution must contain strongly interlinked clusters.
Hadoop Distributed File System is managed by a master process known as NameNode, it decides which chunk is assigned to which server and all information is maintained sequentially. In slave pro- cess, DataNodes interacts with the operating system by logically dividing data into fix sized regions. All these regions contain HFiles which are capable of storing datasets located in columns sequen- tially. When a particular region exceeds the minimum size limit, it is divided into two parts. The operations like integration and decomposition are carried out through background processes and never affect the normal behavior. These approaches permit HBase to build a random-access database over a file system which sup- ports sequential reading of data. ElasticSearch (ES) can be defined as a software framework which depends upon Lucene and provides distributed as well as real-time search on large datasets. An index is a special kind of data structure which stores the HDFS data in an efficient format. The Lucene indexing model provides a better solu- tion for building efficient and scalable indexes.
Motivation of the proposed model:

Traditional models failed to classify the high dimensional docu- ments with different initialization parameters.
Traditional bio-medical document classification models are evaluated on the limited datasets with limited memory con- straints and runtime factors.
Gene similarity based biomedical document classification mod- els were tested on static training gene-sets without symbolic gene-names.
As the size of the uncertainty and noisy documents increases, the accuracy and true positivity of the traditional models decreases due to memory constraints and lack of optimal fea- ture selection measures.

Proposed model
and classification, which is responsible for construction of predic- tive classification models. In this model, feature based relational gene clusters are used to find the probability of a document belonging to a specific gene related clusters. If a document has a low similarity index in one cluster, then it is grouped in another cluster which is having a high similarity index. There is an effective index which helps in extraction of related documents from previ- ous index. The proposed method reduces the estimated cost of clustering and classification.
In this proposed model, medical documents are collected from each data source as structured or unstructured format. In this sys- tem, a novel gene based document’s relationships are analyzed using Map-Reduce framework to discover the textual patterns from a large collection of medical document sets.
Gene feature selection measures

Gene-disease feature selection is performed using the proposed feature selection methods as:
Gene mutual information measure
Gene based document features are extracted using the gene mutual information measure. This measure is implemented in the Mapper phase. As the size of the biomedical documents increases, this measure finds the filtered candidate sets with high- est probabilistic measure. This measure takes clustered documents as input and finds the topmost features from inter and intra clus- tered document sets.
Gene mutual information can be computed using the following formula.
GeneMI GMI  max Prob GDW WVD  log	Prob(GDWi/Clusters[m])
Prob(Clusters[m])
(1)
Prob(GDWi/Clusters[]) is the probability of the GDWi weighted genes that exist in the given documents clusters.
Gene Chi-square measure
Gene based chi-square measure computes the relationship between the biomedical document clusters. This measure also evaluates the dependency between the gene term to the MeSH term of the clustered documents.
Prob(GDWi /Clusters[m]) max Prob(GDWi/WVDj)}.max{Prob(GDWi /WVDj)} 

The proposed model concentrated on finding gene features identification with the document clustering and classification
GCHI =
Q Prob(GDWi)× Q Prob(Clusters[m])


(2)

shown in Fig. 1. This model finds quality gene-disease information from the unstructured data. By merging rich document representa- tions, it resolves textual classification issues in machine learning. Gene based vector representation helps as a measure of semantics. The proposed model optimizes the gene based features clustering
where Prob(GDWi/Clusters[m]), the probability of the gene is
related feature vector that appear in the mth cluster.

Proposed MapReduce algorithm




Input: Medline Source MS, Pubmed Source PS.

MD{D1
2	3
MD  MD
, ... Dp
}represents medline training documents

PD{D1 , D2 , D3 , .. . Dq }represents PubMed trianing documents,
PD  PD  PD	PD
u denotes threshold
VD : Vector Document set
Output: Gene based Entity Identification and Clustering
// Mapper Phase
Mapper (MD{D1 , D2  , D3 , .. . Dp }, PD{D1 , D2 , D3 , .. . Dq }, k, Q)
MD  MD  MD	MD	PD  PD  PD	PD
Let M[] = Mapper(nodes(), Di)

D = (D1 , D2 , D3
, ... Dp , D1 , D2 , D3 , .. . Dq )

MD  MD  MD
MD  PD
PD  PD	PD

For each document in D do
VecDoc(Di, VDi) = Doc2Vec(Di);//Document to vector representation
Done


For each Vector v in VDi do
Represent each document in the form of <VecDoc(Di),tf, itf, W>
//Replace missing values if(v == /)
then
For each term t in VecDoc(Di)
do
simv[] = Prob(t/VD)* Prob(t)
done
VecDoc(v) = max{simv[]};
end if
VecWgt = log(  VecDoc(Di)/(tf)) ⁄ itf; //vector weight, term frequency and inverse term frequency
//Weighted Vector Document
WVDi(1.. .VecDoc[i].length) ≤ VecDoc(Di),tf,itf,VecWgt > Done
// Computing the similarity between the Gene documents. Let Gene Database and Gene Synonyms are represented as G[] = GeneDB Corpus
Syn[] = SynDB Corpus
for each weighted vector term WVDi(t) in WVDi do for each gene keyword j in G[] do if(WVDi(t)==G[j])then
//compute gene weight as
Genew= (prob(tfi ∩ G[j])/prob(tfi)) * WVDi(VecWgt)
Selected Gene with weights
GW < WVDi,t,Gene weight>=< WVDi,t,Genew> else
continue; end if end for
for each weighted vector term WVDi(t) in WVDi do
// Document wise total genes weight TGW=	Genew/N;
if(TGW > u) // Total genes weight must satisfy the threshold
Then
// Extract synonyms of the high weighted gene for each synonyms s in Syn[] do
SynList < WVDi(t),S[]>=<WVDi(t),Syn[WVDi(t)]> SGenew (prob(si ∩ G[j])/prob(si)) * WVDi(VecWgt) Done
NewGenew = TGW +	SGenew/N
GDW < WVDi, TotalGenesWeight(TGW) >=< WVDi, NewGenew >
else
Continue; end for
Clusters[] = AgglomerativeClustering(WVDi, GDW, G[], Syn[])
Reducer < WVDi, GDW, G[], Syn[], Clusters >
Select top gene features using the Gene based Chi-square
Measure, Gene Based Mutual Information to the Clustered Gene data.
For each cluster instances GWDi in the mapper Mi do
Build the modified multi-nominal naïve Bayesian tree to each GWDi. for each attribute Ai in the GWDi do
Compute the attribute selection measure using the integrated feature selection measure as: Amax = Max{GeneMI, GCHI}
Let Amax be the attribute with the maximum attribute selection measure. Construct the MNB tree using the maximum attribute Amax.
Return the Mapper Mi tree to the Reducer phase Reduce < Mi, return MNB(GWDi)>.
done done




Fig. 1. Proposed model.


In the Mapper phase, each biomedical document is processed to find the sparsity and null values. Initially, training dataset D is pre- pared using the Medline and PubMed repositories. Each document in the training data is represented in vector format. Here, docu- ment gene, disease and MeSH terms are extracted using gene term entity discovery procedure. For each document in the vector docu-
is used to find inter and intra cluster gene documents for feature selection measures. Here, proposed gene mutual information and chi-square feature selection measures are used to find the gene dependencies on the disease patterns. Finally, documents with highest gene-disease document patterns are extracted as candidate sets to Reducer phase.




// Reducer Phase
Reducer < Mapper-M[i], Patterns[]>
for each M[i] in the list do Apply HashJoin operation and display the top k patterns in the pattern list Done
In the reducer phase of the Hadoop framework, HashJoin operation is used to find the top k patterns from the gene-disease document patterns as decision patterns.



ment set, term similarity and weights are computed in vector for- mat as weighted vector document sets WVD. In the next step, the similarities between the Gene documents are computed using gene database and gene-synonyms extraction method. Documents with highest gene-disease patterns are extracted for document cluster- ing and feature selection procedure. Agglomerative cluster method
Experimental results

In this experimental study, we have applied the proposed framework on Medline and PubMed repositories [13,14] .We employ the current Apache Hadoop framework with Amazon AWS server. The configuration of the Amazon AWS Server contains



10–50 cluster nodes with 10 CPU cores and 24 GB RAM to each mapper node.
Amazon web services EC2, has three different types of storage sizes such as small, medium, large and this type include all kinds of cloud resources. These instance types are available for several zones. Nowadays, cloud computing technology has become wide- spread in many application domains. In most of the cases, the cus- tomers are unaware of their cloud usage. Small and medium scale industries are migrating towards cloud computing due to its faster access to their applications and decreased the amount of infras- tructure cost. The process of cloud computing can be considered as a business model in which the computation services are sold and rented. The major concern of cloud computing technology is to provide different cloud services according to user’s need with a nominal amount of charges. Amazon Elastic Compute Cloud (Amazon EC2) provides variable-sized computation capacity in the cloud. EC2 provides simple and easy web-scale computation for developers.
Amazon EC2 supports all major operating systems, including RedHat Linux, Windows server, SuSE Linux, Ubuntu, Fedora,
Debian, Cent OS, Gentoo Linux, Oracle Linux, and FreeBSD. Amazon plans to include several additional operating systems to EC2 instances in future. For successful operation of this work, a 64-bit Ubuntu server 12.04 is implemented. Generally, AMI uses t1.large, as it supports both 32-bit as well as 64-bit operating system.
In Table 1, the Hadoop cluster nodes, document size and its sta- tistical computations are illustrated. From the table, an Avg gene specifies the average number of genes identified in the biomedical document sets, ranked gene documents specifies the ranking of the gene to disease patterns and its relational synonyms. From Table 1, it is observed that the proposed model has high average ranked discovery patterns and relational genes on the large training datasets.
Table 2, illustrates the Hadoop cluster nodes, documents size and its performance analysis. From Table 2, an Average genes spec- ifies the average number of genes identified in the biomedical doc- ument sets. AvgTime specifies the average time taken by the Hadoop cluster nodes in Hadoop framework. From Table 2, it is observed that proposed model has high average gene discovery patterns and less average time on the large training datasets.




Table 1
Relational gene documents and its statistics with different hadoop cluster nodes.




Table 2
Average gene documents and its time computation with different hadoop cluster nodes.




Table 3
Average ranked gene documents and its quality cluster rate with different Hadoop cluster nodes.



Fig. 2. Average ranked gene documents and its quality cluster rate with different Hadoop cluster nodes.


Table 4
Document clustering and classification rate of the proposed model to the traditional models.




Fig. 3. Performance analysis of proposed model to existing models.


Table 3, illustrates the Hadoop cluster nodes, documents size and its performance analysis. From Table 3, a ranked gene docu- ment specifies the ranking of the gene to disease patterns and its relational synonyms in the biomedical document sets. Cluster quality rate specifies the cluster rate in terms of correctness of the gene terms in the document in the hadoop framework. From Table 3, it is observed that proposed model has high cluster rate on the large training datasets.
From Fig. 2, a ranked gene document specifies the ranking of the gene to disease patterns and its relational synonyms in the biomedical document sets. Cluster quality rate specifies the cluster rate in terms of correctness of the gene terms in the document in the hadoop framework.
From Table 4 and Fig. 3, proposed model achieved (~97.5%) accuracy than the traditional ensemble model on different node configurations and threshold with Accuracy on X-axis and different models on Y-axis. As the size of the gene pattern threshold increases from 0.5 to 0.9 values, proposed model has high compu- tational accuracy as compared to traditional models.

Conclusion

Biomedical document clustering and classification is one of the essential machine learning models for the knowledge extraction process of the real-time user recommended systems. As the amount of information in the biomedical repositories increases, many organizations are facing the unprecedented issues of how to process the available volumes of data efficiently. In this paper, a novel feature selection based document clustering and classifica- tion model was implemented with the multiple integrated biomedical databases such as Medline and PubMed repositories. This model is used as a user recommended system on larger docu- ment sets using the Hadoop framework. Experimental results show that the proposed model has a high computational cluster quality rate (~96%) and true positive classification rate (~98%) compared to traditional document clustering and classification models. The
computational complexity of the Mapper phase is O(nlogn) and Reducer phase is O(logn). In future, this work can be extended to protein clustering and classification using the Hadoop framework.

References

Rojcek M. System for fuzzy document clusterng and fast fuzzy classification. In: 15th IEEE international symposium on computational intelligence and informatics; 2014. p.39–42.
Aïtelhadj A, Boughanem M, Mezghiche M, Souam F. Using structural similarity for clustering XML documents; 2011. p. 109–139.
Chan SW, W Chong M. Unsupervised clustering for nontextual web document classification. Decis Supp Syst 2004:377–96.
Curtis D, Kubushyn V, Yfantis EA, Rogers M. A hierarchical feature decomposition clustering algorithm for unsupervised classification of document image types. In: Sixth international conference on machine learning and applications; 2007. p.423–8.
Dai W, Xue G, Yang Qi, Yu Y. Co-clustering based classification for out-of- domain documents. In: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM; 2007. p. 210–9.
Diaz-Valenzuela I, Loia V, Martin-Bautista MJ, Senatore S, Vila MA. Automatic constraints generation for semisupervised clustering: experiences with documents classification. Soft Comput. 2016;20(6):2329–39.
Hachenberg C, Gottron T. Locality sensitive hashing for scalable structural classification and clustering of web documents. In: Proceedings of the 22nd ACM international conference on information & knowledge management. ACM; 2013. p. 359–63.
Jiang S, Lewris J, Voltmer M, Wang H. Integrating rich document representations for text classification. In: IEEE systems and information engineering design conference (SIEDS ’16)”; 2016. p. 303–8.
Ke W. Least information document representation for automated text classification. In: Proceedings of the American Society for Information Science and Technology 49.1; 2012. p. 1–10.
Lin B, Chen T. Genre classification for musical documents based on extracted melodic patterns and clustering. In: Conference on technologies and applications of artificial intelligence; 2012. p. 39–43.
Nam LN, Quoc HB. A combined approach for filter feature selection in document classification. In: IEEE 27th international conference on tools with artificial intelligence; 2015. p. 317–24.
Shruti S, Shalini L. Sentence clustering in text document using fuzzy clustering algorithm. In: International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT); 2014. p. 1473–6.
https://www.ncbi.nlm.nih.gov/pubmed/.
https://www.nlm.nih.gov/bsd/pmresources.html.
