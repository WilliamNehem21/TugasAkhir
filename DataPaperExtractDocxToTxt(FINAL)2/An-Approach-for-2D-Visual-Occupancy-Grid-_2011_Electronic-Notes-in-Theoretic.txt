Available online at www.sciencedirect.com



Electronic Notes in Theoretical Computer Science 281 (2011) 175–191
www.elsevier.com/locate/entcs

An Approach for 2D Visual Occupancy Grid Map Using Monocular Vision
Andr´e M. Santana, Kelson R. T. Aires, Rodrigo M. S. Veras1
Department of Informatics e Statistics - DIE Federal University of Piau´ı - UFPI
Teresina-PI, Brazil
Adelardo A. D. Medeiros2
Department of Computer Engineering and Automation - DCA Federal University of Rio Grande do Norte - UFRN
Natal-RN, Brazil

Abstract
This paper presents an approach that uses planar information (homography matrix) to build a visual 2D occupancy grid map from monocular vision. Initially, a segmentation step is necessary to classify parts of the image in floor or non floor. From this classification is possible to determine which parts of the image are free and which parts of the image are obstacles. Practical results are presented to validate the proposal.
Keywords: Visual Occupancy Grid Maps, Monocular Vision, Image Segmentation.


Introduction
Investigation on automatic map construction using robots has a long history. An early representative of the mapping approach is [15,16], who presented an important algorithm for environment modeling using occupation grids. With this form of representation, the continuous spaces of the environment are discretized in such a way that the environment is from such moment represented under the configuration of a multi-dimensional (2D or 3D) grid or matrix. Each matrix’s element, called cell, represents a place from the environment that may be, according to the probabilistic formulation, occupied, empty or even not yet explored. Elfes’ approach boosted many other works in the 90’s such as [5,9,10].

1 Email: andremacedo@ufpi.edu.br
2 Email: adelardo@dca.ufrn.br

1571-0661 © 2011 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2011.11.033

Fig. 1 shows how the representation of part of a environment in occupancy grid would be with data obtained from a sonar beam. There, the black cells represent objects detected by the sonar, white cells represent free regions or ones not occupied by objects and, finally, gray cells represent regions not yet mapped. It is highlighted that the probabilistic spatial model based on occupation grid is presented as a map that may be used directly for navigation tasks, such as path planning with obstacle deviation and position estimation.

Fig. 1. Representation of a sonar beam on a 2D occupancy grid


A problem faced by Elfes [16] was the existence of uncertainties on the measure- ments returned by the sonar. Although methods for filtering unwanted information were adopted, the information from the sound beam supplied only and indirect da- tum about the environment occupied region (for example, the sonar only supplied the distance between the robot and the obstacle, but it does not inform about the position of such obstacle). Because a sonar beam only informed if a work environ- ment volume was empty or occupied, Elfes [16] modeled the beam using functions of probability distribution. Such functions described the trust that many points inside the cone (beam’s mathematical model) represented regions free of obstacles, and the uncertainty about the localization of a point (real obstacle) on its surface. The information returned by the probability distribution functions on the real space volumes are projected onto a matrix, with cells describing a pre-defined area of the environment [35]. These probabilities were combined with other sound beams probabilities for the same real space region. Therefore, the map is summarized as a
matrix whose cells stored values between [−1, 1], corresponding to the probability
of the region being empty (negative values) or occupied (positive values). Cells
with null value represented the lack of knowledge about not investigated regions’ occupation.
The reason of this probabilistic techniques’ popularity comes from the fact that the environment mapping process suffers from a strong influence from the senso- rial information’s existing uncertainties and, with the probabilistic techniques, it is possible to treat this problem modeling explicitly the various noise sources and its effects on measurements [35]. Thus, the probabilistic algorithms have been pre- sented as a good alternative to treat the uncertainties and noise problems, producing more satisfactory results for the mapping.

Probabilistic Formulation
The default statistic formula representing the mapping with occupancy grid is given by the mathematical expression shown in Eq. (1) [16,35]. It is opportune to remem- ber that the algorithm supposes the robot’s pose knowledge (position and orienta- tion).
(1)	P (M|Z0:t)
On this equation, M represents the acquired map and Z0:t is the sensorial mea-
surements set until the t moment. It is important to make clear, that the algorithm supposes that the Cartesian coordinates and robot’s orientation are known.
The environment continuous space on which the robot is found is discretized on cells, which, together, represent approximately the mapped environment. This discretization may correspond to a 3D environment longitudinal cut on the sensors’ plane, in case of a 2D grid, or a 3D discretization of the environment, in case of a 3D grid. This depends on the characteristics and/or the used sensor model.
Considering the environment discretization on cells, it may be defined a map with a finite cell set mx,y, where each one has an aggregated value that corresponds to the probability of it being occupied. The cells may have values between the interval of 0 to 1. The value 0 signifies that the cell is empty and 1 that the cell is occupied.
From a map being a finite cell set, the mapping problem may be decomposed on a set of problems for estimating each cell’s value. The Eq. (2) represents an instance of the cell mx,y estimation and is translated as the probability of cell mx,y being occupied, when the sensorial measurements z0:t until the t moment are given.
(2)	P (mx,y|z0:t)
For reasons of numerical instability with probabilities near 0 or 1, it is common to calculate log-odds (logarithmical probability) of P (mx,y|z0:t) instead of P (mx,y|z0:t). The log-odds is defined by:

(3)
t x,y
= log  P (mx,y|z0:t)  1 − P (mx,y|z0:t)

The value of the cell occupancy probability may be recovered through Eq. (4).
1
(4)	P (mx,y|z0:t)=1 − elt
The probability value may be estimated recursively on any t moment through the Bayes rule applied to P (mx,y|z1:t) (Eq. 5).

(5)
P (m

x,y
|z0:t
)= P (zt|z0:t−1, mx,y )P (mx,y|z0:t−1)
P (zt|z0:t−1)

P (zt|z0:t−1, mx,y) represents the probabilistic model of the reach sensor,P (mx,y|z0:t−1) is the mx,y cell occupation value on the t − 1 moment and P (zt|z0:t−1) is real value measured by the sensor.
Supposing that the mapping is made on statistic environments, ergo, it is valid the assertive that the present measurement of the sensor is independent of the past

measurements, given a m map, on any moment, this implies in Eqs. (6) and (7).
(6)	P (zt|z0:t−1, m)= P (zt|m)
(7)	P (zt|z0:t−1)= P (zt)
Since the map is decomposed into cells, this assumption may be extended as it is shown on Eq. (8).
(8)	P (zt|z0:t−1, mx,y)= P (zt|mx,y)
Based on the made assumptions, Eq. (5) may be simplified into Eq. (9).

(9)
P (m

x,y
|z0:t
)= P (zt|mx,y )P (mx,y|z0:t−1)
P (zt)

The Total Probability rule applied to Eq. (9) implies into Eq. (10). On it is com- puted the probability if cell mx,y being occupied, having as base the P (zt|mx,y) sensor probabilistic model and the available occupation value of the cell a priori P (mx,y|z0:t−1).

(10)
P (mx,y
|z1:t
)=	P (zt|mx,y )P (mx,y|z0:t−1)
mx,y P (zt|mx,y )P (mx,y|z0:t−1)

Related Works
In an older work from Oriolo et al. [29], the authors show that it is possible to formulate and solve perception and planning problems dealing with its uncertainties using the Set Theory (fuzzy logic). The map built by this technique is defined as a fuzzy set, where each point is associated to a real number that quantifies the possibility of it being part of an obstacle. The main advantage presented is the possibility of using many types of fuzzy operators to model uncertainties and aggregate information from multiple sources.
Konolige [24] proposes a method that talks about a mathematical refining of the mapping method presented by Elfes [15], named MURIEL (MUltiple Rep- resentation Independent Evidence Log), aiming to treat the intrinsic problems of sonar, with multiple reflections and reading redundancy. Borenstein and Koren [6] created a method based on histograms, which main objective is diminishing the computational cost intrinsic to the representation based on occupancy grid.
Another variation can be verified at the work of Yguel et al. [37]. This last work approaches data storage and representation problems for large maps and, to do that, proposes a representation form on occupancy grids based on wavelets: Wavelet Occupancy Grids.
About the environment representation it is important to think that, beyond researching data for mapping, it is possible to, for example, associate auxiliary information such as soil deformation, humidity and temperature. This concept of joinning auxiliary information is hard to incorporate inside the traditional SLAM. Estrada et al. [17] elaborated a method called DenseSLAM to allow this kind of incorporation. Because a robot moves through the environment, auxiliary data are stored in an appropriate data structure, such as an occupancy grid, and the

region represented by each network of cells is determined by a set of the map’s local marks. The result is a capacity to maintain constantly the information spatial locality through the grid.
Other investigation strands are related to the hybrid maps confection. This representation focus on the possibility of supplying subsidies for the robot’s highest level tasks, for example, path planning. Ahn et al. [1] build a map with features of 3D points and lines for internal environments. Yet Wurm et al. [36] represent a grid and features map, where the mapped characteristic is an exploration loop’s center. Blanco et al. [3] present a metric and topologic representation for a big SLAM experiment.
Using a force formulation that decreases with time, Andrade and Sanfeliu [2] show a solution for dynamic environments mapping using an occupancy grid. At each reobservation of a mark the force associated to it is reinforced and a Kalman filter is used to filter low force marks. Canas and Matellan [11] make a comparison between various mapping techniques for dynamic environments using grid.
Still about dynamic environments grids, Mitsou and Tzafestas [28] propose an algorithm capable of mapping environments where the objects’ position may change with time. The authors classify objects as: static, low dynamic objects (chairs and doors, for example) and high dynamic objects (human beings, for example). The algorithm identifies correctly the objects’ dynamic’s models in the environment and inserts this information in a grid map to register the temporal dimension and use these information on the robot’s tasks.
Yguel et al. [37] proposes the use of GPUs to fuse information and build a grid map, vacating the processor so that it may be used on other tasks addressed to the robot. On the image processing strand, the work of Merhy et al. [27] presents occupancy grid refining strategies based on probabilistic segmentation. The authors consider that the probability associated to each cell from the grid map may be codified as texture levels in an image.
Sabo [30] uses a single camera for building an occupancy grid. Notable points are identified in the image through color gradient difference techniques and are marked on the grid with an uncertainty region associated to odometer. Braillon et al. [8] detect obstacles fusing stereo vision information and optical flow to represent an occupancy grid and in a latter work Braillon et al. [7] proposes a strategy for not calculating it at each step due to the high computational cost. Blanco and Gonzalez [4] present a matching technique for maps built with an occupancy grid based on images stored during the process. This work’s differential is that maps may be compared even with they were discretized differently.
Proposing the use of a camera as visual sonar, Choi and Oh [12] makea domestic environment map using as single sensorial source a camera. The objective was building a grid map, and not a characteristics one, based only on visual information and comparing the method with maps produced by sonar. The visual sonar was modeled as a set of virtual beams on the image and the detection of limit points (final points) were found using Mahalanobis color diagonal. The authors comment that this approach’s problem is the detection of spurious points and suggest filtering

techniques to eliminate it.
Other works propose the building of volumetric maps through occupancy grids [18,25]. On the first, a 3D grid, using multiple cameras, is used to make a tracking of the people in an outdoor environment. The second one builds a grid map, using stereo vision, with the terrain height variance information. This infor- mation is useful because of the need to calculate the crossing probability for cell from the map considering that the mapped environment is defective.
On a more recent work, Hata and Wolf [20] propose the use of a 2D grid built through 3D points clouds aiming to identifying navigable regions in a certain terrain. Through this grid’s information the robot may infer navigation levels: navigable, partially navigable and not navigable.
According to the presented works, it is verified on the literature that many of them involving mapping uses cameras as the main sensor. Hereafter works related to visual mapping are presented more specifically.
Mapping is an investigation area very active on Robotics and on Artificial In- telligence for at least two decades and, besides the significant progress in this area, there are still great challenges. One of them is related to the existing solutions’ cost and the other to the found maps’ quality. Based on this, solution using cameras as main sensor are very interesting.
Although they present many advantages, the solutions based on optical sensors present problems such as: lens distortion, restrict vision field and low resolutions. Bigger detailing about these difficulties, as well as solutions for them can be found at Taylor et al. [34].
Horswill [21] comments that indoor environments have two proprieties that should be explored: a) Plane terrain: internal environments rarely have irregular floors and are normally constituted by one or more very defined planes connected through a two dimensional map set; b) Textures: internal environments usually have uniform textures are usually coated by tiles or carpet. The texture information may be used to classify areas as occupied or unoccupied.
Examples of visual mapping works based on textures are those from Howard and Kitchen [22] and Dollner and Hinrichs [14]. The use of geometric features for building visual maps is also found in the literature. In Smith et al. [33] horizontal and vertical lines are extracted as marks. The vertical ones are used to extract the doors and the horizontal ones to estimate the roof and the floor. Jeong and Lee [23] comment that the use of line is interesting, because they are less sensitive to noise when compared to points. This happens because they are constituted by a set of points, that is, the noise on a point usually does not affect substantially the line’s position and orientation. The lines also supply information about the environment’s geometry allowing for more sophisticated inference about the world’s structure. Besides that, they are easily observed from different positions.
Some visual mapping works used images segmentation to detect free areas. Mar- tin [26] found that the floor in an indoor environment presents uniform color and that edges are the best indicators for the environment’s walls.
Taylor et al. [34] use image segmentation to incorporate free spaces in a global

map. This is done through two steps: first, an edge extractor is used (Canny) to identify frontiers and then, using the segmented image information, the free spaces in the map are filled. Besides this visual information, a sonar is used to give more trust to data extracted from the image.
Sim and Dudek [32] analyzed the problem of building automatically an envi- ronment’s visual map with special attention to selecting an exploration policy that balances precision and efficiency. They present advantages and disadvantages of exploration based on straight, concentric circular, eight shaped, random, triangle and star paths.
Ah and Chung [1] propose the building of a hybrid map formed by an absolute map and a relative one. The absolute map is defined by world coordinates. This map is described as vertical planes formed by visual protrusions of characteristics localized in walls or furniture planes in an indoor environment. The relative map is defined in each planes’ coordinates. It contains information associated to each plane individually.

Proposed System
The 2D probabilistic modeling of the occupancy grid obtained through a monocular vision system presupposes the robot’s pose, pose uncertainties and homographic matrix knowledge. On this case, the obtaining of these three requirements is possible through a SLAM visual from the work of Santana and Medeiros [31].
The SLAM system [31] uses a homography matrix, calculated at the beginning of the exploration, to make mapping from image to world of the existing lines on the floor directly. A segmentation algorithm is used to classify the world in ”floor” and ”non floor”. From this classification it is possible to use the homography matrix to map as free cells the ”floor” part of the image and it is also possible to use it to map the ”non floor” parts frontiers as world’s obstacles. Fig. 2 shows this occupancy grid building proposal’s scheme.
The steps needed for the building of an occupancy grid presented in this work are: discretization of the image on cells, classification of the image cells as ”floor” (free cells) or ”non floor” (not free cells), reclassification of the not free cells as obstacle or occlusive, mapping the obstacle cells to the world using a homography matrix and, finally, expand the obstacle cells using the uncertainties of the robot’s pose.

Discretization and Classiﬁcation
On this approach, the image processing is a fundamental step. For the image discretization cells (square portions of fixed size from the image) were used and for the classification an analysis of these cells’ color components was done.
The idea of the considering the area immediately below the robot as a free area was used and so this area could be used as default from the classification. The default is mapped in the image on a pre-defined area and the Mean Shift algorithm




Homography Matrix (A):
calculated once at the beginning of the experiment



(
Visual SLAM:
robot pose and pose´s uncertainties
Image Segmentation




a	a	a
11  12  13
a	a	a
21  22  23
a	a	a
31  32  33  A


SLAM

Segmentede Image


Fig. 2. Visual occupancy grid building scheme.
is used in order to calculate the averages and variations of the color components from this region. Fig. 3 illustrates the commentary.
The density function’s gradient estimative, known as Mean Shift, was developed initially by Fukunaga and Hosteler [19]. For Comaniciu and Meer [13], the Mean Shift can be defined as the non-parametric gradient density estimator and is com- monly employed as preservation filter for discontinuity and image segmentation, through the colors and gray levels’ spatial domain.

Fig. 3. Mean Shift algorithm acting area.

Once the default region’s color variables (R, G and B’s average and variance) are calculated using the Mean Shift, this variables are used as comparison points between this region and the other parts from the image. The image captured by the camera is discretized into cells and the comparison between the default region’s color components and the each cell’s color components is used to classify them as free or not free.
In order to find the ideal discretization size for cell from the image, in example, finding the window’s size, in pixels, which will be used for making the comparison, a
study with various measurements was done. Using 5 × 5 pixels windows the average processing time was 600ms, 10 × 10 pixels windows resulted into an average time of 300ms, with 15 × 15 windows the average time was of 220ms and with 20 × 20 windows the time was of 170ms.
Based on this last value, this was the used windows default size. Fig. 4 shows the time processing graph using each one of the described measures. On it, besides noting the presented values, it is also possible to notice that the system has a stable processing time.





900


800


700


600


500


400


300


200


100


0
0	50	100	150	200	250


Fig. 4. Processing time graph of the segmentation system.

To exemplify the classification algorithm’s result used on this work, Fig. 5 shows an image containing objects with more classic shapes and Fig. 6 shows an image containing an object with a more complex shape. The region in black was classified as floor and the region in white was classified as non floor.
Image to World Mapping and Reclassiﬁcation
An inherent problem to this approach is the camera inclination effect that makes the image’s points represent a different shape in the world. Fig. 7(a) shows points distributed uniformly in the 640 × 480 pixels image and Fig. 7(b) presents these
points’ mapping into the world using a homograph matrix. The homography matrix
was calculated so that the camera’s vision field was of 1m. Observing this picture it is possible to notice that a cell in the image (four points set) can be mapped into more than a grid cell in the world.


	
	
Fig. 5. Classification: (a) Original image, (b) Image classified using the proposed algorithm.


Fig. 6. Classification: (a) Original image, (b) Image classified using the proposed algorithm.














Fig. 7. Camera inclination effect in a set of points.
Fig. 8(a) illustrates a grid in an image where the squares have 20 pixels for side and Fig. 8(b) shows that a cell’s mapping in a grid of 10cm of discretization can be done in more than a cell and that many cells can be mapped into a single cell from the grid.
Another problem of this approach is that an obstacle in the image is mapped to world projectively. Thus, every cell in the image that is considered not free cannot be mapped into the 2D grid, because there is only information about the frontier region between the obstacles and the floor. To remedy this problem an algorithm for reclassifying the not free cells as obstacle or occlusive was implemented.
The algorithm consists of calculating a vector which module is half the grid’s discretization size and in the way of the (0, 0) point and, then, testing if the vector’s extremity is contained in a free cell or in a not free cell. If the extremity is inside a




Fig. 8. Camera inclination effect in image to world mapping.
not free cell, the cell is considered occlusive and will not be mapped effectively in the grid; otherwise, the cell is considered obstacle and is mapped in the grid.
Occupation Probability and Expansion
In order to embed into the grid building process the robot’s pose’s uncertainties, the x(σ2) and y(σ2) components’ variances were used in an obstacle expansion step.
x	y
Although these measures do not represent faithfully the robot’s pose uncertainty in
the world, because the the correct representation would be the uncertainty ellipse, these measures serve to represent it approximately. The expansion is: if σx is greater that half the grid’s discretization this value is incorporated to the x components of each occupied cell in the t moment; if σy is greater than half the grid’s discretization this value is incorporated to the y components of each occupied cell in the t moment. The next step consists of calculating each cell’s occupation probability. Thus, the utilized camera is modeled as a distance sensor and this allow us to use the classical probabilistic formulation proposed by Elfes. He comments that the sensors’ modeling should be done respecting their functioning principle and verifying the restrictions that lead to the inaccuracies. Thus, a reach sensor modeling can be
given through a Gaussian function (Eq. 11):
	1		1  (r − dx,y)2 

(11)
P (r|dx,y)= √2πσ  exp  − 2	2

where r is the distance measured by the sensor, dx,y is the Euclidian distance be- tween the cell where the sensor is and mx,y cell that is being analyzed and σr is the variance that represents the inaccuracy in the distance measured by the sensor. The sensor’s variance behavior is given by Eq. (12). The values from the constants (by experimentation) with a = 0.0002 and b = 3.
(12)	σ2(r)= a · eb·r
Experimental Results
The experiments were done using a robot (see Fig. 9) whose wheels were operated by DC motors with differential actioning. Each motor has an optical encoder and

a dedicated board based on microprocessor that controls the local speed. Besides that, the robot has linked to its structure a colored webcam that captures 640 × 480 images and the system processing is done by an embedded PC.

Fig. 9. Robotic System.

Fig. 10 shows a set of experiments done with full functional system. An oc- cupation grid is built from the images. It is highlighted that the presented result considers the robot to be still and with minimum uncertainty.
Situations where the obstacles were close to each other and little more far apart (Fig. 10 (a) and (b)) were tested. Situations were a bigger obstacle did not interfere in the targeting of a smaller obstacle and the opposite situation (Fig. 10 (c) and (d)) were also tested. This last result shows and advantage of the proposed method in comparison to the traditional occupational grid building with one-dimensional laser or sonar, because with these sensor both obstacles would not be mapped simultaneously since these sensor’s reading are only in one direction.
Another experiment, this time with the robot moving, was also done to validate this proposal. The robot navigated in a typical internal environment where the obstacles are not in the same navigation plane as the robot. Fig. 11 illustrates the environment and the path commanded to the robot. During this experiment 1, 238 images were process in an average time of 360ms including the necessary processing for the robot’s localization system.
The occupancy grid built on this experience is shown in Fig. 12. The result was satisfactory since it is possible to identify in this image some real characteristics from mapped environment such as the hall between the rooms, this hall’s walls, the bed’s extremities besides the navigated environment’s complete structure.
The third experiment was done in the Departamento de F´ısica Te´ørica e Ex- perimental (Theoretical and Experimental Physics Department) - DFTE of the Universidade Federal do Rio Grande do Norte (Federal University of Rio Grande do Norte) - UFRN. This building can be seen in the online mapping system Google Earth using the coordinates 550j029”S3511j49”W . Fig. 13 shows four views from the interior of this building.
The DFTE-UFRN was chosen for this third experiment because it has a square shaped free area allowing the testing of our loop closing proposal. Fig. 14 illustrates


  



Fig. 10. Visual occupancy grid map: Experience 1.
this local’s characteristics.
This experiment’s idea was to map not only the walls but also the areas that are not navigable by the robot. It was assumed that areas with different texture than the default used for navigation are not navigable areas and, therefore, are mapped as obstacles.
On this experiment the robot executed an almost rectangular closed circuit path inside this building. During its movement 1, 962 images were processed and the occupation grid built on it is shown on Fig. 15.
Conclusions and Perspectives
This work presented an approach for probabilistic occupation grid building using only monocular vision. The implemented segmentation algorithm is used to classify the work in ”floor” and ”non floor” and from such classification it is possible to use the homography matrix to map as free cells the ”floor” part of the image and it is also possible to map the frontiers from the image ”non floor” parts as obstacles



7.0 m
7.0 m 
Fig. 11. Description of the indoor environment.


Fig. 12. Visual occupancy grid map: Experience 2.



from the world.
Three experiments with a real robot were done to validate the proposal: the first one considering the robot to be still and several configurations of obstacles organized in the environment; the second one in a typical indoor environment; and the third one in a closed loop environment.
The results were satisfactory and the system proved to be stable and working inside an acceptable computational time for robotic application. The experiments show that this approach has a good potential for utilization by service robot, as an example.
A future work is intended to: study more profoundly the sensor’s probabilistic modeling; evolve this approach to a SLAM solution and; perfect the terrain seg- mentation algorithm in order to make it more adaptive - using Neural Networks, as an example.










Fig. 13. DF-UFRN: four views.















Fig. 14. Description of the outdoor environment.



References
Fig. 15. Visual occupancy grid map: Experience 3.


Ahn, S., K. Lee, W. Chung and S. Sang-Rok Oh (2007), ’SLAM with visual plane: Extracting vertical plane by fusing stereovision and ultrasonic sensor for indoor environment’, IEEE International Conference on Robotics and Automation.
Andrade-Cetto, J. and A. Sanfeliu (2002), Concurrent map building and localization on indoor dynamic

environments, Internetional Journal on Pattern Recognition and Artificial Intelligence, Vol. 16, No. 3,
pp. 361-374
Blanco, J., J. Antonio, F. Madrigal and J. Gonzalez (2007), ’A new approach for large-scale localization and mapping: Hybrid metric-topological SLAM’, IEEE InternationalConference on Robotics and Automation (ICRA), pp. 2061-2067 .
Blanco, J. and J. Gonzalez (2007), ’A new method for robust and eficient occupancy gridmap matching’, Iberian Conference on Pattern Recognition and Image Analysis.
Borenstein, J. and Y. Koren (1991), ’The vector field histogram: fast obstacle avoidance for mobile robots’, IEEE Journal of Robotics and Automation, pp.278-288.
Borenstein, J. and Y. Koren (1997), ’The vector field histogram fast obstacle avoidance for mobile robots’, IEEE Journal of Robotics and Automation.
Braillon, C., C. Pradalier, K. Usher, J. Crowley and C. Laugier (2008), ’Occupancy grids from stereo and optical flow data’, Experimental Robotics, Springer, 367-376.
Braillon, C., K. Usher, C. Pradalier, J. Crowley and C. Laugier (2006), ’Fusion of stereo and optical flow data using occupancy grids’, IEEE Intelligent Transportation Systems Conference, pp. 1240-1245.
Buhmann, J., W. Burgard, A. Cremers, D. Fox, T. Hofmann, F. Schneider, J. Strikos and S. Thrun (1995), ’The mobile robot rhino’, AI Magazine.
Burgard, W., A. Cremers, D. Fox, G. Hofmann, J. Lakemeyer, G. Schulz, W. Steiner and S. Thrun (1997), ’Experiences with an interactive museum tour-guide robot’, Artificial Intelligence, pp.3-55.
Canas, J. and V. Matellan (2006), ’Dynamic gridmaps: comparing building techniques’, Mathware and Soft Computing.
Choi, Y. and S. Oh (2006), ’Grid-based visual SLAM in complex environment’, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) .
Comaniciu, D. and P. Meer (1999), ’Mean shift analysis and applications’, IEEE International Conference on Computer Vision (ICCV), pp.1197-1203 .
Dollner, J. and K. Hinrichs (2000), ’Dynamic 3d map and theier texture’, International Conference on Computer Graphics (ICCG) .
Elfes, A. (1987), Sonar-based real-world mapping and navigation, Journal of Robotics and Automation, pp.249-265.
Elfes, A. (1989), Occupancy Grids: A Probabilistic Framework for Robot Perception and Navigation, Tese de doutorado, Electrical and Computer Engineering, Carnegie Mellon University.
Estrada, C., J. Neira and J. D. Tards (2005), ’Hierarchical SLAM: Realtime accurate mapping of large environments’, IEEE Transactionson Robotics, pp.588-596 .
Fleuret, F., J. Berclaz, R. Lengagne and P. Fua (2008), ’Multicamera people tracking with a probabilistic occupancy map’, IEEE Transaction on Pattern Analysis and Machine Intelligence, Vol. 30, No. 2, pp. 267-282 .
Fukunaga, K. and L. Hostetler (1975), ’The estimation of the gradient of a density function, with applications in pattern recognition’, IEEE Trans. Info. Theory, Vol. 21, pp. 32-40 .
Hata, A. and D. Wolf (2010), ’Mapeamento e classificao de terrenos utilizando aprendizado supervisionado’, Congresso Brasileiro de Automtica
Horswill, I. (1993), ’Polly: a vision-based artificial agent’, AAAI Conference on Artificial Intelligence .
Howard, A.and L. Kitchen (1997), ’Fast visual mapping for mobile robot navigation’, IEEE International Conference on Intelligent Processing Systems, pp. 1251-1255
Jeong, W. and K. Lee (2006), ’Visual SLAM with line and corner features’, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.2570-2575 .
Konolige, K. (1997), ’Improved occupancy grids for map building’, Autonomous Robots Vol.4, pp.351- 367.
Marks, T., A. Howard, M. Bajracharya, G. Cottrell and L. Matthies (2008), ’Gamma- SLAM: Using stereo vision and variance grid maps for SLAM in unstructured environments’, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3717-3724 .


Martin, M. C. (2002), ’Genetic programming for robot vision’, International Conference on the Simulation of Adaptive Behavior, pp. 256-265 .
Merhy, B., P. Payeur and E. Petriu (2008), ’Application of segmented 2-d probabilistic occupancy maps for robot sensing and navigation’, IEEE Transaction on Instrumentation and Measurement, Vol. 57, No. 12, pp. 2827-2837.
Mitsou, N. and C. Tzafestas (2007), ’Temporal occupancy grid for mobile robot dynamic environment mapping’, Mediterranea Conference on Control and Automation .
Oriolo, G., G. Ulivi and M. Vendittelli (1997), ’Fuzzy maps: a new tool for mobile robot perception and planning’, Robotics Systems, Vol. 14, No. 3, pp.179-197.
Sabo, R. (2002), ’Incremental map building using an occupancy grid for an autonomous monocular robot’, lnternational Conference on Control, Automation, Robotics And Vision, pp. 613-618.
Santana, A. and A. Medeiros (2011), ’A Line-Based Approach to SLAM Using Monocular Vision’, IEEE Latin America Transaction, Vol. 9, N. 3, pp. 231-239.
Sim, R. and G. Dudek (2008), ’Effective exploration strategies for the construction of visual maps’, IEEE/RSJ Conference on Intelligent Robots and Systems (IROS), pp.69-76 .
Smith, P., I. Reid and A. Davison (2006), Real-time monocular SLAM with straight lines, British Machine Vision Conference (BMVC) .
Taylor, T., W. Boles and S. Geva (2007), ’Map building using cheap digital cameras’, BiennialConference of the Australian Pattern Recognition .
Thrun, S., W. Burgard and Fox. D. (2005), Probabilistic Robotics, MIT Press.
Wurm, K., C. Stachniss, G. Grisetti and W. Burg (2007), ’Improved simultaneous localization and mapping using a dual representation of the environment’, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
Yguel, M., O. Aycard and C. Laugier (2006), ’Efficient gpu-based construction of occupancy grids using several laser range-finders’, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
Yguel, M., O. Aycard and C. Laugier (2006), Wavelet Occupancy Grids: a Method for Compact Map Building, P. Corke Fields and S. Sukkarieh (Eds.): Field and Services Robotics, 219-230, Springer-Verlag Berlin Heidelberg
