Electronic Notes in Theoretical Computer Science 176 (2007) 37–59	
www.elsevier.com/locate/entcs

Optimisation Validation
David Aspinall1
LFCS, School of Informatics, University of Edinburgh, U.K.
Lennart Beringer2
Institut fu¨r Informatik, Ludwig-Maximilians-Universita¨t Mu¨nchen, Germany
Alberto Momigliano3
LFCS, School of Informatics, University of Edinburgh, U.K. and DSI, University of Milan, Italy

Abstract
We introduce the idea of optimisation validation, which is to formally establish that an instance of an optimising transformation indeed improves with respect to some resource measure. This is related to, but in contrast with, translation validation, which aims to establish that a particular instance of a transformation undertaken by an optimising compiler is semantics preserving. Our main setting is a program logic for a subset of Java bytecode, which is sound and complete for a resource-annotated operational semantics. The latter employs resource algebras for measuring dynamic costs such as time, space and more elaborate examples. We describe examples of optimisation validation that we have formally verified in Isabelle/HOL using the logic. We also introduce a type and effect system for measuring static costs such as code size, which is proved consistent with the operational semantics.
Keywords: Compiler Optimisation, Translation Validation, Program Logic, Java Virtual Machine Language, Cost Modelling, Resource Algebras, Lightweight Verification.


Introduction
We are interested in certifying the resource usage of mobile code for the Java plat- form. In previous work [3,1,6] we have described a proof-carrying code infrastructure which accomplishes this for memory usage. A class file is accompanied by a proof certificate which describes the resource usage of the main method of the program;

1 Email: da@inf.ed.ac.uk.
2 Email: beringer@tcs.ifi.lmu.de.
3 Email: amomigl1@inf.ed.ac.uk.

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2006.06.017

we use a program logic with judgments of the form D e : {P }, stating that expression e satisfies assertion P . For example
D emain : {r = F (h)}
where h is the starting heap of the program (in particular, containing the arguments to the main method) and r is the memory consumption of the program expressed as a function of the size of the arguments in h.
In this paper we investigate a significant extension of this framework and a particular application. First, we generalise the form of resources so that a wider range of notions is covered, in an uniform fashion. Second, we consider orderings on resources, which allow us to talk about optimisation validation, in the sense that we can establish when one program consumes fewer resources than another.
This turns out to be of interest for the compiler community, where much research has been invested in trying to select (the order of) the best compiler transforma- tion in the current context, given the available resources. However, this may be problematic:
“[. . . ] current optimisation strategies do not always achieve the performance goals. Indeed, it is well known that optimizations may degrade performances in certain circumstances. The difficulty is that current techniques cannot always determine when it is beneficial or harmful to apply an optimization.” [31]
This is where optimisation validation comes to the rescue: technically, it is inspired by the idea of translation validation [23], an alternative to the wholescale verification of translators and compilers. In this approach, one instead constructs a validation mechanism that, after every run of a compiler, formally confirms that the target code produced on that run is a correct translation of the source producing
“[. . . ] the same result while (hopefully) executing in less time or space or con- suming less power.” [24]
(our emphasis). Here, optimisation validation take the improvement in resource usage as being the primary motivation, and therefore, what should be checked. This is appropriate in scenarios such as the safety policies considered in proof-carrying code, where resource usage may even be a more important concern than correctness, because it encompasses the security requirements of the domain.
Notions of optimisation.
To consider validating optimisations, we must first define what we mean by optim- isation in our setting. We suppose that a program is given as a collection of classes, one of which includes a nominated main method. A simple notion of dynamic optimisation refers to every terminating execution of this method. Let P1 be the program before optimisation and P2 be the program after:
P1 −→ P2

We only need to consider the costs for the bodies of the main method in each program,
e1 −→ e2
Changes in other methods may be optimising, neutral or even non-optimising; at this point we do not study optimisations within nested program contexts. To be considered an optimisation, we want to establish that the transformation is improv- ing with respect to a cost model. We capture the latter with the notion of resource algebra R, which contains components for measuring the cost of executing each kind of instruction, along with an ordering on those costs. The overall (dynamic) cost may depend on the input of the program, and it is measured by execution in a operational semantics annotated with calculations using R. If for all input heaps both e1 and e2 converge, then the resource consumption of e2 should improve on that of e1:
h ▶ e1 ⇓ r1 ∧ h ▶ e2 ⇓ r2 =⇒ r2 ≤ r1
where the ordering ≤ refers to the ordering from R. We may assume, without loss of generality, that the input pointer for the argument(s) to main is fixed on every execution.

Optimisation sequences.
The above defines our notion of a single-step optimisation. For several optimisations in sequence, it is enough to consider an optimisation between the initial and final program for the resource algebra of interest R. However, we often want to decom- pose a sequence of optimisations into several transformations which are individually optimising. Then we can show the existence of a sequence of optimising steps:
P1 −→ P2 −→ · · · −→ Pn

where each Pi −→ Pi+1 is an optimisation for some particular resource algebra Ri. Additionally, each step in the optimisation should be non-increasing for the target cost model R. A proper optimisation sequence has at least one step for which costs in R strictly decrease from some Pi to Pi+1.

Validating optimisations by program logic.
To state and prove (dynamic) cost optimisations, we use a program logic that provides assertions about functions bounding the resources consumed. We must find assertions of the form:
ST 1 D e1 : {F1(h) ≤ r}	ST 2 D e2 : {r ≤ F2(h)}
where the speciﬁcation tables STi associate an assertion to each method and loop in the program, providing the appropriate invariant. The assertions state that the resources consumed when executing P1 are bounded from below by some function F1 of the input heap, and that the resources consumed by P2 are bounded from

above by a function F2. To show that P2 is an optimisation of P1 we must now prove that:
∀h. F2(h) ≤ F1(h) (in particular, this holds trivially in case F1 = F2).

Static optimisations.
Static costs such as code size are commonly used as metrics for optimisation and some dynamic costs can be usefully approximated with static measurements. We cover both possibilities by introducing a notion of static resource algebra S. To measure static costs, we use a type system with effects. For two function bodies e1 and e2 we must find a type t and effects s1 and s2 such that:

Γmain ▶Σ1 e1 : t, s1	Γmain ▶Σ2 e2 : t, s2

where the typing context for the body of main has the form args : String[] and Σ1 and Σ2 are the resource typing signatures of programs P1 and P2 respectively, see Sect. 5. For P2 to be a static optimisation of P1 we should establish that s2 ≤ s1, where the ordering ≤ now refers to the ordering on static costs. An ideal notion of optimisation would be w.r.t. a pair (R, S) of target dynamic and static cost models; a sequence of optimisations might alternate dynamic and static reductions as appropriate. A typical example is to use time and code size to validate optimisations such as loop unrolling, see Sect. 4.1. To simplify exposition here we consider the costs separately.

This paper is organized as follows. In Sect. 2 we present the dynamic semantics of our language, introduce resource algebras, and describe some typical instanti- ations. In Sect. 3, we present a program logic that generalizes the logic presented in [1] to arbitrary resource algebras. Sect. 4 gives example optimisation validations, including standard compiler optimisation steps, tail-call optimisation and an ap- plication specific one. Sect. 5 examines the static system, while Sect. 6 concludes with a summary and discussion of related work.

Resource annotated operational semantics
We use a functional form of Java bytecode called Grail [7], although the approach would work for other languages endowed with a structural operational semantics. Grail retains the object and method structure of JVML, but represents method bodies as sets of mutually tail-recursive first-order functions. The language is built from values v, arguments a, and function body expressions e (in this paper we do not

mention static fields and virtual invocation, which are accounted for elsewhere [1]):
v ::= () | lC | i | nullC
a ::= v | x
e ::= a | prim a a | new C | x.f | x.f := a | e ; e | let x = e in e
|  if e then e else e | call g | C.m(a)
Here, C ranges over Java class names, f over field names, m over method names, x over variables (method parameters and locals) and g over function names (which correspond to instruction addresses in bytecode). Values consist of integer constants i, typed locations lC, the unique element () of type unit and the nullary reference
def	def
nullC. As in JVML, the booleans bool are defined as true = 1, false = 0.
The (impure) call-by-value functional semantics of Grail coincides with an im- perative interpretation of its direct translation into JVML, provided some syntactic conditions are met. In particular, actual arguments in function calls must coincide with the formal parameters of the function definitions. Sample Grail programs are shown in Fig. 1 and 2 in their Isabelle format and in the concrete syntax of our compiler in Fig. 3.
To model consumption of computational resources, our semantics is annotated with a resource counting mechanism based on resource algebras.
Definition 2.1 A resource algebra R is a partially ordered monoid (R, 0, +, ≤),
i.e. (R, 0, +) is a monoid and (R, ≤) a partially ordered set, where
(i) 0 is the minimum element: 0 ≤ x;
(ii) + is order preserving on both sides: x ≤ y entails x + z ≤ y + z and
z + x ≤ z + y.
Moreover, R has constants in R for each expression former: Rint, Rnull, Rvar, Rprim,
Rnew, Rgetf , Rputf , Rcomp, Rlet, Rif, Rcall and a monotone operator Rmeth  : R → R.
C	C,m,v
Each constant denotes the cost associated to an instruction, which are then

composed via the monoidal operation. The operator Rmeth
calculates a cost for

method calls. For some applications, we might parameterise the constants with
additional pieces of syntax, for example if we are tracking read/writes of certain variables or charge differently selected function calls and/or primitive operations. For all the resource algebras considered here, composition is commutative; however, for examples where it is not, the order of the operation in the rules is important and matches the evaluation order.
A useful operation on such algebras is the product, which we simply under-specify as monoidal product; for R = (R, 0, +, ≤) and R' = (R', 0', +', ≤'), define R× R' as (R × R', ⟨0, 0'⟩, ⊙, ≤∗), where:
⟨r1, r' ⟩⊙ ⟨r2, r' ⟩≡ ⟨r1 + r2, r' +' r' ⟩;
1	2	1	2

≤∗ is any partial order on R × R', satisfying the conditions in Def. 2.1.
This allows us to compose various resource algebras without committing to the ordering induced by the product of posets. For instance we may want to take the ordering on R× R' to be lexicographic or simultaneous orderings, see for example the product algebra introduced on page 16. Conversely, we can define the projection πi(Rn) of a product as expected.
The operational semantics defines a judgement
E ▶ h, e ⇓ h', v,r 
which relates expressions e to environments E (maps from variables to values), initial and final heaps h, h', result values v and costs r ∈ R. Heaps are partial maps from locations l to objects, where an object is represented as a class name C together with a field table (a map from field names f to values v). We use the following notations for heaps:
h(l )	class name of object at l;
h(l ).f	field lookup of value at f in object at l; h[l.f '→ v ]	field update of f with v at l.
Argument evaluation in an environment E is defined by eval E(x) = E(x) and
eval E(v)= v, while costs are defined by
cost () = cost (lC )= 0;
cost (nullC)= Rnull;
cost (i)= Rint;
cost (x)= Rvar.

The function fields(C) returns the sequence f of fields in the class C, while initvalfi denotes the initial value of the field fi. For functions and methods, we write bodyg and body C,m to stand for the definition of g and C.m, respectively. The complete listing of the operational semantics rules follows:
a /= lC

E ▶ h, a ⇓ h, eval E(a), cost (a)

E ▶ h, a ⇓ h, va, ra	E ▶ h, a' ⇓ h, v' , r'
a  a
E ▶ h, prim a a' ⇓ h, prim(va, v' ), ra + r' + Rprim
a	a
l = freshloc(h)	fields(C)= f

E ▶ h, new C ⇓ h[l.f i '→ initvalf ], l, Rnew
i	C


E ▶ h, x ⇓ l, h, rx

E ▶ h, x.f ⇓ h, h(l).f , rx + Rgetf
E ▶ h, x ⇓ l, h, rx	E ▶ h, a ⇓ v, h, ra

E ▶ h, x.f := a ⇓ h[l.f '→ v ], (), rx + ra + Rputf

E ▶ h, e1 ⇓ h1, (), r1	E ▶ h1, e2 ⇓ h2, v, r2 E ▶ h, e1 ; e2 ⇓ h2, v, r1 + Rcomp + r2

E ▶ h, e1 ⇓ h1, v1, r1	E⟨x := v1⟩▶ h1, e2 ⇓ h2, v, r2 E ▶ h, let x = e1 in e2 ⇓ h2, v, r1 + Rlet + r2
E ▶ h, e ⇓ h', 1, re	E ▶ h', e1 ⇓ h'', v,r E ▶ h, if e then e1 else e2 ⇓ h'', v, re + Rif + r
E ▶ h, e ⇓ h', 0, re	E ▶ h', e2 ⇓ h'', v,r E ▶ h, if e then e1 else e2 ⇓ h'', v, re + Rif + r
E ▶ h, bodyg ⇓ h', v,r 

E ▶ h, call g ⇓ h', v, Rcall + r
eval E(a)= v	x := v ▶ h, bodyC ,m ⇓ h', v,r 

E ▶ h, C.m(a ) ⇓ h', v, cost (a )+ Rmeth
(r)

Resource algebra examples
Some example resource algebras are shown in Table 1. The Time algebra models an instruction counter that approximates execution time; each Grail expression form is charged according to the number of JVM instructions to which it expands 4 . The Heap algebra counts the size of heap space consumed during execution (ignoring the possibility of garbage collection, which cannot be assumed for an arbitrary JVM). Only the new instruction consumes heap. The Frames algebra counts the maximal number of frames on the stack during execution. The MethCnts algebra traces invocations by accumulating a multiset of invoked method names.

4 There are zero costs for the if instructions because they are compiled as test and branches; similarly, sequential composition has zero cost in these example algebras.

Time	Heap Frames MethCnts	MethFreqId	MethGuard
The notation |v| denotes the length of the list v1 ... vn. For method counts, ∪+ and ⊆+ are multiset union and subset respectively. For frequencies, we define FreqId,n(t, p) = (0, max (t, p)) and FreqC.m,n(t, p) = (n +2 + t, p) for C.m /= Id . Composition in this case is (t, p)+Freq(t', p') = (t + t', max (p, p')) and the ordering (t, p) ≤Freq (t', p') iff p ≤ p'. For guards, GC,m(v) is a boolean valued function for each C, m and b ≤Guard b' iff b = tt or b = b' = ff.

Table 1
Example resource algebras

The MethFreqId algebra calculates a measure of the frequency of calls to the method Id (a long identifier C.m), by accumulating the maximal period between successive calls; this is an example of an application specific algebra (see Sect. 4.3 for a motivating example).
Finally, the MethGuard algebra does not calculate a quantitative resource, but rather maintains a boolean monitor that checks whether arbitrary guards GC,m(v) are satisfied at invocations of method m in class C. If guards are considered as resource usability preconditions (for example, to check that a method parameter lies within some limits), then we may consider an optimisation to be a transformation that ensures the resource preconditions are always satisfied.

In this last case, the resource operator Rmeth
depends on the run-time values

vi, whereas in the other examples it is fixed – only the length of the argument list matters and it is specified by the definition of the method. In general, resource
algebras such as this that depend on runtime values can collect traces along the path of computation. The resulting word may be constrained by further policies, specified for example by security automata [27] or by formulae from logics over linear structures. These can be encoded in the higher-order assertion language of our program logic, introduced next.

Resource-aware program logic
Our primary basis for optimisation validation is a general-purpose program logic for Grail where assertions are boolean functions over all semantic components occurring in the operational semantics, namely the input environment E and initial heap h,

the post heap h', the result value v, and the resources consumed r. An assertion P thus belongs to the type E ×H×H×V ×R → BOOL. A judgement G D e : P in the logic relates a Grail expression e to an assertion P , dependent on a context G =
{(e1, P1),... , {en, Pn)} that stores assumptions for recursive program structures, in the spirit of Hoare’s original proof rule for procedures [17]. The program logic comprises one rule for each expression form, an axiom and a consequence rule, where we use the following syntactical conventions:
the square-bracket notation P [E, h, h', v, r] indicates the instantiation of a pre- dicate P ;
The notation GDe : {Φ(x)} for a formula Φ with some occurrence of an implicitly universally quantified variable x stands for G D e : λx. Φ(x).


(e, P ) ∈ G G Q e : P
G Q e : P  P −→ Q G Q e : Q



G Q a : {h' = h ∧ v = eval E (a) ∧ r = cost (a)}

G Q prim a1 a2 : {h' = h ∧ v = prim(eval E(a1), evalE (a2)) ∧
r = cost (a1)+ cost (a2)+ Rprim}

G Q new C : {v = freshloc(h) ∧ h' = h[v.fi '→ initvalfi ] ∧ r = Rnew}

G Q x.f : {h = h' ∧ (∃l.E(x)= l ∧ v = h(l).f ) ∧ r = cost(x) + Rgetf }

G Q x.f := a : {(∃l. E(x)= l ∧ h' = h[l.f '→ eval E (a)]) ∧ v = () ∧
r = cost(x) + cost (a)+ Rputf }

G Q e1 : P1	G Q e2 : P2
G Q e1 ; e2 : {∃ h1 r1 r2. P1[E, h, h1, (), r1] ∧ P2[E, h1, h', v, r2] ∧
r = r1 + Rcomp + r2}

G Q e1 : P1	G Q e2 : P2
G Q let x = e1 in e2 : {∃ h1 v1, r1 r2. P1[E, h, h1, v1, r1] ∧
P2[E[x := v1], h1, h', v, r2] ∧
r = r1 + Rlet + r2}

G Q e1 : P1	G Q e2 : P2	G Q e3 : P3
G Q if e1 then e2 else e3 : {∃ h1 v1 r1 r2. P1[E, h, h1, v1, r1] ∧
(v1 = 1 =⇒ P2[E, h1, h', v, r2]) ∧
(v1 = 0 =⇒ P3[E, h1, h', v, r2]) ∧ r = r1 + Rif + r2}

G ∪ {(call g,P )} Q body g : P [E, h, h', v, Rcall + r]
G Q call g : P [E, h, h', v, r]


G ∪ {(C .m(a ),P )} Q body C ,m : P [x := eval E(a), h, h', v, Rmeth
(r)]



G Q C .m(a): P [E, h, h', v, r]

Before demonstrating how the program logic is used to verify the resource con- sumption of programs, we summarise some basic meta-theoretical properties. These have been formally proven by representing the operational semantics and the pro- gram logic in the proof assistant Isabelle/HOL; for more details, see [1,2]; the results here are a generalisation with resource algebras of those presented there. First, se- mantic validity, which has a partial correctness interpretation:
Definition 3.1 An assertion P is valid for expression e, written |= e : P , if for all E, h, h', v and r E ▶ h, e ⇓ h', v,r implies that the assertion P [E, h, h', v, r] holds. A context G is valid, |= G, if for all pairs (e, P ) in G, it holds that |= e : P . Assertion P is valid for e in context G, if |= G implies |= e : P .
Indeed, the proof system is sound with respect to the operational semantics:
Theorem 3.2 (Soundness) If G D e : P then G |= e : P.
The proof of Theorem 3.2 proceeds by induction on the height of derivations, employing suitably relativised notions of (context) validity.
Given the adopted partial correctness interpretation, it is clear that non-terminating programs satisfy their specifications vacuously. To verify resource consumption of such programs, an auxiliary termination logic have developed [2].
The treatment of logical completeness, as well as the actual proving methodology, benefits from some admissible rules concerning the proof context. Beyond the usual weakening rule(s), other rules allow one to discharge the proof context, i.e. to derive judgements in the absence of contextual assumptions. This uses a speciﬁcation table ST , which maps function and method calls into assertions. We says that a context G respects the specification table ST , notation ST |= G, if all entries in it consist of a function or method call together with its assertion in the table; moreover their bodies satisfy a corresponding assertion. See [2] for the formal definition.


ST |= G	(e, P ) ∈ G


D e : P
(spectable)




ST |= G	(C .m(a ), ST (C , m, a )) ∈ G



D C .m(b): ST (C , m, b)
(adapt)


The spectable rule accounts for the verification of (possibly mutually recursive) program fragments using the specification table, while adapt may be used to adjust the actual arguments when extracting method specifications.
To prove relative completeness, we define a context Gstrong that associates to each function and method call its strongest specification.

Definition 3.3 The strongest specification for e is
SSpec(e) ≡ {E ▶ h, e ⇓ h', v, r}
Lemma 3.4 For any e, Gstrong D e : SSpec(e).
Furthermore, Gstrong satisfies ST strong |= Gstrong , where ST strong is the specific- ation table defined by (λg. SSpec(call g ), λCma. SSpec(C .m(a ))). From this, we obtain:
Theorem 3.5 (Completeness) For any e and P, |= e : P implies D e : P.
The completeness result means that we can conceivably derive any provable asser- tion using the rules of the program logic, following the structure of the program.

Validated optimisations
The program logic presented in the previous section can be used to justify program transformations that are routinely applied in optimising compilers [20], provided they are in fact improving. In this section we give some example optimisations and sketch the proofs of their validation. While the transformations and the examples we consider in this paper are fairly simple, they serve the purpose of demonstrating our methodology.
Standard low-level optimisations
We first consider the motivating program of [24],
i <- 0; x <- 1; y <- 2;
WHILE i < 24 DO {i <- i + x + y ; g <- 2 * i} EXIT
Our formal verification refers to a translation of this code into (the Isabelle repres- entation of) our language. The result of this (manual) translation is the method R.calc0:
method static int R.calc0() =
let i =0 in let x =1 in let y =2 in let g =0 in call f
fun f(int i, int x, int y, int g) = if i < 24 then call h else var g
fun h(int i, int x, int y, int g) =
let j = i + x in let i = j + y in let g =2 ∗ i in call f

which differs from the original code only in minor ways: we extended the loop prelude by an assignment to variable g, converted the loop into two functions which represent basic blocks, and turned the EXIT statement into a return statement of the final value of g.

Using the resource algebra Time defined previously, we now outline our Isabelle proof of the judgement

DR.calc0([ ]) : {r = 213}	(1)
which states that an invocation of R.calc0 requires 213 units of time. We first define two auxiliary (semantic) functions

costf (n)= 24 ∗ n + 10
costh (n)= 24 ∗ n +1 
that describe the costs of evaluating functions f and h, respectively, where n is the number of loop iterations. Next, we define a specification table ST 0 for calc0 and its local functions f and h.
⎡ R.calc0 '→ {r = 11 + costf (8)}	⎤
call f '→ {∀ J. (E(x)= 1 ∧ E(y)=2 ∧ E(i)=3 ∗ J ∧ J ≤ 8) −→
r = costf (8 − J )}	⎥
⎢	call h '→ {∀ J. (E(x)= 1 ∧ E(y)=2 ∧ E(i)=3 ∗ J ∧ J ≤ 7) −→ ⎥
⎣	r = costh (8 − J )}	⎦
The first line defines the specification of R.calc0 in terms of the auxiliary function costf , while the entries for call f and call h ensure that the auxiliary functions correctly model the costs of the executing the local functions. In both cases, the specifications depend on the value of the variable i; intuitively, the universally quantified variable J represents the number of loop iterations that have already been performed.
Next, we define a context, G0 that associates the specification table entries to the relevant function and method calls, e.g.
G0 = {(R.calc0([ ]), ST 0 R.calc0), (call f, ST 0 call f ), (call h, ST 0 call h)}
The core of the verification consists in establishing ST 0 |= G0. From that, it is just a matter of calling the spectable rule to conclude the proof of (1). The former, in turn, requires us to prove that each entry in G0 is justified: for each entry (call f, P ) – and similarly for method entries – we need to show that the body bodyf satis- fies G0 D bodyf : P [E, h, h', v, Rcall + r]. Using the rules of our program logic, these proofs proceed syntax-directed similarly to the way a Verification Condition Gener- ator would work, leaving side conditions involving numeric constraints. Ideally, we would delegate the solution of those verification conditions to a fully automated (ex- ternal) solvers. Currently, instead, the proof assistant often needs directions when facing large case-splits and quantifier instantiations beyond decision procedures.
In the same fashion, we have established Isabelle proofs of specifications DR.calci([ ]) :
{r = ri} for methods R.calc1 ... R.calc7 which arise from applying the code trans-

formations described in [24] to R.calc0. The resulting code is shown in Fig. 1, while Table 2 summarises the costs ri obtained for each transformation step.

class R {
.. .
method static int calc1 () = let i =0 in let x =1 in let y =2 in let g =0 in call f
fun f(int i, int x, int y, int g) = if i < 24 then call h else var g
fun h(int i, int x, int y, int g) = let i = i +3 in let g =2 ∗ i in call f

method static int calc2 () = let i =0 in let g =0 in call f
fun f(int i, int g) = if i < 24 then call h else var g
fun h(int i, int g) = let i = i +3 in let g =2 ∗ i in call f

method static int calc3 () = let i =0 in let g =0 in call h
fun h(int i, int g) = let i = i +3 in let g =2 ∗ i in if i < 24 then call h else var g

method static int calc4 () = let g =0 in call h
fun h(int g) = let g = g +6 in if g < 48 then call h else var g

method static int calc5 () = let g =0 in call h
fun f(int g) = let g = g +6 in if g < 48 then call h else var g
fun h(int g) = let g = g +6 in if g < 48 then call f else var g

method static int calc6 () = let g =0 in call h
fun h(int g) = let g = g +6 in let g = g +6 in if g < 48 then call h else var g

method static int calc7 () = let g =0 in call h
fun h(int g) = let g = g + 12 in if g < 48 then call h else var g}


Figure 1. A sequence of low level transformations


Table 2
Costs associated to low level transformations

class REV {
method static LIST App(LIST l, int i)= call app
fun app(LIST l, int i)= if l = null then call app 0 else call app 1 fun app 0(int i)= let l = null in let x = new LIST in
x.HD:=i ; x.TL:=l ; var x
fun app 1(LIST l, int i)= let h = l.HD in let t = l.TL in
let t = REV.App(t, i) in l.TL:=t ; var l

method static LIST Rev1(LIST l)= call rev1
fun rev1(LIST l)= if l = null then null else call rev1 1 fun rev1 1(LIST l)= let h = l.HD in let t = l.TL in
let t = REV.Rev1(t) in REV.App(t, h)

method static LIST Rev2(LIST l, LIST acc)= call rev2
fun rev2(LIST l, LIST acc)= if l = null then var acc else call rev2 1 fun rev2 1(LIST l, LIST acc)= let h = l.HD in let t = l.TL in
l.TL:=acc ; REV.Rev2(t, l)

method static LIST Rev3(LIST l, LIST acc)= call rev3
fun rev3(LIST l, LIST acc)= if l = null then var acc else call rev3 1 fun rev3 1(LIST l, LIST acc)= let t = l.TL in l.TL:=acc ;
let acc = var l in let l = var t in call rev3}

Figure 2. Class REV

In general, proofs of functional correctness of arbitrary code fragments may be required to verify statements about resource consumption in this case-study. However, this is not the case for the specific transformations we considered: none of the specifications involved constrains the result values v. Furthermore, none of the transformations increases the dynamic resources consumed. Indeed, except for loop unrolling (the conversion calc4 → calc5), all transformations reduce the costs 5 .

Tail-call optimisation
Next, we consider a recursive program involving heap structures. Figure 2 defines the class REV with method App for appending an element to a list, and methods Rev1,... , Rev3 for reversing a list. We assume that objects of class LIST contain fields HD and TL of type int and LIST, respectively.
Concentrating our attention on the required height of the frame stack, we observe that method Rev1 is formulated using method recursion and employs the auxiliary method App. As all its recursive invocations are nested, Rev1 requires a frame stack of a height that depends linearly on the length of the input list. To express this dependency we define a predicate h, v |=X n that specifies when a reference value v


5 The loop unrolling performed in [24] actually increases the dynamic costs, because it jumps to a shared code block instead of duplicating the continuation code. Using our formalism of static resources we could characterise this as a static optimisation instead, namely reducing code size.

2 REV.App(a ) '→ {∀ x y n X.	3
(a = [x, y] ∧ h, E(x) |=X n) −→
6	7
(h',v |={freshloc(h)}∪X n +1 ∧ h =dom (h)\X h' ∧ r = rApp (n))}
REV.Rev1(a ) '→ {∀ x n X.
6	'	7
(a = [x] ∧ h, E(x) |=X n) −→ (∃ Y. h ,v |=Y n ∧ r = rRev1(n))}
6	7
REV.Rev2(a ) '→ {∀ x n X y m Y.
6	7
(a = [x, y] ∧ h, E(x) |=X n ∧ h, E(y) |=Y m ∧ X ∩ Y = ∅) −→
(∃Z. h',v |=Z n + m ∧ r = rRev2(n))}
6	7
REV.Rev3(a ) '→ {∀ x n X y m Y.
6	7
(a = [x, y] ∧ h, E(x) |=X n ∧ h, E(y) |=Y m ∧ X ∩ Y = ∅) −→
(∃Z.h',r |=Z n + m ∧ r = rRev4(n))}
call rev3 '→ {∀ n X m Y.
6	(h, E(l) |=X n ∧ h, E(acc) |=Y m ∧ X ∩ Y = ∅) −→	7
(∃Z.h',v |=Z n + m ∧ r = rRev4(n))}


Table 3 Specification table for class REV.

represents a non-cyclic (integer) list of length n in a heap region h ]X.
h, v |=∅ 0 ≡ v = null
h, v |=v Y (n + 1) ≡ v ∈ dom(h) ∧ h(v)= LIST ∧ h(v).TL = t ∧ h, t |=Y n
We can now prove a specification that relates the length of the list to the stack depth. The quantitative specification we will prove also indicates that the runtime, the number of jumps, and the number of method invocations all grow quadratically with the length of the input list:
D REV.Rev1([a]) : {∀ n X. h, E(a) |=X n −→ rFrames = n + 1}
Method Rev2 arises from Rev1 by introducing an accumulator that eliminates the invocation to App and formulates the recursion as tail recursion. Its specification imposes some well-structuredness conditions on both arguments: pointers must represent lists, which moreover should be non-overlapping in the heap. The frame depth depends only on the length of the first argument, which gives the same overall depth cost as Rev1 (but a considerable saving in allocated space):
D REV.Rev2([a, b]) : {∀ n X m Y. h, E(a) |=X n ∧ h, E(b) |=Y m ∧ X ∩ Y = ∅
−→ rFrames = n + 1}
In Rev3, the method-level tail recursion is converted into a method-internal loop and the redundant field is eliminated, resulting in a program whose execution only requires a single frame.
D REV.Rev3([a, b]) : {∀ n X m Y. h, E(a) |=X n ∧ h, E(b) |=Y m ∧ X ∩ Y = ∅
−→ rFrames = 1}

The verification of the three specifications follows the same strategy as before. This is an overall optimisation for the resource algebra Frames, but we verified the intermediate steps using a more informative product resource algebra, Frames × MethCntsAll × Time × Heap, where MethCntsAll is similar to the MethCnts algebra shown in Table 1, except that only the size of the multisets is considered (thus we sum calls to all methods), and we stipulate that size(LIST) = 1. We obtain the following resource tuples, which show the costs in terms of of the length n of the (first) input list.


Under the lexicographic order for the product algebra, both steps are optim- ising. To preserve datatype representation conditions across method calls, we need stronger invariants in the specifications than shown above. The full specification table is given in Table 3, which also contains an invariant of the loop represented by the function call rev3.

Optimisation of method call frequency
As well as standard optimisations, our framework can be used to validate optimisa- tions that are custom specified for a particular application.
Consider the hypothetical scenario of a
process-control application where there is

an irregularly shaped chemical tank (illus- trated opposite) whose contents must be carefully monitored to ensure sufficient re- agent. When the amount reaches a crit- ical low point, the reaction must be tem- porarily halted while the tank refills.



...
section(1) section(0)
Sensor.level()

An embedded controller runs a program that which monitors the level gauge. A suitable notion of optimisation in this setting would be to transform the program into one which checks the tank level more frequently, so reducing the latency between noticing a tank empty condition and triggering the refill cycle. Thus the frequency of invoking the Sensor.level() method is a suitable resource measure. Using the same methodology as previously, and with the resource algebra MethFreq, we can validate the transformation of a naive implementation of a program which calculates the amount of reagent into the tank into a better one which checks the level more frequently.

The full listing of the example program (naive version) is in Fig. 3. The method calc(n) calculates the amount of reagent left in n sections of the tank. It is invoked from the runloop method, which is supposed to be the process control loop; in reality, this loop would be run indefinitely and involve other tasks besides level calculation.
class ChemCalc {
field static int alarm
field static int[] section
field static int critical_amount
method static void runloop() =
let
val n = 1000
fun raise_alarm () = putstatic <int ChemCalc.alarm> 1
fun loop_check(int n) = if n>0 then loop(n) else ()
fun loop(int n) =
let
val chem_level = invokestatic <int Sensor.level()> ()
val chem_amount = invokestatic <int ChemCalc.calc(int)> (chem_level)
val n= sub n1 
val critical = getstatic <int ChemCalc.critical_amount>
in
if chem_amount < critical then raise_alarm() else loop_check(n)
end
in loop_check(n) end
method static int calc(int n) =
let
val a= 0
fun sumup(int n, int a) =
let
val cs = getstatic <int[] ChemCalc.section>
val x= get cs n val a= add xa val n= sub n1 
in
sumup_check (n,a)
end
fun sumup_check(int n, int a) =
if n>0 then sumup(n,a)
else a
in sumup_check(n,a) end}
Figure 3. Grail code for an embedded controller
Numerous optimisations are possible in this example to increase the rate of testing the sensor level. For example, we might sum up the section sizes only until we find out that the critical level has been safely exceeded. Or (supposing the dimensions of the tank are fixed during the run of the process), we may calculate the sums for the container sections in advance to avoid looping over the section array each time we test the sensor level. We have not yet undertaken the formal verification of this example, as it goes slightly beyond our formal presentation of the logic as it makes use of arrays; however, the extension is straightforward.
Static semantics
A static resource algebra S is defined exactly as in Def. 2.1, except that the resource constructors depend on the typing context only; in particular, the method operator does not depend on the values of its arguments. For a fixed signature the judgment Γ ▶ e : t, s assigns type t and effect s to Grail expression e in a straightforward way; an example is the rule for if expressions:
Γ ▶ e : bool, se	Γ ▶ e1 : t, s1	Γ ▶ e2 : t, s2
Γ ▶ if e then e1 else e2 : t, se + Sif + s1 + s2





Γ ▶ a : typeΓ(a), scost (a)
Γ ▶ e : t, s	s ≤ s'


Γ ▶ e : t, s'

Γ ▶ a1 : t1, s1	Γ ▶ a2 : t2, s2	Σ(prim)= t1 × t2 → t3
Γ ▶ prim a1 a2 : t3, s1 + s2 + Sprim
Γ ▶ x : C, s	Σ(C.f )= t
Γ ▶ new C : C, Snew	Γ ▶ x.f : t, s + Sgetf

Γ ▶ x : C, sx	Γ ▶ a : t, sa	Σ(C.f )= t Γ ▶ x.f := a : unit, sx + sa + Sputf

Γ ▶ e1 : unit, s1	Γ ▶ e2 : t2, s2
Γ ▶ e1 ; e2 : t2, s1 + Scomp + s2


Γ ▶ e1 : t1, s1	Γ,x : t1 ▶ e2 : t2, s2
Γ ▶ let x = e1 in e2 : t2, s1 + Slet + s2


Γ ▶ e : bool, se	Γ ▶ e1 : t, s1	Γ ▶ e2 : t, s2
Γ ▶ if e then e1 else e2 : t, se + Sif + s1 + s2


Σ(g)= t1 ×· · · × tn → t, s
Γ ▶ call g : t, Scall + s


Γ ▶ ai : ti, si	Σ(m)= t1 ×· · · × tn → t, s

Γ ▶ C.m(a) : t, Σisi + Smeth
(s)


Figure 4. Typing rules
Notice that this is different from usual type and effect systems, where the effect on both branches would be the same. See Fig. 4 for the full listing, where argument typing and static cost are defined as follows:

= Svar
= Sint

type Γ(nullC ) = C	scost (nullC) = Snull
Many of the standard properties of type and effect systems hold, culminating in subject reduction. All proofs are standard and hence omitted. First, it is immediate

to show that our system is conservative w.r.t. the effect-free system defined, as usual, by erasure. Further, a canonical forms property holds.
Fact 5.1 (Canonical forms) Assume that Γ ▶ v : t, s; then:
if t = int then v = i and s = Sint.
if t = unit then v = () and s = 0.
if t = C then either v = nullC and s = Snull or v = lC and s = 0.
From this we observe that every value has constant effect.
Weakening is admissible as is a specialised form of substitution, in which ar- guments play the role of variables and the effect is increased accordingly. This generalises value substitution in type and effects systems, which holds because val- ues are pure (i.e. have zero effect).
We now introduce a generalisation of the well-known relation between static effects and dynamic traces [30], which is key in the statement and proof of subject reduction.
Definition 5.2 Given two resource algebras S and R, an approximation is a rela- tion ± included in S ×R, which reads “static effect s approximates dynamic resource r”, with the following properties:
0S ± 0R and for every constructor c, it holds Sc ± Rc.

If s ± r, then Rmeth
meth
C,m,a
(r).

s1 ± r1 ∧ s2 ± r2 entails s1 +S s2 ± r1 +R r2;
s ± r entails s +S s' ± r.
For example, consider the static approximation SG = ⟨S, {tt}, ∪, ≤SG⟩ of the MethGuard algebra, where the carrier S is {{tt}, {tt, ff}} and the ordering is defined as b ≤SG b' iff b = {tt} or b = b' = {tt, ff}. The approximation relation is ∈−1, which trivially satisfies the above conditions.
Let E : Γ be the usual correspondence between environment and typing. Further, say that a heap h is well-typed if Σ(C.f ) = t implies h(lC ).f : t. Finally, we say that a signature is well-typed if for all g, m ∈ Σ it holds

Σ(g)= t1 × ··· × tn → t, s =⇒ x1 : t1 ... xn : tn ▶ body g : t, s
Σ(C.m)= t1 × ··· × tn → t, s =⇒ x1 : t1 ... xn : tn ▶ body C,m : t, s
Theorem 5.3 (Subject reduction) Assume a well-typed signature, algebras S and R as above. Suppose further that Γ ▶ e : t, s and for a well typed heap h it holds that E ▶ h, e ⇓ h', v,r and E : Γ; then Γ ▶ v : t, scost (v ) and s ± r.
The above result ensures the consistency of the operational semantics with the type system; it is a basis for approximating dynamic measurements using type checking instead of theorem proving.

Conclusions
We have presented a framework and methodology for optimisation validation, based on generic forms of dynamic and static resource costs. We have formalised most of the setting in Isabelle/HOL, particularly including the soundness and completeness of the program logic, which was applied to validate the specific optimisations in Sect. 4.
One can argue against our approach in various ways. For example, validating optimisation with disregard of behavioural equivalence seems pointless (often, the empty program is the ultimate optimisation). Yet, we see resource improvement validation as orthogonal to translation validation; in some settings one may check both things. Optimising compilers usually employ heuristics to decide what to do with code, but in some cases a sequence of transformations may not actually result in improvement even if correctness is preserved; the optimisation is then pointless (as noted in [31]). In others settings, such as our PCC application, the safety policy that we care most about is captured by our resource consumption notion and so resource usage preservation is more crucial than functional equivalence.

Related Work.
By now there is an extensive literature on verifying compiler correctness and op- timisations (e.g. [10,11,20]), but as far as we know, no previous work on formal and static methods for verifying that optimisations in fact improve resource usage. The closest are an early formal approach to performance estimation and monitoring for space and time complexity w.r.t. OO programs [26] and, at the other end of the spectrum, a framework for predicting the impact of optimizations, via models for the latter as well for code and resources [31]. This is empirically tested and used to select the right combination and application strategy of given optimizations.
Specific instances of machine checked correctness proofs have also been pursued: some recent examples concern code elimination [8], tokenization and componentiz- ation transformations [13].
One of the most well-developed approaches is David Sands’ Improvement The- ory [25], a specialisation of the standard theory and reasoning principles of observa- tional equivalence, in which basic observations include some intensional information about computational cost. This is extended to space improvements for effects-free call-by-need languages in [15]. “Paper and pencil” proofs are mostly equational and require considerable ingenuity even in the simplest cases. Our direction is in- spired by translation validation (TV) [23], mainly implemented in the automatic tvoc tool [4]. It addresses both reordering transformation such as loop fusion and structure preserving ones, such as constant folding, where statements can be inser- ted or deleted. In both cases sound rules generate verification conditions entailing a bisimulation between source and target w.r.t. observable variables. Those VC’s are then fed to a theorem prover, in particular CVC. TV subsumes Rinard’s cred- ible compilation [24], which instead requires full code instrumentation. Necula [22] demonstrates an approach to TV based on symbolic execution in the context of the

GNU C compiler intermediate language. Similarly to Rinard, he uses simulation of execution paths, but instead of compiler annotation, a constraint-based algorithm heuristically tries to infer a simulation. The system is robust enough to allow the author to verify structure preserving optimisations in gcc itself.
Several other researchers have considered program logics going beyond tradi- tional functional correctness specification. For example [28] presents a generic Hoare calculus for reasoning about computational monads and is formalised in HasCasl.
With a similar aim as us, Denney and Fischer [12] introduce a framework for safety
policies: given a semantically defined safety property (such as “no division by zero”) and an operational semantics, the aim is to derive specialised Hoare rules to enforce the property; however, for “stateful” properties, such as memory writes limits, the approach becomes technically quite involved.
Since we consider optimisation from one program to another, a natural approach suggests itself, namely using a logic which relates two programs at once. In Benton’s relational Hoare logic [5] judgements {R} c1 ∼ c2 {S} refer to the execution of two (possibly) different programs c1 and c2 while the pre- and post-conditions are re- lations (rather than predicates) over states. As a special case, the program logic contains a proof system in which (functional) equivalence of programs can be dir- ectly verified, in contrast to our approach where separate judgements are needed. A similar logic is used in Rinard’s report [24]. In both cases, proofs of soundness are included while completeness is not examined.
Elsewhere, forms of cost algebras (monads) and partial orders similar to ours have been investigated for analysis of resource consumptions, e.g., [19,14] and op- timisation [21]. General static analysis techniques having similarities with the setup of our type and effect system include [16,29]. There is also considerable work on specific static analysis for different notions of resource usage: to name one, the use of abstract interpretation for certification of bound memory usage in Java byte code [9], but a more complete survey would lead us well beyond the scope of this overview.
Future work.
There are several avenues for pursuing this work. First, by considering finer-grained transformations individually, perhaps by generalising Improvement Theory to re- source algebras. Second, it would be noteworthy if our static analysis was able to validate optimisations directly and avoid the need for the program logic: this is in fact possible in restricted (e.g. boolean) domains, but further assumptions are needed in the general case. To scale our techniques to routine application we would need either an automatic technique based on the type system or better automatic assistance for using the program logic. Endowing relational Hoare logics with a notion of resource algebra seems also a swift way to combine semantics preservation with optimisation validation.
Finally, the considerable generality of resource algebras allows examples that are less directly related to optimisation, but useful for validating other safety properties (including correspondence properties in protocols, or resource usage analysis in the

sense of [18]), and we would like to apply our general techniques to those examples too.
The sources for the core logic (and much more) are available at: http://www.tcs.ifi.lmu.de/∼hwloidl/mrg/MRG-infra-0805.tgz The examples presented in this paper can be downloaded from: http://homepages.inf.ed.ac.uk/amomigl1/papers/cocv06.tar
Acknowledgement
This work was funded in part by the Information Society Technologies programme of the European Commission, Future and Emerging Technologies under the IST- 2005-015905 MOBIUS project. This paper reflects only the authors’ views and the European Community is not liable for any use that may be made of the information contained therein.

References
Aspinall, D., L. Beringer, M. Hofmann, H.-W. Loidl and A. Momigliano, A program logic for resource
verification, in: K. Slind, A. Bunker and G. Gopalakrishnan, editors, TPHOLs2004, LNCS 3223 (2004),
pp. 34–49.
Aspinall, D., L. Beringer, M. Hofmann, H.-W. Loidl and A. Momigliano, A program logic for resources
(2005), to appear in Theoretical Computer Science.
Aspinall, D., S. Gilmore, M. Hofmann, D. Sannella and I. Stark, Mobile resource guarantees for smart devices, in: G. Barthe, L. Burdy, M. Huisman, J.-L. Lanet and T. Muntean, editors, CASSIS 2004, LNCS 3362 (2005), pp. 1–26.
Barrett, C. W., Y. Fang, B. Goldberg, Y. Hu, A. Pnueli and L. D. Zuck, TVOC: A Translation Validator for Optimizing Compilers, in: K. Etessami and S. K. Rajamani, editors, CAV, LNCS 3576 (2005), pp. 291–295.
Benton, N., Simple relational correctness proofs for static analyses and program transformations, in:
N. D. Jones and X. Leroy, editors, POPL (2004), pp. 14–25.
Beringer, L., M. Hofmann, A. Momigliano and O. Shkaravska, Automatic certification of heap consumption, in: A. V. Franz Baader, editor, LPAR 2004, LNCS 3425 (2005), pp. 347–362.
Beringer, L., K. MacKenzie and I. Stark, Grail: a functional form for imperative mobile code, in:
Foundations of Global Computing, number 85.1 in ENTCS (2003), pp. 1–21.
Blech, J. O., L. Gesellensetter and S. Glesner, Formal verification of dead code elimination in Isabelle/HOL, in: B. K. Aichernig and B. Beckert, editors, SEFM (2005), pp. 200–209.
Cachera, D., T. Jensen, D. Pichardie and G. Schneider, Certified memory usage analysis, in:
J. Fitzgerald, I. J. Hayes and A. Tarlecki, editors, FM’05, LNCS 3582 (2005), pp. 91–106.
Cousot, P. and R. Cousot, Systematic design of program transformation frameworks by abstract interpretation, in: POPL, 2002, pp. 178–190.
Dave, M. A., Compiler verification: a bibliography, SIGSOFT Softw. Eng. Notes 28 (2003), pp. 1–4.
Denney, E. and B. Fischer, Correctness of source-level safety policies, in: K. Araki, S. Gnesi and
D. Mandrioli, editors, FME 2003, LNCS 2805 (2003), pp. 894–913.
Genet, T., T. P. Jensen, V. Kodati and D. Pichardie, A Java Card CAP converter in PVS, ENTCS
82 (2003).

Grobauer, B., Cost recurrences for DML programs, in: ICFP’01 (2001), pp. 253–264.
Gustavsson, J. and D. Sands, Possibilities and limitations of call-by-need space improvement, in:
ICFP’01 (2001), pp. 265–276.
Hankin, C. and D. L. M´etayer, A type-based framework for program analysis, in: SAS, 1994, pp. 380– 394.
Hoare, C. A. R., Procedures and parameters: An axiomatic approach, in: E. Engeler, editor, Symp. Semantics of Algorithmic Languages,, Notes in Mathematics 188 (1971), pp. 102–116.
Igarashi, A. and N. Kobayashi, Resource usage analysis, ACM SIGPLAN Notices 37 (2002), pp. 331– 342.
Jay, C. B., M. Cole, M. Sekanina and P. Steckler, A monadic calculus for parallel costing of a functional
language of arrays, in: C. Lengauer, M. Griebl and S. Gorlatch, editors, Euro-Par, LNCS 1300 (1997),
pp. 650–661.
Kennedy, K. and J. R. Allen, “Optimizing compilers for modern architectures: a dependence-based approach,” Morgan Kaufmann Publishers Inc., 2002.
Knoop, J., O. Ru¨thing and B. Steffen, Optimal code motion: Theory and practice, ACM TOPLAS 16
(1994), pp. 1117–1155.
Necula, G. C., Translation validation for an optimizing compiler, in: PLDI ’00 (2000), pp. 83–94.
Pnueli, A., M. Siegel and E. Singerman, Translation validation, in: B. Steffen, editor, TACAS, LNCS
1384 (1998), pp. 151–166.
Rinard, M., Credible compilation, Technical Report MIT-LCS-TR-776, MIT Laboratory for Computer Science (1999).
Sands, D., Improvement theory and its applications, in: A. D. Gordon and A. M. Pitts, editors, Higher Order Operational Techniques in Semantics, Publications of the Newton Institute, Cambridge University Press, 1998 pp. 275–306.
Schmidt, H. W. and W. Zimmermann, A complexity calculus for object-oriented programs, Hournal of Object-Oriented Systems 1 (1994), pp. 117–147.
Schneider, F. B., Enforceable security policies, ACM Transactions on Information and System Security
3 (2000), pp. 30–50.
Schr¨oder, L. and T. Mossakowski, Monad-independent Hoare logic in HasCASL, in: M. Pezze, editor,
FASE, LNCS 2621 (2003), pp. 261–277.
Skalka, C. and S. F. Smith, History effects and verification, in: W.-N. Chin, editor, APLAS, LNCS
3302 (2004), pp. 107–128.
Wadler, P. and P. Thiemann, The marriage of effects and monads, ACM Trans. Comput. Log. 4 (2003),
pp. 1–32.
Zhao, M., B. R. Childers and M. L. Soffa, Predicting the impact of optimizations for embedded systems, in: LCTES (2003), pp. 1–11.
