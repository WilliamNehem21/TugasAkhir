Electronic Notes in Theoretical Computer Science 214 (2008) 25–49	
www.elsevier.com/locate/entcs


Completeness of ASM Refinement
Gerhard Schellhorn1
Lehrstuhl fu¨r Softwaretechnik und Programmiersprachen Universit¨at Augsburg
D-86135, Augsburg, Germany

Abstract
ASM refinements are verified using generalized forward simulations which allow to refine m abstract operations to n concrete operations with arbitrary m and n. One main difference to data refinement is that ASM refinement considers infinite runs and termination. Since backward simulation does not preserve termination in general, the standard technique of adding history information to the concrete level is not applicable. The powerset construction also adds infinite runs and is therefore not applicable too. This paper shows that a completeness proof is nevertheless possible by adding infinite prophecy information, effectively moving nondetermism to the initial state. Adding such prophecy information can be done on the semantic level, but also by a simple syntactic transfor- mation that removes the choose construct of ASMs. The completeness proof is also ported to give a completeness proof for IO automata.
Keywords: refinement, completeness, abstract state machines, IO automata


Introduction
ASM refinement has been originally introduced by B¨orger ([4], [5]) into the framework of Gurevich’s ASMs ([15], [8]). to solve problems in compiler veri- fication. A formal definition has been given in [31] which originated from the formal correctness proofs ([28], [29], [30]) in the interactive theorem prover KIV [27] for a Prolog compiler [7]. A methodology for the general use of the ASM refinement paradigm has been defined and numerous applications are described in [6]. A comparison to data refinement has been given in [32].
A specific characteristic of ASM refinement is that it requires that infinite runs of the concrete system must have corresponding infinite runs on the

1  schellhorn@informatik.uni-augsburg.de



1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.06.003

abstract level. The concrete system must always terminate if the abstract system does. For the use in compiler verification the requirement is easily motivated: assuming two ASMs are given that define an operational semantics for source code and compiled code, interpreting the compiled code should not lead to infinite runs, if interpreting the source code never does.
Other refinement definitions do not require that termination is preserved. An example for data refinement (for both the contract as well as the behavioral approach) is given in Section 5. Indeed, termination cannot be preserved in some applications, an example being refinement of a security protocols [34]. There an an attacker can cause infinite runs that contain rejected attacks only, which correspond to no abstract step. Therefore a variation of refinement correctness has been developed [33].
Refinement definitions for distributed systems, such as refinement of IO au- tomata [24] typically also do not require that termination is preserved. Often only infinite runs are considered by adding stuttering steps [2]. Nevertheless these refinement definitions face a similar problem, since they have to preserve input and output done on infinite runs, which may be infinite.
Preserving infinite runs (or the input/output done on them) creates a prob- lem for the completeness proof, since backward simulation is no longer correct in all cases. The current solutions either require finite (invisible) nondeter- minism [2] or consider preserving finite input/output only ( ≤∗T in [24]).
Backward simulation has therefore never been used in ASM refinement (variations of data refinement such as the one used in B [1] also do not use backward simulation).
Instead ASM refinement uses generalized forward simulations, which allow arbitrary commuting “m:n diagrams” which compare m steps of an abstract machine to n steps of the concrete machine. This generalizes forward simu- lations of data refinement, which allow 1:1 diagrams only. Generalizations of data refinement, e.g. weak refinement [10] and coupled refinement [14] have also suggested more general types of diagrams.
Although some examples which need backward simulation in data refine- ment can be verified using generalized forward simulation (in particular, the standard V vs. Y-shaped example), avoiding backward simulations seemed unavoidably to come at the price of incompleteness.
This paper shows that a completeness proof is possible anyway. The basic idea is to define a uniform transformation that moves all nondeterminism of the concrete system into the initial state. This is possible on the semantic level by constructing a prophecy automaton, but also on the syntactic level by using choice functions. The refinement between the resulting ASM and an abstract one then can always be verified using a forward simulation.

Before the completeness proof, a summary of the basics underlying ASM refinement is given (more details are in [31] and [32]). Transition systems which are the semantics of ASMs (but also of other operational formalisms) are defined in Section 2. To express proof obligations for syntactic ASM rules, KIV uses a higher-order variant of Dynamic Logic (or wp-calculus) described in Section 3. The definition of refinement correctness for ASMs is given in Section 4 together with an improved criterion for generalized forward simulation.
The completeness proof is given in Section 6. The proof is applicable to the original definition of ASM refinement, but also to the weaker variant of ASM refinement that preserves invariants given in [33]. Instead of giving details of this variation, Section 7 gives a completeness proof for (the stronger version
≤T of) refinement of IO automata, which is closely related to ASM refinement preserving invariants.
After submitting a first version, a reviewer noted that the work of Wim Hesselink is quite similar to the completeness proof here. In Section 8 we give a first analysis of the relation between the approaches. Finally, Section 9 concludes.
All proofs in this paper have been mechanized with KIV and are available on the KIV web page [23].

Transition Systems and ASMs
The definition of ASM refinement is based on the following simple notion of a transition system:
Definition 2.1 (Transition System)
A transition system M = (S, I, F, SEM) consists of a set S of states, subsets I, F ⊆ S of initial and final states, and a transition relation SEM ⊆ S⊥ × S⊥ which is defined on S⊥ := S ∪ { ⊥ }. The transition relation is required to be strict (⊥, ⊥) ∈ SEM, and total for non-final states S⊥ \ F ⊆ dom(SEM).
For ASMs, the transition relation SEM is given as the semantics [RULE]] of an ASM rule. SEM distinguishes between possible and guaranteed nontermi- nation, so it is an instance of erratic semantics. As an example, the semantics of skip or abort, where skip does nothing, abort always diverges, and or is nondeterministic choice is the relation {(s, s): s ∈ S⊥} ∪ {(s, ⊥): s ∈ S⊥}.
Erratic semantics ([11]) can be defined using a variety of approaches and for a number of formalisms defining operations on a state. For abstract, sequential programs [25] defines the semantics by a pair of relations (r, e) where r ⊆ S × S describes possible initial and final states, and dom(e) describes the domain of

guaranteed termination. From the two relations, erratic semantics can be defined as SEM = r ∪ {(s, ⊥): s ∈ dom(e)}. [26] gives a similar definition for commands c, the relation is s − c → s', the domain of guaranteed termination is c ↓. The variant of ASMs used in KIV is based on a similar definition, except that states include the valuation of higher-order variables which are used to represent the dynamic functions of ASMs. Another alternative is to define SEM based on predicate transformers (see e.g. [17]) and the resulting wp-calculus. The domain of guaranteed termination then consists of those states where wp(α, true) holds, and the relation (s0, s' ) ∈ SEM are those for
which s = s0 → ¬ wlp(α, s /= s' ) holds.
Blocking   semantics   of   operations   OP,   defined   by OP ∪ (S⊥ \ dom(OP)) × {⊥} as used by the behavioral approach to data refinement [3] can be viewed as an instance. Demonic semantics [13] as used by the contract approach [39] can be viewed as an instance too, using the embedding OP ∪ (S \ dom(OP)) × S⊥ ∪ {(⊥, ⊥)}. Although this is slightly different than the standard embedding which adds {⊥} × S⊥ instead of just
{⊥} × {⊥} to get the Smyth powerdomain [36], the difference is not relevant and gives the same conditions for refinement as shown in [32].
For the original ASMs defined in [15],[8] states are first-order algebras over some signature Σ: S = Alg(Σ). Given a state s, the semantics of an ASM rule is defined by first calculating a set of updates (f, a, b) from the function updates f(t) := t' of the ASM rule. For nondeterministic rules this calculation is nondeterministic, a formal definition using SOS rules is given in [8]. If the set contains two updates (f, a, b1) and (f, a, b2) it is inconsistent, and the rule application fails. In implementations this means that the rule will either stop with an error or diverge, and “divergence” and “failure” will be used synonymously in the following. In the semantics divergence results in (s, ⊥) ∈ SEM. If the updates are consistent, they can be applied on s, modifying dynamic function f at a to have the new value b. Applying all updates gives the new algebra s' with (s, s') ∈ SEM.
ASMs often use total rules which never fail. For these the ⊥ element is redundant. Deterministic ASMs are also an important special case. For this case, the logic of [37] uses the predicate ¬ def(RULE) to characterize the domain of guaranteed termination.
For transition systems execution traces σ = (σ0, σ1,.. .) are finite or infinite sequences of elements σi ∈ S⊥ that satisfy
trace(σ) ≡	(∀ i.i + 1 < #σ → σi /∈ F⊥ ∧ (σi, σi+1) ∈ SEM)
∧ (#σ < ∞ → last(σ) ∈ F⊥)
The length of a trace σ written #σ ∈ IN ∪ {∞}, last(σ) is the last element

of a finite trace. The definition implies that traces never pass through final states or ⊥. Finite traces end with ⊥ when the last rule applied fails, or with a final state.
A run of an ASM is a trace that starts with an initial state. To avoid having ⊥ in runs, which will be necessary in the completeness proof, we define predicate run(σ) to hold for a sequence σ, if σ is a run of the ASM with a possible final ⊥ element removed. This gives two disjoint classes of finite sequences for which run(σ) holds: either the last state is final and σ is a run, or the last state is non-final and extending it with ⊥ gives a run.
The definition of refinement in Section 4 will use commuting diagrams which consist of several steps of two ASMs. To characterize these semanti- cally two operators from temporal logic are needed. For s ∈ S⊥ and p⊥ ⊆ S⊥ AF(s, p⊥) (“all executions starting with s will reach a state in p⊥”) and EF(s, p⊥) (“some execution starting with s eventually reaches a state in p⊥”) are defined by
AF(s, p⊥): ↔ ∀ σ. σ0 = s ∧ trace(σ) → ∃ n. σn ∈ p⊥ EF(s, p⊥): ↔ ∃ σ. σ0 = s ∧ trace(σ) ∧ ∃ n. σn ∈ p⊥

Operators AF+(s, p⊥) and EF+(s, p⊥) assert that the number of steps must be positive.
AF+(s, p⊥): ↔ s /∈ F⊥ ∧ (∀ s0. SEM(s, s0) → AF(s0, p⊥))
EF+(s, p⊥): ↔ s /∈ F⊥ ∧ (∃ s0. SEM(s, s0) ∧ EF(s0, p⊥))


Reasoning over ASMs in KIV’s logic
To reason over transition systems in a theorem prover is possible on the se- mantic level, by specifying sets and relations using higher-order logic, and a first layer of the specifications in KIV uses such definitions. They are suitable for proving assertions about the refinement theory.
For case studies it is simpler to use a logic which allows to use the syn- tax of ASM rules directly. In KIV, higher-order wp-calculus [12] is used for this purpose, using notations from Dynamic Logic [22]. This section gives a characterization of the two main operators EF and AF in terms of this logic. The logic of KIV allows to combine rules which are given operationally as abstract programs with predicate logic formulas. Three operators are defined:
[α] ϕ means “all terminating runs of α end in a state where ϕ holds” and corresponds to wlp(α, ϕ) in wp-calculus.

⟨|α|⟩ ϕ means “all runs of α terminate and end in a state where ϕ holds” and corresponds to wp(α, ϕ).
⟨α⟩ ϕ means “there is a terminating run of α which ends in a state where
ϕ holds” and is equivalent to ¬ wlp(α, ¬ ϕ).
The logic of KIV does not talk explicitly about the ⊥ element. Instead, given an ASM rule RULE(s) that modifies state s ∈ S the fact that ⊥ is a possible or the guaranteed outcome of SEM = [[RULE ] can be defined as
maydiverge(s) := ¬ ⟨|RULE(s)|⟩ true (“the rule may fail in s”)
diverges(s) := ¬ ⟨RULE(s)⟩ true (“the rule surely fails in s”)
The characterization of AF and EF then depends on whether ⊥∈ p⊥. In the negative case, given a predicate p with [p]] = p⊥,a state s ∈ S and a predicate final with [final]] = F the operators needed are
AFn(s, p) ↔ ⟨|while ¬ p(s) ∧ ¬ final(s) do RULE(s)|⟩ p(s) EFn(s, p) ↔ ⟨while ¬ p(s) ∧ ¬ final(s) do RULE(s)⟩ p(s)


is:
In the positive case where ⊥∈ p⊥ and [p]] ∪ {⊥} = p⊥ the characterization

EFp(s, p) ↔ ⟨while ¬ p(s) ∧ ¬ final(s) ∧ ¬ maydiverge(s)
do RULE(s)⟩ (p(s) ∨ ¬ final(s) ∧ maydiverge(s))
AFp(s, p) ↔ ⟨| while ¬ p(s) ∧ ¬ final(s) ∧ ¬ diverges(s)
do choose s' with ⟨RULE(s)⟩ s = s'
in s := s'|⟩ (p(s) ∨ ¬ final(s) ∧ diverges(s))
The loop characterizing AFp is the most complex: it chooses steps of

RULE(s) that do not fail, ignoring possible failures. This is done as long as the state is not final and as long as such steps exist (i.e. as long as ¬ diverges(s) holds). Finally, either p must hold, or the next step must definitely be a di- verging step. Note that several of the formulas require to use a wp-formula as
a test in an abstract program.
Given these syntactic characterizations, verification of refinement proof obligations can benefit from using ASM rules directly, which allows to exploit the control structure of rules to automate proofs by symbolic execution (see [27]).

ASM Refinement
This section gives a short summary of the definitions of [31] and [32] that characterize correctness of ASM refinement.

							 ¸¸
¸¸ IO
IO	¸¸¸
								¸ 

							 . . . 

¸¸¸¸
IO	¸¸¸¸IO
    
  IO
... 

 	  	  
¸¸		 
	 . . . 

Fig. 1. Refinement correctness for finite and infinite runs

CM = (SC, IC, FC, SEMC).  States and traces that belong to the abstract and concrete system will be denoted as as, σA and cs, σC respectively.
The basic idea of refinement is that runs σC of the concrete runs simulate abstract runs σA the abstract system. “Simulation” is given a rather liberal definition: it is not required that there is a 1:1 correspondence between steps nor that a specific visible component must be identical (although these are common cases). Instead a relation IO ⊆ SA × SC is used 2 that determine when initial, final and intermediate states are considered to be similar. The definition is given based on the following criteria:
Definition 4.1 (Correctness of ASM Refinement)
A refinement of AM to CM preserves partial correctness with respect to IO, if for every ﬁnite run σC of CM there exists a finite run σA such that initial states are related by IO and the final states are either related by IO too or both ⊥.
A refinement of AM to CM preserves total correctness if it preserves partial correctness an if in addition for every infinite σC run there exists an infinite abstract infinite run σA that starts in an initial state related by IO.
A refinement is correct with respect to IO if it preserves total correctness and if for every infinite σC run there exists an infinite abstract run and two strictly monotone sequences 0 = i0 < i1 < ... and 0 = j0 < j1 < ... of natural numbers, such that for every k IO(σA(ik), σC(jk)) holds.
Informally, an ASM refinement is correct, if finite runs implement finite runs and inﬁnite refined runs pass through infinitely many corresponding states, see Fig. 1. States that correspond to each other via IO are often called states of interest, since these are regarded as observably equal.
To prove refinement correctness commuting diagrams are used like the ones

2 the definition of ASM refinement in [32] had separate relations IR and OR for initial and final states and used IO for intermediate states only. The relation IO used here is their disjunction. Using one relation simplifies the formulas of the completeness proof.

 	  	  	  	  	  	  	  

¸¸ ¸¸
¸¸ ¸¸
 ¸¸

¸ ¸
¸¸ ¸¸
¸¸ ¸¸
¸  ¸
     
    
¸¸¸¸IO

IO
R ¸¸ ¸¸ 
   R
¸¸ ¸¸	¸¸

¸¸ ¸¸	¸¸ ¸¸	¸¸
  	  	  	    	  	  
	   	  	   		 . . . 
¸¸ ¸¸
¸¸ ¸¸

¸¸ ¸¸
¸ ¸
    
   R
    
    
... 

R	¸¸ ¸¸ 
		 	  	 	 	 . . . 
Fig. 2. Commuting diagrams to verify ASM refinement

shown in Figure 2 and a coupling invariant R between abstract and concrete states. In data refinement this relation is usually called a simulation. To preserve traces R must be stronger than IO: if e.g. IO states that both ASMs have done the same output, R may be a conjunct of IO and other properties necessary to prove invariance.
Unlike data refinement, where diagrams have to match one rule applica- tion (1:1 diagram), ASM refinement diagrams may have any shape. “m:n diagrams” are possible, where m abstract transitions match n concrete tran- sitions. Even triangular shapes are allowed, but the case of an infinite con- secutive sequence of 0:n diagrams must be prevented using some well-founded predicate <0n on pairs of states. Similarly, infinite sequences of m:0 diagrams must also be ruled out using <m0 to preserve total correctness and refinement correctness.
The verification condition propagates the invariant forwards through the traces (so it is a forward or downward simulation). Since it does not preserve R in every step, R is called a “generalized forward simulation”. The dual notion, generalized backwards simulation can be defined (some results are in [31]) but has rather weak properties in general. In particular it does not preserve infinite runs as required by ASM refinement correctness. Section 7 discusses that the situation is similar to IO automata where backward simulation is only allowed when infinite (invisible) nondeterminism is forbidden, which is neither an option for ASM refinement nor for data refinement, since the following ASM rule as well as the equivalent Z operation are perfectly legal (even if n is not considered by finalization).


OP
choose n' with n' > n
in n := n'
OP 	

Given total rules, for which runs ending in ⊥ are absent, the proof obliga- tion for a commuting diagram with R to be a generalized forward simulation is as follows:

R(as, cs) ∧ ¬ (as ∈ FA ∧ cs ∈ FC)
→	EF+(as,λ as'.R(as', cs) ∧ (as', cs) <m0 (as, cs))
∨ AF+(cs,λ cs'.	EF+(as,λ as'.R(as', cs')))
∨ R(as, cs') ∧ (as, cs') <0n (as, cs))
Intuitively, the proof condition says: if states as and cs are related by R and not both final, then it must be possible to add a commuting diagram, such that R holds at the end. Either this diagram may consist of abstract steps only to form a triangular m:0 diagram (first disjunct, <m0 must decrease), or (second disjunct) it must finally be possible to complete a diagram for whatever concrete steps are chosen (the size of the diagram may depend on the choices). The number of abstract steps needed to complete the diagram may be positive, resulting in a m:n diagram where both m, n > 0, or it may be zero and <0n must decrease.
For rules that may fail, the condition on commuting diagrams becomes significantly more complex, since in Logic we can talk about the ⊥ element only implicitly using wp-calculus. Nevertheless it is possible to use the defi- nitions of AF and EF of the previous section to derive a syntactic condition equivalent to (VC).
R(as, cs) ∧ as = as0 ∧ cs = cs0 ∧ ¬ (final(as) ∧ final(cs))
→	¬ final(as) ∧ ⟨ARULE(as)⟩
EFn(cs,λ cs'.R(as, cs') ∧ (as, cs') <m0 (as, cs))
∨ if EFn(as,λ as'.¬ final(as') ∧ maydiverge(as')) then AFp(cs,λ cs'.¬ final(cs') ∧ diverges(cs') ∨ ϕ) else AFn(cs,λ cs'.ϕ)
where ϕ =	R(as, cs') ∧ (as, cs') <0n (as, cs)
∨ ¬ final(as) ∧ ⟨ARULE(as)⟩ EFn(as', R(as', cs'))
(VC)
The condition given here improves on the sufficient condition given in [31] in that it is provably maximal. Figure 3 gives a pictorial description, which shows the four types of diagrams that are allowed by (VC).
States and relations after a “▶” symbol, as well as dotted lines must be shown to exist, assuming the rest of the diagram is given. Given two states as and cs, both not final, lines two and three of the condition allows diagrams of type (A). Line four checks whether AM has a run that ultimately applies a failing rule. If such a run exists, failing runs are allowed for CM: line five of the condition uses AFp and allows diagrams of type (C). Otherwise all runs of AM must at some time reach a proper state (AFn) and be of one of the forms (B) or (D). These two cases are the two disjuncts of ϕ. With this verification condition for commuting diagrams correctness of generalized

as R
cs
▶ ARULE+
▶ as
R
cs
▶ ARULE+
▶ as
▶ R
+ cs '

▶ (as', cs) <0n
(as, cs)
CRULE

(A)	(B)
as	▶ ARULE ▶ ⊥	as


R	▶ R
cs	 
CRULE
R
cs	 cs '
CRULE

▶ (as, cs') <m0 (as, cs)
(C)	(D)
Fig. 3. Commuting diagrams in ASM refinement
forward simulation can be shown.
Theorem 4.2 (Generalized Forward Simulation)
A reﬁnement from AM to CM is correct if
∀ cs.init(cs) → ∃ as.init(as) ∧ R(as, cs) (“initialization”)
veriﬁcation condition (VC) holds (“correctness”)
R(as, cs) → IO(as, cs) (“invariant implies IO”)
The “initialization” condition guarantees that every initial state cs has a corresponding initial state as with R(as, cs), assuming two predicates init(as) and init(cs) are given that specify the initial states IA and IC. The proof of the theorem in [31] intuitively follows the construction of commuting diagrams as shown in Figure 2.

Data Refinement and Infinite Runs
In this section we give an example using Z [38] notation, that shows that in the presence of infinite nondeterminism data refinement allows to introduce infinite runs on the concrete level, when the abstract level has finite runs only. The	example	defines	an	abstract	data	type
ADT = (GS, AS, AINIT, AOP, AFIN) with a single operation as follows:


AS 		AOP 	




GS 	
AFIN 	
AINIT 	




The abstract data type starts with ctr = 0 and increments the counter up to a bound bound. Finalization extracts the current value of ctr. The state of the data type can therefore be described by pairs (ctr, bound), and its runs are (0, n), (1, n) ... , (k, n) for any k < n. The data type has infinite nondeterminism for choosing the initial value of bound. The concrete datatype has the same structure, except that it allows the bound to be infinity (∞, with the usual convention n < ∞):

CS 		COP 	




GS 	
CFIN 	
CINIT 	





The concrete data type has the same runs as the abstract one, plus the additional infinite run (0, ∞), (1, ∞),.. .. ASM refinement would reject this as a correct refinement, since the abstract data type has finite runs only, so

the concrete data type must not have infinite runs for a correct refinement.
Data Refinement considers finite runs only, the refinement is correct in the contract as well as in the behavioral approach. Both cases can be proved using a backward simulation, which just requires cctr = actr.
For the main commutativity condition a step of the concrete data type from cs = (m∞, n) to cs' = (m∞, n + 1) is given (with m∞ ∈ IN ∪ {∞} and n < m∞) together with an abstract state as' = (m, n + 1) related to cs' by the backward simulation. The abstract state needed to generate a commuting diagram then is as = (m, n). All other refinement conditions (of the contract as well as the behavioral approach) trivially hold.
The example we give here is not new, similar examples are used in various disguises: in [24] to show that the two refinement notions for IO automata ≤T and ≤∗T differ. In papers on semantics of programming languages, to show that predicate transformers are no longer continuous but just monotone [17], [13]. In refinement of TLA [2] to show that specifications need not have finite invisible nondeterminism.

The Completeness Proof
As was shown in the previous section, ASM refinement differs from data re- finement in considering termination of all ASM runs from a specific initial state as an important property that should be preserved by refinement. As a consequence, general backward simulation is not acceptable for ASM refine- ment.
This makes the first of two standard constructions used for proving refine- ment completeness unavailable, which adds history information to the concrete data type and then proves that there is a backward simulation between the re- sulting data type and the abstract one. Examples of this construction are the proofs in Abadi and Lamport [2] and Lynch and Vaandrager for IO automata [24].
The second standard construction to prove completeness is the power set construction (used e.g. in [9]). It is not applicable either, since this construc- tion may introduce infinite runs, when the original data type has none. To see this, consider again the abstract data type of the previous section, which has no infinite runs. The result of the power set construction would be a data type (PAS, PAINIT, PAOP, PAFIN). Its carrier set is the power set of abstract states: PAS = P(AS). PAINIT = {(n, {0}× IN) : n ∈ IN}, and PAOP ⊆ PAS × PAS is

PAOP = {(ctr, bd): ctr ≤ bd}× {(ctr + 1, bd), ctr + 1 ≤ bd}

The power data type therefore has the infinite run
{0}× IN, {0}× (IN \ {0}), {1}× (IN \ {0, 1}), {2}× (IN \ {0, 1, 2}),... 
Since both constructions are not applicable, it seemed for a long time impos- sible to find a completeness proof. The key idea to finding a solution anyway is to dualize the standard constructions of completeness proofs: both, adding the full history and the power set construction remove all nondeterminism from the past and move it into the future, enabling backward simulation. Instead moving all nondeterminism to the initial state enables a forward simulation. Moving the nondeterminism of a transition system M to the initial state is done by predicting the full run of the system. The resulting sytem Det(M) has deterministic steps only:
Definition 6.1  (Corresponding Deterministic Transition System) Given a transition system M = (S, I, F, SEM) the corresponding deterministic transition system Det(M) := (S', I', F', SEM') is defined as:
S' := S+ ∪ Sω, the set of all finite and infinite sequences of states
I' consists of all σ with run(σ).
SEM' := {(σ, tail(σ)) : #σ > 1} ∪ {((s), ⊥): s /∈ F} where tail(σ) removes the first state from σ. The second set of the union adds transitions to
⊥ iff the run ends with a failed rule application.	Note that these final failing transitions were removed in the definition of run.
F' := {(s): s ∈ F} consists of all sequences consisting of a single final state.
Theorem 6.2 (Equivalence of M and Det(M))
Det(M) is deterministic: each state has at most one successor state.
For every run σ of M Det(M) has a run (σ, tail(σ), tail(tail(σ)),.. .) of the same length. In particular the run is inﬁnite iff σ is inﬁnite.
Every run of Det(M) has the form (σ, tail(σ), tail(tail(σ)),.. .) for a run σ of
M.
M is a correct ASM reﬁnement of Det(M) using IO(s, σ) ≡ σ = (s,.. .).
The proof is almost trivial by inspecting the definitions. Note that the relation IO is a backward simulation between M and Det(M). It is a harmless one, since it preserves infinite traces. It turns out that this is the only one that is needed: given a refinement from AM to CM it is always possible to define a generalized forward simulation between AM and Det(CM).
Theorem 6.3 (Completeness of ASM Refinement) If CM is a correct ASM reﬁnement of AM with respect to IO, then a generalized forward simula-

tion exists between Det(CM) and AM.
The theorem follows directly from
Theorem 6.4 (Completeness for Deterministic Concrete Systems) Given a correct reﬁnement of AM to a deterministic transition system CM with respect to IO, then this reﬁnement can always be proved using the following generalized forward simulation:
R(as, cs)
≡∃ σC, n, σA, m.
run(σC) ∧ run(σA) ∧ IO(σA[0], σC[0])
∧ m < #σC ∧ σC(m)= cs ∧ n < #σA ∧ σA(n)= as
∧ if #σC < ∞
then	#σA < ∞
∧ if last(σA) ∈ FC
then	last(σA) ∈ FA ∧ IO(last(σA), last(σC))
∧ (m = n = 0 ∨ m + 1 = #σC ∧ n + 1 = #σA)
else last(σA) /∈ FA ∧ m = n = 0
else #σA = ∞ ∧ ∃ (ik)k∈IN, (jk)k∈IN, k.
ik = m ∧ jk = n ∧ mon(i) ∧ mon(j)
∧ ∀ k' ≥ k. IO(σA(ik' ), σC(ik' ))
The invariant basically states that as and cs are related if they are states of interest on two runs σA and σC which correspond via the original definition of refinement.
In more detail the formula requires two runs σA and σC whose initial states are related by IO. If σC is finite then σA must be finite too, and either both end in final states related by IO or both end with failed rule applications. In the first case as and cs being states of interest means that they are either both the initial (m = n = 0) or the final state (m + 1 = #σC and m + 1 = #σA), in the second case both states must be initial.
If σC is infinite then σA must be infinite too, and the pair of states must be one of the pairs of states that correspond via IO. mon(j) and mon(i) abbreviate the conditions 0 = j0 < j1 < ... and 0 = i0 < i1 < ... respectively.
The proof that this is a generalized forward simulation first has to show that for a concrete state cs an abstract state with R(as, cs) exists. Now it is easy to prove that for any state cs a trace σC starting with this state exists (in this case even a unique one). The definition of refinement then guarantees the existence of a corresponding run σA. Choosing as to be the initial state of this run together with m = n := 0 it is easy to see that R is satisfied.
Second the correctness condition has to be shown: for two states with

R(as, cs) and each trace starting with cs a commuting diagram must be at- tached, except if both states are final. Since CM is deterministic, there is only one such trace: the part of σC that starts with cs. Therefore the commuting diagram that has to be constructed is just the next commuting diagram that exists according to R: for two infinite traces the diagram ends with states σA(ik+1) and σC(ik+1). These states of course satisfy R again using the same traces σA and σC, and ik+1 jk+1 as values for m and n. For two finite traces the diagram ends with the two final states of σA and σC. If both states as and cs are not final, then both numbers of steps to the final states are positive, and the correctness condition is already satisfied. For the special case, where σA consists of of a single state which is both initial and final, while σC has n > 0 states, a triangular diagram 0:n diagram results. The well-founded relation needed to allow one such diagram makes non-final states bigger than final ones. The case of an m:0 diagram is proved with a dual argument.
The semantic construction of moving all nondeterminism to the initial state is not really convenient for proving actual refinements since it involves recording all the details of future states in the initial state. In practice this is unnecessary. It is sufficient to record the outcomes of nondeterministic choices using additional dynamic functions. This can be done by a purely synactic transformation, since ASMs explicitly specify nondeterminism with a choose construct of the form
RULE ≡ choose x with ϕ(x, y) in RULE1 ifnone RULE2
The rule binds local variable x to some value that satisfies ϕ(x, y) and executes RULE1. If no suitable value exists, i.e. if ∃ x.ϕ(x, y)) is false for the current values y, then RULE2 is executed instead.
To remove nondeterminism a (static) choice function choice(n, y) is speci- fied by the constraint
∀ y.(∃ x.ϕ(x, y)) → ϕ(choice(n, y), y)
which guarantees that choice(n, y) returns a suitable value for each n that can be used as a value for x. The nondeterminism is then removed by transforming the rule to
Det(RULE) ≡
if ∃ x.ϕ(x, y)
then let x = choice(n, y)
in n := n + 1 seq
RULE1
else RULE2

The new rule uses n as a counter that is incremented each time a choice is needed. This guarantees that all choices are independent and may yield different results, even when y always have the same values. seq is ASM notation for sequential composition.
It can be proved that for every run σ of the original ASM a corresponding run σ' of the modified ASM exists and vice versa. σ' starts in a state (alge- bra) that defines a suitable semantics of the choice function which predicts the choices made when running σ. Therefore σ and σ' agree on all program variables (i.e. dynamic functions) except that σ' adds the counter n. Repeat- edly replacing all choose rules with choice functions gives an ASM which has the same finite as well as inﬁnite runs as the original ASM.
The idea of removing nondeterminism from an ASM is not new: it was already described in [37], sketched in [8] (Remark 2.4.1 on p. 76) and we already used it in [32] to prove various equivalences between data types and ASMs.
The construction of moving nondeterminism to the initial state is also not limited to ASMs. For languages based on relational calculus the task is a little more complicated since one has to check for each variable, whether it is changed nondetermistically, and on which other variables the outcome depends, in order not to introduce unneccessary parameters for the choice function. As a simple example consider the translation of Z operation OP to the deterministic operation Det(OP). Since ctr is changed nondeterministically depending on the old value, a function choice(n, ctr) is needed that is specified by choice(n, ctr) > ctr.

OP 		Det(OP ) 	




Completeness of IO Automata Refinement
IO automata are a formalism that has similarities to ASMs. In this section we show, that the completeness proof for ASM refinement also applies to IO automata. We first give a short summary of the necessary definitions, following [24].
Definition 7.1 (IO Automata) An IO Automaton IOM = (S, I, A, SEM) consists of a set of states S, a subset of initial states I, a set of actions

A which always contains the empty action τ, and a transition relation
SEM ⊆ S × A × S.
Definition 7.2 (Fragments, Executions and Action Traces)
An execution fragment is a finite or infinite sequence σ = (s0, a0, s1,.. .), such that all (si, ai+1, si+1) are in SEM
An execution is a fragment starting with an initial state.
For an execution σ = (s0, a0, s1,.. .), it’s action trace 3 trace(σ) is defined to be the sequence (a0, a1,.. .), but with all τ ’s removed.
frag(IOM), exec(IOM) and trace(IOM) are the sets of all fragments, all exe- cutions and all action traces of IOM.
IO Automata can be easily translated to ASMs. A state of the corre- sponding ASM is composed of the automaton state s and the list al of all non-τ actions done so far. Final states (s, al) of the ASM are such that (s, al) ∈ F := {s : ¬ ∃ a, s'.(s, a, s') ∈ SEM}. The ASM rule for an IO automa- ton is:
choose a, s' with (s, a, s') ∈ SEM
in s := s'
if a /= τ then al := (al, a)

Definition 7.3 (Refinement  of  IO  Automata)  An  IO  automa- ton CIOM = (SC, IC, A, SEMC) refines AIOM = (SA, IA, A, SEMA) written CIOM ≤T AIOM iff traces(CIOM) ⊆ traces(AIOM).
The given definition is the stronger one of the two given in [24]. The weaker one, CIOM ≤∗T AIOM , requires only that the ﬁnite action traces of CIOM are a subset of those of AIOM, which is a similar requirement than the one of data refinement. Completeness is proven for this weaker requirement in [24].
Refinement of IO automata is weaker than ASM refinement for two reasons. First, it ignores termination of runs: an IO automaton with a single one-step run (s0, a, s1) can be refined to one with one infinite run (s0, a, s1, τ, s2, τ, s3, τ,.. .) and vice versa. In general adding “stuttering” steps (s, τ, s) for final states, where no step is applicable does not change the set of traces. Second, refinement of IO automata also allows to refine any run with a shorter run. E.g. (s0, a0, s1, a1, s2, a2, s3) can be refined to (s0, a0, s1).
By translating IO Automata to ASMs and choosing relation IO to be iden- tity on the action list the completeness proof for ASMs can be translated to

3 In [24] action traces are just called traces.

a proof purely in terms of IO automata. The proof constructs a deterministic automaton similar to ASMs.
Definition 7.4 (Corresponding Deterministic Automaton)
The corresponding deterministic automaton Det(IOM)= (S', I', A', SEM') to IOM = (S, I, A, SEM) consists of:
S' := frag(IOM)
I' := exec(IOM)
SEM' = {(σ, a, σ'): σ = (s, a, σ')}
This construction is similar to the guess automaton ([24], p. 27), but it does not guess just finite executions, but any execution.
Theorem 7.5 (Equivalence of IOM and Det(IOM))	IOM and Det(IOM) have the same action traces, both IOM ≤T Det(IOM) and Det(IOM) ≤T IOM hold.
Again the proof is immediate by comparing executions. The only run Det(IOM) has from an initial state σ ∈ exec(IOM) is the run executing the actions of σ, shortening the execution on every step. All nondeterminism has been moved to the initial state, Det(IOM) is deterministic 4 .
This result is stronger than what can be proved using the standard back- ward simulation which maps traces to initial states. Backward simulation only implies IOM ≤∗T Det(IOM), the stronger result IOM ≤T Det(IOM) does not follow from Proposition 2.5 of [24], since CIOM is not required to have finite invisible nondeterminism. It also does not follow from Theorem 3.17, since the backward simulation constructed is not image-finite in general.
Det(CIOM) is now linked to AIOM via a forward simulation. For IO au- tomata the definition is
Definition 7.6 (Forward  Simulation  for  IO  automata)  A relation R ⊆ SA × SC between the states of two IO automata CIOM AIOM is a for- ward simulation if
IC(cs) → ∃ as.R(as, cs) ∧ IA(as) (“initialization”)
R(as, cs) ∧ (cs, a, cs') ∈ SEM
→ ∃ σ = (as, a1, as1,... , as') ∈ frag(AIOM): trace(σ)= (a) ∧ R(as', cs') (“correctness”)
CIOM ≤F AIOM means that a forward simulation exists between the two automata. CIOM ≤F AIOM implies CIOM ≤T AIOM as shown in [24].

4 note, that this definition of a deterministic IO automaton is different from the one of [24].

The correctness condition allows n:1 diagrams (with n ≥ 0). This is suf- ficient for ASM refinement too as shown in [31], although less flexible in ap- plications. There is no criterion, that some order must be decreased in 0:1 diagrams, since preserving termination is not required.
Theorem 7.7 (Completeness of IO Automata Refinement) Every correct IO automata reﬁnement CIOM ≤T AIOM can be veriﬁed by proving Det(CIOM) ≤F AIOM.
The theorem follows directly from
Theorem 7.8 (Completeness for Deterministic Concrete IO automata)
If CIOM ≤T AIOM and CIOM is deterministic, then CIOM ≤F AIOM
The forward simulation needed for the proof is
R(as, cs)
↔ ∃ σC, σA, m, n.
σA ∈ exec(AIOM) ∧ σC ∈ exec(CIOM)
∧ m < #σA ∧ n < #σC
∧ as = σA[m] ∧ cs = σC[n]
∧ (#σC < ∞ → last(σC) ∈ F)
∧ ∃ (ik)k∈IN.	in = m ∧ monotone(i)
∧ ∀ k. n ≤ k ∧ k < #σC
→ ik < #σA ∧ trace(σA to k)= trace(σA to ik)

For σ = ((s0, a1, s1,... , sj) the definition uses #σ to denote the length j (which may be ∞). σ[m] is sm and for n < j σ to n denotes (s0, a1, s1,... , sn). For a finite execution last(σ)= sj and monotone(i) means ik ≤ ik+1 for all k.
To establishes the initialization condition determinism is exploited, which implies that for an initial state cs only one possible maximal (i.e. the last state is in F if the execution is finite) execution σC exists. Refinement implies that a corresponding trace σA can be constructed with the same action trace. Setting m and n both to be zero it finally remains to define the sequence (ik) which defines corresponding initial segments of the executions with the same action traces. This is clearly possible since the full executions have the same action traces.
The proof of the correctness condition for forward simulation then is sim- ple. Since the concrete system has at most one possible step it must be the step from σC[n] to σC[n + 1]. The corresponding fragment of σA then is the one from σA[in] to σA[in+1], therefore n, m after the step are chosen to be n + 1 and in+1. The sequence (ik) can remain unchanged.

The theorem establishes, that IO automata refinements can be verified using forward simulation, provided one is willing to move nondeterminism to the initial state by using choice functions. The theorem is dual to Theorem
5.6 of [24], first proved in [35].
It should finally be mentioned, that adding choice functions is possible only in interactive verification, where additional functions can be added eas- ily. It is not an option for finite-state systems when an automatic proof using model checking is intended, since the choice function determines infinitely many values. A similar disparity can be noted for the size of diagrams. [16] defines normed simulations as a variation of IO automata refinement, which use “small” 0:1, 1:1 and 1:0 diagrams only. For normed simulations, choos- ing the number of abstract steps in forward simulations is unnecessary, which simplifies model checking attempts. ASM refinement instead uses “big” dia- grams, since these often define the natural correspondence between runs and often give simpler simulation relations for interactive verification.

Wim Hesselink’s Work
A reviewer of this workshop made us aware that there is a series of papers by Wim Hesselink [18], [19], [20], [21] that give completeness proofs for refinement in the Abadi-Lamport setting [2] that have great similarity to this work.
Indeed, although our setting is technically different (ASMs have finite as well as infinite runs, failing rules are considered, but there is no built-in stut- tering) there is a similarity between the eternity variables used in this setting and the choice functions used in ours.
Our impression is that the ASM refinement construction is simpler than the one given [21] , but also proves a weaker result. The main reason is that a system in the Abadi-Lamport setting does not only define a transition system M, but also a supplementary property P with liveness and fairness constraints. A run that satisfies the supplementary property is called a be- havior. Abadi-Lamport refinement then requires to construct a corresponding abstract behavior for every concrete one, while ASM refinement just requires to constructs an abstract run for every concrete run.
The completeness proof becomes more complex, since the abstract run con- structed by our forward simulations may not be a behavior. As an example the supplementary property could require that a certain state must be con- tained in the run. Even if this state is still reachable from every intermediate state of a run σ that is incrementally constructed by a forward simulation, the final run might avoid it, and therefore not be a behavior. To avoid this prob- lem, forward simulations in the Abadi-Lamport setting require to verify an

additional global condition that ensures that all runs constructed by forward simulations are indeed behaviors.
In [21] the problem is solved by adding a clock and a universal eternity extension. Together these essentially construct an intermediate system with states (σC, n) where σC is a behavior of the concrete system, and n a natural number called a clock, that determines how many steps have been executed. Initial states have n = 0.
The intermediate state (σC, n) of this extension corresponds to our nth state tailn(σC) of Det(CM). While our state guesses information about the future, (σC, n) also keeps full information about the past.
For the syntactic construction of removing choose by choice functions the correspondence becomes even closer: a transition system with an always terminating SEM relation would be encoded as the ASM rule
choose cs' with SEM(cs, cs') in cs := cs'
Removing the global choose from this rule gives the rule
let cs' = choice(cs, n) in n := n + 1 seq cs := cs'
with a static choice function. This ASM would just have to keep a copy of the initial state cs0, to be able to inductively reconstruct all states of σC as csn+1 = choice(csn, n).
Storing full information about the behavior σC in each state would likely be sufficient to allow a similar construction in the ASM setting than what is done in [21] for the Abadi-Lamport setting. There, a function c2a that maps every concrete behavior σC to a specific abstract behavior σA := c2a(σC) is defined a priori. Such a function exists, since refinement holds (using the axiom of choice). Instead of defining a forward simulation as in our construction the function is then used to define a refinement mapping f (i.e. a forward simulation that is a function) from the intermediate level to the abstract level as f(σC, n)= c2a(σC)[n]. Note that f ensures that successive states of the concrete run are mapped to successive states of just one abstract behavior c2a(σC). Thereby the problem that the constructed abstract run might not be a behavior is avoided.
A small technical complication is, that in the Abadi-Lamport setting, a transition system without any additional liveness property can stutter in- finitely instead of doing useful steps. For the intermediate system such be- haviors ((σC, 0), (σC, 0),.. .) must be ruled out by adding a supplementary property which requires that the clock is increased infinitely often.
Finally the different amounts of stuttering of the concrete and the abstract

system that are possible in a refinement in the Abadi-Lamport setting are taken into account in [21] by a construction called temporization, which allows to add a finite amount (say n) of stuttering steps in between the original steps of the concrete level. Together with stuttering forward simulations which allow a concrete step to implement m abstract steps, where all but one are stuttering steps, this seems to give a similar flexibility than our use of m:n diagrams.
Setting up a formal correspondence between the two formalisms remains future work. It seems it should be possible to use the idea of having function c2a in the ASM setting to get a completeness result for ASMs with additional properties. It should also be possible to define generalized forward simulations in the Abadi-Lamport setting. The definition could rely on accumulating observations, similar to the ASM for an IO automaton, that accumulates the actions done so far. An alternative would be to require at most one observable state change in every commuting diagram.


Conclusion
In this paper we have given a completeness proof for ASM refinement. The proof shows that generalized forward simulation alone is sufficient to prove cor- rectness of any ASM refinement, provided that in some cases nondeterminism of the concrete ASM is moved to the initial state by adding choice functions that predict decisions taken during the run. The construction we have used is dual to well-known completeness proofs that add history information and prove the existence of a backward simulation, which are not applicable here, since termination must be preserved for ASM refinement.
We have also shown that a similar completeness result can be obtained for IO automata refinement. The result is obtained for both settings without an assumption of finite invisible nondeterminism.
Our preliminary analysis has shown several parallels to Wim Hesselink’s completeness proof for refinement in the Abadi-Lamport setting. This setting and therefore the completeness proof are more complex than ours, since fair- ness and liveness constraints have to be considered. Nevertheless some of the steps in both completeness proofs are quite similar. An analysis of the formal correspondence still remains further work.
Whether a similar completeness result can be obtained for data refine- ment also remains as an open question. The answer is non-obvious, since a fully deterministic intermediate data type is not possible: any intermediate (conformal) data type will always have a nondeterministic choice between the operations.

Acknowledgement
I would like to thank Richard Banach for his remark “nondeterminism can be moved anywhere” which was the trigger for this work. I would also like to thank my colleagues Holger Grandy, Nina Moebius and Kurt Stenzel for carefully reading drafts of this paper. Thanks also to John Derrick, Heike Wehrheim and Eerke Boiten, who made me understand some of the intricate details of data refinement. Finally I would like to thank the reviewer who directed me to Wim Hesselink’s work.

References
J.-R. Abrial and S. Hallerstede. Refinement, Decomposition, and Instantiation of Discrete Models: Application to Event-B. Fundamenta Informaticae, 21, 2006.
M. Abadi and L. Lamport. The existence of refinement mappings. Theoretical Computer Science, 2:253–284, May 1991. Also appeared as SRC Research Report 29.
C. Bolton, J. Davies, and J.C.P. Woodcock. On the refinement and simulation of data types and processes. In K. Araki, A. Galloway, and K. Taguchi, editors, Proceedings of the International conference of Integrated Formal Methods (IFM), pages 273–292. Springer, 1999.
E. B¨orger. A Logical Operational Semantics for Full Prolog. Part I: Selection Core and Control. In E. B¨orger, H. Kleine Bu¨ning, M. M. Richter, and W. Sch¨onfeld, editors, CSL’89. 3rd Workshop on Computer Science Logic, volume 440 of LNCS, pages 36–64. Springer, 1990.
E. B¨orger. A Logical Operational Semantics of Full Prolog. Part II: Built-in Predicates for Database Manipulation. In B. Rovan, editor, Mathematical Foundations of Computer Science, volume 452 of LNCS, pages 1–14. Springer, 1990.
E. B¨orger. The ASM Refinement Method. Formal Aspects of Computing, 15 (1–2):237–257, November 2003.
E. B¨orger and D. Rosenzweig. The WAM—definition and compiler correctness. In C. Beierle and L. Plu¨mer, editors, Logic Programming: Formal Methods and Practical Applications, Studies in Computer Science and Artificial Intelligence 11, pages 20–90. North-Holland, Amsterdam, 1995.
E. B¨orger and R. F. Sta¨rk. Abstract State Machines—A Method for High-Level System Design and Analysis. Springer-Verlag, 2003.
J. Derrick and E. Boiten. Reﬁnement in Z and in Object-Z : Foundations and Advanced Applications. FACIT. Springer, 2001.
J. Derrick, E. A. Boiten, H. Bowman, and M. Steen. Weak Refinement in Z. In J.P. Bowen,
M.G. Hinchey, and D.Till, editors, ZUM ’97: The Z Formal Speciﬁcation Notation, volume 1212, pages 369–388. Springer LNCS, 1997.
J. deBakker. Mathematical Theory of Program Correctness. International Series in Computer Science. Prentice-Hall, 1980.
E. W. Dijkstra. A Discipline of Programming, chapter 14. Prentice-Hall, Englewood Cliffs, N. J., 1976.
W. de Roever and K. Engelhardt. Data Reﬁnement: Model-Oriented Proof Methods and their Comparison, volume 47 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1998.

J. Derrick and H. Wehrheim.  Using Coupled Simulations in Non-atomic Refinement.  In
D. Bert, J. Bowen, S. King, and M. Walden, editors, ZB 2003: Formal Speciﬁcation and Development in Z and B, volume 2651, pages 127–147. Springer LNCS, 2003.
Y. Gurevich. Evolving algebras 1993: Lipari guide. In E. B¨orger, editor, Speciﬁcation and Validation Methods, pages 9 – 36. Oxford Univ. Press, 1995.
W.O.D. Griffioen and F.W. Vaandrager. Normed simulations. In A.J. Hu and M.Y. Vardi (eds), editors, Proceedings CAV’98, volume 1427 of LNCS, pages 332–344, Vancouver, BC, Canada, 1998.
W.H. Hesselink. Programs, Recursion and Unbounded Choice, volume 27 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1992.
W. H. Hesselink. Eternity variables to simulate specifications. In J. van Leeuwen G. Good,
J. Hartmanis, editor, Mathematics of Program Construction MPC 02, volume 2386 of LNCS, pages 117–130. Springer, 2002.
W. H. Hesselink. Eternity variables to prove simulation of specifications. ACM Trans. on Computational Logic, 6:175–201, 2005.
W. H. Hesselink. Splitting forward simulations to cope with liveness. Acta Informatica 42, 42:583–602, 2006.
W. H. Hesselink. Universal extensions to simulate specifications. Information and Computation, 206:106–128, 2008.
D. Harel, D. Kozen, and J. Tiuryn. Dynamic Logic. MIT Press, 2000.
Web presentation of KIV projects.
URL: http://www.informatik.uni-augsburg.de/swt/projects/ .
N. Lynch and F. Vaandrager. Forward and Backward Simulations – Part I: Untimed systems. Information and Computation, 121(2):214–233, September 1995. also: Technical Memo MIT/LCS/TM-486.b, Laboratory for Computer Science, MIT.
R. D. Maddux. Relation-algebraic semantics. Theoretical Computer Science, 160(1–2):1–85, 1996.
T. Nipkow.	Hoare logics for recursive procedures and unbounded nondeterminism.	In
J. Bradfield, editor, Computer Science Logic (CSL 2002), volume 2471 of LNCS, pages 103–119. Springer, 2002.
W. Reif, G. Schellhorn, K. Stenzel, and M. Balser. Structured specifications and interactive proofs with KIV. In W. Bibel and P. Schmitt, editors, Automated Deduction—A Basis for Applications, volume II: Systems and Implementation Techniques, chapter 1: Interactive Theorem Proving, pages 13 – 39. Kluwer Academic Publishers, Dordrecht, 1998.
G. Schellhorn and W. Ahrendt. Reasoning about Abstract State Machines: The WAM Case Study. Journal of Universal Computer Science (J.UCS), 3(4):377–413, 1997. URL: http://www.jucs.org.
G. Schellhorn and W. Ahrendt. The WAM Case Study: Verifying Compiler Correctness for Prolog with KIV. In W. Bibel and P. Schmitt, editors, Automated Deduction — A Basis for Applications, volume III: Applications, chapter 3: Automated Theorem Proving in Software Engineering, pages 165 – 194. Kluwer Academic Publishers, 1998.
G. Schellhorn. Veriﬁcation of Abstract State Machines. PhD thesis, Universit¨at Ulm, Fakult¨at fu¨r Informatik, 1999. URL:
http://www.informatik.uni-augsburg.de/lehrstuehle/swt/se/publications/.
G. Schellhorn.  Verification of ASM Refinements Using Generalized Forward Simulation.
Journal of Universal Computer Science (J.UCS), 7(11):952–979, 2001. URL: http://www.jucs.org.

G. Schellhorn. ASM Refinement and Generalizations of Forward Simulation in Data Refinement: A Comparison. Journal of Theoretical Computer Science, vol. 336, no. 2-3:403– 435, May 2005.
G. Schellhorn. ASM Refinement Preserving Invariants. In Proceedings of the 14th International ASM Workshop, ASM’07, Grimstad, Norway, 2008. to appear in JUCS, http://ikt.hia.no/asm07/.
G. Schellhorn, H. Grandy, D. Haneberg, N. Moebius, and W. Reif. A Systematic Verification Approach for Mondex Electronic Purses using ASMs. In U. Gl¨asser J.-R. Abrial, editor, Dagstuhl Seminar on Rigorous Methods for Software Construction and Analysis, volume 5115 of LNCS. Springer, 2008.
A. P. Sistla.	Proving correctness with respect to nondeterministic safety specifications.
Infrormation Processing Letters, 39:45–49, 1991.
M. Smyth. Power domains. Journal of Computer and System Sciences, 16:23–36, 1978.
R. F. St¨ark and S. Nanchen. A Complete Logic for Abstract State Machines. Journal of Universal Computer Science (J.UCS), 7 (11):981 – 1006, 2001.
J. M. Spivey. The Z Notation: A Reference Manual. Prentice Hall International Series in Computer Science, 2nd edition, 1992.
J. C. P. Woodcock and J. Davies. Using Z: Speciﬁcation, Proof and Reﬁnement. Prentice Hall International Series in Computer Science, 1996.
