

Electronic Notes in Theoretical Computer Science 253 (2009) 129–144
www.elsevier.com/locate/entcs

Using CLP Simplifications to Improve Java Bytecode Termination Analysis
Fausto Spoto1
Dipartimento di Informatica Universita` di Verona
Italy
Lunjin Lu2
Oakland University USA
Fred Mesnard3
LIM IREMIA
Universit´e de la R´eunion France

Abstract
In an earlier work, a termination analyzer for Java bytecode was developed that translates a Java bytecode program into a constraint logic program and then proves the termination of the latter. An efficiency bottleneck of the termination analyzer is the construction of a proof of termination for the generated constraint logic program, which is often very large in size. In this paper, a set of program simplifications are presented that reduce the size of the constraint logic program without changing its termination behavior. These simplifications remove program clauses and/or predicate arguments that do not affect the termination behavior of the constraint logic program. Their effect is to reduce significantly the time needed to build the termination proof for the constraint logic program, as our experiments show.
Keywords: Java, Java bytecode, static analysis, termination


Introduction
Termination analysis attempts to prove that programs terminate. Since termina- tion of Turing-equivalent programming languages is undecidable [18], termination analysis only succeeds for a (hopefully large) class of programs, although many

1 Email: fausto.spoto@univr.it
2 Email: L2Lu@oakland.edu
3 Email: frederic.mesnard@univ-reunion.fr

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.11.019

terminating programs are not proved to terminate. Despite this limitation, it is in- creasingly important in software technology, since proofs of termination add value to software downloaded from insecure networks into computers or cellular phones: the user wants a proof that that software will actually terminate and yield a result or otherwise he will not use it and pay for it.
Termination analyses have been developed for logic [8,10,7], functional pro- grams [14] and term rewrite systems [11], whose semantics is relatively simple and well understood. More recently, termination analysis has been applied to imperative programs, dealing with primitive values only [9,15], lists [13,6,5,4] or any dynamic data-structure [17]. In all cases, termination is typically proved by showing that some well-founded measure decreases along loops and recursion, so that divergence cannot occur. This measure can be the value of a variable of primitive type, the length of a list, the maximal path of pointers reachable from a given variable [16] or a mix of such values. When generic data structures are considered, the shape of the computer memory must be somehow approximated, since destructive updates mute dynamic data through shared pointers. Possibly cyclical data structures must be detected, since iterations over them might diverge.
In [17], a termination analysis is defined working for any sequential Java bytecode program [12], dealing with any dynamic data structure, possibly cyclical and shared. Since Java is compiled into Java bytecode, that technique can also be used for termination analysis of Java. It works by translating the Java bytecode program into a constraint logic program (CLP ) expressing size relationships between program variables at different program points. It has been proved in [17] that if the CLP program terminates then the original Java bytecode program terminates. Hence all techniques for termination analysis of CLP can be used to prove termination of Java and Java bytecode. In [17], the BinTerm termination prover is used to that purpose. Experiments scale to programs of up to 1000 methods. Although this is already an impressive result, it must be acknowledged that the analysis is expensive in terms of the time needed to build the proof of termination.
In this paper we contribute to the termination analysis of Java and Java bytecode programs. Namely,
we present a set of simplifications of the CLP programs generated by the ter- mination analysis in [17]. They transform the program by removing clauses or variables, yet preserving its behaviour w.r.t. termination;
we prove those transformations correct;
we experiment with those transformations and show them effective: they reduce by orders of magnitude the cost of finding a termination proof for the CLP pro- grams.
These techniques are now embedded in the termination prover for Java bytecode available at the address http://julia.scienze.univr.it/termination.
Although some of our simplifications are, often implicitly, used in the termination analysis of programs, this is not the case for others. Namely, the restriction to only those clauses that form a loop in the code (Subsection 4.1) cannot be applied to

public class List<X> {
private X head; private List<X> tail; public List(X[] values) { this(values,0); }
public List(X h, List<X> t) { head = h; tail = t; } private List(X[] values, int l) {
while (l < values.length && values[l] == null) l++; if (l < values.length) {
this.head = values[l];
if (l + 1 < values.length)
this.tail = new List<X>(values,l + 1);
}
}
public List<X> append(List<X> other) {
if (tail == null) return new List<X>(head,other); else return new List<X>(head,tail.append(other));
}
public void afterInteger() { afterIntegerAux(false); } private void afterIntegerAux(boolean wasInteger) {
if (head instanceof Integer) {
if (tail != null) tail.afterIntegerAux(true);
} else {
if (tail != null) tail.afterIntegerAux(false); if (wasInteger) head = null;
}
}
public String toString() {
if (tail == null) return "* ";
else if (head instanceof Integer) return "* " + tail.tail.toString(); else return "* " + tail.toString();
}
public static void main(String[] args) {
Object[] vs = { new Object(),3,3.14,null,new List<Integer>(3,null) }; List<Object> list1 = new List<Object>(vs);
List<Object> list2 = new List<Object>(vs); list2.afterInteger();
String s = list1.append(list2).toString();
}
}

Fig. 1. An example Java program.

other frameworks, such as the termination analysis of logic programs, since one needs the removed clauses there, in order to take care of instantiation patterns due to the presence of logical variables (which do not exist in our setting). Also the simplifications based on removing variables which are irrelevant for termination are new (Subsection 4.4). Moreover, we present all such simplifications together and prove them correct in a uniform setting, which was not the case before. Furthermore, we experiment with their effects on the termination analysis of real, large software, which was never the case before; in particular, those simplifications have never been applied to the termination analysis of Java bytecode.

Our Running Example
Consider the Java program in Figure 1. It implements a generic list of elements of type X. Two constructors are available. The first builds a list from head and tail; the second builds recursively a list from an array. The method append concatenates two lists this and other. The method afterInteger writes null after all elements of the lists of type Integer. Method toString() yields a String representing the list elements as asterisks, but does not represent the elements that follow an object of type Integer. All these methods are recursive. Method main builds some lists and calls the previous methods.
We compile this program into Java bytecode and analyse the bytecode as in [17].

Our system tells us that the program terminates. We refer to [17] for the detailed description of how our system works. Here, we briefly give an intuition. First, the Java bytecode is transformed into a graph of basic blocks [1], as done in Figure 2 for method append. Recursion is made explicit by linking each method call to the be- ginning of the called method(s), as we do for block 6560 in Figure 2. The makescope τ pseudo-bytecode creates the activation stack for a method with arguments of type τ . The catch pseudo-bytecode marks the beginning of a default exception handler which throws back all exceptions to the caller. Bytecodes inside each block are ab- stracted into a linear constraint c over-approximating the path-length of each local variable and stack element at its beginning and at its end [16]. For instance, for block 6391 we have c = {IS0 − OS1 = 0, IL1 − OS4 = 0, IS0 − OS0 = 0, IL1 − OL1 = 0,
IL0 − OL0 = 0, OS3 ≥ 0, OS2 ≥ 0, IL0 − OS3 ≥ 1, IL0 − OS2 ≥ 1}. The variables ISn
stand for the path-length of the nth stack element at the beginning of the block;
OSn for their path-length at the end of the block; ILn and OLn are the same for the nth local variable. This constraint is then used to build CLP clauses. In principle, there is a CLP clause for each arrow in the graph of basic blocks. Let blocki be a predicate expressing the path-length of the variables in scope at the beginning of block i. Its arity depends on which local variables and stack elements are in scope at the beginning of block i. We build clauses


block6391(IL0, IL1, IS0): −c, block6392(OL0, OL1, OS0, OS1, OS2, OS3, OS4).
block6391(IL0, IL1, IS0): −c, block6560(OL0, OL1, OS0, OS1, OS2, OS3, OS4).
(1)

since two arrows connect block 6391 with blocks 6392 and 6560. Two local variables L0 and L1 are in scope there (L0 implements this and L1 implements other). At the beginning of block 6391 there is only one stack element S0, while there are 5 at its end. Those clauses form a CLP program whose termination entails that of the original Java bytecode program [17]. The clauses of that program have exactly one predicate on their right.
Although the program in Figure 1 is relatively small, the number of arrows in its graph of basic blocks is quite large: the resulting CLP program consists of 297 clauses. The aim of the present paper is to introduce simplification techniques for such CLP programs which shorten the termination proofs. Next sections formalize our notion of CLP programs and show how these programs can be simplified.

CLP over Linear Integer Constraints
We formalise here the CLP programs of the previous section. Namely, they are sets of predicates, each defined by a set of clauses. We require that predicates are named blockx or entryx. Predicates are not distinguished by their arity. That is, two different predicates must be distinct identifiers. For our purposes, clauses arise from arrows in the graph of basic blocks, so we can assume them to have the form p(i) :- c, q(o), where i and o are disjoint sequences of distinct variables and c is a linear integer constraint on i and o. This is similar to [8] and more general




Fig. 2. The basic blocks for the method append in Figure 1.

than [3], where binary clauses express size-change graphs, although a more limited form of constraints is used there. Each local variable or stack element v in the bytecode program induces an input variable iv and an output variable ov in the CLP program. The sequence i consists of only input variables and o of only output variables. For each clause in the program, we refer to three sets of variables V , I and O; they are the sets of bytecode variables, induced input variable and induced output variables, respectively.
Definition 3.1 [Valuation] A valuation θ is a map from a finite set of variables into integers. Let v = v1v2 ··· vk be a sequence of variables and val = val 1val 2 ··· valk ∈ Zk. We write [v1 '→ val 1,... , vk '→ valk] or [v '→ val ] for the valuation θ which is such that θ(vi) = vali for all i = 1,... ,k and is undefined elsewhere. Let c be a

constraint; then cθ is c where each variable v is replaced by θ(v). This notation is extended to any syntactical object, such as sequences of variables and predicates. The valuation θ is a solution of c if cθ is equivalent to true. Let p be a predicate; then c[p(v) '→ p(val)] stands for c[v '→ val].
We define now the operational semantics for CLP over linear integer constraints. It expresses the fact that variables stand for the path-length of concrete data struc- tures in the memory of the system and hence can be undefined but not free, in the sense of logic programming.
Definition 3.2 [Operational Semantics of our CLP Language] Let p, q be predicates and m, n ∈ Z∗. We say that q(n) is derived from p(m) using clause C = (p(i) :- c, q(o)), written p(m) →C q(n), if there is a solution θ of c[i '→ m] such that q(n) = q(o)θ. Clause C in p(m) →C q(n) is often omitted unless nec- essary.  A derivation of p0(n0) is p0(n0) → p1(n1) → ··· → pk(nk) such that
pi+1(ni+1) is derived from pi(ni) for all 0 ≤ i < k. A resolution is a maximal derivation.
The above operational semantics lets us formalise the notion of termination. It uses a partition of the predicates of the program in strongly-connected components. Namely, for every clause p(i) :- c, q(o), we let p ≤ q. Then predicates p0 and p1 belong to the same strongly-connected component if and only if p0 ≤∗ p1 and p1 ≤∗ p0 where ≤∗ is the reflexive and transitive closure of ≤. This means that they are part of the same loop. A predicate q is an entry if it occurs in a clause q(n) :- c, s(m) with q and s in the same strongly-connected component (i.e., in a loop) and also in a clause t(v) :- c, q(w) with q and t in different strongly-connected components. We assume that entries are named entryx. From now on, when we say that a predicate is an entry of a CLP program, we mean that its name is entryx
for some x.
Definition 3.3 [Termination] An entry p terminates in a program P if, for every n ∈ Z∗, all resolutions of p(n) by using the clauses of P , with predicates in the strongly-connected component of p, are finite. Otherwise, p is said to diverge. Let P1 and P2 be programs. P1 terminates more than P2, and we write P1 ± P2, if whenever an entry of P1 terminates in P1, it also terminates in P2. They are termination-equivalent, and we write P1 ≡ P2, if P1 terminates more than P2 and vice versa.
Note that if p is not defined in P then it terminates in P since its derivations have length 1. The notion of P1 terminating more than P2 entails that a proof of termination for the predicates of P2 is also a proof of termination for the predicates of P1.
Definition 3.3 formalizes a loop-local termination. This means that an entry terminates if it terminates by using the predicates of the loop where it occurs. This is importamnt to report a feedback to the user about which loop of which method might introduce the non-termination, without considering entries that diverge just because the computation, after executing the loop where the entry occurs, continues

into another loop that diverges. Entries can also be used to improve the precision of the analysis by computing call-patterns from them to the other blocks [17]. We do not discuss this optimization here.
Next section presents a set of program transformations that simplify a CLP program P into a smaller program Ps. It will always be the case that P and Ps are termination-equivalent.

Program Simplifications
Removing clauses outside loops
In graphs such as that in Figure 2, arrows outside loops cannot be executed during a divergent computation, which stays inside the same strongly-connected component of the entry where it is started (Definition 3.3). Hence it seems reasonable to remove any clause that is not part of a loop i.e., such that its head and tail do not belong to the same strongly-connected component of blocks. For instance, only the second clause in (1) is generated.
The following result formalizes of well-known technique used in many termi- nation analyzers. It allows us to prove termination for the loops of the program. A clause p(n) :- c, q(m) occurs in a loop if p and q are inside the same strongly- connected component of predicates.
Proposition 4.1 (Correctness of clauses outside loops removal) Let P be a program and Ps be the same program deprived of those clauses that do not occur in a loop. Then P ≡ Ps.
Proof. We have Ps ± P since Ps ⊆ P . It remains to prove P ± Ps. Programs P and Ps have the same set of entries. Let q be an entry. If q terminates in P then it terminates in Ps since the latter has less clauses than P . If q diverges in P then
there is an infinite derivation using only predicates inside the strongly-connected component of q. Hence only clauses in Ps are used by that derivation, so that q diverges in Ps.	 
If we apply this simplification to the CLP program derived from the Java pro- gram in Figure 1, the number of clauses decreases from 297 to 12 and the time needed to prove all the entries terminating is 2.72 seconds.
Because of this simplification, from now on we assume that each predicate is only used in its strongly-connected component. Hence termination according to Definition 3.3 corresponds, from now on, to termination by using all the clauses of the program.
Removing clauses by unfolding
If a program contains clauses p(m) :- c1, q(n) and q(v) :- c2, s(w), we can unfold them into the clause p(m) :- c1 ∧ c2 ∧ n = v, s(w) (we assume without loss of generality that clauses are renamed so that they do not share variable). If this is done systematically, for all occurrences of q on the right of the clauses of P , and

the clauses defining q are later removed, we say that we unfold q away from P . The result is a program with less predicates but potentially more clauses than P . However, subsequent simplifications will usually remove most of them, so that this simplification is useful in practice.
Proposition 4.2 (Correctness of unfolding away of a predicate) Let P be a program and q a non-entry predicate in P with no clause of the form q(n) :- c, q(m). Let Ps be P where q has been unfolded away. Then P ≡ Ps.
Proof. Programs P and Ps have the same set of entries. Let p be an entry of Ps. If p diverges in Ps then there is an infinite derivation d for p in Ps. Some steps of this derivation might use clauses derived from unfolding r(m) :- c1, q(n) with q(v) :- c2, s(w). We can replace those steps in d with two steps using those two clauses instead. The result is an infinite derivation for p that uses clauses of
P . Hence p diverges in P . Conversely, if p diverges in P then there is an infinite derivation d for p in P . If a clause such as r(m) :- c1, q(n) is used during that derivation, then the subsequent step must use a clause of the form q(v) :- c2, s(w). Hence those two steps can be merged in d into a unique step that uses the unfolded
clause r(m) :- c1 ∧ c2 ∧ n = v, s(w). The resulting infinite derivation does not refer to q anymore and uses clauses in Ps. Hence p diverges in Ps.	 
Note that Proposition 4.2 does not allow us to unfold away the entries to loops, whose termination is used to tell if each given loop terminates.
If we apply this simplification to the CLP program obtained at the end of Sub- section 4.1, the number of clauses decreases from 12 to 8 and the time needed to prove all the entries terminating goes down from 2.72 to 1.48 seconds (including the time for unfolding).
Removing unsupported or subsumed clauses
By removing unsupported clauses i.e., clauses that call undefined predicates, we maintain the termination-equivalence of programs, since unsupported clauses cannot be used to build an infinite derivation.
Example 4.3 Let P = {C1, C2, C3} with C1 = (entry1(ix) := ix = ox, q(ox)), C2 = (q(ix) :- ix = ox + 1, entry1(ox)) and C3 = (q(ix) :- ix ≥ ox, r(ox)). Predicate r is not defined in P and hence clause C3 is unsupported. Thus P is termination- equivalent to P ' = {C1, C2}.
Proposition 4.4 (Correctness of unsupported clause removal) Let P be a program and Ps be P deprived of unsupported clauses. Then P ≡ Ps.
Proof. Any divergent resolution in Ps is also a divergent resolution in P since Ps has less clauses than P . Any divergent resolution in P is also a divergent resolution in Ps since a divergent resolution in P cannot use any unsupported clause, or otherwise it would be finite.	 
Another simplification consists in removing subsumed clauses (see also [8]). Let for instance C1 = (p(i) :- c1, q(o)) and C2 = (p(i) :- c2, q(o)). We say that C2

subsumes C1 iff c1 |= c2 (c1 entails c2). Note that C1 and C2 only differ in the constraint part.
Example 4.5 The program obtained at the end of Subsection 4.2 contains clauses entry3899(IL0):-OL0 >= 0,IL0 - OL0 >= 1,IL0 >= 2,entry3899(OL0). entry3899(IL0):-OL0 >= 0,IL0 - OL0 >= 2,entry3899(OL0).
The second clause subsumes the first which can hence be removed.
Proposition 4.6 (Correctness of subsumed clause removal) Let P be a pro- gram and Ps be P deprived of subsumed clauses. Then P ≡ Ps.
Proof. Any divergent resolution in Ps is also a divergent resolution in P since Ps has less clauses than P . Hence it is enough to prove that for any divergent resolution in P there is a divergent resolution in Ps. To that purpose, we prove that if C1 = (p(i) :- c1, q(o)) is subsumed by C2 = (p(i) :- c2, q(o)) then p(m) →C1 q(n)
implies p(m) →C2 q(n) for any p, q, m and n, which entails that any derivation
step using C1 can be replicated by using C2. Assume hence that p(m) →C1 q(n). Then there is a solution θ of c1[i '→ m] such that n = oθ. Since c1 |= c2, θ is also a solution of c2[i '→ m] and hence p(m) →C2 q(n).	 
If we apply these simplifications to the CLP program obtained at the end of Subsection 4.2, the number of clauses decreases from 8 to 7 and the time needed to prove all the entries terminating goes down from 1.48 to 1.25 seconds (including the time to apply all the simplifications discussed up to now).
Removing variables
By removing an argument from the clauses of a CLP program, the time needed to build a termination proof of the program decreases, since less arguments means less variables in the data structure implementing the linear constraints and hence better efficiency. Moreover, by removing variables there are chances that distinct clauses get merged because one subsumes another (Subsection 4.3).
Let c be a constraint and let cv = E−{iv,ov}.c and c−v = E{iv,ov}.c. The constraint cv is the v-dedicated part of c since it constrains variables iv and ov only; the constraint c−v is the v-independent part of c since it does not constrain iv nor ov but only the other variables. Let us define an operation that removes a variable
from a predicate, thus reducing its arity:


p(iv ,... , iv
) g v =  p(iv1,... , ivi−1, ivi+1,... , ivn)	if v ≡ vi


Let us define p(ov1,... , ovn) g v similarly. The transformation
Comp−v = {p(i) g v :- c−v, q(o) g v | p(i) :- c, q(o) ∈ Comp}
removes v from a strongly-connected component Comp.

Removal of a variable from a strongly-connected component preserves divergent entries but might introduce more divergent entries.
Proposition 4.7 Let p0 be an entry diverging in Comp. Then p0 also diverges in
Comp−v.
Proof. Since p0 diverges in Comp, there is an infinite resolution
p0(n0) → p1(n1) → p2(n2) → ··· → pk(nk) → ··· 
with pj(ij) :- cj, pj+1(oj) ∈ Comp, pj+1(nj+1) = pj+1(oj)θj and θj solution of
cj[ij '→ nj]. Hence pj(ij) g v :- c−v, pj+1(oj) g v ∈ Comp−v and θj is a solution of
c−v[ij '→ nj] since cj |= c−v. Then θj is a solution of c−v[pj(ij) g v '→ pj(nj) g v]
j	j	j
since c−v is v-independent. Thus,
(pj+1(oj) g v)θj = pj+1(oj)θj g v = pj+1(nj+1) g v
and we can build the following infinite resolution of p0(n0) g v in Comp−v p0(n0) g v → p1(n1) g v → p2(n2) g v → ··· → pk(nk) g v → ··· 
so that p0 diverges in Comp−v.	 
In general, Comp is not termination-equivalent to Comp−v.
Example 4.8 Consider the strongly-connected component
Comp = ⎧⎨ entry1(ix, iy) :- ix ≥ 0, oy = ix, ox = iy, q(ox, oy) ⎫⎬
⎩  q(ix, iy) :- ox = iy — 1, oy = ix, entry1(ox, oy)  ⎭
The entry entry1 terminates in Comp since the value of x decreases in every two other step and is bounded from below by 0. By removing x from Comp we get
Comp−x = ⎧⎨ entry1(iy) :- true, q(oy) ⎫⎬


Now entry1 does not terminate in Comp−x.
The following subsections identify special cases when removal of a variable main- tains the termination-equivalence. A common condition is that the variable is iso- lated from other variables.
Definition 4.9 A variable v is isolated in a strongly-connected component Comp
if, for every clause p(i) :- c, q(o) ∈ Comp, we have c = cv Λ c−v.

Example 4.10 Neither x nor y is isolated in the component Comp of Example 4.8. Instead, both x and y are isolated in the component
Comp = ⎧⎨ entry1(ix, iy) :- ix ≥ 0, ox = ix, oy = iy — 1, q(ox, oy) ⎫⎬
⎩	q(ix, iy) :- ox = ix — 1, oy = iy, entry1(ox, oy)	⎭
Removing right-open/left-open variables
In this subsection we show a first example of a removal of variables for which the converse of Proposition 4.7 holds.
Definition 4.11 [Right or left-open variable] An isolated variable v in a strongly- connected component Comp is right-open if, for every p(i) :- c, q(o) ∈ Comp, we have that cv is either true or iv = ov, or ov ≥ const , ov = const or ov ≤ const (or equivalent) , where const is an integer constant. Left-openness is defined analogously by switching ov with iv in the definition of right-openness.
Example 4.12 The program obtained at the end of Subsection 4.3 contains the component
entry3880(IL0,IL1):-IL1 - OL1 = 0,OL0 >= 0,IL0 >= 2,IL0 - OL0 >= 1, entry3880(OL0,OL1).
where variable L1 is both left- and right-open and can hence be removed obtaining the component
entry3880(IL0):-OL0 >= 0,IL0 >= 2,IL0 - OL0 >= 1,entry3880(OL0).
L1 would still be left-open if there were an extra constraint IL1 >= 3. It would not be left-open anymore if there were also an extra constraint OL1 >= 7.
Consider a resolution of p0(n0) in a strongly-connected component Comp where
v is right-open. Let p(ij) :- cj, q(oj) be the clause used at the jth resolution step.
If cv is iv = ov then the jth step simply copies the value of v from pj to pj+1.
Otherwise, the value of v in pj is not related to that in pj+1: any value satisfying
the v-dedicated part cv of cj may be picked up for v in pj+1; such a value exists
always due to the limited form of cv. This means that v does not contribute to the termination of the predicates in Comp and can hence be removed. This is formally proved below.
Proposition 4.13 (Correctness of left- or right-open variable removal) Let v be right- or left-open in a strongly-connected component Comp. If an entry di- verges in Comp−v then it diverges in Comp.
Proof. We only prove the case when v is right-open. The case when v is left-open is symmetrical. Let hence p0 be a divergent entry in Comp−v. Then there is m0 ∈ Z and an infinite resolution of p0(m0) in Comp−v, which we write as
d0 →C0 d1 →C1 d2 →C2 ··· → dl →CÆ dl+1 ··· 

where every clause p(i) g v :- c−v, q(o) g v used in each portion dl, for l ≥ 0, is obtained from a clause p(i) :- c, q(o) ∈ Comp with cv = (iv = ov) and each Cl
is obtained from a clause C' = (pl(il) :- cl, ql(ol)) ∈ Comp with cv different from
l	l
iv = ov. Let xl ∈ Z be such that, for every l > 0, {ov '→ xl+1} is a solution of cv (hence x0 is completely free). Let p(m) be a call in Comp−v and x ∈ Z. Then we define p(m) ⊕v [x] as the call in Comp obtained from p(m) by putting x at the position for v in the predicate p of Comp. It suffices to prove that there is an infinite resolution of p0(m0) ⊕v [x0] in Comp. Assume that
dl = (pl,0(ml,0) → ··· pl,j (ml,j ) → pl,j+1(ml,j+1) ··· → pl,fÆ (ml,fÆ ))
with p0,0 = p0 and m0,0 = m0. Let pl,j(nl,j)= pl,j(ml,j)⊕v [xl] for each 0 ≤ j ≤ fl. Since pl,j(ml,j) → pl,j+1(ml,j+1), there is pl,j(i) :- (iv = ov) Λ c, pl,j+1(o) ∈ Comp such that pl,j(i) g v :- c, pl,j+1(o) g v ∈ Comp−v and there is a solution θ of c[pl,j(i) g v '→ pl,j(ml,j)] such that pl,j+1(ml,j+1) = (pl,j+1(o) g v)θ. Since c is v-independent, θ ∪ {iv '→ xl, ov '→ xl} is a solution of (iv = ov) Λ c[i '→ nl,j] and (θ ∪ {iv '→ xl, ov '→ xl})(pl,j+1(o)) = (pl,j+1(o) g v)θ ⊕v [xl] = pl,j+1(ml,j+1) ⊕v [xl]= pl,j+1(nl,j+1). Thus, pl,j(nl,j) → pl,j+1(nl,j+1) for 0 ≤ j ≤ fl — 1 and
d' = (pl,0(nl,0) → ··· pl,j (nl,j ) → pl,j+1(nl,j+1) ··· → pl,f (nl,f ))

l
is a derivation in Comp. We show now that p


l,fÆ

(nl,fÆ

) →C'  p
Æ

l+1,0
Æ

(nl+1,0

) so that

we obtain an infinite resolution d'
→ d'
→ ··· d'
'
l+1
→ ··· in Comp. Since

pl,fÆ (ml,fÆ ) →CÆ pl+1,0(ml+1,0), we know that pl = pl,fÆ , ql = pl+1,0 and there
is a solution θ of c−v[pl(il) g v '→ ml,f ] such that ql(ml+1,0) = (ql(ol) g v)θ.
l	Æ

Then θ ∪ {iv '→ xl, ov '→ xl+1} is a solution of c−v[il '→ nl,f ], since c−v
is v-

l	Æ	l
independent, and it is also a solution of cv[il '→ nl,f ] and hence of cl[il '→ nl,f ]
l	Æ	Æ
since cv[il '→ nl,fÆ ]= cv[iv '→ xl]= cv and cv contains only ov and {ov '→ xl+1} is
l	l	l	l
a solution of cv. Also, ql(ol)(θ ∪ {iv '→ xl, ov '→ xl+1})= (ql(ol) g v)θ ⊕v [xl+1]= 

l
ql(ml+1,0) ⊕v [xl+1]= ql(nl+1,0). Hence pl,fÆ (nl,fÆ ) →
Æ pl+1,0(nl+1,0).	 

If we apply this simplification to the CLP program obtained at the end of Sub- section 4.3, the number of clauses goes down from 7 to 6 (because of entailment checks) and there are less arguments in predicates. The time needed to prove all the entries terminating goes down from 1.25 to 1.02 seconds (including the time to apply all the simplifications discussed up to now).
Removing uniform variables
Even if an isolated variable is neither left-open nor right-open, it can still be removed when there is a fixed value that can be put in that variable throughout an infinite resolution. Such a variable is called uniform.
Definition 4.14 [Uniform variable] An isolated variable v is uniform in a strongly- connected component Comp if there is x ∈ Z such that, for every p(i) :- c, q(o) ∈ Comp, the valuation {iv '→ x, ov '→ x} is a solution of cv (note that cv may contain more than one constraint).

Example 4.15 The program obtained at the end of Subsection 4.5 contains the component:
block3853(IL0,IL1,IL2):-IL2 - OL2 = -1,IL1 - OL1 = 0,IL0 - OL0 = 0,
IL1 - IL2 >= 1,block3853(OL0,OL1,OL2). block3853(IL0,IL1,IL2):-IL2 - OL2 = -1,IL1 - OL1 = 0,OL0 = 1,
IL1 - IL2 >= 2,entry3849(OL0,OL1,OL2). entry3849(IL0,IL1,IL2):-IL2 - OL2 = 0,IL1 - OL1 = 0,IL0 - OL0 = 0,
IL0 >= 1,block3853(OL0,OL1,OL2).
By taking x = 1, we conclude that L0 is uniform.
Example 4.16 Uniform variables and left- or right-open variables are different concepts. For instance, variable L0 is uniform in the component of Example 4.15 but it is not left-open nor right-open. Conversely, variable x is left-open in the component
entry1(ix, iy) :- iy ≥ 0, ox = ix, ix ≥ 3, oy = iy — 1, p(ox, oy) p(ix, iy) :- ox = ix, ix ≤ 0, oy = iy, entry1(ox, oy)
but it is not uniform there.
This proposition justifies the removal of a uniform variable from a strongly- connected component.
Proposition 4.17 (Correctness of a uniform variable removal) Let a vari- able v be uniform in a strongly-connected component Comp. If an entry diverges in Comp−v then it diverges Comp.
Proof. Let x ∈ Z as in Definition 4.14. From an infinite resolution in Comp−v, we can construct an infinite resolution in Comp by simply inserting x into each call at the position of variable v.	 
If we apply this simplification to the CLP program obtained at the end of Subsec- tion 4.5, the number of clauses remains 6 but there are less arguments in predicates. The time needed to prove all the entries terminating goes down from 1.02 to 0.67 seconds (including the time to apply all the simplifications).

Experiments
Figure 3 reports the results of our termination analysis and the effects of our sim- plifications on the time needed to build a proof of termination for the entries of the program. Ackermann is an implementation of the traditional Ackermann function. BubbleSort is an implementation of the bubblesort algorithm on arrays. NQueens is a program that solves the n-queens problem by using a library for binary decision diagrams, included in the analysis. JLex is a lexical analyzers generator. Kitten is a didactic compiler for simple object-oriented programs. Our experiments have been performed on a Linux machine based on a 64 bits dual core AMD Opteron



Fig. 3. The termination analyses of some programs. Times are in seconds. The second line (precision), for each program, reports the number of methods proved to terminate. In the header, we refer to the subsection where the simplification is described.

processor 280 running at 2.4Ghz, with 2 gigabytes of RAM and 1 megabyte of cache, by using Sun Java Development Kit version 1.5 and SICStus Prolog version 3.12.8. For each program, we report the number of methods (without the Java libraries) and the time for building a proof of termination with the original, unlocalized tech- nique of [17] and with the successive application of more and more simplifications, described in this paper (the time for the simplifications is included). The header of each column reports the subsection where the simplification is described. The original technique failed to conclude the analysis after 15 minutes for NQueens, JLex and Kitten. In general, more simplifications means better efficiency. This relation is not always true. For instance, building a proof of termination for JLex takes
228.51 seconds if only the simplification of Subsection 4.1 is applied. If also the simplification of Subsection 4.2 is applied, this time increases to 335.85. We explain this behaviour with the fact that simplifications have a cost. Moreover, when the program is too complex, BinTerm uses timeouts, which makes the construction of the proof faster. However, the precision of the proof decreases with the number of timeouts. Hence, below each program, we report the number of methods proved to terminate. This number increases with the number of simplifications applied to the CLP program, since less timeouts are triggered.

Conclusion
We have presented techniques for simplifying the CLP programs that are auto- matically generated during termination analysis of Java bytecode programs. Those techniques are proved to keep the termination-equivalence of the CLP programs. Their application to some real case of analysis shows that they decrease the time for building a proof by some order of magnitude. Moreover, simplified CLP programs induce less timeouts during the construction of the proof of termination, so that our simplification techniques actually induce more precise termination analyses.
In [2], useless variables are eliminated from CLP programs expressing cost rela- tionships for Java bytecode programs. That technique removes most stack variables. We have verified that almost no stack variable survives after our unfolding of clauses

(Subsection 4.2). Our unfolding can be seen as a CLP view of the simplification done in [2] from a Java bytecode perspective. On the one hand, as in [2] the elimination of variables is done earlier, all related static analyses benefit from this simplification. On the other hand, note that we have a correctness proof for that simplification and that subsequent simplifications are not related to that in [2].

References
A. V. Aho, R. Sethi, and J. D. Ullman. Compilers, Principles Techniques and Tools. Addison Wesley Publishing Company, 1986.
E. Albert, P. Arenas, S. Genaim, G. Puebla, and D. Zanardini. Removing Useless Variables in Cost Analysis of Java Bytecode. In R. L. Wainwright and H. Haddad, editors, Proc. of the 2008 ACM Symposium on Applied Computing (SAC’08), pages 368–375, Fortaleza, Ceara, Brazil, March 2008. ACM.
A. M. Ben-Amram and M. Codish. A SAT-Based Approach to Size Change Termination with Global Ranking Functions. In C. R. Ramakrishnan and J. Rehof, editors, Proc. of the 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS’08), volume 4963 of Lecture Notes in Computer Science, pages 218–232, Budapest, Hungary, 2008. Springer.
J. Berdine, A. Chawdhary, B. Cook, D. Distefano, and P. W. O’Hearn. Variance Analyses from Invariance Analyses. In M. Hofmann and M. Felleisen, editors, Proc. of the 34th ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages (POPL’07), pages 211–224, Nice, France, January 2007.
J. Berdine, B. Cook, D. Distefano, and P. W. O’Hearn. Automatic Termination Proofs for Programs with Shape-Shifting Heaps. In T. Ball and R. B. Jones, editors, Proc. of the 18th International Conference on Computer Aided Verification (CAV’06), volume 4144 of Lecture Notes in Computer Science, pages 386–400, Seattle, WA, USA, August 2006. Springer.
A. Bouajjani, M. Bozga, P. Habermehl, R. Iosif, P. Moro, and T. Vojnar. Programs with Lists Are Counter Automata. In T. Ball and R. B. Jones, editors, Proc. of the 18th International Conference on Computer Aided Verification (CAV’06), volume 4144 of Lecture Notes in Computer Science, pages 517–531, Seattle, WA, USA, August 2006. Springer.
M. Codish. Proving Termination with (Boolean) Satisfaction. In A. King, editor, Proc. of the 17th International Symposium on Logic-Based Program Synthesis and Transformation (LOPSTR’07), volume 4915 of Lecture Notes in Computer Science, pages 1–7, Kongens Lyngby, Denmark, 2007. Springer.
M. Codish and C. Taboch. A Semantics Basis for Termination Analysis of Logic Programs. Journal of Logic Programming, 41(1):103–123, 1999.
B. Cook, A. Podelski, and A. Rybalchenko. Terminator: Beyond Safety. In T. Ball and R. B. Jones, editors, Proc. of the 18th International Conference on Computer Aided Verification (CAV’06), volume 4144 of Lecture Notes in Computer Science, pages 415–418, Seattle, WA, USA, August 2006. Springer.
S. Genaim and M. Codish. Inferring Termination Conditions for Logic Programs using Backwards Analysis. Theory and Practice of Logic Programming (TPLP), 5(1-2):75–91, 2005.
J. Giesl, P. Schneider-Kamp, and R. Thiemann. Automatic Termination Proofs in the Dependency Pair Framework. In U. Furbach and N. Shankar, editors, 3th International Joint Conference on Automated Reasoning (IJCAR’06), volume 4130 of Lecture Notes in Computer Science, pages 281–286, Seattle, WA, USA, August 2006. Springer.
T. Lindholm and F. Yellin. The JavaTM Virtual Machine Specification. Addison-Wesley, second edition, 1999.
A. Loginov, T. W. Reps, and M. Sagiv. Refinement-Based Verification for Possibly-Cyclic Lists. In
T. W. Reps, M. Sagiv, and J. Bauer, editors, Proc. of Theory and Practice of Program Analysis and Compilation, Essays Dedicated to Reinhard Wilhelm on the Occasion of His 60th Birthday, volume 4444 of Lecture Notes in Computer Science, pages 247–272. Springer, 2006.
P. Manolios and D. Vroon. Termination Analysis with Calling Context Graphs. In T. Ball and R. B. Jones, editors, Proc. of the 18th International Conference on Computer Aided Verification (CAV’06), volume 4144 of Lecture Notes in Computer Science, pages 401–414, Seattle, WA, USA, August 2006. Springer.
A. Podelski and A. Rybalchenko. Transition Predicate Abstraction and Fair Termination. ACM Transactions on Programming Languages and Systems (TOPLAS), 29(3), May 2007.

F. Spoto, P. M. Hill, and E´. Payet. Path-Length Analysis for Object-Oriented Programs. In First International Workshop on Emerging Applications of Abstract Interpretation (EAAI’06), Vienna, Austria, March 2006. Available at the web address http://profs.sci.univr.it/∼spoto/papers.html.

F. Spoto, F. Mesnard, and
E´. Payet.	A Termination Analyser for Java Bytecode Based on

Path-Length.	Submitted for publication in September 2007. Available at the web address
http://profs.sci.univr.it/∼spoto/papers.html.
A. Turing. On Computable Numbers, with an Application to the Entscheidungsproblem. London Mathematical Society, 42(2):230–265, 1936.
