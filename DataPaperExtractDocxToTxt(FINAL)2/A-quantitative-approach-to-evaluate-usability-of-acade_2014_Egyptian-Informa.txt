
ORIGINAL ARTICLE
A quantitative approach to evaluate usability of academic websites based on human perception

Sharmistha Roy a, Prasant Kumar Pattnaik a,*, Rajib Mall b

a School of Computer Engineering, KIIT University, Bhubaneswar, India
b Department of Computer Science and Engineering, Indian Institute of Technology (IIT), Kharagpur, India

Received 20 November 2013; revised 20 August 2014; accepted 26 August 2014
Available online 23 September 2014

Abstract In this competitive world, websites are considered to be a key aspect of any organiza- tion’s competitiveness. In addition to visual esthetics, usability of a website is a strong determinant for user’s satisfaction and pleasure. However, lack of appropriate techniques and attributes for measuring usability may constrain the usefulness of a website. To address this issue, we conduct a statistical study to evaluate the usability and accessibility levels of three popular academic websites based on human (user) perception. Two types of usability evaluation techniques were employed in this study. First one is Questionnaire-based evaluation and second one is perfor- mance-based evaluation. Usability assessment was performed by analyzing the results from the observed task success rates, task completion times, post-task satisfaction ratings and feedback. We also investigate the possibility of there being any impact of task completion times on partici- pant’s satisfaction levels. The results of the questionnaire based evaluation were observed to be consistent with the results of performance-based evaluation. Accessibility evaluation was carried out by testing the degree of compliance of the web pages as per WCAG 2.0 guidelines.
© 2014 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University.




Introduction

There has been an increasing focus on Usability engineering in the last few decades. In context to websites, usability can be defined as a quality attribute that describes how easy it is for

* Corresponding author.
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
a user to navigate through the website [1]. A website not only serves as a platform for the educational institution to interact with its stakeholders, but also helps to shape its image [2]. Academic websites are meant to provide information to a wide variety of users. Users of educational website are mostly con- cerned with two major points – finding the information being sought with ease and finding it in a timely fashion [3,4]. This requires achieving high levels of usability. We perform the usability test on the websites of three prominent educational Institutions namely, Institute K, Institute KGP and Institute
D. All the three Institutions provide Under Graduate (B.Tech), Post Graduate (M.Tech) and Post Graduate (Ph.D) courses. All the three Institutions have excellent, highly

1110-8665 © 2014 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. http://dx.doi.org/10.1016/j.eij.2014.08.002

160	S. Roy et al.


educated and renowned professors who provide immense knowledge and brilliant teaching to the students. We found that these information’s are very essential to be reached to the users and the only medium is via websites. Thus we studied the websites of these three Institution’s and tried to find out that which website meets the user’s needs easily and fulfill users satisfaction [5]. The different types of users accessing the academic websites are as follows:

Prospective students and parents looking for general infor- mation about the Institute, details about the various short and long term courses offered by it and admission procedures.
Current students and faculties of the institute who access the site to get up-to-date information regarding the announcements, current happenings and future events like conferences, workshops and seminars.
Researchers who would like to know about the ongoing research activities in the Institute.
Alumni who would like to stay connected with the Institute.

This paper has been organized as follows. Section 2, provides a related work on the Usability. Following which, Section 3 provides the main objective of this paper. Section 4 provides detail report on the different methodologies used for Usability Evaluation of the websites. Analysis and the experimental results are provided in Section 5. Finally, Sec- tion 6, concludes our study.

Literature survey

Mustafa and Al-Zoua’bi [3] in 2008 focused on evaluating nine different Jordanian universities. The evaluation was performed using questionnaires and online automated tools. After a study of related literature, they came up with a list of 23 website usability criteria. The questionnaire was developed and designed based on identified usability criteria, which were divided into 5 categories - Content organization and readabil- ity, Navigation and links, User Interface design, Performance and effectiveness and Educational Information. Each of these categories deals with one usability aspect. Usability index for each category and the overall usability is computed and the usability levels are determined. Automated tools - HTML Toolbox and Webpage Analyzer were used to measure the websites internal attributes-load time, html check errors and browser compatibility issues of the websites, which cannot be perceived by users.
Okene and Enukpere [6] in 2011 conducted a study to eval- uate the websites of Delta State Polytechnics, namely Delta State Polytechnic, Ozoro and Delta State Polytechnic, Ogwashi-Uku. This study was inspired from research conducted by Mustafa and Al-Zoua’bi mentioned above. Two online automated tools - HTML Toolbox and WebPage Analyzer were used along with a questionnaire.
Manzoor and Hussain in their research in 2012 designed a ‘‘Web Usability Evaluation Model’’ and used it for evaluating ten higher educational websites in Asia [7]. The websites were evaluated in two phases. First they conducted a survey among thirty students in three different universities to determine the problems faced by the students while interacting with the websites. After analyzing the survey results, they came up with
the model (WUEM) to evaluate the usability of the websites. WUEM consists of 17 measures which were divided into five feature categories - website contents, webpage design, naviga- tion, page design layout and accessibility.
In an evaluation of Utrecht University website carried out by Lautenbach et al. [8] in 1999, researchers proposed that ‘surveyability’ and ‘findability’ are reliable and effective mea- sures of usability of web pages. Surveyability criteria are the users’ satisfaction with the legibility and comprehensibility of the pages and findability is the users’ ability to find informa- tion on the pages. A study was conducted during which the subjects were observed while they performed one of the four different set of search tasks for information on the university website. Each of these set of search tasks contained three sub-tasks. After completion of each search, subjects answered a questionnaire to measure the user’s ability to survey the pages and find the information. A final score for usability of the web pages was calculated by taking the average of overall score for surveyability and findability.
In a usability study performed by Chaparro in 2007 [9], the university portal website at Wichita State was evaluated before it went ‘live’. Three groups of users – faculties, staff and stu- dents participated in the study. Tasks were identified which were representative of common activities conducted by each user group. These included searches for both general and spe- cific information within the portal. The participants were asked to complete a series of tasks, one at a time. Post each task completion; participants rated its ease/difficulty on a 5- point scale. Performance data – success, task completion time and subjective data-perceived task difficulty, satisfaction were gathered for each participant. Post completion of all tasks, participants completed a satisfaction survey and was inter- viewed for overall comments.
In 2007 Kostaras and Xenos [10] employed Heuristic evaluation for usability assessment of Hellenic Open Univer- sity website. The evaluation was conducted in two phases by usability experts. In the first phase, the evaluators were encour- aged to navigate through the application to get a feel of the system. In the second phase, evaluators validated the implementation of each Heuristic rule derived by Nielsen. At the end all the evaluators submitted their individual reports describing their finding which included the rule violations that were detected by them. The detected violations for each heuris- tic rule are presented and discussed in this paper.
Daher and Elkabani in their research performed a qualita- tive study on usability of web portals in six Lebanese universi- ties [11]. Further they performed a usability study on Beirut Arab University (BAU) web portal. During the first part of the study, the researchers distributed questionnaires among students of six Lebanese Universities to gain an overview of the usability problems encountered. The questionnaire study measured nine common services on the web portals of the uni- versities. The researchers performed a comparative study of the usability of the common services available on the university web portals based on the results of the questionnaire. In the second part of the study, both qualitative and quantitative evaluation of usability of BAU web portal was performed. The students, faculty members and employees at BAU partic- ipated in the study. The researchers performed qualitative study by distributing a questionnaire among the participants. As part of the quantitative study, participants were asked to perform specific tasks while being videotaped, to gather

Quantitative approach to evaluate usability of academic websites	161


performance data including the time on task, webpage changes and the mouse clicks. After completing the tasks and filling the post-task questionnaire, the Six Sigma method was used to cal- culate the effectiveness, efficiency and satisfaction usability metrics. These usability metrics were then standardized to cal- culate the SUM usability metric to summarize the usability of the overall web portal services.
In 2011 Mentes and Turan [2], in their research work used WAMMI (Website Analysis and Measurement Inventory) to measure the usability of the Namık Kemal University (NKU) web site which is based on a questionnaire filled by vis- itors of a website, and gives a measure of how useful and easy it is to use the visitors found the site. Their study also focused on whether participants’ usability perceptions of NKU web sites have significant differences based on gender, age, internet experience and position at the university.
In 2013, Roy and Pattnaik [12], also studied different usability evaluation techniques and testing methods which can be used for usability evaluation of websites. In this paper, the author has focused on two new usability attributes namely physically disabled person and device independent which add flavors to the product orientation.
Objectives of the study

The main objective of this paper is to evaluate the software usability and accessibility of websites of the following three educational institutions:

Institute KGP.
Institute K.
Institute D.

The specific objectives are:

To analyze the websites usability in scales of ‘Attractive- ness’, ‘Controllability’, ‘Efficiency’, ‘Helpfulness’, ‘Learna- bility’, ‘Information Sharing’, ‘Multiple Language Support’, ‘Navigation’, etc.
To get participant’s subjective opinion regarding the best aspects of the website and whether participants felt any- thing was missing from the website.
To check whether the users were able to complete the task successfully.
To perform basic accessibility compliance checks of the websites.


Methodology of usability evaluation

Two types of usability evaluations approaches were considered for performing usability evaluation of the three websites under study.

Questionnaire based evaluation. Performance based evaluation.
Questionnaire based evaluation

Questionnaire-based evaluation involved responding to a stan- dardized questionnaire-WAMMI (Website Analysis and Mea-

surement Inventory) to assess the perceived usability of the three websites. It is widely accepted and recommended as a standard usability evaluation tool for evaluating usefulness of websites. It’s a scientifically proven psychometric questionnaire.
WAMMI is composed of 20 statements to capture the par- ticipant’s view points on a website’s ease-of-use. It also consists of some open text questions to gather participant’s opinion regarding the best aspects of the website and whether partici- pants felt anything was missing from the website. The scores along each usability scale is composed of weighted contribu- tion from the ratings received from the statements. It calculates an average of the five usability sub-scales and provides a Glo- bal Usability Score (GUS) to the website.
WAMMI not only rates the websites as per the feedback provided by participants of the survey, but also benchmarks the website. It compares the computed scores with similar data collected from some other web sites and predicts how the website under study performs as compared to scores for other similar websites in WAMMI database [13].
The feedback of the participants get submitted and stored in WAMMI database. After the end of the survey, WAMMI shared reports corresponding to each institute detailing the usability analysis performed based on the feedback received from the participants.

Performance based evaluation

In Performance-based evaluation, Remote Usability testing method was chosen to perform usability tests on three websites. This usability evaluation approach also involved par- ticipants answering post-task questionnaire – After-Scenario Questionnaire (ASQ) and post-study questionnaire – Website Analysis and Measurement Inventory (WAMMI).
A pilot study was conducted during the initial stages of our research during which we interacted with a group of 15 under- graduate students to identify some of the frequent tasks that are ordinarily performed on academic websites by students. Based on their inputs, a task list comprising of five tasks was created that the participants will have to perform during the test. We mainly prepare different sets of questionnaires and conducted the test for only a particular group of users i.e. pro- spective students and parents who are looking for general information and other necessary details about the institute. These tasks were representative of tasks that students will ordi- narily perform on academic websites during their day-to-day interaction.
All the participants had to perform these five tasks on one of the 3 websites under study. Each of these tasks involved retrieving some information from the website. The participants recorded their usability sessions using Windows Media Encoder software that captures screen activity and shared the generated videos for our performance analysis.
Further, post completion of each task, the participants filled ASQ (After Scenario Questionnaire). A 7-point rating scale ranging from ‘‘strongly disagree’’ to ‘‘strongly agree’’ is associated with each of the 3 questions in the questionnaire. It is meant to measure user satisfaction after completion of each task. After finishing all the tasks, participants also under- took the Questionnaire-based evaluation which involved answering the post-study questionnaire, WAMMI.

162	S. Roy et al.


Usability metrics

The metrics considered for performing usability evaluation in this study include measurement of performance (Task success, Task completion time, Number of clicks) as well as subjective (satisfaction) metrics [14].

Performance metrics
The performance metrics used in this study includes:

Task success

It was used to measure whether the users were able to complete the tasks successfully. Since all the tasks that the par- ticipants had to perform on the websites involved finding answer to some questions and providing a written response in a document, it was possible to measure task success.

Task completion time

A task was considered to have completed as soon as the participant finds the information from the website and pro- vides the same in his response. The time elapsed between the start and end of task was expressed in seconds. It is a measure of efficiency of the system.

Number of clicks

Efficiency was also assessed based on the amount of clicks that the users had to perform on the website to complete a task. The number of clicks was counted during the time between the start and end of each task.

Self-reported metrics
Self-reported data is being captured at the end of each task (post task rating) using After Scenario Questionnaire (ASQ) and at the end of the usability session (post-study rating) using WAMMI. The ratings were provided online for both ASQ as well as WAMMI.

Test conduction

Email invitations for participation in this study were sent across to a group of people consisting of students (under-grad- uates) currently pursuing engineering degree and some engi- neering graduates who are potential post-graduate students. The participants were true representative of the target users of the systems under study.
Some participants agreed to take part in the Performance based evaluation. Others provided their consent to participate in the WAMMI questionnaire based evaluation. The partici- pants in the study were randomly divided into three groups (one corresponding to each website under study) and a between-subjects study was used to compare results for differ- ent participants.
Written instructions detailing the steps needed for partici- pating in the usability test and filling the questionnaires (WAMMI, ASQ) were shared over e-mail with the participants.
Documents detailing the list of tasks and the instructions for participating in this study were shared with the participants of Questionnaire-based evaluation. Participants had to per- form the set of tasks on the website under study and then fill the post-study questionnaire, WAMMI.
An informal discussion was also conducted with all the par- ticipants to discuss the objectives of this study, the test being performed and the guidelines for participating in this study. They were assured about this being an evaluation of the ‘web- sites under study’ and not of the ‘participants’.

Accessibility evaluation

Automated testing was performed to determine if the three Institutions websites met the accessibility guidelines put forth by WCAG 2.0 [15]. W3C WAI has listed a set of tools that can be used for accessibility evaluation of websites [16].
Although these evaluation tools help in broadly determin- ing if a web page conforms to WCAG 2.0, passing the tool based checks does not necessarily guarantee accessibility. They should always be complemented with Manual accessibility testing which is a relatively more accurate method for deter- mining accessibility and helps in finding accessibility problems which are not found by testing tools by involving users with disabilities.
Since the purpose of the test was just to perform basic accessibility compliance checks, hence the evaluation was per- formed only using a testing tool. The tool chosen for this study was ‘A-Checker’ since it is an open source web accessibility evaluation tool which can be used to check compliance to WCAG 2.0. It tests single HTML pages for conformance to various accessibility guidelines.
A-Checker lists the identified problems in the following sections:

Known problems: Problems which the tool knows with cer- tainty are accessibility barriers.
Likely problems: Problems which the tool has identified as probably barriers but require human intervention to be sure.
Potential problems: Problems that the tool cannot detect and require human decision.

The Home page as well as two random internal web pages was to be tested for accessibility compliance on each of these three websites. The web pages are to be checked for Web Con- tent Accessibility Guidelines (WCAG) 2.0 compliance at Level A (minimum conformance). These web pages were tested only to see if there were any ‘Known problems’ that were identified by the tool.

Findings and analysis

Usability testing

68 People participated in this usability study. All of them par- ticipated in the questionnaire-based evaluation. Out of these, 30 participants agreed to take part in the Performance based evaluation of the three websites.

Quantitative approach to evaluate usability of academic websites	163

Performance based evaluation
The participants were willing to record their usability sessions using Windows Media Encoder software and share the generated videos for performance analysis. These participants were divided into 3 groups of 10 participants each so as to ensure equal number of participants perform the usability test on each of the 3 websites under study.
The recorded videos were individually reviewed to collect the performance metrics - Task success, Task completion time, number of clicks. Further, the response provided by the partic- ipants to ASQ after completion of each task was analyzed to gauge the post-task satisfaction levels.


Task success rate

The videos were analyzed for individual tasks to measure whether the users were able to complete the tasks successfully. Participants were either assigned a value of ‘success’ or ‘failure’ based on whether they were able to complete the individual tasks. ‘Failure’ was again sub-categorized into two categories – ‘Incorrect’ and ‘Not Found’, depending on whether the task failure was due to inability in finding information on the website or if it was due to providing an incorrect response.
Data was collected pertaining to all the participants for the five tasks across the 3 websites. To be able to quantify these scores and determine the Task Success rate, these scores were converted to numerical values. 1’s were assigned every time a participant succeeded in completing a task and 0’s were assigned for failure in completing the tasks.
Task success rates were calculated for each of these websites by averaging the 1’s (success) and 0’s (failures) in the above tables for each task.

In Institute K’s website only 1 out of 10 participants performing the tasks was able to successfully complete the TASK 2. None of the tasks were performed successfully by all the participants.
In Institute KGP’s website, apart from a single case of a participant not being able to successfully complete TASK 3, all the participants were able to complete the tasks successfully.
In website of Institute D, only two participants were able to successfully complete TASK 2. All the participants were able to successfully complete rest of the tasks in Institute D’s website.

Institute KGP’s website was observed to have a better overall Task Success Rate from among the three websites.
The average task success rates as observed across the 3 Institutions is shown in Figure 1 below.

Task completion time

The data pertaining to the task completion time observed for each of the tasks across the three websites under study was measured. Time data was analyzed for tasks in which participants were successful as well as ones in which they were unsuccessful so as to obtain a relatively more accurate time data. The average amount of time taken in completion of the individual tasks was analyzed across the three websites. 95% Confidence intervals was calculated on time data and reported
Figure 1	Average Task Success Rates across 3 Institutions.

to show the potential variability across the participants as shown in Figure 2 below.
The findings from the analysis of Task completion times observed across the three websites include:

In Institute D’s website, the average task completion time was observed to be highest in case of TASK 2. Further this task was observed to be taking the highest amount of time in Institute D’s website from among the three websites.
Performing TASK 5 took the longest time among all the tasks in Institute K’s website. Further this task was also observed to be taking the highest amount of time in Institute K’s website from among the three websites.

The average Task completion time was observed to be lowest in Institute KGP’s website in case of three of the tasks (TASK 2, TASK 4 and TASK 5). In two of the tasks (TASK 1 and TASK 3), the completion Time was observed to be lowest in website of Institute D.
Independent T-Tests were conducted to test the statistical significance of difference between the means among the three Institutes websites in terms of their Task completion time for each of the performed five tasks. It was done using MSTATC statistical tool developed by Dr. Russel D. Freed of Michigan State University, USA. The result of the analysis is presented below for individual tasks.

The difference in Task Completion Time observed among the three websites in case of TASK 1 and 3 was observed to be not significantly different.














Figure 2	Average Task Completion Time across 3 Institutions.

164	S. Roy et al.


In the case of TASK 2,
Institute KGP’s website was observed to have lower Task Completion Time as compared to Institute K with the difference being statistically significant at 5%.
Institute KGP’s website was also seen to have better Task Completion Time compared to Institute D with the difference being observed at both 5% and 1% level of significance.
No significant difference was observed in Task Comple- tion Times in case of Institute K and Institute D’s websites.

With respect to TASK 4,
Institute KGP website’s Task Completion Time was seen to be significantly lower than that of Institute K’s website, observed at 5% level of significance.
The difference in Task Completion Time was not signif- icant between websites of Institute KGP and Institute D as well as between Institute K and D.

In case of TASK 5,
Institute KGP’s website was observed to have a better Task completion time as compared to Institute K, observed at 5% level of significance.
Also, website of Institute D was observed to have signif- icantly lower Task completion time compared to Insti- tute K.

Number of clicks

This metric provides a measure of the amount of effort required for completing the tasks. It also analyze the naviga- tional prospect from one page to another required for informa- tion gathering and successful task completion. It was calculated only for successful tasks and was not considered in case of task completion failure, as the number of clicks incase of task failure does not fully represent the effort required for completing the task as shown in Figure 3 below.

With respect to TASK 1, the number of clicks required for performing the task was observed to be lowest in case of Institute K (1) with the highest number of clicks being per- formed in case of Institute KGP (4).
The average number of clicks performed for completing TASK 2 was observed to be 5 in case of Institute D. Highest number of clicks is observed in this task for Institute K (12) and Institute KGP (8).
The average number of clicks made for performing TASK 3 was observed to be lowest in case of Institute D (4), fol- lowed by Institute KGP (5) and then Institute K (6).
The average number of clicks made for performing TASK 4 was observed to be same across the three websites.
With respect to TASK 5, highest number of clicks was reg- istered in case of Institute KGP website (6) with equal num- ber of clicks being made in Institute D and Institute K websites (5).
Post task satisfaction rating.

Post-Task Satisfaction rating was provided using After Sce- nario Questionnaire (ASQ). Participants filled ASQ post com- pletion of each task. The ASQ contains 3 questions, each with a 7-point rating scale (1 = Strongly disagree 7 = Strongly agree) with higher values indicating greater satisfaction associ- ated with the ease of use and the amount of time taken to com- plete the task.
To be able to quantify the scores, the arithmetic mean of the response to the three questions for each task was calcu- lated. Further, overall ASQ score for the websites were calcu- lated by averaging the scores obtained for each of the five tasks for all the participants.
The overall ASQ score calculated for the three websites were Institute KGP (6.2), Institute K (4.4) and Institute D (5.1) with Institute KGP website scoring the highest levels on participant’s satisfaction levels, followed by Institute D’s website and Institute K’s website as shown in Figure 4 below.
The task specific ASQ scores observed across the 3 Institu- tion websites are presented graphically below in Figure 5.
One-way ANOVA was used to statistically compare the means of task-specific overall ASQ scores obtained across the three Institutes’ web sites to study whether the task-specific satisfaction levels vary significantly across the three Institutes’ websites.
Statistical analysis revealed that

In case of TASK 1 and TASK 2, ASQ scores varied signif- icantly between websites of Institute KGP and Institute K. The difference was also significant between websites of Institute KGP and Institute D. Institute KGP registered better ASQ score in both the cases.
In case of TASK 3 and TASK 4, the ASQ scores were seen to vary significantly across the 3 institutes with Institute KGP recording the highest scores and Institute Ka record- ing the lowest scores.
















Figure 3	Number of mouse clicks across 3 Institute’s.	Figure 4	Overall ASQ score of 3 Institution’s.

Quantitative approach to evaluate usability of academic websites	165













Figure 5	Task specific ASQ scores observed across 3 Institution’s.


With respect to TASK 5, a significant difference in ASQ score was observed between Institute KGP and Institute K websites. Similarly the difference in scores was significant between Institute K and Institute D websites.


Impact of task completion time on satisfaction levels
Correlation and regression analysis was performed to statisti- cally study the impact of Task completion time on partici- pant’s satisfaction levels (Overall ASQ scores) for the three Institutions. The mutual association of Task Completion Time with Satisfaction levels (ASQ score) was measured by correla- tion coefficient ‘r’. The adjusted coefficient of determination (Adjusted R2) was used to determine the extent of variability of satisfaction levels caused by its relationship to Task Com- pletion Time.
The findings from the Correlation and Regression analysis performed to study the impact of Task completion time on participant’s satisfaction levels in case of the three Institutions are:

In case of TASK 1 and TASK 4, a statistically significant negative correlation was observed between Task completion time and the satisfaction levels in case of Institute KGP website only. It implied that the decrease in time taken by the participants in completing the tasks enhanced the satis- faction levels in Institute KGP website.
No significant correlation was found in case of Institute K and Institute D websites for the specific tasks.
In case of TASK 2, TASK 3 and TASK 5, the satisfaction levels across the three websites were seen to increase when time taken for task completion was less. The contribution of Task completion time to satisfaction levels was seen to be highest in case of Institute K among the 3 websites in TASK 3 (80.20%) and TASK 5 (87.90%). In case of TASK
2, contribution of Task Completion Time to Satisfaction levels was seen to be high in case of Institute D (83.4%). All these observations were made at both 5% and 1% level of significance.


Questionnaire based evaluation
All the participants took part in a Questionnaire-based evalu- ation involving responding to a standardized questionnaire - WAMMI to assess the perceived usability of the three web- sites. The participants performed the five tasks in the website
Figure 6	WAMMI Score-Institute KGP.

and then provided their feedback regarding the website by answering the WAMMI questionnaire. The analysis of WAM- MI reports of the three Institutes’ websites have been presented below:

Institute KGP’s WAMMI Report

Twenty-one people answered WAMMI questionnaire to provide their feedback regarding Institute KGP’s website. Institute KGP’s website was provided a Global Usability Score (GUS) of 48.48. The graphical report is shown below in Figure 6.
On one of the scales (attractiveness), the website has scored 52.7, which is more than the average score as per the data con- tained in WAMMI database for other evaluated websites. In rest of the scales the scores have been observed to be near about the average score as per WAMMI database.

Institute K’s WAMMI Report

Twenty-four people answered WAMMI questionnaire to provide their feedback regarding Institute K’s website. Insti- tute K’s website was assigned a Global Usability Score (GUS) of 34.71. The graphical report is shown below in Figure 7.
On all the scales the website has scored less than the average score as per the data contained in WAMMI database for other evaluated websites.
















Figure 7	WAMMI Score-Institute K.




166	S. Roy et al.



















Figure 8	WAMMI Score-Institute D.


Institute D’s WAMMI Report

Twenty-three people answered WAMMI questionnaire to provide their feedback regarding Institute D’s website. Insti- tute D’s website scored a Global Usability Score (GUS) of
39.39. The graphical report is shown below in Figure 8.
On all the scales the website scored less than the average as per the data contained in WAMMI database for other evalu- ated websites.

Conclusion and future scope

This study was restricted to users performing 5 tasks on the website which were chosen based on a pilot study conducted early during this research involving some under-graduate stu- dents to identify the tasks that users normally perform while interacting with the academic websites.
The results and analysis of the collected data indicates that the website of Institute KGP was seen to be exhibiting highest level of usability, followed by Institute D’s website. As per the results generated, Institute K’s website was found to be rela- tively less usable among the three websites. Regarding the Multiple Language Support all the three websites share infor- mation in English and national language and also in regional language wherever applicable. Hence, all the three websites satisfies this objective.
Also, the results of the questionnaire-based evaluation (WAMMI) were observed to be consistent with the perfor- mance-based evaluation with Institute KGP’s website being assigned the highest global usability score and Institute K get- ting the lowest score.
The research also included statistical analysis performed to study if any impact of task completion time on partici- pant’s satisfaction levels. The analysis indicates that the correlation between task completion timing and satisfaction levels was seen to vary across individual tasks and websites. In case of three of the tasks, a negative correlation was found between the task completion time and satisfaction levels implying the decrease in time taken by users in per- forming the tasks enhanced their levels of satisfaction. In the remaining two tasks, a statistically significant negative correlation was observed only in case of Institute KGP’s website.
Results from the accessibility evaluation of the websites performed using an automated tool indicated that none of the three institutes were able to meet even the basic accessibil- ity standards.
Within the scope of this study, the usability of Institute KGP’s website was found to be highest among the three web- sites. Outcome of this study may vary if more number of tasks is considered.
In comparison to previous related works, our work addresses certain differences as follows:
We have evaluated the websites in two ways: Questionnaire based evaluation as well as Performance based evaluation. In the previous works the authors have taken either of them. Firstly, to access effective and perceived usability we have col- lected standardized questionnaires from WAMMI, which are scientifically proven and has a reliability data rating of 0.90. For better usability evaluation we have prepared two sets of questionnaires, one is After-Scenario Questionnaire (ASQ) and another one is Post study Questionnaire which results a better Global Usability Score (GUS) of the website. Moreover, it not only rate the website based on the feedback mechanism but also benchmark the website by comparing the score with other websites. Secondly, Performance evaluation is done based on certain metrics which are measured while performing task on the website and is further calculated using regression and correlation analysis. The result also shows an impact of task completion time on participant’s satisfaction levels which is very much essential in order to measure the websites usabil- ity. Further the accessibility of the websites is also evaluated using WCAG tools which also differentiate the present work from the other works.
In future, we will be conducting the same study on all groups of users starting from researchers, alumni, current stu- dents of the institute, etc.

Acknowledgment

We would like to thank Dr. Jurek Kirakowski, Director of Human Factors Research Group, University College Cork, Ireland for granting us permission to use WAMMI free of charge for evaluating web usability of the three Institution websites.

References

Stewart T. Websites-quality and usability. Behav Inform Technol 2012;31(7):645–6.
Mentes SA, Turan AH. Assessing the usability of university websites: an empirical study on Namik Kemal University. Turk Online J Educat Technol 2012;11(3):61–9.
Mustafa SH, Al-Zoua’bi LF. Usability of the academic websites of Jordan’s Universities: an evaluation study. In: International Arab conference on information technology, Tunisia; 2008. p. 1– 9.
Hasan L, Morris A, Probets S. A comparison of usability evaluation methods for evaluating e-commerce websites. Behav Inform Technol 2012;31(7):707–37.
Skov MB, Stage J. Training software developers and designers to conduct usability evaluations. Behav Inform Technol 2012;31(4):425–35.
Okene DE, Enukpere VE. Comparative analysis of the usability of academic websites of Delta State Polytechnics. J Emerg Trends Eng Appl Sci (JETEAS) 2011;2(6):1042–6.

Quantitative approach to evaluate usability of academic websites	167


Manzoor M, Hussain W. A web usability evaluation model for higher education providing Universities of Asia. Sci Technol Dev 2012;31(2):183–92.
Lautenbach MAE, Schegget IS, Schoute AM, Witteman CLM. Evaluating the usability of web pages: a case study; 1999. p. 1–13.
Chaparro BS. Usability evaluation of a university portal website. Usability News 2008;10(2).
Kostaras N, Xenos M. Assessing educational web-site usability using heuristic evaluation rules. In: 11th Panhellenic conference in informatics; 18–20 May, 2007. p. 543–50.
Daher LA, Elkabani I. Usability evaluation of some Lebanese Universities Web Portals. In: Proceedings of the 13th interna- tional Arab conference on information technology ACIT; 10–13 December, 2012. p. 127–34.
Roy S, Pattnaik PK. Some Popular Usability Evaluation Tech- niques for Websites. Advances in Intelligent Systems and Com-

puting, 247. Springer International Publishing Switzerland; 2014.
p. 535–43.
Claridge N, Kirakowski J. WAMMI: website analysis and measurement inventory questionnaire. <http://www.wam- mi.com/>; 2011 [viewed 08.01.12].
Sorum H, Andersen KN, Vatrapu R. Public websites and human– computer interaction: an empirical study of measurement of website quality and user satisfaction. Behav Inform Technol 2012;31(7):697–706.
Vanderheiden G, Caldwell B, Reid LG, Cooper M. (W3C Recommendation). Web Content Accessibility Guidelines (WCAG) 2.0. <http://www.w3.org/TR/WCAG20/>; 2008.
W3C/Web Accessibility Initiative (WAI). <http://www.w3.org/ WAI/RC/tools/complete>; 2012.
