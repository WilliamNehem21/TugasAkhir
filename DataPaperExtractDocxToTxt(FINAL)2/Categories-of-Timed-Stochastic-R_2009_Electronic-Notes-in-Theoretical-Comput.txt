

Electronic Notes in Theoretical Computer Science 249 (2009) 193–217
www.elsevier.com/locate/entcs
Categories of Timed Stochastic Relations
Daniel Brown1 Riccardo Pucella2
College of Computer and Information Science Northeastern University
Boston MA, USA

Abstract
Stochastic behavior—the probabilistic evolution of a system in time—is essential to modeling the complexity of real-world systems. It enables realistic performance modeling, quality-of-service guarantees, and especially simulations for biological systems. Languages like the stochastic pi calculus have emerged as effective tools to describe and reason about systems exhibiting stochastic behavior. These languages essentially denote continuous-time stochastic processes, obtained through an operational semantics in a probabilistic transition system. In this paper we seek a more descriptive foundation for the semantics of stochastic behavior using categories and monads. We model a first-order imperative language with stochastic delay by identifying probabilistic choice and delay as separate effects, modeling each with a monad, and combining the monads to build a model for the stochastic language.
Keywords: probability, stochastic behavior, category theory, monads, partial additivity


Introduction
Stochastic temporal behavior is crucial for modeling real-world systems with non- functional requirements like quality-of-service guarantees [11]. Such requirements often take the form of soft real-time constraints such as “do a before time t with probability 0.99”. Multimedia applications and collaborative virtual environments are well-known examples of systems exhibiting such characteristics.
To model and program systems with soft constraints, we need languages to ex- press probability distributions over the delays experienced during the evolution of the system. PEPA [26] and the stochastic pi calculus [43] are two examples of languages that express this kind of stochastic temporal behavior. The semantics of these languages is operational, given in terms of a labelled probabilistic transi- tion system. The transition systems themselves denote continuous-time stochastic processes, often continuous-time Markov stochastic processes.

1 Email: dbrown@ccs.neu.edu
2 Email: riccardo@ccs.neu.edu

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.07.091

An operational semantics in terms of probabilistic transition systems, however, does not directly describe stochastic temporal behavior—it delegates the task to the metatheory. In this paper, we initiate a study of the foundation of stochastic temporal behavior, working towards a natural denotational semantics for stochastic languages. In particular, we are interested in relating the kind of probabilistic be- havior found in languages with probabilistic choice with the kind of stochastic tem- poral behavior found in the stochastic pi calculus. Following Giry’s [21] approach to the categorical foundation of discrete- and continuous-time Markov processes, we explore a categorical model of stochastic temporal behavior. This approach has two immediate advantages. First, the resulting model is sufficiently abstract to be generally applicable. Second, we obtain a principled derivation of semantic models for stochastic languages.
To ground our intuitions, we study stochastic temporal behavior in the context of a simple language of while loops [24,52]. Languages of while loops are relatively simple, yet structured and Turing-complete. Moreover, being first-order, their deno- tational semantics requires no heavy-duty machinery. In §2, we review the standard categorical semantics for such languages, taking advantage of Moggi’s insight that monads can be used to lift the semantics of a pure language to an extension with effects [37,38]. We illustrate the approach with two different effects: iteration and probabilistic choice.
Stochastic temporal behavior ultimately amounts to adding delay to computa- tions. In §3, we present an abstract approach for adding delay to the categorical semantics of our imperative language by introducing a monad to express timed computations. We examine how this monad interacts with monads capturing other effects in the language. In particular, we investigate conditions under which adding timed computations to a semantic model that correctly handles iteration yields an extended semantic model that also correctly handles iteration.
In §4, we instantiate our abstract approach to derive semantic models for a lan- guage of while loops extended with a deterministic delay operator and for a language of while loops extended with a probabilistic delay operator. In §5, we instantiate
our abstract approach to derive semantic models for a probabilistic language of while
loops [32] extended with a deterministic delay operation. We call the resulting se- mantic models categories of M-timed stochastic relations TSRelM, extending the category SRel of stochastic relations commonly used to give semantics to proba- bilistic languages of while loops. In these categories, we draw a relationship between probabilistic choice and stochastic temporal behavior by showing that both are in fact derivable from a primitive that lets us sample probability distributions.
We review related work in §6 and conclude in §7. Due to space restrictions, proofs of our technical results are only sketched where useful, and full proofs have been relegated to the full version of the paper [12].

Categories for Imperative Languages with Effects
Standard denotational semantics for languages of while loops are state-transformer semantics: the meaning of a statement is a partial function from states to states, where states are assignments of values to variables. Partial functions are necessary because loops need not terminate.
This sort of semantics can be given abstractly in any category with the right structure. We review such a categorical semantics below. The material in this section is well known and we claim no novelty, but our presentation of it may be unfamiliar: because our approach to adding delay in §3–5 relies on first separating iteration from the model, we treat nontermination as an effect and model it with a monad in the style of Moggi [37,38].
We define a family of typed imperative sequential languages ISLext , where ext
represents some language extensions that carry effects. The base language in this family, ISL0, is an imperative sequential language without iteration.
Syntax of ISL0:
τ ::=	type
bool
... 
E ::=	expression
... 
S ::=	statement
skip	skip
S1; S2	sequencing
let v : τ = E in S	allocation
v := E	assignment
if E then S1 else S2	conditional
Our focus is on statements and their semantics, so we elide the details of the ex- pression language E and types other than bool; we assume only that expressions are effect free. In examples we freely use expressions that include variable refer- ence, Boolean operations, and rational arithmetic. We assume a countable set V of variables, ranged over by u, v, w.
We use a standard type system (e.g., [42,24]) to simplify our semantics. Judge- ment Γ ▶ E : τ states that the expression E has type τ in typing context Γ. Judgement Γ ▶ S states that the statement S is well formed in typing context Γ. A typing context Γ is a sequence of pairs v : τ .
Typing Rules of ISL0:



Γ ▶ skip
Γ ▶ S1	Γ ▶ S2


Γ ▶ S1; S2
Γ ▶ E : τ	v : τ, Γ ▶ S


Γ ▶ let v : τ = E in S

Γ,v : τ, Γ' ▶ E : τ	Γ ▶ E : bool	Γ ▶ S1	Γ ▶ S2

Γ,v : τ, Γ' ▶ v := E (v ∈/ Γ)
Γ ▶ if E then S1
else S2



The only subtlety in the typing rules is the side condition for assignment: v must not occur in Γ, preventing non-unique types, but may occur in Γ', permitting bound variables to be shadowed by inner let bindings. In other words, typing contexts “grow on the left”.
The standard state-transformer semantics for ISL0 can be given in any distribu- tive category, that is, a category with finite products (for state spaces), finite co- products (for Booleans), and distributivity of products over coproducts given by an

inverse to the canonical map

X×Y +X×Z
[1 × ι1, 1 × ι)2]

X×(Y +Z)
(for conditionals [20]).

Following Moggi, we model effectful extensions of ISL0 in the Kleisli category of a suitable monad, allowing the semantics for the pure language ISL0 to remain uni- form as the extensions vary. We thus parameterize the semantics for ISL0 over an
arbitrary monad.

Let C be a distributive category and T : C → C a monad with unit	ηT
1
and
T

multiplication	μT
TT 
. We define the monadic semantics of ISL0 via a pair of maps:
T

 −)Γ assigns to every well-formed statement a Kleisli arrow on states and − : τ )Γ
assigns to every well-typed expression a pure arrow from states to values:
 S)Γ :  Γ) → T Γ)	(if Γ ▶ S)
 E : τ )Γ :  Γ) →  τ )	(if Γ ▶ E : τ )

Since expressions have no effects, their semantics is given by arrows in the base category C. Types denote objects; in particular,
 bool)   1+1 

the object representing the two truth values. The state object Γ) denoted by the typing context Γ is the product of the objects denoted by the types in the context:
 v1 : τ1,..., vn : τn)    τ1) ×· · · × τn)


We write
XT

f)T
YT


or X	TY 

for a Kleisli arrow in CT with underlying arrow
T

f
X	TY 
in C, and we abbreviate components of natural transformations	ϕX
FX	GX

as	ϕ
FX	GX
when the object is clear from context. We abbreviate ηT and μT as η


and μ when the monad T is clear from context.
Monadic Semantics of ISL0:

  skip)Γ 

      Γ)     )

=	)1
 Γ)T


T Γ)  T

 Γ)T

  S1; S2)Γ 
=


 Γ)
 S1)Γ
T Γ)
 S1)Γ
T S2)Γ	 μ
TT Γ)
 S2)Γ

T Γ)  T


 Γ)T
	)T

 Γ)T
	)T

 Γ)T



 let v : τ = E in S)Γ 


 Γ)
⟨ E : τ )Γ, 1)⟩


 τ )× Γ)
 S)v:τ,Γ


T ( τ )× Γ))
 T π)2


T Γ)

Γ,v:τ,Γ'
⟨π , E : τ )Γ,v:τ,Γ' ,π ⟩	η

 v := E)
 Γ)× τ )× Γ')	1
3) Γ)× τ )× Γ')  )
T ( Γ)× τ )× Γ'))

 if E then S1
else S2)Γ 
 Γ)
 E : bool)Γ?
 Γ)+ Γ)
[ S1)Γ, S2)Γ]
T Γ)

Identities and composition in the Kleisli category model skip and sequencing. Stan- dard product constructions in the base category model let and assignment. Standard coproduct constructions along with guards [20] model conditionals, where guards

map every predicate
X
p	to the arrow
1+1

p?
X	X+X
⟨1, p⟩
X
X×(1+1)
[1 × ι1, 1 × ι2]−1
X×1+X×1
 π1 + π)1
X+X


The inverse [1 × ι1, 1 × ι2]−1 exists because C is distributive. The standard deno- tational model of ISL0 can be recovered using the category Set with the identity monad.
We illustrate the use of a monad T with our first extension, iteration, and its associated effect, nontermination.
Iteration Extension: while
Syntax:	Typing Rules:
S ::=  ··· | while E do S	Γ ▶ E : bool   Γ ▶ S
Γ ▶ while E do S

ISLwhile is the standard language of while loops, often called IMP [24,52]. To model while, we need a monad that imposes enough structure on its Kleisli category to support iteration. Following Manes and Arbib [35], we take this to mean that the Kleisli category should be partially additive.
Intuitively, a loop is the limit of the finite unrollings of its body. Partial ad- ditivity models this limiting process through an infinite summation operator on hom-sets. Partial additivity is the combination of a few simple structures (see [35]) but we present it as one large, aggregate definition that suffices for our purposes. The subtleties of this definition are less relevant to our goals than how it enables us to interpret loops, which we give below.
Definition 2.1 A category D is partially additive if and only if
D has countable coproducts.
Every hom-set D(X, Y ) is a partially additive monoid. That is, it has a partial function	from countable subsets of D(X, Y ) to D(X, Y )—we say the family {fi}i∈I is summable if  {fi}i∈I is defined—subject to:
Partition-associativity axiom: Given a countable family {fi}i∈I and a count-

able partition {Ij}j∈J of its indexing set,
Σ {fi}i∈I = Σ Σ {fi}i∈Ij ,j∈J
In particular, the sum on the left is defined if and only if all of the sums on the right are defined.
Unary sum axiom: Singleton families are summable with	{f} = f .
Limit axiom: A countable family is summable if every finite sub-family is summable.
We abbreviate	{fi}i∈I variously as	i∈I fi,	I fi, or	fi, depending on context.

Composition distributes over sum: Given  X
f)i

Y ,i∈I
summable,

{g; fi}i∈I is summable and g; (Σ fi) = Σ g; fi, for all
)g	;

{fi; h}i∈I is summable and (Σ
fi); h =
W	X
fi; h, for all	 h	.
Y	Z

Compatible sum axiom: A countable family X
f)i
Y ,i∈I
is summable if some

f
X	I·Y
makes all diagrams


X	f) I · Y ρi
v
Y


commute, where I · Y =  I Y and the family 



I·Y

ρ)i
Y ,i∈I

is an instance of

the more general family of quasi-projections  ‘ Xi
ρ)i
Xi,


i∈I
defined by

ρi   [0X1,Xi , . .., 0Xi−1,Xi , 1Xi , 0Xi+1,Xi , . . . ]

where the arrows 0X,Y are zeroes for composition and units for sum, which exist as 0X,Y = ΣX,Y ∅ in any category satisfying (2) and (3).

Untying axiom: If the two arrows
X
and	g
Y	X
are summable, then so
Y

are
X
f ; )ι1

Y +Y
and
X
g; )ι2
.
Y +Y

Two familiar examples of partially additive categories are the category Par of sets with partial functions and the category CPO of complete partial orders and con- tinuous functions. In Par, a family of partial functions is summable if and only if the functions are defined on mutually disjoint subsets of the domain, and the sum is the union of their graphs. Partial additivity in CPO is even more familiar: a family is summable if and only if it is a directed subset of the function space, and the sum is the least upper bound.
The key consequence of partial additivity in our setting is that every arrow

f
X	X+Y
decomposes into arrows
X
f)1
and
X	X
f)2
Y such that f = Σ {f1; ι1, f2; ι2},

and the iterate of f

f†   Σ fi; f2

i<ω
is defined, where g0 = 1 and gi+1 = g; gi. The morphism f† satisfies the equation
f† = Σ{f1; f†, f2}

which can be seen as the defining equation of the while loop.
Given a monad T : C → C whose Kleisli category CT is partially additive, we model ISLwhile by extending the ISL0 semantics with an interpretation for loops:
Monadic Semantics of while:

 while E do S)Γ



 Γ)



 Γ)
 E : bool)Γ?





 Γ)+ Γ)
 S)Γ +)η
†
T Γ)+T Γ) )




T Γ)


A nice property of this semantics is that it does not rely on any particular monad but is defined abstractly over the class of monads that yield partially additive Kleisli categories. So when we consider additional effects and monads to model them, we have a canonical interpretation for while as long as we have partial additivity. The standard categorical model of ISLwhile can be recovered with Set as the base category and the partiality monad −⊥ = − + 1. The resulting Kleisli category Set−⊥ is isomorphic to Par, which is partially additive as we already noted.
Probabilistic extensions of ISL0 form the basis of our study. Throughout the paper we assume the reader is familiar with basic probability and measure the- ory [9,5,19,29]. The simplest way to model probabilistic behavior is to use a prob- abilistic choice operator:
Probabilistic Choice Extension: +p
Syntax:	Typing Rules:
S ::=  ··· | S1 +p S2	Γ ▶ S1   Γ ▶ S2
Γ ▶ S1 +p S2

The statement S1 +p S2 reads “execute S1 with probability 1 − p and S2 with probability p”. This operator is nicely modeled with Markov kernels: functions that map states to probability distributions over states. Markov kernels are the Kleisli arrows for Giry’s [21] probability monad over measurable spaces but, since they fail to be partially additive, the monad is only suitable for modeling ISL+ and not the richer language ISLwhile,+ that includes iteration.
Panangaden [41] solves this problem by considering sub-Markov kernels obtained from the monad Π of subprobability distributions. A subprobability distribution is like a probability distribution except it allows the probability of the whole space to be any value between 0 and 1; this relaxation enables the partiality inherent in modeling iteration. The subprobability functor Π over the category of measurable

spaces can be summarized as:
Π : Meas → Meas
(X, ΣX ) '→ (ΠX, Σ• )


X	Y '→ ΣX
ν	'→
[0,1]	ΣY
−1	ν)
ΣX


[0,1] 

The measurable space (ΠX, Σ• ) is the set of all subprobability distributions over X equipped with the smallest σ-algebra that makes measurable all evaluation functions ϵA : ΠX → [0, 1], where A ∈ ΣX and ϵA(ν) = ν(A). The arrow action produces a measurable map on distributions (ν ∈ ΠX) '→ (f−1; ν ∈ ΠY ). The functor is a monad with unit and multiplication:
ηX : X → ΠX	μX : Π2X → ΠX
x '→ δx	P, A '→	ν(A) P (dν)
ΠX
The unit η maps a point x to its point-mass distribution δx and multiplication μ evaluates a distribution over distributions down to its average distribution. A comment about notation: when defining functions into spaces of distributions, we find it convenient to take a measurable set as an extra argument—that is, we define a map X → ΠY in its uncurried form, X × ΣY → [0, 1].
Panangaden presents MeasΠ more directly as the category of sub-Markov ker- nels or stochastic relations, SRel. Its objects are the same as Meas, and an arrow
f	is a function f : X × ΣY → [0, 1] such that every f (x, −) is a subprobability
X	Y
distribution and every f (−, B) is measurable. We can think of f as a probabilistic variant of a relation: it gives the probability that a point in X is “related to” a

measurable subset of Y . Arrow composition	f
X	Y
is then defined as
Z


(f ; g)(x, C) =	f (x, dy) g(y, C)	(1)
Y
which can be read: the probability that (f ; g) relates x to the measurable set C is the probability that f relates x to something that g then relates to C. It is easy to see that SRel and MeasΠ are isomorphic: currying a stochastic relation


X×ΣY
f
[0,1]
gives a Kleisli arrow	f˜
X
, and Kleisli composition is just a curried
ΠY

version of (1). Throughout the paper we will freely interchange MeasΠ and SRel.
To support a semantics for ISLwhile, the base category Meas must be distributive and MeasΠ must be partially additive. Panangaden [41] establishes partial addi-

tivity: 3 the sum of a family 

X×ΣY
f)i
[0,1],

i∈I
is the pointwise sum of the functions

if the sum is a valid stochastic relation—it does not exceed 1 anywhere—otherwise
the family is not summable. For distributivity, we first need products and coprod- ucts. Like the category of topological spaces, Meas is topological over Set [2] and

3 Abramsky [1] first observed that SRel is a traced monoidal category. Panangaden refined this by fleshing out its partially additive structure—see Haghverdi’s thesis [25] for details on how the iteration operator in a partially additive category induces a trace.

inherits both completeness and cocompleteness. Limit spaces are limits from Set equipped with the initial σ-algebra, and colimit spaces are colimits from Set with the final σ-algebra. In the cases of products and coproducts, this means that the product space (X, ΣX )×(Y, ΣY ) is (X ×Y, ΣX ⊗ΣY ), where ΣX ⊗ΣY is the smallest

σ-algebra that makes the projections

X×Y
π)1
and
X

X×Y
π)2
measurable. Similarly,
Y

the coproduct space (X, ΣX )+ (Y, ΣY ) is (X + Y, ΣX ⊕ ΣY ), where ΣX ⊕ ΣY is the

largest σ-algebra making the injections	ι1
X
and	ι2
X+Y	Y
measurable. Thus,
X+Y

ΣX ⊗ ΣY = σ({A × B : A ∈ ΣX,B ∈ ΣY })
is the σ-algebra generated by the “rectangles” A × B with measurable sides, and ΣX ⊕ ΣY = {A ∪· B : A ∈ ΣX,B ∈ ΣY }
is the set of disjoint unions of pairs of measurable sets from X and Y . Distributivity then follows by an elementary argument that the inverse in Set for the canonical

map

X×Y +X×Z
[1 × ι1, 1 × ι)2]

X×(Y +Z)
is measurable.

It only remains to describe how to interpret probabilistic choice. It is easy to see
that SRel is closed under subconvex combinations of morphisms: given a sequence

ai ∈ R+ such that Σ ai ≤ 1, any family of arrows 
f)i	,
becomes summable

	,	X	ΠY  i∈N
		
guarantees that the pointwise sum of the family does not exceed 1. Specializing this to families with two arrows defines an abstract interpretation for probabilistic choice in any category whose hom-sets are closed under subconvex combinations:
Monadic Semantics of +p:

 S1 +p
S2)Γ 


 Γ)
Σ (1 − p) S1)Γ, p S2)Γ}


T Γ)


As Panangaden notes, this semantics for ISLwhile,+ in SRel is the same as the semantics of Kozen’s probabilistic language of while loops [32,33].
Adding Delay
We begin by showing how to abstractly add delay to a monadic semantics for ISL. We are ultimately interested in modeling ISL0 extended with delay and other ef- fects; the method we present in this section is parameterized over the monad that models the class of effects to which delay should be added. This parametric story is instantiated in §4 to add delay to the Par model of ISLwhile and in §5 to add delay to the SRel model of ISLwhile,+.
Modeling delay requires a notion of time which can be conveniently and ab- stractly captured using a monoid (M, e, m) [4]. Common examples include the naturals (N, 0, +) and nonnegative reals (R+, 0, +). Since we are defining semantics
categorically, we abstract our model of time one step further and use a monoid in a
category C: an object M in a category C with finite products equipped with a unit

arrow	e
1
and a multiplication arrow
M


M×M
m	subject to the commutativity of
M


two diagrams corresponding to the unit laws and associativity.
M 1 ×)e M 2 (e × 1 M	M 3	m ×)1 M 2
1 × m	m
v
M	M 2 	) M
m
In outline, we add delay to a semantics for ISLwhile as follows. We start with a monadic semantics of ISLwhile,ext , for some language extension ext , in a partially additive category CT . Given a monoid (M, e, m) in C to represent time and a strength for T , we get a distributive law of the monad −× M over T , making the composition T (−× M ) a monad. We then identify reasonable assumptions under which CT (−×M) inherits partial additivity from CT , enabling a natural extension of the CT semantics to one in CT (−×M) that also models delay.
Let (M, e, m) be a monoid in a distributive category C. Our notion of delay is expressed simply:
Delay Extension: wait
Syntax:	Typing Rules:

τ ::= ··· | time
S ::=  ··· | wait E
Γ ▶ E : time


Γ ▶ wait E


The statement wait E delays execution by E time units, where E has type time. We use the monoid M to interpret values of type time:
 time)   M
This means that time expressions denote arrows into M :
 E : time)Γ : Γ) → M
But what monad is appropriate to model delay? Or, more directly, what Kleisli arrows should interpret statements in the timed language?
To guide our intuition, we first consider adding wait to an effect-free model of ISL0 in C so that statements are interpreted just as arrows X → Y . To associate
a delay with a pure computation, we can use arrows X → Y × M that compute a time in addition to a new state:
 S)Γ : Γ) → Γ) × M
The statement wait E should then denote an arrow that computes the delay E and leaves the state unchanged:

 wait E)Γ =



 Γ)
⟨1, E : time)Γ⟩



 Γ)×M

Statements with no substatements, like skip, should denote arrows that record no delay, and sequenced statements should combine their delays:
 skip)Γ =	⟨1, !⟩	1 × e
 Γ)	 Γ)×1	 Γ)×M

 S ; S )Γ =
 S1)Γ; ( S2)Γ × 1)	α
1 × m

1	2	 Γ) 	)
( Γ)×M )×M
 ) Γ)×(M×M ) 	)
 Γ)×M



where	!
X
is the terminal arrow from X and
1


(X×Y )×Z
α
X×(Y ×Z)
associates prod-


ucts. If we continued considering interpretations for the rest of the statements in ISL0, we would see that it is exactly the monadic semantics over the well-known monad of monoid actions −× M : C → C extended with an interpretation for wait.
Proposition 3.1 Given a monoid (M, e, m) in a category C with ﬁnite products, the functor −× M : C → C is a monad with
η	=	⟨1, !; e⟩	μ	=	α	1 × m
X	X×M	(X×M )×M	X×(M×M )	X×M


where	!
X
is the terminal arrow from X.
1

The above gives a semantics for ISLwait in C−×M , but it leaves no room for other monads capturing additional effects. Given a semantics for some extension ISLext in a Kleisli category CT , we want to combine the monads T and −× M to obtain a monad modeling the effects in both languages. Combining monads is difficult in general, but in our setting the straightforward approach of functor composition and distributive laws [6] suffices.
If we want to combine T and −×M by composition, which order is appropriate? Intuitively, we think of partial additivity as giving a way to take partially defined arrows and combine them into a single arrow that aggregates all of their partial information. This suggests that the (T−) × M order is inappropriate since a Kleisli arrow X → TY × M decomposes into an arrow X → TY giving the partial state transformation from X to Y and an arrow X → M giving the non-partial delay computation. So, unless we put heavier demands on the monoid M , it seems futile to look for partial additivity on hom-sets X → TY × M . On the other hand, hom-sets X → T (Y × M ) give T the “last word” by framing the Y × M result— a new state and a delay value—within T . For example, consider Set−⊥ : arrows X → (Y × M )⊥ have a natural notion of failure whereas arrows X → Y⊥ × M do not, unless we invent a second notion of failure within M .
For the composition T (−× M ) : C → C to form a monad, a distributive law

λ : (T−) × M → T (−× M )

suffices to define the combined multiplication and satisfy the monad laws.
Proposition 3.2 (Beck [7]) If S, T : C → C are monads with a distributive law of S over T
λ : ST → TS 

then TS : C → C is a monad with unit and multiplication


ηTS =
1
ηT ◦ ηS
TS 

μTS =



TSTS 
 T λ)S



TTSS
μT ◦ μS
TS 

where ◦ is horizontal composition.
Further, distributive laws for − × M arise from strong monads as defined by Moggi [38]. Even though any distributive law of −× M over T suffices to achieve our goals, the monads we use in §4 and §5 are both strong and strength is somewhat more familiar than distributive laws, so we restrict our attention to distributive laws arising from strong monads.
Proposition 3.3 If C is a category with ﬁnite products, T : C → C is a strong monad, and (M, e, m) is a monoid in C, then a tensorial strength for T
tX,Y : X × TY → T (X × Y )
gives a distributive law of −× M over T
λX = t¯X,M : TX × M → T (X × M )
where t¯	=	γ	tY,X	T γ	is t	commuted.
TX×Y	Y ×TX	T (Y ×X)	T (X×Y )
Not only does the distributive law give the monad T (−× M ) : C → C that extends the pure semantics for ISL0 to model delay in addition to the effects in the original language ISLext , but it also induces a monad −× M : CT → CT that directly extends the effectful semantics of ISLext .
Proposition 3.4 If S, T : C → C are monads and S distributes over T, then S
lifts to a monad S : CT → CT where

S	 fT	 =

S)f
λ)Y	 )

)
XT	YT

(SX)T

	
  SX	STY	TSY  T

(SY )T



S X)T
XT


(SX)T
= X
TS X
TSX  T




(SSX)T


S X)T



(SX)T

= SSX
(ηT ◦ μS))X



TSX  T

where ◦ is horizontal composition. Further,
(CT )S ∼= CTS 
This is an instance of a Kleisli lifting of a functor [39,40] where, since the natural transformation classifying the lifting is a distributive law, the lifted functor is a monad and its Kleisli category coincides with the one for the composite monad.
We can now give a monadic semantics for the wait statement. Let T : C → C
a monad such that −× M distributes over T . This yields a monad T (−× M ) : C → C with which we instantiate the monadic semantics of §2 and extend with an interpretation for wait:

Monadic Semantics of wait:

 wait E)Γ 


 Γ)
⟨1, E : time)Γ⟩


 Γ)×M
 )η


T ( Γ)×M )


We must also ensure, however, that the new models in CT (−×M) can still interpret
T ’s original effects.
Our languages become uninteresting without iteration, so we seek conditions to ensure that if CT is partially additive then CT (−×M) is partially additive as well. In particular, we seek a set of conditions much smaller than the somewhat cumbersome set of properties in Definition 2.1. If we keep in mind the lifted monad
−× M : CT → CT while trying to prove partial additivity for CT (−×M), we arrive at a pair of simple, sufficient conditions:
Definition 3.5 A functor S : D → D' between partially additive categories pre- serves partial additivity 4 if and only if
{fi}i∈I summable implies {Sfi}i∈I summable
S(Σ fi) = Σ(Sfi)
Proposition 3.6 If a monad S : D → D preserves partial additivity, then DS is partially additive where

XS
(fi))S
YS ,


i∈I
is summable if and only if X
f)i
SY ,


i∈I
is summable

Σ XS
(fi))S
YS = Σ X
f)i


Y S S

Checking conditions (2)–(5) in the definition of partial additivity is lengthy but straightforward. Condition (1), that DS has countable coproducts, follows imme- diately from D having countable coproducts [49] since it is partially additive.
The result of all of this is that we can model delay in a monoid from the base category of a monadic model of ISL0 and interpret the extended language, given that we can establish a distributive law and preservation of additivity. Modeling delay is the easy part—all the work goes into making sure we can still model iteration.
Theorem 3.7 Let S, T : C → C be monads with CT partially additive. If S distributes over T and S : CT → CT perserves partial additivity, then CTS is partially additive.
Corollary 3.8 Let C have ﬁnite products, let T : C → C be a strong monad with CT partially additive, and let M be a monoid in C. Then T (−× M ) : C → C is a monad and, if −× M : CT → CT preserves partial additivity, then CT (−×M) is partially additive.
The rest of the paper studies two applications of this theorem.
As a sanity check, we can verify that using the trivial one-element monoid to model time gives back the original semantics. Let (M, e, m) = (1, !1, !1×1) and ob-

4 Haghverdi [25] observes that this is the same as a functor enriched over the category of partially additive monoids and calls such a functor additive.

serve that, since terminal objects are the unit for product, X × 1 ∼= X, the extended Kleisli category collapses to the original one: CT (−×1) ∼= CT . Intuitively, since a
terminal object has exactly one point, modeling delay in the trivial monoid amounts to throwing away the language’s information about the duration of computations.

Adding Delay to Par
Before extending a probabilistic variant of ISL0 with delay, we first consider ISLwhile since it has a simple semantics over partial functions. Then, to obtain a useful intermediate between the two, we specialize the deterministic semantics to model probabilistic delays while retaining deterministic behavior on states.

Deterministic Delay
Consider the Par ∼= Set−⊥ semantics for ISLwhile mentioned in §2. To extend the semantics with delay by following the program outlined in §3, we need a few things: a monoid M in Set, a strength for −⊥ to make (−× M )⊥ a monad, and preservation of partial additivity for the lifted monad −× M on Par. These things are easy to obtain.
Fix a monoid (M, e, m) in Set to model time. It is well known that −⊥ is strong [38], making the composite functor (−× M )⊥ a monad.
Proposition 4.1 −⊥ : Set → Set is a strong monad with tensorial strength
tX,Y : X × Y⊥ → (X × Y )⊥
(x, y) '→ (x, y) (x, ⊥) '→⊥ 
Corollary 4.2 The functor (−× M )⊥ : Set → Set is a monad with unit and multiplication
ηX : X → (X × M )⊥	μX : ((X × M )⊥ × M )⊥ → (X × M )⊥
x '→ (x, e)	((x, b), a) '→ (x, m(a, b)) (⊥, a) '→⊥ 
⊥ '→⊥ 

The lifted monad can be shown to preserve partial additivity by straightforward reasoning with sums of partial functions.

Proposition 4.3 The monad −× M : Set−⊥ → Set−⊥ preserves partial additiv- ity.
Corollary 4.4 The category Set(−×M)	is partially additive.
Instantiating the monadic semantics of ISLwhile,wait from §2 and §3 with CT =

Set(−×M )
, we see that statements are interpreted as partial functions where the

result contains a delay component capturing the cumulative delay incurred by the statement:
 S)Γ : Γ) → ( Γ) × M )⊥
In particular, the wait statement terminates and records the specified delay:
 wait E)Γ(x) = (x, E : time)Γ(x))

Sequenced statements combine their delays with the monoid multiplication m, and pure statements represent the fact that they incur no delay with the monoid unit e.

Probabilistic Delay
Deterministic delays are too simple to model systems with complex time behavior. A more expressive language would be able to represent the duration of complex computations stochastically by sampling delays from probability distributions. Here we consider a language with probabilistic delays and deterministic state behavior before moving to a fully probabilistic language in the next section.
Consider a variation on ISLwhile,wait where delays are sampled from probability distributions. This is easily achieved by using the time type to classify expres- sions describing distributions over time. One way to understand such an expression language is to view expressions as deterministically specifying the probability dis-
tribution over their possible delays. For the sake of examples consider including expressions exp(E), an exponential distribution with parameter E, and bern(E), a Bernoulli distribution yielding false with probability E and true with probability 1 − E (with suitable default behavior if E is out of range).
Since expressions E : time now describe distributions instead of what we were previously thinking of as deterministic durations, the language is easily modeled within the Par semantics just presented: use a monoid of probability distributions over time. But what kind of monoidal structure is meaningful? In particular, what should we use for multiplication?
Consider how Kleisli composition should operate on a pair of sample denotations in Set(−×ΠR+) , where we abbreviate T = (−× ΠR+)⊥:




 Γ)T
 v := v + 1; wait exp(2))v


 Γ)T
 v := v + 1; wait exp(4))v


 Γ)T

Since the mean of exp(λ) is 1 , we expect this composition to increment v twice with mean delay 1 + 1 = 3 . Put differently, if we define a random variable observing the
2	4	4
delay of each program, then we want the random variable of the composite to be the sum of those for the first and second program. This suggests using convolution of measures as our monoid multiplication.
Definition 4.5 Given μ, ν ∈ ΠR+, their convolution μ ∗ ν ∈ ΠR+ is:
μ ∗ ν = (μ × ν)+ = A '→ ∫∫ χA(x + y) μ(dx) ν(dy)

where χA(x) = 1 if x ∈ A and 0 otherwise.
Proposition 4.6 (ΠR+, δ0, ∗) is a monoid in Set, where the unit δ0 is the point mass at 0 and multiplication ∗ is convolution.
Generalizing this from R+ to arbitrary monoids is straightforward: replace the monoid (R+, 0, +) with any monoid (M, e, m) that is also a measurable space, and the integral will be defined if multiplication m is measurable.  More simply, we
can just ask for a monoid in Meas since the unit e is always measurable: given a

monoid (|M|, e, m) in Set where M is a measurable space,	e
1
since both subsets of the terminal object 1 are measurable.

|M|
is measurable

Definition 4.7 A measurable monoid is a monoid in Meas.
We write M for measurable monoids and M for monoids in Set.
We can now generalize convolution to arbitrary measurable monoids and get a monoidal structure on their spaces of probability distributions.
Definition 4.8 Given μ, ν ∈ ΠM over a measurable monoid (M, e, m), their con- volution μ ∗m ν ∈ ΠM is:


μ ∗m ν = (μ × ν)m = A '→
M2
χA(m(a, b)) μ(da) ν(db)

We write μ ∗m ν as μ ∗ ν when the monoid is clear from context.
Proposition 4.9 If (M, e, m) is a measurable monoid then (ΠM, δe, ∗m) is a monoid in Set where the unit δe is the point mass at e and multiplication ∗m is convolution of measures.
Equipped with a compelling monoidal structure over distributions, we can now instantiate the Par semantics from §4.1 and derive a model for the (deterministic) language with probabilistic delay. Type time now corresponds to distributions:
 time)   ΠM
Since expressions E : time now represent distributions and are interpreted as
 E : time)Γ : Γ) → ΠM the abstract semantics for wait becomes:

 wait E)Γ =

 Γ)
⟨1, E : time)Γ⟩

 Γ)×ΠM
 ηT

T ( Γ)×ΠM)

With T = −⊥, the resulting Set(−×ΠM)  semantics is very close to the Set(−×M)
semantics except it uses a monoid of distributions to interpret stochastic delay alongside deterministic behavior on states. The interpretation of statements is now
 S)Γ : Γ) → ( Γ) × ΠM)⊥

The wait statement terminates and records the expressed distribution over time:
 wait E)Γ(x) = (x, E : time)Γ(x))
Sequenced statements combine their delay distributions by convolution. Pure state- ments represent the fact that they incur no delay with δe, the point mass at the monoid unit e.
Adding Delay to SRel
The language in the previous section expresses stochastic computations with prob- abilistic delay but fails to capture systems that also have probabilistic behavior on states. To achieve both we add delay to the probabilistic language ISLwhile,+ from
§2; probabilistic delay falls out of the combination. This section uses the method from §3 to extend the SRel semantics for ISLwhile,+ to also model delay, giving a semantics for a probabilistic language with stochastic temporal behavior.
Consider the SRel ∼= MeasΠ semantics for ISLwhile,+ described in §2. Following
the method in §3 is again straightforward: take a monoid M in Meas, construct a strength for Π to make Π(−× M) a monad, and establish that the lifted monad
−×M : SRel → SRel preserves partial additivity.
Fix a measurable monoid M to model time. Strength for Π is straightforward:
Proposition 5.1 Π : Meas → Meas is a strong monad with tensorial strength
tX,Y : X × ΠY → Π(X × Y )
(x, ν),C '→ ν(Cx)
where Cx = {y : (x, y) ∈ C}. Equivalently, t maps to the product measure
tX,Y (x, ν) = δx × ν
The equation follows easily by considering both sides’ action on measurable rectan- gles, which uniquely determines product measures. Working with product measures then enables easy proofs of measurability and naturality, the former because the

product map

ΠX×ΠY
×
Π(X×Y )
is measurable. Finally, proving the required equali-

ties for strength is straightforward.
Proposition 5.1 gives a monad combining probability and delay:
Corollary 5.2 The functor Π(−× M) : Meas → Meas is a monad with unit and multiplication


ηΠ(−×M) =
1
ηΠ ◦ η−×M





Π(−×M)

μΠ(−×M) =


Π(Π(−×M)×M)
Πλ(−× M))

Π2(−×M)
μΠ ◦ μ−×M
2

Π(−×M)

where λ = t¯ is the distributive law obtained from strength for Π, and ◦ is horizontal composition.

It is worthwhile to spell these out in detail. The unit just introduces the point-mass

distribution and the monoid’s unit:  ηΠ(−×M)(x) = δ
(x,e)
. Multiplication is more

interesting:
μΠ(−×M)(P )(C) = ∫




Π(X×M)×M


ν({(x, b) : (x, m(b, a)) ∈ C}) P (dν, da)

where P ∈ Π(Π(X × M)×M) and C ∈ ΣX×M. The behavior of μΠ(−×M) is similar to μΠ, which averages a distribution over distributions down to a single distribution, except μΠ(−×M) must also incorporate the monoid action.
Although the above multiplication is complicated, it corresponds to a nice Kleisli composition and supports a satisfying direct presentation analogous to SRel:
Definition 5.3 The category TSRelM of M-timed stochastic relations has mea-

surable spaces as objects and an arrow
X
f	is a function f : X × ΣY ×M → [0, 1]
Y


such that every f (x, −) is a subprobability measure and every f (−, C) is measurable.

The identity arrow is
1)X
is 1X (x, C) = δ(x,e)(C), and the composition	f	g

(f ; g)(x, C) = 
Y ×M

Z×M
f (x, d(y, a)) g(y, d(z, b)) χC((z, m(b, a)))

We think of a stochastic relation
X
f	as giving the probability that a point in
Y


X relates to a measurable subset of Y ; similarly, we think of a timed stochastic

relation
X
f	as doing the same for measurable subsets of Y × M—points in Y
Y

and values in the monoid M, which we interpret as time delay. We can then read composition as: f ; g relates x to C if f relates x to y with delay a, g relates y to z with delay b, and z paired with the aggregate delay m(b, a) is in C. The probability that f ; g relates x to C is then the sum of the probabilities of each of these sufficient cases.

Since currying a timed stochastic relation

X×ΣY ×M
f
[0,1]
produces a Kleisli ar-

row
X
 f˜


Π(Y ×M)
we expect to also have the isomorphism TSRelM ∼= MeasΠ(−×M).

Indeed, using change of variables and Fubini’s theorem it is a straightforward calcu- lation to show that Kleisli composition is just a curried version of composition for timed stochastic relations, and the isomorphism is then easy to construct. We freely
interchange MeasΠ(−×M) and TSRelM to take advantage of both the curried and
uncurried forms of timed stochastic relations.
Now that we have a category TSRelM capable of modeling probabilistic choice and delay, the last step is to show that it can also interpret iteration by establishing partial additivity. Because SRel is partially additive, it suffices to show that the lifted monad −×M on SRel preserves partial additivity. This also follows by an elementary measure-theoretic argument.

Proposition 5.4 The monad −×M : MeasΠ → MeasΠ preserves partial addi- tivity.
Corollary 5.5 The category MeasΠ(−×M) is partially additive.

Additionally, we recover the SRel semantics as TSRel1 which ignores delay by collapsing everything in the one-element monoid.
TSRelM interprets delay statements wait E for deterministic durations as


 wait E)Γ =


 Γ)
⟨1, E : time)Γ⟩


 Γ)×M
 )η


Π( Γ)×M)

but what about probabilistic delays like in §4.2? The monad Π(− × M) gives distributions over both state and time, so we expect to be able to model these as well without taking M itself to be a space of distributions.
So that deterministic and probabilistic delay can coexist, we introduce a second delay statement, pwait, and a new family of types:
Probabilistic Delay Extension for TSRelM: pwait
Syntax:	Typing Rules:

τ ::= ··· | prob τ
S ::= ··· | pwait E
Γ ▶ E : prob time


Γ ▶ pwait E


The statement pwait E samples the time distribution E : prob time and delays execution by the resulting number of time units. Types prob τ denote spaces of probability distributions over values of type τ :

 prob τ )   Π τ )

This means that prob τ expressions denote arrows into these spaces of distributions:
 E : prob τ )Γ : Γ) → Π τ )

As in §4.2, we assume expressions for exponential distributions exp(E) and Bernoulli distributions bern(E), but now with type prob τ where E : τ .
We expect pwait E)Γ to be a timed stochastic relation that relates a state x
only to itself with delay sampled from E : prob time)Γ(x):

 pwait E)Γ(x, C) =	χC(x, a)  E : prob time)Γ(x, da)
M
= (δx × E : prob time)Γ(x))(C)

This semantics is concisely expressible using strength for Π.
TSRelM Semantics of pwait:

 pwait E)Γ 


 Γ)
⟨1, E : prob time)Γ⟩


 Γ)×ΠM
 )t


Π( Γ)×M)


We can even characterize probabilistic delay in TSRelM in terms of the orig-

inal probabilistic delay in Set(−×ΠM)
given in §4.2 where state transitions were

deterministic:
 pwait E)Γ =






 Γ)


  wait E)Γ





( Γ)×ΠM)⊥


 ϕ)






Π( Γ)×ΠM)


 Π)t






Π2( Γ)×M)


 μΠ





Π( Γ)×M)

where  wait E)Γ is the interpretation in Set(−×ΠM)	of wait E from the language
of §4.2. We bridge the two semantic categories with a map ϕ : −⊥ → Π that sends points to their point-mass distributions and failure to the distribution that measures everything as 0:
ϕX : X⊥ → ΠX
x '→ δx
⊥ '→ 0

TSRelM models probabilistic choice and probabilistic delay, and both operators are based on sampling a probability distribution. This suggests that extending the language with a construct to sample probability distributions should enable us to express both operators.
Sampling Extension: ←
Syntax:	Typing Rules:
S ::=  ··· | v ← E	Γ,v : τ, Γ' ▶ E : prob τ

Γ,v : τ, Γ' ▶ v ← E	(v ∈/ Γ)

The statement v ← E samples the distribution E : prob τ and assigns the result to
v. It is tempting to formulate this as an expression, like sample(E), but doing so would introduce effects into the expression language and complicate our framework.
The sampling operator is easily modeled in our probabilistic categories. We define v ← E)Γ : Γ) → ΠS Γ) for any monad S on Meas that composes with Π; taking S to be 1 gives a denotation in SRel, and −×M gives one in TSRelM. Recall that distributions are interpreted as E : prob τ )Γ : Γ) → Π τ ).
Monadic Semantics of ←:
 v ← E)Γ,v:τ,Γ' 

⟨π , E : prob τ )Γ,v:τ,Γ' ,π ⟩	tˆ
ΠηS

 Γ)× τ )× Γ')	1
3) Γ)×Π τ )× Γ')  )
Π( Γ)× τ )× Γ'))  )
ΠS( Γ)× τ )× Γ'))

The key is the arrow

tˆ	=	t × 1	t
X×ΠY ×Z	Π(X×Y )×Z







Π(X×Y ×Z)

which reifies the distribution produced by E : prob τ )Γ as a probability distribution over the whole state space.
We can now express pwait E simply by sampling E and then waiting the length of time specified by the result. Sampling also generalizes probabilistic choice: sam- ple a Bernoulli distribution and branch. The following proposition captures these

intuitions using the TSRelM semantics, illustrating how our model can validate equivalences between stochastic programs.
Proposition 5.6
pwait E)Γ = let v : time = 0 in v ← E; wait v)Γ	(v ∈/ Γ)
S1 +p S2)Γ = let v : bool = true in v ← bern(p); if v then S1 else S2)Γ (v ∈/ Γ)

Related Work
Related work broadly falls into three categories: models for stochastic temporal behavior, languages for expressing stochastic temporal behavior, and applications of the probability monad to develop semantic models.
Several frameworks exist to describe and model stochastic temporal behavior, including queueing systems [30], stochastic automata [15,16], generalised stochastic petri-nets [36], and generalised semi-Markov processes [22]. Our approach shares much in common with stochastic automata. Roughly speaking, stochastic automata extend standard deterministic automata with clock variables, just like timed au- tomata [3]. Upon entering a state, some of those clocks are set by sampling a prob- ability distribution, and then all clocks decrement at the same rate. Transitions are labeled with an input symbol and a set of clocks, and a transition is enabled once its labeling clocks reach 0. Stochastic automata are usually interpreted us- ing a probabilistic transition system with two classes of states, states from which nondeterministic choices are made, and states from which probabilistic choices are made, the latter essentially corresponding to probabilistic delays. It is possible to view our work as a partial reframing of stochastic automata in a categorical setting, providing them with a direct transition semantics.
As far as languages for stochastic temporal behavior are concerned, much of the original impetus came from finding reasonable languages in which to composi- tionally and finitely represent models for the study of stochastic temporal behavior in systems with soft constraints. Stochastic process calculi, with their support for concurrency and their ready compositionality, have proved popular [23,43,26,8,27]. Stochastic process calculi, especially derived from the stochastic pi calculus [43], are especially popular for biological modeling [46,45,14,10,17]. In the tradition of process calculi, the semantics of those languages is operational, using an annotated reduction semantics that records the rate of reaction (which correspond, roughly, to the time delays introduced in the reduction). Stochastic process calculi generally use exponential distributions to model delays, and the reduction semantics can be shown to yield continuous-time Markov processes. Restricting to Markov processes implies that we can reason more efficiently about the resulting processes expressed in the stochastic pi calculus, or stochastic automata, for that matter; see, for in- stance, Bryans et al. [13]. Priami [44] shows how to extend the stochastic pi calculus to general distributions. Recently, Klin and Sassone [31] developed a general oper- ational reduction semantics for stochastic process calculi that unifies much of the ad hoc presentation in earlier papers. Our work is essentially denotational and can

be seen as complementary. We have not yet applied it to process calculi.
Variants of the Giry probability monad [21], based on earlier work by Law- vere [34], have been the basis of most denotational semantics for probabilistic lan- guages [48,28,41,50]. Doberkat [18] offers an exhaustive overview of the probability monad and stochastic relations from a categorical perspective. Doberkat extends stochastic relations with monoids to model software architectures, but he consid- ers component pipelines without cycles, whereas iteration is central to our study. Ramsey and Pfeffer [47] use the probability monad as a semantic foundation for a stochastic lambda calculus. The interaction between the probability monad and other monads has been studied in a few contexts. Breugel [50] shows that a distribu- tive law between the Giry probability monad and the partiality monad −⊥ gives rise to the subprobability monad Π. Other distributive laws relate the probability monad to nondeterminism—see Varacca and Winskel [51] and references therein. Our work can be seen as studying the interaction of the probability monad with various forms of monoid tensor addition.





Conclusion

Our paper presents an approach to adding delay to a categorical semantics for languages of while loops by generalizing the category of stochastic relations SRel to a family of categories of timed stochastic relations TSRelM. Our approach is suitable for modeling both probabilistic choice and stochastic temporal behavior in a single categorical framework.
Our work is preliminary, and several questions remain. For instance, TSRelM is parameterized by a monoid M; what is the role of the “re-timing” functors TSRelM → TSRelN induced by monoid homomorphisms? What is the exact relationship between TSRelM and continuous-time Markov chains, which must appear in TSRelM in some form? Another question relates to time dependence. Our transitions cannot depend on the time at which a transition occurs since time is not provided in the domains of arrows in our categories. Making transitions time dependent is not difficult, but in some sense everything would then collapse down to SRel, at least in the probabilistic case: time-dependent transitions can be encoded by including time as part of the state and restricting to morphisms that update the time correctly. We have not explored the exact relationship between time-dependent models and our own. In addition, it would be interesting to explore extensions to higher-order recursive languages.
Finally, we need to examine the relationship between our models and the more operational models used in the stochastic process calculus literature. A starting point is to use our categories or variants thereof to give a semantics to stochastic process calculi. We hope to report on this research in the near future.

Acknowledgement
Thanks to Jesse Tov and Aaron Turon for comments on a preliminary version of this paper.

References
Abramsky, S., Retracing some paths in process algebra, in: Proc. 7th International Conference on Concurrency Theory (CONCUR’96), Lecture Notes in Computer Science 1119 (1996), pp. 1–17.
Ad´amek, J., H. Herrlich and G. E. Strecker, “Abstract and Concrete Categories: The Joy of Cats,” John Wiley & Sons, 1990.
Alur, R. and D. L. Dill, A theory of timed automata, Theoretical Computer Science 126 (1994), pp. 183– 235.
Asarin, E., P. Caspi and O. Maler, Timed regular expressions, Journal of the ACM 49 (2002), pp. 172– 206.
Ash, R. B. and C. A. Dol´eans-Dade, “Probability & Measure Theory,” Academic Press, 1999.
Barr, M. and C. Wells, “Toposes, Triples and Theories,” Grundlehren der mathematischen Wissenschaften 278, Springer-Verlag, New York, 1985.
Beck, J., Distributive laws, in: Seminar on Triples and Categorical Homology Theory, Lecture Notes in Mathematics 80 (1969), pp. 119–140.
Bernardo, M. and R. Gorrieri, A tutorial on EMPA: A theory of concurrent processes with nondeterminism, priorities, probabilities and time, Theoretical Computer Science 202 (1998), pp. 1–54.
Billingsley, P., “Probability and Measure,” Wiley-Interscience, 1995.
Bortolussi, L. and A. Policriti, Modeling biological systems in stochastic concurrent constraint programming, Constraints 13 (2008), pp. 66–90.
Bowman, H., L. Blair, G. S. Blair and A. Chetwynd, “Formal Specification of Distributed Multimedia Systems,” University College London Press, 1998.
Brown, D. and R. Pucella, Categories of timed stochastic relations, Technical Report NU-CCIS-09-02, Northeastern University (2009).
Bryans, J., H. Bowman and J. Derrick, Model checking stochastic automata, ACM Transactions on Computational Logic 4 (2003), pp. 452–492.
Ciocchetta, F. and J. Hillston, Bio-PEPA: An extension of the process algebra PEPA for biochemical networks, Electronic Notes in Theoretical Computer Science 194 (2008), pp. 103–117.
D’Argenio, P., “Algebras and automata for timed and stochastic systems,” Ph.D. thesis, University of Twente, Enschede, The Netherlands (1999).
D’Argenio, P. R. and J.-P. Katoen, A theory of stochastic systems part I: Stochastic automata, Information and Computation 203 (2005), pp. 1–38.
Dematt´e, L., C. Priami and A. Romanel, Modelling and simulation of biological processes in BlenX, SIGMETRICS Performance Evaluation Review 35 (2008), pp. 32–39.
Dobertkat, E.-E., “Stochastic Relations: Foundations for Markov Transition Systems,” Chapman & Hall/CRC, 2007.
Folland, G. B., “Real Analysis: Modern Techniques and Their Applications,” Wiley-Interscience, 1984.
Gibbons, J., Conditionals in distributive categories, Technical report, Oxford Brookes University (1997).
Giry, M., A categorical approach to probability theory, in: B. Banaschewski, editor, Categorical Aspects of Topology and Analysis, Lecture Notes in Mathematics 915 (1981), pp. 68–85.
Glynn, P. W., A GSMP formalism for discrete event simulation, Proceedings of the IEEE 77 (1989),
pp. 14–23.


G¨otz, N., U. Herzog and M. Rettelbach, Multiprocessor and distributed system design: The integration of functional specification and performance analysis using stochastic process algebras, in: Performance/SIGMETRICS Tutorials, 1993, pp. 121–146.
Gunter, C. A., “Semantics of Programming Languages: Structures and Techniques,” MIT Press, 1992.
Haghverdi, E., “A Categorical Approach to Linear Logic, Geometry of Proofs and Full Completeness,” Ph.D. thesis, University of Ottawa (2000).
Hillston, J., “A Compositional Approach to Performance Modelling,” Distinguished Dissertations in Computer Science, Cambridge University Press, 1996.
Hillston, J., Process algebras for quantitative analysis, in: Proc. 20th Annual IEEE Symposium on Logic in Computer Science (LICS’05) (2005), pp. 239–248.
Jones, C. and G. D. Plotkin, A probabilistic powerdomain of evaluations, in: Proc. 4th Annual IEEE Symposium on Logic in Computer Science (LICS’89) (1989), pp. 186–195.
Kallenberg, O., “Foundations of Modern Probability,” Springer-Verlag, 2002.
Kleinrock, L., “Queueing Systems, Volume I: Theory,” John Wiley & Sons, 1975.
Klin, B. and V. Sassone, Structural operational semantics for stochastic process calculi, in: Proc. 11th International Conference on Foundations of Software Science and Computation Structures (FOSSACS’08), Lecture Notes in Computer Science 4962 (2008), pp. 428–442.
Kozen, D., Semantics of probabilistic programs, Journal of Computer and Systems Sciences 22 (1981),
pp. 328–350.
Kozen, D., A probabilistic PDL, Journal of Computer and Systems Sciences 30 (1985), pp. 162–178.
Lawvere, F. W., The category of probabilistic mappings (1962), unpublished manuscript.
Manes, E. and M. Arbib, “Algebraic Approaches to Program Semantics,” Springer-Verlag, 1986.
Marsam, M. A., G. Conte and G. Balbo, A class of generalised stochastic petri nets for the performance evaluation of multiprocessor systems, ACM Transactions on Computer Systems 2 (1984), pp. 93–122.
Moggi, E., Computational lambda-calculus and monads, in: Proc. 4th Annual IEEE Symposium on Logic in Computer Science (LICS’89) (1989), pp. 14–23.
Moggi, E., Notions of computation and monads, Information and Computation 93 (1991), pp. 55–92.
Mulry, P. S., Lifting theorems for Kleisli categories, in: Proc. 9th International Conference on Mathematical Foundations of Programming Semantics (MFPS 9), Lecture Notes in Computer Science 802 (1994), pp. 304–319.
Mulry, P. S., Lifting results for categories of algebras, Theoretical Computer Science 278 (2002),
pp. 257–269.
Panangaden, P., The category of Markov kernels, in: Proc. 1st International Workshop on Probabilistic Methods in Verification (PROBMIV’98), Electronic Notes in Theoretical Computer Science 22 (1999),
pp. 171–187.
Pierce, B. C., “Types and Programming Languages,” MIT Press, Cambridge, MA, USA, 2002.
Priami, C., Stochastic pi calculus, The Computer Journal 38 (1995), pp. 578–589.
Priami, C., Stochastic pi-calculus with general distributions, in: Proc. 4th Workshop on Process Algebras and Performance Modelling (PAPM ’96), 1996, pp. 41–57.
Priami, C. and P. Quaglia, Modeling the dynamics of bio-systems, Briefings in Bioinformatics 5 (2004),
pp. 259–269.
Priami, C., A. Regev, E. Shapiro and W. Silverman, Application of a stochastic name-passing calculus to representation and simulation of molecular processes, Information Processing Letters 80 (2001),
pp. 25–31.
Ramsey, N. and A. Pfeffer, Stochastic lambda calculus and monads of probability distributions, in: Proc. 29th Annual ACM Symposium on Principles of Programming Languages (POPL’02) (2002), pp. 154–165.

Saheb-Djahromi, N., CPOs of measures for nondeterminism, Theoretical Computer Science 12 (1980),
pp. 19–37.
Szigeti, J., On limits and colimits in the Kleisli category, Cahiers de Topologie et G´eom´etrie Diff´erentielle Cat´egoriques 24 (1983), pp. 381–391.
van Breugel, F., The metric monad for probabilistic nondeterminism (2005), unpublished manuscript, available from http://www.cse.yorku.ca/~franck/research/drafts.
Varacca, D. and G. Winskel, Distributing probability over non-determinism, Mathematical Structures in Computer Science 16 (2006), pp. 87–113.
Winskel, G., “The Formal Semantics of Programming Languages,” MIT Press, 1993.
