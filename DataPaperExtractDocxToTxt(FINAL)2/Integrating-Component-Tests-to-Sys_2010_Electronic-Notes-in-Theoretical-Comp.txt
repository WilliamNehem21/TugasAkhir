

Electronic Notes in Theoretical Computer Science 260 (2010) 225–241
www.elsevier.com/locate/entcs
Integrating Component Tests to System Tests
Bernhard Sch¨atz Christian Pfaller
Technische Universita¨t Mu¨nchen, Fakult¨at fu¨r Informatik, Boltzmannstr. 3, D-85748 Garching bei Mu¨nchen, Germany


Abstract
During the verification phase in component-oriented approaches to (embedded) system development com- ponent tests are generally followed by system tests, often using different testing specifications and envi- ronments. Thus, when integrating components in the overall system, the actual implementation platform may differ from the development platform, leading to a deviation in the component behavior. This raises the issue of testing the functionality of a component integrated in a larger system without the use of an – instrumented – development platform, allowing to perform glass-box-testing.
We show how a test case of a component can be transformed into a black-box test case of the overall system, allowing to deduce the validity of the test case of the component from the validity of the test case of the system. To mechanize the transformation, we provide a formalization of test cases and their integration in component networks in WS1S, and apply the Mona model checker based on this formalization to automatically generate system test cases from component test cases.
Keywords: Test case, generation, component, system, integration, WS1S, model checking

1	Introduction
In many application domains, e.g., automotive systems or production and automa- tion, embedded software has become the driving force on innovation, leading to an increasing demand for software-controled functionality. To meet this increasing demand for software, a component-based approach to the construction of (software) system has become the foundation of state-of-the-art software engineering. Besides reducing the complexity of the overall engineering process, component-based soft- ware engineering also supports the construction of customized systems by combining pre-fabricated components with others custom-made for the specific system.
In a component-based approach, quality assurance in the constructive phase is usually performed in the form of component tests, and integration or system tests. However, especially in the construction of customized embedded systems like automobiles, quality assurance is faced with two problems, which often occur during system evolution where most components of the system are reused and only a few components are changed or replaced:

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.12.040

testing of the final implementation should be restricted as much as possible to the changed/custom-made components to efficiently cope with the large number customized systems
system tests have to be performed on the implementation-level platform, which does not supply glass-box-testing with access to all internal data needed for testing a component without costly or invasive instrumentation of the implementation
Therefore, in the following, an approach is introduced that allows to transform com- ponent tests to system tests, supporting the verification of test cases for individual components using only the interface of the overall system. A transformation is not always possible, for a given component test (on component level) an equivalent test case on system level may not exist.
After discussion the context and contribution of the presented approach in the remainder of this section, a formalization of observations about components and networks of components is introduced in Section 2 to provide the basics for the applied techniques. As the main part, Section 3 introduces a method for generating such test cases on a system level from test cases for components. Finally, in Section 4 the presented approach is summarized, and compared to other methods of test case generation.

Context
In software testing test cases are derived for a certain system under test (SUT) with the aim of detecting faults in this SUT. The test cases are defined over the interface of the considered SUT, and the interface of the test execution and observation harness usually reflects this interface of the SUT; thus test cases can by applied directly. This is the standard situation of test case derivation and execution. In some cases the situation is different: Assume that a specific component which is already an integrated part of a larger system (consisting of further components) should be tested. We assume further that there is no direct access to the component’s interface from the system interface. Now the question arises how to test the (sub-)component if we only have the possibility to execute test cases and observe its results at the interface of the complete system. To distinguish between the complete system and the considered specific component we use the term component under test (CUT) for such a component in the further.
Hence it is the aim of the presented method to do component testing but execute the test cases at the interface of the integrated system—we do not aim on system or integration tests. In practice there are many situations where such settings occur, for example: (1) There is already a test harness for executing tests at the system level, but no test harness or execution environment for test cases on component level is available. (2) A new third-party component (which may be faulty) was integrated in an otherwise unchanged system and there is no direct access to the component interface available. (3) Setting up a specific test execution environment for every single component is to costly and too time consuming, or not possible. Especially, if embedded devices are to be tested on the implementation level control unit, internal



But:Ind Bat:Vlt





Dia:Cmd
Mot:Dir








Sts:Inf


Fig. 1. Power-Window Control Unit: System including Error Store

data within that unit may not be accessible from the outside. Such an approach may be also useful if we (later) know about a certain defect in an integrated sub- component and want to see the effect of that defect to the complete system at the interface of the implementation-level platform. Reuse may also be a motivation for executing component tests on system interface level: Often component tests are already available and these should be translated in a way that they can be executed at the implementation-level – it will be very useful to know which of these test cases actually test the CUT independently from the surrounding components. The other way around there might already exist system tests and we want to know which of these system tests shed specific light on the behavior of a single sub-component.

Contributions
The introduced method aims at deducing a system test from a component test, allowing to verify that component test case at the system level without instrumen- tation of the system. To that end, we
introduce a common formalization of behavioral specifications in the form of the exemplary behavior – like test case descriptions – as well as complete behav- ior – like component descriptions – including the combination of components to networks.
define the notion of a test case to be satisfied for a component or system, allowing to check for the validity of a test case.
define the concept of a system test case verifying a test case for a component of that system.
provide automatic support to generate a test case for the overall system from a test case of a component, verifying that test case of the component if such a test case exists.
To illustrate the approach, the example of an automotive electronic control unit of a power-controlled window is used. As shown in Figure 1, the control unit holds three software components:
Window Control, which translates a button signal received via port But indicating the direction of the intended movement of the window (Ind = Up, Hd, Dn) into a motor signal sent via Mot indicating the executed movement of the window (Dir
= Lo, Zr, Hi), provided the battery signal received via Bat indicating the current


Req?Vt,Err?OK
:Res!OK

Req?Vt:Res!Ft



Req?Rs,Err?OK: Res!OK




Normal
Req?No,Err?LV: Res!No

Req?Vt,Err?LV: Res!OK




Failure


Req?Rs:Res!OK


Req?No,Err?OK: Res!No

Req?No:Res!No

Fig. 2. Behavior of the Error Store Component
voltage (Vlt = Lo, Hi) states sufficient power; an error signal sent via port Err
indicating the error code (Cod = LV, OK) provides a low voltage error otherwise.
Error Store, which stores an error signal received via port Err indicating the error code; if requested to return or reset the error status by a request signal received via port Req indicating the command (Cmd = Vt, Rs, No), the result of the request is returned as result signal via port Res indicating the corresponding information (Inf = Ft, OK, No).
Diagnosis Management, which translates a diagnostic signal received via port Dia – indicating the diagnostic command – into a corresponding request signal sent via port Req – indicating the requested command – and forwards the result signal received via port Res – indicating the returned information – as a status signal sent via port Sts – indicating the status information.
Since all three components are packaged and deployed onto the control unit, only the external signals received and sent via Bat, But, Dia, Mot, and Sts are available for testing any software component deployed on the unit. Thus, to check the validity of a test case as shown in Figure 4, verifying the reset-set-query functionality of the Error Store component, a corresponding test case is needed that is immediately executable at the interface of the control unit and is indirectly assuring the validity of the test case of the component.

Describing Behavior
In this section, components are introduced as building blocks for the construction of reactive systems. The behavior of a component is described as a transition system, controlling the interaction of the component with its environment or sur- rounding system via input and output signals, and formalized as the set of possible observations about the interaction with its environment, described by the history of exchanged signals. Finally, the notion of observations is extended to systems described as networks of communicating components.

Transition Systems
To describe the behavior of a reactive component, often transition systems are used in the form of state-transition-diagrams. Here, the notation and formalism introduced in [12] and [11], resp., is used which allows the modular formalization

of clocked, hierarchical transition systems. For reasons of brevity, in the following only non-hierarchical state-transition diagrams are considered.
Figure 2 shows the graphical notation used to describe the transition system of a component for the example of the Error Store component, with
the set Loc of control locations, used to describe its control state; e.g., the set
Loc = {Normal, Failure}
the set V ar of variables, used to describe its data state; e.g., the set V ar =
{Err, Req, Res}
the set Trans of transitions of the form (a, pre, post, b), each consisting of a start location a and end location b from the set of locations Loc, as well as a pre-state pre and post-state post expressions assigning values to variables from V ar; e.g., the transition covering the simultaneous occurrence of a voltage error and the empty command is described by the labeled transition from Normal to Failure with a label consisting of Req?No, Err?LV and Res!No
Using these elements, the behavior of a transition system in terms of simple single- step executions is defined. Each transition corresponds to a single step of computa- tion/interaction. When entered through its entry location, it reads the values of its variables; it then changes the variable state by writing new values and terminates by exiting via its exit location.
To describe a transition, we use the notation described in [12]. Using the above example, the first part of the label states that whenever the no-command signal No is received via variable Req and – at the same time – the voltage-error signal LV is received via variable Err, then the transition is enabled. The second part of the
label states that, whenever the transition is triggered, in the next state the no-error signal No is sent via variable Res. These parts use a short-hand notation for reading pre-transtion values and writing post-transition values. They correspond to terms
`Err = LV and Res´= No, respectively, using variables `v with v ∈ V ar for values of v prior to execution of the transition, and variables v´with v ∈ V ar for values of v after its execution.
The formalization of a transition system is based on the assignment of values to variables – for the modeling of the data state – via −V−a→r = Var → Val . 1 Abstracting
from a concrete graphical representation, a transition is described as the structure (a, t, b) with entry location a, exit location b, and transition label t over −V−a→r × −V−a→r .
t corresponds to the conjunction of the pre- and the post-part of the label. Its behavior is the set of simple executions containing all elements
(a, before ◦ after, b)
(a, before ◦ after )
(a, before)
(a, ⟨⟩) 2

1 We assume a universal set V al containing all possible values of variables.
2 ⟨⟩ describes the empty sequence.

with t = (before, after ). Consequently, the behavior of the above transition is the set consisting of all simple executions (Normal, before ◦ after, Failure), (Normal, before ◦ after ), (Normal, before), and (Normal, ⟨⟩), such that before(Err) = LV and before(Req) = No, as well as after (Res) = No.
Note that transitions need not explicitly assign a value to each of its variables from V ar; non-assigned variables in the pre- as well as post-condition are implic- itly treated as getting a value nondeterministically assigned. E.g., The request- transition in the Failure-location of Figure 2 with label Req?Vt : Res!Ft corresponds to a set of executions of the above form with before(Err) ∈ {OK, LV}.


Component Behavior
The description of a component capsules the transition system describing its be- havior. Graphically, it consists of the description of the transition system, e.g., provided by Figure 2, and by the description of its interface, e.g., provided by the Error Store element in Figure 1.
Therefore, including its transition system, a component is described by
declaring a (non-empty) subset Init ⊆ Loc of initial control locations; e.g., the location Normal, marked by a black dot in Figure 2
declaring a (non-empty) subset of InOut ⊆ V ar of input and output variables for exchanging signals with the environment; e.g., the input variables Err and Req as well as output variable Res, depicted by empty and filled circles in Figure 1
Based on these additional descriptions, the behavior of a component can be defined. For a state s : V ar → V al with V ar' ⊆ V ar, the notation s ↑ V ar' is used for restrictions (s ↑ V ar')(v) = s(v) for all v ∈ V ar'. This restriction is extended to sequences of states through point-wise application. For sequences r and t we use the notation r ◦ t to describe the concatenation of r and t.
First, the set of executions of a component is introduced. An execution is either a triple (a, t, b) consisting of a finite sequence t ∈ −V−a→r ∗ of states corresponding to
an execution starting at location a and ending at location b, changing variables according to t; or it is a pair (a, t) consisting of a finite sequence t of states, starting at location a. In the context of testable behavior a restriction to finite observations is sufficient. The set of executions is inductively characterized by all (a, t, b) and (a, t) with
(a, t, b) and (a, t) are simple executions of the transition system of the component
t = t1 ◦ s ◦ t2 is the concatenation of t1, s, and t2 such that (a, t1 ◦ s, c) and (c, s ◦ t2, b) as well as (c, s ◦ t2) are executions of the component for some b ∈ Loc.
Using the example of the Error Store, (Normal, s1 ◦ s2 ◦ s3, Failure) with
s1(Err) = OK, s1(Req) = No, s1(Res) = No
s2(Err) = OK, s2(Req) = Rs, s2(Res) = No
s3(Err) = LV, s3(Req) = No, s3(Res) = OK


Res?OK:
Req!No,Sts!Ft

Dia?Vt,Res?OK: Dia?Vt,Res?Ft: Req!Vt,Sts!OK Req!Vt,Sts!Ft

Bat?Lo,But?Up: Err!LV,Mot!Zr
But?Dn:Mot!Zr,Err!OK

Bat?Lo,But?Dn: Err!LV,Mot!Zr
But?Up:Mot!Zr,Err!OK



Up


Bat?Hi,But?Up: Mot!Up,Err!OK
Stop




Bat?Hi,But?Dn: Mot!Lo,Err!OK
Down

Res?Ft: Req!No,Sts!Ft
Dia?Rs,Res?OK: Dia?Rs,Res?Ft: Req!Rs,Sts!OK Req!Rs,Sts!Ft
But?Hd: Mot!Hi,Err!OK
But?Hd:Mot!Zr,Err!OK
But?Hd: Mot!Lo,Err!OK

Fig. 3. Behavior of the Diagnosis Management and Window Control Components

is a possible execution of the component.
In a second step, executions are reduced to observations. Since observations about a component are restricted to the interface of a component, locations as well as non-input/output-variables have to be removed from the executions to form observations. Therefore, the set of all observations o of a component is defined by the set of all executions (a, t, b) and (a, t) with
a is an initial interface location of the component, i.e., a ∈ Init 3
s is the restriction of t to the component interface, i.e., s = t ↑ InOut
b is an interface location of the component
Finally, the behavior of a component is the set Obs ⊆ I−−n−O−→ut ∗ of all its observations. Again using the example of the Error Store component, s1◦s2◦s3 as defined above is a possible observation of the component, since here InOut = Var and Normal ∈ Init .

Component Networks
To deal with the issues of system integration described in Section 1, the defini- tion of observations of components must be extended to hierarchical systems, i.e., components which themselves contain subcomponents communicating via common variables. Graphically, networks of components communicating over variables are described by component diagrams as shown in Figure 1. The communication via common variables is indicated by lines, linking output variables to input variables. For each non-hierarchical component its behavior is described by a transition sys- tem; e.g., the behaviors of Diagnosis Management and Window Control are shown in Figure 3.
As introduced in Subsection 2.2 in the case of the description of components, the interface InOut S of a network S in term of its input and output variables is a subset of the combined interfaces InOut Ci of its components Ci. As in the previous case, non-interface variables from ( i=1,...,n InOut Ci )\InOut S are considered to be hidden variables, used only for internal communication.
Like the interface of a network of components, the behavior of a network can be deduced from the behaviors of its components. An observation of the complete system S is obtained from the projection to the behavior of its subcomponents

3 As an extension, also initial values of output variables may be defined.





C1,... Cn. Formally,
Fig. 4. Reset-Set-Query-Test Case of the Error Store

(1)
t ↑ InOut S ∈ ObsS ⇔	 
i=1,...,n
t ↑ InOut Ci ∈ ObsCi


Generating Tests
In this section, test cases are introduced as (sets) of observations, defining the valid- ity of a test case of a component as its containment in behavior of the component. Furthermore, the notion of an integrated test case is introduced, demonstrating its ability to test a component behavior of the system level. Finally, its automatic generation using model checking is illustrated.
In this section, instead of using the set-based notation ObsC ⊆ I−−n−O−u−t→  from
∗
Section 2 for the observations of a component C with interface InOut C, an equivalent predicate-based notation C : I−−n−O−u−t→∗ → B is used via the characteristic predicate
C(t) ⇔ t ∈ ObsC

Test Case Speciﬁcation
Intuitively, a test case describes an observation of a component or system, i.e., a sequence of values sent and received on the interface of a component or systems. Since in Section 2, the behavior of a component or system is described as a set of observations about it, a test case simply is an element of that set. While basically such an element can be described by a simple (linear) transition system, often special diagrams are used.
Graphically, a test case can, e.g., be specified using variations of Message Se- quence Charts or Sequence Diagrams. Figure 4 shows the representation of such a test case, using timed Extended Event Traces as introduced in [12]. The test case
description consists of

a labeled life line described by a vertical line for the component participating in the test case, indicating the sequence of observations from top to bottom; e.g., the life line of the Error Store component
a set of interactions depicted as labeled arrows describing the exchange of data between the participating component as well as its environment; e.g., receiving the value Rs via variable Req from the environment (or test bed), or sending the value No via variable Res to the environment (or test bed)
a set of time regions delimited by dashed lines collecting simultaneous interactions within those regions; e.g., the region containing the simultaneous receiving of No and LV via Req and Err, resp., as well as the sending of OK via Res
Note that this technique can also be applied to describe the interaction between several components as well as their environment. Figure 5 shows such a description for a sequence of interactions of Error Store, Diagnosis Management and Window Control.
Formally, such a diagram can be described as an observation (or set of observa- tions) about the interface of the participating components; the diagram describes a sequence (or set of sequences) of states, with each state assigning a value to an interface variable according to its corresponding time region. Thus, e.g., Figure 4 describes an observation of length 5, with the third state assigning the values OK, No, and LV to the variables Res, Req, and Err, resp.
In the setting of components with input and output variables as described above, the description of a test case consists of two parts:
the specification of the input signals received by the system under test from the environment during the execution of the test
the specification of the output signals sent from the system under test to the environment during the execution of the test
The system under test the satisfies this test case, when the output actually provided by the system corresponds to the output prescribed by the test case, given the system received the input prescribed by the test case. Similar to the description of the components, a predicate-based notation is also used for test case via
T (t) d=ef t ↑ InC ∈ ObsT ↑ InC ⇒ t ↑ Out C ∈ ObsT ↑ Out C
for a set ObsT characterizing the behavior specified by a test case for a component
C with input variables In and output variables Out . 4
Intuitively, a test case and its graphical representation is interpreted as the set of observations about the component under test that correspond to the prescribed behavior, i.e., with either an input differing from the prescribed input or an output corresponding to the prescribed output. Thus, a test case characterizes all legal behaviors of that component under test; it therefore is a super-set of all the expected observations of the component.
Consequently, a test case is valid for a given component if the set of observations

4 The restriction t ↑ Var is extended to restrictions on sets of sequences via element-wise application.



Fig. 5. Composed Observation of ErrorStore, DiagnosisManagement, and WindowControl

corresponding to the test case contains the set of observations corresponding to the behavior of the component under test. If interpreting the description of the test case as well as the behavior of the component as predicates over observations over the alphabet of the component, the following definition of the validity of a test case is obtained.
Definition 3.1 Satisfied Test Case A system or component C satisﬁes a test case
T if
∀t ∈ −V−a−r→∗.C(t) ⇒ T (t)
For example, the sequence diagram in Figure 4 shows a possible behavior of the
Error Store specified by the transition system of Figure 2.

Integrated Test Cases
In the following, components C and B are subcomponents of system A, with in- terfaces Var C, Var B, and Var A, resp, where Var A ⊆ Var B ∪ Var C. Furthermore, C is assumed to be completely integrated into A, i.e., having no direct connec- tion to the environment of A; Figure 1 shows an example of such a situation with C corresponding to ErrorStore, B corresponding to DiagnoseManagement and Win- dowControl, and A corresponding to their composition. The following definition, theorem, and proof can be easily extended to the more general case with C being only partially integrated.
As mentioned in Section 1, during integration the interface of a component C embedded into the overall system might not be directly observable. Therefore, to ensure a certain property given in form of a test case T , indirect observation may be

needed. Intuitively, we are looking for test case S for the overall system – consisting of C and the rest of the system B – such that if the system under test fulfills that test case S, also C must fulfill test case T .
This concept of a test case for a component, integrated into a overall system to deduce a test case for the complete system, is captured in the following definition, again interpreting a test case as well as system specification as a predicate over the observations corresponding to the test case and system specification, resp.
Definition 3.2 Integrated Test Case A test case S is a test case for T integrated into B if 5
S(s) d=ef ∀t ∈ −V−a−r→∗.(B(s, t) ⇒ T (t))
For example in the case of the power window implemented by the network of Er-
rorStore, DiagnoseManagement, and WindowControl as shown in Figure 1, the reset-
set-query test case shown in Figure 4 cannot be directly validated since the interface of ErrorStore is completely hidden within the control unit. Any observation on its ports Err, Req, and Res can only be made indirectly over the interface of the control
unit.
The explicit characterization of an integrated test case S given by 3, which is especially suited for the application of text case generation as shown in Subsection 3.3, can also be equivalently characterized more implicitly:
S(t ↑ Var A) ⇔ (B(t ↑ Var B) ⇒ C(t ↑ Var C)
This characterization will be used in the following to verify the intended properties of an integrated test case.
Thus, to verify that Error Store actually satisfies the behavior prescribed by the
component test case, a different test case is needed, which is
ensuring the validity of the test case for Error Store
relying only on the behavior of Diagnose Management and Window Control, but not on the behavior of Error Store
reading and writing the values of Err, Req, and Res through the interface of the control unit.
Figure 5 shows the description of such an integrated test case. The greyed-out parts of the test case description correspond to execution of the reset-set-query test case of Error Store, which is not directly observable from the interface of the control unit; the solid parts correspond to the reading and writing of ports at the interface of the unit, performed, e.g., by the test-bed for the control unit.
This is reflected in the life lines of the integrated test case. While the interactions attached to the life line of Error Store correspond to the component test case shown in Figure 4, the interactions attached to the life lines of Diagnose Management and Window Control reflect their corresponding behavior as described by the transition systems in Figure 3.



5 The notation (r, s) for r ∈ −−−→∗ and s ∈ −−−→∗ with Var ∩ Var
= ∅ is used to denote the sequence t

Var 1	Var 2	1	2
of point-wise combined states with t ↑ Var 1 = r and t ↑ Var 2 = s.

If the values of the input ports Bat, But, and Dia are written as described by the integrated test case, the behaviors of Diagnosis Management and Window Control ensure that the corresponding values of the internal ports Err and Req are assigned to the values prescribed by the component test case. Symmetrically, the behavior of Diagnose Management (and Window Control) ensures that if the values of the output port Sts (and Mot) are read as described by the integrated test case, the values of the internal port Res are assigned to the values prescribed by the component test case.
A system test executable in a test-bed for the control unit can be obtained from Figure 5 simply by removing the greyed-out part corresponding to the component test. Note that the validity of the system test ensures the validity of the component test, as stated in the following theorem.
Theorem 3.3 Satisﬁed Integrated Test Case An integrated test case S actually is a positive system test for the component test, i.e., component C satisﬁes test case T if the system consisting of B and C satisﬁes S, provided such a test is possible within the context of B.
The proof of Theorem 3.3 can be preformed immediately using first-order logic.
Proof (Theorem 3.3) Assuming that
the system under test A consists of B and C
the system under test A satisfies test case S
S is a test case for T integrated into B
we then can conclude that
C satisfies test case T (restricted to the context of B) by means of
(2, 1) ⇒ ∀t.(B(t ↑ Var B) ∧ C(t ↑ Var C)) ⇒ S(t ↑ Var A)
(4) ⇒ ∀t.(B(t ↑ Var B) ∧ C(t ↑ Var C)) ⇒ (B(t ↑ Var B) ⇒ T (t ↑ Var C))
⇒ ∀t.(B(t ↑ Var B) ∧ C(t ↑ Var C)) ⇒ T (t ↑ Var C)
⇒ ∀t.B(t ↑ Var B) ⇒ (C(t ↑ Var C)) ⇒ T (t ↑ Var C)
and thus (2) also holds for C and T , within the context of B.	 
Note that an integrated test case may not exist, e.g., because B cannot ensure that only input values for C are used as required by T based on input values delivered by the environment; or because B cannot ensure that only output values of C are provided as prescribed by T based on output values delivered to the environment. In such a case, only the trivial solution S(t) = F exists.
Furthermore, the invalidity of the system test ensures the invalidity of the com- ponent test, as stated in the following theorem.
Theorem 3.4 Unsatisﬁed Integrated Test Case An integrated test case S actually is a negative system test for the component test, i.e., component C does not satisfy test case T if the system consisting of B and C does not satisfy S.

Again, the proof of Theorem 3.4 can be preformed immediately using first-order logic.
Proof (Theorem 3.4) Assuming that
the system under test A consists of B and C
C does satisfy test case T
S is a nontrivial test case for T integrated into B
we then can conclude that
the system under test does satisfy test case S
by means of
(2) ⇒ ∀t.C(t ↑ Var C) ⇒ T (t ↑ Var C)
⇒ ∀t.(B(t ↑ Var B) ∧ C(t ↑ Var C) ⇒ T (t ↑ Var C)
⇒ ∀t.(B(t ↑ Var B) ∧ C(t ↑ Var C) ⇒ (B(t ↑ Var B) ⇒ T (t ↑ Var C)) (4, 1) ⇒ ∀t.A(t ↑ Var A) ∧ S(t ↑ Var A)
and thus (2) also holds for A and S.	 

Implementation
To effectively use the definition of integrated test cases, (automatic) support for the generation of test cases is necessary. Since behavior is defined by (possibly infinite) sets of finite traces, test cases are defined as elements of such sets, and integrated test cases are defined via first-order expressions over those sets, a trace-based formalism is best-suited.
Therefore, here WS1S (weak second order monadic structure with one successor function) is used to implement automatic test case generation. This formalism is, e.g., supported by the model checker Mona [8]. Using WS1S, behavior is specified
by predicates over observation traces, which are – as defined in Subsection 2.2 – sequences of states, i.e. sequences of assignment of values to variables.
Intuitively, these predicates are built up from the most elementary propositions about observations, declaring that a variable has a certain value at a specific point in time in a observation. E.g., looking at the observation corresponding to the reset-
set-query-test case in Figure 4, a suitable proposition is “variable Req has value Vt
in the fourth state”. Using this approach, a trace can be precisely described by characterizing the states, for which a certain variable has a specific value. E.g., in the above example, a part of the description is characterized by “variable Req has value No in states 0,2,4, value Rs in state 1, and value Vt in state 3 ” (numbering states beginning with 0).
Using this approach, predicates are defined by characterizing a trace via the sets of state indices, for which these basic propositions – linking variables and val- ues – hold. WS1S provides variables (and quantors) over sets of indices, called second-order variables (or quantors) in contrast to zero-order and first-order vari- ables (quantors) for Booleans or indices. Using these second-order variables for sets

corresponding to all combinations of variables and values, the above test case can be formalized as
pred ErrorStoreTest(
var2 ErrOK, var2 ErrLV,
var2 ReqNo, var2 ReqVt, var2 ReqRs, var2 ResNo, var2 ResOK, var2 ResFt) =
with var2 declaring second-order parameters, and e.g., ReqNo corresponding to the set of indices characterizing states with Req = No. 6 Using the same principle, also the observations of transition-systems can be formalized in a similar fashion, leading to corresponding predicates for WinCtrl and DiagMgt for the WindowControl and DiagnosisManagement components.
The definition 3.2 of an integrated test case in Section 3.2 can be directly im- plemented in WS1S, allowing an automatic construction of the corresponding trace using the witness functionality provided by Mona:
pred SystemTest(
var2 BatLo, BatHi,
var2 ButDn, ButHd, ButUp, var2 DiaVt, DiaRs, DiaNo, var2 MotLo, MotZr, MotHi, var2 StsNo, StsOK, StsFt) =
all2 ErrOK, ErrLV:
all2 ReqNo, ReqVt, ReqRs:
all2 ResNo, ResOK, ResFt: ((
WinCtrl(BatLo, BatHi, ButDn, ButHd, ButUp, MotLo, MotZr, MotHi, ErrOK, ErrLV) &
DiagMgt(RqSRd, RqSBs, DiaNo, DiaVt, DiaRs, ResNo, ResOK, ResFt, ReqNo, ReqVt, ReqRs, StsNo, StsOK, StsFt))
=>		ErrorStoreTest(ErrOK, ErrLV, ReqNo, ReqVt, ReqRs, ResNo, ResOK, ResFt));
Using the above predicate SystemTest, Mona can check for its satisfiability, return- ing the following witness:
BatLo = {1} BatHi = {0,2,3,4,5}
ButDn = {} ButHd = {} ButUp = {0,1,2,3,4,5} DiaVt = {2} DiaRs = {0} DiaNo = {1,3,4,5} MotLo = {} MotZr = {0,2} MotHi = {1,3,4,5}

6 For reasons of readability disjointness assertions like ReqNo inter ReqRs ensuring unique signal values are skipped here.

StsNo = {1,2,4} StsOK = {0,3} StsFt = {5}


This corresponds to the observation in Figure 5, using the notions for sequence diagrams.
Conclusion
The previous sections gave a detailed account of the basics and techniques support- ing an automated method for integrating component tests into system tests. In this section, a short summary of the presented approach and an outlook into other possible areas of application of the basic techniques are given. Furthermore, it is set into perspective to other work on automatic test case generation.

Summary and Outlook
In the previous sections, an approach was introduced for the automatic generation of test cases for integrating a component into a larger overall system. This approach allows to combine a given test case for a component of a system with the behavioral description of the rest of the system, to analyse the behavior of that component without direct access to the interface of the component. This combination delivers a test case executable at the interface of the overall system, which verifies that the component satisfies the given component test case in case that the overall system satisfies the resulting system test case. A typical area for the application of such an approach is, e.g., the construction of embedded software; here, on the imple- mentation level often no means are provided of directly observing the interface of a component integrated in a larger system. This approach is especially useful in the context of system evolution, when only the component under test has been altered, leaving the rest of the system unchanged.
Note that instead of deducing an observation about the overall system from a given observation of a component plus the remainder of the system, also the opposite approach – deducing the required component behavior from the observed system behavior – has its merits in the context of system evolution. This is especially helpful in the context of fault localisation, e.g., when trying to identify possible faulty behavior of a changed component that can result in a faulty behavior at system level, identified during the testing of the otherwise unchanged system. To solve that problem of deducing a component observation from a system observation as well as the behavioral specification of the rest of the system, the same formalizations and techniques can be applied.

Related Approaches
In this paper we showed a further application for using a model checker in test case derivation – here we generated tests on the system level for testing a sole component. On component level, model-based test case generation by model checking is well- known: In [2] the SPIN model checker is used to verify execution traces and to

generate further test cases. Ammann et al. generate test cases by using the SMV model checker on mutants of the original specification [1]. In [10] model checking is used to generate test cases fulfilling structural coverage on the test model. A similar approach is presented in [4]. For test case generation in general various further techniques exist, as for example using constraint logic programing [9]. Regardless what specific technique is used, to our knowledge all model-based test generation techniques so far assume that the test model (from which test cases are generated) is given at the interface level of the system under test and the SUT’s interface is also the interface for test case execution. In contrast to these approaches the presented method in this paper uses test cases over the component interface and extends this test cases to the interface of a whole system in which the component is integrated in. Using model checking we are able to decide if a test case is able to unambiguously verify intended behavior or detect errors caused by the component under test (and not by other parts of the system).
The described technique in this paper is focused on supporting issues of inte- gration testing. Other approaches on integration testing include methods based on coverage criteria for integration testing like [5] (data-flow coverage) or [7] (coverage based on component coupling). In [3] mutation testing by mutants of the coupling on the internal interfaces is described as a method for integration testing. [6] gives an overview of various techniques of component testing which are able to be applied by the component user during system integration.

References
Ammann, P., P. Black and W. Majurski, Using model checking to generate tests from specifications, Formal Engineering Methods, 1998. Proceedings. Second International Conference on (1998), pp. 46–54.
Callahan, J., F. Schneider and S. Easterbrook, Automated software testing using modelchecking, in:
Proceedings 1996 SPIN Workshop, 1996, also WVU Technical Report #NASA-IVV-96-022.
Delamaro, M., J. Maidonado and A. Mathur, Interface mutation: an approach for integration testing, Software Engineering, IEEE Transactions on 27 (2001), pp. 228–247.
Hamon, G., L. de Moura and J. Rushby, Generating efficient test sets with a model checker, in: Software Engineering and Formal Methods, 2004. SEFM 2004. Proceedings of the Second International Conference on, 2004, pp. 261–270.
Harrold, M. and M. Soffa, Selecting and using data for integration testing, Software, IEEE 8 (1991),
pp. 58–65.
Jaffar-ur Rehman, M., F. Jabeen, A. Bertolino and A. Polini, Software component integration testing: A survey, Technical Report 2005-TR-41, Consiglio Nazionale delle Ricerche, Istituto di Scienza e Technologie dell’Informazione ”Alessandro Faedo” (ISTI), Pisa, Italy (2005).
Jin, Z. and A. Offutt, Integration testing based on software couplings, Computer Assurance, 1995. COMPASS ’95. ’Systems Integrity, Software Safety and Process Security’. Proceedings of the Tenth Annual Conference on (1995), pp. 13–23.
Klarlund, N. and A. Møller, “MONA Version 1.4 User Manual,” BRICS, Department of Computer Science, University of Aarhus (2001).
Pretschner, A. and H. L¨otzbeyer, Model Based Testing with Constraint Logic Programming: First Results and Challenges, in: Proc. 2nd ICSE Intl. Workshop on Automated Program Analysis, Testing and Verification (WAPATV’01), Toronto, 2001.
Rayadurgam, S. and M. Heimdahl, Coverage based test-case generation using model checkers, in: Engineering of Computer Based Systems, 2001. ECBS 2001. Proceedings. Eighth Annual IEEE International Conference and Workshop on the, 2001, pp. 83–91.

Sch¨atz, B., Modular Functional Descriptions, in: Proceedings 4th International Workshop on Formal Aspects fo Component Software (FACS’07), Nice, 2007, to appear in ENTCS.
Sch¨atz, B. and F. Huber, Integrating formal description techniques, in: J. M. Wing, J. Woodcock and
J. Davies, editors, FM’99 (1999), lNCS 1709.
