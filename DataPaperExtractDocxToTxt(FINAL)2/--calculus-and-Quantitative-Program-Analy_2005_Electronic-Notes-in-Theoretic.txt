Electronic Notes in Theoretical Computer Science 112 (2005) 5–18	
www.elsevier.com/locate/entcs


λ-calculus and Quantitative Program Analysis (Extended Abstract)
Chris Hankin
Department of Computing Imperial College London London, UK
Herbert Wiklicky
Department of Computing Imperial College London London, UK

Abstract
In this paper we show how the framework of probabilistic abstract interpretation can be applied to statically analyse a probabilistic λ-calculus. We start by reviewing the classical framework of ab- stract interpretation. We choose to use (first-order) strictness analysis as our running example. We present the definition of probabilistic abstract interpretation and use it to construct a probabilistic strictness analysis.
Keywords: Probabilistic λ-calculus, strictness analysis, probabilistic abstract interpretation.

Introduction
In this paper we aim to show how probabilistic abstract interpretation [8,9] can be used to analyse terms in a probabilistic λ-calculus. Our running example will be a simple strictness analysis [14,2]. This analysis has been used in the non-probabilistic setting to optimise lazy functional languages by allowing lazy evaluation to be replaced by eager evaluation without compromising the semantics. We suggest that, in the probabilistic setting, strictness analysis might be used to perform a more speculative optimisation which replaces


1571-0661 © 2004 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.01.016


lazy by eager evaluation as long as the risk of introducing non-termination is sufficiently low.
In order to illustrate how quantitative elements change classical analysis, we will present an example borrowed from the theory of stochastic processes (see Example 2.1 of [4]), which is related to economics and in particular to risk management.
Example 1.1 [Random Walk] A company starts with initial capital of Cap0, at each time step its income is Ini and its outlay to meet claims is Outi; the sequence of incomes and outlays are modelled by mutually independent and identically distributed variables. The fortunes of the company are modelled by a simple random walk with an absorbing barrier at 0 and jumps Stepn =
Inn − Outn:


Capn
= ⎧⎨ Capn−1 + Stepn if Capn−1 > 0 and Capn−1 + Stepn > 0
⎩ 0	otherwise

Qualitatively, we can analyse the random walk and just conclude that Cap ranges over the interval [0, ∞); quantitatively, we can ask the more interesting question: What is the probability of bankruptcy for a given statistical behaviour of claims and income? Obviously, one can ask similar questions also with respect to computational processes which in one way or another use limited computational resources.
The rest of this paper is organised as follows. We start by introducing the probabilistic λ-calculus. In the next Section we review the main features of classical abstract interpretation and show how the framework may be ap- plied to produce a strictness analysis for a first-order fragment of an applied λ-calculus. The paper [2] shows how these ideas can be extended to the higher-order case. We then present our approach to semantics based on linear operators and describe probabilistic abstract interpretation [8,9]. The final main section returns to the problem of strictness analysis in the probabilistic λ-calculus.

Probabilistic λ-calculus
A number of authors have introduced probabilistic features into the λ-calculus, see [17] and [16] for recent examples. Danos and Harmer, [6], show that most forms of probabilistic behaviour can be encoded using a coin flip. We follow this minimalist programme and simply extend the λ-calculus with the ability to make a binary, probabilistic choice between terms. We define, ΛP , the class of probabilistic λ-terms to be the least class defined by:

Each variable x is a term.
Each constant, including ⊥, is a term.
For M, N ∈ ΛP , (MN) ∈ ΛP and (λx.M ) ∈ ΛP .
For M, N ∈ ΛP , (M ⊕p N) for some probability p.
This is the usual λ-terms plus terms of the form M1 ⊕p M2 (indicating a probabilistic choice, the right hand summand being chosen with probability p).
We assume a leftmost reduction strategy. We write e1 →p e2 to mean that e1 reduces to e2 with probability p. Configurations, e, are a pair of a term and an environment; ρ maps Var to ΛP . The semantics of terms are given by the following reduction system:
(var)	(x, ρ) →1 ρ(x)
(M, ρ) →p (P, ρ')

(app)
((MN), ρ) →p
((PN), ρ')

(β)	((λx.M )N, ρ) →β (M, ρ[x := N])


(δ1)	((M ⊕p N), ρ) →δ
(M, ρ)

(δ2)	((M ⊕p N), ρ) →δ (N, ρ)
Terminal configurations have a term which is a λ-term or a constant. The
app and β rules together enforce the leftmost reduction strategy.

Classical Abstract Interpretation
Program analysis aims to determine some property of a program without run- ning it. A classical example is Reaching Deﬁnitions analysis which determines, for each node in a flowchart, which definitions (assignments) reach it [15]. The results of this analysis might be used to perform a constant folding transfor- mation of the program. Such transformations should be semantics preserving and it is therefore important that the analysis gives correct information about the program. Often the properties that we are interested in are undecidable and so correctness is replaced by some approximation notion (see below).
We start by sketching the classical approach to semantics-based program analysis: abstract interpretation [3,15]. The semantics of a program f identi- fies some set V of values and specifies how the program transforms one value v1 to another v2: f ▶ v1 −→ v2.


In a similar way, a program analysis identifies the set L of properties and specifies how a program f transforms one property l1 to another l2: f ▶ l1 Dl2.
As we have seen, every program analysis should be correct with respect to the semantics. For first-order program analyses, i.e. those that abstract properties of values, this is established by directly relating properties to values using a correctness relation: R : V × L → {true, false}.
The intention is that v R l formalises our claim that the value v is described by the property l.
To be useful one has to prove that the correctness relation R is preserved under computation: if the relation holds between the initial value and the initial property then it also holds between the final value and the final property.
This may be formulated as the implication

v1 R l1 ∧ f ▶ v1 −→ v2 ∧ f ▶ l1 D l2 ⇒ v2 R l2

The most common scenario in abstract interpretation is when both V and L are complete lattices. We then impose the following relationship between R and L:

v R l1 ∧ l1 ± l2 ⇒ v R l2
(∀l ∈ L' ⊆ L : v R l) ⇒ v R . L' 

The first of these concerns safety [15]: if we have a property which correctly describes a value, then any larger property is also a safe description. The second concerns the existence of best descriptions: if we have a set of properties that correctly describe a value then their meet will also be a correct description and is more accurate.
The correctness relation is often achieved via a Galois connection: (V, α, γ, L) is a Galois connection between the complete lattices (V, ±) and (L, ±) if and only if α : V → L and γ : L → V are monotone functions that satisfy: γ ◦ α ± λv.v and α ◦ γ ± λl.l.
Having defined a suitable “set” of properties we then define suitable inter- pretations of program operations. The framework of abstract interpretation guarantees that the analysis will be safe as long as we use an interpretation, Fabs, of each language operator, F, that satisfies: Fabs ± α ◦ F ◦ γ.
Since interesting languages involve iteration or recursion we also have to
construct efficient implementations; a generic solution to this problem is the theory of widenings and narrowings [3].

Strictness Analysis

Strictness analysis [14,2] aims to answer for some function f: Does f ⊥ = ⊥? If the function has this property then it either uses its argument or is the bottom function. In either case, an affirmative answer would mean that arguments can be passed by value rather than using (the more costly) lazy evaluation. We will restrict ourselves to a first order functional language with integers as the only data type.
We can construct a Galois connection (УH (Z⊥), α, γ, Two) where УH is the Hoare Powerdomain construction and Two is {0, 1} ordered by 0 ± 1. The elements of the Hoare Powerdomain in this case are just down-closed sets ordered by subset inclusion: i.e. every set contains ⊥.
We define:

α(Z) = ⎧⎨ 0 if Z = {⊥}
⎩ 1 otherwise
γ(S) = ⎧⎨ {⊥} if S =0 
⎩ Z⊥  if S =1 


We can construct the induced operations that correspond to the operations in this first-order applied λ-calculus:

The conditional takes three arguments (x, y, z); the predicate must be defined and then the result is at least as defined as either of the branches. Thus the abstract interpretation of

(λ x.if x =0 then 15 else 42)

is (λ x.(xH1)H(1H1)) ≡ λx.x. Since (λ x. x) 0 = 0 this tells us that our original function is strict. We now extend this approach to a probabilistic λ-calculus. We could just apply the classical framework to this new setting [12,13]; instead we will apply the techniques of Probabilistic Abstract Interpretation [8,9].

Linear Representations

The vector space V(X) over a set X is the space of formal linear combinations of elements in X with coefficients in some field W (e.g. W = R), i.e.
V(X)= Σ cx→x | cx ∈ W, x ∈ X, .
The semantics of terms in our extended calculus can be represented by a probabilistic reduction graph where edges are labelled with probabilities. We associate to each quantitative relation R ⊆ X × W × X a matrix, i.e. a linear operator MR on V(X) defined by:
⎧⎨ w iff ΣR(x ,w',x )w' = w

(MR)ij =
i	j
⎩ 0 otherwise

For probabilistic relations W = [0, 1] and this gives a Stochastic matrix.


Linear Semantics for the Probabilistic λ-calculus
We start by defining three operators which represent transitions corresponding to β-reduction, δ-reduction (for the probabilistic choice) and idling (for terms in normal form) respectively. Each operator is of type V(ΛP ) → V(ΛP ).
Before defining the operator for one-step β-reduction, we define an appro- priate notion of active context – these are λ-terms with a single hole which determines where the next redex to be reduced is to be found. Such contexts are used in the definition of compatible closure in the standard construction of one-step reductions [1]; since we are interested in call-by-name evaluation, we do not reduce redexes in the argument or under λs. Given this intuition, C[ ], the class of active contexts with a single hole is the least class such that:
[ ] ∈ C[ ].
C1[ ]N ∈ C[ ] for any C1[ ] ∈ C[ ] and N ∈ ΛP .
The operator B (for β reduction) has the following matrix representation for each bound variable x and term N ∈ ΛP :


B(x, N)


t1 ,t2
= ⎧⎨ 1 if t1 ≡ C[(λx.M )N], t2 ≡ C[M[x := N]]
⎩ 0 otherwise

The operator C for the probabilistic choice operator has the following



matrix representation:
⎧⎪ (1 − p) if t1 ≡ C[M ⊕p N], t2 ≡ C[M]

Ct1 ,t2 = ⎪⎨ p	if t1 ≡ C[M ⊕p N], t2 ≡ C[N]
⎪⎪⎩ 0	otherwise
Finally, the idling operator, N (for normal forms), has the following matrix representation:


Nt1,t2
= ⎧⎨ 1 if t1 ≡ t2, t1 is a βδnf
⎩ 0 otherwise

The operator, T, which describes the transitions available from a term is then defined as:
T = C + N + Σx,N B(x, N)
In practice we will restrict to the reachable terms from some given term,
M:
R(M)= {N | M →∗ N}
and work with the restricted transition operator:

T = πM TπM
where πM is the projection on to V(R(M)).
The semantics of a term is then recovered by iterated application of T to

the vector M→
representing the initial term M:
lim TiM→ . 
i→∞

If we have only finitely many reachable terms, e.g. if the reduction of a given term M terminates after a finite number of reduction steps, the reduced operator T operates (in effect) on only a finite dimensional sub-vector space of V(ΛP ). It is well known that all topologies on finite dimensional vector spaces which are compatible with the algebraic structure are equivalent[10, Section 7.18]. This means that for finite reductions the re-construction of the operational semantics will lead to the same limit, independently of the topology considered.
It is also possible to extend the construction to the case of non-terminating reductions, or more generally the case of infinitely many reachable terms. However, this requires a more careful consideration of the topological structure used to construct the limit. In order to keep our presentation succinct we will omit here a detailed investigations of this situation as it requires more elaborate concepts from functional analysis and operator theory.

Example 4.1 Consider the term:
((λx.0) ⊕ 1 (λx.x))(⊥ ⊕ 3 42)
	
2	4
An enumeration of the reachable terms is:
((λx.0) ⊕ 1 (λx.x))(⊥ ⊕ 3 42)
	
2	4
(λx.0)(⊥ ⊕ 3 42)
4
(λx.x)(⊥ ⊕ 3 42)
4
0
⊥ ⊕ 3 42
4

⊥
42
and we have:


⎛ 0 1 1 0 0 0 0 ⎞

2 2
⎜	⎟
0 0 0 1 0 0 0
 	⎜ 0 0 0 0 1 0 0 ⎟
T = ⎜ 0 0 0 1 0 0 0 ⎟
⎜ 0 0 0 0 0	⎟
4 4
⎜	⎟
0 0 0 0 0 1 0
0 0 0 0 0 0 1
Probabilistic Abstract Interpretation
Given two probabilistic domains (i.e. Hilbert spaces over λ-terms), C and D, a probabilistic abstract interpretation [8,9] is a pair of linear maps, A : C '→ D and G : D '→ C, between the concrete domain C and the abstract domain D, such that G is the Moore-Penrose pseudo-inverse of A, and vice versa. Let C and D be two Hilbert spaces and A : C '→ D a bounded linear map between them. A bounded linear map A† = G : D '→ C is the Moore-Penrose pseudo-inverse of A iff
A ◦ G = PA and G ◦ A = PG
where PA and PG denote orthogonal projections onto the ranges of A and G. Alternatively, if A is Moore-Penrose invertible, its Moore-Penrose pseudo-
inverse, A† satisfies the following:
AA†A = A,

A†AA† = A†,
(AA†)∗ = AA†,
(A†A)∗ = A†A.
where M∗ is the adjoint of M. It is instructive to compare these equations with the classical setting. For example, if (α, γ) is a Galois insertion: α◦γ◦α = α and γ ◦ α ◦ γ = γ.
A simple method to construct a probabilistic abstract interpretation is as follows: Given a linear operator Φ on some vector space V expressing the probabilistic semantics of a concrete system, and a linear abstraction function A : V '→ W from the concrete domain into an abstract domain W, we com- pute the (unique) Moore-Penrose pseudo-inverse G = A† of A. The abstract
semantics can then be defined as the linear operator on the abstract domain
W:
Ψ= A ◦ Φ ◦ G = GΦA.
In the case of classical abstract interpretation the abstract semantics con- structed in this way is guaranteed to be correct. In our quantitative setting the induced abstract semantics is the one closest to the concrete semantics. This is due to the relation between the Moore-Penrose pseudo-inverse and so-called the least-square approximation, cf e.g. [7,5].
To be more precise: Let C and D be two finite dimensional vector spaces, A : C '→ D a linear map between them, and A† = G : D '→ C its Moore- Penrose pseudo-inverse. Then the vector x0 = yG is minimising the distance between xA for any vector x in C and y, i.e.

inf
x∈C
 xA − y  = x0A − y  .

In other words, if we consider the equation xA = y we can identify a (exact) solution x∗ as a vector for which x∗A − y  = 0. In particular in the case that no such solution vector x∗ exists we can generalise the concept of a exact solution to that of a “pseudo-solution”, i.e. we can look for a x0 such that x0A is the closest vector to y we can construct. This closest approximation to the exact solution is now constructed using the Moore-Penrose pseudo-inverse,
i.e. take x0 = yA†.
Returning to our program analysis setting, suppose that we have an op- erator Φ and a vector x. We can apply Φ to x and abstract the result giv- ing xΦA or we can apply the abstract operator to an abstract vector giving xAA†ΦA. Ideally, we would like these to be equal. If A is invertible then
its Moore-Penrose pseudo-inverse is identical to the inverse and we are done.
In program analysis A is never a square matrix (there is always some loss of precision) and thus AA† in xAA†ΦA will lead to some loss of precision.


The Moore-Penrose pseudo-inverse is as close as possible to an inverse if the matrix is not invertible and thus for the particular choice of A, A†ΦA is the best approximation of Φ that we can have.




Probabilistic Strictness Analysis

In many cases, and particularly in strictness analysis, the abstraction is a surjective function. An alternative view of abstraction in this case is that it maps concrete values to equivalence classes. Equivalence relations can be represented by a particular kind of operator: a classification operator.
We call an n × m-matrix K a classiﬁcation matrix if it is a 0/1-matrix, where every row has exactly one non-zero entry and columns have at least one non-zero entry. Classification matrices are thus particular kinds of stochastic matrices. We denote by K(n, m) the set of all n × m-classification matrices (m ≤ n). Let X = {x1,... , xn} be a finite set. Then for each equivalence relation ≈ on X with |X/≈| = m, there exists a classification matrix K ∈
K(n, m) and vice versa. Each column in the classification matrix represents a
(non-empty) equivalence class.
The pseudo-inverse of a classification matrix K ∈ K(n, m) corresponds to its normalised transpose or adjoint (these coincide for real K).
K† = U (KT ).
where the normalisation operation U is defined for a matrix A by:



U (A)ij =

Aij
ai
if ai = Σj
Aij /=0 

, 0	otherwise.

A suitable abstraction for probabilistic strictness analysis classifies terms as undefined, don’t know or defined. We abstract every term in the enumer- ation to one of these values. The don’t know value is used to classify terms whose definedness is not yet determined. Classically 0 represents definite non- termination whilst 1 represents possible termination. The use of three values here allows a more informative analysis – the defined value means deﬁnitely terminating. This abstraction is achieved by a classification operator.

Example 6.1 A suitable classification matrix for our running example is
⎛ 0 1 0 ⎞
⎜ 0 1 0 ⎟
⎜ 0 1 0 ⎟
K = ⎜ 0 0 1 ⎟
⎜ 0 1 0 ⎟
⎜⎝ 1 0 0 ⎟⎠
which has Moore-Penrose pseudo-inverse
⎛ 0 0 0 0 0 1 0 ⎞
K† = ⎜ 1	0	0 0 ⎟
⎝ 4 4 4	4	⎠
The abstract semantics of our original program is
⎛ 1 0 0 ⎞



K†TK =	 1  1
⎝ 16 2
 7 
16 ⎠

0 0 1
The middle row and column represent the don’t know value. The value in the middle row, middle column gives a bound on how much the other two values in that row might change when we iterate – in this sense, it gives a measure of the precision of the current abstract operator. Iterating this abstract operator causes the probability of a transition from don’t know to don’t know to decrease rapidly; for example after three iterations we have:
⎛ 1 0 0 ⎞



(K†TK)3 =	 7 
⎝ 64
1 49

8 64 ⎠

0 0 1
Achieving a defined outcome becomes more and more likely. This result could be used to support the decision to speculatively evaluate the argument.
One advantage of interpreting relations as linear operators allows us to measure them. The standard way to measure the “size” of a linear operator

is via an operator norm which in turn may have its origins in a vector norm:
 →x  ≥ 0
 →x  =0 ↔ →x = →o
 α→x  = |α|→x 
 →x + →y  ≤  →x  + →y 
For example, we could use the 1-norm (sum of absolute values), euclidean norm (square root of the sum of squares of absolute values) or the supremum norm (supremum of absolute values).

Example 6.2 An accurate abstraction of the original program, computed from the reduction graph, is:
⎛ 1 0 0 ⎞

⎝ 8	8 ⎠

Considering the difference between this and our first abstraction we get:
1
T# − K†TK ∞ = 
2
whilst

T# − (K†TK)3 ∞ = 
8

We have abstracted T but we could also iterate this operator.


Example 6.3 We find that:

⎛ 0 0 0 1 0 1 3 ⎞

2	8 8
⎜	⎟

⎜ 0 0 0 0 0	⎟
 
⎜	4 4 ⎟

⎜ 0 0 0 0 0	⎟
4 4
⎜	⎟
0 0 0 0 0 1 0
0 0 0 0 0 0 1



The abstraction of this is:
⎛ 1 0 0 ⎞

†3	⎜  5 	27 ⎟



and
(K T K)= ⎜⎝ 32 0 32 ⎟⎠


T# −	†3	1
32
Finally, it should also be noted that:
†3	†	3	1
 K T K − (K TK)  ∞ = 8
Conclusions
We have reviewed the classic approach to abstract interpretation and also shown that Di Pierro and Wiklicky’s notion of probabilistic abstract interpre- tation is a natural analogue of the classical framework. We have illustrated the approach for the λ-calculus in the context of a simple strictness analysis. A present shortcoming of our work is that neither the linear semantics nor the strictness analysis are defined in a compositional way. If we were able to give a compositional linear semantics, we would expect that compositional strictness analysis would be straightforward. Unfortunately this has to remain work for the future but it is possible that earlier work on the relationship between λ-calculus and operator algebras [11] could help in this endeavour.

References
H. P. Barendregt. The Lambda Calculus: Its Syntax and Semantics. North-Holland, 1981.
G. Burn, C. Hankin and S. Abramsky. The Theory and Practice of Higher-order Strictness Analysis. Science of Computer Programming, 1986.
P. Cousot and R. Cousot. Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixedpoints. In Proceedings of POPL, pages 238–252, 1977. ACM.
D. Cox and H. R. Miller. The Theory of Stochastic Processes. Chapman and Hall. 1965.
Ben-Israel, A., Greville, T.: Generalised Inverses — Theory and Applications. second edn. Springer Verlag, New York — Berlin (2003)
V. Danos and R. Harmer. Probabilistic Game Semantics. ACM Transactions on Computational Logic, 2001.
F. Deutsch. Bet Approximation in Inner Product Spaces, volume 7 of CMS Books in Mathematics. Springer Verlag, New York — Berlin, 2001.


A. Di Pierro and H. Wiklicky. Concurrent Constraint Programming: Towards Probabilistic Abstract Interpretation. In Proceedings of PPDP’00, 2000. ACM.
A. Di Pierro and H. Wiklicky.  Measuring the precision of abstract interpretations.  In
Proceedings of LOPSTR’00, LNCS 2042, 2001. Springer Verlag.
W.H. Greub. Linear Algebra, volume 97 of Grundlehren der mathematischen Wissenschaften. Springer Verlag, New York, third edition, 1967.
P. Malacaria and L. Regnier. Some results on the Interpretation of λ-calculus in Operator Algebra. In Proceedings of LICS, pages 63–72, 1991, IEEE Press.
D. Monniaux. An abstract Monte-Carlo method for the analysis of probabilistic programs (extended abstract). In Proceedings of POPL, pages 93–101, 2001. ACM.
D. Monniaux. Backwards abstract interpretation of probabilistic programs. In Proceedings of ESOP ’01, number 2028 in LNCS. Springer Verlag, 2001.
A. Mycroft.	Abstract Interpretation and Optimising Transformations for Applicative Programs. Ph.D. Thesis, University of Edinburgh, 1981.
F. Nielson, H. Riis Nielson and C. Hankin. Principles of Program Analysis. Springer Verlag, 1999.
Sungwoo Park. A calculus for probabilistic languages. ACM SIGPLAN Notices, 37(9), pages 206–217, 2002. ACM.
N. Ramsey and A. Pfeffer. Stochastic lambda calculus and monads of probability distributions. In Proceedings of POPL, pages 154–165, 2002. ACM.
