	Electronic Notes in Theoretical Computer Science 199 (2008) 89–106	
www.elsevier.com/locate/entcs

Redundancy Elimination for LF

Jason Reed1
Carnegie Mellon University Pittsburgh, Pennsylvania jcreed@cs.cmu.edu


Abstract
We present a type system extending the dependent type theory LF, whose terms are more amenable to compact representation. This is achieved by carefully omitting certain subterms which are redundant in the sense that they can be recovered from the types of other subterms. This system is capable of omitting more redundant information than previous work in the same vein, because of its uniform treatment of higher- order and first-order terms. Moreover the ‘recipe’ for reconstruction of omitted information is encoded directly into annotations on the types in a signature. This brings to light connections between bidirectional (synthesis vs. checking) typing algorithms of the object language on the one hand, and the bidirectional flow of information in the ambient encoding language. The resulting system is a compromise seeking to retain both the effectiveness of full unification-based term reconstruction such as is found in implementation practice, and the logical simplicity of pure LF.
Keywords: Proof Compression, Dependent Type Theory, Bidirectional Type Checking


Introduction
The use of logical frameworks in domains such as proof-carrying code [7] makes the efficiency of proof representation and manipulation a nontrivial issue. Proofs of safety for realistic programs can be, if na¨ıvely represented, unfeasibly large. Necula and Lee [8] developed one technique which addressed this issue. They give a way of representing proof terms in the logical framework LF [1] in a more efficient way, by rewriting them with whole subtrees of the proofs erased. They then describe an algorithm which recovers these omitted parts, using typing information found in other parts of the proof.
Their experimental results are good: proofs so represented tend to have size roughly O(√n) of the originals, with similar improvements in checking time.
To get a flavor of how omission works, consider the following example of encoding

1 This work was supported by NSF Grant CCR 0306313 “Efficient Logical Frameworks”.

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.11.014

a natural deduction proof theory in LF . We have a signature
o : type	pf : o → type
⊃: o → o → o	∧ : o → o → o
where o is declared as the type of propositions, pf is the type family of proofs, indexed by proposition, and ⊃ and ∧ are the familiar logical connectives. Take one of the two natural deduction elimination rules for ∧:
A ∧ B ∧E
A
In LF it becomes
ande1: Πa:o.Πb:o.pf (∧ a b) → pf a
Consider a use of this proof rule, ande1 a b d. Here d must be a derivation of a ∧ b, and this larger proof ande1 a b d is a proof of a. This is excessively verbose, in a sense: knowing what type d is supposed to have (that is, pf (∧ a b)) reveals what a and b must be. We would like to just write ande1 d. It is not at all obvious, however, that the object d itself uniquely determines its type. This is a central issue, and we return to it below.
Another sort of apparent redundancy appears if we examine the introduction rule for implication. The natural deduction rule is

A
B·
A ⊃ B ⊃I
The hypothetical derivation of B under the hypothesis A is represented by higher-order abstract syntax [9] as a function from pf a to pf b, and the rule is encoded as
impi : Πa:o.Πb:o.(pf a → pf b) → pf (⊃ a b)
Here we may notice that if we havea term impi a b f , and if we know it, as a whole, is being checked against a certain type, pf (⊃ a b) then we can read off what a and b have been. If we knew that the type is going to be provided ‘by the environment’ somehow, then we can simply write impi f instead.
It is this sort of omission of arguments that LFi obtains its savings from. How- ever, the technique uses a notion of ‘reconstruction recipes’ external to the type system to control which arguments are omitted. This work aims to put the basic idea of Necula and Lee on firmer type-theoretical footing, explaining the mechanism of omission in the types themselves. We describe an extension of the LF language,
called LF∗, such that the same sort of arguments to type families and constants can be omitted. Our priorities are, in order, (1) making sure that the extension is conservative, (2) making the theory logically well-motivated, (3) making sure

that an eventual implementation is simple and easy to trust, and only finally (4) maximizing the number of subterms that can be omitted omitted.
It should be noted that this general idea of ‘implicit’ syntax is not new: It can be found in the earlier work of Hagiya and Toda [3] with LEGO, and Miquel [6] and Luther [4] with the Calculus of Constructions.
However, some approaches (such as [6]) do not treat implicit terms as anything more than a user-interface convenience. Though the front-end reconstructs argu- ments omitted by the user, and erases them once again when terms are printed, the core of the implementation works with fully explicit terms. The meaning of the implicit calculus is in any event deﬁned in terms of the explicit calculus: an implicit term is well-typed if it can be elaborated uniquely into an explicit term. Both [4] and [3] agree that it seems “difficult to directly give a foundation to the implicit calculus.” That is exactly the aim of this work.
The remainder of the paper is structured as follows. We first present the type theory of LF∗, followed by a description of a decision procedure for the judgments therein. The proof of correctness of this algorithm is sketched. We give a description of a translation from LF to LF∗ and argue that it preserves typing and is bijective on terms, so that it witnesses the equivalence of the new language and the old.

Type Theory
The two critical questions left unanswered in the introductory example are when does an object uniquely determine its type? and when do we already know, from the surrounding context, what type an object must have? These are answered by organizing the language and type-checking algorithms of a system so as to support bidirectional type-checking.
The terms are divided into normal terms, which can be type-checked if a type is provided as input, atomic terms, which can be type-checked in such a way that uniquely determines (one says it synthesizes) a type as output if type-checking suc- ceeds. Ordinarily in λ-calculi, we know that functions are normal, and application of a constant or variable to a list (or spine) S of arguments is atomic. That is, our grammar of terms looks something like
terms M ::= N | R normal N ::= λx.M atomic R ::= x · S | c · S spines S ::= () | (M ; S)
Our reasoning about the example, however, suggests that we may want some constants c — such as impi from the example — to require that c · S receive a type as input before type-checking proceeds, so that some omitted arguments in S can be recovered. We divide, therefore, the constants into two halves, the synthesizable
constants c+ and the checkable constants c−. Therefore we write ande1+ instead of ande1, and impi− instead of impi, for the latter will depend on the ‘inherited’

type information for reconstruction, where the former does not. In general a spine headed by a c− constant is a normal term, rather than atomic.
Now we have a further problem, however. The fate of constants such as ande1
is in doubt, because they require certain of their arguments to be synthesizing. What if the proof we have in mind of A  B uses impi− as its last step? There are two conflicting requirements: ande1+ wants to get type information from impi− to proceed with reconstruction, and vice versa.
We fill this gap by allowing type ascriptions to appear inside spines, so that when an argument does not provide its type, and the constant which it is an argument of requires it, the type can be simply written down in the term. We make a production rule for spine elements
E ::= M | M + | ∗ 
which says that an argument may either be an ordinary term, a term which is adequate when a synthesizing term is required, (see immediately below) or else a placeholder for an omitted argument. Spines are then given by
S ::= () | (E; S)
Terms are now
M ::= N | R
—
N ::= λx.M | c  · S
+
R ::= x · S | c  · S
and the M + used above has the production
M + ::= (N : A) | (R :)
The new syntax (R :) here seems peculiar: it would seem more natural to put simply
R. For an atomic term is adequate when a synthesizing term is required, and so is a normal term with a type ascription. However, when we define substitution, it is necessary to know syntactically when we come across an atomic argument in a spine, whether it is in a position that actually requires a synthesizing term or not. The (R :) signals that if substitution produces a normal term, then a type ascription must be introduced.
Now we turn to the language of types in LF∗. They are given by the grammar basic types A, B ::= a · S | Π−x:A.B
general types Z ::= a · S | Πρx:A.Z omission modes μ ::= s | i
polarities σ ::= + | − 
Π-annotations ρ ::= σ | [μ]
Expanding out the grammar, there are four dependent function types, each of which determines how its argument functions with regard to omission and recon- struction. The Π− functions are just the ordinary dependent functions from LF .

They receive a  superscript to make them stand in contrast with Π+, which require their argument to be synthesizing. When there are Π+ arguments, earlier arguments may be omitted via making their functional dependency Π[s], which marks a func- tion whose argument is omitted by synthesis. Finally, Π[i] indicates a function whose argument is omitted by inheriting it from the result type the function application is checked against. In this language, the types of the proof rules in the example are (writing A → B for Π−x:A.B when x doesn’t appear in B)
ande1: Π[s]a:o.Π[s]b:o.Π+pf (∧ a b).pf a
impi : Π[i]a:o.Π[i]b:o.(pf a → pf b) → pf (⊃ a b)
Note that there is a distinction between the A, B are ‘basic’ types, which variables in a context may have, and Z which are the more general types that c− constants can have. It would be more felicitously uniform if we could have simply one notion of type which constants and variables shared, but so far we have not been able to
overcome the technical difficulties that arise when function variables are allowed to omit some of their arguments.
We elide for space reasons the grammar for kinds, and often refrain from men- tioning the cases for kinds in the results below. Extending the definitions and results to that level is easy and uninteresting. Sometimes it is useful to write W, V as a ‘wildcard’ standing for a term or type or kind, for a briefer treatment of judgments and statements that are relevant for all three levels.
Substitutions
We elect a style of presentation which follows that of the concurrent logical frame- work CLF [12], in that we keep all terms in canonical form, that is, β-normal η-long form. This saves us from the complexity of dealing directly with βη-convertibility and the ensuing complex logical relations proofs of decidability of equality (for an example, see [2]) This complexity doesn’t wholly disappear, though it reappears in a more tractable form: it is delegated to the definition of substitution. Substitution of a normal term in for a variable may create a redex, and the definition of substi- tution must carry out the reduction to ensure that the result is still canonical. To show that this process terminates we must pay attention to the decrease in the size of types of redices, logically parallel to the induction in structural cut elimination [10]. For this reason, CLF indexes the substitution operators with the type at which they operate. In fact, to show just termination of the substitution algorithm, only the skeleton of the type is required, but for our purposes, we need the full type for an independent reason.
Namely, it is possible that a variable-headed term, say, x · () appears in a spine in a position which needs to be synthesizing. As the matter stands, this is perfectly acceptable, for variables applied to spines are synthesizing. However, we may sub- stitute a term for x, say c− · (), that produces a result which no longer synthesizes. Therefore, before we set out on the substitution, we must specify what type the substituted object has, so that we can create a type ascription to ensure that the

result synthesizes.
We define, therefore, partial operations [M/x]AM ', [M/x]AA', [M/x]AS, sub- stitution of M for x in M ', A', S, respectively, at the type A. Since substituting for a variable in a synthesizing term may require wrapping it in a type, we have have a σ-indexed partial operations [M/x]AR. When σ is plus it outputs an M +, and when it’s −, an M . The operation [M · S]A resolves the redex M · S, for M at type A, and similarly produces an M  or M according to σ.
To see that this definition is well-founded, one can analyze the simple type of the type in the superscript, that is, the result of erasing all dependencies and changing every Π to a mere →.
The term subj(M +) is defined by subj(R :) = R and subj(N : A) = N . We
write [M +/x]A to mean [subj(M +)/x]A.
[M · ()]a·S = M
[R · ()]a·S = (R :)
[N · ()]a·S = (N : a · S)
[λx.M · (M '; S)]Π− x:A.B = [[M '/x]AM · S][M/x]AB
σ	σ

[M/x]Ax · S = [M · [M/x]AS]A
σ	σ
[M/x]Ay · S = y · [M/x]AS
[M/x]Ac+ · S = c+ · [M/x]AS

[M/x]Ac− · S = c− · [M/x]AS [M/x]Aλy.M = λy.[M/x]AM [M/x]AR = [M/x]AR

[M/x]Atype = type
[M/x]Aa · S = a · [M/x]AS
[M/x]A(Πρy:B.Z)= Πρy:[M/x]AB.[M/x]AZ

[M/x]A() = ()
[M/x]A(E; S)= ([M/x]AE; [M/x]AS)

[M/x]A(R :) = [M/x]AR
[M/x]A(N : B)= ([M/x]AN : [M/x]AB)

[M/x]A∗ = ∗

Strictness
We have still so far neglected to pin down formally what it means for, say, one argument to have a sufficiently good occurrence in another argument to allow the former to be omitted. We can see that clearly a has an occurrence in pf (∧ a b) in such a way that we can ‘read it off,’ but the general higher-order case can be more
complicated. The variable simply appearing in the syntax tree of the type is not enough, for the process of substituting in other arguments may cause β-reductions which make that appearance vanish. We therefore need to define strict occurrences, so that an argument which strictly occurs in the type of a synthesizing argument, or in the result type of a c− constant, may be safely omitted.
The definition of strict occurrences that follows closely follows the definition of
Pfenning and Schu¨rmann [11] used to describe the theory of notational definitions. The notion of pattern spine at the heart of it is originally due to Miller [5]. The guiding idea is that a strict position cannot be eliminated by other substitutions, and that, as a result, the operation of substituting [M/x]N is injective in the argument M when x is strict in N . This injectivity means that we can uniquely recover M from [M/x]N . That is, the important consequence is that the corresponding matching problem is decidable and has a unique closed solution.
A key limitation of the way strictness is defined here, from the standpoint that more strict occurrences means more opportunities to omit redundant information, is that x cannot generally have a strict occurrence in (∗, S), even if it does have a strict occurrence in S. This is because we actually need more than just the term
being uniquely determined when it is substituted for a strictly occurring variable: for technical reasons in the unification algorithm, we need its type to be uniquely determined as well.
The strictness judgments are Γ Hs x ∈ Z, (x has a strict occurrence in some argument of the type Z) Γ Hi x ∈ Z, (x has a strict occurrence in the output of the type Z) Γ; Δ H x ∈ W , (x has a strict occurrence in W in the presence of local bound variables Δ) and Δ ▶ S pat. (S is a pattern spine, that is, a sequence of distinct bound variables)


Top-level
 Γ; · H x ∈ S 
Γ Hi x ∈ a · S

	Γ; · H x ∈ A	
Γ Hs x ∈ Π+y:A.B




Types
Γ,y : A Hμ x ∈ B

Γ Hμ x ∈ Πρy:A.B

 Γ; Δ H x ∈ S 
Γ; Δ H x ∈ a · S

  Γ; Δ, y H x ∈ B			Γ; Δ H x ∈ A	
Γ; Δ H x ∈ Πρy:A.B	Γ; Δ H x ∈ Πρy:A.B




Pattern Spines
Since all terms are in η-long form, define x →∗
H (“H is an η-expansion of the

variable x”) by
y1 →∗ H1	···	yn →∗ Hn


x →∗ λy1 ... λyn.x · (H1; ··· ; Hn)
Then the definition of pattern spine is
x →∗ H	Δ1, Δ2 ▶ S pat



Terms
Δ ▶ () pat
Δ1, x, Δ2 ▶ (H; S) pat

 Γ; Δ, y H x ∈ M 
Γ; Δ H x ∈ λy.M
y ∈ Δ	Γ; Δ H x ∈ S
Γ; Δ H x ∈ y · S
	Δ ▶ S pat	
Γ; Δ H x ∈ x · S
  Γ; Δ H x ∈ S 
Γ; Δ H x ∈ cσ · S

Type Checking
We define over the language of LF∗ two typing judgments Γ ▶def M : A and Γ ▶alg M : A, with analogous judgments at the type and kind levels. The former is definitionally simpler, and consequently far easier to reason about, but nonalgo- rithmic. The latter, however, is transparently decidable, and can be implemented directly.
Establishing correctness of the system as a whole now has two parts. The first part is to show that the algorithm embodied by Γ ▶alg M : A is sound and complete relative to Γ ▶def M : A. After that we must still connect Γ ▶def M : A over LF∗ to the same typing judgment over the original language of LF , which we construe as a syntactic subset of LF∗.
In a diagram, the task ahead looks like
(—)∗
LF/ ▶def 	) LF∗/ ▶def == LF∗/ ▶alg
Where (—)∗ is a bijective translation from LF to LF∗.

We first give the rules that Γ ▶def M : A, Γ ▶alg M : A have in common. This consists of all of the objects in the theory except for spines. Think of each rule with
▶ as implicitly quantified by ‘for all ▶ ∈ {▶def , ▶alg}, .. .’.
When we come to assigning types to spines there are two directions which a spine can be checked. The more familiar one is Γ ▶ S : Z > C, where the type Z and the spine S are given, and the type C is output. This is read as meaning that if a head (i.e. variable or constant) of type Z is applied S, the result will be of
type C. However, we have introduced constants that require the output type to be known, so we also require a judgment Γ ▶ S : Z < C which is identical in meaning to the other judgment, except that the type C is input rather than output.
Kinding
a : K ∈ Σ	Γ ▶ S : K > type
Γ ▶ a · S : type
Γ ▶ A : type	Γ, x : A ▶ B : type
σ
Γ ▶ Π x:A.B : type
Notice here that Πμ types are well-kinded only in the event that the variable
they bind actually has a strict occurrence. This is a key property when proving soundness of the system.
Γ ▶ A : type	Γ,x : A ▶ B : type	Γ,x : A Hμ x ∈ B
Γ ▶ Π[μ]x:A.B : type

Typing

 Γ ▶ A : type 	 Γ ▶ R : A 
Γ ▶ (N : A): A	Γ ▶ (R :) : A

x : A ∈ Γ	Γ ▶ S : A > C
Γ ▶ x · S : C
  Γ, x : A ▶ M : B	
Γ ▶ λx.M : Π−x:A.B
c+ : Z ∈ Σ	Γ ▶ S : Z > C
Γ ▶ c+ · S : C
c− : Z ∈ Σ	Γ ▶ S : Z < C
Γ ▶ c− · S : C
Spines: Deﬁnitional Typing
The definitional typing system ▶def uses the following rules to typecheck spines. So that we can write down rules only once that work the same way for both > and <, say ><s means > and ><i means <. Recall that μ,μ' are variables standing for either s or i.

Γ ▶def
() : type ><μ' type




Γ ▶def
() : a ·
S ><μ
a · S

Γ ▶def
M : A	Γ ▶def
S : [M/x]AV ><μ' W

Γ ▶def
(M ; S): Π−x:A.V ><μ' W

A = A'
Γ ▶	M + : A'	Γ ▶
S : [M +/x]AV ><μ' W

def
Γ ▶def
def
(M +; S): Π+x:A.V ><μ' W

Γ ▶def
A	μ'
M : A	Γ ▶def S : [M/x] V ><	W

Γ ▶def
(∗, S): Π[μ]x:A.V ><μ' W

These rules as a system are impractical for an implementation because of the final rule. If read bottom-up, it requires the omitted argument M of a spine to be nondeterministically guessed.
Algorithmic Typing
The algorithmic type checking judgment does higher-order matching (that is, uni- fication where all of the right-hand sides of equations have no free variables) to recover missing arguments.
Matching
We use P to denote sets of equations:

.
P ::= T |	1
2 ∧ P |
.
1	2 ∧ P |
.
1	2 ∧ P

Q for sets of typing constraints:
Q ::= T | (M : A) ∧ Q
and U for unification problems that track two sets of equality constraints, and one set of typing constraints:
U ::= ∃Ψ.(P, P ', Q)
where Ψ denotes a list of variables
Ψ ::= · | Ψ,x : A
It will also be necessary to talk about lists θ of substitutions:
θ ::= · | [M/x]Aθ
There are several technical details about such substitutions θ that must be treated (not least of which, typing them) but for space reasons we do not cover them here.
The idea at a high level is that to solve a unification problem
∃x1:A1,... , xn:An.(P, P ', Q)

is to find a set of instantiations for x1,... , xn that make P, P ',Q all true. Given that every xi has a strict occurrence in P , which is maintained as an invariant of the algorithm, we can decompose equations in P while preserving any solutions that might exist, either instantiating variables, or postponing equations by transferring
them to P ', until P is empty, and all that remains is P ' and Q. Since P is empty, our invariant says that no variables remain, so both P ' and Q are closed, and can be checked directly. The only potential difficulty is the fact that we recursively call the typechecker on Q. But by inspection, the algorithm only puts strictly smaller type-checking problems into Q.
We define a transition relation =⇒θ ‘takes one step, resulting in substitution θ’ via the following rules. The basic rules for working on a set of equations are quite straightforward, and all result in the empty substitution.

These are used via

These less trivial rules handle the occurrence of variable on the left. Recall that we are doing matching, not full unification, so ∃-quantified variables do not occur on the right.

Iterated =⇒θ is the relation =⇒∗, defined by



|= is defined, like ▶, uniformly over |=def and |=alg as follows:
Γ ▶ M : A	Γ |= Q

Γ |= T
Γ |= (M : A) ∧ Q
	Γ |= P	

Γ |	.	∧ P
= (W = W )
Γ |= P	Γ |= P '	Γ |= Q
Γ |= (P, P ', Q)
Now we are able to give a definition of the core of the algorithm, the constraint generation judgment, which takes the form
Γ; Ψ; Ψ' ▶ S : Z ><μ' C/(P, Q)
This claims that if we are trying to apply a head of type Z to S, and the resulting type is C, then we must find instantiations for the variables in Ψ' to satisfy the equations P and type constraints Q. Γ, Ψ, S,Z are input to this judgment, and Ψ', P,Q are output. C is input if μ' = i, and output if μ' = s. The judgment is defined by the following rules.



Γ; Ψ; · ▶ () : a · S < a · S'/(a ·
.
S = a
· S ∧ T, T)



Γ; Ψ; · ▶ () : a · S > a · S/(T, T)
Γ; Ψ,x : A; Ψ' ▶ S : Z ><μ C/(P, Q)
Γ; Ψ; x : A, Ψ' ▶ (∗; S): Π[μ]x:A.Z ><μ' C/(P, Q)
Γ ▶alg M + : A'
Γ; Ψ; Ψ' ▶ S : [M +/x]AZ ><μ C/(P, Q)

Γ; Ψ; Ψ' ▶	+	+
μ'	.
∧ P, Q)

Γ; Ψ; Ψ' ▶ S : [M/x]AZ ><μ C/(P, Q)



are

Γ; Ψ; Ψ' ▶ (M ; S): Π−x:A.Z ><μ' C/(P, (M : A) ∧ Q)
Finally, the toplevel rules which tell how to algorithmically typecheck a spine



When we have the type as input (Γ alg S : Z < C) we invoke constraint generation to produce Ψ', P, Q, and call unification to check that the constraints are satisfied. If unification succeeds, then type-checking does. If we are to output a type (Γ ▶alg S : Z < C) then we furthermore use the substitution returned by unification, and apply it to the type C which constraint generation produced, and return this as the result type of S.

Correctness
The statements of soundness and completeness of unification are somewhat techni- cal:
Lemma 2.1 (Soundness of Unification) Suppose that

'	'	∗	''	'
EΨ .(P, P , Q) =⇒θ0 (T,P ,Q )
and Γ |= (T,P '', Q). Then there is a θ' such that θ' = θ0 and Γ ▶ θ' : Ψ' and
Γ |= θ'(P, P ', Q).
Lemma 2.2 (Completeness of Unification) Suppose there exists θ' such that
Γ ▶ θ' : Ψ' and Γ |= θ'(P, P ', Q). Suppose further that for every x ∈ Ψ' that
there is an equation W =. W ' in P and a set Δx of variables disjoint from those
declared in Γ, Ψ' such that Γ; Δx H x ∈ W . Then there exist P '', Q, θ0 such that

'	'	∗
''	'	'
''	'

EΨ .(P, P , Q) =⇒θ0 (T,P ,Q ) and θ = θ0 and Γ |= (T,P ,Q ).
The main thrust of them, however, as is standard with such transition systems, is that (a) all of the individual transitions preserve solutions, and in our case, preserve strict occurrences as well, and (b) each transition decreases the size of the problem, so that solvability of a problem is decidable. The correctness of unification then leads to the correctness of the typing algorithm ▶alg with respect to the definition
▶def .
Lemma 2.3 (Soundness and Completeness of ▶alg)
If Γ ▶alg M : A, then Γ ▶def M : A.
If Γ ▶def M : A, then Γ ▶alg M : A.

Equivalence
Having defined LF∗ and establishing that the definitional typing judgment is decid- able, we turn now to the issue of showing that it is equivalent to LF . As mentioned previously, we construe the language of LF as a strict subset of the language of LF∗. Henceforth we syntactically distinguish every LF object with a ◦ in the subscript and every LF∗ object with a ∗ subscript. The grammar of LF is
M◦ ::= N◦ | R◦ N◦ ::= λx.M◦
R◦ ::= x · S◦ | c+ · S◦ E◦ ::= M◦
S◦ ::= () | (E◦; S◦)
A◦, B◦ ::= a · S◦ | Π−x:A◦.B◦
K◦ ::= type | Π−x:A◦.K◦
This is simply the LF∗ grammar with c−, Π+, Π[μ], (∗; S), (M +; S) removed. The typing judgments and rules that apply to this subset of LF∗ are exactly the ordinary typing rules for LF . The only difference is cosmetic: here we say c+, Π− where one would of course find merely c, Π in a normal treatment of LF .
It remains to show that LF∗ is isomorphic to LF , in the sense that every proof term in LF∗ corresponds to one and only one proof term in LF . Fix for the sake of discussion signatures Σ◦ and Σ∗, in LF and LF∗ respectively, and assume that they assign types and kinds to exactly the same constant and type family symbols, except
that whenever Σ has c+, we find exactly one of c+ or c− in Σ∗. Under suitable further assumptions (described below) that Σ◦ and Σ∗ are in fact equivalent signatures, we aim to show that there is a translation from well-formed objects in Σ◦ to well-formed objects in Σ∗ that is bijective, homomorphic with respect to typing, and so on.
One difficulty in establishing this result via such a translation comes from the
fact that neither LF nor LF∗ prima facie bears strictly more information than the other: LF∗ signatures have more information in the form of Π-annotations, and its terms contain type ascriptions foreign to LF , while an LF term generally contains subterms that are omitted in its LF∗ counterpart. Because of this, we cannot simply define an erasure function W '→ W ∗ from LF to LF∗ that erases some subterms
to ∗. We need another erasure W '→ W ◦ which erases Π-annotations, and we need
W '→ W ∗ to fill in necessary type ascriptions.
This notation is chosen to suggest that (—)∗ takes objects into LF∗, and that (—)◦ takes objects back to LF , though this latter statement is not strictly true. The general idea is that both mappings erase some information, and that objects W◦ and W∗ ought to be considered equivalent when the mappings bring them together, when ‘(W◦)∗ = (W∗)◦’.
The mapping (—)◦ for Π-types is defined by (Πρx:A.W )◦ = Π−x:A.(W ◦). Oth- erwise, W ◦ = W . However, the definition of (—)∗ is less simple. Since it needs to

insert type ascriptions, it cannot be merely a function from terms to terms, types to types, and so on. To know which type to insert, we must carry along the type, and in order to know the type of variables, we must carry along a context as well. We write this translation, then, using the same syntax as the typing judgment itself, as (Γ◦ ▶ M◦ : A◦)∗ for terms, and (Γ◦ ▶ A◦ : type)∗ for types.
For spines it is still not enough to write something of the form (Γ◦ ▶ S◦ : Z◦ > C◦)∗. We need an additional argument Z∗, because its Π binders carry the required extra annotations required to translate the spine, (dictating, importantly, which arguments to erase) whereas Z◦ does not. Therefore the translation function for
spines takes the form (Γ◦ ▶ S◦ : Z◦ > C◦)∗ .
The translation is defined as follows:
Terms
(Γ◦ ▶ λx.M◦ : Π−x:A◦.B◦)∗ =
λx.(Γ◦,x : A◦ ▶ M◦ : B◦)∗
(Γ◦ ▶ x · S◦ : C◦)∗ = x · (Γ◦ ▶ S◦ : A◦ > C◦)∗	∗
(Γ◦ ▶A◦:type)
(if x : A◦ ∈ Γ◦)
+	∗	σ	∗
(Γ◦ ▶ c  · S◦ : C◦) = c  · (Γ◦ ▶ S◦ : A◦ > C◦)
(if cσ : A∗ ∈ Σ∗ and c+ : A◦ ∈ Σ◦)
Spines We mention only the case for typed (not kinded) spines. The other case is analogous. We split cases on the subscript Z∗. Make the abbreviations

A∗ = (Γ◦ ▶ A◦ : type)∗, and S∗ = (Γ◦ ▶ S◦ : [M◦/x]A◦ Z◦ > C◦)∗
, and

M∗ = (Γ◦ ▶ M◦ : A◦)∗. Then for Πσ we do
(Γ◦ ▶ (M◦; S◦): Π−x:A◦.Z◦ > C◦)∗ σ
⎧⎨ ((M∗ : A∗); S∗)	if σ = +, M∗ normal;
=	((M∗:); S∗)	if σ = +, M∗ atomic;
⎩ (M∗; S∗)	otherwise.
Observe that we only add the type annotation A∗ when it is necessary. For Π[μ] we
simply erase the argument, and make the same recursive call on S◦ as before:

(Γ◦ ▶ (M◦; S◦): Π−x:A◦.Z◦ > C◦)∗ [μ]

x:A∗ .Z∗
= (∗; S∗)


Types
(Γ◦ ▶ () : C◦ > C◦)∗
= ()

(Γ◦ ▶ Π−x:A◦.B◦ : type)∗ =
Π−x:(Γ◦ ▶ A◦ : type)∗.(Γ◦,x : A◦ ▶ B◦ : type)∗ (Γ◦ ▶ a · S◦ : type)∗ = a · (Γ◦ ▶ S◦ : K◦ > type)∗
(if a : K∗ ∈ Σ∗ and a : K◦ ∈ Σ◦)
We may also translate contexts in the evident way, namely by translating each of the types in them. With these maps we can state the correspondence condition for the two signatures:

Definition 3.1 Σ◦ and Σ∗ are equivalent if
For every c, we have that cσ : A∗ ∈ Σ∗ and c+ : A◦ ∈ Σ◦ implies (A∗)◦ = (· ▶ A◦ :
type) .
For every a, we have that a : K∗ ∈ Σ∗ and a : K◦ ∈ Σ◦ implies (K∗)◦ = (· ▶ K◦ :
type) .
When two signatures are equivalent, the theories they generate should be equiv- alent. This essentially amounts to two properties, that the image under the trans- lation of the terms of a type actually belong to the translation of the type itself, and that the translation restricted to any one type is a bijection.
Theorem 3.2 (Type Preservation) Suppose that Σ◦ and Σ∗ are equivalent. Then
if Γ◦ ▶Σ◦ M◦ : A◦, then (Γ◦)∗ ▶Σ∗ (Γ◦ ▶ M◦ : A◦)∗ : (Γ ▶ A◦ : type)∗
if Γ◦ ▶Σ◦ S◦ : A◦ > C◦ and (Γ◦ ▶ A◦ : type)∗ = (A∗)◦, then (Γ◦)∗ ▶Σ∗ (Γ◦ ▶ S◦ :
A◦ > C◦)∗  : A∗ > (Γ ▶ C : type)∗
If Γ◦ is a Σ◦-context, then (Γ◦)∗ is a Σ∗-context.
if Γ◦ ▶Σ◦ A◦ : type then (Γ◦)∗ ▶Σ∗ (Γ◦ ▶ A◦ : type)∗ : type.
if Γ◦ ▶Σ◦ K◦ : kind then (Γ◦)∗ ▶Σ∗ (Γ◦ ▶ K◦ : kind)∗ : kind.
Stating and proving the bijectivity of the translation, though important, is con- siderably more difficult, and so we do not develop it here.

Conclusion
We have described a type system which internalizes facts about which parts of terms can be safely omitted, while preserving representational adequacy. An implementa- tion can achieve significant savings by not representing these omitted parts at all, and still ‘prove the same theorems’ as before.
The empirical advantage of this species of change of representation has been confirmed by earlier work. Ours retains several of its key properties. By working in a system derived from LF , we have at our disposal all of its representational techniques, such as higher-order abstract syntax. Like LFi, full unification is not used, and instead only a subset — in our case, higher-order matching — is necessary. This is important for a maximally simple and trustable implementation.
The divergence from LFi is that LF∗ seeks to make type theoretic sense out of the possibility that subterms can be redundant. We do not have LFi’s ability to assign both an ‘inference recipe’ and a ‘checking recipe’ to a single constant, since we impose the restriction that a constant has a single type, which gives its
reconstruction recipe once and for all. However, preliminary investigation suggests that in many cases — most especially when the object language is a type theory admitting a bidirectional typing algorithm itself — a constant is consistently always or almost always used in one way or the other. Thus, only one recipe is really necessary most of the time.

There is also a possible answer to this difficulty from using notational definitions. It is still an open problem whether notational definitions could feasibly be combined with this system, but if they could, then we could regain the ability to freely use different recipes by introducing a constant as being definitionally equal to an old one: one which, by virtue of being exposed at a new type, specifies a different reconstruction strategy for its arguments.
On the other side of the balance, there are forms of omission which LF∗ can handle, which LFi cannot. Since LF∗ places a priority on pushing the mechanics of omission into the language itself at as fundamental a level as possible, the design of it is such that all terms, types, and kinds can contain placeholders for omitted information as a matter of course: the indices to a type family are general terms, and terms may contain placeholders. LFi, on the other hand, has restrictions on
when placeholders can appear in types. We anticipate, therefore, that encoding techniques that use more high-order and high-level constructions may benefit from the uniform treatment of omission afforded by LF∗. A more precise evaluation of the effectiveness of the proposed system still awaits implementation and experimen-
tation, which we hope to complete soon.

Acknowledgements
Many thanks are due Frank Pfenning for his encouragement and help with both the conceptual and technical portions of this work, and to Kevin Watkins for the original formulation of the type system.

References
Robert Harper, Furio Honsell, and Gordon Plotkin. A framework for defining logics. Journal of the Association for Computing Machinery, 40(1):143–184, January 1993.
Robert Harper and Frank Pfenning. On the equivalence and canonical forms in the LF type theory. Technical report, Carnegie Mellon University, 2001.
Masami Hagiya and Yozo Toda. On implicit arguments. In Logic, Language and Computation, pages 10–30, 1994.
Marko Luther. More on implicit syntax. In Automated Reasoning. First International Joint Conference (IJCAR’01), Siena, Italy, June 18–23, 2001, Proceedings, volume 2083 of Lecture Notes in Artificial Intelligence, pages 386–400, Berlin, 2001. Springer-Verlag.
Dale Miller. A logic programming language with lambda-abstraction, function variables, and simple unification. Journal of Logic and Computation, 1(4):497–536, 1991.
Alexandre Miquel. The implicit calculus of constructions: Extending pure type systems with an intersection type binder and subtyping. In S. Abramsky, editor, Proc. of 5th Int. Conf. on Typed Lambda Calculi and Applications, TLCA’01, Krakow, Poland, 2–5 May 2001, volume 2044, pages 344–359. Springer-Verlag, Berlin, 2001.
George C. Necula. Proof-carrying code. In Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Langauges (POPL ’97), pages 106–119, Paris, January 1997.
George C. Necula and Peter Lee. Efficient representation and validation of logical proofs. In Proceedings of the 13th Annual Symposium on Logic in Computer Science (LICS’98), pages 93–104, Indianapolis, Indiana, 1998. IEEE Computer Society Press.
Frank Pfenning and Conal Elliott. Higher-order abstract syntax. In Proceedings of the ACM SIGPLAN ’88 Symposium on Language Design and Implementation, pages 199–208, Atlanta, Georgia, June 1998.

Frank Pfenning. Structural cut elimination I. intuitionistic and classical logic. Information and Computation, 157(1/2):84–141, mar 2000.
Frank Pfenning and Carsten Schu¨rmann. Algorithms for equality and unification in the presence of notational definitions. In T. Altenkirch, W. Naraschewski, and B. Reus, editors, Types for Proofs and Programs, pages 179–193, Kloster Irsee, Germany, March 1998. Springer-Verlag LNCS 1657.
K. Watkins, I. Cervesato, F. Pfenning, and D. Walker. A concurrent logical framework I: Judgments and properties. Technical report, Carnegie Mellon University, 2003.
