Electronic Notes in Theoretical Computer Science 45 (2001)
URL: http://www.elsevier.nl/locate/entcs/volume45.html 11 pages


A Generalisation of Stationary Distributions, and Probabilistic Program Algebra

A.K. McIver 1,2
Department of Computing Macquarie University NSW, Australia


Abstract
We generalise the classical notion of stationary distributions of Markov processes to a model of probabilistic programs which includes demonic nondeterminism. As well as removing some of the conditions normally required for stationarity, our gen- eralisation allows the development of a complete theory linking stationary behaviour to long-term average behaviour — the latter being an important property that lies outside the expressive range of standard logics for probabilistic programs.
Keywords: Probabilistic program semantics, probability, demonic nondetermin- ism, Markov process, stationary distribution, Markov decision processes.



Introduction
Programs or processes which can make probabilistic choices during their exe- cution exhibit a range of (probabilistic) behaviours outside those describable by purely qualitative formalisms; moreover even well-known quantitative adap- tations of familiar program logics — the foremost being probabilistic temporal logic [18,2] — are still not expressive enough in some cases. One such is the so- called “average long-term” behaviour [3,4], which we illustrate in the context of the program presented in Fig. 1. The program FP represents a specifica- tion of a simple failure-repair mechanism. The system it describes is intended to execute repeatedly, and the state evolves according to the specified prob- abilistic statements. The average long-term behaviour of FP determines (for example) the proportion of time that the state is ok, and is always well-defined [4]. Other related terms are “availability” [17] and “the stationary probability of ok” [8]. In this particular case an elementary analysis reveals that ok holds

1 This work was done at Oxford University, UK, and was funded by the EPSRC.
2 Email: anabel@ics.mq.edu.au
◯c 2001 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.


FP := if ok
then (ok 2/3⊕ ¬ok) [] ok
else (ok 1/2⊕ ¬ok) [] ok
fi
ok and ¬ok toggle the states corresponding to working and broken behaviour. The operator 2/3⊕ records a probabilistic update, whereas [] records a nondeterministic update. Used together like this, we are able to specify tolerances on failure rates — at every execution, there is at least a probability 1/2 of reestablishing ok (since the only other alternative to the probabilistic branch establishes ok with certainty).
Fig. 1. An abstract failure-repair mechanism




on average at least 3/5 of the time — yet probabilistic temporal logic cannot describe that behaviour. (de Alfaro gives a nice discussion of the issues [3].)
In elementary probability theory, long term average behaviour is, in some special cases, determined by “stationary distributions” — a property of (some) Markov processes. Though some authors [10,16] have used Markov processes as a model for probabilistic programs, more recently a generalised form [13,9,2] has been found to be more suitable, since it supports the notions of (demonic) nondeterminism (or abstraction) and the induced partial order known as re- finement. That is the model we shall work with here, and we give details in Sec. 5.
Thus our main contribution (in Sec. 3) is to give an axiomatic account of stationary behaviour and convergence to it, one which extends and sim- plifies the classical notion. Not only is our notion of generalised convergence applicable to all Markov processes (rather than only to some special cases) but it completes the theory linking stationary behaviour to average long-term behaviour. The details are set out in Sec. 5.
We develop our theory following the algebraic style already available in theories of concurrency, where it has proved a powerful tool for analysing nondeterministic programs that execute repeatedly.
We use “.” for function application; ≤,+ and H denote respectively “is no more than”, addition and minimum applied pointwise to real-valued functions. Throughout S is a finite state space and S is {F | S → [0, 1] ·  s:S F.s = 1}, the set of (discrete) probability distributions over S. For real k, we write k for the constant real-valued function with range {k}. If α is a real-valued function over S then (Hα) and (Hα) denote respectively the maximum and minimum value taken by α as the state varies over S; and (kα) or k(α) represents the the function α pointwise multiplied by the real k. We introduce other notation as we need it.

Probabilistic sequential programs
We summarise two equivalent models for probabilistic programs; more details are given elsewhere [13,9]. The semantics for probabilistic sequential programs supports the interpretation of traditional programming control structures to- gether with a binary probabilistic choice operator p⊕, where the operational meaning of the expression A p⊕ B is that either A or B is executed, with probability respectively p or 1−p. Since there is no determined output, that behaviour is sometimes called “probabilistic nondeterminism”. Probabilistic nondeterminism is however very different from “demonic nondeterminism”, denoted by “[]”, already present in standard guarded commands [5], and which can model underspecification or demonic scheduling in distributed systems.
And the two operators are modelled very differently — as usual prob- abilistic information is described by (output) probability distributions over final states, whereas demonic behaviour is described by subsets of possible outputs. Putting those two ideas together leads to a model in which programs correspond to functions from initial state to sets of distributions over final states, where the multiplicity of the result set represents a degree of nondeter- minism and the distribution records the probabilistic information after that nondeterminism has been resolved. We have the following definition for the probabilistic program space HS [9,13] for programs operating over the ab- stract state space S, 3 and its treatment of nondeterminism is similar to that of other models [2,15,4]:

HS:= S → PS . 
More generally, like Markov processes, every program in HS can be considered to be a function from probability distributions over initial states, but in this case to sets of probability distributions over final states [9].
We order programs using program refinement, which compares the extent of nondeterminism — programs higher up the refinement order exhibit less nondeterminism than those lower down:
Q ± P  iff (∀s: S · P.s ⊆ Q.s) .
Classical Markov processes can be identified with the subclass of “determin- istic”, or purely probabilistic programs in HS, and as such are maximal with respect to ±. For instance the (demonically deterministic) program ok1/2⊕¬ok has no proper refinements at all.
One consequence of ± above is that (worst case) quantitative properties improve as programs become more refined. If Q guarantees to establish a predicate φ with probability at least p (irrespective of the nondeterminism), then P must also establish φ with probability at least that same p.
That observational view of probabilistic systems (in which the frequency of outputs is recorded) is captured more generally with the idea of “expected val- ues”. Kozen was the first to exploit this fact in his probabilistic program logic

3 This basic model can also be enhanced to include nontermination [13] and miracles [14].


(but for deterministic programs). His insight was to regard programs as oper- ators which transform real-valued functions in a goal-directed fashion, in the same way that standard programs can be modelled as predicate transformers [5]. The use of real-valued functions instead of predicates allows expressions to incorporate quantitative (as well as qualitative) information. The idea has been extended by others [13] to include demonic nondeterminism as well as probability. We write SS for the space of real-valued functions (expectations) over S, and 7 S for the associated space of “expectation transformers”, defined next.
Definition 2.1 Let r: S → P(S) be a program taking initial states in S to sets of final distributions over S. Then the greatest guaranteed pre-expectation at state s of program r, with respect to post-expectation α in SS, is defined
wp.r.α.s	:=	(HF : r.s · ∫ α) ,
F
where ∫F α denotes the expected value of α with respect to distribution F .
4
We say that wp.r is an expectation transformer corresponding to r, and we define 7 S to be wp.7S.
Programs are ordered by comparing the results of qualitative observations: thus
t ± t'	iff	(6α : S+S · t.α ≤ t'.α) ,
where S+S denote the non-negative expectations. There is no conflict in using “±” to denote the order in both 7S and 7 S, since the definitions correspond [13].
In the special case that the post-expectation takes values in {0, 1} and thus represents a predicate, the pre-expectation represents the greatest guaranteed probability of the program establishing that predicate. Nondeterminism, as for predicate transformers, is interpreted demonically.
Although the two views are equivalent [13], we usually use 7 S because its arithmetic properties make it more convenient for proof than 7S. Transform- ers in 7 S are continuous (in the sense of real-valued functions) and subaddi- tive, that is
t.(kα + k'β — k'')	≥	k(t.α)+ k'(t.β) — k'' ,
which can be strengthened to additivity in the case of deterministic programs (classical Markov processes). We interpret basic program constructs as op- erations on transformers: thus (t; t').α:= t.(t'.α); (t [] t').α:= t.α [] t'.α and (t p⊕ t').α:= p(t.α)+ (1—p)(t'.α), from which we see that determinism is preserved by p⊕ and ; , but not by [].
The next lemma can be proved very simply using the notions of 7 S. Define the norm ||.|| on expectations as ||α||:= (Hα) — (Hα). Our definitions imply

4 In fact ∫
∫	F
α is just Σ


s:S
α.s×F.s because S is finite and F is discrete [6]. We use the

that if ||α|| = 0 then α is constant on S.
Lemma 2.2 Let t, t' be an expectation transformers in 7 S. If t is determin- istic, and t'; t = t'; and furthermore if there is some 0 ≤ c < 1 such that for any α we have ||t.α|| ≤ c||α||, then t' is deterministic.
Proof: The above discussion suggests that we just need to show that t' is additive, which follows by continuity of transformers in 7 S.
Even though Lem. 2.2 is more generally true for any programs in 7 S satisfying the conditions, it actually characterises the property which underlies whether a Markov process converges to its so-called stationary distribution or not, namely that it acts like a contraction with respect to ||.||. The term “contraction” however is more general and can be applied to the whole of 7 S, not just to its deterministic portion: FP in Fig. 1 is a contraction for instance, though it is not a Markov process.
Conversely, if tn is not a contraction for any power of t then it can be shown that there is some proper subset of states that is left invariant by tn, for some n. Such programs are also called “periodic”, and we shall return to them later.

A program-algebraic treatment of ‘stationary behaviour’
In this section we study some algebraic properties of programs or systems that execute repeatedly. Algebraic approaches have proved to be very powerful in the development of concurrency theory [1]; we find them to be extremely effective in this context as well.
Our basic language (in Fig. 2) consists of two binary operators (“;”, se- quential composition and “[]”, demonic nondeterministic choice), one constant (1, “do nothing”) and a unary operator (“∗”, the “Kleene star”). Both ; and [] are associative and [] is commutative; 1 is the identity of ;. Observe that for probabilistic models [] fails to distribute to the left. (Other nonprobabilistic interpretations would allow full distributivity [1].) We interpret x∗ in 7 S as the transformer x∗.α:= (νY · α H x; Y ), 5 which corresponds to the program that from initial state s outputs the strongest set of invariant states containing
s. We shall also use the special program chaos which denotes a nondetermin- istic selection over all the states in S. A program t which can reach all states from all initial states (with probability 1) has no proper invariants, and thus satisfies t∗ = chaos.
Next we introduce our first generalisation — a probabilistic operator p⊕; its properties [9] also appear in Fig. 2. Observe that the sub-distribution of p⊕ corresponds to subadditivity of 7 S.
We say that a probability distribution F in S is stationary with respect to a Markov process t if whenever the input states are distributed as F , the

5 ν forms the greatest fixed point with respect to ≤ on E S.


x ± y	⇔	x [] y = x	x∗ = 1 [] x [] x∗; x∗
x; (y [] 1) ± x	⇒	x; y∗ = x
x; (y [] z)	±	x; y [] x; z	x; y ± y	⇒	x∗; y = y
(y [] z); x	=	y; x [] z; x


x p⊕ y = y 1−p⊕ x	x [] y ± x p⊕ y
x; y p⊕ x; z	±	x; (y p⊕ z)	(y p⊕ z); x	=	y; x p⊕ z; x


x p⊕ (y q⊕ z) = (x 	p	 ⊕ y)	−	⊕ z
(p+q−pq)
x, y, z are interpreted as programs in T S, and 0 < p < 1. The axioms without p⊕ are similar to Kozen’s axiomatisation of Kleene’s language for regular expressions [11].
Fig. 2. Basic axioms

output states are also distributed exactly according to F . In this section we generalise this idea to all programs in 7 S.
Observe first that any F in S can be modelled as the program that outputs

F — we call such programs deterministic assignments. Writing Fˆ
for the

deterministic assignment that outputs F for any initial state, we can see that the definition of stationarity above is the same as saying that Fˆ; t = Fˆ holds as an equality in 7 S.
Our crucial generalising step is now to consider any program t' satisfying t'; t = t' to represent stationary behaviour (rather than only those programs Fˆ generated from distributions F as above); that takes us beyond the classical treatment.
To fill in the details, we begin with the idea of weakest stationary program, as follows. We make use of x∗ to encode “all invariants of x”, noted above.
Definition 3.1 Define x∞ to be the the least program that is stationary with respect to x (that is, which satisfies x∞; x = x∞) and which preserves all invariants of x (that is x∗ ± x∞; x∗). We have
x∞	:=	([]y : 7S · y; x ± y	Λ	x∗ ± y; x∗) .
Note that an important intuitive property of x∞ is that it preserves all invariants of x — an alternative definition that only considers stationarity (the first conjunct in Def. 3.1) gives the incorrect
([]y : 7S · y;1 ± y) = chaos /= 1∞ =1 
for the case x = 1.
Program t∞ can be thought of as delivering from initial state s the strongest invariant reachable from s, whilst preserving the probabilistic stationary be-


x∗; x∞ = x∞	x∞∞ = x∞
x∞ ± xn∞	x∗ ± xn∗
x∗ ± x∞	x∗∞ = x∗
x; (y; x)∞ ± (x; y)∞; x	x; x∞ = x∞ = x∞; x


(p > 0) ⇒ (x p⊕ 1)∞ ± x∞ x∗; x = x∗ ⇒ x∗ = x∞ x; y ± z; x ⇒ x; y∞ ± z∞; y xn∗ = x∗ ⇒ x∞ = xn∞
To avoid clutter, we write xn∞ etc. instead of (xn)∞.
Fig. 3. A selection of basic theorems

haviour. In fact t∞ in 7 S is the the limit of the increasing chain of programs t∗ ± t∗; t ± t∗; t2 ± ... ± t∗; tn ± ... That limit is well-defined since 7 S is directed-complete, and hence we have the additional fact
(1)	(6n > 0 · x∗; xn ± y)	⇒	x∞ ± y . 
In Fig. 3 we set out some general theorems about ∞ and ∗, all implied by
the axioms of Fig. 2 and the properties of ∞ set out in Def. 3.1 and (1).
To see the difference between ∗ and ∞ we reconsider FP from Fig. 1. The only nontrivial invariant set of states is {ok, ¬ok}, hence FP∗ = ok [] ¬ok; but this program is not stationary with respect to FP, and so FP∞ /= FP∗. In fact FP∞ = (ok 3/5⊕ ¬ok) [] ok, the generalised distribution in which the probability of ok is at least 3/5.

Extended Markov theory
From (1) it is easy to see that in the general setting, any program t (if ex- ecuted for long enough) achieves some notion of stationary behaviour en- capsulated by the program t∞. But that is not the view taken by classical Markov process theory. To see where the general and the classical theo- ries diverge, consider the program b := 1—b, where the variable b can only take values in {0, 1}. The classical theory says that this program does not converge (because it oscillates between b’s two values). On the other hand (b := 1—b)∞ = (b := 1—b)∗ = (b := 0 [] b := 1), which says that the long term stationary behaviour is a program that assigns to b nondeterministically from its type. That behaviour is disqualified by the classical theory because it is not deterministic and so does not represent a distribution. We discuss the “observational” intuition behind this solution in the next section.
For now we end this section by demonstrating that our generalised notion of convergence really supersedes the classical theory. We present a new proof of the important result about convergence to a stationary distribution of “ape-

riodic” Markov processes; the proof relies crucially on the ability to postulate the existence of t∞ for all Markov processes, and not just those permitted by the classical theory.
Recall that a distribution is modelled as a deterministic assignment which is independent of the initial state. A transformer t which corresponds to such an assignment is additive and, for any α, the expectation t.α is a constant function. For example wp.(ok 2/3⊕ ¬ok).α returns the expected (final) value of α, which is constant at 2(α.ok)/3+ α.(¬ok)/3, whatever the initial value.
Hence in our terms all we need do is show that ∞ maps the aperiodic deterministic programs to transformers that correspond to deterministic as- signments.
Aperiodicity is a property of t provided that all states are eventually reach- able from all other states, and the probability of returning to the original state with a definite period is strictly less than 1 [8]. The first property is the same as saying that t∗ = chaos, and the second is the same as saying that tn∗ = t∗ for all n > 1 — in the case that the equality fails for some n, we are saying that t exhibits a period of n. The general theorem about convergence of Markov processes is then as follows.
Theorem 4.1 If t in 7 S is deterministic and aperiodic then t∞ is a deter- ministic assignment.
Proof:  The comment after Lem. 2.2 implies that tn must be a contraction for some n > 0, and hence tn∞ must be a deterministic assignment (also by Lem. 2.2). The result follows from Fig. 3 since tn∗ = t∗.

Applications to long-term average behaviour
The properties of systems that execute indefinitely are usually investigated using an adaptation of temporal logic — in our case probabilistic temporal logic. Formulae are interpreted over trees of execution paths — in our case probabilistic distributions over execution paths [15,2]. The interpretation of a typical formula φ over a path-distribution yields the proportion of paths satisfying φ. As de Alfaro points out [3] however, this kind of “probabilistic satisfaction” refers to the aggregate path-distribution; put another way it measures the chance of a single event occurring among paths, and ignores the frequency with which events occur along paths. But this is precisely what is called for in availability or long-term average analyses of failing systems. In this section we show that both are determined by t∞ — even for systems that include nondeterminism, such as FP in Fig. 1.
We define long-term average behaviour as de Alfaro [3] does. Given a sequence seq of expectations, let seqi be the i’th element, and define the partial sum  k seq = seq1 + seq2 + ... + seqk.
Definition 5.1 Let t in 7 S execute indefinitely, and let α be a predicate. The long-term average number of occurrences of α observed to hold as t executes



is given by Vt.α in

Vt.α	: =	lim inf
Σk seq ,

where in this case seqk:= t∗; tk.α.
k→∞	k

Def. 5.1 corresponds to the average result after sampling the state of the system at arbitrary intervals of time as t executes repeatedly. Here we assume that at the k’th sample point, the system has executed at least k times — and in that case the chance that α holds at the time of the test is t∗; tk.α. When t corresponds to a Markov process that converges classically, that average is determined by the stationary distribution. We have a corresponding result here, but it is valid for all programs.
Lemma 5.2 Let t be a program in 7S and α an expectation in SS. Then we have t∞.α  = VP .α .
To illustrate the above, recall the program b := 1—b, and let [b = 0] represent the expectation that evaluates to 1 at states where b is 0 and to 0 elsewhere. To calculate Vb:= 1−b.[b = 0] we consider
wp.(b := 1—b)∗; (b := 1—b)n.[b = 0] = 0 ,
hence Vb:= 1−b.[b = 0]= 0 as well.
Alternatively, (b := 1—b)∞ = (b := 0[]b := 1), hence wp.(b := 1—b)∞.[b = 0] = 0 also.
These results can be understood operationally in the context of a tester who is allowed to choose when to sample the state of the program. Clearly if the tester only observes the state after an even number of executions of b := 1—b then he will deduce that b is never 0 on average (or even at all). The point about aperiodic programs in the classical theory is that the average measurement is to an extent robust against such accidental testing bias. And the same applies here: whatever the proposed testing regime, the proportion of time that FP is ok will be found to be at least 3/5, since FP∞ = (ok 3/5⊕
¬ok) [] ok.

Conclusion
Our main contribution is to extend the notion of stationary behaviour of Markov processes to a model that includes demonic nondeterminism, setting it on a par with other programming concepts. The main insight was to model stationary behaviour explicitly as a distribution-generating program in 7 S; that allows access to the techniques of program algebra and probabilistic mod- els [1,13]. The generalisation proposed here allows the completion of the theory linking long-term average behaviour and stationary behaviour — both are now always defined, and they determine each other. Moreover our generalisation provides a striking simplification to classical theory of convergence.

The operator t∗ presented here is unable to express many of the esperiments offered by the much more elaborate framework due to de Alfaro [3]. The main difference is that results are assigned to states rather than transitions. Nevertheless many useful performance measures are covered by this simpler framework. Examples include average waiting times and availability measures.
Further work is needed to incorporate other programming notions such as coercions [12], which significantly increase the power of algebraic reasoning.
An important consequence is that stationary behaviour is now susceptible to other programming techniques such as refinement and data abstraction [7].

References
Ernie Cohen. Separation and reduction. In Mathematics of Program Construction, 5th International Conference, Portugal, July 2000, number 1837 in LNCS, pages 45–59. Springer Verlag, 2000.
L. de Alfaro. Temporal logics for the specification of performance and reliability.
Proceedings of STACS ’97, LNCS volume 1200, 1997.
L. de Alfaro. How to specify and verify the long-run average behavior of parobabilistic systems. In Proceedings of ’LICS ’98, 23-24 June, Indianapolis, 1998.
C. Derman. Finite State Markov Decision Processes. Academic Press, 1970.
E.W. Dijkstra. A Discipline of Programming. Prentice Hall International, Englewood Cliffs, N.J., 1976.
W. Feller. An Introduction to Probability Theory and its Applications, volume 1. Wiley, second edition, 1971.
P. H. B. Gardiner and C. C. Morgan. Data refinement of predicate transformers.
Theoretical Computer Science, 87:143–162, 1991.
G. Grimmett and D. Welsh. Probability: an Introduction. Oxford Science Publications, 1986.
Jifeng He, K. Seidel, and A. K. McIver. Probabilistic models for the guarded command language. Science of Computer Programming, 28(2,3):171–192, January 1997.
D. Kozen. Semantics of probabilistic programs. Journal of Computer and System Sciences, 22:328–350, 1981.
D. Kozen. A completeness theorem for Kleene algebras and the algebra of regular events. Information and Computation, 110:336–390, 1994.
C. C. Morgan. Programming from Specifications. Prentice-Hall, second edition, 1994.


C. C. Morgan, A. K. McIver, and K. Seidel. Probabilistic predicate transformers. ACM Transactions on Programming Languages and Systems, 18(3):325–353, May 1996.
C.C. Morgan. Private communication. 1995.
R. Segala. Modeling and verification of randomized distributed real-time systems. PhD Thesis, 1995.
M. Sharir, A. Pnueli, and S. Hart. Verification of probabilistic programs. SIAM Journal on Computing, 13(2):292–314, May 1984.
N. Storey. Safety-critical computer systems. Addison-Wesley, 1996.
M. Vardi. Automatic verification of probabilistic concurrent finite-state systems. Proceedings of 26th IEEE Symposium on Found. of Comp. Sci., pages 327–338, 1985.
