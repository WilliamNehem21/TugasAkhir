Egyptian Informatics Journal 24 (2023) 100414








Knowledge graph completion method based on hyperbolic representation	 learning and contrastive learning
Xiaodong Zhang a, Meng Wang a,*, Xiuwen Zhong a, Feixu An b
a School of Computer Science, Nanjing Audit University, Nanjing 211815, China
b University of Toronto, Toronto M2J4A6, 211189, Canada



A R T I C L E I N F O 

Keywords:
Knowledge graph completion Hyperbolic representation learning Comparison learning
Adversarial samples
A B S T R A C T 

Knowledge graph completion employs existing triples to deduce missing data, thereby enriching and enhancing graph completeness. Recent research has revealed that using hyperbolic representation learning in knowledge graph completion yields superior expressive and generalization capabilities. However, the long-tail problem and the presence of hyperbolic metrics make it challenging to effectively learn low-frequency entities or relations, resulting in embedding space distortion and impacting the original semantic relationships. Therefore, this paper proposes a knowledge graph completion method (Att-CL) that integrates hyperbolic representation learning and contrastive learning. In this approach, knowledge is embedded into a hyperbolic space, and samples with limited hierarchical characteristics and insufficient feature information are enhanced by introducing adversarial noise. The loss function of the embedded samples is backpropagated into embedding vectors, perturbations are adjusted in the gradient direction to promote smoothness and locality, and hyperparameters are introduced for fine-tuning the adversarial strength in the construction of adversarial samples for data augmentation to enhance model robustness. To mitigate data distortion due to hyperbolic metrics, a penalty term is introduced in the contrastive loss function to control the distances of the embedding vectors from the origin, thereby reducing the impact of
the metrics and further improving the model’s completion ability. Experimental results on the WN18RR and
FB15K-237 benchmark datasets demonstrate significant improvements in metrics such as MRR, Hits@1, and Hits@3 compared to traditional knowledge graph completion models, providing ample evidence of the model’s effectiveness.





Introduction

A knowledge graph is a comprehensive and organized network of structured data, designed to illustrate the connections and relationships among various entities. It consists of nodes and edges, consists of nodes that represent distinct entities, and edges that denote the relationships between these entities [1]. At present, knowledge graphs are widely used in many AI fields, such as natural language processing and search recommendation [2]. However, although standard knowledge graphs (e. g., WordNet) are widely applied, they still face a problem of information incompleteness [3]. They contain large amounts of unrevealed semantic information. Therefore, knowledge graph completion, with the goal of automatically inferring the missing content based on the existing in- formation and external data in order to fill in missing and erroneous information to construct a more complete and accurate graph, has become a vital focus of research for enhancing the quality and
application value of knowledge graphs [4,5]. The following are some examples of ways to achieve this purpose. Currently, hyperbolic repre- sentation learning and contrastive learning have become popular research topics in the field of graph embedding. In hyperbolic repre- sentation learning, the nodes in a graph are mapped to a hyperbolic space, and the representation and computation of the nodes are per- formed in this space. The rich geometric structure of the hyperbolic space itself is utilized to better portray the nonlinear relationships and hierarchical structure of the data [6]. Meanwhile, contrastive learning is also increasingly combined with knowledge embedding models to enable the models to better distinguish different entities and relation- ships and obtain more semantically informative representations by comparing the similarities and differences between positive and nega- tive sample pairs [7]. Most existing contrastive learning methods rely excessively on pre-existing training data; consequently, they perform poorly when dealing with entirely new, unseen data and generalize



* Corresponding author.
E-mail addresses: 270050@nau.edu.cn (M. Wang), xufei.an@mail.utoronto.ca (F. An).

https://doi.org/10.1016/j.eij.2023.100414
Received 24 September 2023; Received in revised form 22 October 2023; Accepted 30 October 2023
Available online 4 November 2023
1110-8665/© 2023 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



poorly to low-frequency data when dealing with long-tailed distribu- tions [8]. Hyperbolic representation learning methods also suffer from an insufficient number of samples during model training due to the long- tail problem, and the hyperbolic nature of the spatial metric causes nodes in embedding vectors that are farther away from the origin to be more susceptible to data distortion, making it challenging to represent the semantic relationships between them accurately [9].
To address the abovementioned issues, this paper proposes the Att- CL method, which integrates hyperbolic representation learning and contrastive learning. This method involves embedding knowledge into a hyperbolic space. For samples with limited hierarchical characteristics and insufficient feature information, a method of overlaying adversarial noise is employed. The loss function of the embedded samples is back- propagated to the embedding vectors. Perturbations are added and adjusted in the gradient direction to make them smoother and more localized. Simultaneously, a hyperparameter is introduced to fine-tune the adversarial strength when constructing adversarial samples for
data augmentation, thereby enhancing the model’s robustness and mitigating the impacts of insufficient features and insufficient training
due to the long-tail problem in knowledge graph completion tasks. To optimize the data distortion caused by hyperbolic metric constraints, a penalty term is introduced in the contrastive loss function to control the distances of the embedding vectors from the origin, reducing the influ- ence of the metrics and further improving the completion ability of the model.

Related work

Embedding is a technique commonly employed for addressing the knowledge graph completion problem. It involves mapping entities and relations into a lower-dimensional vector space, allowing for the quan- tification of entity similarity and relation distance. Effectively capturing the interconnectedness among entities while retaining the integrity of the original information [10,11]. The TransE model [12] uses the Euclidean distance to measure the matching between entities and re- lations; it combines entity vectors with relation vectors to capture interentity relations, but it has difficulty handling complex relations. The DistMult model [13] uses the dot product to measure entity and relation similarity; it avoids illegitimate entity combinations by restricting the relation vectors, but it is limited in its representation ability. The ComplEx model [14] extends the DistMult model to the complex domain and uses complex vectors to represent entities and re- lations and compute their similarity, but it has many parameters and is computationally complex. Graph neural network models focus on capturing complex entity and relation interactions to forecast and recover missing data [15]. The ConvE model [16] uses convolutional neural networks to learn the interaction patterns of entities and re- lationships, which are then applied for information prediction and filling.
In contrast, hyperbolic representation learning uses hyperbolic ge-
ometry to map nodes into a hyperbolic space and is better at handling hierarchical structures and nonlinear relationships [17]. Learning the hyperbolic distances between nodes is more suitable for dealing with sparse, high-dimensional data, leading to stronger expressive and generalization capabilities [18]. The MuRP model [19] is an extension of
Poincar´e embedding [20] in which entities and relations are embedded
into the Poincar´e sphere and the embedding vectors are optimized using
amplified, resulting in data distortion in the embedding space, which limits the accurate representation of semantic relationships.
In contrastive learning [23], by comparing the similarities and dif- ferences between positive and negative sample pairs and increasing the diversity of the training samples, it is possible to better distinguish be- tween different entities and relationships to obtain more semantically informative representations [24]. SimCTG [25] is a contrastive learning framework proposed for neural text generation tasks that addresses the problems encountered in decoding methods, encourages diversity and coherence, and improves the calibration of the representation space of language models. SimKGC [26] uses a dual encoder architecture, in- troduces three harmful sample types, and uses the cosine similarity for tail entity prediction to improve the learning efficiency. KGE-CL [27]
captures the semantic similarity of related entities and entity–relation- ship pairs in different triples through contrastive learning and in-
corporates adaptive weighting, joint sampling, and an optimized sampling distribution to improve the sampling efficiency and model performance. However, when faced with long-tailed data, these methods have limited model generalization capabilities due to insufficient training examples.
To address the aforementioned issues, this paper proposes a knowl- edge graph completion model that integrates hyperbolic representation learning and contrastive learning. Positive and negative samples are constructed for contrastive learning using triples from the existing knowledge graph. For samples with insufficient feature information, adversarial samples are created by adding adversarial noise, thereby
enhancing the model’s robustness and addressing the long-tail problem through data augmentation. Additionally, a penalty term is introduced
to achieve better control over the contrastive loss function. This opti- mization mitigates the data distortion caused by the long-tail problem and the use of hyperbolic metrics, thereby enhancing the graph completion ability.

Knowledge graph completion method based on hyperbolic representation learning and contrastive learning

Formal definitions

In this paper, S is used to represent the set of all triples in the knowledge graph, and each triple is denoted by (h, r, t), where h denotes the head entity, r denotes the relationship, and t denotes the tail entity. Z
denotes the set of entities, R denotes the set of relationships, and eH and
rH denote the hyperbolic embeddings of entities and relations, respec- tively. c is the curvature of the hyperbolic space, and ⊕ c denotes Mo¨bius addition. dc denote the distance measure in hyperbolic space, θr and φr
are formal definitions of rotations and reflections with relation speci- ficity, G±(θ) are the Givens transformations, bh and bt are hyperbolic decision boundaries, xE and yE denote the embedded data points of a
particular relation in the hyperbolic space after reflection and rotation transformations, and the attention weight vector is denoted by α. The formal definitions are expressed as follows:
Exponential arithmetic function:
 2 
( , ) = √c	(  ‖ — ⊕  ‖)	(1)

Exponential arithmetic function:
v

a negative contrastive loss function. AttH [21] considers interentity re- lations in conjunction with attention mechanisms to enhance or atten- uate entity interactions through rotation, reflection and attention. KGAT
[22] combines graph neural networks with hyperbolic representation learning, using attention mechanisms to capture entity and relationship importance from knowledge embedded in a hyperbolic space. Methods based on hyperbolic representation learning have demonstrated excel- lent performance. However, due to the metric properties of hyperbolic space, distances between nodes far from the origin may be excessively
c
‖ ‖
Logarithmic arithmetic function:
y
‖ ‖
Mo¨bius addition of vectors:
(2)



(3)




Fig. 1. Construction of adversarial samples: (a) Against the sample building process, (b) Adversarial sample joining training.



x ⊕ cy =
1  2CxT y  C y 2 x	1  C x 2 y
1 + 2CxT y + C2 ‖x‖2 ‖y‖2	(4)
sample pairs, and mismatched entities and relations are randomly generated as adversarial sample pairs. However, when facing the long- tailed data problem, especially when the number of dataset levels and

Rotation operation:
Rot(θr) = diag(G+(θr, 1), ⋯, G+(θr, d))	(5) Reflection operation:
Ref (φr) = diag(G—(φr, 1), ⋯, G—(φr, n) )	(6) Hyperbolic attention function:
 αx, αy) = Softmax αxxE, αyyE)	(7)
Combined vector function:
q(h, r) = (αxqH + αyqH ; ar)⊕cr rH	(8)

Hyperbolic embedding scoring function:
the overall network branching situation are limited and fragmented, more information about the sample features is needed. Consequently, it is challenging to generate uniformly effective positive and negative samples for data augmentation, leading to a reduction in model per- formance with respect to its generalization capabilities. Therefore, this paper proposes an adversarial sample data enhancement method based on hyperbolic representation learning. Adversarial samples refer to the creation of samples that are similar to real ones but slightly different. This is achieved by introducing specific perturbations or noise into the original data. After obtaining hyperbolic embedding vectors using the hyperbolic representation learning model ATTH, we superimpose adversarial noise to generate adversarial samples. This approach not only increases data diversity and broadens the training data’s scope but
also empowers the model to more effectively understand and handle the
rare and imbalanced entities and relationships in the knowledge graph. Additionally, it diminishes the influence of the long-tail problem on knowledge graph completion and concurrently alleviates overfitting.

f (h, r, t) = — dcr (q(h, r), eH 2
bh + bt	(9)
Since the introduced adversarial noise is based on the distribution of real data, the model can more effectively learn the intrinsic data features, reducing its dependence on specific sample distributions. This

Adversarial sample data enhancement method based on hyperbolic representation learning

Traditional models for hyperbolic representation learning utilize hyperbolic space to embed knowledge graphs during graph completion tasks. The existing triples in the knowledge graph are utilized as positive
improvement enhances the model’s robustness and generalization abilities.
In this paper, we propose a data enhancement method based on hyperbolic representation learning called Generating Adversarial Sam- ples (GAS), which generates adversarial samples for embedded samples
qi(hi, ri). For the embedded samples for which the number of relationally



connected entities in each triple is less than the average value, adver- sarial noise is used to construct adversarial samples for data enhance- ment. That is, the embedded samples are the first pairs of vectors qi(hi, ri). For qi+(hi, ri), the loss function L Att is calculated; then, this loss
L Att is backpropagated to the embedded vectors in the input samples,
and L Att is used to calculate the gradient of the pair of input embedded vectors, ∇L. This gradient will indicate the direction in which to increase the loss given the current model parameters. Next, the gradient vector is normalized with respect to ‖∇L‖ to control the range of the gradient. A perturbation  that is proportional to the calculated gradient is added to
the embedding vector in the gradient direction. By scaling the pertur- bation δ relative to the size of the gradient, a smoother and more localized result is obtained, resulting in a smooth and continuous embedding and thus preserving the original semantic information. To
algorithm is given below.


Algorithm 1: Generating Adversarial Samples (GAS)

Input: Triplets (qi , qj ), Avg rel numn, dataset, ε, α
Output: adversarial samples
Step 1: Calculate the Adversarial Loss L Att ;
L Att ← Calculate loss (qi , qj );
Step 2: Check the number of entities connected by relations and the average number;
foreach triplet (qi , qj ) in dataset do
if num enti rel(qi ) < Avg rel numn then
proceed to Step 3; else
skip to the next triplet;
end end
Step 3: Backpropagate to compute the gradient ∇Li ;

this end, a hyperparameter α for scaling the gradient is introduced to achieve fine-grained control of the adversarial strength; i.e., by adjust-
∇Li
∂L Att
← ∂qi ;

ing α, the strength of the adversarial perturbation can be fine-tuned. The perturbation δ is also multiplied by the gradient sign sign(∇′ ) to make the adversarial samples move in the increasing direction of the loss
function so that the constructed adversarial samples will have better discriminative properties in the hyperbolic space. The constructed adversarial samples are then mapped back to the hyperbolic space by means of an exponential operation. This approach allows the model to more effectively accommodate various forms of noise and disruptions, thereby enhancing the model’s generalization capacity and overall
performance. The relevant formulas are as follows:
Step 4: Generate counter noise;
δ←ε • sign(∇Li ) • α;
Step 5: Normalize the gradient; norm_grad ← normalize(∇Li );
Step 6: Generate adversarial samples;
adversarial_samples ← { };
foreach embedding qi in triplets do adversarial sample ←qi + δ • sign(∇Li ); adversarial-samples.add(adversarial-sample);
end
Step 7: Index mapping;
adversarial_samples ← Index mapping(adversarial_samples);
return adversarial_samples

∇Li = ∂L Att qi, qi+ )
√√√∑̅̅̅d̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅2̅

(10)



Contrastive loss method incorporating hyperbolic representation learning

‖∇Li‖ =

′

i=1

 ∇Li 
(∇Li)
(11)
This paper proposes a contrastive loss method with a penalty term to
integrate contrastive learning with hyperbolic representation learning and avoid distortion of the embedding vectors caused by the hyperbolic

∇Li = c ‖∇Li ‖	(12)
δ = ε⋅sign(∇Li)⋅α	(13)
qad = expc(logc(q) + δ⋅sign(∇L′ ))	(14)
L Att = ∑ log(1 + exp(f (h, r, t) ) ) + ∑ log(1 + exp(f (h, r, t′) ) )	(15)
spatial metric, which can cause the samples to be overdispersed in the hyperbolic space. By introducing an appropriate penalty term into the contrastive loss function, the hyperbolic distance of the embedding vector from the origin is adjusted to avoid the influence of the metric, which causes embedding vectors to deviate far from the origin, resulting in data distortion. With this penalty term, after positive samples are mapped into the hyperbolic space by means of the contrastive loss

(h,r,t)∈Z
(h,r,t′)∈∕Z
function, they will have similar features that retain as much information as possible. At the same time, they will be distributed as evenly as

Due to the nature of the spatial metric in the hyperbolic space, to
avoid the constructed adversarial samples being distorted by the influ- ence of nodes that are too far away from the origin, the variable c is set in Eq. (12) to control the range of the gradient and the distance from the

possible over the space.
In this paper, we propose the Contrastive Loss in Hyperbolic Space (CLHS) method for fused hyperbolic representation learning. the first
step is to compute the hyperbolic distances dcr (q , q ) between two

and ε is the magnitude of the perturbation; sign(∇L) is a symbolic
function of the gradient ∇L that takes the sign of each component of the gradient, thus retaining the direction information, and α is the control factor that is used to scale the magnitude of the gradient to control the
range and size of the perturbation. In Eq. (14), q is the embedding vector of the input sample, and sign is the sign function, which ensures that the direction of the added perturbation is consistent with the direction of the gradient. The adversarial samples that are obtained by adding pertur- bations in the direction of the gradient are then used along with the
original positive samples to train the model, thereby enhancing the model’s resistance to different types of noise and attacks. In Eq. (15), f(h, r, t) is the hyperbolic embedding scoring function, which compares the
resulting embedding with the target tail entity after the isometric
transformation through the hyperbolic distance is applied, and L Att is the final cross-entropy loss function for the embedding. The process of constructing adversarial samples is illustrated in Fig. 1, and the
embedded vector samples (Eq. (1) and use the exponentials of the dis-
tances, edcr (qi ,qj ) and edcr (qj ,qi ), to help the model learn the similarities and differences between the samples. This is because, in contrastive learning, it is desired to have smaller distances between pairs of positive samples and larger distances between pairs of negative samples. Therefore, considering the distances in both directions allows for better learning of the similarities and differences between samples. In the second step, normalization factors are calculated, denoted by zi and zj. Due to the unique nature of metrics in hyperbolic space, the distances between nodes far away from the origin and other nodes will tend to- wards infinity. To avoid drastic changes in the output results, leading to unstable calculation results, normalization is needed. In the third step, the hyperbolic distance is converted into a similarity score for subse- quent processing using the contrastive loss function. In the fourth step, the distance from the origin is considered in the contrastive loss func-
tion. The embedding vector’s distance from the origin is adjusted by adding a penalty term to avoid the influence of the hyperbolic spatial
metric, which leads to data distortion. When setting the weight of the




Fig. 2. Overall diagram of the Att-CL framework.



penalty term, λr, the relation-specific curvature cr of the embedding vector is adjusted through a mapping function, and the result is normalized. The output value of the mapping function is divided by a constant K so that the final value of the penalty term weights will lie in the range of [0,1].
dcr qj, qk denote the hyperbolic distances between the embedding vectors qi and qj and the embedding vector of the k-th sample in the sample set. zi denotes the sum of the exponentials of the hyperbolic
distances from the embedding vector qi to the embedding vectors of all
samples in the sample set; similarly, zj denotes the sum of the expo-

N	N

z = ∑ edcr (qi ,qk ), z = ∑ edcr (qj ,qk )	(16)
nentials of the hyperbolic distances from the embedding vector qj to the

i
k=1
j
k=1
embedding vectors of all samples in the sample set. In Eq. (17), edcr (qi ,qj )
denotes the exponential of the distance from embedding vector q to q ,

i	j

1 edcr (qi ,qj ) s qi, qj = — 2 ( z
f c ′	1
(1 + exp(—cr))
λ r	f (cr′) K
edcr (qj ,qi )
+	z	)	(17)

(18)


(19)
and edcr (qj ,qi ) denotes the exponential of the distance from embedding vector q to q . In contrastive learning, the exponentials of the distances between pairs of positive samples is desired to be smaller, and the ex- ponentials of the distances between pairs of negative samples should be larger. Therefore, considering the distances in both directions can help the model better learn the similarities and differences between samples. The sigmoid function is used in Eq. (18) to map cr to the range [0,1] such that relationships with smaller cr map to values closer to 0 and re-
lationships with larger cr map to values closer to 1. In this way, the

	1	
|x|x(|x| — 1)
s qhi , 0)}
|x|	|x|

i=1,j=1 j=∕i
max{0, ρ — s qhi , qhi )
+ s(qhi , qhj )
+ λ r



(20)
curvatures are converted into penalty term weights that are used to calibrate the embedding vectors in the representation space in the following contrastive loss function. In Eq. (20), λr is a hyperparameter used to control the penalty term based on the embedding vector’s dis-
tance from the origin, which can simultaneously bring positive sample

In Eq. (16), qi and qj denote the two embedding vectors of the input samples for which the comparison loss calculation is performed, namely,
the two samples in the positive (or negative) sample pair. N represents the overall count of samples within the sample set, and dcr (qi, qk) and
pairs closer and push negative sample pairs farther apart while con- straining the embedding vectors to not be too far away from the origin.



The contrastive loss algorithm is given below.


Algorithm 2: Contrastive Loss in Hyperbolic Space (CLHS)


Input: Triplets (qi , qj ), curvature c, K, |X|, ρ
Output: Contrastive Loss L cl
Step 1: Calculate Hyperbolic Distance dc; dc← 2 • (,̅c̅ —qi ⊕ cqj );
is adjusted to control the hyperbolic distances of the embedding vectors from the origin to avoid distortion of data far from the origin due to the influence of the hyperbolic spatial metric. Finally, by means of the contrastive loss function, Att-CL maps positive samples into the hyper- bolic space to obtain close features. At the same time, as much infor- mation as possible is retained in the features, ensuring that this
information is uniformly distributed in the hyperbolic space. The overall

Step 2: Calculate Normalization Factors zi and zj ; zi ←0, zj ←0;
training objective L

Att—cl
is defined as:

foreach k = 1 to N do
z ←z + edc (qi ,qk );
L Att—cl = L Att + L cl	(21)

z ←z + edc (qj ,qk );

j  j
end
Step 3: Compute Similarity Score s; 1 (edc (qi ,qj ) edc (qj ,qi ))
 

where L Att is the loss function used in the adversarial sample data enhancement method based on hyperbolic representation learning, as presented in Section 3.2, and L cl is the comparative loss function



Step 4: Compute Penalty Term;
c′ ←	1	
r  1 + exp(—cr )
λ  c′
Fig. 2, and the Att-CL model algorithm is given below.


Algorithm 3: Att-CL Framework

Input: Knowledge graph triplets S(h, r, t), curvature c, K, N, |X|, Avg_rel_num n,

r ←r ;
K
Step 5: Calculate Contrastive Loss L cl;
L cl←0;
foreach i = 1 to |X| do foreach j = 1 to |X| do
if j  i then
s ←s q , q ) —s(q , q ) + λ • s q , 0);
dataset, ε, α, ρ
Output: Overall Loss L Att—cl
Step 1: Hyperbolic embedding vector by ATTH;
q(h, r)←ATTH(h, r, t);
Step 2: Generate Adversarial Samples using GAS;
Adversarial samples←GAS(q, n, dataset, ε, α);
L Att ←Calculate Adversarial Loss(q, adversarial samples);

L cl←L cl + max(0, ρ — sij);
end end
end
L cl←CLHS(adversarial samples , c, K, |X|);
Step 4: Overall Loss Loss L Att—cl;
L Att—cl ←L Att + L cl ;
return L Att—cl

L cl
	1	
←	L
|X| • (|X| — 1)


cl ;

return L cl




Overall model framework

To address the problems of long-tailed data and the influence of the hyperbolic spatial metric, which lead to poor model generalization and easy data distortion for samples with insufficient feature information, this paper proposes a knowledge graph completion method based on hyperbolic representation learning and contrastive learning, called Att-
CL. The overall framework is as follows for all triples in the knowledge graph S(h, r, t), positive sample pairs are constructed with the same relationship or the same entity–relationship pair based on the head entity or tail entity, and mismatched entity–relationship combinations are randomly selected from the knowledge graph to construct adversa-
rial sample pairs. To improve the representativeness of the embedding vectors, Att-CL employs the AttH method for hyperbolic representation learning. In this method, the samples are transformed by rotation and reflection operations. An attention mechanism is introduced to weigh
the vectors, which are summed with the relation vectors to form the final aggregated vectors. The scores f(h, r, t) between pairs of samples and the loss function L Att are computed. To cope with the situation in which the
number of relationally connected entities in the knowledge graph is low, Att-CL makes a judgement on each triple. Suppose that the number of connected entities is lower than the average. In that case, adversarial samples are generated via the introduction of adversarial noise for data augmentation, and the generated adversarial samples are added to the sample pairs for contrastive learning.
During contrastive learning, a penalty term is introduced into the contrastive loss function of Att-CL, and the weight λr of the penalty term
Experimental analysis

Datasets

In this paper, we consider two benchmark datasets commonly used for knowledge graph completion tasks, namely, WN18RR [28] and FB15k-237 [29]. The information characteristics of each dataset are shown in Table 1. The link prediction task was used in experiments to evaluate the proposed method and verify its generality over different knowledge graphs. The WN18RR dataset is a rearranged version of the WordNet [30] dataset, used for the WordNet knowledge graph completion task. It contains 18 relationship types (Relations) and 40,943 entities (Entities). Compared to the original WN18 dataset, the WN18RR dataset has been rearranged to increase the task difficulty by eliminating simple triple violations and increasing the number of negative samples in the training set, making it more consistent with real-world knowledge graph completion tasks. The FB15K-237 dataset is a subset of Freebase
[31] that contains 237 relationship types and 14,505 entities. FB15K- 237 is a streamlined version of the original FB15K dataset obtained by removing some uncommon relationship types and entities to reduce the complexity and size of the dataset.

Evaluation metrics and parameters

For a comprehensive evaluation of model performance, several evaluation metrics were used in the experiments, including the following main metrics:
Mean reciprocal rank (MRR): MRR serves as a vital evaluation metric for gauging a model’s ranking quality for correct answers within the link prediction task.


Table 1
Information on the FB15k-237 dataset and the WN18RR dataset (ρ denotes the average number of entities per relational connection).



Table 2
Parameters.


Parameter name	Notation	Parameter value
N
MRR
N i=1 ranki

(22)

Number of convolutional layers	L	1
Gradient control factor	c	0.003
Adversarial perturbation control factor	α	0.6
Perturbation magnitude	ε	0.1
Vector dimensionality	d	32
Batch size	B	512
Number of model iterations	M	600
Learning rate	γ	0.01
where  1  denotes the inverse rank of the i-th sample and N denotes the
i
number of test samples. The higher the positions of the correct answers in the sorted list and the greater their inverse ranks, the higher the model’s performance.
Top-k accuracy (Hits@k): Hits@k is another important evaluation
metric that measures the accuracy of a model in top-k prediction.






Fig. 3. Parameter sensitivity analysis: (a) FB15K-237, (b) WN18RR.


Table 3
Knowledge graph link prediction results for the FB15k-237 dataset and WN18RR dataset.


Table 4
Knowledge graph link prediction results for the FB15k-237 dataset and WN18RR dataset (sparse data).


N
Hits k	I rank	k N i=1
contrastive learning. Compared with the seven models mentioned above, the Att-CL method stands out with substantial enhancements in the majority of evaluation metrics. On average, we observed remarkable

I(ranki ≤
k) =
1, ranki	k
0, ranki > k
improvements of 5.71 % in MRR, 2.98 % in Hits@1, 4.04 % in Hits@3, and 4.88 % in Hits@10. The Att-CL method utilizes hyperbolic isometric transformations (rotations and reflections) and hyperbolic attention to

The value of Hits@k is obtained by averaging the values of the in-
dicator function over all samples.
During the experiments, the RSGD [32] optimizer was used for hy- perbolic embedding, while the Adam [33] optimizer was employed for contrastive learning. The model’s hyperparameters were selected
through a grid search and determined based on the best performance on
the validation set, as shown in Table 2. When constructing adversarial samples, α is a crucial hyperparameter primarily used to fine-tune the magnitude of the perturbations, thereby controlling the strength of the adversarial samples. This is because excessively strong perturbations might introduce noise, harming the model’s representation ability,
while overly weak perturbations might not effectively enhance the
model’s robustness. To validate the optimal α value, experiments were conducted withα = 0.2, 0.4, 0.6, 0.8, using MRR and Hits@10 as the assessment criteria. The outcomes of the experiments are depicted in
Fig. 3, the results of the experiments suggest that the model achieved optimal performance with α = 0.6.

Results and analysis of experiments

Comparative analysis of overall model performance
To evaluate the knowledge graph completion method combining hyperbolic representation learning and contrastive learning that is proposed in this paper, the following models were selected for comparative experiments: the translation-based TransE and MuRE models in Euclidean space; the ConvE model, based on a convolutional neural network; the tensor decomposition-based DistMult model; the RotatE and ComplEx models in the complex domain; and the MuRP and ATTH models in hyperbolic space. These models represent the most
commonly used and advanced methods for knowledge graph completion tasks. You can find the results of our experiments in Table 3. We’ve highlighted the best outcomes in bold to make them stand out. On the other hand, we’ve underlined the suboptimal results.
The experimental results provide valuable insights. They reveal that
the Att-CL method, which we introduced in this paper, effectively har- nesses the strengths of both hyperbolic representation learning and
perform hyperbolic embedding. By employing adversarial noise super- position to construct adversarial samples for data enhancement, the hierarchical structure and logical patterns of knowledge graphs can be more effectively captured, making the proposed method especially suitable for processing knowledge graphs with tree-like hierarchical structures. By introducing a penalty term into the contrastive loss function to adjust the hyperbolic distances between the embedding vectors and the origin, similar entities and relations are represented as similar locations in hyperbolic space, while dissimilar entities and re-
lations are represented as more distant locations. In this way, the model’s capacity for expression experiences a notable enhancement, and the model becomes better able to distinguish semantic associations be-
tween different entities and relations, thus effectively overcoming the data distortion problem suffered by traditional hyperbolic representa-
tion learning models, preserving the knowledge graph’s global structure and hierarchical relationships, and improving the model’s performance
in knowledge graph completion. Owing to the comparatively modest dimensions of the WN18RR dataset and the fact that the relations it contains mainly involve associations between word meanings, it is pri- marily focused on semantic relations. Therefore, the Att-CL model shows greater improvement on the FB15k-237 dataset than on the WN18RR dataset.

Ablation experiments
To explore the efficacy of constructing adversarial samples for data augmentation, assess the model’s performance in handling infrequent and uncommon relationships in a dataset, and conduct a comprehensive
assessment of the impact of the method for constructing adversarial samples on enhancing the efficiency of knowledge graph completion. This study selects the ATTH, MURP, and Att-CL models for examination. Link prediction experiments were executed on the triples within each dataset, focusing on which relationships with fewer connected entities
than the dataset’s average. The outcomes are presented in Table 4, with the best results highlighted and the suboptimal results underscored.
The experimental findings reveal that the utilization of adversarial




Fig. 4. Embedded vector visualization and analysis diagrams: (a) MURP model, (b) ATTH model, (c) Att-CL model.



Table 5
Comparison of the spatial complexities of knowledge graph completion models.

Method	spatial complexity


TransE	O(|E|n + |R|n)
TransH	O(|E|n + |R|n)
TransR	O(|E|n + |R|n2)
DistMult	O(|E|n + |R|n)
ComplEx	O(2|E|n + 2|R|n)
RESCAL	O(|E|n + |R|n2)
RotatE	O(|E|n + |R|n)
  Att-CL	O(|E|n + |R|n)	

sample construction for data augmentation effectively enhances the model’s capacity to learn from long-tailed data and improves its generalization capabilities. In comparison to both the ATTH and MURP
models, which generate affirmative and negative samples, the technique of data enhancement through adversarial sample construction exhibits a pronounced advantage, resulting in average enhancements of 2.03 %,
2.2 %, 1.6 %, and 2.62 % in MRR, Hits@1, Hits@3, and Hits@10, respectively. Furthermore, further enhancement of the embedding space by integrating hyperbolic representation learning and contrastive learning mechanisms can yield superior results and performance in the task of completing knowledge graphs.
In addition, this paper presents visualizations to verify the effec- tiveness of introducing a penalty term in the contrastive loss function to control the embedding vectors’ distances from the origin to avoid node
distortion. Fig. 4 illustrates the relevant results.
The MURP model (Fig. 4, a), ATTH model (Fig. 4, b) and Att-CL model (Fig. 4, c) were selected for experiments to compare the result- ing embedding vector distortion on the FB15K-237 dataset, and the
embedded nodes were mapped onto Poincare´ disc for visualization. In
hyperbolic space, the radius of the Poincare´ disc grows exponentially. Usually, nodes with rich entity–relationship feature information are
located near the centre of the Poincare´ disc, and the model is fully
trained on such nodes. In contrast, nodes representing sparse and long- tailed data are located closer to those with similar angular coordinates. As seen in Fig. 4, the problems of imbalance, undertraining, and hy- perbolic embedding distortion observed on the long-tailed data in the sparse FB15K-237 data are greatly mitigated by the construction of adversarial samples for data enhancement as well as the addition of the proposed penalty term in the loss function. These modifications enhance
the variety of the training data and, to some degree, enhance the model’s capacity for generalization, avoiding the construction of excess outlier
pairs of samples in the embedded hyperbolic space, which can lead to node distortion. With the combination of these two approaches, the model pays more attention to samples in the long tail of the distribution during training, thereby balancing the sample distribution, improving the embedding of such long-tail samples, and improving their prediction accuracy.

Complexity analysis

Table 5 shows the spatial complexity of the Att-CL model compared to several popular models. It can be observed that Att-CL has a similar spatial complexity compared to classical KGC models like TransE, Dis- tMult, and RotatE.

Conclusion

Existing hyperbolic representation learning models can preserve the hierarchical structure and logical patterns of nodes in the knowledge graph completion task but still suffer from the long-tail problem and node distortion. This paper introduces the Att-CL model, which in- tegrates hyperbolic representation learning with contrastive learning to more accurately preserve the hierarchical structure and logical patterns
of knowledge graphs, thereby improving the effectiveness of knowledge graph completion. Data augmentation is achieved by constructing adversarial samples and increasing the quantity of training samples corresponding to the long tail of the data distribution. This enhancement
aims to improve the model’s capacity to generalize when faced with
long-tailed data, and a penalty term is introduced in the contrastive loss function to effectively avoid excessive deviation of adversarial samples and long-tail data in the hyperbolic space and thus mitigate the node distortion problem. The results of experiments conducted on various datasets consistently demonstrate that this method performs stably and reliably on different datasets and achieves improvements relative to mainstream benchmark algorithms, especially on the FB15k-237 data- set, which has a more pronounced degree of sparsity. In subsequent research, we will compare the effects of multiple different contrastive learning methods in terms of model performance to search for a more suitable contrastive learning strategy for the task of knowledge graph completion, and we will attempt to apply the model introduced in this paper within various contexts, such as recommender systems, text generation, and significant language models to further improve the application scope of this model.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgements

We thank all those that have contributed to our projects over the years, and the reviewers whose critical feedback helped to improve this article.

Funding

This study was supported by The National Key Research and Development Program of China (No. 2019YFB1404602) and the Post- graduate Research & Practice Innovation Program of Jiangsu Province (SJCX23_1096).

Ethical statement

Not applicable

References

Ji S, Pan S, Cambria E, Marttinen P, Philip SY. A survey on knowledge graphs:
Representation, acquisition, and applications. IEEE Trans Neural Netw Learn Syst 2021;33:494–514.
Chen X, Jia S, Xiang Y. A review: Knowledge reasoning over knowledge graph.
Expert Syst Appl 2020;141:112948.
Zamini M, Reza H, Rabiei M. A review of knowledge graph completion. Information 2022;13:396.
Chen Z, Wang Y, Zhao B, Cheng J, Zhao X, Duan Z. Knowledge graph completion: A review. IEEE Access 2020;8:192435–56.
Shen T, Zhang F, Cheng J. A comprehensive overview of knowledge graph
completion. Knowl Based Syst 2022:109597.
Dhingra B, Shallue CJ, Norouzi M, Dai AM, Dahl GE. Embedding text in hyperbolic spaces. ArXiv Preprint ArXiv:180604313 2018.
Moon W, Kim J-H, Heo J-P. Tailoring self-supervision for supervised learning. Eur Conf Comput Vis 2022:346–64.
Purushwalkam S, Morgado P, Gupta A. The challenges of continuous self-
supervised learning. Eur Conf Comput Vis 2022:702–21.
Wang Z, Lai KP, Li P, Bing L, Lam W. Tackling long-tailed relations and uncommon entities in knowledge graph completion. ArXiv Preprint ArXiv:190911359 2019.
Tiddi I, Schlobach S. Knowledge graphs as tools for explainable machine learning: A survey. Artif Intell 2022;302:103627.
Wang Q, Mao Z, Wang B, Guo L. Knowledge graph embedding: A survey of approaches and applications. IEEE Trans Knowl Data Eng 2017;29:2724–43.
Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko O. Translating
embeddings for modeling multi-relational data. Adv Neural Inf Process Syst 2013; 26.



Yang B, Yih W, He X, Gao J, Deng L. Embedding entities and relations for learning and inference in knowledge bases. ArXiv Preprint ArXiv:14126575 2014.
Trouillon T, Welbl J, Riedel S, Gaussier E´, Bouchard G. Complex embeddings for
simple link prediction. Int Conf Mach Learn 2016:2071–80.
Chami I, Ying Z, Re´ C, Leskovec J. Hyperbolic graph convolutional neural
networks. Adv Neural Inf Process Syst 2019;32.
Dettmers T, Minervini P, Stenetorp P, Riedel S. Convolutional 2d knowledge graph embeddings. Proceedings of the AAAI conference on artificial intelligence, vol. 32, 2018.
Peng W, Varanka T, Mostafa A, Shi H, Zhao G. Hyperbolic deep neural networks: A survey. IEEE Trans Pattern Anal Mach Intell 2021;44:10023–44.
Sun Z, Chen M, Hu W, Wang C, Dai J, Zhang W. Knowledge association with
hyperbolic knowledge graph embeddings. ArXiv Preprint ArXiv:201002162 2020.
Balazevic I, Allen C, Hospedales T. Multi-relational poincare´ graph embeddings. Adv Neural Inf Process Syst 2019;32.
Nickel M, Kiela D. Poincare´ embeddings for learning hierarchical representations.
Adv Neural Inf Process Syst 2017;30.
Chami I, Wolf A, Juan D-C, Sala F, Ravi S, Re´ C. Low-dimensional hyperbolic knowledge graph embeddings. ArXiv Preprint ArXiv:200500545 2020.
Wang X, He X, Cao Y, Liu M, Kgat C-S. Knowledge graph attention network for
recommendation. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining; 2019. p. 950–8.
Wang T, Isola P. Understanding contrastive representation learning through
alignment and uniformity on the hypersphere. Int Conf Mach Learn 2020:9929–39.
Jaiswal A, Babu AR, Zadeh MZ, Banerjee D, Makedon F. A survey on contrastive self-supervised learning. Technologies (Basel) 2020;9:2.
Su Y, Lan T, Wang Y, Yogatama D, Kong L, Collier N. A contrastive framework for neural text generation. Adv Neural Inf Process Syst 2022;35:21548–61.
Wang L, Zhao W, Wei Z, Liu J. SimKGC: Simple contrastive knowledge graph
completion with pre-trained language models. ArXiv Preprint ArXiv:220302167 2022.
Luo Z, Xu W, Liu W, Bian J, Yin J, Liu T-Y. KGE-CL: Contrastive learning of tensor decomposition based knowledge graph embeddings. ArXiv Preprint ArXiv: 211204871 2021.
Han X, Cao S, Lv X, Lin Y, Liu Z, Sun M, et al. Openke: An open toolkit for knowledge embedding. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations; 2018.
p. 139–44.
Toutanova K, Chen D. Observed versus latent features for knowledge base and text
inference. Proceedings of the 3rd workshop on continuous vector space models and their compositionality, 2015, p. 57–66.
Miller GA. WordNet: a lexical database for English. Commun ACM 1995;38:39–41.
Bollacker K, Evans C, Paritosh P, Sturge T, Taylor J. Freebase: a collaboratively created graph database for structuring human knowledge. In: Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data; 2008.
p. 1247–50.
B´ecigneul G, Ganea O-E. Riemannian adaptive optimization methods. ArXiv
Preprint ArXiv:181000760 2018.
Kingma DP, Ba J. Adam: A method for stochastic optimization. ArXiv Preprint ArXiv:14126980 2014.
