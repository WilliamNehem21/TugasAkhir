Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 321 (2016) 3–21
www.elsevier.com/locate/entcs

Conditional Monte Carlo With Intermediate Estimations for Simulation of Markovian Systems
H´ector Cancela1,2
Facultad de Ingenier´ıa Universidad de la Repu´blica Montevideo, Uruguay
Leslie Murray1,3
Facultad de Ciencias Exactas, Ingenier´ıa y Agrimensura Universidad Nacional de Rosario
Rosario, Argentina
Gerardo Rubino1,4
INRIA–Rennes
Bretagne Atlantique, Campus de Beaulieu Rennes, France

Abstract
For systems that are suitable to be modelled by continuous Markov chains, dependability analysis is not always straightforward. When such systems are large and complex, it is usually impossible to compute their dependability measures exactly. An alternative solution is to estimate them by simulation, typically by Monte Carlo simulation. But for highly reliable systems standard simulation can not reach satisfactory accuracy levels (measured by the variance of the estimator) within reasonable computing times. Conditional
Monte Carlo with Intermediate Estimations (CMIE) is a simulation method proposal aimed at making accurate estimations of dependability measures on highly reliable Markovian systems. The basis of CMIE is introduced, the unbiasedness of the corresponding estimator is proven, and its variance is shown to be lower than the variance of the standard estimator. A variant of the basic scheme, that applies to large and highly reliable multicomponent systems, is introduced. Some experimental results are shown.
Keywords: dependability, simulation, rare event, Conditional Monte Carlo.


1 This work has been partially supported by Programa de Desarrollo de Ciencias B´asicas, PEDECIBA – Inform´atica, and by STIC-AMSUD projects 13STIC-01 AMMA, “Accelerating Markov Models for analysis and design of dynamic WDM optical networks”, and 15STIC-07 DAT, “Dependability Analysis Tool”.
2 Email: cancela@fing.edu.uy
3 Email: leslie@eie.fceia.unr.edu.ar
4 Email: Gerardo.Rubino@inria.fr

http://dx.doi.org/10.1016/j.entcs.2016.02.002
1571-0661/© 2016 The Authors. Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Introduction

We consider systems which can be modelled by a continuous time homogeneous Markov chain X irreducible on the finite state space S (see [5,10,11] or Chapter 6 in [22]). The chain can also be absorbing and the techniques described here still work, but they are easier to present in the irreducible case. In this context, some measures of dependability need the evaluation of the probability γ = P{τD < τu}, where the times τu and τD are defined as follows. The state space of the Markov chain is partitioned as S = U ∪ D, such that in U the system is up and in D the system is down. The process X starts at some initial state u ∈ U . Define τu as the return time to u, that is, τu = inf{t > 0: X(t)= u and X(t−) /= u}, and τD as the hitting time of D, that is, τD = inf{t > 0: X(t) ∈ D}.
The simplest and most basic dependability metric is the Mean Time To Failure (MTTF), defined as the expected life–time of the system, that is, the mean time until the system enters the subset D: MTTF = E{τD}. This metric admits the well–known representation MTTF = E{min(τD, τu)}/γ.
Since we focus on the estimation of γ, we can just collapse all D into a single
state d, made absorbing. As before, event {τd < τu} means that X gets absorbed at d before coming back to u. For systems with a large (or infinite) number of states, the exact computation of γ is not feasible, and the standard Monte Carlo simulation will work, unless γ 1, in which case we are facing a rare event problem, a context in which acceptable values of the estimator’s variance can only be achieved at the expense of a very high number of replications. Monte Carlo methods must therefore be improved and adapted to address efficiently this rare event problem. Research has resulted in a large number of solutions in this regard, most of which derive from two well known families of techniques named, respectively, Splitting [7,8,13,19,26] and Importance Sampling [4,5,9].
Some applications of Splitting in the context of highly reliable systems can be found in [24] and [25], where the reliability and availability estimations of repairable systems are analysed using a variant called RESTART. Recently, some results in the context of static systems have been published in [3] and [12]. Some methods derived from Importance Sampling, like Zero-Variance [15,16,17] and Cross-Entropy [20,23] have been successfully applied in the simulation of systems affected by rare events.
Conditional Monte Carlo [12,21] is a classic variance reduction technique that has not given rise to many methods in the field of rare events applied to reliability estimation. However, some applications can be found in [1,2,6,27], but most of them are aimed at the rare events probability estimation in models that deal with heavy– tailed distributions. This article addresses the problem of reliability estimation in the model so far defined and introduces a Conditional Monte Carlo simulation scheme, suitable for the estimation of γ.
The rest of this paper is organized as follows. Section 2 shows a basic application of Conditional Monte Carlo on Markovian systems. Section 3, the core of this paper, introduces modifications to the basic Conditional Monte Carlo algorithm, in order

to make it usable and efficient. Sections 4, 5 and 6 discuss some properties and features of the proposed method. Section 7 shows how to apply it to the particular case of Markovian multicomponent systems. Some experimental results are included in Sections 6 and 7. A comparison with Splitting is shown in Section 8. Conclusions and future directions can be found in Section 9.
Conditional Monte Carlo algorithm
There are different simulation methods to estimate value of γ. In the crude or standard simulation, N1 replications start at state u and they are simulated until they either come back to u, in time τu, or hit state d, in time τd. Let I be the indicator random variable of the event {τd < τu}:

I = ⎧⎨ 1 w.p.	γ,

Then, γ = E{I}. Its standard estimator, γ , is:
N1

(1)

γ^s
1
=
N
j=1
I(j),	(2)

where I(j),j = 1, 2,..., N1 are N1 independent values sampled from distribution (1).
Let C = {d, k, u}, where k is any state in S, other than d or u, and let XC be a random variable defined as the ﬁrst state in C hit by a replication started at u:



XC =
k w.p. pk,
⎪⎪⎩ u w.p. pu.


See that pd ≤ γ, because γ is the probability that any replication that starts at u reaches d before coming back to u, whereas pd is the same probability, provided that “the path does not contain k”. Similarly, pu ≤ 1 − γ.
The expectation of I, conditioned on the values of XC, is given by the following expressions: E{I | XC = d} = 1, E{I | XC = k} = γk and E{I | XC = u} = 0 (γk is the probability that a replication that starts at state k, hits state d before it hits state u). Thus, E{I | XC} is a random variable with the following probability distribution:


E{I | XC} =
1 w.p. pd,
γk w.p. pk,

(3)

and the following expectation:
E{E{I | XC}} = E{I} =1 × pd + γk × pk +0 × pu = γ.
The expected value of both random variables, I and E{I | XC}, is γ. As a conse- quence, another estimator of γ —namely, a Conditional Monte Carlo estimator— is:

1
γ^c = N
N1

j=1
E{I | X(j)},	(4)

where E{I | X(j)},j = 1, 2,... are N1 independent random variables sharing distri- bution (3). The samples E{I | X(j)} are obtained in two steps: first, X(j) is sampled
C	C
and then, the corresponding value E{I | X(j)} is computed. In this introductory example the only three possible values of X(j) to be sampled are {u, k, d}, whereas the exact values of E{I | X(j)} associated with them are, respectively, {1, γk, 0}.
If the set C includes more intermediate states besides k, the method applies as
well. If, for example, C = {d, 1, 2,..., n, u}, the distribution of E{I | XC} becomes:
⎧ 1 w.p. pd,
γ1 w.p. p1,
⎪

⎪
γn w.p. pn,
0 w.p. pu,
where γi is the probability that a replication that starts at state i hits state d before it hits state u. Now:
n
E{E{I | XC}} = E{I} =1 × pd +	γi pi +0 × pu
i=1
n
=	γi pi = γ,
i=0
where the notation γ0 = 1 and p0 = pd is included for simplicity. The estimator given in Expression (4) remains valid, with the only difference of sampling from the distribution (5) instead of (3).
Figure 1 depicts the set of probabilities so far defined and shows the nomencla- ture used to refer to them in the rest of this article (as γu = 0, the term pu × γu equals 0, reason why it is shown in Figure 1 but does not appear in any further expression).
For any given set C = {d, 1, 2,..., n, u}, call C = C \ {d, u}, i.e. the subset



Fig. 1. The set of probabilities used in all calculations.

The variance of the Conditional Monte Carlo estimator in (4) is:


V{γ } = 1
 E{E{I | X }2}− E{E{I | X
}}2 

c	N1
1
=
N1
C
n
piγ2 − γ2
i=0
C

.	(6)

On the other hand, the variance of the standard estimator given in (2) is known to be:


V{γ } =
1	γ − γ2 = 1
N1	N1
n

i=0

piγi

— γ2
.	(7)

Comparing expressions (6) and (7) and considering that γi ≤ 1, i = 0,..., n, because all these values are probabilities, it is clear that:

n	n
Σ piγ2 ≤ Σ piγi,
i=0	i=0
what means that the variance of the Conditional Monte Carlo estimator given in (6), is never larger than the Standard Monte Carlo estimator variance given in (7). This is, of course, a general fact on Conditional Monte Carlo methods, but it is worth making it explicit in our context.

Conditional Monte Carlo with Intermediate Estima- tions
The main problem in the use of Conditional Monte Carlo, as it was introduced so far, is the fact that the values γ1, γ2, .. ., γn are unknown, and that may be even as

hard to evaluate as the exact value of γ itself. To work around this problem, these values will be now replaced by estimators.
It will be shown that after such replacement, the method is still unbiased. This is the core of the proposal introduced in this article and the basis of the so–called Conditional Monte Carlo with Intermediate Estimations (CMIE ) method. The method will now be described and, at the end of this section, the variance of the corresponding estimator will be determined.
To address the following calculation, it is better to express γ in terms of the ran- dom vector I¯ = (I0, I1,..., In+1), whose components are dependent binary random variables such that one and only one has value 1:
⎧ (1, 0, 0,..., 0, 0) w.p. pd,
(0, 1, 0,..., 0, 0) w.p. p1,
⎪

⎪
(0, 0, 0,..., 1, 0) w.p. pn,
⎪ (0, 0, 0,..., 0, 1) w.p. pu.
Then, being γ0 = 1, the standard estimator, γs, is:
N1

γ^s
1
=
N
j=1
N1
I(j) × γ0

n
+ I(j) × γ1
+ I(j) × γ2
+ ... + I(j) × γn
(j)
n+1

=  1 
N1 j=1
(j)
k	k
k=0
(9)

In (9), the samples I(j), I(j), . . . , I(j) are obtained by the simulation, whereas the
0	1	n
values γ1, γ2, ..., γn must be calculated. However, if such calculation is too hard, or
simply impossible, these values can be replaced by standard estimators. In order to do this, every time the simulation reaches a state i ∈ C, N2 independent replications must be started at i and simulated until they either reach d (and accumulate 1) or u (and accumulate 0). Once these N2 replications started at i are completed, a standard estimator γ^i can be evaluated and used in place of γi. To compute these




J = ⎧ 1 w.p.	γi,
⎩ 0 w.p. 1 − γi,

i = 1, 2,..., n.	(10)

The samples of Ji are obtained from the actual simulation of the Markov chain, which is the same as sampling them from distribution (10) (J0 =1 w.p. 1). Then,

if γk is replaced by the estimator γk in (9), the standard estimator is transformed



1 N1
γ

 Σn

N
(j) × 1

(i)

^cie = N
1 j=1
N1
Ik
k=0
n	N2
Jk
N2 i=1

=   1	 Σ Σ
  

Σ I(j)J(i).







E {γ	} =	1
,⎨ΣN1 Σ
I(j)J(i),

^cie


N1N2
N,1
j=1 n

k=0 N2
i=1	,

  1	
=
N1N2
n
Σj=1
kΣ=0
pkγk
i=1

=
k=0
To determine the the variance of γ
pkγk = γ.

, let I¯(x) be any possible replication of I¯, what

(x)
(x)
(x)
^cie



V{γ	} = V{E{γ	| I¯(x)}} + E{V{γ	| I¯(x)}} .
cie				
A	B
Terms A and B are analysed separately and after some algebra (see [18]), we obtain:


V{γ

} = A + B =
1 Σn
p γ2 − γ2	+	1
γ − Σ

p γ2

.	(11)

^cie


N1	k k
k=0


N1N2
k k
k=0

Term A is the value of the variance of the Conditional Monte Carlo estimator when the values γ1, γ2, .. ., γn are known exactly (see (6)). Term B is the variance increase due to the fact that the values γ1, γ2, .. ., γn are replaced by estimators.
Multiple Sets of Intermediate States
The key to the application of our Conditional Monte Carlo to Markov chains (as described in Section 2) —call it pure Conditional Monte Carlo— is the knowledge of the probabilities γ1, γ2, .. ., γn. The lack of these values makes it necessary to use estimations instead (as described in Section 3). This technique is the heart of the CMIE method proposed in this article. As shown, the estimators γ1, γ2, .. ., γn can be obtained by standard simulation started every time one of the intermediate states 1, 2, .. ., n is reached. But these values can be estimated more accurately,


































H	I	J
Fig. 2. The case of two sets of Intermediate States, C˜1 and C˜2


applying the same Conditional Monte Carlo method recursively, in the following way.
Suppose that two sets of intermediate states, C1 and C2, are defined, instead of one, as shown Figure 2. Assume that C1 ∩ C2 = ∅ and u, d /∈ C1, C2. Then, once a state i ∈ C1 is reached, N2 replications must be started at state i, and they must be simulated until they either hit a state in C2, go back to u, or get absorbed at d. This can be considered the second recursive level of the simulation. It is intended

to obtain the values γj , γj , .. ., γj
, which indicate the probability that each of

1	2	n1
these N2 replications get absorbed at d. These values are not estimated by means
of standard simulation, they are estimated more accurately by this recursive level of Conditional Monte Carlo simulation that makes use of C2 as the set of intermediate states. It is simple to extend this mechanism to more recursive levels (with more sets of intermediate states).
The variance analysis can be extended to the case of two sets of intermediate

states,
C˜1 and C˜2, in a straightforward manner. The probabilities involved are






V{γ

} = 1
 Σn1

p γj2 − γ2

1 n1
+	p

1/N
 Σn2

γ2 − γj2	+

^cie


N1	l  l
l=0
1	n2


N1	l
l=0
2
k=0
lk k	l




Given this expression, it follows that the variance obtained in a model with two sets of intermediate states, C1 and C2, is lower than or equal to the variance

Comparative Analysis of Variances
The variance of the CMIE estimator for the case of only one set of intermediate
states, V{γcie}, was derived in Section 3. In this section, this variance is compared



As N2
→ ∞, V{γcie
}→ V{γ }. Clearly, if the number of replications used in the

estimation of the probabilities γi, i = 0,..., n, is infinite, the estimators converge
to the corresponding exact values, and the method becomes the pure Conditional Monte Carlo.
At the end of Section 2 it has been shown that V{γ } ≤ V{γ }, meaning that
^c	^s
^c	^cie
V{γcie} ≤ V{γ }, meaning that the proposed estimator is never less accurate than

Standard Monte Carlo estimator.
Σn	2	2
Σn	2

V{γcie} =
k=0	 +
N
	k=0	
N N

1
Σn	2	2
1  2
Σn	2





N1
≤ V{γ }.
N1	N1

The inequality holds, no matter the values of N1 and N2. This means that the pro- posed estimator, γcie, is never less accurate than Standard Monte Carlo estimator,

^c	^cie
V{γ }, which means that CMIE is always more accurate than crude or Standard Monte Carlo, but never as accurate as pure Conditional Monte Carlo, in which the exact values γi, i = 0,..., n, are used.
Intermediate States Analysis
The variance reduction capacity of CMIE depends on the choice of the set of inter- mediate states. In this section two properties of the sets of intermediate states are considered. Their proofs can be seen in [18]. The first one states that after adding a new state to an existing set, the variance of the estimator never increases and, therefore, a variance reduction may be expected. The second one says that if we compare the variance reduction obtained by two disjoint sets, the highest variance reduction comes from the cut that is somehow “closer” to the initial state, u.
These two properties are consistent because, if the addition of one state to an existing set of intermediate states yields a variance that is lower than, or at worst equal to, the variance before the addition, the set that yields the least variance is





Fig. 3. Continuous time Markov chain used in the experimental variance analysis
the one composed of all the states: C = S. But, from the implementation point of view, the set C = S is equivalent to the set formed by the adjacent states to u, because, if C = S, for any replication started at the initial state u, the only reachable states are the adjacent ones.
In the case of two or more sets of intermediate states, the choice of the second, and the consecutive ones, must be somehow similar to the choice of the first one with respect to the initial state u. Whenever possible, the second set (between the existing one and state d) must be formed by the adjacent states to the existing set. However, this is not straightforward and must be analyzed for every particular model.
These two properties are now tested on a continuous time Markov chain proposed and used by Juneja and Shahabuddin in [11] and shown in Figure 3. The system has 2 components of class A and 4 components of class B. The components can only be operational or failed. The state is the pair (NA,NB), where Ni indicates the number of failed components in class i. Failure rates of classes A and B are, respectively, ϵ/2 and ϵ. The system fails if all components of all classes fail. Group repair (all failed components of a class are repaired simultaneously) begins if two components of the same class fail. Group repair rates for both classes are equal to 1. There is one repair-person in the system, and class A gets preemptive priority over class B.
Tables 1, 2 and 3 show the results obtained by CMIE simulations over this model. The sets C1, C2 and C3 are all cuts between u and d, and they are referred to, making use of the numbers placed above each state in Figure 3. Table 3 shows the ratio V{γs}/V{γcie} when CMIE and standard simulation run the same execution time.
The CMIE method was programmed in the C language, using the gcc compiler. The estimator γcie and an unbiased estimator of its variance were calculated as follows:

1
γ^cie = N

N1
γ(j)	and
j=1


V^{γ^cie} =
1	1

	
N1 − 1	N1
N1
⎝j=1

γ(j)
2⎞⎠ − γ^2
⎞⎠ .	(12)

The results in Table 1 show that the cut that attains the lowest variance is the

one formed by the adjacent states to u. The variances of the estimators whose associated cut is close to state u are also low and quite similar. But, when the cuts are “far” from state u, the variance greatly increases. In these experiments the number of replications started at state u was 10,000 and the number of replications launched from the intermediate states was also 10,000.
The experiment whose results are in Table 2 show that as the number of cuts increases, the variance of the estimator γcie decreases. In these experiments the number of replications started at state u was 10,000 and the number of replications launched from the intermediate states was 100 for all cases.
The experiments in Table 3 are included to briefly show the variance reduction capacity of the CMIE method. The number of replications launched from interme- diate states was 1,000 for all cases; the number of replications starting at state u was adjusted so that the total execution time of each of the four simulations was t = 500 sec. This time was fixed in advance and equal for all methods in order to have a fair comparison of the accuracy that was obtained by the different experiments.
Table 1
Model in Figure 3, є = 0.01



C˜1
γcie

6.18e−06
V^{γ^cie}












Table 2
Model in Figure 3, є = 0.01



C˜1
C˜2
C˜3
γ^cie
V^{γ^cie}



1–5	—	—	4.00e−06	4.00e−12
1–5	2–6–10	—	6.19e−06	6.31e−14
1–5	2–6–10	3–7–11	6.13e−06	2.27e−15

Application to Large Systems
Sometimes the state space S of the Markov chain is extremely large and, therefore, the choice of intermediate states is hard to be done explicitly. The idea of CMIE fits better to these models if it is adapted in the following way. In every replication, the computed values are samples of the probability of interest, γ, conditioned to the

Table 3
Model in Figure 3, є = 0.001



C˜1
C˜2
C˜3
γ^cie
V^{γ^cie}
V^{γ^s}/V^{γ^cie}

4–8–12	—	—	6.53e−09	4.60e−20	87
3–7–11	9–13	—	6.38e−09	7.75e−21	516
3–7–11	4–8–12	9–13	6.44e−09	1.28e−21	3125
values of the random variable XC (or to the intermediate states that are reached). In the end, the values of XC are a function of the path πu followed by the trajectory started at state u. But the value of γ can be conditioned to events related to different functions that take the path πu as their argument. In the following subsections three possible alternatives for these functions are introduced.

Forward Steps
For some systems it is possible to detect whether, at every jump, the replications move towards the final state d, or not. In these cases it is possible to make the recursive calls only after moving D ≥ 1 steps closer to the target state, every time. Proceeding this way, wherever the simulation starts, it must keep moving forwards and backwards until it either comes back to u, or moves D steps forward to d. If u is reached, the replication terminates; if the simulation moves D steps closer to d, a new replication (recursive call) is launched.

Consecutive Failures
In multicomponent systems subject to fails and repairs, every failure produces a forward step, that is a step towards the final state d. For a replication that starts at some state i, there are many ways (paths) to get D steps closer to the target state d. One of them corresponds to the case in which D consecutive failures occur. If the system is composed of more that D components, there will be many different ways in which D consecutive failures may occur, all of them rarer than the case in which the D steps are completed after a zigzag of forward and backward steps. Thus, the indicator random variable I can be conditioned on such a sequence of D consecutive failures.
Measure of Rarity
Let πi,j be a path that starts at state i and ends at state j, without hitting state
u. If this path is composed of the sequence of states i, k, .. ., l, j, the probability that the simulation goes through it, is: P{πi,j} = pik × ... × plj, where pxy is the probability of going from state x to the neighbour state y, no matter if this jump is a fail or a repair.
In order to apply CMIE, the indicator variable I can be conditioned on the event

P{πi,j} ≤ B, where B is a fixed bound. But, as in highly reliable systems most of the probabilities pxy are likely to be low, the values P{πi,j} are likely to be extremely low. Therefore, it may be better to apply logarithm as follows: − log(P{πi,j}) = 
— log(pik) − ... − log(plj) and to condition on the event − log(P{πi,j}) ≥ W (where
W = − log(B)).
Experimental Comparison
The three implementations proposed are now subject to an experimental compari- son. All the values —obtained by using the formulas indicated in (12)— are com- pared to results obtained from published papers.
The model used in the first set of experiments was used by Cancela et. al. in [5]. It is a computer that is composed of a multiprocessor, a dual disk controller, two RAID disk drives, two fans, two power supplies, and one dual interprocessor bus. When a component in a dual fails, the subsystem is reconfigured into a simplex. This tandem computer system requires all subsystems, one fan, and one power supply for it to be operational. The failure rates are 5ϵ, 2ϵ, 4ϵ, 0.1ϵ and 3ϵ for the processors, the disk controller, the disks, the fans, the power supplies and the bus respectively, with ϵ = 10−5 failures/hour. There is only one repairman and the repair rates are 30 repairs/hour for all the components, except for the bus, which has repair rate equal to 15 repairs/hour. In the experiments shown in Table 4, the multiprocessor and the disks have two units each, and only one is needed for the system to be working. FB, SFB and SFBP are all Importance Sampling methods used in [5]. Table 5 shows the results obtained for the same system, but with with a four-unit multiprocessor (only one of the four processors is required to have an operational system), and with each RAID being composed of 5 drives, only 3 of which are required for the system to be operational.
The third system used, also taken from [5], consists of a replicated database in which there are four sites, and each site has a whole copy of the database, on a RAID disk cluster. All clusters are identical, with the same redundancies (7-out-of- 9), and with failure rate (for each disk) equal to ϵ = 10−2. There is one repairman per class, and the repair rate is 1. The system is considered up if there is at least one copy of the database working. Results are shown in Table 6.
Measure of Rarity is efficient only if failure and repair rates are considerably different. When this is not the case, the measure of rarity increases significantly at both, failures and repairs and, as a consequence, an increase of such measure is not an indication that the systems is moving towards the target event. In the case of the replicated database, failure and repair rates are, respectively, 10−2 and 1. Compared to the rates of the other systems analyzed, these rates are considerably close. This is the reason why Measure of Rarity is not computed in Table 6.
In the second set of experiments the models are the ones used by L’Ecuyer et. al. in [17]. In the first case (Example 5 in [17]), the system is composed of two sets of processors with two processors per set, two sets of disk controllers with two controllers per set, and six clusters of disks with four disks per cluster. The failure rates for processors, controllers, and disks are 5 × 10−5, 2 × 10−5 and 2 × 10−5,


Table 4
Tandem computer, 1st version in [5]


γcie

1.33e−06
V^{γ^cie}× t















Table 5
Tandem computer, 2nd version in [5]


γcie

1.24e−07
V^{γ^cie}× t












Table 6
Replicated database in [5]

γcie

8.54e−13


V^{γ^cie}× t

Table 7
Example 5 in [17] (γ = 5.60e−05)


цcie

—
V^{ц^cie}× t
















respectively. The repair rate is 1 for each type of component. In each disk cluster, data is replicated, which means that the failure of a single disk does not provoke a system’s failure. The system is operational if all data is accessible from both processor types, meaning that at least one processor of each type, one controller of each set, and three disks of each cluster are operational. Results are shown in Table 7. BFB, SBLR, ZVA(v0), ZVA(v1), ZVA(v2), and ZVA(v3) are all Importance Sampling methods used in [17].
The last example is the one referred to as Example 6 in [17]. The system is composed of 20 types of components numbered from 0 to 19, with 4 components of each type. All repair rates are assumed to be 1, but component’s failure rates differ: type–i components have failure rate λi = (1 + i/10)ϵ for 0 ≤ 1 ≤ 9 and λi = iϵ2/10 for 10 ≤ 1 ≤ 19, where ϵ = 10−3. The system is failed whenever a total of 7 components are failed. Results are shown in Table 8.
All the CMIE estimations can be considered in the same order of precision and efficiency of the other methods to which the comparisons has been made.



CMIE vs. Splitting
If the sets of intermediate states are cuts (between u and d) in the graph that models the Markov chain, there is a formal equivalence between CMIE and Splitting [7,8,13,14,26].

Table 8
Example 6 in [17]

цcie

3.10e−11


V^{ц^cie}× t














1 N1
ц

 Σn

N
(j) × 1

(i)

^cie = N
1 j=1
N1
Ik
k=1
n	N2
Jk
N2 i=1

=   1	 Σ

Σ Σ I(j)J(i).	(13)


A different analysis on the same model shows that, any path starting at state u has a probability, say P1, to reach —any state of— the set C before coming back to u. In the same way, a path starting from any state in the set C has a probability, say P2, to reach state d before coming back to u. The set C can be seen as a bound or threshold in the paths going from u to d and, therefore, Splitting can be applied in
the estimation of ц. This Splitting estimation takes the form: ц^ = P^1 × P^2, where
P1 and P2 are, respectively, standard estimators of P1 and P2, as in any ordinary
Splitting application. Figure 4 shows part of a set of replications, some of which start at u and goes forward to C, and some others that start at C and goes forward to d. According to this approach, the estimators of P1 and P2 are:




P^ =


N1
I(j)


and
N1

j=1
P =
n
(j)
k
k=1
N2
(i)
k
i=1
,



N1 j=1
k
k=1
2	N1
N2
j=1
n
(j)
k
k=1

and the Splitting estimator is:
ц^spl = P^1 × P^2

=	1	Σ

Σ I(j)

Σ J(i) = ц^

.	(14)


C1
Fig. 4. Some trajectories in a CMIE vs. Splitting comparison

This leads to the conclusion that, if the set C = {1, 2,..., n} is a cut in the graph of the Markov chain, CMIE and Splitting (based on a single level set) produce the same estimation. In other words, Splitting with a single level set C is the particular case of CMIE in which the set C is a cut in the graph of the Markov chain.
In a basic Splitting model there are bounds or thresholds between the initial and the final state, just like the set C in Figure 4. The consecutive probabilities P1, P2,
... , need to be estimated somehow. One of them is the probability of reaching the final state from the threshold that is immediately before. In systems like the ones used in Section 7, there are usually more than one final state scattered through all the Markov chain, some of which may be located between thresholds. This requires a particular effort to design a Splitting function of importance, while the application of CMIE is straightforward.
Another feature that may cause complications in a basic Splitting model is failure propagation. Sometimes a particular failure may cause the simultaneous occurrence of a set of other failures, with a given probability. In a basic Splitting model this translates into crossing more than one threshold simultaneously, what makes necessary to modify the basic approach according to system under analysis. CMIE is not affected by failure propagation.
Conclusions and Open Research Lines
This article proposes a Monte Carlo method, referred to as Conditional Monte Carlo with Intermediate Estimations (CMIE ), designed to reduce the variance of the estimator in a context of large and highly reliable Markovian systems.
CMIE was conceived to estimate the probability of visiting the failure state before coming back to the initial state (accepted as the state in which the system is up). The application of ordinary Conditional Monte Carlo to this type of model requires the knowledge of the exact value of some probabilities in the model. To overcome the fact that this probabilities are unknown, the proposal of CMIE is to estimate them, for which it is necessary to launch the method, recursively, from some selected states called intermediate states.
Splitting can be seen as the particular case of CMIE in which the events are

implicitly defined by means of thresholds (cuts) in the state space of the Markov chain. However, the way in which the target probability is recursively computed in CMIE is simpler than the Splitting algorithm, which needs to determine the prob- abilities of crossing each threshold conditioned to the previous cross, keeping track of the number of times each threshold is crossed. Another advantage of CMIE over Splitting, comes up in systems in which there are more than one target state and/or fault propagation. The presence of more than one target state is a drawback in the determination of thresholds (cuts in the graph). Due to the presence of fault prop- agation, multiple thresholds crosses may occur. A particular effort then is required to adapt Splitting to these particular settings, whereas the CMIE implementations are straightforward and do not differ with respect to ones in which there is only one target state and there is no fault propagation.
CMIE was empirically tested against some other methods taken from the liter- ature. In all cases, the results show that CMIE is in the same range of efficiency as the methods to which it was compared, not only in the variance, but also in the precision gain comparison. Some properties of CMIE were demonstrated and, besides, its variance was given a closed form. CMIE can be easily extended to other types of rare event problems like, for instance, network reliability estimation, either under a static (classic) or a dynamic approach [19].
One possible line of future work is to refine the asymptotic analysis of the be- haviour of CMIE and to see, for instance, how close or how far it is to have Bounded Relative Error and/or Bounded Normal Approximation. Besides, two important is- sues to work on are: methods and criteria for the selection of the intermediate states sets and the trade–off between accuracy and execution time.

References
Asmussen, S. and D. Kortschak, Error rates and improved algorithms for rare event simulation with heavy Weibull tails, Methodology and Computing in Applied Probability (2013), pp. 1–21.
Blanchet, J., J. Li and M. K. Nakayama, A conditional monte carlo method for estimating the failure probability of a distribution network with random demands, in: Proceedings of the Winter Simulation Conference, WSC ’11 (2011), pp. 3837–3848.
URL http://dl.acm.org/citation.cfm?id=2431518.2431974
Botev, Z. I. and D. P. Kroese, Efficient Monte Carlo simulation via the generalized splitting method, Statistics and Computing (2010), pp. 1–16, dOI: 10.1007/s11222-010-9201-4.
URL  http://dx.doi.org/10.1007/s11222-010-9201-4
Botev, Z. I., P. L’Ecuyer and B. Tuffin, Markov chain importance sampling with applications to rare event probability estimation, Statistics and Computing 23 (2013), pp. 271–285.
URL  http://dx.doi.org/10.1007/s11222-011-9308-2
Cancela, H., G. Rubino and B. Tuffin, MTTF estimation using Importance Sampling on Markov models., Monte Carlo Methods and Applications 8 (2002), pp. 321–341.
Chan, J. C. and D. P. Kroese, Rare-event probability estimation with conditional monte carlo, Annals of Operations Research 189 (2011), pp. 43–61.
URL  http://dx.doi.org/10.1007/s10479-009-0539-y
Garvels, M. J. J., “The Splitting Method in Rare Event Simulation,” Ph.D. thesis, Faculty of mathematical Science, University of Twente, The Netherlands (2000).
Glasserman, P., P. Heidelberger, P. Shahabuddin and T. Zajic, Splitting for rare event simulation: Analysis of simple cases, in: Proceedings of the 1996 Winter Simulation Conference (1996), pp. 302– 308.

Glynn, P. W. and D. L. Iglehart, Importance sampling for stochastic simulations, Management Science
35 (1989), pp. 1367–1392.
Goyal, A., P. Shahabuddin, P. Heidelberger, V. F. Nicola and P. W. Glynn, A unified framework for simulating markovian models of highly dependable systems, IEEE Transactions on Computers 41 (1992), pp. 36–51.
Juneja, S. and P. Shahabuddin, Fast Simulation of Markov Chains with Small Transition Probabilities, Management Science 47 (2001), pp. 547–562.
URL http://dx.doi.org/10.1287/mnsc.47.4.547.9827

Kroese, D. P., T. Taimre and Z. I. Botev, “Handbook of Monte Carlo Methods,” Wiley Series in Probability and Statistics, Wiley, 2013.
L’Ecuyer, P., V. Demers and B. Tuffin, Rare events, splitting, and quasi-Monte Carlo, ACM Trans. Model. Comput. Simul. 17 (2007).
URL http://doi.acm.org/10.1145/1225275.1225280

L’Ecuyer, P., F. Le Gland, P. Lezaud and B. Tuffin, Splitting techniques, in: G. Rubino and B. Tuffin, editors, Rare Event Simulation using Monte Carlo Methods, John Wiley & Sons, 2009 pp. 39–61.
L’Ecuyer, P., G. Rubino, S. Saggadi and B. Tuffin, Approximate zero-variance importance sampling for static network reliability estimation, IEEE Transactions on Reliability 8 (2011), pp. 590–604.
L’Ecuyer, P. and B. Tuffin, Effective approximation of zero-variance simulation in a reliability setting, in: Proceedings of the 2007 European Simulation and Modeling Conference, EUROSIS, Ghent, Belgium, 2007, pp. 48–54.
L’Ecuyer, P. and B. Tuffin, Approximating Zero-Variance Importance Sampling in a Reliability Setting, Annals of Operations Research 189 (2011), pp. 277–297.
Murray, L., “New variance reduction methods in Monte Carlo rare event simulation,” Ph.D. thesis, Facultad de Ingenier´ıa, Universidad de la Repu´blica, Montevideo, Uruguay (2014).
Murray, L., H. Cancela and G. Rubino, A Splitting algorithm for network reliability estimation, IIE Transactions 45 (2013), pp. 177–189.
Ridder, A., Importance sampling simulations of Markovian reliability systems using cross-entropy, Annals of Operations Research 134 (2005), pp. 119–136.
Ross, S. M., “Simulation, Fourth Edition,” Academic Press, Inc., Orlando, FL, USA, 2006.
Rubino, G. and B. Tuffin, “Rare Event Simulation Using Monte Carlo Methods,” Wiley, 2009.
Rubinstein, R. Y. and D. P. Kroese, “The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte Carlo Simulation and Machine Learning,” Information Science and Statistics, Springer, 2004.
Vill´en-Altamirano, J., Importance functions for RESTART simulation of highly–dependable systems, Simulation 83 (2007), pp. 821–828.
URL http://portal.acm.org/citation.cfm?id=1363142.1363148

Vill´en-Altamirano, J., RESTART simulation of non–Markov consecutive–k–out–of–n: F repairable systems, Reliability Engineering & System Safety 95 (2010), pp. 247–254.
Vill´en-Altamirano, M. and J. Vill´en-Altamirano, RESTART: A method for accelerating rare events simulations, in: Proceedings of the 13th International Teletraffic Congress (1991), pp. 71–76.
Zuk, J. and L. Rosenberg, Rare-event simulation for radar threshold estimation in heavy-tailed sea clutter, in: IEEE Workshop on Statistical Signal Processing, SSP 2014, Gold Coast, Australia, June 29 - July 2, 2014, 2014, pp. 436–439.
URL http://dx.doi.org/10.1109/SSP.2014.6884669
