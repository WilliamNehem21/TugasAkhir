

Electronic Notes in Theoretical Computer Science 221 (2008) 23–35
www.elsevier.com/locate/entcs

Characterizing Computable Analysis with Differential Equations
Manuel L. Campagnolo 1
D.M./I.S.A., Technical University of Lisbon and SQIG - Instituto de Telecomunicac¸˜oes Lisbon, Portugal
Kerry Ojakian 2
Department of Mathematics
SQIG - Instituto de Telecomunicac¸o˜es and IST, Portugal Lisbon, Portugal

Abstract
The functions of Computable Analysis are defined by enhancing the capacities of normal Turing Machines to deal with real number inputs. We consider characterizations of these functions using function algebras, known as Real Recursive Functions. Bournez and Hainry 2006 [5] used a function algebra to characterize the twice continuously differentiable functions of Computable Analysis, restricted to certain compact domains. In a similar model, Shannon’s General Purpose Analog Computer, Bournez et. al. 2007 [3] also characterize
the functions of Computable Analysis. We combine the results of [5] and Gra¸ca et. al. [13], to show that a different function algebra also yields Computable Analysis. We believe that our function algebra is an improvement due to its simple definition and because the operations in our algebra are less obviously designed to mimic the operations in the usual definition of the recursive functions using the primitive recursion and minimization operators.
Keywords: Computable Analysis, Real Recursive Functions, Differential Equations.


Introduction
Computable Analysis is a well accepted paradigm of computability for real num- bers and real functions. Nonetheless, other approaches to computation over the reals have been proposed. Among these there are models that evolve step by step, like BSS-machines [1] or real random access machines, and continuous-time models like Shannon’s General Purpose Analog Computer (GPAC) [19] [12], continuous neural networks [15] or Moore’s real recursive functions [16] (for an up-to-date review of

1 Email: mlc@math.isa.utl.pt
2 Email: ojakian@math.ist.utl.pt

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.12.004

continuous-time models see [2]). Recently, there has been a quest for character- izations of computability of real functions (in this paper, “computable” with no additional reference will always refer to Computable Analysis) based on alternative models, namely on those with a continuous state space that evolve in continuous time. One motivation for this research has been to understand the computational power of continuous-time systems, and search for an analog of the Turing-Church thesis in the context of real computation.
The first true model of a universal continuous time machine was proposed by Shannon [19]. However, this has been proved to be weaker then Computable Analy- sis since computable functions like Euler’s Γ or Riemann’s ζ are not “GPAC genera- ble” [17] [18]. Note that the computability of all GPAC generable functions follows from the results in [13], and the fact that they correspond precisely to the solutions of polynomial differential equations as shown in [12].
To improve the upper bounds on Shannon’s GPAC a new notion of limit com- putation with polynomial differential equations (called GPAC-computability) was proposed in [11] and explored further in [3]. In that paper, it is shown that the real computable functions are precisely the GPAC-computable functions as long as we restrict ourselves to compact domains. Although GPAC-computability relies on a simple dynamics, the parameters of the differential equations can be arbitrary computable reals and the limit operator applies to functions with an arbitrary slow convergence rate (see [8] for some remarks on that model).
A more general approach to computability over the reals with differential equa- tions was proposed in [16]; it is called Real Recursive Functions. It is a recur- sion theory on the reals and provides algebraic characterizations of classes of real functions. The general theory contains many non-computable functions, result- ing essentially from the inclusion of a powerful zero finding operator over the real line. Restricting the theory, it is possible to define classes that extend to the reals certain classes of discrete functions like the primitive recursive or the elementary computable functions [6] [7].
Instead of asking which functions over the naturals have extensions in the reals, Bournez and Hainry proposed classes of real recursive functions that correspond precisely to classes of real computable functions. They describe analog character- izations for the real elementary computable functions [4] and for the functions of Computable Analysis, all restricted to C2 functions on compact domains [5]. This last characterization contains a set of basic functions and is closed under compo- sition, linear integration, a limit operation and a root finding operation. In this paper we propose a class of real recursive functions which is simpler in some re- spects, avoiding the use of a root-finding operation, which has the disadvantage of directly mimicking the operation of minimization over the naturals. This is ac- complished by strengthening the operation of integration, removing the linearity restriction. A significant point is then to show that this class is not too strong. To accomplish this, we use the results of Gra¸ca et. al. [13] to show that even this general integration preserves computability.

Formulating the Main Result
We now provide the basic definitions and state the result, leaving the proof for the next section. We start by defining a flexible set of computable functions over the reals, R, placing no restriction on the domains of the functions (see [14] or [20] for details on Computable Analysis and Type-2 computability). Unless otherwise stated, for us, a function will always have some domain E ⊆ Rk, for some positive integer k, and codomain R.
Definition 2.1 We say a function f is in C(R) iff there is a Type-2 Turing Ma- chine M such that for any x¯ in the domain of f, M calculates f (x¯) on input x¯; for x¯ not in the domain of f, we ask nothing of M.
Though the above definition is convenient, useful theorems will consider restrictions of the domains.
Definition 2.2 Suppose F is a set of functions. By its compact restriction, de- noted [F], we mean the following set of functions: Consider any function f (x1,..., xk) in F and any product of intervals ([a1, b1] × ... × [ak, bk]), where ai, bi are rationals. If the domain of f includes this product, then the restriction of f to this product is in [F].
We will basically consider the set [C(R)] ∩ C2, where for an integer k ≥ 1, we let Ck refer to the set of (possibly partial) functions which are k times continuously differentiable with respect to any variable on their domain of definition.
We now turn our attention to function algebras. We use the term operation to refer to a function whose inputs and outputs are real functions.
Definition 2.3 Suppose B is a set of functions (called basic functions), and O is a set of operations. Then FA[B; O] is called a function algebra, and it denotes the smallest set of functions containing B and closed under the operations in O. For ease of readability, we often list the elements of B or O simply as a list separated by commas.
Some of the most important operations will be defined using differential equa- tions. In general an initial value problem (IVP) is given by a system of equations like the following:

h1(x¯, 0) = f1(x¯)
.
hk(x¯, 0) = fk(x¯)

 ∂ h1(x¯, t) = g1(x¯, t, h1,..., hk)
.
 ∂ hk(x¯, t) = gk(x¯, t, h1,..., hk).

The functions fi are used to give initial conditions depending on the parameters x¯ and the functions gi are used to describe the differential equations. More succinctly, the above system of equations will be written as:

h¯(x¯, 0) = f¯(x¯)
 ∂ h¯(x¯, t) = g¯(x¯, t, h¯),
with the bars on top of the functions indicating a vector of functions. Given func- tions f¯, g¯, by a solution to the corresponding IVP, we essentially mean the usual notion. However, since f¯ and g¯ may be partial, we further clarify what we intend. We mean to take functions h¯ such that for every x¯, either h¯(x¯, t) is not defined for
any t, or is defined for t on some open interval containing 0; in this latter case, we
take the interval to be a maximal such interval.
We can now define the operations we will be using (note that in the operations there is an implicit choice of which arguments of the function we choose to use; any choice is allowed):
The operation ODE takes as input some functions f¯ and g¯ of appropriate
arities, sets up the corresponding IVP discussed above to obtain a solution h¯ (as discussed above), and returns h1, the first function in the list h¯.
LI is the same as ODE, with the restriction that the g¯ are linear in the h¯.
Let comp be the operation which takes two functions and returns the functional composition; a composition of partial functions is defined on the maximal well- defined domain.
Let Inverse be the operation which takes a function f (t, x¯) such that for any x¯, f (t, x¯) is a bijection in t, such that  ∂ f (t, x¯) > 0 for all t. The operator Inverse then returns Inverse(f,t), the inverse f−1 in t, i.e. f (f−1(t, x¯), x¯)= t = f−1(f (t, x¯), x¯).
The operation UMU takes a function f (t, x¯) such that:
For any x¯, f (t, x¯) is increasing (not necessarily strictly) in t, and
For any x¯, there is a unique t such that f (t, x¯) = 0 (and at that t,  ∂ f > 0), then returns the function UMU(f, t) = the unique t such that f (t, x¯)= 0.
Besides constant functions like “0” we will also use the following basic functions:
For a positive integer k, let θ (x) = ⎧⎨ 0,  x < 0; , a Ck−1 version of the dis-
xk, x ≥ 0.
continuous function which indicates whether a number is to the left or right of zero.
Let P be the set of projection functions (e.g. P(2,1)(x, y) = x, P(3,2)(x, y, z) = y, etc.).
We now define all the function algebras we will be using.
Definition 2.4 (Function Algebras)

Let BH(∞) be FA[0, 1, −1, θk, P; comp, LI, UMU]
Let BH(c) be the functions of BH(∞) that can be deﬁned using c or less applications
k	k
of the operation UMU.
Let L(∞) be FA[0, 1, −1, θk, P; comp, LI, Inverse]
Let L(c) be the functions of L(∞) that can be deﬁned using c or less applications
k	k
of the operation Inverse.
Let Gk be FA[0, 1, −1, θk, P; comp, ODE]
To make the connection to Computable Analysis, we consider a limit operation.
Definition 2.5
Let LIM∗ be the operation which takes a function f (t, x¯) and returns F (x¯) =
limt→∞f (t, x¯) if the limit exists, |F (x¯) − f (t, x¯)|≤ 1/t, for t ≥ 1, and F is C2.
Given a class of functions F, we let F(LIM∗) denote the closure of F under the operation LIM∗ (for all the classes considered, it will in fact suffice to apply the operation LIM∗ just once).
Note that we use the expression LIM∗ to distinguish it from the limit LIM that we use in [9], which is the same, except that it has no restriction that the resulting F need be C2.
We now state the main claim, that the compact restriction of our function alge- bra coincides with the compact computable functions which are C2.
Theorem 2.6 For r ≥ 3, C2 ∩ [C(R)]= [Gr(LIM∗)].
The Proof
The following 4 steps prove theorem 2.6, modulo a few lemmas (whose proofs follow).
A series of lemmas (3.2, 3.5, 3.11) will show the following inclusions for any
c ≥ 0 and k ≥ c + 3:
BH(c) ⊆ L(c) ⊆ Gk−c ⊆ C(R).
k	k
Lemma 3.1 will show that for some fixed constant “bh”, and for any k ≥ 3, we have:
C2 ∩ [C(R)] ⊆ [BH(bh)(LIM∗)].
C(R) is closed under LIM∗ (easy to see; discussed in [9] after lemma 13 for the case of elementary computable functions).
Putting together the previous three steps we arrive at the following inclusions (for k ≥ bh + 3) by closing under limits and considering compact restrictions:
C2 ∩ [C(R)] ⊆ [BH(bh)(LIM∗)] ⊆ [L(bh)(LIM∗)] ⊆
k	k
⊆ [Gk−bh(LIM∗)] ⊆ [C(R)(LIM∗)] ∩ C2 ⊆ [C(R)] ∩ C2
Theorem 2.6 follows by choosing k = r + bh, with r ≥ 3 in the preceding

inclusions.
The main result from [5] that we use is the following lemma (with an altered form discussed in [8]); also note that they did the construction for k = 3, but it is similar for larger k, while maintaining the restriction to just C2 functions. Furthermore, we include a fixed constant we call bh, allowed since their construction is uniform.
Lemma 3.1 There is a ﬁxed constant bh, such that for any k ≥ 3, we have
C2 ∩ [C(R)] ⊆ [BH(bh)(LIM∗)].

Note that [5] uses a restricted kind of LI operation, which they use in proving the opposite inclusion to the above lemma; however, we can simply use LI since we will not use their argument to show the opposite inclusion.
The rest of the paper will prove the three inclusions of step i above, in order.
The following lemma proves the first inclusion of step i.
Lemma 3.2 For k ≥ 2 and c ≥ 0, BH(c) ⊆ L(c).
k	k
Proof. We just need to show that any application of UMU in BH(∞) can be simu- lated by a single application of Inverse in L(∞). Suppose f (t, x¯) satisfies the condi- tions for the application of UMU. This means for any x¯, f is non-decreasing in t and
has a unique zero. Let F (t, x¯)= θk(f )et − θk(−f )e−t, and we can check that F has the same zero as f for any x¯, and also has the property that its derivative is positive at all the t except possibly at its zero, where it may be 0. Now consider the function G(t, x¯)= f (t, x¯)+ F (t, x¯). It has the same zero as f and has a positive derivative in t, everywhere (using the fact that to apply UMU to f one of the conditions was that its derivative in t be positive at the zero of t). Now to get the root given by UMU(f, t), we let H be Inverse(G, t) and return H(0, x¯).	 
Now we work towards the second inclusion of step i. Note that when working in Gk, the ODE operation can solve differential equations with the initial condition not just for t = 0, but for t equal to a constant definable in Gk. The following often used lemma follows by an inductive proof on Gk, using the key fact that from y' = f (y, t), if f is Ck, then so is the solution y; also recall that the basic function θk is Ck−1.
Lemma 3.3 Any function in Gk is Ck−1 on its domain.
We often want at least k ≥ 3, since the guarantee of C2 functions allows us to switch the order of partial derivatives.
Lemma 3.4 For k ≥ 1, if f (t, x¯) ∈ Gk and f satisﬁes the conditions:
 ∂ f > 0
 ∂ f ∈ Gk
then Inverse(f, t) ∈ Gk.

Proof. First, we notice that for any function f (t, x¯) ∈ Gk we have following prop- erty: For any x¯ such that f (t, x¯) is deﬁned for some t, there are some constants (independent of t, but not x¯) α, β ∈ Gk such that f (α, x¯) is deﬁned and f (α, x¯)= β. The property is easily verified by structural induction on the description of f in Gk. In addition, Gk contains the function h(z)= 1/z, since it can be defined by the ODE h(1) = 1 and h' = −h2.
Now we prove the lemma by applying the Inverse Function Theorem to f (t, x¯), where x¯ is fixed. By hypothesis we know  ∂ f ∈ Gk and  ∂ f /= 0; additionally we
∂t	∂t
know that Gk contains h and is closed under composition, and thus we can define the differential equation
∂ f−1(t, x¯)=	1
∂t	 ∂ f (f−1(t, x¯), x¯)

in Gk, with initial condition is given by f−1(β, x¯)= α. Therefore, f−1(f (t, x¯), x¯)= 
t = f (f−1(t, x¯), x¯), i.e. f−1 = Inverse(f, t).	 

The next lemma shows that c applications of the inverse operation are captured by Gk−c, the second inclusion in step i of the above proof outline.

Lemma 3.5 For c ≥ 0 and k ≥ c + 3, we have L(c) ⊆ Gk−c.

Proof. We will prove a stronger result:
We show that if g ∈ L(c), then g ∈ Gk−c and g' ∈ Gk−c−1, where g' denotes any
partial derivative of g.
We first notice that and Gk ⊆ Gk−1 since θk(t) = t θk−1(t). The proof will be done by induction first in c and then in the description of the function. We skip the base case (c = 0) since it can easily be proved using the techniques below and we proceed to prove the induction step. Let’s then suppose that if f ∈ L(c−1), then f ∈ Gk−c+1 and f' ∈ Gk−c, and consider some function g ∈ L(c). We now proceed
by structural induction on L(c):
g is a basic function of L(c) and therefore is in Gk−c. All basic functions have derivatives in Gk−c ⊆ Gk−c−1 except for θk whose derivative is k θk−1 and belongs to Gk−c−1.
For composition, let g = u ◦ v, where u, v ∈ Gk−c. Therefore g ∈ Gk−c and since (u ◦ v)' = (u' ◦ v) · v', all the derivatives of g are in Gk−c−1 by inductive hypothesis.
Suppose that g(t, x¯) is defined by linear differentiation with respect to t, i.e.


g(0, x¯)= u(x¯),
∂ g(t, x¯)= v(t, x¯) g(t, x¯)
∂t

for some u, v ∈ Gk−c. Then g ∈ Gk−c. We consider the two types of derivatives:  ∂ g

and  ∂  g. It is clear that  ∂ g ∈ Gk−c ⊆ Gk−c−1. For  ∂g
we calculate:

∂xi
∂t

∂g (0, x¯)= ∂u (x¯) and	∂

∂g = ∂
∂xi

∂g = ∂v g + v ∂g

∂xi
∂xi
∂t ∂xi
∂xi ∂t
∂xi
∂xi

since the derivation variables t and xi can be switched (justified because g ∈ Gk−c
and k ≥ c + 3, so we apply lemma 3.3 to conclude that g is C2). By hypothesis,

 ∂u ,  ∂v
∈ Gk−c−1 and v ∈ Gk−c−1 and thus we obtain a differential equation that

∂xi ∂xi

defines  ∂g
i
in Gk−c−1.

Suppose that g = Inverse(f, t), where f ∈ L(c−1); note that to apply Inverse,
we are assuming f is a bijection in t, such that ∂f > 0. By inductive hypothesis
f ∈ Gk−c+1 ⊆ Gk−c and f' ∈ Gk−c. By Lemma 3.4 we conclude that g ∈ Gk−c also.
Let’s look at the two types of derivatives of g: ∂g and  ∂g . By the Inverse Function

Theorem,

∂ g(t, x¯)= 
∂t
∂t
1
 ∂ f (g(t, x¯), x¯)
∂xi

.

This shows that ∂g is in Gk−c ⊆ Gk−c−1. To prove the claim for  ∂g we derivate ∂g
∂t	∂xi	∂t
with respect to xi and switch the order of the derivatives. This leads to a linear

differential equation in  ∂g
i
which involves f , g and ∂f
in Gk−c ⊆ Gk−c−1 and the

second derivatives ∂2f
∂2f
∂t∂xi
. Using similar techniques, it can be shown that

the second derivatives of f are in Gk−c−1. As a result,  ∂g
is also in Gk−c−1.	 

We will now need to introduce some theory from [13] in order to prove the last inclusion in step i (recall that it was Gk ⊆ C(R)).
Definition 3.6 A subset E ⊆ Rk is r.e.-open if it is open and there are computable
sequences of rationals {an} and {rn} such that E =  ∞  B(an, rn), where B(a, r)
is the open ball in Rk with center a and radius r.
In general, an r.e.-open subset can be exhibited in many ways. At a certain point we will want to make sure that the r.e.-open set is exhibited in the following “robust” manner.
Definition 3.7
A rational ball is an open ball with a rational number radius, which has a vector of rationals as its center.
An r.e.-open set is given robustly if for any ball B in its representation, and any rational ball A ⊆ B, A is in the representation.
Given an r.e. representation of a set, we can devise another r.e. representation which is robust by enumerating the original representation, while regularly pausing in order to put in appropriate rational sub-balls of the balls already enumerated. Since we are dealing with computable rationals which we know exactly, this new enumeration is r.e.
Lemma 3.8 If a set is r.e.-open, then this can be exhibited robustly.

The next lemma will be used in the proof of the inductive step of lemma 3.11.
Lemma 3.9 Given two computable functions with r.e.-open domains, their compo- sition is computable with an r.e.-open domain.
Proof. It is well known that the composition of real computable functions is real computable. We consider the r.e.-open condition. Suppose f and g are computable unary functions, both with r.e.-open domains, J and G, respectively (restricting to unary functions simplifies our discussion to the consideration of intervals rather than balls). By lemma 3.8 we assume that G is exhibited robustly; additionally, we can
assume that if the interval G is in G, then the closure G¯ is in the domain of g. Now
to give an r.e. listing of the domain of f◦g we proceed as follows. We simultaneously list the intervals for J and G. For an open interval G of G, we consider its closure G¯, which is in the domain of g. The minimum, m, and the maximum, M , of g on G¯ are computable. If the interval [m, M ] is contained in one of the intervals in the listing of J, then we include G in our listing of the domain of f ◦g. We continue this process, eventually comparing all intervals. There is the technical issue of how the algorithm decides a containment of the form: [m, M ] ⊆ (a, b). In the situation we are concerned with, the a and b are exact rational numbers, but the m and M are
real numbers that we do not have exactly, but can compute to any desired precision. Since the procedure we are concerned with is a computable enumeration, we can continually revisit an interval [m, M ], so when we say to consider the containment [m, M ] ⊆ (a, b), we mean to revisit the question with increasingly more accurate m and M , till we can decide the containment. As long as m /= a and M /= b we can eventually decide the inclusion, and it will turn out not to matter that we can never decide the other inclusions.
Now we show that the algorithm works correctly. First note that we are not making the domain of f ◦ g too big, since by construction, we only include an interval in our listing when we are sure that f ◦ g is defined for all values in the interval. Second, for any x in the domain of f ◦ g, we can show that our algorithm includes an interval containing x. For x in the domain of f ◦ g, that means x is in the domain of g and g(x) is in the domain of f . Since g(x) is in the domain of f , there is some open interval J = (a, b) of J containing g(x). Let J' = (a', b') be any open interval containing g(x), not necessarily in J, such that a < a' < b' < b. By continuity of g, there is an open interval I containing x and such that g(I) ⊆ J'. Some open interval of G must contain x, and so by the robustness of G, there is a
rational interval R in G such that x ∈ R ⊆ I. At some point the algorithm considers the inclusion g(R¯) ⊆ J . We know g(R¯) ⊆ J¯', thus not only do we have g(R¯) ⊆ J , but we know that the endpoints of the interval J are not the same as those of g(R¯), so we can eventually decide the inclusion, putting R, which contains x, into our listing.	 
The next lemma follows easily from the hard work of [13] (the proof below points to the relevant parts of that paper. Note that in that paper, though not always stated, by a “computable function” they mean to include the restriction to an r.e.- open or “r.e.-closed” domain; we do not concern ourselves with “r.e.- closed”).

Lemma 3.10 If f is a C1 computable function with a computable gradient function, then the solution to the initial value problem y' = f (t, y), y(0) = a (a computable) is computable and the domain of y is r.e.-open.
Proof. Since f and its gradient function are computable, theorem 2.7 of [13] shows that f is “effectively Lipshitz” (in fact theorem 2.7 just refers to derivatives, but, as discussed with D. S. Grac¸a, it should refer to the gradient and then a similar proof works to prove it). We do not concern ourself with the definition of this notion of Lipshitz, but simply note that theorem 3.1 of [13] shows that if f is C1 and effectively Lipshitz, then y is computable on its maximal interval of definition containing the initial condition; furthermore, this interval is r.e.-open.	 
Lemma 3.11 For k ≥ 3, Gk ⊆ C(R).
Proof. We proceed by induction, showing the following holds for any function of
Gk:
 Any function and any partial derivative of it is computable, and furthermore have r.e.-open domains.
Note that in general a computable function need not have a computable derivative, nor is Gk closed under differentiation, so we include the condition on the derivative in . Now we discuss the three cases in the induction.
For basic functions  is true.
For composition, consider functions g(x) and f (u, y) which are computable, with computable partial derivative functions, where all these functions have r.e.- open domains. We first note that the composition h(x, y)= f (g(x), y) is computable with an r.e.-open domain, by lemma 3.9. For the derivatives note that:


∂ h =
∂y
∂ f (g(x), y)
∂y



∂ h =
∂x
∂ f (g(x), y) ∂g
∂u	∂x


Applying lemma 3.9 again, we see that both derivatives are computable with
r.e.-open domains, because we have expressed them as compositions and products of computable functions with r.e.-open domains (by inductive hypothesis).
For the ODE operation consider the initial value problem:
y(0, x)= g(x),	 ∂ y(t, x)= f (t, y, x)
Note that to simplify the discussion we have assumed just a single equation and a single x (relaxing these assumptions we would just use the multi-variate chain rule below and get a larger linear system in the end). Inductively we assume that f and g and their partial derivative functions in t or y are computable with r.e.-open domains (even if not in the class); thus, due to the definition of the gradient in

terms of its partial derivatives, we know the gradient is computable. So we can apply lemma 3.10 to say that y is computable with an r.e.-open domain. Now consider the derivatives of y. The derivative in t is just f , which is computable with an r.e-open domain. For the derivative in x we start with the defining equation for y, and calculate:


y =
∂x∂t
∂ f (t, y, x)
∂x


 ∂  ∂y  = ∂f  ∂y  + ∂f,
where the change in the order of differentiation from ∂x∂t to ∂t∂x is justified because
y is C2. The last differential equation defines ∂y as a linear differential equation
using the functions ∂f and ∂f . Note that ∂f =  ∂  f (t, y) should be understood as:
∂y	∂x	∂y	∂y
Differentiate f with respect to its second argument and then compose the function y
at this place. By inductive hypothesis, the derivatives  ∂  f and  ∂  f are computable.
∂y	∂x
We already know that y is computable, so ∂f composed with y is computable.
Since ∂y is defined by a linear differential equation using computable functions, it is computable. Furthermore, the domain of the solution	y is the same as	f ,
∂x	∂y
since the differential equation is linear. Since the domain of  ∂  f is r.e.-open, so is
the domain of  ∂  y.	 

Future Work, Questions, and Conjectures
We now discuss possible improvements of our results. Three specific ways to improve the results are as follows: Removing the restriction to C2 functions, allowing more flexible domains (perhaps non-compact), and simplifying the function algebras. On the first point, as in our paper [9] which removed this restriction, we believe this is just an artifact of the current proof, and expect that is restriction can be removed. Concerning the second point, the use of compact domains is used in a significant way in the proof of [5] (i.e. they use this constraint to show a computable C2 function has a computable derivative; this is in general false for unbounded domains). We do not conjecture anything here, but will ask a question along these lines. On the third point, in line with our paper [8], we believe that we can dispense with the function θk, in essence showing that it suffices to work with an approximation of it that can be built with other functions in the respective function algebras. We are thus lead to the following conjecture and question.
Conjecture S upposing Jk is one of the function algebras we have considered in this paper, let aJ be the same function algebra, but without the basic function θk (in such a case we also leave off the now useless subscript k). Then
[C(R)] = [aBH(∞)(LIM)]= [aL(∞)(LIM)] = [aG(LIM)].

Question C an the restriction to compact domains be removed, or can such a restriction be proved necessary in our context?
In our paper [8], we show how to use our “method of approximation” to remove the function θk for a different function algebra. While it seems that the same approach might work here, there are some challenges. As a final note, we point to a new paper [10] with the claim that theorem 3.1. of [13] can be improved by replacing the assumption that f be effective Lipschitz by the assumption that f be continuous and the solution to the differential equation is unique. This could mean an improvement of our result along with a simpler proof.
Acknowledgement
This work was partially supported by Funda¸c˜ao para a Ciˆenciaea Tecnologia and EU FEDER POCTI/POCI, and grant SFRH / BPD / 16936 / 2004.

References
Blum, L., M. Shub and S. Smale, On a theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines, Bulletin of the American Mathematical Society 21 (1989), pp. 1–46.
Bournez, O. and M. L. Campagnolo, A survey on continuous time computations, in: S. Cooper, B. L¨owe and A. Sorbi, editors, New Computational Paradigms. Changing Conceptions of What is Computable, Springer-Verlag, New York, 2008 pp. 383–423.
Bournez, O., M. L. Campagnolo, D. S. Gra¸ca and E. Hainry, Polynomial differential equations compute all real computable functions on computable compact intervals, Journal of Complexity 23 (2007),
pp. 317–335.
Bournez, O. and E. Hainry, Elementarily computable functions over the real numbers and R-sub- recursive functions, Theoretical Computer Science 348 (2005), pp. 130–147.
Bournez, O. and E. Hainry, Recursive analysis characterized as a class of real recursive functions, Fundamenta Informaticae 74 (2006), pp. 409–433.
Campagnolo, M. L., C. Moore and J. F. Costa, Iteration, inequalities, and differentiability in analog computers, Journal of Complexity 16 (2000), pp. 642–660.
Campagnolo, M. L., C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002), pp. 977–1000.
Campagnolo, M. L. and K. Ojakian, Using approximation to relate computational classes over the reals, in: J. Durand-Lose and M. Margenstern, editors, MCU 2007, Lecture Notes in Computer Science 4664 (2007), pp. 39–61.
Campagnolo, M. L. and K. Ojakian, The elementary computable functions over the real numbers: applying two new techniques, Archives for Mathematical Logic 46 (2008), pp. 593–627.
Collins, P. and D. S. Gra¸ca, Effective computability of solutions of ordinary differential equations - the thousand monkeys approach, in: Proceedings of the Fifth International Conference on Computability and Complexity in Analysis (CCA 2008), 2008, in print.
Gra¸ca, D. S., Some recent developments on Shannon’s general purpose analog computer, Mathematical Logic Quarterly 50 (2004), pp. 473–485.
Gra¸ca, D. S. and J. F. Costa, Analog computers and recursive functions over the reals, Journal of Complexity 19 (2003), pp. 644–664.
Gra¸ca, D. S., N. Zhong and J. Buescu, Computability, noncomputability and undecidability of maximal intervals of IVPs, Transactions of the American Mathematical Society (2007), to appear.

Ko, K.-I., “Complexity Theory of Real Functions,” Birkhau¨ser, 1991.
Maass, W., Networks of spiking neurons: the third generation of neural network models, Neural Networks 10 (1997), pp. 1659–1671.
Moore, C., Recursion theory on the reals and continuous-time computation, Theoretical Computer Science 162 (1996), pp. 23–44.
Pour-El, M. B., Abstract computability and its relation to the general purpose analog computer (some connections between logic, differential equations and analog computers), Transactions of the American Mathematical Society 199 (1974), pp. 1–28.
Rubel, L. A., A survey of transcendentally transcendental functions, American Mathematical Monthly
96 (1989), pp. 777–788.
Shannon, C., Mathematical theory of the differential analyzer, Journal of Mathematical Physics 20
(1941), pp. 337–354.
Weihrauch, K., “Computable Analysis: An Introduction,” Springer-Verlag, 2000.
