Electronic Notes in Theoretical Computer Science 85 No. 2 (2004)
URL: http://www.elsevier.nl/locate/entcs/volume85.html 18 pages


Dialogue Games for Inconsistent and Biased Information

Henk-Jan Lebbink 1,a,b,c, Cilia L.M. Witteman b and John-Jules Ch. Meyer a
a Institute of information and computing sciences, Utrecht university
P.O. Box 80.089, 3508 TB Utrecht, The Netherlands
b Diagnostic decision making, faculty of social sciences, Radboud university,
Nijmegen, The Netherlands
c Emotional Brain research and consultancy, Almere, The Netherlands


Abstract
In this article, a dialogue game is presented in which coherent conversational se- quences with inconsistent and biased information are described at the speech act level. Inconsistent and biased information is represented with bilattice structures, and based on these bilattice structures, a multi-valued logic is defined that makes it possible to describe a dialogue game in which agents can communicate about their cognitive states with inconsistent and biased information. A dialogue game is formalized by, first, defining the agent’s cognitive state as a set of multi-valued theories, second, by defining the dialogue rules that prescribe permissible commu- nicative acts based on the agent’s cognitive state, and last, by defining update rules that change the agent’s cognitive state as a result of communicative acts. We show that an example dialogue with inconsistent and biased information can be derived from our dialogue game.


Introduction
Even in simple conversation, humans often ask for and offer information with different modalities, such as alethic modalities of necessarily and contingently true and false, or probabilistic modalities of certainly and possibly true and false. A different type of modality is epistemic inconsistency and biased state- ments. We would not be surprised to hear somebody say things like “If I had to judge, based only on the weather forecast, whether it is going to rain, I would have said no, but those clouds are just more convincing to say yes.”

1 Email: henkjan@cs.uu.nl
◯c 2004 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.


In this situation, there seems to be convincing evidence to believe that it is going to rain, but this does not dispel the evidence that it is not going to rain, that is, the agent is not complete certain, but biased to believe the former. A belief state is called biased when more evidence exists to believe than to disbelieve something or vice versa. A special case of a biased belief state is when an agent is not convinced either to believe a statement or to disbelieve it, that is, an agent may have evidence to believe something but it also has an equal amount of evidence to believe the contrary. In such a situation, an agent’s belief state is considered inconsistent from an epistemic perspective. Howto cope with these inconsistent and biased belief states in dialogues?
G¨ardenfors [7] gives an epistemic description of a belief state with epistemic commitment functions in which, given an epistemic input, changes of belief states are prescribed. In the context of computational entities called agents [19], Beun [3] proposes a similar approach by identifying three structures that form a dialogue game which enable agents to communicate in a sensible way. That is, agents need to have a cognitive state to represent their world of interest, for example, the information that they believe to be the case. In addition, agents need to have dialogue rules that define in which situations they are allowed to communicate information, and update rules that define howto process incoming information. In Labrou [11], a modal logic is used to describe the semantics of agent interactions, and McBurney and Parsons [14] use geometric semantics to compare dialogue game protocols. In the same vein of the FIPA work on agent communication languages (ACLs), we are trying to work out a semantics of speech acts that can cope with different epistemic modalities.
What is still lacking in all these approaches is an analysis of dialogue games in multi-agent systems that can cope with epistemic modalities. We consider locutions to be inconsistent when the information content is inconsistent or when the information content is contradictory with the content of other lo- cutions. Information is biased when an agent has more evidence to believe a statement than to disbelieve it, or vice versa. Such an information state may be inconsistent but biased in the true or false directions. Agents performing belief revision [7] prevent their belief state from becoming inconsistent; the agents we are to present have by definition no aversion to inconsistencies. To make this possible, we introduce a multi-valued logic in which we can focus on the semantics of inconsistent and biased information exchange in conversation. Our objective is to show that in dialogue games inconsistent and biased information can be dealt with consistently. To do this, we introduce truth- values from bilattice structures [8,6] in all aspects of the dialogue game. As a result, the cognitive state of the agent can be formulated as a set of multi- valued (logical) theories based on the bilattice structure. The update and dialogue rules are defined to cope with biased and inconsistent information. In this approach, agents can deal with monotonic information additions leading to possibly inconsistent information states without being forced into belief

revision. An example dialogue is proven to be a valid sequence of speech act utterances in our dialogue game.
A fictitious dialogue from Sesame Street serves as a running example. In this example Elmo wants to know (which is not shown in the dialogue) whether he can go play outside. He states this in a question 2 directed to Oscar (line 1); the answer is distributed between Tv, Grover and Oscar, and, after a number of other questions and answers, Oscar gives two answers (line 5 and 7). Note that the uttered information becomes inconsistent, but, as will become clear, no belief revision occurs.
Dialogue 1 (Sesame Street)
Elmo to Oscar  ‘Hi Oscar, do you know whether I can go play
outside?’
Oscar to Tv	‘Hey Tv, is it dry outside?’
Oscar to Grover	‘Grover, are you wearing a raincoat?’
Tv to Oscar	‘I think it is dry outside, but I’m not sure.’
Oscar to Elmo	‘Hey Elmo, maybe you can go play outside.’
Grover to Oscar	‘Yes Oscar, I’m wearing a raincoat.’
Oscar to Elmo	‘Elmo! You cannot play outside!
But on the other hand, maybe you can.’
The remainder of this article is structured as follows. Inconsistent and biased information is formalised using bilattice structures and a multi-valued logic based on these structures is described in section 2. In section 3 our dialogue game based on a multi-valued logic is presented. In addition, the example dialogue is translated into a formal one. A proof of the validity of the example dialogue is given in section 4; we conclude with section 5.
Representing Information with Bilattice Structures
We consider stating inconsistent information to be an option when agents cannot make up their mind or when they are not in a position to resolve the inconsistency. A theoretically sound way is needed if agents are to represent and reason with inconsistent and biased information.
Whereas in classical logic terms are assigned a truth-value true or false, in multi-valued logics (MVL) newtruth-values are introduced to represent epis- temic attitudes. In this section, truth-values representing unknown states, inconsistent information states and even a continuum of truth-values to rep- resent biased information states are described. A biased information state is epistemically between true and false. For example, the truth-value ‘biased true’ is closer to true than to false, and, in addition, a biased information

2 Please notice that questions in the example dialogue are confined to simple ones that can be answered with a yes or no. Questions with answers from an enumerable domain, for example ‘what type of weather is it outside?’ could be addressed by splitting up the question in a finite number of questions that can be answered by a yes or no.

state has a level of information between unknown and inconsistent.
In the Sesame Street example dialogue, a classical truth-value is used when Oscar asks Tv whether it is true that it rains outside (line 2). This informa- tion state is represented with truth-value true. The answer conveys a biased information state, because, when Tv says that it rains outside and also that he is not sure (line 4), Tv states that his epistemic state is biased to true that it rains outside. An inconsistent information state occurs (in line 7) when Oscar says that Elmo cannot play outside and also that maybe he can. The information content of this utterance is biased to false, because Oscar says that it is false and partially true that Elmo can go play outside. Obviously, this information is not very helpful to Elmo. Nevertheless, Elmo needs to have a method to handle incoming inconsistent information, if he is to decide later what to do with it.

Bilattice Structure
Before the bilattice structure is described, other general notions are presented. Given the set P ordered according to ≤ and S ⊆ P , an element x ∈ P is an upper bound of S if s ≤ x for all s ∈ S. A lower bound is defined dually 3 . The set of all upper bounds of S is denoted by up(S) and the set of all lower bounds by lo(S). If up(S) has a least element x, then x is called the least upper bound of S. Analogously, if lo(S) has a greatest element x, then x is called the greatest lower bound of S.
Definition 2.1 (lattice) A complete lattice is a structure ⟨B, ≤⟩ such that B is a non-empty set ordered according to ≤ and for all S ⊆ B, there is a greatest lower bound H and a least upper bound H of S, cf. [4].
The notion of a bilattice was first introduced by Ginsberg [8] as a general framework for many AI applications, such as truth maintenance systems [5] and default inferences [15]. A bilattice is in essence an algebraic structure that formalizes an intuitive space of generalized truth-values with two lattice order- ings. Bilattices are used for reasoning about the semantics of logic programs [6], for pragmatics in linguistics [17], or for semantics of logical systems [1]. In section 3, bilattices are used to represent inconsistent and biased information that is part of the agent’s cognitive state.
Definition 2.2 (bilattice) Given two complete lattices ⟨B, ≤1⟩ and ⟨D, ≤2⟩, the structure B(B, D)= ⟨B × D, ≤k, ≤t⟩ is a complete bilattice if the partial orders are defined as follows (given ∼ ⊆ B × D): b1∼d1 ≤k b2∼d2 if b1 ≤1 b2 and d1 ≤2 d2, and b1∼d1 ≤t b2∼d2 if b1 ≤1 b2 and d2 ≤2 d1. cf. [6]. To each ordering are associated join (⊕) and meet (⊗) operations according to the following equations:

3 which means that it is defined like the upper bound with reversed order.



Knowledge order (≤k) b1∼d1 ⊗k b2∼d2 = (b1 H1 b2)∼(d1 H2 d2) b1∼d1 ⊕k b2∼d2 = (b1 H1 b2)∼(d1 H2 d2)
Truth order (≤t)
b1∼d1 ⊗t b2∼d2 = (b1 H1 b2)∼(d1 H2 d2) b1∼d1 ⊕t b2∼d2 = (b1 H1 b2)∼(d1 H2 d2)

The intuition is that B provides evidence for believing a statement and D provides evidence for disbelieving a statement. We assume that B and D both have at least two elements: 0 for a lack of evidence for believing or disbeliev- ing, and 1 for maximal evidence for believing or disbelieving. Consequently, a bilattice has at least four truth-values: 0∼0, 1∼0, 0∼1 and 1∼1, abbreviated with u, t, f and i respectively. Truth-value t (1∼0) represents full evidence for believing and no evidence for disbelieving; this is considered the orthodox ‘true’ from classical logic. Opposite to true is truth-value f (0∼1) that rep- resents no evidence for believing but maximal evidence for disbelieving; this is the orthodox ‘false’. In truth-value u (0∼0) neither evidence for believing nor for disbelieving exists, that is, information is lacking completely. In truth- value i both maximal evidence for believing and for disbelieving exists, that is, the information state is inconsistent. Note that these truth-values do not represent fact-related, ontological uncertainty or inconsistency but epistemic attitudes an agent may have towards its world.
The intuitive space of truth-values is ordered according to ≤t that describes the differences in the ‘measure of truth’ of a truth-value. If holds that b1 ∼ d1 ≤t b2∼d2, then there is less or equal evidence to believe in situation 1 than in 2 and there is more or equal evidence to disbelieve in situation 1 than in
2. Formally, b1∼d1 ≤t b2∼d2 ⇔ b1 ≤ b2 ∧ d2 ≤ d1. In the example dialogue, Grover says to Oscar that it is true that he is wearing a raincoat (line 6), a different answer with lower degree of truth is, ‘No, I’m not wearing a raincoat’. However, truth-values can also be ordered according to ≤k that describes the ‘measure of information’ in a statement. If holds that b1∼d1 ≤k b2∼d2, then there is less or equal evidence to believe in situation 1 than in 2 and there is also less or equal evidence to disbelieve situation 1 than in 2. Formally, b1 ∼d1 ≤k b2 ∼d2 ⇔ b1 ≤ b2 ∧ d1 ≤ d2. Stated differently, situation 2 has more information than 1. In the example dialogue, line 5 has less information than line 7; this is discussed in section 2.2 on biased information. The two orderings behave differently under negation, viz.	a statement that is true becomes false after being negated, and a statement in which nothing is known remains unknown after being negated, cf. [8]. We do not use negation and leave it aside. The upper bound of ≤k is denoted upk, the lower bound of ≤k
is denoted lok.
The smallest complete bilattice is presented graphically in figure 1 and is, in fact, a well-known four-valued logic introduced by Belnap [2]. The meet, join and negation operations associated with the ≤t ordering confined to {f, t} are those of classical logic. When u is added to these truth-values, Kleene’s strong three-valued logic [10] is obtained.


/❛❛❅
/❛❛❅
❛❛ / 1∼1 ❅ ❛❛


≤k	f
/ i
/
❛❛/
❅
❅
❅❛❛ t

≤k	/❛❛
/❅
0.5∼1❅
❅/
❅
/1∼0❅.5

❅❛❛

❅	/	0∼❅1
/0.5∼❅0.5
/1∼0

❅	/
❅	/
❅/❛❛



≤t
❅
0∼0.❅5

❅
❅/
0∼0
≤t
❅//
/ 0.5∼0

Figure 1. Smallest bilattice
Figure 2.	Bilattice representing partial, inconsistent and biased information

Informally, the greatest lower bound θ1⊗k θ2 can be thought of as the truth- value representing information which is mutual in θ1 and θ2 or the consensus of θ1 and θ2, for example, f ⊗k t = u. Likewise, the least upper bound θ1 ⊕k θ2 is thought of as the information that results after combining θ1 and θ2, for example, f ⊕k t = i, see figure 3. The greatest lower bound θ1 ⊗t θ2 is the least amount of mutual ‘truth’, for example, f ⊗t t = f; and likewise is the least upper bound θ1 ⊕t θ2 the most amount of mutual ‘truth’, for example, f ⊕t t = t, see figure 4.

Biased Information with Bilattices
When Oscar says to Elmo that he maybe can go play outside (line 5), or, stated differently, Oscar makes a statement in which it is partially true that Elmo can go play outside; we denote this information state with truth-value 0.5∼0. Later, Oscar says to Elmo that he cannot go play outside, but, on the other hand, he maybe can (line 7). Oscar has a statement with partial and inconsistent information in which it is false that Elmo can go play outside and partially true that he can; this information state is denoted with truth-value 0.5∼1.
Some statements are more general than others in the sense that information in the less general statements is subsumed under the information of the more general ones. When Oscar says to Elmo that he maybe can go play outside (line 5, 0.5∼0), this information state is subsumed under the information of the sentence in which Oscar says to Elmo that he cannot play outside and also that he maybe can (line 7, 0.5∼1). Formally, we have 0.5∼0 ≤k 0.5∼1.

Multi-valued Logics
Sentences from a multi-valued logic (MVL) are constructed in a fashion that is considered truth-value bearing, capable of being the object of belief or ignorance. In section 3.2, these sentences represent the epistemic contents of an agent’s attitude and theories of MVL represent mental constructs.
Definition 2.3 (language of MVL) Given complete bilattice B = ⟨B ×

,✏i
❛❛ i

✒/❅✑	/❅

/
,/✏
❅
 ❅,✏
/
,/✏
❅
 ❅,✏

≤k	/❛❛	⊕k	❅❛❛
≤k	/❛❛   	⊕t
❅❛❛

✒❅✑	✒/ ✑
✒f❅✑	✒/ ✑

❅	/
❅	/
❅/❛❛



≤t
❅	/
❅	/
❅/❛❛



≤t

Figure 3.  Join f ⊕k t = i
Figure 4.  Join f ⊕t t = t

D, ≤k, ≤t⟩ and ontology 4 O, the language of MVL LB is the smallest set satisfying:
if ψ ∈O and θ ∈ B × D then ψ:θ ∈ LB,	(atomic sentence)
if ν, µ ∈ LB and θ ∈ B × D then (ν → µ):θ ∈ LB. (conditional sentence)
Atomic sentences consist of a propositional formula taken from an ontology that is assigned a truth-value from a bilattice. Sentence ψ:θ is read as: ‘ψ has at least truth-value θ’ (with respect to ≤k). Non-atomic, conditional sentences resemble the conditionals from classical logic. Remark that the truth-values of antecedents and consequents are embedded in conditionals and that other connectives like the ‘or’ or ‘not’ are not defined. A theory of MVL is defined as a set of sentences from the language of MVL in which the following properties hold.
Definition 2.4 (Theory of MVL) Given a language of MVL LB,a theory of MVL T B ⊆ LB is the smallest set of sentences satisfying:
if ψ:θ ∈ LB then ψ:u ∈T B,	(founded information)
if ψ:θ1 ∈T B and θ2 ≤k θ1 then ψ:θ2 ∈T B,	(subsumed information)
if ψ:θ1, (ψ:θ1 → ϕ:θ2):t ∈T B then ϕ:θ2 ∈T B.	(implied information) A dual theory of MVL T B∂ ⊆ LB is the smallest set of sentences satisfying:
if ψ:θ ∈ LB then ψ:i ∈T B∂,	(dual founded information)
if ψ:θ1 ∈T B∂ and θ1 ≤k θ2 then ψ:θ2 ∈T B∂. (dual subsumed information)
T B is complete if ψ:θ1 ∈T B and ψ:θ2 ∈T B then ψ:θ1 ⊕k θ2 ∈T B.
T B∂ is complete if ψ:θ1 ∈T B∂ and ψ:θ2 ∈T B∂ then ψ:θ1 ⊗k θ2 ∈T B∂ .
Truth-value assignment ψ:θ states that at least (with respect to ≤k) ev- idence θ holds for ψ. Consequently, the total lack of information associated with truth-value u always applies to all sentences from a theory (i). This least possible amount of information provides the lower bound of information a theory encodes. The reading of the truth-value assignment enforces that all truth-values below a truth-value from a sentence already part of a the-

4 For our current purpose, an ontology is nothing more than a set of concepts with unique propositional formulas identifying the concepts.


ory (with respect to ≤k), are also part of the theory (ii). For example, if there exists a lot of evidence to believe a proposition, then there also exists some evidence to believe the proposition; in such a case information is said to be subsumed. Implied information (iii) results from a generalisation of the classical implication: if a conditional has a designated truth-value 5 and the antecedent of the conditional is part of the theory, then so is the consequent (cf. Rescher [16]). Dual founded information and dual subsumed information are dual counterparts of the normal theory, that is, a dual theory has a re- versed ≤k order. Conditionals for a dual theory can be defined in a similar way, but in our use to represent ignorance (section 3.2) there is no need for dual implied information.
The closure cl denotes the set of MVL-propositions that is closed under the theory of MVL. The upper bound of a theory, upk(T B), is defined as the set of sentences from T B with a truth-value that is the upper bound with respect to ≤k for all sentences of the theory. Formally, upk(T B)= {ψ:θ1 ∈ T B | θ1 ∈ upk({θ2 | ψ:θ2 ∈T B})}.
In subsequent sections the notion of a prerequisite set is needed to describe information that is missing from a background theory. If a background theory T1 is extended to include T2 by adding T3 set-theoretically to background theory T1 then T3 is called a prerequisite for extension T2. ET1 (T2) is the set of all prerequisite theories to obtain extension T2 from the background theory T1, or, stated differently, a prerequisite T3 is a set of propositions that is missing from a background theory T1 if the extension T2 is to be part of this background theory. The prerequisite set includes all prerequisite theories T3.
Definition 2.5 (Prerequisite set) Given closed theory of MVL T1 ⊆ LB
and set of sentences T2 ⊆ LB, the prerequisite set ET (T2) is defined as:
ET (T2)= {T3 ⊆ LB | T2 ⊆ cl(T1 ∪ T3)} .

Agent Dialogues
The dialogue game by Beun [3] is refined to make it possible for agents to communicate about inconsistent and biased information.	In this paper, a dialogue is a sequence of communicative acts governed by a set of dialogue rules. A dialogue game defines a set of different dialogues, prescribed by a set of dialogue rules, a set of update rules and the agents’ initial cognitive states. For an agent to engage in conversation, it needs to have the following structures. In section 3.5, dialogue rules are presented that describe applicable communicative acts (section 3.1) given an agent’s cognitive state (section 3.2). Second, in section 3.6, update rules or epistemic commitment functions are presented that prescribe a cognitive state given a communicative act and a

5 Only t is considered designated. It could be argued that other truth-values with partial information can be given a semantics similar to the certainty factors model [18] in which the partiality of the implication rule is propagated to the consequent.

cognitive state.
Communicative Acts
We take communicative acts to be utterances at the speech act level used by agents to manifest (parts of) their cognitive state, cf. Grice [9]. Communica- tive acts to offer information (Lebbink et al. [13]) or to set up obligations, etc. are not discussed. In the following, agents use the language of MVL based on a static ontology in three different communicative acts: questions, statements of belief and statements of ignorance, tagged with the following markers: ?,?+ and ?− respectively.
A question is a request for a belief addition, that is, an agent x asks an agent y whether it may add sentence ψ :θ to its belief state, [x, y, ψ :θ]?. In a statement of belief, an agent x states to agent y that sentence ψ:θ is part of its belief state and that y may add this to its belief state, [x, y, ψ:θ]?+. A belief statement can be an approval of a request for a belief addition. On the other hand, this request can also be denied, which is in effect a statement of ignorance, that is, an agent x states to agent y that it is ignorant about sentence ψ:θ, [x, y, ψ:θ]?−.
Both questions and ignorance statements express the agent’s lack of evi- dence supporting a content sentence ψ:θ. The difference between these two acts is that in the former, the agent (implicitly) requires an answer; in case an agent utters an ignorance statement, it does not. Only additions of con- tent sentences are considered; consequently, communicative acts only result in expansions of the cognitive state. The three communicative acts have their duals for derogations or retractions of content sentences, which would result in contractions of information from the agent’s cognitive state [7], these are analysed in Lebbink et al. [12].
An ontology O with propositional formulas {p, r, c}⊆O is needed to for- malize the Sesame Street example. The formula r denotes the concept ‘it rains outside’, or equally, ‘it is not dry outside’, c denotes ‘Grover is wearing a rain- coat’, and p denotes ‘Elmo can go play outside’. The assignment r:f denotes ‘it is (at least with respect to ≤k) false that it rains outside’, or equivalently, ‘it is (at least) true that it is dry outside’. Question [elmo, oscar, p:t · p:f]? is a shorthand for the sequential uttering of questions [elmo, oscar, p:t]? and [elmo, oscar, p:f]?. The example dialogue can now be represented by the fol- lowing sequence of communicative acts.
Dialogue 2 (Sesame Street)
[elmo, oscar, p:1∼0 · p:0∼1]? ‘Hi Oscar, do you know whether I can go
play outside?’
[oscar, tv, r:0∼1]?	‘Hey Tv, is it dry outside?’
[oscar, grover, c:1∼0]?	‘Grover, are you wearing a raincoat?’ 4a. [tv, oscar, r:0∼0.5]?+	‘I think it is dry outside,
4b.	[tv, oscar, r:0∼1]?−	but I’m not sure (that it is dry outside).’

[oscar, elmo, p:0.5∼0]?+	‘Hey Elmo, maybe you can go play outside.’
[grover, oscar, c:1∼0]?+	‘Yes I’m wearing a raincoat.’
[oscar, elmo, p:0.5∼1]?+	‘Elmo! You cannot play outside!
But on the other hand, maybe you can.’
The Agent’s Cognitive State
In the dialogue game we propose, an agent’s cognitive state consists of a set of mental constructs which are theories of MVL that express the agent’s attitude towards aspects of its world. Three different mental constructs are used, viz. the agent’s beliefs, desires and ignorance. In addition, two types of mental construct are distinguished: private constructs represent the agent’s own attitudes and manifested constructs represent other agents’ attitudes that have been communicated explicitly. For example, formula r represents ‘it rains outside’, but whether Tv believes this to be the case depends on the truth- value assigned to r that is part of Tv’s private mental construct known as its belief. Oscar is a priori not aware that Tv believes this to be the case, therefore this assignment is not part of Oscar’s manifested belief of Tv. Apart from that, Oscar is also a priori not aware that Tv is ignorant about this, and subsequently this assignment is not part of Oscars manifested ignorance of TV.
Private belief Bx ⊆ LB is a complete theory of MVL that represents x’s belief. ψ:θ in Bx states that x believes that formula ψ has at least truth-value θ. By default, an agent does not believe anything, that is, for all ψ:θ in Bx holds that θ equals u.
Private desire to believe DBy ⊆ LB is a theory of MVL that represents x’s desire what y is to believe. ψ:θ in DBy states that x desires sentence ψ:θ to be part of agent y’s private belief. By default, an agent does not desire anything,

that is, for all ψ:θ in DBy
holds that θ equals u.

To motivate questions, as will be described in subsequent sections, only an agents desire in which an agent is to believe something is needed, that is, only

the mental construct DBy
is needed in which x and y coincide. Remark that

a private desire is not a complete theory, and, as a result, the upper bound

upk
(DBy ) can have more than one truth-value for a propositional formula.

This happens when, for example, Elmo likes to know whether he can go play
outside (line 1); he is interested to believe that he can go play outside or that he cannot. Elmo is not interested in the inconsistent state of believing both. We chose to represent this ‘or’ in the desire state with a non-complete theory. Perhaps one would also like to represent this disjunction in the object language by introducing the semantics of the or-operator; this is future research.
An agent’s private ignorance is not represented because private ignorance equals those sentences not part of the agent’s private belief state. Another cognitive state of interest which is not discussed is the private desire to be ignorant which is the dual of the private desire to believe. This cognitive state could motivate questions to retract propositions from the speaker’s belief state.



i❛❛
upk
(DBy )
i❛❛
M I	i❛❛

//❅❅
\◗ /❅
x y ///❅❅

//	❅❅
//	❅❅
\	◗ ❅
/\	❅
//❛❛/❅///
❅❅❅❛❛

/❛❛/	❛❛
❛❛/ \❛❛
◗
❛❛ ❅❛❛
//❛❛
❅ ❛❛
/ ❛❛
❅❅❛❛

f	/	t
f	❛❛ ❅
◗
❛❛ t
f	❅/ /❅	t

❅❅	/
❅ ❅	/ /
❅❅	/	❅/❛❛

❅
❛❛/
❅
//	❛❛
❅ ❛❛	❅

❅❅	//❅
DBy ❅	/
❅❅	//❅

❅❅❛❛// Bx
x	❅❛❛/
❅❅❛❛//MxBy



Private belief Bx.
Private desire DBy
Manifested belief MxBy and ignorance MxIy

Figure 5. Examples of mental states.

Manifested belief MxBy ⊆ LB is a complete theory of MVL that represents the manifested beliefs of y which x is aware of. ψ:θ in MxBy states that x is aware that y has believes sentence ψ:θ. By default, an agent is not aware of beliefs of other agents, that is, for all ψ:θ in MxBy holds θ equals u.
Manifested ignorance MxIy ⊆ LB is a dual theory of MVL that represents the manifested ignorance of y which x is aware of. ψ:θ in MxIy states that x is aware that y is ignorant about sentence ψ:θ. By default, an agent is not aware of the ignorance of other agents, that is, for all ψ:θ in MxIy holds that θ equals i.
Manifested ignorance is not a set theoretic complement of manifested be- lief, because the ignorance of other agents needs to be manifested explicitly and cannot be derived from manifested beliefs. For example, if agent x has stated ψ : θ to an agent y, y may not assume that x is ignorant about the complement of the sentence because x may have stated only parts of its belief. An agent x can never be completely aware what an agent y believes, because yet another agent z may have stated ψ:θ1 to y just after y has stated ψ:θ2 to x (with θ2 ≤k θ1). Notice that sentences may be part of both the manifested ignorance and the manifested belief. This situation is encountered when an agent x first poses a question to y asking for ψ:θ1 and later y states ψ:θ2 to x (with θ1 ≤k θ2), then for y’s cognitive state holds that ψ:θ1 ∈ MyIx and also ψ:θ1 ∈ MyBx; this will be shown in section 3.6 on update rules.
Manifested-manifested ignorance MxMyIx ⊆ LB is a dual theory of MVL that represents the manifested ignorance of x which x is aware of that y is aware of. ψ : θ in MxMyIx states that x is aware that y is aware that x is ignorant about sentence ψ : θ. By default, an agent is not aware about manifested ignorance of other agents, that is, for all ψ:θ in MxMyIx holds that θ equals i.

Manifested desire Mx
By ⊆ LB is a theory of MVL that represents the

manifested desires of y which x is aware of. ψ:θ in Mx
By states that x is

aware that y desires to believe sentence ψ:θ. By default, an agent does not believe anything about the desires of another agent, that is, for all ψ : θ in

By holds that θ equals u.
Manifested-manifested desire MxMyDBx  ⊆ LB is a theory of MVL that
represents the manifested desires of x which x is aware of that y is aware of.

ψ : θ in MxMyDBx
states that x is aware that y is aware that x desires to

believe sentence ψ:θ. By default, an agent does not believe anything about the manifested desires of another agent, that is, for all ψ:θ in MxMyDBx holds
that θ equals u.
Cognitive Processes
In our formalism, agents can perform two cognitive processes, viz. agents can accept to believe stated proposition and agents can deduce consequences of newly accepted beliefs. Other central concepts such as choosing between per- missible communicative acts will not be included in the descriptions presented here.
We assume that agents are very credulous, in other words, agents accept to believe all propositions that are directed to them in a statement of belief. An agent’s reasoning is restricted to its capacity to draw conclusions based on believed implication rules and believed antecedents. If an agent x believes an implication rule (with the designated truth-value t), (ψ:t → φ:f):t ∈ Bx and x accepts to believe the antecedent, ψ:t ∈ Bx, then x also concludes to believe the consequent φ:f ∈ Bx. If x already believed φ:t, then its belief state becomes inconsistent, that is, φ:i ∈ Bx. The closure cl(Bx ∪ {ψ:t}) corresponds with the set of propositions including x’s beliefs plus ψ:t with consequents.
Motivations to Communicate
Agents can have different motivations to communicate. As described by Beun [3], agents have the incentive to reduce an imbalanced desire and belief state when they do not believe a proposition but they do desire to believe the propo- sition. In such case, it is said that the agent is interested to add the proposition to its belief state. This interest motivates to pose questions, however, the same imbalanced belief and desire state gives also rise to the incentive to remove desires from the agent’s cognitive state; the former scenario will be formalised in our dialogue rules and the latter is not addressed.
In Lebbink et al. [13] an analogous situation is described in which agents are motivated to offer information to others. If an agent x desires that another agent y is to believe a proposition and x is not aware that y already believes the proposition, then x is allowed to offer y the proposition.
Yet another motivation to communicate is to prevent belief states from becoming inconsistent. If inconsistencies are allowed as a possible cognitive state, inconsistent belief states may motive to resolve the inconsistencies by convincing oneself or others to be ignorant towards a proposition, that is, to forget beliefs. This could be represented by an agent’s desire that itself or another agent to be ignorant about a set of propositions denoted by DIy ; this

scenario is not addressed.
Restrictions on these motivations are given by the he pragmatics of the Gricean maxims of cooperation [9]. In a nutshell, these maxims state that, (i) agents are neither allowed to utter information that is not part of their belief state, (ii) nor are they allowed to ask for information that they already believe or are aware of that the other is ignorant about, and in addition, (iii) questions always need to be answered. In this sense, applicability of the dialogue rules is a normative relation that all agents adhere to. The following dialogue rules respect these Gricean maxims and balance an agent’s belief and desire state.
Dialogue Rules
Dialogue rules define which communicative acts are applicable in a dialogue game. The applicability relation (denoted ❀) is not an analytical relation that enforces question-answer pairs but a semantic relation between the agent’s cognitive state and a speech act. Note that if different acts are simultaneously applicable, agents need to make a choice.
Questions
Two roles of questions are distinguished. The first role is to reduce the imbal- ance between an agent’s private desire and belief state, that is, the question is about a sentence the agent itself is interested in to believe. The second role is to reduce an imbalanced desire and belief state of another agent, that is, the question is about a sentence another agent is interested in to believe. In this case, the agent that poses the question needs to be aware of another agent’s imbalanced desire and belief state. This role is described in section 3.5.3 on counter-questions. An agent is said to be interested in sentence ψ : θ when ψ:θ ∈ upk(DBx ) and ψ:θ /∈ Bx .
Questions are restricted to be sensible by the Gricean maxims of cooper-
ation, that is, an agent is not allowed to pose questions if it is aware that the addressed agent cannot answer them. A question from agent x to agent y is sensible if the sentence of the question is not part of y’s ignorance as x is aware of, formally, ψ:θ /∈ MxIy . In addition, a question needs to be fresh, that is, x is not allowed to pose a question for the same information more than once because x may assume the other agent y is already aware that x is interested in the sentence, formally, ψ:θ /∈ MxMyDBx . A question from x to y is applicable when x is interested in a sentence and this sentence is sensible and fresh for y.
Definition 3.1 (question) If ψ:θ ∈ upk(DBx ), ψ:θ /∈ Bx, ψ:θ /∈ MxIy and
ψ:θ /∈ MxMyDBx then a question [x, y, ψ:θ]? is applicable, denoted ❀?.
Belief Statements
Three dialogue rules that define applicability of belief statements are identified next. An agent x assumes another agent y to be interested in a sentence ψ:θ


when x is aware that, first, y desires to believe the sentence, and second, that y

is ignorant about this sentence. Formally, ψ:θ ∈ upk
(Mx
DBy ) and ψ:θ /∈ M
By.

All agents are sincere and can, consequently, only state sentences that are part
of their private belief state. Formally, ψ:θ ∈ Bx .
Depending on the cognitive state of the agents, belief statements can have different roles in a dialogue. That is, a belief statement can be a partial, over- informative or minimal answer to a question. A belief statement from agent x to agent y is applicable as a partial answer when x considers y interested in a sentence, but this sentence does not (necessarily) balance the belief and desire state of y.

Definition 3.2 (partial answer) If ψ:θ ∈ Mx
DBy , ψ:θ /∈ M B
and ψ:θ ∈

Bx then belief statement [x, y, ψ :θ]?+ is applicable with the role of a partial answer, denoted ❀?+1.
Partial answers do not necessarily balance the belief and desire state be- cause there may exist sentences that are part of agent’s desire state that are not subsumed under the sentence from the (partial) answer. On the other hand, agents can also state sentences in which all sentences of interest are subsumed, but these sentences may have more information than asked for, resulting in an over-informative answer.

Definition 3.3 (over-informative answer) If ψ : θ2
∈ Mx
DBy , ψ : θ	/∈

MxBy, ψ : θ1
/∈ Mx
DBy , θ	≤
θ1 and ψ : θ1
∈ Bx
then belief statement

[x, y, ψ:θ1]?+ is applicable with the role of an over-informative answer, denoted
❀?+2.
If an agent states a sentence that is part of the upper bound of a manifested desire state, then this statement is less informative that an over-informative answer. Nevertheless, it balances the desire and belief state without stating more information than asked for. This belief statement has the role of a minimal answer.

Definition 3.4 (minimal answer) If ψ:θ ∈ upk
(Mx
DBy ), ψ:θ /∈ M B
and

ψ : θ ∈ Bx then belief statement [x, y, ψ : θ]?+ is applicable with the role of a
minimal answer, denoted ❀?+3.
Counter-questions
If an agent is aware that another agent is interested in a particular sentence that is not part of its own private belief state, but, it does know a prerequisite for this sentence, then it may pose a counter-question asking for this prerequi- site. Answers to this question may result in an extension of the agent’s belief state in which the particular sentence is included. A question from agent x to agent y is applicable as a counter-question when x considers y interested in sentence ψ:θ, this sentence is not part of x’s belief state, but, in the prerequisite set for ψ:θ a sentence exists that is fresh and sensible.

Definition 3.5 (counter-question) If ψ:θ1
∈ upk
(Mx
DBy ), ψ:θ
/∈ Mx
By,



ψ:θ1 /∈ Bx, ϕ:θ2 ∈ 7 ∈ SBx
({ψ:θ1}), ϕ:θ2 /∈ MxIz and ϕ:θ2 /∈ MxMzDBx
then

question [x, z, ϕ:θ2]? is applicable with the role of counter-question, denoted
❀?' .
A special case of a counter-question is when the agent considers itself interested in a sentence, this is not elaborated.

Ignorance statement
If an agent cannot balance a desire and belief state of another agent, that is, it can neither utter a belief statement nor a counter-question, than it may state its ignorance. An ignorance statement from agent x to agent y is applicable when x is aware that y is interested in sentence ψ:θ which is not part of x’s belief state, and in addition, all sentences (possibly none) that are part of the set of prerequisites of ψ :θ are neither fresh nor sensible to y, in this case x states its ignorance to y regarding ψ:θ.

Definition 3.6 (ignorance statement) If ψ : θ ∈ upk
(Mx
DBy ), ψ : θ /∈

MxBy, ψ : θ  /∈ Bx, (67	∈ SBx
({ψ : θ}))(7	⊆ MxMzDBx
∪ MxIz) and

ψ:θ /∈ MxMzDBx then ignorance statement [x, z, ψ:θ]?− is applicable, denoted
❀?−.

Update Rules
In the previous section, dialogue rules were identified that define when com- municative acts are applicable given an agent’s cognitive state. Update rules or epistemic commitment functions describe the opposite: they prescribe the cognitive state of both the sending and the receiving agent after the informa- tion in a communicative act is accepted by both agents. Different attitudes that agents may have towards incoming information are represented with dif- ferent types of update rules. Contrary to belief revision [7] in which agents can be said to have an aversion to inconsistent belief states, no restriction on incoming information is assumed, that is, agents accept all communicative acts, possibly rendering their belief states inconsistent. The following rules are confined to k-monotonous updates.
If agent x has uttered a belief statement to agent y, agent y believes sentence ψ:θ, that is, ψ:θ ∈ By; y is aware that x believes the sentence, that is, ψ:θ ∈ MyBx; and x is aware that y believes the sentence, that is, ψ:θ ∈ MxBy.
Definition 3.7 If [x, y, ψ :θ]?+ is uttered, then ψ :θ ∈ By, ψ :θ ∈ MyBx and
ψ:θ ∈ MxBy hold; this update after a belief statement is denoted ✄?+.
If agent x has uttered a question to agent y, agent y is aware that x has the desire to believe sentence ψ:θ, that is, ψ:θ ∈ MyDBx ; and y is aware that x is aware that y has the desire to believe the sentence, that is, ψ:θ ∈ MxMyDBx .

Definition 3.8 If [x, y, ψ : θ]? is uttered, then ψ : θ ∈ MyDBx
and ψ : θ ∈

MxMyDBx hold; this update after a question is denoted ✄?.

If agent x has uttered an ignorance statement to agent y, agent y is aware that x is ignorant of sentence ψ:θ, that is, ψ:θ ∈ MyIx; and x is aware that y is aware that x is ignorant of the sentence, that is, ψ:θ ∈ MxMyIx.
Definition 3.9 If [x, y, ψ:θ]?− is uttered, then ψ:θ ∈ MyIx and ψ:θ ∈ MxMyIx
hold; this update after an ignorance statement is denoted ✄?−.
Example Dialogue
An overviewof the proof that the example dialogue follows from the dialogue game is presented next. 6 First, the initial state of the agents’ cognitive states is specified. Such a state is the set of all mental constructs of the cognitive states of the agents participating in the dialogue. The domain description from section 3.1 is used.
In the initial agent’s cognitive state (state 0) only the following sentences are included. (i) Elmo desires to believe that he can go play outside or that he cannot play outside, (ii) Tv believes that it possibly rains outside, and (iii) Grover believes that he is wearing a raincoat. Oscar believes a lot: (iv a) if it rains outside, then Elmo cannot play outside; (b) if it does not rain outside, then Elmo can play outside; (c) if it possibly rains outside, then it is possible that Elmo cannot play outside; (d) if it possibly does not rain outside, then Elmo can possibly play outside; and (e) if Grover wears a raincoat, then it rains outside.
p:t, p:f ∈ DBelmo
r:0∼0.5 ∈ Btv
c:t ∈ Bgrover
(a) (r:t → p:f):t ∈ Boscar
(r:f → p:t):t ∈ Boscar
(r:0.5∼0 → p:0∼0.5):t ∈ Boscar
(r:0∼0.5 → p:0.5∼0):t ∈ Boscar
(c:t → r:t):t ∈ Boscar
Proof (overview) The following nine lemmas show that Dialogue 2 follows from the initial state and the rules of the dialogue game.


6 Full proof of the example dialogue’s validity is omitted.

7.	state 6 + 8 ❀?+2 [oscar, elmo, p:0.5∼1]?+ ✄?+ state 9.
Conclusions
The aim of this article was to show that with the introduction of a multi-valued logic to dialogue games, dialogues with inconsistent and biased information can be dealt with consistently. We presented a dialogue game by defining the agent’s cognitive state as sets of multi-valued theories. We defined dia- logue rules that enable agents to pose questions, to state partial, minimal and over-informative answers to question and to state ignorance statements when no other answer could be found. Lastly, we defined update rules to process incoming information.
We separated the semantics of a domain as described in an ontology from the epistemic attitude agents can have of this domain. Concepts from the ontology are combined with truth-values from the bilattice structure to form mental constructs, which on their turn form the agent’s cognitive states. The update rules and especially the dialogue rules arrange the possible commu- nicative acts independent of the domain theory (ontology) and the epistemic theory (bilattice), making the dialogue game a multi-agent notion by delegat- ing the ontology and epistemic theory to individual agents.
In the fictitious dialogue from Sesame Street, it was shown that a complex dialogue between four participants is possible with truth-values other than the orthodox true and false. In addition, the formal counterpart of the example dialogue was proven to follow from an initial state and the presented dialogue game. We have shown in essence that the relation between communicative acts and the agents’ cognitive state does not have to suffer from inconsistent cognitive states. In other words, the grammar of the dialogue remains con- sistent while the contents agents talk about may become inconsistent, that is, agents can communicate about inconsistent beliefs in a sensible, consistent way.
Future research includes a definition of communicative acts for derogation of sentences which result in contractions of the agent’s cognitive state, and ac- companying dialogue and update rules that can cope with k-non-monotonous behaviour. Introduction of derogations will make it possible to define be- lief revision at the multi-agent level of description. A second focus is to use these dialogue games for defining the semantics of FIPA ACLs restricted to information communication performatives. Definition of specialised dialogue rules for ontological relations such as classifications, taxonomic and mereologic relations are also part of ongoing research.
References
Ofer Arieli and Arnon Avron, The value of the four values, Artificial Intelligence
102(1) (1998), pp. 97–141.


N. D Belnap Jr.,A Useful Four-valued Logic, In: J. Michael Dunn and
G. Epstein, editors, “Modern Uses of Multiple-Valued Logic,” pages 8–37, 1977.
Robbert-Jan Beun,
On the Generation of Coherent Dialogue: A Computational Approach, Pragmatics & Cognition 9(1) (2001), pp. 37–68.
B. A. Davey and H. A. Priestley, “Introduction to Lattice and Order,” Second Edition, Cambridge University Press, 2002.
J. Doyle, A truth maintenance system, Artificial Intelligence 12 (1979).
Melvin Fitting, Bilattices and the semantics of logic programming, Journal of Logic Programming 11, pp. 91–116.
Peter G¨ardenfors, “Knowledge in Flux: Modeling the Dynamics of Epistemic States,” A Bradford book, The MIT Press, 1988.
Matthew L. Ginsberg, Multivalued Logics: A Uniform Approach to Reasoning in Artificial Intelligence, Computational Intelligence 4 (1988), pp. 265–316.
H.P. Grice, Logic and conversation, In: P. Cole and J.L. Morgan, editors, Speech Acts, volume 11 of Syntax and Semantics, pages 41–58. Academic Press, 1975.
S. C. Kleene, “Introduction to Methmathematics,” Van Nostrand, 1950.
Yannis Labrou, Standardizing Agent Communication, Lecture Notes in Computer Science 2086 (2001), pp. 74–98.
Henk-Jan Lebbink, Cilia Witteman, and John-Jules Meyer, A dialogue game for belief revision in multi-agent systems, Technical report, Information & Computing Sciences, Utrecht University, Netherlands, in preparation.
Henk-Jan Lebbink, Cilia L.M. Witteman, and John-Jules Ch. Meyer, A Dialogue Game to Agree to Disagree about Inconsistent Information ,
In: Ivana Kruijff-Korbayova´ and Claudia Kosny, editors, 7th  Workshop
on the Semantics and Pragmatics of Dialogue (Diabruck’03), pages 83–90, Wallerfangen, Germany, September 4-6 2003.
Peter McBurney and Simon Parsons, Geometric Semantics for Dialogue Game Protocols for Autonomous Agent Interactions, In: Patrick Cousot et al., editor, Electronic Notes in Theoretical Computer Science 52 (2002).
R. Reiter, A logic for default reasoning, Artificial Intelligence 13(1–2) (1980),
pp. :81–132.
Nicholas Rescher, “Many-valued Logic,” McGraw-Hill, 1969.
Andreas Scho¨ter, Evidential Bilattice Logic and Lexical Inference, Journal of Logic, Language and Information 5(1) (1996), pp. 65–105.
Mark Stefik, “Introduction to Knowledge Systems,” Morgan Kaufmann publishers, Inc, San Francisco, California, 1995.
M.J. Wooldridge and N.R. Jennings, Intelligent Agents: Theory and Practice, Knowledge Engineering Review 10(2) (1995), pp. 115–152.
