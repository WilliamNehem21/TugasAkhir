
ORIGINAL ARTICLE

Determining the number of clusters for kernelized fuzzy C-means algorithms for automatic medical image segmentation
E.A. Zanaty

Computer Science Department, Faculty of Science, Sohag University, Sohag, Egypt

Received 12 December 2011; revised 18 January 2012; accepted 29 January 2012
Available online 23 February 2012

Abstract In this paper, we determine the suitable validity criterion of kernelized fuzzy C-means and kernelized fuzzy C-means with spatial constraints for automatic segmentation of magnetic res- onance imaging (MRI). For that; the original Euclidean distance in the FCM is replaced by a Gaussian radial basis function classifier (GRBF) and the corresponding algorithms of FCM meth- ods are derived. The derived algorithms are called as the kernelized fuzzy C-means (KFCM) and kernelized fuzzy C-means with spatial constraints (SKFCM). These methods are implemented on eighteen indexes as validation to determine whether indexes are capable to acquire the optimal clus- ters number. The performance of segmentation is estimated by applying these methods indepen- dently on several datasets to prove which method can give good results and with which indexes. Our test spans various indexes covering the classical and the rather more recent indexes that have enjoyed noticeable success in that field. These indexes are evaluated and compared by applying them on various test images, including synthetic images corrupted with noise of varying levels, and simulated volumetric MRI datasets. Comparative analysis is also presented to show whether the validity index indicates the optimal clustering for our datasets.
© 2012 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.




E-mail address: zanaty22@yahoo.com

1110-8665 © 2012 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.

Peer review under responsibility of Faculty of Computers and Information, Cairo University.
doi:10.1016/j.eij.2012.01.004

Introduction

Clustering is one of the most popular classification methods and has found many applications in pattern classification and image segmentation [1–5]. Clustering algorithms attempt to classify a voxel to a tissue class by using the notion of sim- ilarity to the class. Unlike the crisp K-means clustering algo- rithm [4], the FCM algorithm allows partial membership in different tissue classes. Thus, FCM can be used to model the partial volume averaging artifact, where a pixel may contain multiple tissue classes [2,3]. The kernelized fuzzy C-means

40	E.A. Zanaty


(KFCM) [6–9] used a kernel function as a substitute for the in- ner product in the original space, which is like mapping the space into higher dimensional feature space. There have been a number of other approaches to incorporating kernels into fuzzy clustering algorithms. These include enhancing clustering algorithms designed to handle different shape clusters [8]. More recent results of fuzzy algorithms have been presented in [9] for improving automatic MRI image segmentation. They used the intra-cluster distance measure to give the ideal num- ber of clusters automatically; more discussion can be found in [9]. Also, possibilistic clustering which is pioneered by the possibilistic c-means (PFCM) algorithm was developed in [10–12]. They had been shown that PFCM is more robust to outliers than FCM. However, the robustness of PFCM comes at the expense of the stability of the algorithm [11]. The PCM- based algorithms suffer from the coincident cluster problem, which makes them too sensitive to initialization [12].
Most fuzzy methods have several advantages including yielding regions more homogeneous than other methods; reducing the spurious blobs; removing noisy spots; reduced sensitivity to noise compared to other techniques. However, they require prior knowledge about the number of clusters in the data, which may not be known for new data [13]. In liter- ature, many studies in dealing with this problem are available in [14–18], and, so, there are many cluster validity indexes in this regard. Compactness and separation are two criteria for the clustering evaluation and selection of an optimal clustering scheme [14]. The variation of data within clusters indicates compactness and isolation between clusters indicates separa- tion, respectively.
Though some compatibility or similarity measure can be applied to choose the clusters to be merged, no validity mea- sure is used to guarantee that the clustering result after a merge is better than the one before the merge. Partial results were sta- ted in [19] to answer the questions: ‘‘Can the appropriate num- ber of clusters be determined automatically? And if the answer is yes, how?’’ More existing methods were found in [14–21] to review few validity indexes that can combine with fuzzy c- means algorithms. But, the performance of wide range indexes is not found in any literature before; especially when they ap- plied to kernelized fuzzy c-means (KFCM) or kernelized fuzzy c-means with spatial constraints (SKFCM) methods.
In this paper, we seek the answer to the previous questions for exploring which indexes can achieve high accuracy segmen- tation whey they performed with KFCM and SKFCM. Our objective is not to improve the segmentation accuracy via enhancing the kernel function, but is to find the indexes with KFCM and SKFCM capable to produce good MRI segmen- tation. For that; the original Euclidean distance in the FCM algorithm is replaced by the Gaussian radial base function (GRBF)-induced kernel, which is shown to be more robust than FCM (with Euclidean distances). This will make a gener- alization of the existing FCM methods. The KFCM and SKFCM algorithms based on Gaussian RBF kernel are de- rived and applied independently on each image. Based on these algorithms, eighteen indexes are implemented to estimate the number of clusters that represents the best structure of a given image. Key existing solutions are evaluated to obtain the clus- ter validity in the domain of image segmentation. A wide num- ber of various validity indexes from the classical and more recent indexes are examined. As segmentation of medical images is of particular interest in our application, the work
here includes the assessment of those indexes on 3D MRI datasets.
The rest of this paper is organized as follows: Section 2 pre- sents the kernel methods. Several criteria to determine the number of clusters are briefly reviewed in Section 3. Experi- mental comparisons are presented in Section 4. Finally, Sec- tion 5 gives our conclusions.

Kernel methods

The kernel methods [8,13,22–26] are one of the most researched subjects within machine learning community in the recent few years and have widely been applied to pattern recognition and function approximation. A common philosophy behind these algorithms is based on the following kernel (substitution) trick, that is, firstly with a (implicit) nonlinear map, from the data space to the mapped d feature space, W: X → F (x → W(x)), a dataset { x, .. . , x} c X (an input data space with low dimen- sion) is mapped into a potentially much higher dimensional feature space or inner product F, which aims at turning the original nonlinear problem in the input space into potentially a linear one in rather high dimensional feature space so as to facilitate problem solving as proved by Girolami [23]. A kernel K(x, y) in the feature space can be represented as:
K(x; y)= (W(x); W(y))	(1)
where (W(x), W(y)) denotes the inner product operation.
An interesting point about kernel function is that the inner product between W(x) and W(y) can be implicitly computed in F, without explicitly using or even knowing the mapping W.
So, kernels allow computing inner products in the space, where one could otherwise not practically perform any compu- tations. Three commonly-used kernel functions in literature
[25] are:

Gaussian  Radial  basis  function  (GRBF)  kernel:
K(x, y) = exp (—x — y  2/r2).
Polynomial kernel: K(x, y)= (Æx, y) + 1)d.
Sigmoid kernel K(x, y) = tanh(aÆx, y) + b).

where r, d, a and b are the adjustable parameters of the above kernel functions. The main motives of using the kernel meth- ods consist of: (1) inducing a class of robust non-Euclidean dis- tance measures for the original data space to derive new objective functions and thus clustering the non-Euclidean structures in data; (2) enhancing robustness of the original clustering algorithms to noise and outliers, and (3) still retain- ing computational simplicity.
Sigmoid kernel is a two-layer neural network kernel and is used as a particular kind of two-layer sigmoid neural network. For this, only a set of parameters satisfying the Mercer theo- rem can be used to define a kernel function [23–26]. The inter- ested reader may refer to [25] for more details. In this section we only stress on GRBF, which is shown to be more robust than FCM (with Euclidean distances) [7].

Fuzzy C-means method (FCM)

Fuzzy C-means clustering (FCM), also known as fuzzy ISO- DATA, is a data clustering algorithm in which each data point belongs to a cluster to determine a degree specified by its

Determining the number of clusters for kernelized fuzzy C-means algorithms	41

membership grade [1–3]. Bezdek [1] proposed this algorithm as an alternative to earlier k-means clustering. FCM partitions a collection of N vector xi, i = 1, ... , N into C fuzzy groups, and finds a cluster centre in each group such that an objective function of a dissimilarity measure is minimized. The major difference between FCM and k-means is that FCM employs fuzzy partitioning such that a given data point can belong to several groups with the degree of belongingness specified by membership grades between 0 and 1. In FCM, the membership
W(xj)— W(ci))2 = (W(xj)— W(ci))T(W(xj)— W(ci))
= WT(xj)W(xj)— WT(xj)W(ci)
— W(xj)WT(ci)+ WT(ci)W(ci)
= K(xj, xj)— 2K(xj, ci)+ K(ci, ci)
In	GRBF	kernel	K(x, c) = exp(—x — c  2/r2),
K(xj, xj)= 1, K(ci, ci) = 1, and WT(xj)W(ci) = W(xj)WT(ci).
From Eqs. (2) and (4), we get:

matrix U = [uij] is allowed to have not only 0 and 1 but also	C	N
the elements with any values between 0 and 1. This matrix sat-	Jm = 2 X X um(1 — K(xj, ci))	(5)

isfies the constraints:
i=1
j=1

C	N	The objective of this paper is to determine the validity cri-
X uij = 1,	∀j = 1, .. . , N; 0 6 uij 6 1,	X uij > 0,	∀i	terion of kernelized fuzzy C-means (KFCM) when applied to

i=1
j=1
The objective function of FCM can be formulated as
MRI data sets. It was shown in [22] that the GRBF kernel,
has better segmentation results on simulated MR images cor- rupted by noise and other artifacts than the based polynomials

follows:
C
Jm =
i=1


N
j=1
um  xj — ci  2	(2)
algorithms [21–26]. We confine ourselves to the GRBF kernel to seek the best index that can be used for reliable kernelized fuzzy C-means clustering.
In a similar way to the FCM algorithm, the objective func- tion Jm in Eq. (5) can be minimized under the constraint of U.

where C is the number of clusters; ci is the cluster centre of fuz-
zy group i and the parameter m is a weighting exponent on each fuzzy membership. Fuzzy partitioning is carried out through an iterative optimization of the objective function shown above, updating of membership uij and the cluster cen- tres c by:
Specifically, taking the first derivatives of Jm with respect to uij and ci, and zeroing them respectively, two necessary but not sufficient conditions for Jm to be at its local extrema will be ob- tained. The fuzzy membership matrix U can be obtained from:
PC  (1 — K(x , c ))1/(m—1)

1
uij =
j  i
(3)

C
k=1
PN


||xj —ci || 2/(m—1)
||xj —ck ||
The cluster center ci can be obtained from:
PN umK(xj, ci)xj

m
c =	j=1 ij
ci =
j=1 ij
PN  m
(7)

i	N  m
j=1 ij
In image clustering, the most commonly used feature is the
gray-level value, or intensity of image pixel. Thus the FCM objective function is minimized when high membership values are assigned to pixels whose intensities are close to the centroid of its particular class, and low membership values are assigned when the point is far from the centroid.
Kernelized fuzzy C-means method (KFCM)
j=1uij K(xj, ci)
Through the following section, we will only use the GRBF
kernel for the simplicity of derivation of Eqs (6) and (7). For other kernel functions, the corresponding equations are a little more complex, because their derivatives are not as simple as the GRBF kernel function. The standard kernelized fuzzy C-means (KFCM) algorithm can be summarized in the following steps:

Step 1: Fix c, tmax, m > 1 and e > 0 for some positive constant.
Step 2: Initialize the memberships u0 , C, m.

The algorithm that uses inner products can implicitly be exe-
cuted in the feature space F. This trick can also be used in clus-
Step 3: For t = 1, 2, ... , tmax do
Update all prototype ct with Eq. (7);

tering, as shown in support vector clustering [22] and kernel
Update all memberships ut
with Eq. (6);

(fuzzy) C-means algorithms [23,24]. A common ground of
Compute Et = maxi,j|ut — ut—1|, if Et 6 e, stop;

these algorithms is to represent the clustering centre as a line- arly-combined sum of all W(xk), i.e. the clustering centres is lo- cated in feature space. A kernelized FCM algorithm is
End;
ij	ij

constructed with objective function as following:
Kernelized fuzzy C-means with spatial constraints

c	N	(SKFCM)
Jm = X X um||W(xj)— W(ci))||2	(4)

i=1  j=1
In this section, we select three kinds of fuzzy c-means methods

where W is an implicit nonlinear map as described previously. Unlike Refs. [23,24], W(ci) here is not expressed as a linearly- combined sum of all W(xk) anymore, a so-called dual repre- sentation, but still reviewed as a mapped point (image) of ci in the original space, then with the kernel substitution trick, we have:
which almost cover all objective functions. The objective func-
tion consists of two parts: the original objective function and penalty called spatial constraint. All improvements of fuzzy c-means methods lie on modifying spatial constraint formula. Based on this formula, we can divide fuzzy methods into three categories: firstly, the spatial constraint is only based on

42	E.A. Zanaty


Euclidean distance as in Ahmed et al. [2]. In the next category, the spatial constraint is only based on membership values as presented in Zhang et al. [6]. The third one, it uses Euclidean distances based on weighted averaging image window as in
Taking the partial derivative of Lm with respect uij and ki, and then setting them to zero, we have:

∂Lm = 0 →⇒ 2mum—1(1 — K(x , c )) +  2a mum—1 X(1

Kang et al. [7]. Others recent methods try to enhance one of these objective function such as in [9,12]. Here, we replace the Euclidian distance by GRBF kernel of the KFCM with
∂uij
ij	j  i
— K(xr, ci)) + kj(—1)
NR	ij
r∈Nj

spatial constraint to induce the generalization of these meth- ods. For example, Ahmed et al. [2] introduced the modified objective function of FCM as:
∂Lm
= 0	(13)
XC

J = X X um  x — c 
+  a  X X um X 
x — c 
2	(8)
∂kj
i=1

The Euclidean distance of objective function in Eq. (8) is re-
placed by Gaussian RBF kernel as:
11/(m—1)

XC  XN

 

2a XC  XN	X



  	

kj
u =		 

— K(xr, ci))	(9)
where Nj stands for the set of neighbors that exist in a window
around xj (not including xj itself) and NR is the cardinality of
Substituting (15) into (14) gives:
(15)

Nj. The parameter a controls the effect of the penalty term and lies between zero and one inclusive.
1/(m—1)
j

2m
C
k=1
1	1/(m—1)
(1 — K(xj, ck)) + a Pr∈(1 — K(xr, ck))

This penalty term  2a PC
PN umP
NR
(1 — K(x , c )) con-

tains spatial neighborhood information, which acts as a regu-
larizer and biases the solution toward piecewise-homogeneous
labeling. Such regularization is helpful in segmenting images corrupted by noise.

  k  1/(m—1)
(16)
1

The objective function J
under the constraint of u
and c	2m
PC ((1 — K(x , c )) + a ÿP
(1 — K(x , c )) —1/(m—1)

Theorem. Let X = {xi, i = 1, 2, .. ., N|xi e Rd} denotes an image with N pixels to be partitioned into C classes (clusters), where xi represents feature data. The algorithm is an iterative
Finally, substituting Eq. (17) into Eq. (15), we get:
1
uij =	1/(m—1)
P	(1—K(xj ,ci ))+ a	(1—K(x ,c ))
(18)

(9) with the following constraints:


k=1 (1—K(xj ,ck ))+ a P	(1—K(xr ,ck))

XC	XN

	

NR  r∈Nj
2  2

> 0,	∀i	(10)
Then uij and ci must satisfy the following equalities:
1
∂Lm
∂ci
= 0 →⇒ 2
N

N
j=1
um(1 — K(xj, ci))(xj — ci)(—1/r2)+ 
 2a
NR

uij =
P	!1/(m—1)
× X um X(1 — K(x , c ))(x — c )(—1/r2)

j k  NR
r∈Nj	r k
= 0

P  um{(1 — K(xj, ci)xj +  a  P
(1 — K(xr, ci))xr}

i	PN um{(1 — K(x , c )) +  a  P
(1 — K(x , c ))}

j=1 ij
i  i	NR
r∈Nj
r  i
(12)
N	a
um((1 — K(x , c ))x +
X(1 — K(x , c ))x )

Proof. The minimization of constraint problem Jm in Eq. (9)
ij
j=1
N
j  i	j
NR r∈Nj
r  i	r

under constraints can be solved by using the Lagrange
= X um((1 — K(x , c )) + a
X(1 — K(x , c )))c

multiplier method. Now we define a new objective function with constraint condition (10) as follows:
ij
j=1
j  i	N
R r∈Nj
r  i	i

XC  XN
XC XN	X
P  um((1 — K(xj, ci))xj + a P
(1 — K(xr, ci))xr)

Lm = 2
uij (1 — K(xj, ci)) + N
uij	(1
i	PN um((1 — K(x , c )) +  a  P
(1 — K(x , c )))

— K(xr, ci)) +
N
j=1
kj  1 —
C
i=1
uij!

This completes the proof.  h
(19)

Determining the number of clusters for kernelized fuzzy C-means algorithms	43


The objective function of Zhang et al. [6] can be defined as:
C	N	C	N
J  =	um  x — c 2 +	um	(1 — u )m

antee that the clustering result after a merge is better than the one before the merge, i.e. we want to explore which indexes can achieve high accuracy segmentation [19]. In the sequel,




Similar to Ahmed et al. [2], the objective function of Zhang et al. [6] can be derived as:
In this section, we will seek the most suitable validity crite-
rion in the following indexes, regrouped into three categories. The first category uses only the membership values. The sec-

Jm = 2
C
i=1
N
j=1
um(1 — K(xj, ci)) +
 a  C
NR i=1
N
j=1
m	(1 — uir)
r∈Nj
ond one involves both the U matrix and the dataset itself. Sta- tistical indexes are presented in the third one. In our implementation, the under-sampled dataset is obtained by

(20)
The objective function Jm is minimized under the constraint
of uij and we get:
1
averaging every 2 · 2 pixels (2 · 2 · 2 voxels for 3D data) in the original dataset. One desirable effect here is that the resul- tant half-sized dataset contains smaller noise, which ought to lead to better cluster estimation. The KFCM and SKFCM

uij =
P
  (1—K(xj ,ci ))+ a P
(1—u )m !1/(m—1)
(21)
are defined by a matrix U = [uij], where uij denotes the degree

k=1 (1—K(xj ,ck ))+ a P	(1—uir )m
cluster representatives have been defined. We denote a crite-



Because the penalty function does not depend on ci the nec-
essary conditions under which Eq. (12) attains its minima is identical to that of standard KFCM (Eq. (7)).
The objective functions of Kang et al. [7] is as follows:
C	N	C	N
the plot of q versus C (number of clusters). Also, in case that q exhibits a trend with respect to the number of clusters, we seek a significant knee of decrease (or increase) in the plot of q.

Indexes involving only the membership values

J = X X um||x — c ||2 + a X X um||x¯* — c ||2	(22)

Similarly, in Kang et al. [7], the objective function can be
derived as:
The partition coefficient is proposed by Bezdek et al. [27], and
defined as:

Jm = 2
C
i=1
N
j=1
um(1 — K(xj, ci)) + 2a
C
i=1
N
j=1
um(1
PC =
1  N
N i=1
C
2
ij
j=1
(26)

— K(x¯*, ci))	(23)
where x¯* represents the grey value of pixel in the weighted aver- aging image window, more discussions can be shown in [7].
Similarly to Eqs. (11) and (12), the membership functions and cluster centers are updated by the following expressions:
1
The PC index values range in [1/C, 1], where C is the num- ber of clusters. The closer the index to unity the ‘‘crisper’’ the clustering is. In case that all membership values to cluster par- tition are equal, that is, uij = 1/C, the PC coefficient obtains its lowest value. Thus, a value close to 1/C indicates that there is no clustering tendency in the considered dataset or the cluster-

uij =

C	(1—K(xj ,ci ))+a(1—K(x¯*,ci )) 1/(m—1)
k=1 (1—K(xj ,ck ))+a(1—K(x¯* ,ck ))
PN um K(xi, ci)xi + a(1 — K(x¯*, ci))x¯* 
(24)
ing algorithm failed to reveal it.

The partition entropy coefficient (PE)
The partition entropy coefficient is defined as [27]:

i	PN um{(1 — K(x , c )) + a(1 — K(x¯*, c ))}
1 XN  XC



except in step 3(a) and (b), Eqs. (18) and (19) in Ahmed
et al. [2], Eqs. (21) and (7) in Zhang et al. [6], and Eqs. (24) and (25) in Kang et al. [7] are used instead of Eqs. (6) and
(7) to update the memberships and centers.

Indexes for determining number of clusters

One of the most important issues in cluster analysis is the eval- uation of clustering results to find the partitioning that best fits the underlying data. Although fuzzy methods [3–7] have several advantages in segmentation accuracy and less sensitive to noise, they have a drawback in requiring prior knowledge about the number of clusters in the data, which may not be known for new data. The final number of clusters is still always sensitive to define the threshold criterion for merging. Though some
The index is computed for values of C greater than 1 and its values range in [0, log C]. The closer the value of PE to 0, the harder the clustering is. As in the previous case, the values of index close to the upper bound (i.e. log C), indicate absence of any clustering structure in the dataset or inability of the algo- rithm to extract it.

The modification of the PC index (MPC)
The drawback of PC is its monotonous dependency on the number of clusters. Thus, we seek significant knees of increase (for PC) or decrease (for PE) in plot of the indexes versus the number of clusters. Modification of the PC index proposed by Dave [28] can reduce the monotonic tendency by using the fol- lowing formula:

compatibility or similarity measure can be applied to choose the clusters to be merged, no validity measure is used to guar-
MPC(C)= 1 — C — 1 C
(1 — C*PC)	(28)

44	E.A. Zanaty


where 0 6 MPC(C) 6 1. Note that the MPC index is equiva- lent to the non-fuzziness index (NFI). In general, an optimal
1  C
DB =
C
max Rij,	i = 1, ... , C, i – j	(31)

cluster number C\ is found by solving max
26C6N—1
MPC(C)
j=1

to produce a best clustering performance for the data set X.
where


Indexes involving the membership values and the dataset
Rij
= si + sj
dij
The similarity between clusters is obtained and the maxi-

The Xie–Beni index (S)
The Xie–Beni index, also called the compactness and separa- tion validity function, is defined as [29]:
mum value is denoted as max Rij. If ci denotes the centroid
of cluster mi, with:
sﬃﬃﬃﬃﬃﬃﬃ1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃXﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

( 1  XC


2 ),{D


min
}2	(29)
si =
card(mi)
x∈mi
x — ci  2
(32)

where
N
r2 =	uij  xj — ci  2
dij = max ci — cj  i,	j = 1, .. . , C
The number of clusters which minimizes the DB index is the
optimal one.

j=1
where xj: j = 1, ... , N is a set of N feature vectors that is to be
partitioned (clustered) into C clusters, for each cluster ci, i = 1, .. . , C, represents its prototype, or center, and Dmin is the minimum distance between the prototypes (cluster centers).
The cluster validity measure (VM)
The cluster validity measure (VM) is defined as [33]:
VM = (C + (f × G(2, 1)+ 1)). Da
D

(33)

A large D
min
value means a lower reciprocal value. Each r2 is a

fuzzy weighted mean-square error for the ith cluster, which is smaller for more compact clusters.
where Da which measures the compactness of the clusters, is
defined as:


The modified Xie–Beni index (XB)
It is a modification [30] of the Xie–Beni index by summing only
Da =
1  C
N i=1

Xx∈mi
x — ci
2	(34)

the members of each cluster rather than over all N exemplars for each cluster. Also the reciprocal XB = 1/S is taken so that a larger value of XB indicates a better clustering and the XB is called modified Xie–Beni clustering validity measure.

The I-indexes (I)
Consider a data set of N points partitioned into C clusters. The
I index [31] is defined as follows:
  D	 p
De which measures the average separation between two clus-
ters over all possible pairs of clusters, is defined as:
De = average(ci — cj  2),  i = 1, 2, ... , C, j
= i + 1, .. . , C	(35)
f is some natural constant; G(2, 1) is a GRBF with mean value
equal to 2 and standard deviation equal to 1, and ci is the clus- ter center of the cluster mi. The VM measure should be mini- mized to get a good segmentation result coming from




where
N	C
i=1  j=1
The Fukuyama–Sugeno index (FS)
The Fukuyama–Sugeno index is defined as [34]:
FS = X X um(x — c 2 — c — c 2)	(36)

ered to be the correct number of clusters. The first factor Dmax is the maximum distance between the prototypes (cluster cen- ters). It will increase with the number C, hence reducing the in- dex as the number of cluster increases. The second factor  1 will try to reduce the index as the number of cluster increases. The
third factor 1 , which measures the total fuzzy dispersion, will
C
penalize the index as it is increased. The power of p is used to
control the contrast between the different cluster configura- tions. In our implementation, we take p = 2.

The Davies–Bouldin index (DB)
Assume a similarity measure R(mi, mj)= Rij between two clus- ters mi and mj is defined based on a measure of dispersion
s(ci)= si of a cluster mi, and a dissimilarity measure d(mi, mj)= 
c
i=1 i
arated clusters we expect small values for FS. The first term in
the parenthesis measures the compactness of the clusters and the second one measure the distances between two clusters centers.

The fuzzy hyper volume (FHV)
The fuzzy hyper volume is proposed by Gath and Geva [35] based on the concepts of hyper volume and density. The fuzzy hyper volume is given by:
C
FH =  Vj	(37)
j=1

dij between two clusters mi and mj. Rij is defined to be non
 X 
  PN um(x — c )(x — c )T!1/2

V =	=
i=1 ij	i	j	i	j
(38)

Determining the number of clusters for kernelized fuzzy C-means algorithms	45


Small values of FH indicate the existence of compact clusters.

The average partition density (PA)
The average partition density is defined as [32]:
1 XC  Sj

 

A large PCAES(C) value means that each of these c clusters is compact and separated from other clusters. A small PCAES(C) value means that some of these c clusters are not compact or separated from other clusters. The maximum of PCAES(C) with respect to c, could be used to detect the data structure with a compact partition and well-separated clusters. Thus,  an  optimal  c\  can  be  found  by  solving




with Sj =	x∈Xj uij, where xj is the set of data points within a window around j (i.e. the center of cj cluster), Sj is called the
sum of the central members of the cj cluster.

The partition density index (PD)
The partition density index is given by [32]:
for the data set X.

3.2.12. The PBMF index
The PBMF-index [38] can provide a measure of goodness of clustering on different partitions of a data set and is defined as follows:

PD = S/FH
where
1
PBMF = c ×
E1 × max 
i,j
ci — cj 
(43)

where C is the
XC
re, E1 = PN uij  xj — ci  .

S =	Sj	(40)
j=1
The PD and PA measures should be at a minimum to get
good segmentation results.

The separation and compactness index (SC)
A validity function proposed by Zahid et al. [36] is defined by:
SC(C)= SC1(C)+ SC2(C)	(41)
where
It is seen that the factor E1 in the expression of the index is
a constant term for a particular data set. The maximum value of the index is supposed to give the appropriate number of clusters.

3.2.13. The compose within and between scattering (CWB) index
CWB index [39] is defined by:
CWB = aScat(C)+ Dis(C)	(44)
where

Pnc
ci — ci 2/C
PC	T
1/2

1	PC
(PC
(um)x — c 
2 /PC  u )
Scat(c)= C	i=1	
Dis(C)

and
Dmax XC
 XC  
!—1

PC—1PC
(PC
(min(uij, ulj))2)/PC
min(uij, ulj)
= D	ci — cr

SC2(C)= 
i=1
l=i+1
PC
j=1
(max u )2/PC
j=1
max u
min
i=1
r=1

j=1 16i6C  ij
j=1 16i6C ij
r(X)= 
1 XN
(xj — x)
x : center of the whole dataset

Both SC1 and SC2 measure the ratio of separation and compactness. SC1 considers the geometrical properties of the
N j=1

data structure and membership functions. SC2
considers only
*
 	N
r(c )=	u (x — c )2	wherei = 1, .. . , C

solving min26C6NSC(C) to produce the best clustering perfor-
mance for the data set X.

The partition coefficient and exponential separation (PCAES) index
The PCAES validity index is defined as [37]:
N
PCAESi =	u2/uM — exp — min{ci — ck  }/bT	(42)
j=1
Dmax = max{ci — cr},  i, r = 1, ... , C and i „ r
Dmin = min{ci — cr},  i, r = 1, ... , C and i „ r
a = Dis(cmax)
CWB tends to find an optimum value of both compactness and separation in fuzzy c-partitions. In this index, Scat(C) is


where
uM = min
16i6C

and
ij
j=1

N
2
ij
j=1
k–i
average scattering for c classes and Dis(C) is a distance func- tion associated with distance between class centers. The first term represents the compactness and second term the separa- tion. These two terms usually show opposite trends as C is changed. The minimum value of the index is supposed to give the appropriate number of clusters.

Statistical indexes

PC   c
— c 2

b =   l=1  1	
T	C
Obviously we have —C 6 PCAES(C) 6 C.

Some of the widely adopted criteria for statistical model selec- tion are used for determining the number of clusters. Recently,

46	E.A. Zanaty


El-Melegy et al. [19] presented two indexes for this reason, one is based on Akaike’s information criterion (AIC) [40] and the other is based on Cross-Validation [41].
vital importance to the segmentation process. The compari- son score S for each algorithm as proposed in [44] is defined as follows:


The index based on Akaike’s information criterion (AIC)
The classical AIC is defined as:
A ∩ Aref
S =
ref
(47)

AIC = Da + 2lr2	(45)
where l(C)= (C — 1)N + C in soft case, and l(C)= N + C
in case of hard, is the number of degree of freedom of the mod-
el, Da can be computed from Eq. (34), and r the noise level, can be estimated from
Da(C*)
where A represents the set of pixels belonging to a class as
found by a particular method and Aref represents the reference cluster pixels.

Kernelized fuzzy C-means algorithms

To assess the capabilities of the validity indexes to accurately

r2 =
qN — l(C*)
(46)
identify the number of clusters present in an image, both

where C\ is the maximum number of clusters, q is the co- dimension of the model (q = 1). The smaller the AIC value
is, the better the clustering performance for the data set.

The index based on cross-validation (V)
This index is based on cross-validation [19], which is an old, standard tool in statistics [41,42]. The data are divided into two sets, one used for determining the clusters and the other one is used to validate the obtained clusters. The underlying idea here is to validate them on a dataset different from the one used for cluster estimation. For the task of image segmen- tation, the two subsets of data can be formed in several ways. One way, which we will follow, is to use an under-sampled ver- sion of the dataset for cluster estimation and the original data- set for validation.

Results and discussion

The experiments were performed with two types of data. The first type of data consists of two simple synthetic images (synthetic1 and synthetic2), one corrupted by 9%, 12% salt and pepper noise, and the other corrupted by Gaussian noise of standard deviation (ST) 50% and 60%; and the image size is 64 · 64 pixels, as shown in Fig. 1a and b, respectively. The second type of data includes T1-weighted 3D MRI brain data with slice thickness of 1 mm, corrupted by 3% and 6% noise, and no intensity inhomogeneities [43]. The image size is 129 · 129 pixels obtained from the classical simulated brain database of McGill University [43]. Two slices drawn from the simulated MRI are shown in Fig. 1d and e. On other hand, we will point to the original data as clean data (0% noise). The quality of the segmentation algorithm is of
KFCM and SKFCM methods were implemented. Through
our implementation, we set the following parameters: r = 150 (GRBF kernel width),a = 0.5, m = 2, e = 0.00001, and NR = 0.5 (a 2 · 2 window centered around each pixel, ex- cept the central pixel itself). Note that the correct number of clusters for synthetic1, and synthetic2 is 2 and 4 clusters respectively. For the 3D simulated data, the correct number of clusters is 10. The standard KFCM algorithms (using itera- tive process Eqs. (6) and (7)) are applied independently on each image The eighteen indexes are used to estimate the best cluster of each image. In case of SKFCM, the objective functions of Ahmed et al. [2] is used which always gives stable and good re- sults. We use the iterative process in Eqs. (11) and (12), more discussion can be shown in [7]. The outcome of each index on the different test images is shown in Table 1.

In the case of KFCM algorithm
Dataset1 (synthetic1): the KFCM clustering algorithm is ap- plied to synthetic1 corrupted by 9%, 12% salt and pepper noise for the cluster number C = 2. By optimizing the valid- ity functions, most of the indexes indicate that 2 is an opti- mal cluster number which actually matches the structure of the image set except FS, PE, PC, SC PD indexes in the ori- ginal image and PD, PBMF, CWB, FS, PC, PCAES and V indexes for 9% noise. The PD, PBMF and CWB indexes gave the cluster number 3 for this dataset. However, FS, PC, PCAES and V consider that 5 is a good cluster number estimate. Furthermore, for 12% noise, PA, FHV, I, AIC, and V still achieve optimal clusters. The estimated cluster numbers by the validity indexes are shown in the synthetic1 column of Table 1.
Dataset2 (synthetic2): the KFCM clustering algorithm is applied to synthetic2 image corrupted by 50% and 60%














Figure 1	Test images: (a) synthetic1, (b) synthetic2, (c) 3D simulated data, (d) and (e) two original slices from the 3D simulated data (slice91 and slice100).


















48	E.A. Zanaty


Gaussian noise of ST. Intuitively, 4 clusters are suitable for the data set. The estimated cluster numbers by the validity indexes are shown on the synthetic2 column of Table 1. By optimizing the validity functions, most of the indexes gave optimal cluster number except PD, PA, FS, PE, XB, and MPC for the original image. For the noisy image of ST 50%, FHV, I, AIC, and PBMF indicate that 4 is an optimal cluster number which actu- ally matches the structure of the image set. PD, PA, FS, S, XB, VM, and SC indicate that 2 is the best cluster number estimate. DB and MPC indexes gave the optimal cluster number 7. PCAES indicates that 5 is the best cluster number estimate. V and (PE, PC, CWB) indexes gave the optimal cluster num- ber 6 and 3, respectively. According to the index values shown in Table 1, only FHV, I, and AIC indexes indicate that 4 is the cluster number estimate for the noise of ST up to 60%.
Dataset3 (simulated volumetric MR data): we tested the efficiency of the validity indexes for a T1-weighted MR data with 3% and 6% noise respectively. As shown in Table 1, PA, FHV, XB, I, AIC, SC, PCAES, and V indicate that 10 is an optimal cluster number for the original image set. For 3% noise, PD, FHV, I, DB, AIC, PCAES, PBMF, and V in-
dexes gave the optimal cluster. However only I, AIC, and PBMF gave optimal results for this dataset with 6% noise while PD, DB, PCAES, and V gave 9 clusters for this dataset. Others indexes achieved inconsistent results.
Overall, the AIC, FHV, and I indexes gave the optimal number of clusters 2, 4, and 10 for the three test datasets with low noise level which actually matches the structure of the images.

In the case of SKFCM algorithm
Dataset1 (synthetic1): the SKFCM algorithm applied to syn- thetic1 image corrupted by 9% and 12% salt and pepper noise. By optimizing the validity functions, most of the indexes indi- cate that 2 are optimal clusters for this dataset which actually matches the structure of the image except: FS and MPC for original image and FS, PE, PC, DB, VM, MPC, PCAES,
CWB and V for 9% noise. However, PE, PC, VM, MPC, PCAES and V considered that 5 be the optimal cluster number. For 12% noise dataset, only PD, FHV, PE, I, AIC, and PBMF gave the actual clusters as shown in synthetic1 of Table 1.
Dataset2 (synthetic2): the SKFCM algorithm is applied to synthetic2 image corrupted by Gaussian noise of ST 50%, 60% respectively. The estimated cluster numbers by the valid- ity indexes are shown in the synthetic2 of Table 1. Most of in- dexes indicate that 4 are optimal clusters for original dataset except: FS, PE, and MPC indexes. FHV, DB, AIC and I in- dexes gave the optimal cluster number 4 clusters for this data- set which actually matches the structure of the image. But PD, PA, XB, SC gave 2 clusters, PBMF gave three clusters, FS, PE, PC, VM, MPC, PCEAS, CWB, and V indexes indicate that 7 is the optimal cluster number. In the case of dataset with noise 60% of ST, only FHV, I, AIC, and PBMF gave the actual clus- ter number.
Dataset3 (simulated volumetric MR data): we tested the efficiency of the validity indexes for a simulated volumetric MR data (with 3% and 6% noise). As shown in Table 1, most of indexes gave optimal cluster number except PD, PC, XB, VM, SC, and CWB for original image. The FS, PE, PC, DB, AIC, I, MPC, PCAES, and V indexes indicate that 10 is the optimal cluster number for 3% noise dataset which actu- ally matches the structure of the image. The PD, PA, S, XB, and SC indexes considered that 2 is the optimal cluster num- ber. However, FHV and VM indexes considered 8 and 9 are the best cluster number. For 6% noise of dataset, only DB, AIC, I, PBMF, FHV indexes gave the actual cluster number. But PB, PCAES, and V indicate that 9 is best cluster number.

Different noise levels investigation

The performance of each index against noise is evaluated. Figs. 2–8 depict the relationship between the number of clusters and various levels of noise for all indexes when KFCM is applied


























Figure 2	The relationship between number of clusters and noise level for PBMF, CWB, MPC, PCAES and S indexes when the KFCM is applied to the synthetic1 image.

Determining the number of clusters for kernelized fuzzy C-means algorithms	49























Figure 3	The relationship between number of clusters and noise level for PC, PE, FS, FHV and SC indexes when the KFCM is applied to the synthetic1 image.
























Figure 4	The relationship between number of clusters and noise level for XB, PA, PD, VM and I indexes when the KFCM is applied to the synthetic1 image.




to the two synthetic images. It is clear that when MPC, XB, AIC and SC indexes are used, the optimal number of cluster is constant for different level of noise less than 12%, while for the CWB index this relationship is unstable. For FS, as the level noise increases, the obtained number of clusters in- creases. The FHV, I and PBMF indexes give inconsistent behavior on the synthetic2 and synthetic1, respectively.
Figs. 9–17 show the relationship between the number of clusters and noise level for all indexes when applying the SKFCM algorithm to the synthetic images. When MPC, S, PC, XB, PA, SC, V and AIC indexes are used, the obtained number of clusters is constant for various levels of noise,
whereas this relationship is unstable for CWB and PBMF in- dexes. The FHV, I indexes seem to be working better than the others for the KFCM algorithm. However, it has shown tendency to be affected by noise. On the other hand, for the SKFCM algorithm PBMF index seems to be working better than the others. Things look discouraging as no index has shown optimum performance throughout all noise levels (using fuzzy KFCM and SKFCM cases). It is important to note that some indexes, such as the AIC, I and FHV indexes have demonstrated better performance with the fuzzy KFCM and SKFCM algorithms rather than others. Overall, AIC, FHV, and I present good stable cluster number at various

50	E.A. Zanaty

























Figure 5	The relationship between number of clusters and noise level for AIC, DB and V indexes when the KFCM is applied to the synthetic1 image.































Figure 6	The relationship between number of clusters and noise level for PBMF, CWB, MPC, PCAES and S indexes when the KFCM is applied to the synthetic2 image.


levels of noise, especially AIC gives the optimal cluster number in all our tests in case of noisy and non noisy data sets.
On the other hand, in case of KFCM, one can read from Table 1 that although many indexes give accurate results on the 3D volume, only the AIC, FHV, and I yield correct or al- most correct results on different levels of noises.
Analogously, on applying KFCM algorithm to noisy syn- thetic images, the corresponding relationships between the found number of clusters and noise standard deviation have revealed that most indexes have constant outcomes for various levels of noise, whereas this relationship is unstable for CWB, PCAES, FS, and PC indexes.

Determining the number of clusters for kernelized fuzzy C-means algorithms	51































Figure 7	The relationship between number of clusters and noise level for PC, PE, FS, FHV and SC indexes when the KFCM is applied to the synthetic2 image.
































Figure 8	The relationship between number of clusters and noise level for XB, PA, PD, VM and I indexes when the KFCM is applied to the synthetic2 image.

52	E.A. Zanaty




























Figure 9	The relationship between number of clusters and noise level for AIC, DB and V indexes when the KFCM is applied to the synthetic2 image.


































Figure 10	The relationship between number of clusters and noise level for PBMF, CWB, MPC, PCAES and S index es when the SKFCM is applied to the synthetic1 image.

Determining the number of clusters for kernelized fuzzy C-means algorithms	53
































Figure 11	The relationship between number of clusters and noise level for PC, PE, FS, FHV and SC indexes when the SKFCM is applied to the synthetic1 image.































Figure 12	The relationship between number of clusters and noise level for XB, PA, PD, VM and I indexes when the SKFCM is applied to the synthetic1 image.




54	E.A. Zanaty
























Figure 13	The relationship between number of clusters and noise level for AIC, DB and V indexes when the SKFCM is applied to the synthetic1 image.


























Figure 14	The relationship between number of clusters and noise level for PBMF, CWB, MPC, PCAES and S indexes when the SKFCM is applied to the synthetic2 image.




Segmentation accuracy

In the previous section, we noted that I, FHV, and AIC indexes always give better results than others. Our tests are fo- cused on applying the standard FCM and most popular SKFCM such as: Ahmed et al. [2], Zhang et al. [6], and Kang et al. [7] with these indexes on T1-weighted MR phantom with nine slices thickness of 1 mm, 3% noise. We set the parameter r = 150 (GRBF kernel width),a = 0.5, m = 2, and NR = 0.5.
Table 2 shows the corresponding accuracy scores of the four methods: standard FCM and SKFCM of Ahmed et al. [2], Zhang et al. [6], and Kang et al. [7] for the nine classes. Obviously, the standard KFCM gives the worst segmentation accuracy in case I and FHV indexes, because I and FHV failed to determine the true clusters number of slice 8, while all methods with AIC data give satisfactory results. On the other hand, the SKFCM of Ahmed et al. [2] and Kang et al. [7] acquire the best segmentation performance in case of I and

Determining the number of clusters for kernelized fuzzy C-means algorithms	55























Figure 15	The relationship between number of clusters and noise level for PC, PE, FS, FHV and SC indexes when the SKFCM is applied to the synthetic2 image.




































Figure 16	The relationship between number of clusters and noise level for XB, PA, PD, VM and I indexes when the SKFCM is applied to the synthetic2 image.



FHV respectively. Overall, the SKFCM of Kang [7] with AIC
index is more stable and achieves much better performance
than the others in different classes even with misleading of true tissue of validity indexes.




56	E.A. Zanaty
































Figure 17	The relationship between number of clusters and noise level for AIC, DB and V indexes when the SKFCM is applied to the synthetic2 image.




Conclusion

In clustering, the role of a validity index is very important. The hope is that the number of clusters within an image can be determined automatically. The aim of this paper was to con- sider the performance of 18 of the most popular indexes by applying them in turn to a range of simulated and real data sets, including 2D and 3D data sets, corrupted with different types of noise of varying levels. To the best of our knowledge, no such comprehensive survey and comparison has been re- ported before in literature.
From the results of the experimentation, it is not possible to identify definitively an index which will work well in all cases, and hence be the most suitable as a general index for all cases. However some cluster validity indexes can guide the selection of the appropriate number of clusters existing in a dataset. We have arranged these indexes into three categories. In the first category: AIC, FHV, and I indexes appear to be good gen- eral indexes and have exhibited the best overall performance in all the experiments, outperforming all other indexes. The PBMF, MPC, XB, PD, and DB indexes have shown accept- able results, but have shown unstable performance on all test

Determining the number of clusters for kernelized fuzzy C-means algorithms	57


images under varying noise levels. The third category: V, S, SC, PC, PCAES, CWB, VM, FS, PA, and PE gave incorrect results in all test images.
Overall, on synthetic images, AIC, I and FHV indexes yield the true number of clusters, whereas the FHV index shows bet- ter performance in KFCM compared SKFCM. Furthermore, the tests strongly suggest that the AIC index should be consid- ered as the most robust index for general data and in particu- lar, for determining the correct number of clusters using KFCM and SKFCM for MR medical images. Moreover, our tests prove that one can confide AIC method for determin- ing the correct number of clusters using KFCM and SKFCM for MR medical images. The most recent SKFCM with AIC has obtained good segmentation performance (i.e. up to 90% in synthetic images and up to 80% for MRI images).
This initial investigation should be expanded to consider further testing on different real data sets from a wide range of applications including medical images and reverse engi- neered data. It would also be interesting to consider the effect of systematic and random error noise levels within the data sets to further establish the effect of this error on the performance of the indexes. Further tests should also be carried out on clus- tering improvement of the fuzzy methods using a wide range of validity indexes for automatic clustering algorithms, via the help of specialists within the fields of application.

References

Bezdek JC. Pattern recognition with fuzzy objective function algorithms. New York: Plenum Press; 1981.
Ahmed MN, Yamany SM, Mohamed N, Farag AA, Moriarty T. A modified fuzzy C-means algorithm for bias field estimation and segmentation of MRI data. IEEE Trans Med Imag 2002;21:193–9.
Maulik U, Bandyopadhyay S. Fuzzy partitioning using a real coded variable length genetic algorithm for pixel classification. IEEE Trans Geosci Remote Sens 2003;41(5):1075–81.
Wu S, Liew AWC, Yan H. Cluster analysis of gene expression data based on self-splitting and merging competitive learning. IEEE Trans Inform Technol Biomed 2004;8:5–15.
Zhu L, Chung FL, Wang S. Generalized fuzzy C-means clustering algorithm with improved fuzzy partitions. IEEE Trans Syst Man Cybernet: Part B 2009;39(3):578–91.
Zhang Dao-Qiang, Chen Song-Can. A novel kernelized fuzzy C-means algorithm with application in medical image segmenta- tion. Artif Intell Med 2004;32:37–50.
Kang Jiayin, Min Lequan, Luan Qingxian, Li Xiao, Liu Jinzhu. Novel modified fuzzy C-means algorithm with applications. Digital Signal Process 2009;19:309–19.
Kim DW, Lee KY, Lee D, Lee KH. A kernel-based subtractive clustering method. Pattern Recog Lett 2005;26(7):879–91.
Zanaty EA, Aljahdali Sultan, Debnath Narayan. A kernelized fuzzy C-means algorithm for automatic magnetic resonance image segmentation. J Comput Methods Sci Eng (JCMSE) 2009: 123–36.
Timm H, Borgelt C, Doring C, Kruse R. An extension to possibilistic fuzzy cluster analysis. Fuzzy Sets Syst 2004;147(1): 3–16.
Zhang JS, Leung YW. Improved possibilistic c-means clustering algorithms. IEEE Trans Fuzzy Syst 2004;12(2):209–17.
Ji Ze-Xuan, Sun Quan-Sen, Xia De-Shen. A modified possibilistic fuzzy c-means clustering algorithm for biasfield estimation and segmentation of brain MR image. Comput Med Imaging Graph- ics 2011;35(5):383–97.
Yuhua G, Lawrence OH. Kernel based fuzzy ant clustering with partition validity. In: IEEE international conference on fuzzy

systems. Vancouver (BC, Canada): Sheraton Vancouver Wall Centre Hotel; 2006. p. 16–21.
Wang W, Zhang Y. On fuzzy cluster validity indices. Fuzzy Sets Syst 2007;158:2095–117.
Zhang Yunjie, Wang Weina, Zhang Xiaona, Lic Yi. A cluster validity index for fuzzy clustering. Inform Sci 2008;178:1205–18.
Zanaty EA. A new modified fuzzy c-means for segmenting magnetic resonance images (MRIS). IJICIS J 2011;11(2):209–22.
Jegatha Deborah L, Baskaran R, Kannan A. Survey on internal validity measure for cluster validation. Int J Comput Sci Eng Survey (IJCSES) 2010;1(2).
Alp Erilli N, Yolcu Ufuk, Eg˘riog˘lu Erol, Hakan Aladag C¸, O¨ner
Yu¨ksel. Determining the most proper number of cluster in fuzzy clustering by using artificial neural networks. Expert Syst Appl 2011;38:2248–52.
El-Melegy MT, Zanaty EA, Abd-Elhafiez Walaa M, Farag Aly. On cluster validity indexes in fuzzy and hard clustering algorithms for image segmentation. In: IEEE international conference on computer vision, vol. 6; 2007. p. VI 5–8.
Xu Y, Richard G, Brereton A. A comparative study of cluster validation indices applied to genotyping data. Chemomet Intell Lab Syst 2005;78:30–40.
Pakhira MK, Bandyopadhyay S, Maulic U. Validity index for crisp and fuzzy clusters. Pattern Recog 2004;37:487–501.
Nasser H, Sweilam AA, Tharwat NK, Moniem Abdel. Support vector machine for diagnosis cancer disease: a comparative study. Egypt Inform J 2011;11(2):81–92.
Girolami M. Mercer kernel-based clustering in feature space. IEEE Trans Neural Networks 2002;13(3):780–4.
Zhang DQ, Chen SC. Fuzzy clustering using kernel methods. In: Proceedings of the international conference on control and automation, Xiamen, China, June 2002.
Zanaty EA, Afifi Ashraf. Support vector machine with universal kernels. Int J Appl Artif Intell 2011;2(5).
Zhang D-Q, Chen S-C. Clustering incomplete data using kernel- based fuzzy C-means algorithm. Neural Process Lett 2003;18(3):155–62.
Bezdeck JC, Ehrlich R, Full W. FCM: fuzzy c-means algorithm. Comput Geosci 1984.
Dave RN. Validating fuzzy partition obtained through c-shells clustering. Pattern Recog Lett 1996;17:613–23.
Xie XL, Beni G. A validity measure for fuzzy clustering. IEEE Trans Pattern Anal Mach Intell 1991;13:841–7.
Carl GL. A fuzzy clustering and fuzzy merging algorithm. Technical report CS-UNR-101; 1999.
Maulik U, Bandyopadhyay S. Performance evaluation of some clustering algorithms and validity indices. IEEE Trans Pattern Anal Mach Intell 2002;24(12).
Davies DL, Bouldin DW. A cluster separation measure. IEEE Trans Pattern Anal Mach Intell 1979;1:224–7.
Liao T, Compsc B. Image segmentation-hybrid method combin- ing clustering and region merging. Thesis. Monash University; 2003.
Fukuyama Y, Sugeno M. A new method of choosing the number of clusters for the fuzzy C-means method. In: Proceeding of fifth fuzzy system symposium; 1989. p. 247–50.
Gath Geva AB. Unsupervised optimal fuzzy clustering. IEEE Trans Pattern Anal Mach Intell 1989;11:773–81.
Zahid N, Limouri M, Essaid A. A new cluster-validity for fuzzy clustering. Pattern Recog 1999;32:1089–97.
Wu KL, Yang MS. A cluster validity index for fuzzy clustering. Pattern Recog Lett 2005;26:1275–91.
Malay KP, Sanghamitra B, Ujjwal M. A study of some fuzzy cluster validity indices, genetic clustering and application to pixel classification. Fuzzy Sets Syst 2005;155:191–214.
Yun X, Brereton GRG. A comparative study of cluster validation indices applied to genotyping data. Chemomet Intell Lab Syst 2005;78:30–40.

58	E.A. Zanaty


Andre T, Antonini M, Barlaud M, Gray RM. Entropy-based distortion measure and bit allocation for wavelet image compression. IEEE Trans Image Process 2007;16(12):3058–64.
Saha S, Bandyopadhyay S. Application of a new symmetry-based cluster validity index for satellite image segmentation. IEEE Trans Pattern Anal Mach Intell 2008;5(2):166–70.
Mezer Aviv, Yovel Yossi, Pasternak Ofer, Gorfine Tali, Assaf Yaniv. Cluster analysis of resting-state fMRI time series. Neuro- Image 2009;45:1117–25.
BrainWeb. Simulated brain database. McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University.
<http://www.bic.mni.mcgill.ca/brainweb>.
Masulli F, Schenone A. A fuzzy clustering based segmentation system as support to diagnosis in medical imaging. Artif Intell Med 1999;16(2):29–47.
