
FULL-LENGTH ARTICLE
Grounded theory and action research as pillars for interpretive information systems research: A comparative study

Manal A. Abdel-Fattah

Faculty of Computers and Information, IS Department, Helwan University, Egypt

Received 6 April 2015; revised 7 July 2015; accepted 8 July 2015
Available online 6 September 2015

Abstract In the literature survey, there is evidence ‘‘why an interpretive paradigm is more suitable for evaluating e-government systems”. However, more than one method can be used when applying interpretive paradigm for evaluating information systems (as we do not consider e-government sys- tems as exception) such as Action Research (AR) and Grounded Theory (GT). In this regard, two problems will arise: First, there is no explicit method that clarifies how AR and GT methods can be used for evaluating information systems. The second problem is to determine which method of them will be more appropriate for evaluating information systems.
Accordingly, two frameworks for evaluating e-government systems have been proposed, namely ‘Grounded Evaluation Framework’ (GEF) and ‘Action Research Evaluation Framework’ (AREF), which are based on Grounded Theory (GT) and Action Research (AR) methods respectively, to give an example how GT and AR methods can be used in evaluating information systems. The suggested GEF and AREF have been applied to the ‘‘University Enrolment Service” in Egyptian e-government, and the findings have been analyzed to conclude that GEF is more appropriate for evaluating e-government systems.
© 2015 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).


Introduction





E-mail address: Manal_8@hotmail.com
Peer review under responsibility of Faculty of Computers and Information, Cairo University.


http://dx.doi.org/10.1016/j.eij.2015.07.002
There has been growing criticism on the quality of most of the researches published regarding evaluating information systems in the last ten years, such as the lack of using theory to build frameworks, as the existing evaluation frameworks neither explicitly state the methodologies nor sufficiently justify the paradigms selected as a base [1–3]. Hence, the choice of research methodology and paradigm should be justified by the choice of philosophical assumptions (ontological, epistemological, and methodological) underlying a particular

1110-8665 © 2015 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

310	M.A. Abdel-Fattah



research [4–6]. Four basic research paradigms have been served as a framework for thinking about assessment and evaluation, which are positivist, post-positivist, critical, and interpretive paradigms. A comparison between these paradigms is shown in Appendix B, Table B1.
It has been stated by Wiredu [7] that the nature of problems in e-government systems and subjective understanding of their reality require qualitative methods. Based on Walsham [8,9] and [10], the qualitative direction is fully compatible with the interpretive information systems (IS) research. Therefore, what is needed is an accent on evaluation that is concerned with the process of devising questions, and interpreting the answers that give results in a systematic way. In addition, it has been discussed by Abdel-Fattah and Galal-Edeen [11] ‘‘why an interpretive paradigm is more suitable for evaluating e-government systems?”, and the research clarified that the interpretive paradigm will not be suitable for all e-government environments, which means that the interpretive paradigm is likely to be more suitable when the objectives of e-governments are providing intangible benefits and achieving improvement in social and public service.
The paper is organized as follows: Section 2 reviews the existing literature on the recent studies that have been centered on the context of interpretive research. The two proposed frameworks (i.e., Grounded Evaluation framework and Action Research Evaluation framework) will be presented in Section 3. The following section presents applying the proposed frame- works to the ‘‘University Enrolment Service”. Analysis of the research results and discussion will be offered in Section 5. The conclusion will be outlined in the final section.
Interpretive research background

Interpretive IS research has been increasingly used since 1993– 2000, accounting to 12–17%. Mingers (2003) cited in [9], as interpretivism encourages researchers to be more interpretive and inductive, rather than seeking to test and validate hypotheses, also, taking into account different perspectives of participants.
The interpretive research can be applied by more than one method, such as Case Study (CS), Research Development
(RD), Ethnographic Research (ER), Action Research (AR), and Grounded Theory (GT) [12,13]. Each of them has its key features and processes when applied. There is a similarity between Action Research and Grounded Theory, as each of them main thrust is to develop a theory regarding social phe- nomena, and they have the process of devising questions, and interpreting the answers in an iterative way [14–17]. Also, merging GT and AR has been suggested by [18–20] due to the similarity of the cyclic nature of both of GT and AR. There- fore, it seems that AR and GT fulfill the features required for applying interpretive research in evaluating e-government systems. However, the question is which of them is more suit- able for evaluating e-government systems? The research endeavors to answer this question.
Grounded theory

It has been mentioned by [21] that the prime emphasis of a researcher, who attempts to build a Grounded Theory, is the systematic collection, coding and validation (in an iterative fashion) of data that may help to describe a phenomenon of interest, and the most important recommendation of Grounded Theory is to ‘‘let the data talk”, but at the same time to formalize such ‘‘talk”, and to use such formalization to guide further data collection and analysis. Also, the impor- tance of the GT method is that it provides a sense of vision, and the techniques and procedures used provide the means for bringing that vision into reality [14]. According to [12], Fig. 1 graphically illustrates the processes and concepts of Grounded Theory. Grounded Theory method was applied for the interviews and analysis in the e-government field by [22,23].
The justification for using Grounded Theory in evaluating e-government systems is based on the principle that GT pro- vides a set of procedures for coding and analyzing data which suits the interpretive approach since it would keep the analysis close to the data and provide inductive discoveries about the phenomena under study [14]. In brief, the methodology of Grounded Theory is iterative, requiring a steady movement between concept and data, as well as comparative, requiring a constant comparison across different types of evidence to




Context: data in substantive area


Discovery
Systematic collection and refinement


Saturation




	






Figure 1	Model of grounded theory ‘Adopted from Villiers [12]’.

Grounded theory and action research	311
control the conceptual level and the scope of the emerging theory.
Action research


A simple definition for Action Research (AR) is provided by
[24] that Action Research has two constructs, which are ‘ac- tion’ and ‘research’ and the links between them. It is quite possible to take action without research or to do research without taking action. It should be borne in mind that the unique combination of the two constructs is what distin- guishes action research from other forms of enquiry. AR was born to solve practical problems, and worked to bridge the gap between theory and practice [15]. Also, another fac- tor that differentiates it from common problem- solving research is that AR engages the participants in the research’s activities.
There are four types of Action Research, namely, Tradi- tional Action Research, Contextual Action Research (Action Learning), Radical Action Research, and Educational Action Research [25]. This research focuses on ‘‘Traditional Action Research”, as it is applied to the area of Socio-technical sys- tems (e.g., Information systems, e-government systems) [9,26]. Based on our literature survey, Action Research could be described as follows:
Subjective: It is based on a subjective epistemology, which regards reality as a subjective or a social construc- tion, as there are no real structures in social world, and reality is a product of the mind or a product of individ- ual consciousnesses.
Formative: It aims to improve an ongoing system; there- fore, problems can be identified as they emerge and the next cycle can be improved as the system is being developed.
Qualitative: It operates more via verbal aspects rather than by numbers [12].
Interpretive: It investigates issues not easily measured in an empirical manner [27].
Collaborative: It empowers all individuals concerned with the intention of improving the practices carried out, where stakeholders are full participants in the research process [12,28].
Responsive: It reacts and adapts flexibly to the findings from each previous cycle [12].
Reflective: It advances through cycles, ‘starting’ with reflection on action, and proceeding to a new action which is then further researched. In addition, outcomes from each cycle are used in designing subsequent steps and events.
Experiential model of inquiry: All individuals involved in the study are known and contributing participants [15].
Cyclic in nature: It cannot be conducted on a once-off basis, but rather is a continuous process. In other words, action research is an emergent process with a dual cycle, an action cycle integrated with a research cycle [15]. Each cycle has four steps (plan, act, observe, and reflect) as depicted in Fig. 2, which is adopted from [29].
Theory developer: It generates ideas and perceptions to be tested and validated for more than one cycle, and then it ends by developing a theory.
















Figure 2	Action research cycle.
Open-ended research: It starts with a concept, percep- tion, or idea that has been developed, rather than start- ing with a fixed hypothesis [15].
Informal: There is no one correct way; the researcher must decide what is right for his/her, and develop his/ her own views.
Form of self-evaluation: It is the developmental process of following the perception or idea, seeing how it is progressing, and constantly checking its develop- ment [15].
To sum up, AR seeks to produce practical solutions to real problems, and expand scientific knowledge, especially when the situations are too ambiguous to conduct a precise research question [12,25]. Since AR is a participative research, its focus is on turning the people involved, into researchers who are more willing to apply what they have learned when they try it themselves [25], which enhances the competencies of all participants, unlike other research methods which keep the observer and the participants iso- lated [12,16]. It offers a guide model to monitor and improve the progress of a research project, through iterative stages. Finally, there are opportunities for a theory to emerge rather than always following a previously formulated theory [16], given that most approaches have a lack of test- ing and developing a theory. All these factors have given a justification why AR is chosen for evaluating e-government systems.
Proposed grounded evaluation & action research evaluation frameworks

In this section, we endeavor to postulate two frameworks for evaluating e-government systems, namely,
Grounded Evaluation Framework (GEF) and Action Research Evaluation Framework (AREF).



Grounded evaluation framework

Our proposed Grounded Evaluation Framework (GEF) for evaluating e-government systems is divided into three phases as outlined in Fig. 3.

Phase I (planning)
This phase focuses on two steps:
Service surveying step, a quick overview for the service (which will be evaluated) should be made through the e-government portal, to determine the characteristics of this service, and whether it is mandatory service or not, etc. Hence, we can initially postulate some questions that should be investigated during data collection phase.
Stakeholder identification step, this step, identifies stake- holders; their perspectives or points of view, their positions,
i.e. the participants for evaluating the service should be determined during this step.

Phase II (data collection)
This phase focuses on two methods of data collection, which are documentation and interviews. The purpose of using a combination of data collection methods is to minimize weak- nesses inherent in a single method [30] and increase the robust- ness of results [31]. Data collection phase will be divided into three steps:
Provider interview (background) step, this step aims to determine the providers adopted view of evaluation, the rel- ative position of the provider to the organization. This interview focuses on background or history about the ser- vice that will be evaluated without getting into further details.


Phase I: planning	Phase II: Data Collection
Figure 3	The proposed grounded evaluation framework (adopted from [47]).



Service studying step, this step examines the documents for the selected service, which are provided from the service provider. These documents should define the strategic objectives, vision, mission, and the benefits of a service/sys- tem, and any other available documents.
Provider interview (details) step, this step focuses on the details of the participants’ present experience in the topic area of study, problems such as incomplete, missing, unus- able data should be identified and resolved during this phase, rather than after its completion [32]. Through this step we will use the evaluation criteria according to the pro- vider’s perspective, which are presented in [33], as a guide to make this interview. For each case (e-government service) these criteria will be customized according to the nature of this case.
Phase III (analysis of data and interpretation)
This phase encompasses a set of steps:
The description step, the researcher describes the facts which are socially shared realities agreed upon by all participants [34]. The background and the details interviews of the pro- vider should be read several times, then divided into para- graphs, and then each paragraph is given a code. This step will end by writing up the provider interview transcripts.
Customizing evaluation criteria step, depending on the inter- view transcripts given by the provider in the previous step, and the study of the service which is provided in the second step of the data collection phase, we can customize the eval- uation criteria for e-government website according to the user’s perspective, which are presented in [33]. The cus- tomized evaluation criteria will be used for interviewing the user.
Cascading participants step, the researcher determines the way participants ascribe meaning to their separate realities by how they perceive cause and effect [34]. After applying this step on the users, we will repeat the same step done with the provider, to reach the user transcript interview.
Perform open coding step, this step applies Grounded The- ory (mainly open coding and axial coding). The researcher identifies categories (also referred to as concepts) [14] based upon patterns and ideas that have been obtained from the interviewees’ transcripts (service provider and users). As s/he reads looking for primary concepts that are repeated and stand out, some of these concepts are simply words or phrases used by the interviewees. As more data are col- lected the open coding is continued and categories begin to emerge. Categories are then compared and integrated and more abstract categories are formed leading to a hierarchy of categories [35]. Open coding serves two main functions, which are, to reduce large amounts of data, and to help the researcher to build cognitive map to understand what is happening in each case.
While [20] divided preliminary descriptive coding into five categories, which are environmental, organizational, indi- vidual, object and supplier, the perform open coding step is applied twice to obtain the coded concepts for the user and the coded concepts for the provider.
The evaluation step, the researcher identifies themes (or invariants) that emerge from the research and these are then used to develop common interpretations [34]. The coded
concepts (produced from step four) are used at this step for evaluating the services, but it should be emphasized that the coded concepts used in this evaluation will differ from a service to another. Moreover, coded concepts for one ser- vice differ from the service provider’s perspective to the user’s perspective.

Action research evaluation framework

Our proposed Action Research Evaluation framework (AREF) for evaluating e-government systems is divided into two cycles. Each cycle encompasses five phases as outlined in Fig. 4. These phases are as follows:
Phase I: Identifying initial idea for the situation/case study
This step identifies the needs and requirements of the case study, which in turn triggers the need for a change and series of actions to be designed, implemented and evaluated [36].
Phase II: Planning action/intervention
This step reviews objectives, designs the process to achieve the identified objectives, and participants should be agreed on. Also, tasks/actions, form task groups, and schedule/timeline should be determined. In addition, pilot survey (questionnaire) may be considered [37]. Hence, evaluation items can be listed. Furthermore, it should be taken into account that the plan should be flexible enough to adapt to any unforeseen effects and unrecognized constraints [38].
Phase III: Implementing action/intervention
This step implements the plan. Implementation activities could be a sort of discussion for the material provided in greater depth in small groups [25], participant observations, interviewing, process consultation and task Group reports [15,36].

Phase IV: Analyzing and evaluating on action
This step, initially, analyzes the action taken, and then evalu- ates the results. Observations, formative and summative eval- uations could be used to investigate the effectiveness of the workshops [15].

Phase V: Reflecting on action
This steps provides a feedback on the evaluation phase and outcomes, and makes decisions for the direction of the next cycle of Action Research Evaluation Framework. The results of this reflection phase are then used to plan the action of the next iteration (Baskerville and Wood-Harper, 1996), cited by [36].
Applying the proposed frameworks to the case study

Case study background

The University Enrolment Service (UES) is one of the services that is provided by the Egyptian e-government portal www. Egypt.gov.eg. The UES background could be divided into

Identifying initial idea for the situation

Feedback





Reflecting on action
Planning action

Reflecting on action




Re-planning action


Analyzing & evaluating

Analyzing & evaluating on action
Implementing action
on action

Implementing action



Cycle I	Cycle II
Figure 4	The proposed action research framework (synthesized by the author).


two stages. The first stage is before initiating the service, and the second stage is after initiating it:
First: The situation before initiating the UES service: Usu- ally, each year, following the announcement of the Egyptian General Secondary Certificate results, students flooded the university enrolment offices to buy the paper application forms (costing EGP40.00 for each form). This application process took place in 19 offices distributed all over Egypt in order to try to cover all regions in Egypt. Students are asked to fill the admissions application by listing their choices of their desired discipline and university in a descending order of pref- erence. The forms were filled manually by affixing a stamp for each choice, the total number of choices that each student had to fill being 48. The applications are then submitted back to the university enrolment offices.
All student applications ultimately end up at a center in which all data from each and every student application form are entered by seasonal data entry personnel, into the back- end legacy system where the matching process is carried out. Once the enrolment phase results were declared, students are notified of their results by post mail.
Second: The situation and benefits after initiating the UES service: The UES taken was replacing the traditional paper process by a comprehensive web-based application that accepts student university enrolment applications, and is sup- ported by a 24/7 call center hotline. Students were able to access the application with their student IDs and a special PIN code that they received along with their secondary school certificates. The online application was offered entirely free of charge.
The application provides the students with guidelines, rules and interactive online help together with the ability to update personal data. All other incentives are added automatically to the student’s grades. Also, students now have the chance to alter their choices after submitting them, as long as it is in the timeframe. In addition, huge savings resulted as well for the operational costs of the government, such as savings in paper forms, and seasonal staffing of university enrolment
offices. Enrolment results are communicated to the students not only through mail but also, through SMS and the web application.
Applying grounded evaluation framework

Phase I (planning)
Phase I: Service surveying step. The UES is a season- able service. Hence, it is not available in the Egyptian e- government portal all the year. Also, to examine this service student ID and password should be provided to log in.
Phase I: Stakeholder identification step. The project manager of the UES project is determined as the service provi- der. The users for this service could be all Universities’ stu- dents as it is a mandatory service, however, the students of Faculty of Computers & Information are chosen to be the users of this service.
Phase II (data collection)
Phase II: Provider interview (background) step. The project manager of the UES was contacted and interviewed. The interview was open-ended and conducted at his work- place. It took approximately one hour and was recorded man- ually. It was designed to reveal the project manager’s perception of the UES, the following questions were asked: What was the motivation to develop the UES? what were the strategies used to implement the UES? and what were the ben- efits resulting from the UES?.
Phase II: Service studying step. Documentation of the UES has revealed the information that is presented in section (4.1, case study background), which are provided by the Min- istry of State for Administrative Development (MSAD).
Phase II: Provider interview (details) step. The second interview was conducted; it took approximately one and half an hour and was recorded manually. In order to prevent the



interviewee and the interviewer from digressing into a trivial conversation, a semi-structured interview protocol was used as a reference for the interviewer. However, the actual inter- view process was managed as an open-ended interview to serve and reveal unintended latent constructs.
The protocol questions were largely grouped into five sets, and the evaluation criteria according to the provider’s perspec- tive, provided in [33] were used as a guide for this interview. However, some modifications were made as shown in Appen- dix A Table A1.
Phase III (analysis of data and interpretation)
Phase III: The description step. The background and the detail provider interviews were then read three times. Some information from the documentation in the background inter- view to get further details through some questions was merged too. The interviews were divided up into paragraphs, and then each paragraph was coded. By applying this step, a transcript consisting of 46 paragraphs in 5 pages was coded.
Phase III: Customizing evaluation criteria step. This step started by determining the criteria that should drive the customization, after reading the previous transcript (provided in the description step) and studying the UES and its docu- mentation, it was noticed that some features should be merged, deleted or new ones added in the evaluation criteria, provided in [33]. Some examples can be given as shown below:
The provider also mentioned that the UES was available earlier than its actual time to offer the students a chance to test the service. Hence, an additional feature was added regarding this test.
The provider mentioned that the students were offered the technology clubs and faculties laboratories on the UES as multiple-channels for using the service, so we added a new feature asking the students whether the service was really provided through multiple channels or not.
The service should be available 24 h a day, seven days a week (24/7), and the technical support on using this service was also added.
It was mentioned that the objectives of this service is to allow the students to alter their choices more than once without any fees. Thus, we added a new feature to evaluate whether this objective was achieved or not.
To sum up, if we need to add a new feature to the evaluation criteria, it should be explicitly mentioned by the provider or in the documentation (not deduced). In case of deleting or merging any features, there should be a reason- able reason. Table A2 in Appendix A describes the evaluation criteria according to the user’s perspective after customization.
Phase III: Cascading participants step. In this step, Table A2 was used to interview the users (students). Initially, twenty students from the first grade of Faculty of Computers and Information (Cairo University) were interviewed. On interviewing those students, it was found that most of their opinions were similar in most of the evaluation criteria. Con- sequently, we did not need to add more users. As we did in the provider interview, the user interviews were divided up into paragraphs, and then each paragraph was coded. By applying this step, we continued from the paragraph 46 to reach the paragraph 466 and from page 5 to reach page 45 (these para- graphs/pages will not be presented in this research), only an example for these paragraphs will be presented, as shown in Fig. 5.
Phase III: Perform open coding step. This step was applied twice. The first time was performed for the provider interview transcript and the second time was for the users’ interviews transcript. These transcribed interviews were manu- ally analyzed to identify the underlying concepts.
First for the provider interview transcript:
By reading the provider interview transcript, initially, the concept, its property and the value of each coded para- graph were conducted. Each of the concept, property and value were highlighted by using yellow, blue and green respectively as shown in Fig. 5.
A table was created for each concept, which encompasses three columns. The first column is for the property. The sec- ond is for the dimensional range or scale for the values, and if it is ‘‘nominal” which means that the value is not deter- mined or ‘‘ordinal” which means the range of the value could be determined. The third column is for the value. ‘‘ CRM ” denotes the concept name of the table. Table 1 provides a ‘‘concept” example, which is one of the ‘‘con- cepts” of Fig. 5.
We can notice that Fig. 5 has more than one concept, which is useful to find the relationship between these concepts. The same steps were applied for the entire provider inter- view transcript coded paragraphs; at the end we obtained the 17 ‘‘concept” tables. After reading these tables care- fully, we found that three concepts (problems, students, ser- vice) were redundant, so they were merged in the same table. Policy and performance concepts contained one property for each; hence we merged them in the service con- cept table. Again, after determining all concepts, we read the provider interview transcript carefully, and then we compared it with the determined concepts to ensure making all the appropriate modifications to reach the final 12 ‘‘con- cepts” tables, and then the provider’s ‘‘concepts” were counted to determine the frequency of each concept. For example, the frequency of ‘‘application concept” provided in Fig. 6 is 15.



Figure 5	Paragraph example from the provider interview transcript.

316	M.A. Abdel-Fattah

Table 1  The concept table example.
 CRM 

Property	Dimensional range
Value

Communication method
Time of communication
Ordinal	Phone, email
Ordinal	Morning & night & work hours & day off

Problem	Nominal	Network problem & technical
problem

Time for response
Ordinal	Immediately, later

Response method  Ordinal	Phone, email and SMS

The research assumes that the number of codes for each concept determines the weight of this concept in the evalu- ation (assumption). Thus, the concept which has a low fre- quency of occurrence e.g. 1 or 2 should not be considered as a main concept. Perform open coding step ends by finding the relationships between these concepts, as shown in Fig. 6 (details of Fig. 6 are offered in Table 3). As the main objec- tive for this step was to devise the providers’ coded con- cepts, before obtaining them, so all the concepts with similar meaning were merged in one concept and the con- cepts that don’t relate to the UES were deleted.
Second for the users’ interviews transcript:
The same steps which were followed in the provider inter- view transcript were applied to the users’ interviews transcript. The proposed coded concepts according to the user’s perspec- tive, are outlined in Fig. 7.

Phase III: The evaluation step. This step was applied twice, once for evaluating according to the provider’s perspec- tive and the other for evaluating according to the user’s perspective.
First for the provider’s perspective:
On examining the provider’s coded concepts, provided in Fig. 6, it was found that the student concept is not appropriate for evaluating the provider. Hence, it was deleted and exam- ined in the evaluation of the service according to the user’s per- spective. Table 2 depicts the weight (expressed as a percentage) for each concept according to the provider’s perspective.
The steps followed to calculate the total evaluation percent- age for UES can be summarized as follows:
Concept weight = (concept) frequency × 100)/R category frequency.
For example, ‘‘service” (Table 2)= (18 × 100)/68 = 26.47%.

Each item was evaluated according to range scale. This
scale encompasses three values: (1) less than 50% poor;
(2) 50% – less than 80% good; and (3) more than 80% very good as shown in Table 3. For items which contain sub items, we calculate the percentage of the available sub items to score the item.
of     sub     items)    ×    100. Score of the item = (number of available sub items/number





Figure 6	Coded concepts according to the provider’s perspective.

Grounded theory and action research	317

items)      ×     category       weight. 3. Category (relative weight) = (R score of items/number of
For example, ‘‘service” (Table 3) = (8/8) × 26.4 = 26.4%.
Total evaluation% = R category (relative weight)
For example, total evaluation percentage (Table 3) = 0.22
+ 0.10 + 0.13 + 0.264 + 0.06 + 0.04 + 0.074
+ 0.09 = 0.98.
Second for the user’s perspective:
Table 4 outlines the weight (expressed as a percentage) for each concept according to the user’s perspective.
Some notes should be considered for evaluation of the UES according to the user’s perspective:
Some interviewee (students) did not answer the questions in the required way. As an example, when we asked the stu-
dents whether the information presented in the service was useful, the student answered that he didn’t read this information, this answer cannot be taken into account when we evaluate ‘‘useful information” item. Consequently, only the number of students who answered this question was considered in the evaluation as shown in Table 5.
The ‘‘number of approving users” column in Table 5 denotes the number of users (the number of students who agreed on
the evaluation item). As an example, for ‘‘service availabil- ity” 13 students only agreed that service was available all the time. The ‘‘percentage” column in Table 5 calculates the percentage of the ‘‘number of approving users” column.
The ‘‘total” column in Table 5 is a result for mathematical calculations in which the total percentage of the items was
divided on the number of the items for each category. As an example, the total percentage for the ‘‘service category” was 13.39 and the number of ‘‘service category” was 15 items, then the percentage for evaluating the ‘‘service cate- gory” was (13.39/15 = 0.892). According to Table 4, the weight of ‘‘service category” was 73%, then, the total eval- uation  percentage  for  the  ‘‘service  category”  is
(0.892 × 0.73 = 0.65). We did the same calculations exactly to obtain the percentage for evaluating the ‘‘user items”
which was (0.21). Finally, we added 0.65 + 0.21 = 0.86 to obtain the total percentage evaluation for the UES according to the user’s perspective.

Applying action research evaluation framework

Two issues should be considered for evaluating the UES by using AREF:






Figure 7	Coded concepts according to the user’s perspective (UES).
For example, ‘‘application benefits” (Table 3) contains 5
was 5/5 × 100 = 100% which was evaluated as very good. sub items, all these sub items are available, hence, the score
The UES is a seasonable service, hence, the reflection/out- come of the first iteration will be considered for the plan action of the second iteration in the following year. Hence, two years were required to evaluate the UES.
As it was discussed, in Section 2.2, the Action Research Evaluation Framework is a cyclic framework by nature, and depends on the action of participants to resume the cycle. Thus, it will not be appropriate for evaluating the UES according to the provider’s perspective, and only eval- uating the UES according to the user’s perspective will be taken into account




First iteration
Phase I. Identifying initial idea for evaluating the UES: An open-ended interview was conducted to the project man- ager of the UES to determine the initial idea and requirements for evaluating the UES. The main requirement, for him, was testing the quality of the service, which means for the provider that he will not receive any problem by phone/email through CRM center.
Phase II. Planning action: 20 students from the labora- tory of the Faculty of Computers and Information were selected. They were not students in this Faculty; they only used the Faculty’s laboratory, as they were not able to access the Internet outside the Faculty without charge. The students were divided into two groups equally. An idea was given to all of them about the objective of the UES evaluation and the required task, aiming at encouraging the students for effective participation. The schedule for the workshop was planned as follows:
First session, the students work in the laboratory without any interference. The researcher only observes them, and then there is a break for 15 min.
Second session, questionnaire will be distributed among the students, and then group discussions will be held.
Phase III. Implementing action: All the students per- formed their tasks according to the plan. The first workshop was assigned to the first group. All the observations for using the UES application were recorded manually by the researcher, and then the students were asked to reflect their opinions using a questionnaire. Table A2 was customized to be used in this questionnaire, as follows:
Some items were deleted, as there is no meaning to test them, for example: ‘‘Service availability (24/7)” and ‘‘ser- vice cost” items were deleted.
Three-point rating scale was used to test the items. (1) Poor means ‘‘1” point; (2) Good means ‘‘2” points; and (3) Very good means ‘‘3” points, as shown in Table 6.
It was assumed that all items have the same weight, i.e. there is no item that has more weight than another item.
The items were organized under four categories (service items, information items, security items and user items) as in Table 5 so that we can compare between the results of Tables 5 and 6.
Based on the researcher observations, an item was added to Table 6, which is ‘‘using multi-Internet browser”, i.e., Explorer and Firefox.
Finally, this workshop was ended by group discussions about the recorded observations. As such the students validate the researcher’s observations. Also, they answered and gave justifications for the researcher’s queries. All these steps were repeated for the second group.

Phase IV. Analyzing and evaluating on action: On examining the observation reports for the two groups, the researcher found that:
Only one Internet browser was used by all students, which was Internet Explorer.
‘‘6” students took more than the determined time to register their desires.
‘‘4” students made few modifications after they regis- tered their desires.
‘‘4” students terminated the UES application before the determined time, and without doing any modifications.
‘‘2” students were confused and they asked for help.
All these observations acted as a key driver for ongoing dis- cussions among students, rather than a final conclusion report of facts, and the students gave answers/justifications for them as follows:
For ‘‘a”: The UES worked only on the Internet Explorer, so they used it.
‘‘b”: They did know that they can test the UES early before using it.
‘‘c”: They could not determine the faculties’ places and by using Google map they made
modifications for their desires.
‘‘d”: They tested the service before using it, and they knew exactly what they would do.
‘‘e”: They did not use the computer before.
For evaluating the questionnaire, we agreed that all the items weight are equal, and the total items were ‘‘16” items, as outlined in Table 6. Thus, we can calculate each category (relative weight) by this equation: Relative weight for cate- gory = R category items/number of all items.



Table 3  Evaluating UES according to the provider’s perspective.
(continued on next page)

Table 3  (continued)
Evaluation Items (features)	Evaluation method	%	Total percentages

Item Ava.
Range scale

8.Service upgrading	U	8.0
Relative weight (8 items)	8/8 = 1.0 × 0.264 = 26.4%
Benefit items
Government	◦	◦	⊙	1.0
Paper forms	U
Seasonal staﬃng	U
Entry personnel	U
Citizen	1.0
Saving application fees	U
Saving transportation & accommodation costs	U

Transparency (Separate provider from the public)
1.0
U	1.0

Saving (payment)	U	1.0
Use ICT	U	1.0
Relative weight (5 items)	5.0	5/5 = 1.0 × 0.06 = 6%
Security items
PKI	X	0.0
PIN code	U	1.0
Policy	◦	◦	⊙	1.0
Student	U
System	U
Relative weight (3 items)	2.0	(2/3) × 0.06 = 0.04
CRM items
Communication method	◦	◦	⊙	1.0
Call center	U
Emails	U
Reply method	◦	◦	⊙	1.0
Emails	U
Telephone	U
SMS	U
Time of communication (available any time)	◦	◦	⊙	1.0
Time for reply (high speed)	◦	◦	⊙	1.0
Relative weight (4 items)	4.0	4/4 = 1.0 × 0.074 = 7.4%
Service category (20 items)	0.264 + 0.06 + 0.04
+ 0.074 = 0.438 = 44%
Problem Items (solved)
Employee problem	U	1.0
Form mistakes	U	1.0
Limited time	U	1.0
Long line	U	1.0
Reduce student chance	U	1.0
Transfer	U	1.0
Travelling problem	U	1.0
Cost (burdens)	U	1.0
Desires changing Problem	U	1.0
Enrolment process	U	1.0
Documents	U	1.0
Relative weight (11 items)	11.0  11/11 = 1.0 × 0.09 = 9%
Total evaluation%	0.45 + 0.44 + 0.09 = 0.98 = 98%
U Item available.
⊙ The achieved grade.

For example, ‘‘service” (Table 6) = (4/16) = 0.25; ‘‘infor- mation” (Table 6) = (5/16) = 0.3125, and so on.
To calculate item score, the maximum score 100% = 60 points as the students were 20, and if all students gave ‘‘very good” i.e. ‘‘3 points” for the item, then 3 * 20 = 60 points.
For example, ‘‘information details” item, two students selected ‘‘poor”, which means 2 * 1 = 2; two students
selected ‘‘good”, which means 2 * 2 = 4; and 16 students selected ‘‘very good”, which means 16 * 3 = 48; then this
item is evaluated by 54/60, as shown in Table 6. It should be noted that, only the number of students who answered the question was considered in the evaluation, i.e., ‘‘Testing service early” item scoring from ‘‘12” as four students only answered it.

report for all the students’ problems on using the UES. A dis- cussion was made to clarify all the presented documents, and the provider promised to cover all these comments on the fol- lowing year.







The following equation was used to calculate the total cat- egory (relative weight).
Total category (relative weight)
= R score of items × category weight.
vice” = 0.78 × 0.25 = 0.195, as depicted in Table 6. For example, Total Category (relative weight) of ‘‘ser-
Finally, the total score for evaluating UES was calculated
by the equation below, which was 0.85:
Total evaluation% = R Total category (relative weight).
4.3.1.5. Phase V. Reflecting on action: The results of the eval-
uation were presented to the UES provider along with the
Second iteration
All the previous phases of the first iteration were performed in the following year by the new comers/students, and the UES evaluation result was (0.90), as shown in Table 7.
Discussion of findings
The results of evaluating UES by using GEF and AREF (according to the user’s perspective) were outlined in Table 8 and Fig. 8. Some considerations should be borne in mind:
Although Grounded Evaluation Framework used ‘‘20” items (Table 5) for evaluating the UES according to the user’s perspective (which were obtained by applying the coded concepts) and Action Research Evaluation Frame- work only used ‘‘16” items (Table 6), the final results of evaluation by using GEF, and the first iteration of AREF were similar (0.86 and 0.85 respectively).
On using the Grounded Evaluation Framework, 20 items were used (Table 5) for evaluation, and by comparing them with the 18 items of Table A2, the difference was very












Figure 8	A comparison of the evaluation results of the ‘‘GEF” & ‘‘AREF”.


limited, as only two items were added. The researcher thinks that the reason behind this is, the UES is a manda- tory service and the user does not have any alternatives except using it. However, if the GEF is applied to the ‘‘Birth Certificate Extract” BCE service, the result will differ as BCE is an optional service.
The final result of evaluating the UES by using the second iteration of Action Research Evaluation Framework was
0.90 (Table 8), which was different from the result of evalu- ating the UES by using Grounded Evaluation Framework. This may be justified by two reasons: (1) The second itera- tion was in the following year, i.e., the UES application was already updated, for example, the application was used by multi-Internet browser; and (2) If the GEF is applied to the UES for this year, the evaluation result will differ.
Four categories were used for evaluating the UES, which are ‘‘service”, ‘‘information”, ‘‘security” and ‘‘user” as depicted in Fig. 8, and the results for evaluating them were similar, except ‘‘service” category, and that is because the relative weight for this category by using GEF was 0.33 and it decreased to 0.25 by using AREF. Also, the evalu- ated items for the former were 8 items whereas 4 items were used for the later. In addition, a new item for evaluating ser- vice category, which was ‘‘using multi-Internet browser”, was added by AREF as a result of using observations.
General notes for applying GEF and AREF could be:
The result of evaluating the UES according to the provi- der’s perspective was 98% (Table 3), which was consistent with the evaluation results of international organizations,
e.g. the United Nations, the report of United Nations [39] ranked the Egyptian e-government services as the 23rd from 192 countries, as the evaluation of the United Nations depends on testing the website service stages (emerging, enhanced, interactive, transactional, connected), and the service which reaches a high stage such as transactional or connected, will be evaluated as a good service. This heeds that the service should not be only evaluated accord- ing to one perspective and that the user’s perspective is crit- ical and should be taken into account.
Although applying AREF is much easier than applying GEF, applying AREF takes much time than applying GEF, particularly, for e-government systems as it is difficult to update these systems before one year or at least several months.
The researcher believes that AREF has a great advantage for developing or improving information systems (case studies) taking into consideration that these systems should be updated rapidly.



Both GEF and AREF (interpretivist research frameworks) have the following advantages in common:
Investigating issues which are not easily measured in an empirical manner (traditional research), especially when the user plays a critical role for evaluating the systems.
Keeping the observer and the participant attached, and the participant is considered as a key driver for both frameworks
Processing of research can be refined gradually over time, because both of them have a cyclic nature.
Generating ideas and using them to be tested and vali- dated, and ends by developing a theory
Seeking to produce practical solutions to problems in real world settings.
Conclusion
There has been growing criticism of the quality of most of the researches published in the last ten years such as the lack of using theory in evaluating e-government systems. The research tried to avoid these shortcomings by proposing two frame- works using interpretive research, which are ‘‘Grounded Eval- uation Framework” and ‘‘Action Research Evaluation Framework”. Also, the aim of this research is comparing AR and GT to increase our insight into both, and giving an exam- ple how they can be used in evaluating information systems.
The research started by giving an introduction about research paradigms; and a comparison between the research paradigms has been presented. Background about the interpre- tive research is also outlined. The Grounded Theory and Action Research concepts have been discussed, and then the two frameworks for evaluating e-government systems have been proposed and applied to the UES, which are GEF and AREF respectively.
Analysis of the findings of applying the two frameworks to the UES has revealed that although Action Research Evalua- tion Framework has a greater advantage for developing or improving a case study than Grounded Evaluation Frame- work, However, Grounded Evaluation Framework is more appropriate than Action Research Evaluation Framework for evaluating e-government systems for the following reasons:
E-government systems by nature are stable and these types of systems could not be changed in a short time.
Applying AREF to e-government systems requires a long time.
AREF will not be able to evaluate the case study that requires evaluating the system according to more than one perspective.
GEF has a greater ability for formalizing, interpreting and analyzing data than AREF.
The cycle of GEF encompasses classification of data into categories, and postulation of networked relation- ships among these categories.

Appendix A

See Tables A1 and A2.




Table B1  A comparison between research paradigms.

Element	Post-positivist paradigm	Critical paradigm	Interpretivism paradigm

Purpose	● The primary goal is an explanation that
leads to prediction and control of phe-
nomena [30,40].
Positivist stresses ‘‘theory verification” (Lincoln & Guba, 2000, p. 107) cited
in [40].
The goal of research is etic in that it attempts to discover the ‘truth’ (Guba
and Lincoln, 1994) cited in [41].
Ontology	● There is one reality and it is observable
by an inquirer who has little if any
impact on the object being observed [4].
Positivists contend that there is one true reality that is apprehendable, identifi-
able, and measurable (a position known as naı¨ve realism) [40].
Positivism; is where a simple reality is
assumed governed by natural laws;
knowledge is context free and con- trolled by cause and effect laws (Guba & Lincoln 1994), cited in [41].
Epistemology  ● Positivists  emphasize  dualism  and
objectivism. That is, the researcher
and the research participant and topic are assumed to be independent of one another ‘‘Knower (research participant) and known (the researcher) are dual- ism”, and by following rigorous, stan- dard procedures, the participant and topic can be studied by the researcher without bias (objectivism) [30,40].
Methodology  ● It involves empirical analysis to test
hypotheses. The condition of the exper-
iment is controlled to prevent bias (Guba & Lincoln, 1994), cited in [12,41].
It acknowledges an objective reality that is only imperfectly apprehendable. This
position holds that human intellectual mechanisms are flawed and that life’s phenomena are basically intractable, and therefore, one can never fully cap- ture a ‘‘true” reality [40].
The postpositivist stresses ‘‘theory falsifi- cation” (Lincoln & Guba, 2000, p. 107)
cited in [40].
Postpositivists accept a true reality, but they believe it can only be apprehended
and measured imperfectly (a position known as critical realism)[40].
Postpositivism; is where imperfect ‘real- ity’ is assumed because of imperfect
human intelligence and the complex nat- ure of phenomena (Guba & Lincoln 1994), cited in [41].


Postpositivists advocate a modified dual- ism/objectivism. This position acknowl-
edges that the researcher may have some influence on that being researched, but objectivity and researcher–subject independence remain important guideli- nes for the research process.
[40].
It emphasizes falsifying hypothesis. Data that are collected about a situation allow
for the discovery of knowledge, view- points are solicited, without interactions, to interpret people’s actions (Guba & Lincoln, 1994), cited in [41].
The goal is the elimination of oppressive human relationships
(oppressive is defined in terms of forced assimilation) [42].
The goal is inciting transformation in the participants that leads to
group empowerment and emanci- pation from oppression [40].

Reality shaped by ethnic, cultural, gender, social, and political values
[40].
Over time this ‘reality’ is assumed to be ‘real’. (Guba & Lincoln
1994), cited in [41].
Reality is multiple and constructed.
Reality is interpreted, negotiated and consensual. Reality is inter-
preted through internal and exter- nal signs [46].
The relationship between researcher and participant is trans-
actional and subjective; the rela- tionship is also dialectic in nature [40].




Requires a dialectical methodol- ogy so that misconceptions are
transformed into an informed understanding of the research sub- ject (Guba & Lincoln 1994), cited in [41].
Obtaining an understanding of the sub- jectively created social world ‘‘as it is”
[43,44].
The goals of interpretivism are both idio- graphic and emic [40].
It aims to characterize how people expe- rience the world, the ways they interact
together, and the settings in which these interactions take place [45].
Reality consists of individuals mental constructions of the objects with which
they engage, and that the engagement impacts on the observer and the situa- tion being observed [4].
Reality is internal and multiple, is sub- jective and influenced by the context of
the situation, the individual’s experience and perceptions, the social environment, and the interaction between the individ- ual and the researcher [30,40].
Interpretivists advocate a transactional and subjectivist stance that maintains
that reality is socially constructed and, therefore, the dynamic interaction between researcher and participant is central to capturing and describing the ‘‘lived experience” [40].
Subjective point of view.
Knower and known are inseparable [46].
Aims to produce an understanding of the social context of the phenomenon and
the process [46], whereby the use of a hermeneutic (interpretive) dialectic (rep- resents) circle [5].






Appendix B
See Table B1.

References

Hiller JS, Belanger F. Privacy strategies for electronic govern- ment. Price Water House Coopers; 2001.
Alalwan Jaffar, Thomas Manoj A. A holistic framework to evaluate e-government systems. In: AMCIS 2011 proceedings – all submissions. Paper 67; 2011.
UN. United Nations E-Government survey 2014. E-government for the future we want. New York, Department of Economic and Social Affairs; 2014.
Roberts GMOB. Action research PhD. Agriculture and Rural Development. Richmond, NSW, Australia, The School of Agri- culture and Rural Development, University of Western Sydney, Hawkesbury; 1997.
Guba EG, Lincoln Y. Fourth generation evaluation. California (Sage): Newbury Park; 1989.
Burrell G, Morgan G. Sociological paradigms and organizational analysis. New Hampshire: Heinemann Educational Books; 1979.
Wiredu G. Challenges in developing an incipient information systems strategy for local e-government: an empirical study of a British local authority. In: 9th European Conference on e- Government. University of Westminster, London, UK, 29-30 June 2009. Academic Publishing Limited; 2009.
Walsham G. Interpretive case studies in IS research: nature and method. Euro J Inform Syst 1995;4:74–82.
Walsham G. Doing interpretive research. Euro J Inform Syst 2006;15:320–30.
Goldkuhl G. Pragmatism vs interpretivism in qualitative infor- mation systems research. Euro J Inform Syst 2012;2(21):135–46.
M. A. Abdel-Fattah and G. H. Galal-Edeen (2009). Why an interpretive paradigm is needed for evaluating e-government systems? 9th European Conference on e-Government, University of Westminster, London, UK. 29–30 June 2009, Academic Publishing Limited, UK: 1–10.
Villiers MRD. Three approaches as pillars for interpretive information systems research: development research, action research and grounded theory. In: ACM international conference proceeding series; vol. 150 South African Institute for Computer Scientists and Information Technologists; 2005.
Dı´az Andrade A. Interpretive research aiming at theory building: adopting and adapting the case study design. Qual Rep 2009;14 ():42–60.
Strauss A, Corbin J. Basics of qualitative research: techniques and procedures for developing grounded theory. London: SAGE; 1999.
Vaccarino F, Comrie M, Culligan N, Sligo F. Action research initiatives: the Wanganui adult literacy and employment pro- gramme. Series: adult literacy and employment in Wanganui ISSN 1176-9807, Massey University Department of Communica- tion and Journalism, Wellington and Palmerston North, New Zealand; 2006.
Koshy V. Action research for improving practice a practical guide. Paul Chapman Publishing, SAGE; 2005, ISBN 1-4129-0755-1.
Ng K, Hase S. Grounded suggestions for doing a grounded theory business research. Electr J Bus Res Meth 2008;6(2):155–70.
Walters D. Grounded practice: merging grounded theory principles and action research to secure authentic school improvement. Exeter University Graduate School of Education, UK. 2012. <http:// www.thinkingschoolsinternational.com/site/wp-content/uploads/ 2012/02/Grounded-practice-Final.pdf> [Accessed 06.07.15].
Butterfield J. Using grounded theory and action research to raise attainment in, and enjoyment of, reading. Educ Psychol Pract 2009;25(4):315–26.
Dick B. AR and grounded theory, what can action researchers learn from grounded theorists. In: Research symposium at the Australian and New Zealand ALARPM/SCIAR conference, Gold Coast, 4–5 May; 2003. <http://www.aral.com.au/DLitt/DLitt_ P60andgt.pdf>.
Galal-Edeen GH. Information systems requirements engineering: an interpretive approach. Egypt Inform J 2005;6:154–74.
Owen BB, Cooke L, Matthews G. UK government policy on citizens’ access to public information. In: 9th European conference on e-Government. University of Westminster, London, UK, 29– 30 June 2009. UK: Academic Publishing Limited; 2009.
Bernardi R. IT enactment of new public management in Africa: the case study of health information systems in Kenya. In: 9th European conference on e-Government. University of Westmin- ster, London, UK, 29–30 June 2009. UK: Academic Publishing Limited; 2009.
Hall W, Keynes M. Action research: a guide for associate lecturers. The Open University SUP887593; 2005.
O’Brien R. An Overview of the Methodological Approach of Action Research]. In: Richardson Roberto, editor. Theory and practice of action research, Brazil: University of Toronto. (English version); 2001. <http://web.net/~robrien/papers/arfinal.html#_
Toc26184662 [accessed 01.09.14].
Baskerviller, Myers. Special issue on action research in informa- tion systems: making IS research relevant to practice-foreword. MIS Quart 2004;28(3):329–35.
Fowler Danielle C, Swatman Paul A. Building information systems development methods: synthesising from a basis in both theory and practice. In: Proceedings of software engineering conference, 9–13 Nov 1998. Adelaide (SA): IEEE; 1998. p. 110–7.
Cohen L, Manion L, Morrison K. Research methods in educa- tion. 5th ed. London (USA) and Canada): Taylor & Francis e- Library; 2005.
Kemmis S, McTaggart R. Strategies of qualitative inquiry – chapter 10, Participatory action research. In: Denzin NK, Lincoln YS, editors. Handbook of qualitative research; 2007. p. 271–330.
Woods M, Trexler CJ. Expanding the agricultural education research toolbox: a case for an interpretive perspective. In: Proceedings of the 27th annual national agricultural education research conference; 2000.
Rowlands B. Employing interpretive research to build theory of information systems practice. AJIS 2003;10.
Mittman BS, Qualitative methods and rigorous management research: (how) are they compatible? White paper prepared for the Department of Veterans Affairs Management Research in VA Workshop. Sponsored by the HSR&D Management Decision and Research Center; 2001.
Abdel-Fattah MA, Galal-Edeen GH. Towards flexible evaluation for e-government websites quality: a multi-perspective evaluation framework. In: 8th European conference on e-Government. Ecole Polytechnique, Lausanne, Switzerland, 10–11 July 2008. UK: Academic Publishing Limited; 2008. p. 1–12.
Kellier F. Interpretivism and the pursuit of research legitimisa- tion: an integrated approach to single case design. Electr J Bus Res Method 2005;3:123–32.
Hughes J, Jones S. Reflections on the use of grounded theory in interpretive information systems research. EJISE Electr J Inform Syst Eval 2002;6.
Suhonen J. Scientific methodology in computer science – fall 2009. Lecturer Notes 2009;1, <http://cs.joensuu.fi/pages/suhonen/Sci- Met2009/SciMet_2009.html> [accessed 01.08.14].
Sormunen M, Terhi Saaranen T, Tossavainen K, Turunen H. Process evaluation of an elementary school health learning intervention in Finland, vol. 112, no. 3. Emerald Group Publish- ing Limited; 2012, pp. 272–291.
Bryant P. Collaborative action research ‘‘on the cutting edge”. Master thesis, The University of Lethbridge; 1995.

Grounded theory and action research	327

UN. United Nations E-Government survey 2010. Leveraging e- government at a time of financial and economic crisis. New York, Department of Economic and Social Affairs; 2010.
Ponterotto JG. Qualitative research in counseling psychology: a primer on research paradigms and philosophy of science. J Counsel Psychol 2005;52:126–36.
Shakespeare W. The qualitative quantitative choice. Chapter 6 – Research Paradigms; 2004. P. 103–18. <http://adt.curtin.edu.au/ theses/available/adt-WCU20050721.094049/unrestricted/07Chap- ter6.pdf> [accessed 10.01.12].
Aikenhad GS. A framework for reflecting on assessment and evaluation. In: Globalization of science education: international conference on science education. Seoul, Korea the Korean Education Development Institute; 1997.
Mattila M, Aaltio I. From tools to social construction of organizational reality: studying value dissemination in three
case companies. EJBO Electr J Bus Ethics Organ Stud 2006;11: 15–23.
Ting-Toomey S. Qualitative research: an overview. In: Kim WBGAYY, editor. Methods of intercultural communications research. Beverly Hills, California: Sage Publications; 1984.
Packer M. Interpretive research; 1999. <http://www.mathcs.duq. edu/~packer/IR/IRlogic.html> [accessed 06.01.11].
Guba EG, Lincoln YS. Evaluation paradigms: worldviews or belief systems that guide evaluators; 2001. <http://www.evaluate- europe.net/projects/eval3/Dublin-workshop/Gubba-Lincoln/at- tach/Gubba-Lincoln.doc> [accessed 09.01.13].
Abdel-Fattah MA. An evaluation framework for e-government services [unpublished Ph.D. thesis]. Faculty of computers and information-Department of information systems, Cairo Univer- sity; 2010.
