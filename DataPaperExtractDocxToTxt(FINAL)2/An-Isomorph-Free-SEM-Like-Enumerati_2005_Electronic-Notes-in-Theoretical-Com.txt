Electronic Notes in Theoretical Computer Science 125 (2005) 91–113 
www.elsevier.com/locate/entcs


An Isomorph-Free SEM-Like Enumeration of Models 1
Thierry Boy de la Tour2	Prakash Countcham3
LEIBNIZ laboratory, IMAG - CNRS
INPG, 46 avenue F´elix Viallet F-38031 Grenoble Cedex, France

Abstract
We investigate the integration of the enumeration of finite models of a formula, including unit prop- agation and pruning mechanisms, as provided by the system SEM, into McKay’s general method of isomorph-free exhaustive enumeration. The two techniques turn out to be nicely compatible, though this requires some adaptations, and to prove some non-trivial properties.
Keywords: finite model building, isomorph-free search, computational group theory


Introduction
In the recent past much work has been devoted to the automated construction of finite models of first order formulas. Such models may bring much help either in automated reasoning (in semantic strategies) or for interactive use, for instance for debugging purposes, or simply for refuting a conjecture. Many systems have been developed, and many efforts devoted to reducing the huge search spaces involved. The most successful methods are those able to derive as much information as possible from the given formula.
Other powerful methods are used that are quite independent of the given formula: this is the case of symmetries. Systems like FMC (see [7]) and SEM (see [9]) both profit from techniques that help to eliminate counter-models

1 This work has been supported by the CNRS and the MENRT
2 Email:Thierry.Boy-de-la-Tour@imag.fr
3 Email:Prakash.Countcham@imag.fr



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2005.01.003


which present some restricted form of isomorphisms to known counter-models. SEM’s method is the Least Number Heuristic, LNH, and has been extended to XLNH in [1], in order to account for more general isomorphisms (see our analysis in [4]). But no method has yet been found to account for general isomorphisms between interpretations.
There is however a general method for building isomorph-free enumerations of combinatorial structures, due to Brendan McKay, see [6]. We show how this method can be used in conjunction with the most powerful mechanisms of SEM, which are unit propagation and partial evaluation (i.e., evaluation in partial interpretations). We provide in Section 2 an abstract though accurate account of these features, and prove some original properties. In Section 3 we show that they are compatible with the necessary group theoretic framework. We then show how McKay’s method can be used in this context, and in Section 4 we provide an algorithm SEMK that also includes the pruning mechanism of SEM. But SEMK is exhaustive only if called with many different inputs. We show in Section 5 that a slight modification ensures completeness with only one call to SEMK.

Unit Propagation in Partial Interpretations
We are given a sorted signature Σ, whose elements are function symbols, and a finite domain Ð. There is a sort of boolean values, and Ð contains the boolean values T (true) and ⊥ (false). The domain Ð is the disjoint union of the nonempty domains attributed to each sort, and we may say that each element of Ð has a unique sort. A function symbol whose range sort is the boolean sort is a predicate symbol.
For any symbol f ∈ Σ of arity n, an f-cell is a tuple of the form
⟨f, v1,... , vn⟩ where the vi’s are elements of Ð. However, we will only consider the f -cells that are compatible with the sort profile of f as given in Σ, i.e., vi is taken only in the domain associated to the sort of the ith argument of f . The set of these compatible f -cells, for all symbols f ∈ Σ, will be noted C . The cell ⟨f, v1,... , vn⟩ will be noted f [v1,... , vn].
A Σ-interpretation in Ð is a function from C to Ð, though obviously not all functions from C to Ð can be considered as well-sorted interpretations, since once again we may only assign values of the correct sort to any given cell (this kind of restriction is a particular case of the notion of constraint used in SEM).
We are next given a first-order Σ-formula with equality ϕ; the problem addressed by SEM is to find a model (or all models) of ϕ in Ð. The search for such models is considered in SEM as a Constraint Satisfaction Problem

(CSP).
We now give a short description of SEM. By suitable transformations de- scribed below we may restrict our considerations to the case where ϕ is a set of ground clauses. To each cell is initially associated a set of possible values. A search tree is then developed according to the following basic principle: at each node we choose a cell c whose value is not yet defined, i.e., it still has a set V of possible values. Then for each possible value v ∈ V we recur-
sively consider the case where c has the unique value v. At the leaves of the
search tree all cells have a single value; these leaves correspond to all possible interpretations of Σ, and among them can be found all possible models of ϕ. This basic search tree is pruned in two ways. The first is based on the fact that we may not need definite values for all cells in order to compute a truth value for ϕ. If for instance we have enough information to evaluate a clause of ϕ to false, then no further refinement may lead to a model, and it is therefore safe to backtrack. The second optimization is to use short-cuts down the tree, by directly inferring values for cells, from the knowledge that each clause must eventually become true. This is performed by unit propagation, thoroughly defined and analyzed below. But we first turn to evaluation with
partial information.


Partial Interpretations and Evaluation
It is therefore convenient to define a partial interpretation as a binary relation I on C ×Ð such that to each cell corresponds at least one element in Ð. For a cell c in C and a relation R ⊆ C × Ð, we note R[c] the set {v ∈ Ð | c R v}; the domain of R is the set dom(R) = {c ∈ C | R[c] /= ∅}. Therefore, the relation I is a partial interpretation when dom(I) = C. Given two partial interpretations I and J , we say that J is a reﬁnement of I if J ⊆ I.
The required restriction for the possible values of cells can then easily
be expressed by considering only the refinements of a given initial partial interpretation I0. For instance, if P ∈ Σ is a predicate symbol, and c a P - cell, then I0[c] is the set of boolean values {T, ⊥}. For any f ∈ Σ (except for some constants, see below), and any f -cell c ∈ C, the set I0[c] is the domain associated to the range sort of f . We note I the set of all partial interpretations I such that I ⊆ I0.
For any relation R ⊆ C ×Ð and any subset A of C, the restriction R|A of R to A is the relation on C ×Ð defined by: c R|A v iff c R v and c ∈ A. The functional part fp(R) of R is the biggest restriction R|A which is a function (from A to Ð). Finally, for any I ∈ I, the masking of I by R is the relation

I R defined by:
c (I R) v ⇔ c R v or (R[c]= ∅ and c I v).
Example 2.1 Consider two constants a and b, and two values 1 and 2 in the domain, and let I = {⟨a, 1⟩, ⟨a, 2⟩, ⟨b, 1⟩, ⟨b, 2⟩} and R = {⟨a, 1⟩}. Obviously, R is a partial function, while fp(I)= ∅. We have I  R = {⟨a, 1⟩, ⟨b, 1⟩, ⟨b, 2⟩}, so that fp(I  R)= R. Masking may also increase an interpretation (or change it in other ways), for instance we have (I  R)  {⟨a, 1⟩, ⟨a, 2⟩} = I.
For any I ∈ I, it is easy to see that if R ⊆ I, then I  R is a refinement of
I, and is in I. For any cell c and any subset E ⊆ Ð, we note [c, E] the relation
{⟨c, v⟩| v ∈ E}. For v ∈ Ð, we write [c, v] for [c, {v}]. An interpretation is a partial interpretation I ∈ I which is a function, hence such that I = fp(I).
The notion of Σ-term of a given sort is as usual. An atomic formula is either a Σ-term of boolean sort, or an equation t = t' where t and t' are two Σ-terms of the same sort. A literal is a possibly negated atomic formula, a clause is a disjunction of literals, and a formula ϕ is a conjunction of clauses.
We do not consider variables and quantifiers for the following reason: in SEM each input clause with n universally quantified variables C(x1,..., xn) is replaced by the conjunction of the ground clauses C(v1,... , vn) for all possi- ble 4 values vi ∈ Ð. Since the elements of Ð are not constant symbols, we re- place each value v ∈ Ð by a special, new constant symbol av. These constants are special because they are given special values in I0; we take I0[av]= {v}, and of course we implicitly add them to Σ. We will not prove that any first order formula with equality ψ can thus be transformed into a set of ground clauses ϕ, such that searching models of ψ in Ð is equivalent (through a 1-1 correspondence) to searching models of ϕ among the refinements of I0.
Example 2.2 For instance, if the original (skolemized) formula contains the clause P (x, y), and the universal variables x, y have the same sort, which is in- terpreted in the domain {1, 2}, then ϕ contains the clauses P (a1, a1), P (a1, a2), P (a2, a1) and P (a2, a2). Moreover, we have I0[a1] = {1} and I0[a2] = {2}, while for all P -cells c we have I0[c]= {T, ⊥}.
Another special property of these new symbols is that for any u, v ∈ Ð of the same sort, for any clause C in ϕ where au occurs, there is a clause C' in ϕ obtained from C by replacing au by av.
We now need to extend the well-known notion of the value of a term, literal, clause or formula in an interpretation, to the notion of value in a partial interpretation. But what could the value  ϕ)I of a formula ϕ in a

4 This is restricted by the sorts of the variables xi. This transformation is of course expo- nential in n.


partial interpretation f be? An obvious answer is to take the set of values of ϕ for all interpretations included in f. But this would of course solve the satisfiability problem, hence would require exponential time to compute. SEM consequently adopts a more efficient computation rule, yielding a superset of the set of all possible values.
Definition 2.3 The value  t)I of a Σ-term t in a partial interpretation f is
defined inductively as:


 f (t ,... ,t ))
= ⎧⎨ f[ f [v1,... , vn]] if | ti)I| =1 for all 1 ≤ i ≤ n,

1	n I
⎩ f0[ f [v1,... , vn]] otherwise,

where vi ∈ ti)I for all 1 ≤ i ≤ n. Note that f0[f [v1,... , vn]] does not depend on the particular vi’s chosen in ti)I. Note that t)I cannot be empty.
This definition is valid for atomic formulas other than equations. The value
of an equation t = t' is similarly defined by:
 t = t')I  = ⎪⎨ {⊥}	if  t)  ∩ t')  = ∅.
I	I
⎪⎪⎩ {T, ⊥} otherwise,
The value of any literal is thus obviously defined. We then define the value of a clause C (resp. a formula ϕ) in f as the set of all truth values obtained by disjunction (resp. conjunction) of all the possible truth values of its literals (resp. clauses) in f. In other words, we have
Since the formula ϕ is an input of the search process, for convenience we may keep it implicit, and note f^ for ϕ)I. An interpretation f is a model (resp. a counter-model ) if f^ = {T} (resp. {⊥}).
Example 2.4 As an example, we consider a constant a and a unary function symbol f , and two values 1 and 2. We therefore have
I0 = {⟨f [1], 1⟩, ⟨f [1], 2⟩, ⟨f [2], 1⟩, ⟨f [2], 2⟩, ⟨a, 1⟩, ⟨a, 2⟩, ⟨a1, 1⟩, ⟨a2, 2⟩}, and we consider the clause f (a)= a. We have  a)I0 = {1, 2}, hence  f (a))I0 =
{1, 2}, and therefore f (a)= a)I0 = {T, ⊥}.
This evaluation mechanism is monotonic in the sense that, if J ⊆ f are two partial interpretations, then for any term t we have t)J ⊆ t)I, which


can easily be proved by induction on t. This property then obviously extends to literals and clauses, hence:
J ⊆ f ⇒ J^ ⊆ f^.
The search for a model in SEM is guided by the evaluation of f for suc- cessive refinements f of f0, based on the following property, which follows directly from the previous one:
For all f ∈ I, if there is a J ⊆ f which is a model of ϕ, then T ∈ f.
Hence the search may be pruned if f = {⊥}. We now show how the search is conducted, by propagating refinements obtained from unit clauses.

Unit propagation
Unit propagation is an important feature of SEM. We have to define precisely in our setting, in order to prove a confluence property that is not essential to SEM, but that will be needed later.
Definition 2.5 We say that a term t is deﬁnable in f if s)I is a singleton for all strict subterms s of t. If t = f (t1,... , tn) we have unique values vi ∈ ti)I, and we therefore associate to t the cell cellI(t) = f [v1,... , vn]. A definable term t in f is deﬁned in f if t)I is a singleton. In both cases, by Definition
we have t)I = f[cellI(t)].
This definition holds for atomic formulas of the form P (t1,... , tn), by considering them as terms (of boolean sort). Similarly, an equation t = t' is deﬁnable in f if t and t' are definable in f; and it is moreover deﬁned in f if
 t = t')I is a singleton. Note however that an equation t = t' may be defined
though t or t' aren’t (if their undefined values are disjoint).
A conjunction, disjunction or negation ϕ is deﬁnable in f whenever all its conjuncts, disjuncts or negated formula are definable in f; and it is moreover deﬁned in f if ϕ)I is a singleton.
A clause C is unit in f if it is definable yet undefined in f, and contains only one literal l which is not defined in f. We refer to l as the unit literal in C (relative to f).
Example 2.6 Following Example 2.4, a is not defined in I0 (though it is definable, as any constant), hence f (a) is not definable in I0. However, if we take I1 = I0  [a, 1], then a is clearly defined in I1, hence f (a) is definable, and cellI1 (f (a)) = f [1]. The literal f (a)= a is also definable in I1. Hence the clause f (a)= a is unit in I1.
It is of course possible to draw useful information from the knowledge that a unit clause of ϕ, hence its unit literal, must be true. We thus define an

abstract reduction relation on I:
Definition 2.7 For any clause C of ϕ we define a reﬁner relation YC,I. If C
is unit in f, with unit literal l, we may apply one of the following rules:
if l is P (t1,... , tn), then YC,I = [cellI(P (t1,..., tn)), T],
if l is чP (t1,... , tn), then YC,I = [cellI(P (t1,... , tn)), ⊥],
if l is t1 = t2, then YC,I = [c1, E] ∪ [c2, E], where E = f[c1] ∩ f[c2],
if l is t1 /= t2 and |f[c2]| = 1, then YC,I = [c1, f[c1] \ f[c2]],
if l is t1 /= t2 and |f[c1]| = 1, then YC,I = [c2, f[c2] \ f[c1]], where ci = cellI(ti). In all other cases, we take YC,I = ∅.
For any two partial interpretations f, J ∈ I, we say that f unit-reﬁnes to J , and write f >ϕ J , if there exists a clause C in ϕ, such that J = f YC,I and J /= f. As above, we generally omit ϕ and just write f > J . If there is a K such that f >٨ K and J >٨ K, we say that f and J are joinable, and write f ↓ J . If there is no J such that f > J , then f is a normal form.
It is easy to see that YC,I is included in f, hence so is f YC,I. Therefore, if f > J then J Ç f, which proves that the unit-refinement reduction > terminates. In Example 2.6, the clause f (a)= a is unit in I1, and we get the refiner Yf(a)=a,I1 = [f [1], 1], so that I1 > I1 [f [1], 1]; this assigns the value 1 to the cell f [1].
Each time a value is assigned to a cell, new clauses may become unit, and thus induce further refinements of f YC,I. This is why > is not obviously confluent, and in fact, the following example shows that it is not confluent.
Example 2.8 Consider two propositional variables P and Q, and let I =
{⟨P, T⟩, ⟨P, ⊥⟩, ⟨Q, ⊥⟩}. We consider the formula ϕ with two clauses C =
P V Q and C' = чP V Q, which are both unit in I. Let J = I   YC,I =
{⟨P, T⟩, ⟨Q, ⊥⟩} and J' = I YC',I = {⟨P, ⊥⟩, ⟨Q, ⊥⟩}, we have I > J and I > J', and of course J /= J'. No clause is unit in J or J', hence they are normal forms. They are also both counter-models of ϕ.
This last fact however suggests that a restricted form of local confluence may hold. This is indeed the case, as we now prove. Since we have to consider all five rules of Definition 2.7 the proof is somewhat tedious, though not very difficult, so we give it in the Appendix.
Lemma 2.9 If f > J , f > J ', and T ∈ J^ or T ∈ J^' then J ↓ J '.
Note that even if both J and J are non-counter-models, it may be the case that they only join on a counter-model. Hence we have not proven local confluence of the restriction of > to non-counter-models, and cannot use


Newman’s Lemma (see e.g. [2]) to get a confluence result for this restriction. We can still adapt the proof of Newman’s Lemma to obtain a computationally meaningful notion of a normal form.
Theorem 2.10 If f >٨ J and K is a normal form of f (i.e., such that
f >٨ K) and T ∈ K then J >٨ K.
Proof. We prove by well-founded induction based on > that it is true for all
f. So we suppose that it is true for all f' such that f > f', and show that it must then be true for f. If f = J or f = K then the conclusion J >٨ K is obvious.

We now suppose that f =/
J and f =/
K, so we have f > J ' >٨ J and

f > f' >٨ K. Since T ∈ K and K ⊆ f' we have T ∈ f', and we may apply Lemma 2.9. We therefore have a partial interpretation L such that f' >٨ L and J ' >٨ L. By induction hypothesis, applied to f', we have L >٨ K, hence J ' >٨ K. Again by induction hypothesis, applied to J ', we obtain J >٨ K.	 
It is then clear that f can have at most one normal form which is not a counter-model. Moreover, its existence is easily decided:
Corollary 2.11 If f >٨ J and J^ = {⊥}, then f has no normal form K
such that T ∈ K^.
Proof. Suppose K is a normal form of f and T ∈ K, then by Theorem 2.10 we have J >٨ K. We then have K ⊆ J , hence T ∈ J^, a contradiction. 
This means that, if we find a contradiction (i.e., a counter-model) in the course of computing a normal form of f, then we know that no other sequence of unit-reductions would lead to a normal form which is not a counter-model.
In other words, we only lose confluence on counter-models. Of course, we may stop reducing when a contradiction is found.
Definition 2.12 If f has a normal form J such that T ∈ J , then we note it f↓. Otherwise, f↓ denotes any counter-model such that f >٨ f↓. Despite this last non-determinism, we call f↓ the normal form of f.
In Figure 1 we give the version of SEM that does not stop on the first model of ϕ that it finds. It uses the Assign-and-Propagate function APϕ(f, c, v) = (f [c, v])↓, that assigns the value v to the cell c, and propagates this value through unit-refinements, by computing the normal form of f [c, v].
Example 2.13 Following Example 2.4, where ϕ only contains the clause f (a) = a, we have APϕ(I0, a, 1) = I1 ↓ where I1 = I0  [a, 1]. As noted above, we have Yf(a)=a,I1 = [f [1], 1], so that I1 > I2 = I1  [f [1], 1]. Note


SEMϕ(f)= if T /∈ f then ∅
else if ⊥ /∈ f then {f}
else
choose c ∈ C such that f[c] is not a singleton;

 

v∈I[c]
SEMϕ( APϕ(f, c, v) ) 

Fig. 1. An abstract of SEM

that f (a)= a)I2 = {T}, so that no clause is unit in I2, and I2 is not a counter-model, hence I1↓= I2.
Of course SEMϕ(f0↓) is complete in the sense that for any interpretation ł which is a model of ϕ, there is a model ł' ∈ SEMϕ(f0↓) such that ł ⊆ ł'. It is easy to see that for any two different models ł, ł' ∈ SEMϕ(f0↓), we have ł /⊆ ł'. However, for any ł ∈ SEMϕ(f0 ↓), and any ł' which is isomorphic to ł, we have ł' ∈ SEMϕ(f0↓). In order to prune the search space so as to obtain an isomorph-free search, we now give a group theoretic
account of isomorphisms.
It should first be mentioned that the very simple description of SEM given above misses an important source of efficiency, obtained by a complex back- track mechanism. But since we are going to replace the enumeration provided by the algorithm SEM above by a rather different algorithm, it is not possible to recover this backtracking as such. We rather focus on propagation, which is probably the most important feature of SEM.

The Invariance of Propagation
Our aim is to reduce the search space of SEM by making it isomorph-free. This means that once a partial interpretation has been considered, than no partial interpretation isomorphic to this one will be considered. The notion of isomorphism between Σ-interpretations is well-known: they are special bijec- tions between their carrier sets. But since we have only one carrier set, namely Ð, our isomorphisms are permutations of Ð. It is therefore more convenient
to define isomorphisms starting from permutation groups.
So we first consider the group Sym(Ð) of permutations σ of Ð. For each
v ∈ Ð, the image of v by σ is noted vσ, as is standard in group theory. The
product in Sym(Ð) is defined by vσσ' = (vσ)σ' . An action of a group G on
a set S is a morphism h from G to the group Sym(S). If only one action
is defined on S, it is unambiguous to write sσ for sh(σ), for any s ∈ S and
σ ∈ G. For example, if σ is a permutation of integers, say σ = (2 3), we


may apply it directly to a set of integers, say {1, 2}(2 3) = {1, 3}, rather than translating explicitly σ as a permutation of sets of integers, which would yield h(σ)= ({1, 2} {1, 3})({2} {3})({2, 4} {3, 4}) etc.
The G-orbit of an element s ∈ S is the set sG = {sσ | σ ∈ G}, and the automorphism group of s is the set Aut(s) = {π ∈ G | sπ = s}, which is a subgroup of G. For H a subgroup of G, a coset of H in G is a set Hσ = {µσ | µ ∈ H}, or a set σH = {σµ | µ ∈ H}, for a σ ∈ G. It is easy to see that for all σ ∈ G, we have Aut(sσ)= {σ−1πσ | sπ = s} = σ−1Aut(s)σ.
We define an action of Sym(Ð), first on Σ, by taking f σ = f for all f ∈ Σ except the special constants av for v ∈ Ð, for which we take (av)σ = avσ . We
then define an action on the set C of cells by: (f [v1,... , vn])σ  = f σ[vσ,... , vσ].
1	n
We further define an action on the set of binary relations on C ×Ð by taking Yσ = {⟨cσ, vσ⟩| c Y v}. In other words, we have c Y v iff cσ Yσ vσ, which can also be written Yσ[cσ] = (Y[c])σ = {vσ | v ∈ Y[c]}. It is easy to see that for any partial interpretation f, the relation fσ is also a partial interpretation.
Moreover, since f[a ]= {v}, then fσ[a ]= (f[aσ−1 ])σ = {vσ−1 }σ = {v}.
v	v	v
Example 3.1 Following Example 2.4, we let σ be the permutation swapping values 1 and 2, which is noted in cycle notation by σ = (1 2). We may apply it to the cell f [1], which yields (f [1])σ = f σ[1σ] = f [2]. Similarly, we have [f [1], 2]σ = [f [2], 1], and the reader can check that (I0)σ = I0.
In the sequel, we consider the following group of isomorphisms 5
G = {σ ∈ Sym(Ð) | fσ = f0 ∧ Tσ = T },
which means that we enforce the preservation of sorts and of truth values. We now define the action of G on syntactic objects. For any Σ-term t and any permutation σ ∈ G, the term tσ is simply obtained from t by replacing all symbols f by f σ. The definition is similar for literals, clauses and formulas.
Note that for any clause C of ϕ, the clause Cσ is also a clause of ϕ (due to the special property of the constants av mentioned above), so that ϕσ = ϕ (up to the AC property of conjunction).
It is now very easy to prove, by structural induction, that for all terms t we have tσ)Iσ = ( t)I)σ. Hence for all literals l we have lσ)Iσ = l)I, and for all clauses C we have Cσ)Iσ = C)I. Finally, we have
6σ ∈ G, f^σ  = f^.
As usual, we say that the partial interpretations f and fσ are isomorphic; this defines an equivalence relation. The isomorphism class of f is the G-orbit of f.

5 Any element in the group is an isomorphism, in the usual sense, between two suitable interpretations.

We now show that SEM’s propagation is compatible with the action of
G. First, it is obvious that for any term t and literal l, t and l are defined (resp. definable) in f if and only if tσ and lσ are defined (resp. definable) in fσ. Therefore, a clause C is unit in f iff Cσ is unit in fσ. In this case
we have the refiner relation YCσ ,Iσ , and the reader can easily check that it is exactly (YC,I)σ. Finally, it is trivial to prove that for any relation Y we have fσ  Yσ = (f Y)σ, which clearly yields
f > J  ⇒ fσ > J σ.
Therefore, if f↓ is not a counter-model, we get fσ ↓= (f↓)σ, and this is not a counter-model either. If f↓ is a counter-model, we still have fσ >٨ (f↓)σ, which is also a counter-model. The freedom given by the non-determinism in the definition of the normal-forms of f and fσ therefore leaves us the possibility to choose (f↓)σ as the value of fσ↓. This yields the invariance of the Assign-and-Propagate function:
APϕ(fσ, cσ, vσ) = [APϕ(f, c, v)]σ.
If in practice the property is not true on counter-models, this is not a problem since counter-models are immediately pruned. The argument above proves that this pruning is invariant under the action of G.

McKay’s Exhaustive Enumeration
McKay’s algorithm is general in the sense that it leaves some freedom in the definition of a number of mathematical objects, which should of course meet a number of properties. We will see how this freedom allows us to use the propagation defined above, and what problems this use entails. We also try to provide an explanation of McKay’s method, which is by far nontrivial, while building our objects.
Canonical Elements
In the beginning are “labeled” and “unlabeled” objects: the former are the objects we know how to enumerate (i.e., partial interpretations), and the latter are the isomorphism classes of labeled objects, that we wish to enumerate. The terminology comes from graph theory, since a graph’s vertices are actual elements of an actual set (the labels), while an isomorphism class of graphs can be seen as one abstract graph, or a Platonic notion of a graph, hence unlabeled.
A more trivial example is provided by representing sets as lists. Lists are the labeled objects, and many different lists represent the same set. By giving a suitable definition of isomorphism between lists (by permuting the elements’


positions), we can obtain the property that two lists represent the same set exactly when they are isomorphic. Sets are then the unlabeled objects, and correspond exactly to the isomorphism classes of lists.
An essential requirement in the method is to be able to compute a single canonical element for each isomorphism class. In the case of lists, this can be performed efficiently by sorting. But in the case of partial interpretations, this problem is equivalent (by polynomial time Turing reductions) to the problem of deciding whether two graphs are isomorphic (see e.g. [3]). This isa problem in NP which is not (known to be) in P, but is however known not to be NP- complete (conjecturing that the polynomial hierarchy does not collapse, see [8]). This nice theoretical property is met in practice by the very efficient and complex algorithm nauty, also by McKay (see [5]). We just mention that it
can be used to compute a canonical form of any partial interpretation f, and incidentally a generating set of the group Aut(f).
More precisely, the canonical element in fG is reached from f by applying a canonical permutation γ(f), i.e., the canonical element is fγ(I). It is canonical since it can be reached from any element in fG, hence for all σ ∈ G we have fγ(I) = fσγ(Iσ ). This is strictly equivalent to saying that µ = γ(f)(σγ(fσ))−1 is an automorphism of f. But saying that µ ∈ Aut(f) is equivalent to saying that the coset µAut(f) is Aut(f). Hence this is equivalent to:
γ(f)γ(fσ)−1σ−1Aut(f)= Aut(f)
⇔  γ(fσ)−1σ−1Aut(f)= γ(f)−1Aut(f).

Inductive Construction
How can we enumerate the canonical elements by eﬃciently eliminating non- canonical elements? The idea of McKay’s method is to model the construction of canonical elements as an inductive process, compatible with G. Objects can generally be built from smaller objects, by an easy and efficient enumeration (think of lists). This results in a construction tree, on which the notion of isomorphism between objects can be applied, yielding a notion of isomorphic subtrees 6 . The idea behind McKay’s algorithm is to prune all non-canonical branches, as soon as possible. This idea requires that we fill in a number of details.
We first ensure well-foundedness by attributing an integer to each object: its order, so that the order increases during the inductive construction. A natural choice for partial interpretations f is the cardinality of the functional

6 since an object can usually be built in many different ways, we should speak of a directed acyclic graph rather than a tree. However, the structure does not exist explicitly in memory, hence it is as expensive to travel through as if it were a tree.

part fp(f). The order is minimal for f0, and maximal for interpretations.
We next have to define lower objects and upper objects. Intuitively, an up- per object should contain all the necessary information to build one successor of f in the construction tree. In our case, each successor of f will be obtained by assigning a value v to a cell c, provided that v ∈ f[c], and also that f[c] is not reduced to the singleton {v}. The successor will then be APϕ(f, c, v), whose order is then guaranteed to be greater than f’s order. We thus define the set of upper objects associated to f as:
U(f) = { ⟨f, c, v⟩ | {v} Ç f[c] }.
Similarly, lower objects should contain all the necessary information to compute all possible predecessors. A given partial interpretation J may be obtained as J = APϕ(f, c, v) where f contains J , and as above we must have
{v} Ç f[c]. In order to avoid redundant information, we need only provide the relation S = f \ J . Also note that J [c] = {v}, so we define the set of lower objects associated to J as:
L(J ) = { ⟨J , S, c⟩ | Ev, J ∩ S = ∅ ∧ {v} = J [c] ∧ S[c] /= ∅
∧ J = APϕ(J ∪ S, c, v) }.
Note that the order of J ∪ S is strictly less than the order of J . We now have to define actions of G on lower and upper objects, and check that for all σ ∈ G we have L(J σ) = L(J )σ and U(fσ) = U(f)σ. Obviously, we take
⟨f, c, v⟩σ = ⟨fσ, cσ, vσ⟩ and ⟨J , S, c⟩σ = ⟨J σ, Sσ, cσ⟩, and the two invariance properties are easy to check, by using the invariance of propagation.
Note that we have not used propagation in the definition of upper objects, hence these are not yet related to lower objects. McKay’s method requires that we define such a relation. We note it R, and define it by:
⟨J , S, c⟩ R ⟨f, c', v⟩ iff
there is a σ in G such that fσ = J ∪ S, c'σ = c and J [c]= {vσ}.
As required, the order of f is smaller than the order of J , and every lower object is related to an upper object. There are two other properties that R should meet. The first is that if an upper object ⟨f, c, v⟩ is related to some lower object, then it should also be the case for ⟨f, c, v⟩σ, for any σ ∈ G. Considering the definition of R, this is obvious. Finally, we let the reader check
that, if we have both ⟨J1, S1, c1⟩ R ⟨f1, c' , v1⟩ and ⟨J2, S2, c2⟩ R ⟨f2, c' , v2⟩,
1	2
then we have:
⟨J , S ,c ⟩G = ⟨J , S ,c ⟩G ⇔ ⟨f , c' ,v ⟩G = ⟨f , c' ,v ⟩G.

1	1  1
2	2  2
1  1	1
2  2	2

SEMKϕ(f) =let A = ∅ in
for all Aut(f)-orbit O in U(f) do choose ⟨f, c, v⟩ ∈ O;
let J = APϕ(f, c, v) in
if T ∈ J and ⟨J , f \ J , c⟩ ∈ m(J ) then if ⊥ /∈ J then A := A ∪ {J }
else A := A ∪ SEMKϕ(J )
done;
A
Fig. 2. The algorithm SEMK
Selecting Lower Objects
The search for models will be pruned by constructing only the partial inter- pretations corresponding to selected lower objects. This selection is based on canonicity, through the following function:
∅	if L(J )= ∅,
m(J ) =
⎩ ⟨J , S, c⟩Aut(J ) otherwise,

where ⟨J , S, c⟩ = l(J γ(J ))γ(J ) . The function l, applied to J , yields an element of L(J ) if there is one (it is a choice function). As mentioned above, the computation of γ(J ) also yields generators for the group Aut(J ). The conditions M1 and M2 in [6] are obvious by definition, and there only remains to show the invariance of m under G. For any σ ∈ G, we have L(J σ)= ∅ iff L(J )= ∅, so in that case we obviously have m(J σ)= m(J )σ = ∅. Otherwise,
we have:
m(J σ)= l(J σγ(J σ ))γ(J σ )−1 Aut(J σ )
= l(J γ(J ))γ(J σ )−1 σ−1 Aut(J )σ
= l(J γ(J ))γ(J )−1 Aut(J )σ
= m(J )σ.
We now adapt the generating procedure from [6], in order to recover the pruning mechanism of SEM. This yields the algorithm SEMK given in Figure
2. Its result does not depend on the choice of ⟨f, c, v⟩, which therefore could be performed non-deterministically. It is easy to see that the pruning performed
according to the value of J^ (which does not exist in McKay’s procedure)
is correct in the sense that no branch leading to a canonical model can be
pruned.
However, we still have a problem of completeness, since McKay’s generat- ing procedure is exhaustive only if applied to all irreducible objects, i.e., the


partial interpretations f such that L(f) = ∅. But L(f) is nonempty only if f can be obtained as the result of assigning and propagating, and therefore if it is a normal form w.r.t. >. Therefore all reducible interpretations in the sense of > are irreducible in the sense of McKay. Even if we were able to compute these interpretations, it is not realistic to run SEMK on all of them.

A Restricted Enumeration
It is however not necessary to generate all isomorphism classes of partial in- terpretations, and we may restrict the search to those interpretations which are logically meaningful, i.e., the interpretations that can be obtained by se- quences of assignments and propagations from f0.
More precisely, we define inductively the set P of partial interpretations
reachable from f0 as the smallest set containing f0↓ and such that:
f ∈ P ⇒ 6c ∈ C, 6v ∈ f[c], APϕ(f, c, v) ∈ P.
It is easy to prove, by induction, that P is stable under the action of G, hence that for all f ∈ P and σ ∈ G, we have fσ ∈ P. Hence it should be possible, by restricting SEMK, to obtain an isomorph-free enumeration of P.
We first restrict upper objects to those in the sets U(f) for all f ∈ P, and call them reachable upper objects. For lower objects we take, for all J ∈ P,
L'(J ) = { ⟨J , S, c⟩ ∈ L(J ) | J ∪ S ∈ P }.
The reachable lower objects are those in the sets L'(J ) for all J ∈ P. It is easy to show that for all σ ∈ G, we still have L'(J σ)= L'(J )σ.
Finally, we note R' the restriction of R to reachable lower and upper ob-
jects. The only property of R that is not trivially preserved by this restriction is the required property that every reachable lower object ⟨J , S, c⟩ should be related to a reachable upper object ⟨f, c', v⟩. But we know that J ∪ S is in P, so that ⟨J ∪ S, c, v⟩, where {v} = J [c], is a reachable lower object, related
to ⟨J , S, c⟩. This of course explains our definition of L'(J ).
The only irreducible object, i.e., element J of P such that L'(J ) = ∅, is clearly f0↓. The enumeration SEMKϕ(f0↓) is therefore complete as SEMϕ(f0↓) is. This completeness however does not come for free: we have to replace the choice function l relative to L by a choice function l' relative to L'.
In the first version of SEMK a lower object l(J ) = ⟨J , S, c⟩ is easy to compute from a reducible J , by choosing a cell c such that J [c] is a singleton, and taking S = f0[c] \J [c]. The problem for computing l'(J )= ⟨J , S, c⟩, for J ∈ P \ {f0}, is that we must now ensure that J ∪ S ∈ P.
Membership in P can be characterized by:
f ∈ P ⇔ f = (f0  fp(f))↓ .


l'(J ) = let j = fp(J ) and I := f0↓ in let C = dom(j) \ dom(fp(I)) in while C /= ∅ do let c ∈ C in
I' := I;
I := APϕ(I, c, j(c));
C := C \ dom(fp(I))
done;
⟨J ,I' \J , c⟩
Fig. 3. A choice function
Unfortunately, there is no guarantee that this property holds for f = J ∪ f0[c], and we may have J ⊆ (f0  fp(f))↓Ç f. Also note that this membership test is not computationally harmless. The algorithm for computing l' that we propose in Figure 3 requires approximately the same amount of time.
This algorithm clearly terminates since the number of cells in C decreases after each iteration (since c /∈ dom(fp(f))). Each value of f is obviously in P, and contains J . This last fact is rather difficult to prove, and is true only because J is not a counter-model. The key point is the following result:
Lemma 5.1 If f > J and v ∈ J [c] then f [c, v] ↓ J  [c, v].
Proof. Let C be the clause such that J = f YC,I, and l be the corre- sponding unit literal, and let f' = f [c, v] and J ' = J  [c, v]. We have v ∈ J [c] ⊆ f[c], hence if f[c] is a singleton we have f = f' and J = J ', hence obviously f' > J '. We now suppose that f[c] is not a singleton, and consider the rule that yields YC,I in Definition 2.7.
If it is rule (i) or (ii), then YC,I = [c', w] where w is a truth value, and we must have f[c'] = {T, ⊥}. If f'[c'] is also equal to {T, ⊥} then C is still unit in f', and of course c /= c'. We then have YC,I' = YC,I, hence
J ' = (f YC,I)  [c, v] = f (YC,I ∪ [c, v]) = f'  YC,I' ,
so that f' > J '. If f'[c'] /= f[c'], we must have c = c' and v = w, hence
f' = J = J '.
If it is rule (iii), then l is an equation t1 = t2, and YC,I = [c1, E] ∪ [c2, E]
with E = f[c1] ∩ f[c2]. Since f =/	J we have E /= ∅ and c1 /= c2.
If C is not unit in f', then we have, say, c = c1, thus J [c] = E, hence v ∈ E. But c /= c2, hence f'[c2]= f[c2] contains v, hence f'[c]∩f'[c2]= {v}. The clause C must have a definite truth value in f'; it must therefore be true, and f'[c2] = {v}. We therefore have f[c2] = {v}, and then J [c2] = J [c]= {v}, and obviously f' = J = J '.
If C is unit in f', then YC,I' = [c1, E'] ∪ [c2, E'] with E' = f'[c1] ∩ f'[c2].

If c /= c1 and c /= c2 then E = E' and YC,I' = YC,I, hence as above
J ' = f'  YC,I', so that f' > J '.
But if, say, c = c1, then of course c /= c2, so that f'[c2] = f[c2]. Since v ∈ E we have v ∈ f[c2], and since f'[c] = {v} we get E' = {v}. Let K = f'  YC,I' , we have of course f' >٨ K, and K = f'  [c2, v]. We have J ' = f'  [c2, E], hence if E is not reduced to the singleton {v}, i.e., if
J ' /= K, then C is unit in J ', with YC,J ' = [c, v] ∪ [c2, v]. Then obviously
K = J '  YC,J ' , and we get J ' > K.
If it is, say, rule (iv), then l is a disequation t1 /= t2, f[c2] is a singleton
{v'} (hence c /= c2), and we have YC,I = [c1, f[c1] \ {v'}]. We also have f'[c2]= {v'}, hence if f'[c1]= f[c1] (hence c /= c1) then C is still unit in f', with YC,I' = YC,I, and we get J ' = f'  YC,I' as above, so that f' > J '.
If f'[c1] /= f[c1] then c = c1, and since f and J differ only on c1, we
obviously have J ' = f'.

It is then easy to prove:
Lemma 5.2 If v ∈ f↓ [c] then (f↓  [c, v])↓= (f [c, v])↓.
Proof. It is easy to prove using Lemma 5.1 that if f >٨ J and v ∈ J [c] then (f [c, v])↓= (J  [c, v])↓, by induction on the length of the derivation f >٨ J . We then take J = f↓.	 
We may then prove that the order in which assignments are made is not relevant.

Theorem 5.3 Let J  = APϕ(APϕ(f, c, v), c', v'), if T  ∈
APϕ(APϕ(f, c', v'), c, v).
J^ then J =

Proof. Since T ∈ J we must have v' ∈ (f [c, v])↓ [c'], hence by Lemma 5.2 we have
J = ((f [c, v])↓ [c', v'])↓ = (f ([c, v] ∪ [c', v']))↓,
hence we must also have v ∈ (f [c', v'])↓ [c], and again by Lemma 5.2 we get
J = ((f [c', v'])↓ [c, v])↓ = APϕ(APϕ(f, c', v'), c, v).
 
Now, since J ∈ P, it must be obtained from f0 by some of the assignments of fp(J ), in some order. Propagating the same assignments in any order eventually leads to the same result J . The value I = J is obtained exactly when C = ∅, so that the preceding value I' of I has an order strictly smaller than the order of J . Hence the final value ⟨J ,I' \J , c⟩ is a reachable lower object.


Note that I' depends on the unspecified order in which the cells c ∈ C are taken in this algorithm. Any order yields a correct result, but since l' must be a function, the choice must be deterministic. The reason is that the algorithm for l' will be called several times on the same canonical element J , and must always return the same result.
Example 5.4 We may illustrate this point on Example 2.4. Starting from SEMKϕ(I0), we have three orbits in U(I0), whose representatives may be chosen to be ⟨I0, a, 1⟩, ⟨I0,f [1], 1⟩ and ⟨I0,f [1], 2⟩, in this order. We obtain a first J1 = APϕ(I0, a, 1) = [a, 1] ∪[f [1], 1] ∪[f [2], {1, 2}]. We keep J1 (i.e., there is a recursive call SEMKϕ(J1)) only if we have ⟨J1, [a, 2] ∪ [f [1], 2], a⟩ ∈ m(J1). Supposing J1 is canonical, hence that γ(J1) is the identity, we then compute l'(J1). The initial value of C is {a, f [1]}, and we clearly keep J1 if the cell a is chosen first in the while loop. Suppose this is the case.
The next iteration after J1 yields J2 = APϕ(I0,f [1], 1) = J1 ∪ [a, 2]. Sup- pose we keep J2, then one iteration in SEMKϕ(J2) yields APϕ(J2, a, 1) = J1. We then keep this J1 only if ⟨J1, [a, 2], a⟩ ∈ m(J1), which is false. The search SEMKϕ(J1) is performed only once.
The implementation of these algorithms is not finished yet. It is of course not certain that the time saved by isomorph-pruning is not entirely lost in the technicalities of SEMK, and only experimentations will assess the interest of this technique. A few discrepancies between SEM, or more generally the problem of searching for finite models of a first order formula, and McKay’s algorithm, should be emphasized.
McKay’s method is very successful at enumerating structures with rather simple inductive definitions, hence simple choice functions. In SEMK the choice function l' is not simple, and requires increasing computational power deeper in the search tree. This departs from what we may call the intension behind McKay’s method. In some sense it may be due to the fact that we cannot consider the enumerated structures (finite models of a formula) as fundamentally inductive in nature.
Another difference is that SEMK may not be efficient for finding a first model, since potentially successful branches may be pruned simply because they are not canonical. Thus SEMK may not have the same range of applica- tions as SEM; it should rather be seen as a way of exhaustively enumerating abstract structures defined in an abstract way, by a formula.

References
Gilles Audemard and Laurent Henocque. The extended least number heuristic. In Rajeev Gor´e, Alexander Leitsch, and Tobias Nipkow, editors, First IJCAR, LNAI 2083, pages 427–442.


Springer-Verlag, 2001.
F. Baader and T. Nipkow. Term Rewriting and All That. Cambridge University Press, 1998.
Thierry Boy de la Tour. On the complexity of finite sorted algebras. In Ricardo Caferra and Gernot Salzer, editors, Automated Deduction in Classical and Non-Classical Logics, LNAI 1761, pages 95–108. Springer Verlag, 2000.
Thierry Boy de la Tour. A note on symmetry heuristics in SEM. In Andrei Voronkov, editor,
Proceedings of CADE-18, LNAI 2392, pages 181–194. Springer Verlag, 2002.
Brendan D. McKay. Practical graph isomorphism. Congressus Numerantium, 30:45–87, 1981.
Brendan D. McKay. Isomorph-free exhaustive generation. Journal of Algorithms, 26(2):306– 324, February 1998.
Nicolas Peltier. A new method for automated finite model building exploiting failures and symmetries. Journal of Logic and Computation, 8(4):511–543, 1998.
Uwe Sch¨oning and Randall Pruim. Gems of Theoretical Computer Science. Springer-Verlag, 1998.
J. Zhang and H. Zhang. SEM: a system for enumerating models. In Chris S. Mellish, editor,
Proceedings of the 14th IJCAI, pages 298–303. Morgan Kaufmann, 1995.

Appendix: proof of Lemma 2.9


If f > J , f > J ', and T ∈ J^ or T ∈ J^' then J ↓ J '.





Proof. Let C and C' be the clauses of ϕ, unit in f (with respective unit literal l and l'), such that J = f YC,I and J ' = f YC',I, and let K = J  YC',J and K' = J '  YC,J ' . There may be one or two cells involved in each of YC,I, YC',I. If these cells are all different, then it is easy to see that YC,I = YC,J ' and YC',I = YC',J , and therefore that K = K'. This proves that J >٨ K and J ' >٨ K, as required. Suppose now that these cells are not all different.
If among these there is a P -cell for some predicate symbol P , say that
l = P (t1,... , tn), then l' involves the same cell c = cellI(P (t1,... , tn)). If
l' is positive, then YC,I = YC',I, hence J = J '. If l' is negative, then
 C')J = C)J ' = {⊥}, which is impossible since T ∈ J or T ∈ J '.
If none of these cells correspond to a predicate symbol, then l and l' are equations or disequations, and there remains to consider the rules (iii) to (v) of Definition 2.7. Let c, d (resp. c, d') be the cells involved in l (resp. l'). By

hypothesis we have c /= d (since f =/
J ) and c /= d', but we may have d = d'.

Let L = K YC,K and L' = K'  YC',K' , we clearly have J >٨ K >٨ L and
J ' >٨ K' >٨ L'. We will join J and J ' at least on L = L', or eventually on
K = K', as illustrated below.
/< J zzz,

C/	'
/
K zCzz,
L

f 
' 
J
' cCccs K'
cccs L'
C'

We now investigate all possible cases for l and l'.
If l and l' are both equations, then rule (iii) applies in the reductions f > J and f > J ', and also in the (eventual) reductions J >٨ K >٨ L and J ' >٨ K' >٨ L'. By assuming that C is unit in J ' and K, and that C' is unit in J and K', we easily compute the values of J , J ', K, K', L and L' on the cells c, d and d' by applying rule (iii). We let A = f[d] ∩ f[c] ∩ f[d'],

and we have
which proves that L = L'. We now have to consider the case where one of C, C' is not unit in a corresponding interpretation. Since C and C' are unit in f, and we consider refinements of f, these clauses may no longer be unit only by becoming true or false.
If they become true, say C)J ' = {T}, obviously because l)J ' = {T}, then J '[d] and J '[c] must be the same singleton {v}, and then we have K' = J ' (because YC,J ' = ∅), so that K'[d] = K'[c] = {v} = J '[d] ∩J '[c]. This means that if clause C or C' becomes true, the formulas above are still valid, and still prove L = L'.
We now prove that C and C' cannot become false. First, since K[c] ⊆
K[d], we have K[c] ∩ K[d] /= ∅, so that l)K /= {⊥}. Hence C is not false in K, and symmetrically C' is not false in K'. Next, we have J '[d] ∩J '[c] = J [c] ∩ J [d'] = A, hence if either  C)J ' or  C')J equals {⊥}, then they

are both equal to {⊥}, so that J = hypothesis.
J^' = {⊥}, which is impossible by

If l is an equation, and l' a disequation, then as above rule (iii) applies to
f > J , and either rule (iv) or (v) applies to f > J ', so we have either
|f[c]| =1 or |f[d']| = 1. We first suppose that f[c]= {v}, then by rule (iv) or (v) we have

so that K = K'. Note that v ∈ f[d], since otherwise YC,I would be empty,


hence J = f, which is impossible since f > J . We also know that C is unit in J ' and C' is unit in J , as they are unit in f, since J [c]= J '[c]= f[c], J [d']= f[d'] and J '[d]= f[d].
We next suppose that f[d'] = {v}. Since C' is unit in f, we must have
v ∈ f[c], which cannot be a singleton. Let A = f[c] ∩ f[d], since C is unit in f we also know that A /= ∅. We have by the same rules as above
J [c]= J [d]= A,	J [d']= {v},
J '[c]= f[c] \ {v}, J '[d]= f[d], J '[d']= {v}.
The clauses C and C' may not be unit in J ' and J (respectively). In particular we have
'	'
 l)J ' = {⊥} ⇔ J [c] ∩J [d]= ∅ ⇔ A \ {v} = ∅ ⇔ A = {v},
 l')J = {⊥} ⇔ J [c]= {v} ⇔ A = {v},
hence l or l' are false in J or J ' only if J = J' = {⊥}, which is impossible (thus A \ {v} /= ∅). We consider now the case where l is true in J ':
 l)J ' = {T} ⇒ J '[c]= J '[d]= {v'} where v' /= v
⇒ f[c]= {v, v'} and f[d]= {v'}
⇒J [c]= J [d]= {v'}
⇒J = J ',
and we are done. We may now assume that C is unit in J ', and thus apply rule (iii), yielding
K'[c]= K'[d]= J '[c] ∩J '[d]= A \ {v}, K'[d']= {v}.
We now consider the case where l' is true in J :
 l')J = {T} ⇒ v ∈/ J [c]= A ⇒ A \ {v} = A ⇒ K' = J ,
and we are done. We assume now that v ∈ A, hence that C' is unit in J , and rule (iv) or (v) applies:
K[c]= J [c] \ {v} = A \ {v}, K[d]= J [d]= A, K[d']= {v}.
We then see that C is still unit in K, since K[c] ∩ K[d] = A \ {v} /= ∅ and
K[d]= A is not a singleton. By rule (iii) we have
L[c]= L[d]= K[c] ∩ K[d]= A \ {v}, L[d']= {v},
and we get L = K'.
The last case to consider is when l and l' are both disequations. Then rules
(iv) or (v) apply to both f > J and f > J ', and we have either |f[c]| =1 or |f[d]| = |f[d']| = 1. We first suppose that f[c]= {v}, thus
J [c]= {v}, J [d]= f[d] \ {v}, J [d']= f[d'].


Note that C' is unit in J as in f since J [c] = f[c] and J [d'] = f[d'], so that
K[c]= {v}, K[d]= J [d]= f[d] \ {v}, K[d']= J [d'] \ {v} = f[d'] \ {v} Symmetrically we get J '[d']= f[d'] \ {v} and J '[d]= f[d], and K' = K, so we are done.
We next suppose that f[d]= {v} and f[d']= {v'}, so that J , J ', K and K' may differ only on c. We have J [c]= f[c] \ {v} and J '[c]= f[c] \ {v'}, and since C and C' are unit in f we must have v, v' ∈ f[c].
Note that we may not have f[c] = {v, v'}, because we would then have
J [c]= {v'} and J '[c]= {v}, thus C')J = C)J ' = {⊥}, which is impossi- ble. If v = v' then J = J ' and we are done, so we assume that v /= v', so that neither J [c] nor J '[c] can be a singleton. This means that C' must be unit in J , and C unit in J '. We therefore have K[c]= f[c] \ {v, v'} = K'[c], thus K = K'.
 
