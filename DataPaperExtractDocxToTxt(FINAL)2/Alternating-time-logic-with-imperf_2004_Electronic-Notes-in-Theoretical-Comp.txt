Electronic Notes in Theoretical Computer Science 85 No. 2 (2004)
URL: http://www.elsevier.nl/locate/entcs/volume85.html  12 pages


Alternating-time logic with imperfect recall

Pierre-Yves Schobbens
Institut d’Informatique Facult´es Universitaires de Namur
Rue Grandgagnage 21
5000 Namur, Belgium


Abstract
We study here a variant of the alternating-time temporal logic (ATL) where each agent has a given memory. We show that it is an interesting compromise, rather realistic but with a reasonable complexity. In contrast, most models with perfect recall and imperfect information have an undecidable model-checking problem.
Key words: alternating-time temporal logic (ATL), imperfect recall, imperfect information, games, verification, temporal logic

Introduction
ATL [2] comes with a modality expressing that a group has a strategy to ensure a temporal property, assuming that each member of the group knows the complete state of the system at each step (complete information). The strategies synthesized by ATL are thus often unrealistic for systems where
global information is not easily accessible.
We propose instead to consider agents that have access to only part of the system state, called their information. This hypothesis is thus called incom- plete information. It is natural for most problems; for instance, a network
node can only know the messages it receives, a robot can only perceive its immediate surroundings, etc. Complete information is slightly different from perfect information, that assumes that all choices of the past are known, but as we will see that this difference is inessential in our setting.
With incomplete information,many authors assume perfect recall [7,10], i.e. that each agent can use the complete history of its information. For practical reasons, we propose here to consider also imperfect recall: the memory of each agent is described explicitly. This does not necessarily mean that what the
agent will record is fixed: we can provide an agent with actions to update its memory at will; the strategy will then decide what to record and when.
◯c 2004 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.


Example 1.1 Before delving into the formal definitions, we provide a simple example.
A banker B at his desk d knows the code c of the safe in the vault v. A robber R wants to open the safe o. He can threaten B with his gun. When threatened t, B is paralysed by fear and tells the code. R can dial any number on the safe when in the vault. R can try to open the safe when in the vault, but it will only open if the last dialled number n is the code s; otherwise the vault door closes, jailing R.
We represent the fact that B knows the code by a private variable con- taining the code. If we use ATL with complete information, all variables are accessible to all agents: thus the robber has a simple strategy to open the safe, since he can go in the vault and type the code that he (magically) reads from the brain of the banker. This strategy is synthesized easily by an ATL model-checker, but it looks unrealistic.
A more realistic model is provided by incomplete information and perfect recall: the robber can threaten the banker to learn the code, remember it, go in the vault, type the code. Unfortunately, such strategies are not computable in general.
Thus, we propose here to include in the model an explicit description of the memory of the agents. We can select two types of solutions for this example: If the robber is modelled as absent-minded, i.e. has not enough memory to store the code, he will not be able to open the safe, since he would have forgotten the code while going to the vault. If he has enough memory, the the perfect recall strategy also applies here.


Definitions
Given a signature, containing:
P , a set of atomic predicates;
Σ, a finite set of agents (also called modules [1] or players);
An imperfect information concurrent game structure (iCGS),or game,consists of:
Q, a finite set of system states;
q0 ∈ Q, the initial state;
C, a finite set of choices (sometimes called actions [10] or alternatives);
e ⊆ Q × Σ × C, a guard that indicates whether an agent is enabled to take a choice in a given state; at least one choice must be enabled in any state for each agent: ∀q ∈ Q, a ∈ Σ, ∃c ∈ C, e(q, a, c).
δ : Q × CΣ → Q, a transition function; we assume all agents decide their choice at the same time, and the transition function gives the next state obtained by the interaction of their choices.


∼: Σ → 2Q×Q, a family of equivalence relations, one per agent, indicating the states that are undistinguishable from its viewpoint. For consistency, we assume that each agent knows which choices are open to him: ∀q ∼a q', e(q, a, c) ⇐⇒ e(q', a, c).
π : Q → 2P , a valuation giving the propositions that are true in a state.
∼a determines equivalence classes on Q: Given a state q, we note [q]a its equivalence class (the view of agent a). Qa is the set of such views [q]a. In many systems, the state is structured as a valuation for a given set of typed variables.  Often, the agent a can only observe a subset ra of the variables
called its read variables. [q]a can then more concretely understood as the
value of the variables in ra, which is a part of the global state q. When these variables are also writable, i.e. if the agent has choices that allow him to set the variable to any value he likes, the agent can use them as its memory.
In this paper, we will consider four types of strategies. In all cases, they will determine the choice of an agent, as a function of his view of the past: fa : V → C. This choice must be enabled.
The model of perfect information and perfect recall, noted IR, is the usual model of ATL. The view of the past V = Q+ is the sequence of all states that the system has been through until now.
The model of perfect information and imperfect recall, noted Ir. The view of the past V = Q is limited to the present state of the system.
The model of imperfect information and perfect recall, noted iR. Only a designated part of the information, described by ∼a, is available to each agent. Thus V = Q+.
The model of imperfect information and imperfect recall, noted ir, that deserves more attention, we claim. An agent can only base its choice on its view of the current state, V = Qa.
Note that all such strategies can also be viewed as functions from Q+, that satisfy the supplementary constraint that sequences that are undistinguishable (e.g. if they have the same present in an imperfect recall model) yield the same choice.
An XY (where XY is IR, iR, Ir, or ir) strategy fΓ for a group of agents Γ ⊆ Σ is a family of XY strategies (fa)a∈Γ, one for each agent. A computation (sequence of states) λ = q0, q1,... is an outcome from q of a strategy fΓ if q = q0 and for each i > 0, there are ca ∈ C, for a ∈ Σ \ Γ, such that qi+1 = δ(qi, c) where ca = fa([qi]a) if a ∈ Γ, and ca = ca for the opponents. So the outcomes are the computations that follow the strategy. The set of outcomes is noted out(q, fΓ).
The syntax of ATL* is given by:


ϕ ::= p |T| ¬ϕ | ϕ1 ∨ ϕ2 | ⟨⟨Γ⟩⟩ϕ1 | ϕ1 U ϕ2 | X ϕ1


where p ∈ P is an atomic predicate and Γ ⊆ Σ is a set of agents. We will also use the usual logical and temporal abbreviations, we write F ϕ for TC ϕ and G ϕ for чF чϕ, and A φ for ⟨⟨∅⟩⟩φ.
The semantics of ATL* is an extension of the temporal logic CTL*; we assume a fixed iCGS:
λ ▶ p iff p ∈ π(λ[0])
λ ▶ T, always
λ ▶ чϕ iff λ /▶ ϕ
λ ▶ ϕ1 V ϕ2 iff λ ▶ ϕ1 or λ ▶ ϕ2
λ ▶ ⟨⟨Γ⟩⟩XY ψ iff there exists a set of XY strategies FΓ, one for each agent in Γ, such that 6a ∈ Γ, 6q' ~a λ[0], 6λ' ∈ out(q', FΓ) we have λ' ▶ ψ
λ ▶ ψ1 C ψ2 iff Ei ≥ 0.λ[i, ∞] ▶ ψ2 and 60 ≤ j < iλ[j, ∞] ▶ ψ1.
λ ▶ X ϕ iff λ[1, ∞] ▶ ϕ
[2] gives a different semantics to ⟨⟨Γ⟩⟩:
q ▶ ⟨⟨Γ⟩⟩φ iff
there is a strategy FΓ such that all outcomes λ ∈ out(q, FΓ) satisfy φ
which has the counter-intuitive effect that, because FΓ is the scope of q, we can make the strategy dependent on q, which amounts to give perfect information on the initial state, even for imperfect information models.
Example 2.1 For our robber example, for each code c consider the strategy of going in the vault and typing code c. This strategy is successful when the code is indeed c. For each state q, we would thus have a successful strategy for the robber with the definition of [2].
ATL is the subset of ATL* where each temporal operator (X , C ) is im- mediately preceded by a cooperation modality (similarly to CTL). ⟨⟨Γ⟩⟩-ATL is further restricted to contain a single cooperation modality ⟨⟨Γ⟩⟩. ATL+ is the subset of ATL* where each temporal operator (X , C ) can occur in a
boolean combination, preceded by a cooperation modality (similarly to CTL+ [4]). In [6], we show that ATL+ has the same expressivity as ATL, but can be exponentially more succinct.
Example 2.2 We will use the Mocha [1] conventions to describe our example: We describe the set of states Q as a set of valuations for finite-domain variables. For this example, the variables are listed in Table 1. The choices C of each agent will be listed as assignments. To ensure compatibility of the choices,
each agent controls disjoint subsets of the variables, and he can only modify
his controlled variables. Then the transition δ is defined as a usual assignment. To describe the undistinguishability relation ~, each agent has a set of read variables. Two states that differ by non-readable variables only are deemed undistinguishable.


Table 1 Variables of the example
module B /* Banker */ controls LB, m
reads c, LB, t, LR update
[] чt → LB := v /* go down in the vault */ [] чt → LB := d /* go up at the desk */
[] t & LR=LB → m := c /* tell the code */ endmodule
Fig. 1. The banker
module R /* Robber */ controls LR, n, o, j, t init
[] LR := d, j := false update
[] LR := v /* go down in the vault */ [] LR := d /* go up at the desk */
[] LR=LB → t := true /* threaten the banker */ [] LR=v → n := ? /* type any number he likes */
[] LR=v → o := (n=c), j := ч(n=c) /* try to open, might jail */ endmodule



Complexity
Fig. 2. The robber

Incomplete information is usually more complex than complete information. But imperfect recall, although restricting even more the capabilities of agents, usually reduces complexity. For the logics considered here, the model-checking problem is undecidable for perfect recall but incomplete information, and im-


perfect recall makes it decidable again.

NP	complete for nondeterministic polynomial time
∆2P = PNP	complete for polynomial calls to an NP oracle
∆ P = PNP NP	complete for polynomial calls to a Σ P oracle EXP	 complete for deterministic exponential time
DEXP	complete for deterministic doubly exponential time U	undecidable
l	size of the formula
n	size of the model
Table 2 Complexity of model-checking
This shows a second possible use of imperfect recall: If we are interested in iR, we can use IR and ir as cheap approximations from above and below:
⟨⟨Γ⟩⟩irφ ⇒ ⟨⟨Γ⟩⟩iRφ ⇒ ⟨⟨Γ⟩⟩IRφ
When their value agree, we have an answer for iR,in spite of its undecidability.

NP-completeness
A ⟨⟨Γ⟩⟩ir-ATL formula is of the form ⟨⟨Γ⟩⟩irφ with φ starting with a temporal operator. We nondeterministically choose a strategy by selecting a subset of the transition relation: we choose an action for each a ∈ Γ and [q]a ∈ Qa. Then we check the CTL formula A φ for the structure so created, where A is the CTL modality quantifying on all paths of the system. This CTL check is done in time linear to the structure and to the formula [3]. So for each state formula, we make a number of choices limited by the size of the transition, and
perform only polynomial operations. The total number of choices is thus still polynomial (|φ|.|Q|.|Σ|) and the time also (viz. |φ|2.|Q|2.|Σ|). This procedure returns a set of states on which the strategy is successful. To comply with our definition, we must then restrict to the states {q ∈ Q|6a ∈ Γ, 6q' ~a q, q' ∈ S},
which can be done in linear time.


For NP-hardness, we encode the SAT problem in the structure. We are given a set of clauses, written using predicates p1,... , pn. We introduce an agent per predicate, plus a dispatcher. A predicate agent will have two choices: to set his predicate to either “true” or “false”. The dispatcher agent will choose
for each clause, which literal to satisfy. For each clause (numbered c) we create a sub-structure with 3 layers: a single initial state qc, which is connected to a state per literal, and the transition is under control of the dispatcher.The
transition from a literal state is under control of the corresponding predicate agent. If it is a positive literal, the choice “true” will lead to qc+1, and the choice “false” will lead to the dead state qd. Conversely for a negative literal.
All literal states are in an equivalence class of the corresponding predicate agent. Said otherwise, predicate agents have no information when they have to play,and their strategy is thus just the choice of a truth value. The structure
will have a single initial state q0, a dead state qd, a success state qf , where f
is the number of the last clause plus one. There is a single logic predicate s (for success), true exactly in qf . The model checking problem will be to check whether q0 ▶ ⟨⟨Σ⟩⟩✸s, which is true iff the choices of the truth values gives a model of the clauses.
Note that we use a turn-based game with no opponent: all the complexity comes from the difficulty to make agents with imperfect recall cooperate.

Complexity of ATL
To show that the model-checking of ATLir is ∆2P -easy, we notice that we can evaluate the formulae bottom-up, starting with the state subformulae. If it is a boolean subformula, we return the set of states that satisfy it. Else, the formula is in ⟨⟨Γ⟩⟩-ATL, and we can use the procedure above. There are at
most linearly many calls to this NP procedure, giving a ∆2P algorithm.
We only conjecture that this problem is also ∆2P -hard; it is at least NP - hard by the proof above.

Complexity of ATL+
We now show that model-checking an ATL+	formula is ∆3P -complete, for
XY = ir, Ir or IR. Let us recall that ∆ P = PNP NP is in the polynomial hierarchy. For the easiness, we can use the fact that CTL+ in in ∆2P = PNP [3, Thm 6.2]. We proceed bottom-up in the formula, thus linearly, each time replacing a state formula of the form ⟨⟨Γ⟩⟩φ (a modality, followed by a boolean combination of temporal formulae on boolean formulae) with a
new predicate, true in the same states. For each such a formula, we can choose non-deterministically an ir strategy, then we check this strategy for the corresponding CTL+ formula, with A replacing ⟨⟨Γ⟩⟩. This can be done in
PNP [3, Thm 6.2]. The polynomial part of PNP can on course be integrated in the NP choice of the imperfect recall strategy. We must call this for every cooperation subformula (at most n times, in P ) yielding a PNP NP algorithm.


We can also use this algorithm for ATL+ : it is just a special case where ~ is the identity. It also works for ATL+ , since it is translatable to ATL [6] and in ATL winning strategies without recall are sufficient [2], so that it is enough
to enumerate Ir strategies.
For the hardness, we encode the typical ∆3P -complete variant of SAT,

into an ATL+
model-checking problem.

A ∆3SAT problem is given by sequence of quantified boolean formula of the form: zi = EXi6Yiφi(Xi ∪ Yi ∪ {zj|j < i}), where each φi is a boolean
formula using former variables zj,and locally quantified variable Xi, Yi. We as- sume that all variables are distinct. Let Xi = {x1i,... , xnii}, Yi = {y1i,... , ymii}. We encode valuations very directly as paths in a game with two players {X, Y }.
At the initial state zn, player A chooses the value of x1n. This can lead to two
states, called x+ and x− . There, X chooses x2n,and so on. This is followed by
Y choosing the values of the yin. Then there is a single state labeled zn−1, and so on until the last state z0 which has a self-loop to comply with the definition of an iCGS. Predicate pi ∈ Xi, Yi are true exactly in node p+. We introduce a
predicate zi true in node zi. We transform the formulae of the SAT problem
into the ATL+ formula ⟨⟨X⟩⟩φn(F xin, F yin, ⟨⟨X⟩⟩φn−1(F xin−1,Fyin − 1), ⟨⟨X⟩⟩φn−2). Thus, the quantifications EXi6Yi are replaced by ⟨⟨X⟩⟩, and the basic predi-
cates have an F prepended. The zj are directly replaced by their definition. These replacements might make the textual form of the formula exponen- tial in size, but fortunately the complexity actually depends on the size with subformula shared (the DAG form), which is still polynomial.



ATL* complexity
The PSPACE-hardness of ATL∗


model-checking is deduced from the fact that

LTL is PSPACE-complete and included in ATL∗.
The PSPACE-easiness is again obtained simply by enumerating the possi- ble ir-strategies, and solving a CTL∗ problem for each of them. This gives a PNP P SPACE = PSP ACE complexity.


Heuristics
A characteristic of NP-complete problems is that, although finding a solution (a strategy, here) is difficult in some cases, it is easy in other cases and it is always easy to check a solution. Model-checking ATLir is no exception to this. We thus provide here heuristic ways to construct ir strategies for ATLir.
A natural idea is to start from the perfect information strategies cheaply provided by ATL: they are constructed in time linear in the size of the struc- ture. We first present this construction.


Strategies with perfect information
The efficiency of this search is based on the fact that recall plays no role for ATL: IR = Ir [2]. Ir Strategies (also called positional strategies) exist whenever IR strategies exist.

Representing strategies
We can describe strategies in two ways: the usual centralised description uses one strategy for the whole group. It will be represented as a set of states φc for
each vector of choices c for A (one per agent). This is noted as	c(φc ∧c), read
“when in φc, you can do c”. The usual computation makes the φcs disjoint (for a deterministic strategy). However, it will be our interest to have the φc as large as possible, so that we will allow overlap. The distributed description

uses one strategy fa per agent a, each represented as a set of states φa
for

each choice c of a, saying when a will choose c. When no overlap is present,

we can switch between the two representations by defining: φc = 
 

c∈c
φa, and

a
c	c∈c
φc. However, in case of overlap, the two representations are not

equivalent. It might be that only some combinations of individual choices lead
to success. This dependency between the choice of several agents cannot be represented in distributed form.

Computing strategies
The algorithm proceeds in ascending order on the subformulae of the given formula. Only the cooperation subformulae are of interest:
For φ' = ⟨⟨Γ⟩⟩(θ1 C θ2): The computation of these strategies can be per- formed by a backward search, starting from θ2, and using only θ1 states. We compute a new winning region r, by computing the states of θ1 where a choice for each member of Γ will guarantee to be at next step in r ∪ θ2. At step n, r is the set of states that can reach θ2 in n steps while staying in θ1. In the strategy f , we need to avoid loops, so we record the choices only for new r states, that were not in r at the previous iteration. (This easy
condition sometimes removes choices that were not looping.)
At the end, r ∪ θ2 is the winning region, and thus the states where φ' is true. r is the domain of f , and on θ2 any strategy can be followed, and is winning. Outside of the winning region, any strategy can be followed, since
A will loose anyway.
We call the strategy computed above the nondeterministic shortest path strategy, or S-strategy.
For φ' = ⟨⟨Γ⟩⟩(G θ2): A greatest fixpoint be computed; Here, loops are included in this computation.
r := θ2
repeat
r = Pre(A, r) ∩ θ2


until r stabilizes
f = {Pre(c, r) ∧ c}

Pre(A, r) computes the controlled predecessors for all choices of A. The strategy simply keeps the successors inside the winning region r. Here, we obtain a most general nondeterministic strategy.

Strategies with imperfect information and recall
The strategies obtained above are Ir strategies, so we have to suppress in

the conditions φa
information that is not available to the agents that must

execute the choices. We define an operator Ka : 2Q → 2Q that restricts φa
to a stricter condition that only uses information available to a: Ka(φ) =
{s ∈ φ|6s' ∈ I, s' ~a s ⇒ s' ∈ φ}, where I is the set of states reachable from the initial state q0. Given a distributed deterministic strategy φa → c, where c is a choice of a single agent a, Ka(φa) → c is an imperfect information strategy. However, this technique is often too restrictive: φa is first restricted
to have a deterministic strategy, and Ka(φa) is even smaller, often empty even though there are winning ir strategies. To reduce this risk, we start from centralized nondeterministic strategies, as given by the algorithm above. We complete it by including all choices when we are out of the winning region,
giving a strategy sc. We can first include in the strategy safe choices: choices
such that, whatever the other agents choose, their combination with the given choice c is inside the centralised strategy. The condition for c to be a safe choice of a is given by Ka((6C/a=sc|c).
Note that this technique is only heuristic and neglects several phenomena:
The agents in a are willing to cooperate, but here we consider them as behaving randomly, because they might not have enough information to act adequately.
The strategies computed above make no effort to transmit information among agents.
Also, the agents could cooperate for remembering relevant parts of the past; again, the strategies above do not try this.

Conclusions
We have studied various possibilities for the model of agents: perfect or im- perfect information, and perfect or imperfect recall. Although adding perfec- tion yields more powerful agents, as expected, perfect information make the model-checking problems easier while perfect recall makes them more diffi- cult. Imperfect information and recall seems a reasonable compromise, with realistic modelling powers and decidable problems.
A number of problems had to be left for future work:


Axiomatization: The new modalities ⟨⟨Γ⟩⟩XY have a different semantics, so that they obey different axioms from the ones of ATL [5]. They are still to be discovered.
Experimentation: Many more heuristics than proposed here are possible. They should then be compared, and also with general heuristics, for instance SAT solvers.
Models of the adversary: the ⟨⟨Γ⟩⟩ modality must ensure its result whatever the adversary does, including unlikely lucky choices. It might be useful to quantify instead on strategies of a more realistic adversary, himself with imperfect information or imperfect recall.
Knowledge: The knowledge of agents [8] plays an essential role in this pa- per, yet is never explicit in the logics we consider here. Adding an explicit knowledge operator seems useful, but what should be its meaning? Addi- tional knowledge can be brought by the strategy used by the cooperating agents, but depends on which strategy is used. It seems thus necessary have a logic where strategies are explicit.

Acknowledgement
Thanks to Wiebe van der Hoek and Mark Ryan for first discussions on the future work mentioned in the conclusion.

References
R. Alur, T. A. Henzinger, F. Y. C. Mang, S. Qadeer, S. K. Rajamani, and
S. Tasiran. MOCHA: Modularity in model checking. In Proc. 10th International Computer Aided Verification Conference, pages 521–525, 1998.
R. Alur, T.A. Henzinger, and O. Kupferman. Alternating-time temporal logic. In Proceedings of the 38th Annual Symposium on Foundations of Computer Science, pages 100–109. IEEE Computer Society Press, 1997.
E. M. Clarke, E. Allen Emerson, and A. P. Sistla. Automatic verification of finite state concurrent systems using temporal logic specifications: A practical approach. ACM Transactions on Programming Languages and Systems, 8(2):244–263, 1986.
E. Allen Emerson and Joseph Y. Halpern. Decision procedures and expressiveness in the temporal logic of branching time. Journal of Computer and System Sciences, 30:1–24, 1985.
Valentin Goranko and Govert van Drimmelen. Complete axiomatization of the alternating-time temporal logic, 2003.
Aidan Harding, Mark Ryan, and Pierre-Yves Schobbens. Approximating ATL∗ in ATL. Lecture Notes in Computer Science, 2294:289–??, 2002.


H. W. Kuhn. Extensive games and the problem of information. In H. W. Kuhn and A. W. Tucker, editors, Contributions to the Theory of Games II, pages 193–216. Princeton University Press, Princeton, NJ, 1953.
John-Jules Ch. Meyer and Wiebe van der Hoek. Epistemic Logic for AI and Computer Science. Cambridge University Press, Cambridge, 1995.
R. Rosner. Modular Synthesis of Reactive Systems. PhD thesis, The Weizmann Institute of Science, 1992.
Ron van der Meyden. Axioms for knowledge and time in distributed systems with perfect recall. In Logic in Computer Science, pages 448–457, 1994.
M. Yannakakis. Synchronous multi-player games with incomplete information are undecidable, 1997. Personal communication (reference reproduced from [2]).
