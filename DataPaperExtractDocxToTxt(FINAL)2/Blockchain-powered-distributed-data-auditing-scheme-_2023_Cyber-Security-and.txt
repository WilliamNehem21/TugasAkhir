Cyber Security and Applications 1 (2023) 100017

		




Blockchain-powered distributed data auditing scheme for cloud-edge healthcare system
Yi Li a, Meiqin Tang b,∗
a Nanjing University of Information Science and Technology, No. 219, Ningliu Road, Pukou District, Nanjing, 210044, China
b Wuxi Vocational College of Science and Technology, No. 8 Xinxi Road, Wuxi, 214029, China


a r t i c l e	i n f o	a b s t r a c t

	

Keywords:
Data auditing Healthcare system Blockchain
Cloud-edge computing
Cloud-edge healthcare system provides storage and computing functions at the hospital servers, bringing low latency for doctors and patients. However, hospital servers cannot be trusted and have limited computing re- sources. Data integrity verification for the cloud-edge healthcare system is an urgent concern. To this end, we proposed a data integrity auditing scheme based on blockchain. First, a distributed data integrity verification method without a third-party auditor is designed. The data are divided into smaller parts and hashed into a hash table. The verification tag is constructed according to the column of the hash table and secret string generated by a pseudo-random function. Then, a detailed blockchain-based data integrity auditing scheme is proposed, in- cluding Proof of Auditing Frequency and block structure. Besides, a security analysis for the common attacks is given. Finally, the proposed scheme is evaluated against two start-of-the-art schemes in a simulated cloud-edge healthcare system. The results demonstrate that the proposed scheme can verify data integrity without losing eﬃciency.





The healthcare system is an actual demand for society. If data has to be shared among multiple healthcare organizations, cloud-edge com- puting will be prominent under architecture. Based on this precondi- tion, blockchain can be a technology to support tamper proofing and anti-collusion. Different hospitals tend to cooperate and share medical data for better medical and diagnostic services. Cloud computing [1] ar- chitecture can serve as a straightforward solution. Users in various hos- pitals, such as doctors and patients, can directly upload data to the cen- tral cloud server to share medical data. However, the centralized cloud computing model faces the problems of a single point of failure and high latency between some users and cloud servers. For this reason, medical systems with cloud-edge computing architecture are favored [2]. While the hospital servers acts as an edge servers, users directly store data with the nearest hospital server, as shown in Fig. 1.
Nothing is perfect. Cloud-edge healthcare system brings the new problem of data insecurity [3]. The hospital servers cannot be trusted like the cloud server by users. Moreover, different hospital servers be- long to various organizations. They do not trust each other. Provable Data Possession [4] and Proof of Retrievability [5] are two classical data integrity verification methods for cloud computing. The schemes proposed after PDP and POR are basically within these two paradigms. Traditional data integrity verification methods for cloud computing are not suitable for cloud-edge healthcare system for two reasons: (1)Third- party auditors (TPAs) for auditing are not ideal for the distributed char-
acteristics of the cloud-edge healthcare system. (2) The hospital server has limited computing resources. In a nutshell, verifying the data in- tegrity for the cloud-edge healthcare system is the problem we want to figure out in this paper.
Blockchain provides a possible direction for data integrity audit- ing for cloud-edge healthcare system [6]. Each hospital server can become a blockchain node to join the consensus and maintain a distributed ledger for data integrity auditing. Intuitively, cloud-edge computing is a prominent architecture for a data-shared healthcare sys- tem, and blockchain is a technology to support tamper-proofing and anti-collusion.
However, there are still pending solutions to establish a decentral- ized architecture among the hospital servers, how to guarantee the ef- ficiency of distributed data integrity verification, and how to defend against the threats of malicious hospital servers. To this end, we pro- posed an eﬃcient data integrity auditing scheme based on blockchain. Our main contributions are as follows:

A distributed data integrity verification method without third-party auditor is proposed. Based on the constructed hashTable, each hos- pital server can audit the data in a distributed manner.
A blockchain-based auditing scheme is designed for cloud-edge healthcare system, which includes the consensus protocol Proof of Auditing Frequency (PoAF) and related block structures.

∗ Corresponding author.
E-mail addresses: 20211155002@nuist.edu.cn (Y. Li), tmq220@163.com (M. Tang).

https://doi.org/10.1016/j.csa.2023.100017
Received 20 November 2022; Received in revised form 7 March 2023; Accepted 3 April 2023
Available online 6 April 2023
2772-9184/© 2023 The Authors. Published by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)




Fig. 1. Edge Computing Architecture.


The proposed scheme is evaluated against two start-of-the-art schemes in a simulated cloud-edge healthcare system and compre- hensively evaluated.
The rest of this paper is organized as follows. The related work is discussed in Section 2. The proposed scheme is given in Section 3 specif- ically. The results of experiments and the performance evaluation of the proposed scheme are shown in Section 4. Finally, Section 5 summarizes this paper and points out our future plan.

Related work

Currently, there is no direct research on cloud-edge computing med- ical systems data auditing. Therefore, this section will focus on the re- lated work of cloud computing data auditing and cloud-edge computing data auditing.

Cloud computing data auditing

The first data auditing mechanism called Proofs Of Retrievability (POR) is proposed in 2007 [7]. This scheme allows users to verify the integrity of outsourced data to ensure safe storage of data, but its disad- vantage is that it cannot be publicly audited. Ateniese et al. [4] propose Provable Data Possession (PDP). A third-party auditors verify the in- tegrity of data on behalf of users, reducing the cost of users. In practical applications, the demand for dynamic operation is more common. Ate- niese et al. [8] propose a scalable PDP scheme, but this method only supports part of dynamic operation. Shacham and Waters [9] improve the POR model, combined with homomorphic authentication technol- ogy to reduce communication overhead. Erway et al. [10] introduce a rank-based authentication skip table and propose the first data in- tegrity audit scheme that supports full dynamic updates. Wang et al.
[11] propose a new data dynamic update integrity verification scheme based on Merkel Hash Tree (MHT). This scheme supports public audit- ing and reduces some overhead, but cannot resist replacement attacks. Mao et al. [12] improve MHT and propose a position-aware Merkle Tree (PMT), with position information in each node to resist replacement at- tacks. Rao et al. [13] design a dynamic audit scheme based on batch authentication MHT to solve the anti-collusion problem. Bowers et al.
[14] design a scheme based on POR. Its authentication does not need to decrypt the file. Yuen [15] propose an auditable consortium chain.
Users participate in transactions after being authenticated by the autho- rized party, preventing malicious users from abusing privacy attributes, and can track malicious users. Gao et al. [16] propose a cloud data audit scheme that supports data deduplication to achieve low-entropy secu- rity. Malicious clouds cannot forge validators to pass audits, and users do not need to interact with third-party auditors. Xu et al. [17] propose a public cloud auditing scheme that periodically updates the audit au- thorization code to prevent malicious clouds from tampering with file keys.

Cloud-edge computing data auditing

For edge servers, a collaborative model has to be established. Li et al. propose an app vendor’s cache data such as VR video integrity for the edge computing environment [18]. They divide the treats into unexpected failures and cheating attacks. They give their scheme around correctness, lightweight, and security objectives, including a light sampling-based probabilistic approach and a new data structure named variable Merkle hash tree. Then, in [19], they propose a coop- erative scheme. A distributed consensus is employed to form an edge caching system. All the edge servers are responsible for guaranteeing data integrity. In addition, in [20] and [21], the privacy and eﬃciency are improved respectively. Li et al. have achieved remarkable results by focusing on solving the edge data integrity problem of application manufacturers. However, the above scenarios all focus on a single ap- plication vendor’s data integrity verification needs. Each edge server is responsible for storing many application vendors’ data. Application ven- dor servers have more computing power and storage space than single- person servers and edge servers. Therefore, these methods are unsuitable for heterogeneous edge servers to provide personal services.
In [22], a blockchain-based framework is proposed without TPA for data integrity verification. Merkle tree-based validation methods and sampling strategies contribute to the framework. In addition, a proto- type is implemented on Ethereum [23] by deploying smart contracts [24], which may not be suitable for resource-limited edge servers. Fan et al. propose a consensus for a blockchain-based data integrity frame- work [25]. They divide the framework into two-layer: the private part and the edge part. The private part maintains a private chain for IoT devices, while the edge part is a public chain constructed by edge servers. Their verification method is similar to Yue et al. [22], based on



the Merkle tree [26]. Two blockchain-based data integrity verification frameworks in [22] and [25] adopt a similar way based on the Merkle tree and sampling. The success rate of verification cannot be guaranteed. To this end, we design novel scheme to improve the success rate.
Therefore, it is urgent to design a blockchain-powered distributed data auditing scheme for the healthcare system in this paper.


Blockchain-powered distributed data auditing scheme

Scheme overview

The proposed system framework has three roles: users, hospital server, and cloud server. Users, such as doctors and patients, can be trusted. They upload their data to the nearest hospital servers and re- quest the service of data integrity verification. Hospital servers cannot be trusted. They save the data for the users, conduct the process of data integrity verification, and attend to the blockchain consensus. The ma- licious edge servers try to hide the fact of losing data. The cloud server is semi-trust. It is only responsible for setting the initial system param- eters and generating the genesis block. Besides being distributed with- out a trusted third party, blockchain can also endue the advantages of tamper-proofing and anti-collusion.
The notation used in this section is shown in Table 1. The scheme includes 3 phase, as shown as follows:

Phase 1: Hospital servers and the cloud server generate their pub- lic keys and private keys. Then they hash their public keys as their addresses and broadcast the public keys and addresses to the other servers. Once the cloud server receives all the public keys of hospital servers, it will assign an order to the hospital servers and generate the first block of the blockchain, in other words, the genesis block. The genesis block includes the timestamp, edge servers order, ad- dresses, and public keys. Then the cloud server sends the genesis block to all hospital servers.
Phase 2: The user (doctor or patient) first generates the public and private keys and hash the public key as his address. Then the user processes the data for the verification information. After that, the user uploads the data to the nearest hospital server and sends the verification information to the blockchain. After consensus, the ver- ification information is stored on the blockchain between all the edge servers.
Phase 3: The user (doctor or patient) requests the service of data integrity verification. The hospital servers validate the data integrity in a distributed way according to the verification information stored
Tag generation
Due to the resource limitation of the edge servers and blockchain, we construct the verification tag of data based on hash function and
pseudo rand function. First, the data is partitioned into 𝑁 data blocks,
represented by 𝐹 = {𝑓1, 𝑓2, … , 𝑓𝑖, … , 𝑓𝑚}. Then, the 𝑉 𝑇 matrix is con- structed with a size of (𝛽1 × (𝛽2)). The construction of 𝑉 𝑇 is shown in
Algorithm 1.

Algorithm 1 Tag construction for data F.
1: InpUt: 𝐹 = {𝑓1, 𝑓2, …, 𝑓𝑖, …, 𝑓𝑚}, 𝑔, 𝑁, 𝛽1, 𝛽2, 𝐻1(), 𝐻2(), 𝑃 𝑅𝐹 (), 𝑘
2: OUtpUt: 𝑉 𝑇
3: 𝑆 = 𝑃 𝑅𝐹 (𝑘)
4: Initial ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 as a binary matrix with a size of 𝑚 × 𝛽1
5: for 𝑖 = 1 to 𝑚 do
6:	Set the 𝑖th row of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 as 𝐻1(𝑔𝑠𝑖 +𝑓𝑖 𝑚𝑜𝑑 𝑁 )
7: Initial 𝑉 𝑇 as a binary matrix with a size of 𝛽1 × 𝛽2, elements 0
8: for 𝑖 = 1 to 𝛽1 do
9:	Set the 𝑡𝑒𝑚𝑝 as the 𝑖th column ofℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒
10:	Set the 𝑖th row of 𝑉 𝑇 as 𝐻2(𝑡𝑒𝑚𝑝)
11: Return 𝑉 𝑇

be found in Table 1. In line 3 to 6, the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 is generated according As shown in Algorithm 1, the explanation of inputs and outputs can to each data block and secret string 𝑠𝑖 of 𝐹 , 𝐻1(𝑔𝑠𝑖 +𝑓𝑖 𝑚𝑜𝑑 𝑁 . Then, in line 7 to 11 is for constructing 𝑉 𝑇 : first, retrieve the 𝑖th column of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 as 𝑡𝑒𝑚𝑝. Second, calculate 𝐻2(𝑡𝑒𝑚𝑝). Third, set the 𝑖th row of
𝑉 𝑇 as 𝐻2(𝑡𝑒𝑚𝑝).

Verification
After phase 1 - tag generation, verification can be carried out by the other hospital servers distributively. The procedure is shown in Algorithm 2.

Algorithm 2 Verification for data F.
1: InpUt: 𝑉 𝑇 , 𝑁, 𝑚, 𝐻2(), 𝛽1, 𝛽2, 𝑃 𝑅𝐹 (), 𝑘
2: OUtpUt: True of False
3: The hospital server calculates the 𝑆 = 𝑃 𝑅𝐹 (𝑘)
4: The hospital server reconstruct the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′
sampling                         𝑡 5: The validator (other hospital server) decides the times for
6: for 𝑖 = 1 to 𝑡 do
7:	The validator generates a random number (𝑟𝑎𝑛𝑑) from [1, 𝛽1]
8:	The validator requests the 𝑖th column of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′
9:	The validator calculates 𝐻2(ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′	)and compares with

on the blockchain. After consensus between the edge servers, the
user will get the result of the data integrity verification.


Distributed data integrity verification method
the 𝑉 𝑇
10:	if 𝐻2(ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′
11:	Return False
12: Return 𝑇 𝑢𝑟𝑒
) ≠ 𝑉 𝑇𝑟𝑎𝑛𝑑 do
𝑟𝑎𝑛𝑑



We design a distributed verification method including three main phases: Tag Generation, Verification, and Tag Updating.


Table 1
Notation.

As shown in Algorithm 2, the explanation of inputs can be found in Table 1. The output is the verification results: True or False. In line 3 to 4, the hospital server calculates the secret string and re constructs
the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′ by the current data file stored in it. In line 5 to 12, the
validator verify the data according the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′ and the 𝑉 𝑇 . First, the decides the times for sampling. Second, it request the 𝑟𝑎𝑛𝑑th column of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′, hashes this value, and compares with the 𝑟𝑎𝑛𝑑th row of 𝑉 𝑇 . Third, if the 𝑡 times sampling all pass the verification, it returns 𝑇 𝑟𝑢𝑒, otherwise returns 𝐹 𝑎𝑙𝑠𝑒. The Algorithm 2 can be carried out by different
hospital servers in parallel, that is, a distributed manner.

Tag updating
string kept by the user. As a result, the old 𝑉 𝑇 is invalid, and the user After carrying out verification, the hospital server knows the secret
has to update the verification tag. Due to the communication overhead,



it is impractical to resend the file to the user. Thus, we design a Tag Updating method without resenting the file, shown in Algorithm 3.

Algorithm 3 Tag Updating.
1: InpUt: 𝑔, 𝑁, 𝑚, 𝐻1 (), 𝐻2 (), 𝛽1 , 𝛽2
2: OUtpUt: 𝑉 𝑇 ′
3: The hospital server sends the 𝑔𝑓1 …𝑔𝑓𝑚 𝑚𝑜𝑑 𝑁 to user
4: The user chooses an new 𝑘′ and calculates 𝑆 ′ = 𝑃 𝑅𝐹 (𝑘′)
5: for 𝑖 = 1 to 𝑚 do
𝑠′ +𝑓𝑖
Step 3: The user generates an 𝑚-bit secret strings by 𝑃 𝑅𝐹 (𝑘) and attaches the 𝑚-bit secret string to the data file 𝐹 .
Step 4: The user processes the 𝐹 by the method in Tag Generation phase subsection and generates the tag 𝑉 𝑇 .
Step 6: The user sends the 𝑉 𝑇 to blockchain. For the upload block
structure, the block header of the upload block includes hash-
value of the previous block, the block height, the merkle root, the
of 𝑉 𝑇 . The Merkle root of the upload block is calculated deter- block type and timestamp. the block body consists of all columns
mined by the PoAF among the hospital server.

6:	Set the 𝑖th row of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 as 𝐻1 (𝑔 𝑖	𝑚𝑜𝑑 𝑁 )

7: Initial 𝑉 𝑇 ′ as a binary matrix with a size of 𝛽1 × 𝛽2 , elements 0
8: for 𝑖 = 1 to 𝛽1 do
9:	Set the 𝑡𝑒𝑚𝑝 as the𝑖th column ofℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒
10:	Set the 𝑖th row of 𝑉 𝑇 ′ as 𝐻2 (𝑡𝑒𝑚𝑝)
11: Return 𝑉 𝑇 ′

be found in Table 1. In line 3, the hospital server sends .𝑔𝑓𝑚 𝑚𝑜𝑑 𝑁 to As shown in Algorithm 3, the explanation of inputs and output can user. Then in line 4, the user chooses an new key 𝑘′ and generates new secret string 𝑆 ′. After that, the user constructs a new 𝑉 𝑇 and sends to
blockchain.

Blockchain design

Based on the proposed distributed verification method, the detailed blockchain design for could-edge computing is given out in this section. There are three phases: setup, consensus, upload, and verification.

Setup phase
In this stage, the cloud server has to determine the initialization, including the hash functions, the pseudo random function, the number of data blocks, the blockchain nodes, and the block structure. proposed scheme has three block structures: genesis block, data upload block, and data verification block. The genesis block is used for storing the public keys of blockchain nodes. The data upload block is for recording the data verification message. The data verification block is for aggregating the verification results generated by the blockchain nodes.

Proof of auditing frequency
To carry out the consensus eﬃciently for Blockchain, we design a novel protocol called Proof of Auditing Frequency (PoAF). The detailed illustration is shown as follows.
Step 1: Each hospital server verifies the data file for 𝑡 times.
Step 2: Hospital server sends the verification results to the other hos-
pital servers.
Step 3: The other hospital servers receive all the verification results and choose the hospital server with the most verification results as the ledger node for this round consensus.
Step 4: The hospital server with the most verification results gener- ates the block and sends to the other hospital servers.
Step 5: All the hospital servers update and sync the blockchain.

Data upload phase
After the setup phase, the cloud server first initializes the genesis block to start the blockchain network among edge servers. Then the user processes the data, uploads it to the nearest hospital server, and sends the tag to blockchain networks. The procedure is shown as follows:
Step 1: All the edge servers generate their private and public keys and send the public keys to the cloud server.
Step 2: The cloud server initializes the genesis block. The block header includes block height, Merkle root, and timestamp. The block body consists of all public keys of the edge servers.The Merkle root is calculated by the cloud server.
Step 7: The user uploads the data 𝐹 to the nearest edge server. With-
out the key 𝑘 of 𝑃 𝑅𝐹 (), the hospital server cannot construct the
ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 without the 𝑘 of 𝑃 𝑅𝐹 ().

Data verification phase
keeps the key 𝑘 of 𝑃 𝑅𝐹 (). When the user needs to verify the data stored After uploading the data to the nearest hospital server, the user only in the hospital server, he only has to send the key 𝑘 of 𝑃 𝑅𝐹 () to the
nearest hospital server and the blockchain maintained by the hospital servers returns the auditing result.The procedure is shown as follows:
Step 1: The user sends the validation request to the blockchain main- tained by the hospital servers.
Step 2: The user sends his key 𝑘 of 𝑃 𝑅𝐹 () to the nearest edge server
to reconstruct the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒. Based on the current data and the the key 𝑘 of 𝑃 𝑅𝐹 (), the edge server calculates the ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′.
Step 3: The blockchain nodes generate a random number 𝑟𝑎𝑛𝑑 be- tween [1,𝛽1] and request the 𝑟𝑎𝑛𝑑th column of ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒′.
Step 4: The blockchain nodes verify the sample according to the method introduced in section 4.2 and send the verification re- sult to the user.
Step 5: The user aggregates the verification results from all the blockchain nodes and send to the blockchain.
Step 6: The user requests the (𝑔𝑓1 … 𝑔𝑓𝑚 𝑚𝑜𝑑 𝑁 ) from the nearest
hospital server, changes the key 𝑘′ of 𝑃 𝑅𝐹 (), and regenerates new verification tag 𝑉 𝑇 ′ to blockchain for the next-time auditing.

For the verification block, the block header includes the previous block’s hash value, the block height, the hash value of the cascade of the block body, the block type, and the timestamp. The block body con- sists of the validated result of each hospital server. The hash value of the concatenation of the verification results of each blockchain nodes is determined by the PoAF. The data is stored on the nearest edge server by the users. The blockchain nodes (hospital servers) only store the ver- ification tag and verification results.

Security analysis

The malicious edge servers try to cover up the facts that the data has been lost or modified. Detailed security analysis for five malicious behavior is given out.

Sybil attack
In proposed scheme, the malicious nodes try to forge identities to attend the data verification. However, all the true identities are bound with their public key and saved on the blockchain. The malicious node cannot tamper with the information saved on the blockchain. Therefore, proposed scheme is resistant to the Sybil attack [27].

Forgery attack
posed scheme. The malicious node tries to forge a valid without key 𝑘 The forgery attack may happen on the data-held edge server in pro- of 𝑃 𝑅𝐹 () kept in the user. Due to the hash function’s one-way property
and the collision-free property, a malicious node can’t forge a valid. So, proposed scheme is resistant to the forgery attack [28].



Replay attack
the verified ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒 for the next-time verification. This attack does In proposed scheme, the malicious data-holder server tries to use
not work to the proposed scheme because the secret string is changed after one verification. In addition, all the communication messages have embedded the timestamp. Therefore, proposed scheme is resistant to replay attack [29].

Collusion attack
In proposed scheme, the adversaries can hide the fact of data loss or modification. In the experimental part, we show that the verification rate is near 100%, even a part of the malicious node exists. So, proposed scheme is resistant to the collusion attack [30].

Man-in-the-middle attack
The man-in-the-middle attack occurs when two parties exchange their public keys. In proposed scheme, all the public keys are stored in the genesis block and cannot be tampered. Therefore, proposed scheme is resistant to the man-in-the-middle attack [31].

Evaluation

To verify the effectiveness of proposed scheme, we built up a proto- type to evaluate its performance in a simulated cloud-edge environment.

Experiment environment and performance metrics

hash function [33] is used for 𝐻1() and 𝐻2(). 𝛽1 and 𝛽2 are 256. The The simulation is implemented with Python 3.6 [32]. An SHA-256
cloud-edge healthcare system environment includes 7 hospital servers and a cloud server with 2 malicious hospital server. Each edge server is equipped with 2 vCPUs, 2 GB memory, and 40 GB storage. The cloud server has 8 vCPUs, 16 GB memory, and 100 GB storage.
Two distributed data integrity verification methods for cloud-edge computing are compared [19,22]. For schemes in [22], the authors ver- ify the data integrity through a Merkle hash tree by sampling mode. For the scheme in [19], the authors hash the entire file to verify the data integrity. In the following part, scheme 1 and scheme 2 are used to rep- resent these two kinds of methods. We use NumPy, a site package of Python, to generate the data file with a matrix structure. In NumPy, an element in the matrix is represented by 64 bits. The minimum data mod- ification is 8 Bytes. The data varied form 128 MB to 2048 MB, which is suﬃcient for general healthcare system [34].
The following three metrics are used to evaluate schemes perfor- mance:
Success rate of corruptiong detection:
We excute the scheme for 1000 times to evaluate the success rate.
Time consumption:
The time consumption reflects the computing overhead, in which times for generating the tag generation and data verification are compared.
Communication overhead:
Communication overhead is the size of transmitted data respectively for tag generation, verification, and consensus.
The parameter settins are shown in Table 2.


Table 2
Parameter settings.




Fig. 2. Success rate of verification with data size varying.


Results of success verification rate

Figure 2 shows the success rate of verification achieved by 3 compet- ing schemes with the data size varying. The corruption size is 40 Bytes, and the corruption ratio is 0.004. The proposed scheme is performed in situations with and without malicious nodes.
As shown in Fig. 2, the success verification rate of the three schemes does not change with the data size increasing. Due to the similar corrup- tion size and ratio, the detection rate is the same.The verification success rate of scheme 1 is a little higher than 50% because of a 200-times sam- pling. Besides, scheme 2 and proposed schemes achieve a 100% success rate. The existence of malicious nodes does not result in adverse effects. When the data size is not more than 1024 MB, the hash operation for a whole data file can guarantee a 100% success rate for scheme 2. A hash operation on a 2 GB file reported a memory error because the memory
cupies a part of the memory. Therefore, scheme2′s experimental result of the experimental virtual machine is 2 GB, and the Ubuntu system oc-
is blank when the data file is 2 GB.
Figure 3 illustrates the impact of the corruption ratio (from 0.001 to 0.016) and corruption size (from 8 Bytes to 72 Bytes) on the success rate of verification with data size fixed at 512 MB. It can be seen that the corruption ratio impacts the success rate more significantly than the corruption size. An overall tendency is that the corruption size does not influence the success rate much for all schemes. Main reason is the one- way and collision-free properties of the hash function. For scheme 1, the success rate of verification increases when the corruption ratio in- creases. A higher corruption ratio makes it easier to sample the corrup- tion data block. For scheme 2, the success rate is 100% due to the direct comparison of the hash value of the data file. For proposed schemes, the success rate is near 100% when the corruption ratio is 0.001. Because of malicious behavior hiding data loss or modification, the success rate of proposed scheme with the malicious nodes is a bit lower than that without malicious nodes. However, the rate is still over 90%.

Results of time consumption

Figure 4 (a) shows that the time consumption for generating a tag is increasing as the data size. The data block size increases, resulting in the time consumption of calculating the hash value. Mainly, due to the memory limitation, scheme 2 cannot be applied with a data size of 2 GB. From Fig. 4(a), the overall time consumption of proposed scheme is much lower than the other two schemes. In Fig. 4(b), because the verifi- cation process is reconstructing the Merkle hash root, the time consump- tion of verification for scheme 1 remains almost the same. For scheme 2, the verification process is recalculating the hash value of the entire data file. So, the time consumption is increasing with more data size.





Fig. 3. Success rate of verification with corruption ratio varifying.





Fig. 4. Time consumption for different stage.




Fig. 5. Communication overhead for different stage.


Similarly, the result of scheme 2 with a 2 GB data file is blank due to the memory limitation.
Results of communication overhead

Figure 5 (a) shows that the communication overhead for tag generat- ing is similar when data size varifies. 8-KB communication overhead is acceptable for proposed scheme. In Fig. 5(b), the communication over- head for verification of scheme 1 is much higher than scheme 2 and proposed scheme. The reason is that scheme 1 adopt a 200-times sam- plings. A communication overhead of less than 1KB for verification is acceptable for proposed scheme. It can also be seen from these two fig- ures that the communication overhead of scheme 2 is lower than ours when the data size is less than 2 GB. The main reason that leads to these
hash value, but proposed scheme uses 256 bits × 256 bits hash table. In experimental results is that the tag information of scheme 1 is a 256-bits
addition, the communication overhead of scheme 1 is much higher than scheme 2 and ours due to the tag information of scheme 1 being a Merkle hash tree, whose space occupancy is much higher.
Conclusion

A blockchain-based scheme without a third-party auditor is proposed to accomplish data integrity auditing for the cloud-edge healthcare sys- tem. First, a distributed data integrity verification method is designed. A verification tag constructed by the hash table of the data file and secret string guarantees a verification rate close to 100%. Then, a specific de- scription of the proposed blockchain-based scheme is given, consisting of the PoAF consensus protocol and related block construction. Finally, the proposed scheme is compared with two state-of-art schemes. The re- sults show that our scheme can verify the data integrity effectively and eﬃciently for the cloud-edge healthcare system. In the future, we will further improve the distributed data auditing method and deploy it in a real blockchain environment.
Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
CRediT authorship contribution statement

Yi Li: Conceptualization, Methodology, Software. Meiqin Tang:
Data curation, Writing – original draft.
References

C. Lin, D. He, X. Huang, K.-K.R. Choo, OBFP: optimized blockchain-based fair pay- ment for outsourcing computations in cloud computing, IEEE Trans. Inf. Forensics Secur. 16 (2021) 3241–3253.
J. Shen, J. Shen, X. Chen, X. Huang, W. Susilo, An eﬃcient public auditing protocol with novel dynamic structure for cloud data, IEEE Trans. Inf. Forensics Secur. 12
(10) (2017) 2402–2415.
X. Jiang, F.R. Yu, T. Song, V.C.M. Leung, A survey on multi-access edge computing applied to video streaming: some research issues and challenges, IEEE Commun. Surv. Tutor. 23 (2) (2021) 871–903.
G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson, D. Song, Prov- able data possession at untrusted stores, in: Proceedings of the 14th ACM Conference on Computer and Communications Security, 2007, pp. 598–609.
A. Juels, B.S. Kaliski Jr., PORs: proofs of retrievability for large files, in: Proceed- ings of the 14th ACM Conference on Computer and Communications Security, 2007,
pp. 584–597.
M. Nofer, P. Gomber, O. Hinz, D. Schiereck, Blockchain, Bus. Inf. Syst. Eng. 59 (3) (2017) 183–187.
K. JuelsA, Proofs of retrievability for large files, in: Proc of Computer and Commu- nications Security, Vol. 584, AcMPress, Alexandria, 2007, p. 597.
G. Ateniese, R. Di Pietro, L.V. Mancini, G. Tsudik, Scalable and eﬃcient provable data possession, in: Proceedings of the 4th International Conference on Security and Privacy in Communication Networks, 2008, pp. 1–10.
H. Shacham, B. Waters, Compact proofs of retrievability, in: International Con- ference on the Theory and Application of Cryptology and Information Security, Springer, 2008, pp. 90–107.
C.C. Erway, A. Küpçü, C. Papamanthou, R. Tamassia, Dynamic provable data pos- session, ACM Trans. Inf. Syst. Secur.(TISSEC) 17 (4) (2015) 1–29.
Q. Wang, C. Wang, J. Li, K. Ren, W. Lou, Enabling public verifiability and data dynamics for storage security in cloud computing, in: European Symposium on Re- search in Computer Security, Springer, 2009, pp. 355–370.
J. Mao, Y. Zhang, P. Li, T. Li, Q. Wu, J. Liu, A position-aware Merkle tree for dynamic cloud data integrity verification, .Soft Comput 21 (8) (2017) 2151–
2164.
L. Rao, H. Zhang, T. Tu, Dynamic outsourced auditing services for cloud storage based on batch-leaves-authenticated Merkle hash tree, IEEE Trans. Serv. Comput. 13 (3) (2017) 451–463.
K.D. Bowers, A. Juels, A. Oprea, Proofs of retrievability: theory and implementation, in: Proceedings of the 2009 ACM workshop on Cloud Computing Security, 2009,
pp. 43–54.
T.H. Yuen, PAChain: private, authenticated & auditable consortium blockchain and its implementation, Future Gen. Comput. Syst. 112 (2020) 913–929.
X. Gao, J. Yu, W.-T. Shen, Y. Chang, S.-B. Zhang, M. Yang, B. Wu, Achieving low-en- tropy secure cloud data auditing with file and authenticator deduplication, Inf. Sci. 546 (2021) 177–191.
Y. Xu, S. Sun, J. Cui, H. Zhong, Intrusion-resilient public cloud auditing scheme with authenticator update, Inf. Sci. 512 (2020) 616–628.
B. Li, Q. He, F. Chen, H. Jin, Y. Xiang, Y. Yang, Auditing cache data integrity in the edge computing environment, IEEE Trans. Parallel Distrib. Syst. 32 (5) (2020) 1210–1223.
B. Li, Q. He, F. Chen, H. Dai, H. Jin, Y. Xiang, Y. Yang, Cooperative assurance of cache data integrity for mobile edge computing, IEEE Trans. Inf. Forensics Secur. 16 (2021) 4648–4662.
B. Li, Q. He, F. Chen, H. Jin, Y. Xiang, Y. Yang, Inspecting edge data integrity with aggregated signature in distributed edge computing environment, IEEE Trans. Cloud Comput. (2021). early access



G. Cui, Q. He, B. Li, X. Xia, F. Chen, H. Jin, Y. Xiang, Y. Yang, Eﬃcient verification of edge data integrity in edge computing environment, IEEE Trans. Serv. Comput. (2021).
D. Yue, R. Li, Y. Zhang, W. Tian, Y. Huang, Blockchain-based verification framework for data integrity in edge-cloud storage, J. Parallel Distrib. Comput. 146 (2020) 1–14.
A. Gervais, G.O. Karame, K. Wüst, V. Glykantzis, H. Ritzdorf, S. Capkun, On the security and performance of proof of work blockchains, in: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 2016,
pp. 3–16.
G. Wood, et al., Ethereum: a secure decentralised generalised transaction ledger, Ethereum Project Yellow Paper 151 (2014) (2014) 1–32.
Y. Fan, H. Wu, H.-Y. Paik, DR-BFT: a consensus algorithm for blockchain-based mul- ti-layer data integrity framework in dynamic edge computing system, Future Gen. Comput. Syst. 124 (2021) 33–48.
M. Szydlo, Merkle tree traversal in log space and time, in: International Confer- ence on the Theory and Applications of Cryptographic Techniques, Springer, 2004,
pp. 541–554.
J.R. Douceur, The sybil attack, in: International Workshop on Peer-to-Peer Systems, Springer, 2002, pp. 251–260.
N. Jovanovic, E. Kirda, C. Kruegel, Preventing cross site request forgery attacks, in: 2006 Securecomm and Workshops, IEEE, 2006, pp. 1–10.
R. Pries, W. Yu, X. Fu, W. Zhao, A new replay attack against anonymous communi- cation networks, in: 2008 IEEE International Conference on Communications, IEEE, 2008, pp. 1578–1582.
B. Kannhavong, H. Nakayama, N. Kato, Y. Nemoto, A. Jamalipour, NIS01-2: A col- lusion attack against OLSR-based mobile ad hoc networks, in: IEEE Globecom 2006, IEEE, 2006, pp. 1–5.
F. Callegati, W. Cerroni, M. Ramilli, Man-in-the-middle attack to the HTTPS protocol, IEEE Secur. Privacy 7 (1) (2009) 78–81.
G. vanRossum, Python reference manual, Department of Computer Science [CS] R 9525, 1995.
S. Gueron, S. Johnson, J. Walker, Sha-512/256, in: 2011 Eighth International Con- ference on Information Technology: New Generations, IEEE, 2011, pp. 354–358.
M.K. Hassan, A.I. El Desouky, S.M. Elghamrawy, A.M. Sarhan, Big data challenges and opportunities in healthcare informatics and smart hospitals, in: Security in Smart Cities: Models, Applications, and Challenges, 2019, pp. 3–26.



Yi Li is a PhD student in the School of Computer Science at Nanjing University of Information Science and Technology, Nanjing, China. His current research interests include informa- tion security, edge computing, data auditing, and blockchain.












Meiqin Tang is working at Internet of Things Technology College at Wuxi Vocational College of Science and Technol- ogy. Her current research interests include Internet of Things, blockchain, and information security.
