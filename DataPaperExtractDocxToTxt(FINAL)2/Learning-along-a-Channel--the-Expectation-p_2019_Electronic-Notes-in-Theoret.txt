Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 347 (2019) 143–160
www.elsevier.com/locate/entcs

Learning along a Channel: the Expectation part of Expectation-Maximisation
Bart Jacobs1
Institute for Computing and Information Sciences (iCIS) Radboud University Nijmegen
The Netherlands

Abstract
This paper first investigates a form of frequentist learning that is often called Maximal Likelihood Estimation (MLE). It is redescribed as a natural transformation from multisets to distributions that commutes with marginalisation and disintegration. It forms the basis for the next, main topic: learning of hidden states, which is reformulated as learning along a channel. This topic requires a fundamental look at what data is and what its validity is in a particular state. The paper distinguishes two forms, denoted as ‘M’ for ‘multiple states’ and ‘C’ for ‘copied states’. It is shown that M and C forms exist for validity of data, for learning from data, and for learning along a channel. This M/C distinction allows us to capture two completely different examples from the literature which both claim to be instances of Expectation-Maximisation.
Keywords: Probabilistic learning, Maximal Likelihood Estimation, latent variables, Expectation-Maximisation, learning along a channel


Introduction
Bayesian networks are graphical models for efficiently organising probabilistic in- formation [1,3,8,15,16,17]. These models can be used for probabilistic reasoning (inference), where the updated probability is inferred from certain evidence. These techniques are extremely useful, for instance in a medical setting, where symptoms and measurements can be used as evidence, and the inferred probability can help a doctor reach a decision.
A basic question is how to obtain accurate Bayesian networks. This question involves two parts: how to determine the underlying graph structure, and how to obtain the probabilities in the conditional probability tables (CPTs) of the network. The first part is called structure learning, and the second part is called parameter

1 Email: bart@cs.ru.nl

https://doi.org/10.1016/j.entcs.2019.09.008
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

learning. Here we concentrate on the latter, especially for discrete probability dis- tributions.
One way of obtaining the parameters of Bayesian network is to learn them from experts. However, it is more efficient and cheaper to learn the parameters from data, if available. The data is typically organised in (very large) tables; we shall describe such tables, say with n dimensions, as n-ary multisets in M(X1 ×· · ·×Xn), where M is the multiset monad on the category of sets. Frequentist learning from a multiset happens by counting and normalising. It is described here as as a natural transformation of the form M∗ ⇒ D, where M∗ is the (sub)monad of non-empty multisets, and where D is the discrete probability distribution monad. This learning technique is called maximal likelihood estimation (MLE), see e.g. [8, Ch.17], [16,
§17.1] or [15, §6.1.1].
This paper takes a fresh, fundamental look at learning. It starts from the basic notion of validity, or expected value, ω |= p of a predicate p in a state (distribution) ω. This validity is a number in the unit interval [0, 1]. In its basic form, learning is described as increasing this validity by changing the state ω, while keeping the predicate/evidence p fixed. Thus learning involves changing ω into ω∗ such that the validity ω∗ |= p is greater than ω |= p. One ways of doing this is taking ω∗ = ω|p, where ω|p is the state ω updated with (conditioned by) p, see below for details.
In this paper we go beyond the validity of a single predicate. We abstractly describe data, as source of learning, in terms of a multiset ψ of predicates. It turns out that there are two different forms of validity for such data, which we will describe as multiple-state validity ω |= ψ and copied-state validity ω |= ψ. The
M	C
distinction corresponds to the well-known situation in probability theory where one
has an urn with coloured balls and one draws with replacement (multiple-state) or without replacement (copied-state). Accordingly, there are two forms of learning, namely M-learning which increases M-validity |= , and C-learning which increase
C-validity |=.
In a next step we look at learning of ‘latent’ or ‘hidden’ variables on a probability space X, which is only indirectly accessible. In the current setting this means that we have data, not on X itself, but on a different space Y , with a channel (Kleisli map) X → Y between them. Obtaining a newly learned state ω∗ on X, from a given state ω, in this situation is the ‘Expectation’ part of the Expectation-Maximisation (EM) learning method [9]. We will identify two approaches, again denoted as M and C, to such learning along a channel.
A key discovery of this paper is that this M/C distinction explains two com- pletely different approaches to learning along a channel in the literature, in clas- sic references [18] and in [10], which are nevertheless both called Expectation- Maximisation.
The paper starts with mathematical preliminaries about states, channels, pred- icates and conditioning in Section 2 and with a concrete illustration of learning from tabular data, which is abstracted to a ‘frequentist learning’ natural transfor-
mation Flrn from multisets to distributions in Section 3. The paper continues in
Section 4 with the identification of data in terms of multisets of predicates, together

with associated forms of M/C-validity and M/C-learning. This is approach is ex- tended to learning along a channel in Section 5, where the main examples from the literature [10,18] are redescribed in the new M/C-framework.
Proofs of the main learning results involve some elementary real analysis and are relegated to the appendix. The calculations in this paper have been carried out with the EfProb library [5] for channel-based probability.
Mathematical preliminaries
A multiset is a ‘set’ in which (finitely many) elements may occur multiple times, with non-negative real numbers, in R≥0, as multiplicities. We write M(X) for the set of such multisets over a set X, defined as:
M(X) := {φ : X → R≥0 | supp(φ) is finite},
where supp(φ) is the support of φ, i.e. the subset {x ∈ X | φ(x) /= 0}. We shall write N (X) ⊆ M(X) for the subset of natural multisets, where φ ∈ N (X) if φ(x) ∈ N for each x. We often write concrete multisets as finite formal sums, using a ‘ket’ notation: φ =   φ(x)|x⟩. Taking multisets on a set is functorial: for f : X → Y
we get M(f ): M(X) → M(Y ) via M(f )(φ)(y) =	φ(x). Alternatively, for formal sums: M(f )( i ri|xi ⟩) = i ri|f (xi) ⟩. In fact, M is a monad on the category of sets, but this is not really needed here.
A probability distribution or a multinomial or a state is a multiset whose multi- plicities add up to one. We define the subset of those as:


D(X) := {φ ∈ M(X) |	x φ(x)= 1}
= {φ : X → [0, 1] | supp(φ) is finite, and
Σx φ(x)= 1}.

This D is also monad on sets.
A channel f : X → Y is a probabilistic computation from X to Y . It is a ‘Kleisli’ map f : X → D(Y ). Such a channel can ‘push’ a state ω ∈ D(X) forward to a state f  ω ∈ D(Y ), via ‘Kleisli extension’ or ‘state transformation’, where (f  ω)(y)=  x ω(x) · f (x)(y). Via  we can define composition g ◦· f of channels as (g ◦· f )(x)= g  f (x).
A predicate on a set X is a function p : X → [0, 1]. Specifically, each subset (event) E ⊆ X gives rise to a ‘sharp’ predicate 1E : X → [0, 1] by 1E(x)=1 when x ∈ E and 1E(x) = 0 when x /∈ E. For an element x ∈ X we whall write 1x for 1{x} and call 1x a point-predicate. We shall write Pred(X)= [0, 1]X for the set of all predicates on X.
The validity ω |= p in [0, 1] of a predicate p ∈ [0, 1]X in state ω ∈ D(X) is defined as the expected value defined on the left below.


ω |= p := Σ
ω(x) · p(x)	ω| (x) := ω(x) · p(x) .

When this validity ω |= p is non-zero, one can update (condition) ω with predicate
p, to get a new state ω|p ∈ D(X), as defined above, on the right.
Given a channel c : X → Y , one can transform a predicate q : Y → [0, 1] on its codomain into predicate c  q : X → [0, 1] on its domain, via: (c  q)(x) = 
y c(x)(y) · q(y). The validities ω |= c  q and c  ω |= q are then equal.
We are now in a position to describe a basic form of learning, in a situation with a validity expression:



distribution / state
¸ω,|= p,¸
predicate / evidence

(1)

Learning involves increasing this validity, by changing the state ω into a new state ω∗ such that ω∗ |= p ≥ ω |= p. Thus, we don’t change the predicate p; we consider it as given evidence that we need to adjust to. We do so by changing the state ω so that it better fits the evidence.
The next result captures a basic intuition, namely that learning can happen via conditioning. The proof is postponed to the appendix.
Theorem 2.1 Updating with p increases the validity of p, as in: ω |= p ≤ ω|p |= p.
In this paper we concentrate on such single steps in learning and not on their iteration. Convergence, either in an order-theoretic or in a metric sense (see [13]), is out of scope.
We still need the basic notion of product, both for states and for predicates. For two states σ ∈ D(X) and τ ∈ D(Y ) on sets X, Y one can form the product state σ ⊗ τ ∈ D(X × Y ) via (σ ⊗ τ )(x, y)= σ(x) · τ (y).
For predicates there are two different products/conjunctions, namely parallel conjunction p ⊗ q and sequential conjunction p1 & p2.
For two predicates p ∈ [0, 1]X and q ∈ [0, 1]Y on different sets X, Y there is a parallel product p ⊗ q ∈ [0, 1]X×Y defined by (p ⊗ q)(x, y) = p(x) · q(y). It is easy to see that then: σ ⊗ τ |= p ⊗ q equals (σ |= p) · (τ |= q).
For two predicates p1, p2 ∈ [0, 1]X on the same set X we can form the sequential product p1 & p2 ∈ [0, 1]X on this same X via: (p1 & p2)(x) = p1(x) · p2(x). Then, using the diagonal map/channel Δ,

ω |= p1 & p2 = ω |=Δ  (p1 ⊗ p2) = Δ  ω |= p1 ⊗ p2 where Δ  ω /= ω ⊗ ω.
(2)
Sequential conjunction & allows us to reduce successive state updates to a single update, since one has:
 ω|p |q = ω|p&q = ω|q&p = ω|q |p.	(3) For more information, see [11,14].

Tables and distributions
This section will elaborate a simple example in order to provide background infor- mation about the setting for learning. Consider the table (4) below where we have combined numeric information about blood pressure (either high H or low L) and certain medicines (either type 1 or type 2 or no medicine, indicated as 0). There is data about 100 study participants:


(4)


We consider several ways to ‘learn’ from this table.
We can form the cartesian product {H, T}× {0, 1, 2} of the possible outcomes and then capture the above table as a multiset over this product:
τ := 10| H, 0 ⟩ + 35| H, 1 ⟩ + 25| H, 2 ⟩ + 5|L, 0 ⟩ + 10| L, 1 ⟩ + 15| L, 2 ⟩.	(5)
We can normalise this multiset τ . It yields a joint probability distribution:
ω := 0.10|H, 0 ⟩+0.35|H, 1 ⟩+0.25|H, 2 ⟩+0.05|L, 0 ⟩+0.10|L, 1 ⟩+0.15|L, 2 ⟩	(6)
Such a distribution, directly derived from a table, is sometimes called an empirical
distribution [8].
The first and second marginals M1(ω) := D(π1)(ω) and M2(ω) := D(π2)(ω) of this joint probability distribution ω capture the blood pressure probabilities and the medicine probabilities separately, as: M1(ω) = 0.7|H ⟩ + 0.3|L⟩ and M2(ω) = 0.15| 0 ⟩+0.45| 1 ⟩+0.4| 2 ⟩. These marginal distributions can also be obtained directly from the above table (4), via the normalisation of ‘totals’ column and row. This fact looks like a triviality, but involves a naturality property (see Lemma 3.2 below).

Next we wish to use the above ta- ble (4) to learn the parameters (table en- tries) for the simple Bayesian network on the right. We then need to fill in the associ- ated conditional probability tables. These entries are obtained from the last column in Table 4, for the initial blood distribution 0.7|H ⟩ + 0.3|L⟩, and from the two rows in
,	,
Blood


Medicine
	J

the table; the latter yield two distributions for medicine usage, via normalisation.
In the categorical look at Bayesian networks (see e.g. [12,14]) these conditional probability tables correspond to channels: Kleisli maps for the distribution monad
D.  In the above case, the channel c : {H, T} → {0, 1, 2} corresponding to the

medicine table in the previous point is:
c(H) = 1 | 0 ⟩ + 1 | 1 ⟩ +  5 | 2 ⟩	c(L) = 1 | 0 ⟩ + 1 | 1 ⟩ + 1 | 2 ⟩.
7	2	14	6	3	2
The second marginal M2(ω) then equals c  M1(ω).
Given a joint distribution P (x, y) there is a standard way to extract a channel P (y | x) by taking conditional probabilities. This process is often called disintegra- tion, and is studied systematically in [6,7]. If we disintegrate the above distribution ω on the product {H, T}× {0, 1, 2} we obtain as channel {H, T} → {0, 1, 2}, pre- cisely the map c from the previous point — obtained in point (3) directly via the Table (4). This is a highly relevant property, which essentially means that (this kind of) learning can be done locally.
3.1 Frequentist learning by counting
The above example illustrates how probabilistic information can be extracted from a table with numeric data — in a frequentist manner — essentially by counting. This will now be described in terms of a natural transformation Flrn from multisets
to distributions, as used in the passage from the multiset τ in (5) to the distri- bution ω = Flrn(τ ) in (6). As mentioned in the introduction, maximal likelihood estimation (MLE) is one kind of parameter learning, see e.g. [8,15,16]. Our cate-
gorical reformulation for discrete probability distributions (multinomials) uses the non-empty multiset functor M∗ and the distribution functor D from Section 2. It turns out that the process of learning-by-counting involves some basic categorical structure: it is a monoidal natural transformation, that can be applied locally.
Definition 3.1 For a set X, let M∗(X) ⊆ M(X) be the subset of non-empty (i.e. non-null) multisets. We deﬁne (discrete) maximal likelihood estimation as the normalisation function Flrn : M∗(X) → D(X), determined by:

φ(x)
Σ	 Σ
Σ   ri 

Flrn(φ)(x) :=
where	|φ| :=
|φ|
y φ(y), i.e. Flrn
i ri|xi ⟩  =
i
Σj rj
|xi ⟩.
(7)

Lemma 3.2 The maps Flrn : M∗(X) → D(X) form a natural transformation
M∗ ⇒ D. This natural transformation is monoidal, but not a map of monads.
Proof. We only prove naturality. For a function h : X → Y ,
 Flrn ◦ M(h) (Σi ri|xi ⟩) = Flrn Σi ri|h(xi) ⟩ 
= Σi ri |h(xi) ⟩	for r = Σi ri
= D(h) Σi ri |xi ⟩  =  D(h) ◦ Flrn (Σi ri|xi ⟩).	 
In Section 3 we mentioned that one can extract a conditional probability table (or channel) either directly from the table of data, or from the associated empirical probability distribution. In the first case one takes the obvious map M(X × Y ) →

M(X)Y . There is a similar ‘disintegration’ mapping D(X × Y ) → D(Y )X , turning a joint distribution into a channel; it involves an additional normalisation step and (thus) a side-condition, see [6]. It is not hard to show that the learning maps Flrn
commute with these two extraction maps, for multisets and for distributions. This
is a fundamental result, since it says that distributions can be obtained locally from a table, as illustrated in Section 3. Nevertheless, we will not elaborate it in detail. Instead, we show later in Proposition 4.3 that Flrn(φ) is optimal, in a suitable sense.
Data, validity, and learning
So far we have described learning with a single predicate. In practice learning happens from ‘data’, so the first question we should address is: what is data? It often comes in the form of sequences or (multidimentional) tables. The order of data items does not matter for updating, see (3), but multiple occurrences of data items are relevant. These two aspects suggest that data items on a set X should be organised in the form of multisets in N (X). However, a more powerful perspective uses multisets of predicates as data. A data item, in the form of a predicate, may be a point-predicate, when there is certainty about the element involved. But when some data is missing, or when there is uncertainty, we may use a uniform predicate. Hence we shall use elements Φ ∈ M(Pred(X)) as data on X, with what we call point-data ϕ ∈ M(X) as special case, corresponding to data with point-predicates 1x for x ∈ X.
Next we wish to introduce validity for data. Before doing so we take a step back and look at some elementary issues. Assume we have a fair coin σ = 1 |H ⟩ + 1 |T ⟩
2	2
as state. Consider the following questions.
What is the probability of getting two heads? Most people will say 1 . The idea behind this answer is that we flip the coint twice, where getting a head has probability 1 each time. More formally, this answer involves a ‘multiple-state’ perspective, where the coin state σ is used twice, as in:

σ ⊗ σ |= 1H ⊗ 1H = σ |= 1H · σ |= 1H  = 1 · 1
= 1 .

There is an alternative perspective in which the probability of getting two heads is 1 , namely: the coin is tossed once, and two (honest, truthfull) observers are asked to tell what they see. They will both report ’head’, so we get two heads, as required. This perspective is reflected by the computation:
σ |= 1H & 1H = σ |= 1H = 1 .
We can similarly ask: what is the probability of getting head and tail? Let’s not worry about taking the order into account and simply ask about first
seeing head, then tail. The first, multiple state perspective again gives 1 as

probability:
σ ⊗ σ |= 1H ⊗ 1T = σ |= 1H · σ |= 1T  = 1 · 1

= 1 .

But if we toss the coin only once and look at the probability that the first of two observers reports ‘head’ and the second one reports ‘tail’ we get outcome zero:
σ |= 1H & 1T = σ |= 0 = 0.
Next assume it is dark and the observers cannot see the coin very clearly. We consider the predicate p = 0.8 · 1H + 0.2 · 1T giving 80% certainty for head, and q = 0.7 · 1H + 0.3 · 1T with only 70% certainty. What is now the probability of seeing p and q? The multiple-state perspective gives:
σ ⊗ σ |= p ⊗ q =  σ |= p · σ |= q  =  1 · 0.8+ 1 · 0.2 · 1 · 0.7+ 1 · 0.3  = 1 .
The alternative perspective now gives:
σ |= p & q = σ |= 0.56 · 1H + 0.06 · 1T = 1 · 0.56 + 1 · 0.06 = 0.31.
2	2
We have referred to the first approach with tensor σ⊗σ of states as the multiple- state perspective. We will refer to the second approach as the copied-state perspec- tive, in the light of (2). These different perspectives lead to different notions of validity of data.
Definition 4.1 Let ω ∈ D(X) be a state and Φ ∈ N Pred(X) be data on X.
The multiple-state validity, or M-validity for short, of Φ in ω is defined as:
ω |=M Φ :=  p ω |= p	.	(8)
Φ(p)

The copied-state validity, or C-validity, of Φ in ω is:
ω |= Φ := ω |= &p pΦ(p).		(9) When Φ consists of point-data, these validities become	x∈supp(Φ) ω(x)Φ(x)  and
ω |= &x∈supp(Φ) 1x, respectively.
For clarity: pn in point (ii) is the n-fold conjunction p & ··· & p, with p0 = 1. When p is sharp, then pn = p, for n > 1. In particular, point-predicates 1x are sharp.
Corresponding to M/C-validity, there is M/C-learning, where the aim is, as before, to modify the state in order to increase validity. The next result gives the essentials. The proof (of the first point) is in the appendix.
Theorem 4.2 Let ω ∈ D(X) be a state with data Φ ∈ M(Pred(X)).
M-learning Mlrn(ω, Φ) |= Φ ≥ ω |= Φ can be done via the newly learned state:

M
Mlrn(ω, Φ) := Σp
M

Φ(p) · ω|
|Φ|	p

where	|Φ| := Σp

Φ(p).

C-learning Clrn(ω, Φ) |= Φ ≥ ω |= Φ can be done directly via Theorem 2.1,
C	C
with Clrn(ω, Φ) = ω &ppΦ(p) .
When we apply point (i) to point-data ϕ ∈ N (X) we get as newly learned state:

Mlrn(ω, ϕ) = Σx
ϕ(x) · ω|
|ϕ|	1x
=	ϕ(x) |x⟩ = Flrn(ϕ).
x  |ϕ|

Hence we rediscover frequentist learning Flrn from Section 3.1. The above result says that frequentist learning gives an increase of M-validity. In fact, it gives the highest possible validity. This is a standard result, see e.g. [16, Ex. 17.5], which we reproduce here (with a proof in the appendix).
Proposition 4.3 For non-empty point-data ϕ ∈ M(X) the predicate “M-validity of ϕ”
D(X)   (−) |=M ϕ [0¸, 1]
takes its maximum at the distribution Flrn(ϕ) ∈ D(X) that is obtained by frequentist learning. This says:
Flrn(ϕ) = argmax ω |= ϕ.
ω	M
Learning along a channel
In the previous section we have looked at validity of data in a state and how to increase this validity by adapting the state. So far we have considered the situation where the state and data are on the same set X. In practice, it often happens that there is a difference, like in:



state to be learned
X¸,◦e  Y ¸,¸
data
(10)

We will assume that there is a channel between the two spaces — as in the above picture — that can be used to mediate between the given data and the state that we wish to learn. This is what we call ‘learning along a channel’. This learning challenge is often described in terms of ‘hidden’ or ‘latent’ variables, since the elements of the space X are not directly accessible, but only indirectly via the ‘emmission’ channel
e. This forms the E-part of what is called Expectation-Maximisation (EM), see [9]. In the M-part the channel e becomes a learning goal in itself. In Expectation- Maximisation these E- and M-parts are alternated. But here we concentrate on the E-part only and assume that the channel e is given and remains fixed.
We describe two different ways to handle this new learning situation along a channel, depending on whether one takes the multiple-state or the copied-state perspective. In both cases we first move data Ψ ∈ N Pred(Y ) on Y to data on X, via the channel e : X → Y . This can be done easily, by using that the multiset operation N is functorial and that predicate transformation yields a function e 

(−): Pred(Y ) → Pred(X). Thus we get data on X via a new data transformation, which we shall write as:
e  N Ψ := N e  (−) (Ψ) = Σp Ψ(p) e  p .	(11)
Definition 5.1 Let e : X → Y be a channel with data Ψ ∈ N Pred(Y ) on its codomain. We define M/C-learning along e from Ψ as M/C-learning from e  N Ψ in (11). Concretely this turns a state ω ∈ D(X) into a newly learned states on X, where:
for M-learning,


Mlrn(ω, e, Ψ) := Mlrn(ω, e  N
Ψ) = Σp
Ψ(p) · ω|
|Ψ|


e  p
.	(12)



for C-learning,
Clrn(ω, e, Ψ) := Clrn(ω, e  N Ψ) = ω &p(e p)Ψ(p) (3)
= ω|(e  p1)n1 ··· |(e  pk )nk	when Ψ =



Σ1≤i≤k ni|pi ⟩.



(13)

Notice that we have overloaded the notation Mlrn and Clrn, where its meaning depends on the number of arguments: two for ordinary learning, as in Theorem 4.2, and three for learning along a channel.
Before looking at examples, we show that there is an alternative description of M-learning along a channel for point-data in terms of the so-called ‘dagger’ or ‘Bayesian inversion’ of the channel, see [6,7]. Given a state ω ∈ D(X) on the domain of a channel e : X → Y we can turn e around to a channel e† : Y → X, via
conditioning with a transformed point predicate:


e† (y) := ω|

e  1y
=	ω(x) · e(x)(y) x .
(e  ω)(y)
x
(14)

This dagger satisfies all sorts of nice properties, see [6,7] and the references given there for more information. Here we concentrate on the relevance of the dagger of a channel in learning — for point-data.
Lemma 5.2 For a state ω ∈ D(X) and channel e : X → Y with point-data ϕ ∈ N (X) we get:


Proof. Since:
Mlrn(ω, e, ϕ) = e†
  Flrn(ϕ).	(15)



Mlrn(ω, e, ϕ)
(12)
=
x
ϕ(x)
|ϕ|

· ω|e 1x
(14)
=
ϕ(x)
x  |ϕ|
· e† (x)

(7) Σ	†	†
=	Flrn(ϕ)(x) · eω(x) = eω   Flrn(ϕ).
x
 

We now come to the two classic examples in the literature that we will redescribe using the newly developed M/C distinction.
Example 5.3 The first example is taken from [18, §20.3], where it is used as an illustration of the Expectation-Maximisation (EM) algorithm. It involves the Bayesian network as below, with (two) bags of candies, described by three features, namely their flavour, their wrapper, and whether or not they have holes.

,
Bag
cccc 
,

J¸¸¸¸

,
Flavour
 
c,cc7 	  
Wrapper
J 
¸¸z	 ,
Holes
J	J

We number the bags as 0 and 1, so that we use the set {0, 1} for bags. The flavours can be cherry and lime, in the set {C, L}; the wrappers can be red or green, in
{R, G}, and there can be a hole or not, in {H, H⊥}. The three arrows in the Bayesian network correspond to three channels f : {0, 1} → {C, L}, w : {0, 1} → {R, G}, h : {0, 1}→ {H, H⊥}, which have equal probabilities in [18]:

f (0) =  6 |C ⟩ +  4 |L⟩    w(0) =  6 |R⟩ +  4 |G⟩    h(0) =  6 |H ⟩ +  4 |H⊥ ⟩

10	10
10	10
10	10

f (1) =  4 |C ⟩ +  6 |L⟩	w(1) =  4 |R⟩ +  6 |G⟩	h(1) =  4 |H ⟩ +  6 |H⊥ ⟩.

10	10
10	10
10	10

These	three	channels	are	combined	into	a	single	(three-)tuple	channel
⟨f, w, h⟩ : {0, 1}→ {C, L}× {R, G}× {H, H⊥}. At 0 it is:
⟨f, w, h⟩(0) = f (0) ⊗ w(0) ⊗ h(0)
=  216 |C, R, H ⟩ +  144 |C, R, H⊥ ⟩ +  144 |C, G, H ⟩ +  96  |C, G, H⊥ ⟩

1000
1000
1000
1000

+  144 |L, R, H ⟩ +  96  |L, R, H⊥ ⟩ +  96  |L, R, H ⟩ +  64  |L, R, H⊥ ⟩.

1000
1000
1000
1000

The point-data ψ ∈ M {C, L}× {R, G}× {H, H⊥}  is given by the multiset:
ψ = 273|C, R, H ⟩ + 93| C, R, H⊥ ⟩ + 104|C, G, H ⟩ + 90| C, G, H⊥ ⟩
+ 79| L, R, H ⟩ + 100|L, R, H⊥ ⟩ + 94| L, R, H ⟩ + 167|L, R, H⊥ ⟩.
We thus have a pattern as described in (10), concretely of the form:

⟨f,w,h⟩  ¸	⊥


state to be learned
{0, 1}	◦
 ,
{C, L}× {R, G}× {H, H
}

data ψ

We are now set for M-learning along the tuple channel ⟨f, w, h⟩ from these point- data on its codomain, via the dagger of the tuple channel, see Lemma 5.2. In [18]

this uses a prior distribution ρ =  6 | 0 ⟩ +  4 | 1 ⟩. We thus compute the newly learned
10	10
distribution on {0, 1} as:
(15)
Mlrn(ρ, ⟨f, w, h⟩	⟨f, w, h⟩†   Flrn(ψ) = 0.6124| 0 ⟩ + 0.3876| 1 ⟩.
, ψ) =	ρ

This probability 0.6124 is exactly as computed in [18, §20.3], but without the (dag- ger) channel machinery. Moreover, the ‘multiple-state’ versus ‘copied-state’ per- spective does not occur there.
Our second example from [10] also claims to be an instance of expectation- maximisation. However, what happens there is completely different from expectation-maximisation as used in [18]. A crucial point of this paper is that the difference can be explained in terms of M-learning versus C-learning along a channel.
Example 5.4 The leading illustration of [10] involves classification of a coin in one of two classes, depending on its bias. There are five separate sets of data (multisets), each with ten coin outcomes head (H) and tail (T ), see the first column in Table (16) below. There is a channel e : {0, 1}→ {H, T} that captures two coins, with slightly different biases:
e(0) = 3 |H ⟩ + 2 |T ⟩	and	e(1) = 1 |H ⟩ + 1 |T ⟩.
5	5	2	2
Thus, the distribution e(0) shows a slight bias towards head, whereas e(1) is unbi- ased. By learning along e the resulting distribution on {0, 1} tells whether the first or the second coin in e best fits the data. The idea is that if the data contains more heads than tails, then the first coin e(0) is most likely: the learned distribution r| 0 ⟩ + (1 − r)| 1 ⟩ has r > 1 .
The second and third columns of the table below give the learned distributions on {0, 1}, both via C-learning and M-learning along the coin channel e. Each line describes a new learning action, independent from the outcomes of earlier lines.




(16)




This table is obtained via the formulas (13) and (12). M-learning seems to per- form binary classification best: picking up the differences between the numbers of heads and tails in the data. This C-learning column is as reported in [10], but uses higher precision. The term C-learning or the copied-state perspective do not

occur in [10]: the explanation of learning via Expectation-Maximisation given there consists basically of a single example.
We thus notice in these Examples 5.3 and 5.4 that [10] and [18] perform com- pletely different computations in their explanation of expectation-maximisation, using, respectively, in the terminology of our setting, M-learning and C-learning along a channel. It is unclear why they do different things and still use the same Expectation-Maximisation terminology.
We conclude this section with some general observations. The most interesting one is that C-learning along a channel is additively compositional. This means that it can handle additional data as it arrives, by performing another C-learning step. This is a clear advantage of C-learning over M-learning.
Proposition 5.5	(i) Data transformation  N from (11) is functorial in the sense that:
id  N Ψ = Ψ	and	(d ◦· e)  N Ψ = e  N  d  N Ψ .

M-learning is sequentially compositional in the sense that it interacts well with channel composition:
Mlrn(ω, id, Ψ) = Mlrn(ω, Ψ)	Mlrn(ω, d ◦· e, Ψ) = Mlrn(ω, d, e  N Ψ).
C-learning is additively compositional, in the sense that:
Clrn(ω, e, Φ+ Ψ) = Clrn Clrn(ω, e, Φ), e, Ψ .
Proof.	(i) We use that predicate transformation is functorial, so that:
id  (−) = id	and	(d ◦· e)  (−) = e  (−) ◦ d  (−) .
The result then follows by functoriality of taking multisets N .
By the previous point:
Mlrn(ω, id, Ψ) = Mlrn(ω, id  N Ψ) = Mlrn(ω, Ψ).
And:
Mlrn(ω, d ◦· e, Ψ) = Mlrn(ω, (d ◦· e)  N Ψ) = Mlrn(ω, e  N (d  N Ψ))
= Mlrn(ω, e, d  N Ψ).
Let’s use the ad hoc notation:
e ≪ Ψ := &p (e  p)Ψ(p).

We then have e ≪ (Φ + Ψ) = (e ≪ Φ) & (e ≪ Ψ) since:
e ≪ (Φ + Ψ) = &p (e  p)(Φ+Ψ)(p) = &p (e  p)Φ(p)+Ψ(p)
= &p (e  p)Φ(p) & (e  p)Ψ(p)
= &p (e  p)Φ(p) & &p (e  p)Ψ(p)
= e ≪ Φ (x)& e ≪ Ψ (x).
Now we are almost done:

Clrn(ω, e, Φ+ Ψ)
(13)
= ω|e≪(Φ+Ψ) = ω|(e≪Φ)&(e≪Ψ)	as just shown
(3)
= ω|e≪Φ|e≪Ψ
(13)
= Clrn(ω, e, Φ)|e≪Ψ

(13)

Conclusions and further work
This paper has developed a systematic approach to (parameter) learning, start- ing from maximal likelihood estimation as a suitable natural transformation Flrn : M∗ ⇒D from multisets to distributions. It then developed a more sophisti-
cated form of data as multisets of predicates, with two associated forms, labeled as
M/C, of validity and learning, even along a channel.
There are several avenues for further research.
Extending the channel-based analysis from the E-part of the EM-algorithm to the whole of EM. This involves, in the context of the diagram (10), not only learning the state, but also learning the channel (as M-part). The Expectation- Maximisation algorithm involves iterating these E- and M-parts until some level of stability is reached. It will be described in an extended version of this paper.
Extending this work to the more sophisticated form of learning called Bayesian learning, see [8, Ch.18], [16, §17.3] or [15, §6.1.2]. It is a form of higher order learning, where one does not immediately obtain the probability distribution in D(X), for a finite set X, but one obtains a distribution over D(X), typically in the form of Dirichlet distributions.

References
Barber, D., “Bayesian Reasoning and Machine Learning,” Cambridge Univ. Press, 2012, publicly available via http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage.
Baum, L., T. Petrie, G. Soules and N. Weiss, A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains, Ann. Math. Statistics 41 (1970), pp. 164–171.
Bernardo, J. and A. Smith, “Bayesian Theory,” John Wiley & Sons, 2000.
Bishop, C., “Pattern Recognition and Machine Learning,” Information Science and Statistics, Springer, 2006.

Cho, K. and B. Jacobs, The EfProb library for probabilistic calculations, in: F. Bonchi and B. K¨onig, editors, Conference on Algebra and Coalgebra in Computer Science (CALCO 2017), LIPIcs 72 (2017).
Cho, K. and B. Jacobs, Disintegration and Bayesian inversion, both abstractly and concretely (2019), Math. Struct. in Comp. Science. See https://doi.org/10.1017/S0960129518000488 or arxiv.org/abs/ 1709.00322.
Clerc, F., F. Dahlqvist, V. Danos and I. Garnier, Pointless learning, in: J. Esparza and A. Murawski, editors, Foundations of Software Science and Computation Structures, number 10203 in Lect. Notes Comp. Sci. (2017), pp. 355–369.
Darwiche, A., “Modeling and Reasoning with Bayesian Networks,” Cambridge Univ. Press, 2009.
Dempster, A., N. Laird and D. Rubin, Maximum likelihood from incomplete data via the EM algorithm, Journ. Royal Statistical Soc. 39(1) (1977), pp. 1–38.
Do, C. and S. Batzoglou, What is the expectation maximization algorithm?, Nature Biotechnology 26
(2008), pp. 897–899.
Jacobs, B., From probability monads to commutative effectuses, Journ. of Logical and Algebraic Methods in Programming 94 (2018), pp. 200–237.
Jacobs, B. and F. Zanasi, A predicate/state transformer semantics for Bayesian learning, in:
L. Birkedal, editor, Math. Found. of Programming Semantics, number 325 in Elect. Notes in Theor. Comp. Sci. (2016), pp. 185–200.
Jacobs, B. and F. Zanasi, A formal semantics of influence in Bayesian reasoning, in: K. Larsen,
H. Bodlaender and J.-F. Raskin, editors, Math. Found. of Computer Science, LIPIcs 83 (2017), pp. 21:1–21:14.
Jacobs, B. and F. Zanasi, The logical essentials of Bayesian reasoning, in: Probabilistic Programming
(book chapter, to appear in 2019), see arxiv.org/abs/1804.01193.
Jensen, F. and T. Nielsen, “Bayesian Networks and Decision Graphs,” Statistics for Engineering and Information Science, Springer, 2007, 2nd rev. edition.
Koller, D. and N. Friedman, “Probabilistic Graphical Models. Principles and Techniques,” MIT Press, Cambridge, MA, 2009.
Pearl, J., “Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference,” Graduate Texts in Mathematics 118, Morgan Kaufmann, 1988.
Russell, S. and P. Norvig, “Artificial Intelligence. A Modern Approach,” Prentice Hall, Englewood Cliffs, NJ, 2003.


A  Appendix
We provide the missing proofs of Theorems 2.1 and 4.2 and of Proposition 4.3. The proof of the latter proposition is standard, but is included because it forms a proper preparation for the proofs of the two theorems — which are new results. All proofs rely on some basic real analysis for finding the maximum of functions with constraints on their inputs. This is done via the Lagrange multiplier method, see e.g. [4, §2.2]. This will be illustrated first. Subsequently we make use of a ‘sum-increase’ lemma to prove the other results.
Proof. (of Proposition 4.3) Let ϕ ∈ M(X) be a fixed non-empty multiset. We need to prove that the function (−) |= ϕ : D(X) → [0, 1] takes its maximum at Flrn(ϕ). We will thus seek the maximum of the function ω '→ ω |= ϕ by taking the
derivative with respect to ω ∈ D(X). We will work with the ‘log-validity’, that is,
with the function ω '→ ln(ω |= ϕ), where ln is the monotone (natural) logarithm

function. It reduces the product of powers in the definition of |=M toa sum of multiplications.
Assume that the support of ϕ = i ri|xi ⟩ is {x1,..., xn} ⊆ X. We look at distributions ω ∈ D({x1,..., xn}); they may be identified with numbers v1,..., vn ∈ R≥0 with  i vi = 1. We thus seek the maximum of the log-validity function:

k(v) := ln Σi
vi|xi ⟩ |=M Σi

ri|xi ⟩  = ln   i
vri  = Σ

ri · ln(vi).


Since we havea constraint ( i vi)−1 = 0 on the inputs, we can use the Lagrange multiplier method for finding the maximum. We thus take another parameter λ in a new function:
K(v, λ) := k(y) − λ · (Σi vi) − 1  =  Σi ri ln(vi) − λ · (Σi vi) − 1 .
The partial derivatives of K are:
∂K (v, λ) = ri − λ	∂K (v, λ) = 1 − Σ v .

Setting all of these to 0 and solving gives the required maximum. First, we have:




Hence λ = Σi ri and thus:
1 = Σi vi
r
=	i λ
=		i ri . λ


vi =
ri =
λ
ri
Σi ri

(7)
= Flrn(ϕ)(xi).	 

We now come to an auxiliary result which we shall call the sum-increase lemma. It is a special (discrete) case of a more general result [2, Thm. 2.1]. It describes how to find increases for sum expressions in general.
Lemma A.1 Let X, Y be ﬁnite sets, and let F : X × Y → R≥0 be a given function. For each x ∈ X, write F1(x) :=  y∈Y F (x, y). Assume that there is an x∗ ∈ X with:

x∗ = argmax G(x, z)	where	G(x, z) :=
z

Then F1(x∗) ≥ F1(x).


y∈Y
F (x, y) · ln F (z, y) .

The proof uses Jensen’s inequality: for a1,..., an ∈ R>0 and r1,..., rn ∈ [0, 1] with  i ri = 1 one has ln( i riai) ≥  i ri ln(ai). This gives a strict increase, except in ‘corner’ cases. The same holds for the above sum-increase lemma. The actual maximum x∗ in that lemma can in many situation be determined analytically
— using the Lagrange multiplier method — but it need not be unique.

Proof. Let x∗ be the element where G(x, −): Y → R≥0 takes its maximum. This
x∗ satisfies F1(x∗) ≥ F1(x), since:

F1(x∗)
ln
F1(x)
= ln
 Σy
F (x∗, y) F1(x)
= ln
 Σy
F (x, y) ·
F1(x)
F (x∗, y)
F (x, y)

≥	F (x, y)
y F1(x)
· ln
F (x∗, y)
F (x, y)
by Jensen’s inequality

=   1	 ·
F1(x)
F (x, y) ·  ln F (x∗, y) − ln F (x, y) 

=   1	 ·  G(x, x∗) − G(x, x)	≥ 0.	 
F1(x)
Proof. (of Theorem 2.1) We have a state ω ∈ D(X) and a predicate p ∈ [0, 1]X and wish to prove a learning-style increase in validity: ω |= p ≤ ω|p |= p. We apply Lemma A.1 with F : D(X) × X → R≥0 given by F (ω, x)= ω(x) · p(x). Then
F1(ω)= Σx F (ω, x)= Σx ω(x) · p(x)= ω |= p.	Σ
and we wish to find the maximum of G(ω, −): D(X) → R≥0. Let X = {x1,..., xn}. We can see G(ω, −) as a function Rn → R≥0 with constraints on its inputs.
These constraints are handled, as before, via the Lagrange multiplier method for finding the maximum. We keep ω fixed and consider a new function H with an additional parameter λ.
H(v, λ) := G(ω, v) − λ · (Σi vi) − 1 
= Σi ω(xi) · p(xi) · ln vi · p(xi) − λ · (Σi vi) − 1 .
The numbers vi are variables for the unknowns ω∗(xi).
The partial derivatives of H are:
∂H (v, λ) = ω(xi) · p(xi) · p(x ) − λ = ω(xi) · p(xi) − λ	∂H (v, λ) = 1 − Σ v .

Setting all these derivatives to zero yields:

1 = Σi vi
Hence λ = ω |= p and thus:
= Σi
ω(xi) · p(xi)
λ
ω |= p
=	.
λ

v = ω(xi) · p(xi)
= ω(xi) · p(xi)
= ω| (x ).	 

i	λ	ω |= p	p	i
Proof. (of Theorem 4.2 (i)) Let ω ∈ D(X) be state on a finite set X and let
p1,..., pn be predicates on X, all with non-zero validity ω |= pi. We claim that the

state ω∗ = Σ
i 1 · ω|p
then satisfies:
(ω∗ |= pi) ≥	(ω |= pi).	(A.1)
i	i

The inequality in Theorem 4.2 (i) is a direct consequence of (A.1). We shall prove (A.1) for n = 2. The generalisation to arbitrary n should then be obvious, but involves much more book-keeping of additional variables.
We use Lemma A.1 with function F : D(X) × X × X → R≥0 given by:
F (ω, x, y) := ω(x) · p1(x) · ω(y) · p2(y).
Then by distributivity of multiplication over addition:
Σx,y F (ω, x, y) = Σx ω(x) · p1(x) · Σy ω(y) · p2(y) = (ω |= p1) · (ω |= p2).
Let X = {x1,..., xn} and let the function H be given by:
H(v, λ) := Σi,j F (ω, xi, xj) · ln vi · p1(xi) · vj · p2(xj)  − λ · (Σi vi) − 1 .
Then:

∂H (v, λ) = Σ
F (ω, xk, xi)+ F (ω, xi, xk)

— λ	∂H (v, λ) = 1 − Σ v .

Setting these to zero gives:


1 = Σk vk
=	k,i F (ω, xk, xi)+ F (ω, xi, xk)
λ
= 2 · (ω |= p1) · (ω |= p2) . λ

Hence λ =2 · (ω |= p1) · (ω |= p2) so that:
v  =	i F (ω, xk, xi)+ F (ω, xi, xk)
k	λ
= 1 · ω(xk) · p1(xk) · (ω |= p2) + 1 · (ω |= p1) · ω(xk) · p2(xk)
2	(ω |= p1) · (ω |= p2)	2	(ω |= p1) · (ω |= p2)
= 1 · ω(xk) · p1(xk) + 1 · ω(xk) · p2(xk)
2	ω |= p1	2	ω |= p2
= 1 · ω|p (xk)+ 1 · ω|p (xk).	 
2	1	2	2
