

Electronic Notes in Theoretical Computer Science 220 (2008) 23–42
www.elsevier.com/locate/entcs

Relational Analysis and Precision via Probabilistic Abstract Interpretation
Alessandra Di Pierro1	Pascal Sotin2	Herbert Wiklicky3

Abstract
Within the context of a quantitative generalisation of the well established framework of Abstract Interpre- tation – i.e. Probabilistic Abstract Interpretation – we investigate a quantitative notion of precision which allows us to compare analyses on the basis of their expected exactness for a given program. We illustrate this approach by considering various types of numerical abstractions of the values of variables for indepen- dent analysis as well as weakly and fully relational analysis. We utilise for this a linear operator semantics of a simple imperative programming language. In this setting, fully relational dependencies are realised via the tensor product. Independent analyses and weakly relational analyses are realised as abstractions of the fully relational analysis.
Keywords: Probabilistic Semantics, Linear Operators, Probabilistic Abstract Interpretation, Relational Analysis

Introduction
Static program analysis aims in providing safe approximations of various program properties. In Abstract Interpretation these properties form a lattice of domains that model possible (abstract) values. An analysis based on a certain abstract do- main can be rather imprecise. One of the reasons for this imprecision is the possible obfuscation of the relationship between variables. Additionally, the representation of a set of (concrete) values by (abstract) elements makes it impossible to discover, for example, “forbidden sections” within an interval. In this paper we investigate a framework for measuring the imprecision of a domain (or analysis) and we con- centrate in particular on the issue of how to analyse relational dependency in this quantitative setting. We illustrate our approach by considering (weakly) relational analyses which preserve partially the relation between variables.
Probabilistic Abstract Interpretation (PAI) [9] – a quantitative generalisation of classical Abstract Interpretation [5] – provides the context in which we can introduce

1 Email: alessandra.dipierro@univr.it
2 Email: pascal.sotin@irisa.fr
3 Email: herbert@doc.ic.ac.uk

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.11.017

a quantitative notion of precision and use it to compare any two analyses. We utilise a Linear Operator Semantics (LOS) of a simple imperative programming language where relational dependencies are realised via tensor products.
In order to compare the results of various analyses we introduce a new mea- sure for the precision of a given analysis. This is based on a quantitative version of the well-known lattice theoretic comparison of Abstract Interpretations cf [18]. The identification of each probabilistic abstraction with a corresponding orthogonal projection operator allows us to exploit the Birkhoff-von Neumann lattice structure on these operators [2] and its ordering to compare PAI’s. The fact that PAI’s are represented by linear operators also allows us to compare the difference between any two of them in metric terms, even when their classical counterparts are not comparable (i.e. one is not a refinement of the other).
Language and Semantics
We consider the probabilistic language pWhile presented in [7]. The usual deter- ministic While language is a proper sub-language of pWhile without the prob- abilistic choice statement. Even when the intention is to analyse only determin- istic, i.e. While, programs, it is necessary to consider probabilities for the “ab- stracted” versions of such programs. One can intuitively see that a statement like if even(x) then S1 else S2 fi corresponds to an abstract version where S1 and S2 are executed with a half probability each, i.e. to the statement choose 0.5 : S1 or 0.5 : S2 (provided we assume that the value of x is uniformly distributed). The situation in classical analysis is similar, although there deterministic programs are usually “abstracted” to non-deterministic ones (operating with sets of possible results).

Syntax.
A program P in pWhile is made up from a possibly empty list of variable declara- tions D followed by a statement S. Formally, P ::= D; S | S with D ::= d; D | d. A declaration d fixes the types of the program’s variables, i.e. int or bool. The syntax of statements S is as follows:
S ::= skip | stop | x ← e | S1; S2 | choose p1 : S1 or p2 : S2
|  if b then S1 else S2 fi | while b do S od
In the choose statement we allow only for constant probabilities pi and assume
w.l.o.g. that they are normalised, i.e. add up to 1. Arithmetic expressions are of the usual form
a ::= n | x | a1 ⊙ a2
with n ∈ Z, x a program variable and ‘⊙’ representing one of the usual arithmetic operations ‘+’, ‘−’, or ‘×’. Boolean expressions are also defined as one would expect by
b ::= true | false | x | ¬b | b1 ∨ b2 | b1 ∧ b2 | a1 <> a2,


A Probabilistic Transition System for pWhile

where x is a Boolean variable and ‘<>’ denotes one of the standard comparison operators for arithmetic expressions, i.e. <, ≤, =, /=, ≥, >.

Linear Operator Semantics.
The Structural Operational Semantics (SOS) of pWhile can, as usual, be given via a transition relation which is in this case a probabilistic one where each transition has a probability associated with it. The configurations are – as in the deterministic case – pairs ⟨S, σ⟩ with S a statement and σ a state, i.e. a map which associates to each variable x a (unique) value in the corresponding sets Z and B of integers and Booleans. We denote by Var the set of all variables, by State = Var → Z ∪ B the set of states, and by Conf the set of all configurations. The transition relation,
−→p⊆ Conf × [0, 1] × Conf, for pWhile is defined in [8] and we recall it here in Table 1. The function E (·)· represents the usual semantics of expressions e in state σ, see e.g. [16].
While the operational semantics of programs is usually defined in terms of transi- tion relations many other areas specify the “dynamics” of a system in terms of other mathematical objects. In the theory of stochastic processes one usually considers matrices, or linear operators, to present a particular system. For Discrete Time Markov Chains (DTMC), cf. e.g. [11], it is, for example, sufficient to specify the transition probabilities between two distinct “states” s1 and s2 (these correspond in our case to configurations and not to states in our sense). All these probabilities pij from si to sj can be written down as the (generator) matrix P = (pij)ij of the

DTMC we want to describe. This matrix P is always a stochastic matrix, i.e. a matrix where all row entries sum up to one.
The SOS transition relation for pWhile – as well as its restriction to the reachable configurations of a given program P , which gives a representation of the SOS semantics of a particular program – can also be encoded as a matrix or linear operator (cf. [6]), i.e. the matrix T (or T(P )) indexed by configurations ci = ⟨Si, σi⟩, cj = ⟨Sj, σj⟩ ∈ Conf defined by (T)ci,cj = p if ⟨Si, σi⟩ −→p ⟨Sj, σj⟩ and 0 otherwise. This representation is based on the definition of some enumeration of all (reachable) configurations.
For programs which visit only finitely many configurations, e.g. programs which utilise only a finite subset Z of Z, this will result in a finite-dimensional matrix. In general, for infinite configuration sets, we obtain this way linear operators on the Banach space l1(Conf). We refer to the matrix representation of the semantics of a program as the program Linear Operator Semantics (LOS). It contains exactly the same information as the SOS semantics.
Note that for the rules for pWhile in Table 1 the operator T obtained this way will always be row-normalised, i.e. stochastic, as the transition probabilities for all configurations add up to one: For the deterministic rules there is simply only one possible transition with probability 1 and for the choose statement we have assumed that p1 and p2 are normalised (if this is not the case, we could normalise them before execution as long as they are constants).
If we start in a configuration c = ⟨S, σ⟩ we can represent this initial situation by a (row) vector →c = (0,..., 0, 1, 0 .. .) with only one non-zero entry for the coordinate corresponding to c. If we compute the product →cT we get a vector where each non- zero entry (→cT)i corresponds to a configuration ci reachable from c; in fact, (→cT)i specifies the probabilitiy pi of making a transition c −→pi ci. One can iterate this and obtain the probability of reaching cj in n steps from c as (→cTn)j.
The current computational state →σ, i.e. the possible values of all variables, to- gether with the remaining program S after n steps is given by one of several possible configurations, each of which is reached with a certain probability (depending on the initial configuration c and the semantics of the program P we consider). We can represent this knowledge simply by the vector →cTn(P ). Each of these vectors has entries which sum up to one (as T(P ) is stochastic). In other words we obtain after a number of iterations a distribution on Conf = Stmt × State (where Stmt is the set of all statements in pWhile).
It is well known from the theory of probability, e.g. [11], that distributions (and measures) on X×Y are represented by elements in the tensor product of distributions over X and over Y . As distributions (or measures) on X are sub-sets of the vector space V(X) = {⟨xs, s⟩s∈X | xs ∈ R} = {(xs)s∈X}, we have:
Dist(Conf) ⊆ V(Conf) = V(Stmt × State) = V(Stmt) ⊗ V(State).
We also observe that a function, i.e. a state σ in Var → Value, can be seen as an element in Value|Var| = Value × ... × Value (for every variable we just specify its value in Value = Z ∪ B). Thus, distributions over State can be represented too



Table 2
Linear Operator Semantics for pWhile

by a tensor product and T is a (stochastic) linear operator on
V(Conf) = V(Stmt) ⊗ V(Value|Var|) = V(Stmt) ⊗ V(Value)⊗|Var|.
It is therefore always possible to decompose T into a linear combination of factors T =  k Tk where each factor Tk is a tensor product expressing the transfer of control between two statements and of the operations on single variables, i.e.
Tk = S ⊗ T1 ⊗ ... ⊗ Tn,
where S expresses the control flow between statements and T1,..., Tn define the update of variables x1,..., xn.
In [8] we presented a syntax-directed construction of the LOS semantics of pWhile which avoids translating (infinite) SOS transition relations into matrices but instead constructs directly a tensor product representation of T(S). For this we need to consider a labelled version of the pWhile syntax: S ::= [skip]l  | [stop]l
| [p ← e]l  | S1; S2 | [choose]l p1 : S1 or p2 : S2 | if [b]l then S1 else S2 fi |
while [b]l do S od, with labels l ∈ Lab(S).
For a given program S we then determine (statically) its probabilistic control flow, F(S), i.e. a set of triples ⟨li, pij, lj⟩ which describe a possible transfer of control from label li to label lj with probability pij (where pij = 1 for all deterministic statements, i.e. except for a choose statement). For tests [b]l (in if and while constructs) we distinguish between the control transfer in case the test succeeds or fails by underlining the target label for success.
The Linear Operator Semantics of a pWhile program S is then defined as the operator T = T(S) on V(Lab(S) × State). It is given by a linear combination of local updates T(li, lj):

T(S) = 
⟨i,pij ,j⟩∈F (S)
pijT(li, lj).

The aim of T(S) is to collect for every triple in the probabilistic flow F(S) of S its effects, weighted according the probability associated to this triple. The operators T(li, lj) which implement the local state updates and control transfers from li to lj are presented in Table 2.
Each local operator T(li, lj) is of the form E(li, lj) ⊗ N where the first factor
E(li, lj) realises the transfer of control from label li to label lj while the second



Table 3
Test and Update Operators for pWhile

factor N represents a state update or – in the case of tests – a filter operator.
The matrix units E(m, n) contain only one non-zero entry,  namely (E(m, n))mn = 1, and I is the identity operator. Using these basic building blocks we can define a number of “filters” P as depicted in Table 3. The operator P(c) has only one non-zero entry: the diagonal element Pcc = 1, i.e. P(c) = E(c, c). This operator extracts the probability corresponding to the c-th coordinate of a vector, i.e. for →x = (xi)i the multiplication with P(c) results in a vector x→' = →xP(c) with only one non-zero coordinate, namely x' = xc. The operator P(σ) performs a similar test for a vector representing the probabilistic state of the computation. It filters the probability that the computation is in a classical state σ. This is achieved by checking whether each variable xi has the value specified by σ namely σ(xi). Fi- nally, the operator P(e = c) filters those states where the values of the variables xi are such that the evaluation of the expression e results in c. The number of (diagonal) non-zero entries of this operator is exactly the number of states σ for which E (e)σ = c.
The update operators U implement the actual state changes. From an initial probabilistic state →σ – i.e. a distribution over classical states – we get a new prob- abilistic state →σ' via →σU. The simple operator U(c) implements the deterministic update of a variable xi to the constant c: Whatever the value(s) of xi are, after applying U(c) to the state vector describing xi, we get a point distribution express- ing the fact that the value of xi is now certainly c. The operator U(xk ← c) puts U(c) into the context of other variables: Most factors in the tensor product are identities, i.e. most variables keep their previous values, only xk is deterministically updated to its new constant value c using the previously defined U(c) operator. The operator U(xk ← e) updates a variable not to a constant but to the value of an expression e. This update is realised using the filter operator P(e = c): For all possible values c of e we select those states where e evaluates to c and then update xk to this c.
For the skip and stop no changes to the state happen, we only transfer control (deterministically) to the next statement or loop on the current (terminal) statement using matrix units E. Also in the case of a choose there is no change to the state but only a transfer of control, however the probabilities pij will in general be different from 1, unlike skip. With assignments we have both a state update, implemented

using U(p ← e) as well as a control flow step. For tests b we use a filter operator P(b = true) to select those states which pass the test or P(b = false) fail it to determine to which label control will pass.
Note that P(b = true) + P(b = false) = I, i.e. at any test b every state will cause exactly one (unambiguous) control transfer. We allow in pWhile only for constant probabilities pi in the choose construct, which sum up to 1 and as with classical While we have no “blocked” configurations (even the terminal stop statements ‘loop’).

Some Notations.
In order to simplify the presentation, we will assume in this paper that each variable can only take values in a finite set. Thus the LOS of the programs we consider – even if they are non-terminating – is presented by finite dimensional matrices. It is possible to generalise our setting to infinite dimensional vector spaces, in particular if we base the LOS on a Hilbert space structure 4 (for details see standard references like [4,13] etc.). Finite dimensional real vector spaces are always also Hilbert spaces.
Linear operators, i.e. linear maps on a vector space, can be represented as matrices. This provides a useful notation for expressing and combining linear oper- ators. We use row vectors, so F(x) = →xF and F ◦ G = GF, →x being a row vector, F and G being both linear operators and their associated matrices. We usually simplify the notation of vectors by →x = x. We denote the identity matrix by I and, if necessary, we indicate its dimension by a sub-script index, i.e. In is the n × n diagonal matrix In = diag(1, 1,..., 1). Any Hilbert space comes with a norm defined by the inner product:  x  2 = ⟨x, x⟩. This norm is extended to the linear operators via the operator norm defined by  F  = supx xF  . This induced norm is said to be sub-multiplicative because by definition, we have xF  ≤  x  · F  and  GF  ≤ G  · F  . We denote by .t the transposition of matrices and vectors,
i.e. ((at)ij) = (aji). In order to save space, we will sometimes use the transposed form of matrices and vectors.
Relational Static Analysis
Muchnick and Jones describe in [12] as an independent attribute method a flow analysis where a state at each control point is the conjunction of possible attributes for each variable, and as relational method a flow analysis where a state at a control point is the possible conjunction of attributes for each variable.
As an example, consider the following small program which assignes the value of y to x which both can take values in the set {1, 2}:
var x,y:[1,2] begin x:=y; stop
We could obtain here two analyses: (i) x ∈ {1, 2} and y ∈ {1, 2} and (ii) x = y, which describe the state at the end of the program. We refer to analysis (i) as non-

4 A linear space is a Hilbert space if it has a scalar (or inner) product ⟨., .⟩ and it is topologically complete wrt the norm generated by the scalar product.

relational, because it forgets the relation x = y, so it allows unreachable states like x = 1, y = 2. Analysis (ii) is relational, because it can remember information linking variables together. For this program, the equality relation is recorded, but as values have been forgotten, the result also contains unreachable states like x = 3, y = 3.
One can also think of weakly relational analyses where some information about the relationship between variables is preserved. Well known examples of such weakly relational analyses are zone and octagon abstraction [14].
Note that there is no direct way to compare the analyses (i) and (ii) above in the classical framework. To overcome this we will present a framework which allows us to measure the precision of abstractions quantitatively. As in the classical case (cf e.g. [16, 4.4.2]), we will see that relationality and tensor product are closely related. Additionally, in our setting we can even express the “strength” of dependencies, i.e. the correlation between (abstract) properties. For our quantitative analysis we will use the PAI framework which we will recall briefly in the next section.

Probabilistic Abstract Interpretation
We abstract the Linear Operator Semantics in the framework of Probabilistic Ab- stract Interpretation [10,9]. The basic idea behind Abstract Interpretation is to analyse a program by looking at a simplified or abstract semantics which only reg- isters aspects of the program that are relevant to the specific analysis. Typically, these aspects are encoded in the definition of an abstract domain which is usually structured, like the concrete domain, as a complete partial order. In the standard Abstract Interpretation theory by Cousot and Cousot the translation between the concrete and the abstract semantics is achieved via Galois connections (i.e. pairs of adjoint maps) which guarantee the correctness of the abstraction [5]. In our setting where we deal with linear operators defined on vector spaces, the relation between the concrete and the abstract semantics is formalised via the notion of a linear gen- eralised inverse which can be seen as a linear analogon of a Galois connection [10]. This is the Moore-Penrose pseudo-inverse which is defined below. We refer in the following definitions to the general notion of Hilbert spaces as our (concrete and abstract) probabilistic domains, although as mentioned in Section 2, in this paper we restrict ourself mainly to the consideration of finite-dimensional vector spaces.
Definition 3.1 Let H1 and H2 be two Hilbert spaces and let A : H1 '→ H2 be a bounded linear map between them. A bounded linear map A† = G : H2 '→ H1 is the Moore-Penrose pseudo-inverse of A iff
A ◦ G = PA and G ◦ A = PG,
where PA and PG denote orthogonal projections onto the ranges of A and G.
A linear operator P on a Hilbert space H is called an orthogonal projection (operator) if we have PP = P2 = P∗ – more about this notion in Section 4.1. We define the image or range, Im(A), of a linear operator A as the set {Ax | x ∈ H}, and its kernel by Ker (A) = {x ∈ H | A(x) = o}, with o the null vector in H, i.e. the neutral element with respect to vector addition.

If C an D are two Hilbert spaces, and A : C → D and G : D → C are bounded linear operators between (the concrete domain) C and (the abstract domain) D, such that G is the Moore-Penrose pseudo-inverse of A, then we say that (C, A, D, G) forms a probabilistic abstract interpretation.
The Moore-Penrose pseudo-inverse, if it exists, is always unique. A necessary and sufficient condition for the existence of the Moore-Penrose pseudo-inverse for a bounded linear operator A on a Hilbert space H is that A is normally solvable, i.e. its range Im(A) is closed [3, Thm 4.24]. All operators on a finite dimensional Hilbert space are Moore-Penrose invertible. The properties of the Moore-Penrose pseudo- inverse (cf. e.g. [1]) guarantee a form of optimality of the abstractions constructed via PAI; in fact, they are the closest to the concrete semantics one can construct, where closeness is defined via the distance induced by the norm on the Hilbert space. As this is a numerical quantity, we can get an estimate of the information lost in the abstraction [10].



Relational Abstraction: Tensor Product
In the following, we consider programs with l labels, v variables, and n possible values for each variable. This means that the first factor in each Tk is a l×l matrix (specifying the control steps within the program) and that each Tk is a tensor product of v + 1 factors (the control flow part and one factor for each variable), and finally, each of the factors corresponding to the variables is an n × n matrix. The three abstraction operators we define below are all of the form A(S) = Il ⊗ S, where the control flow part of the LOS is left unchanged (the first factor is the identity) and S is a matrix describing the individual abstraction we have in mind.
Given a concrete semantics T and an abstraction operator A, we can construct the abstracted version in PAI as T# = A†TA. As our LOS is constructed using tensor products it is important that the Moore-Penrose pseudo-inverse of a tensor product can easily be computed as follows [1, 2.1,Ex 3]:
(A1 ⊗ A2 ⊗ ... ⊗ An)† = A† ⊗ A† ⊗ ... ⊗ A† .
1	2	n
The abstraction of a semantics of the form T = Σk Tk is simply
T# = A†TA = Σ A†TkA = Σ T#.
k	k
The tensor product factor in each Tk = T1k ⊗ ... ⊗ Tnk can be treated separately if the abstraction is of the above form: T# = A†TkA = (A1 ⊗ ... ⊗ An)†(T1k ⊗

... ⊗ T
)(A
⊗ ... ⊗ A
) = (A† T  A ) ⊗ ... ⊗ (A† T
A ).

nk	1
n	1  1k  1
n  nk  n

An abstraction of the form S =  v	Ai (with non-trivial Ai) which abstracts
the concrete state as a tensor product results in a relational analysis. The struc- ture of the LOS semantics guarantees that dependencies and correlations between variables are preserved in the abstract semantics T#.

zz	y	zz	y
x  zzz 1	2	x

1	0.7	0
2	0	0.3
y	0.7	0.3
(a)
0.7
0.3


(b)

Fig. 1. Probability Distributions
Non-Relational or Independent Abstraction
Let us first describe intuitively a probabilistic abstraction which ignores the relations between variables. In the analysis (i) of the program x, y ∈ {1, 2}; x:=y; given in Section 3, we can interpret the two variables x and y as random variables. Then the original state represents the joint probability distribution for both variables, i.e. a tensor product. The non-relational abstraction consists in retaining only their marginal probability distribution, see e.g. [11, Chapter IX].
For example, if x ∈ {1, 2} takes value 1 with probability 0.7, and otherwise we have x = 2, then the final state of the program is depicted in Figure 1(a). The centre of the array represents the concrete state, e.g. P (x = 1 ∧ y = 2) = 0. The last row and column represents the abstract state. If we consider only the abstract state, unreachable states like x = 1, y = 2 are no longer forbidden, because this state also abstracts the situation in Figure 1(b).
This kind of abstractions corresponds to a combination of abstractions for each variable which ignore the values of the other variables. This can be easily achieved by utilising the forgetful PAI which is represented by the abstraction Af = (1,..., 1)t. If it is applied to any distribution it results in the single scalar 1 (i.e. d · Af = (1)). Thus, if we abstract the values of a single variable we need an abstraction of the form
k−1	v
A(xk) = (  Af ) ⊗ Ak ⊗ (   Af )

with Ak the intended abstraction of (x)k. We can combine these abstractions simply
using the direct sum, i.e. S =  v	A(xk).

Weakly Relational Abstraction
In static analysis, forgetting all relationship between variables is a strong abstrac- tion which destroys important information. Attempts to overcome this and to retain some relational information lead to simple and rather efficient algorithms. In his thesis [14], Antoine Min´e introduces weakly relational domains, which rely on lim- ited linear constraints. The zone abstract domain handles invariants of the form X − Y ≤ c and ±X ≤ c. The octagon abstract domain is an extension of the latter and handles invariants of the form ±X ± Y ≤ c.
In the PAI framework we can define weakly relational abstraction by either
allowing weaker abstractions A˜ for some of the variables or abstracting additional
information via abstractions B. More concretely, we can consider abstractions of

the form


k−1	v

A(xk) = (  A˜ i) ⊗ Ak ⊗ (   A˜ i)

and S =   v	A(xk), or alternatively, we could add additional terms in the def-
inition of the non-relational abstraction above, i.e.  S = (  v	A(xk)) ⊕   Bl
l

with

k−1	v

A(xk) = (  Af ) ⊗ Ak ⊗ (   Af ),

and Bl some abstractions, e.g. the difference between any pair of variables as in Min´e’s zone and octagon analysis.

PAI and Precision
We can restrict ourselves w.l.o.g. to abstraction operators which are surjective, i.e. A(C) = D. In fact, given a PAI (C, A, D, G), we can always partition the abstract domain D by identifying those elements with the same concrete meaning. In this way we can ensure that any abstract object in D is the image of a concrete object in C, i.e. we reduce the abstract domain to one which does not contain redundant objects, or equivalently, we turn the abstraction operator A into a surjective one. In this case the closed subspace of C corresponding to the projection G ◦ A = PG is isomorphic to A(C); thus we can restrict ourselves to consider only probabilistic abstract interpretations of the form (C, PG, PG(C), I). This will allow us to identify orthogonal projections on a Hilbert space H (or equivalently its closed subspaces) with all probabilistic abstract interpretations for the given concrete domain H.
Proposition 4.1 Let H be a Hilbert space and let P ⊆ H be a closed subspace of
H. Then (H, A† ◦ A, P, I) is a PAI iff A† ◦ A(H) = P.
Based on this identification, we can define the lattice of probabilistic abstract interpretations on a given Hilbert space H by means of the lattice of orthogonal projections on H.

The Ortholattice of Projections
As it is well-known in Quantum Mechanics, projection operators on a Hilbert space form a non-Boolean – in particular, non-distributive – lattice. This result dates back to the 1936 article by Birkhoff and von Neumann [2] where the authors’ claimed objective was to “find a calculus of propositions which is formally indistinguishable from the calculus of linear subspaces of a Hilbert space with respect to set prod- ucts, linear sums and orthogonal complements, and resembles the usual calculus of propositions with respect to and, or and not”.
If Y is a closed subspace of a Hilbert space H, each vector in H can be expressed uniquely in the form y + z with y ∈ Y and z ∈ Y ⊥, where Y ⊥ is a complementary subspace to Y (i.e. Y +Y ⊥ = H and Y ∩Y ⊥ = {o}). The linear operator P : H → Y

defined by P(y + z) = y is called the (orthogonal) projection from 7 onto Y . It is easy to show that projection operators P are bounded (their norm is always less than or equal to 1) idempotent (P2 = P) and Hermitian. An operator A is said to be self-adjoint or Hermitian if it coincides with its adjoint A∗, that is the unique operator such that the condition ⟨A∗x, y⟩ = ⟨x, Ay⟩ holds for all x, y ∈ 7 (cf. e.g. [13, Thm 2.4.2]). In particular, projections are a special kind of self-adjoint operators, that is positive operators. An operator A is called positive, denoted by A ± 0, if there exists an operator B such that A = B∗B.
Projections can be identified with the closed subspaces of 7. As the range Im(E) = {xE | x ∈ 7} of an orthogonal projection is a closed subspace, (cf. [4, Proposition II.3.2.b]), this correspondence is defined by associating to each pro- jection on 7 its range Im(E). We define the ortho-complement E⊥ of E as the projection with range (Im(E))⊥. Note that for all E, the following holds:
E⊥ = I — E.
The closed subspaces of 7 form a complete lattice under the operations of intersec- tion and closed linear span. The one-to-one correspondence between this set and the collection P (7) of all orthogonal projections on 7 allows us to transfer the lattice structure of the set of all closed subspaces of 7 to P (7).
A partial order on projections (and in general on self-adjoint operators) can be defined directly by E ± F iff F — E is positive (e.g. [13, p105]). This is equivalent to the partial order defined via set inclusion on closed subspaces. More precisely, if E and F are projections from a Hilbert space 7 onto closed subspaces Y and Z respectively, then E ± F iff Y ⊆ Z (cf. [13, Proposition 2.5.2]).
The projections У(7) form a complete lattice with respect to this order, i.e. the least upper bound (lub) E H F and the greatest lower bound (glb) E H F always exist for any pair E and F and we have (E H F)⊥ = E⊥ H F⊥. The bottom element is given by the projection onto the null space, i.e. the operator mapping all vectors x ∈7 to the null vector, and the top element is the identity operator I.
The problem of concretely constructing the lub E H F and glb E H F of two orthogonal projections E and F on 7 is in general considered as being not trivial. However, for commutative projections this can be constructed as E H F = EF = FE and in the general case, using the Moore-Penrose pseudo-inverse, according to [1]:
E H F = 2E : F = 2E(E + F)†F.
In Example 4.2 we will illustrate the use of this lattice structure on projections for comparing and constructing PAI’s.

Example 4.2 We consider a small example presented in [18], where the concrete domain is the lattice Sign depicted in Figure 2.
Suppose that the concrete function to be analysed is sq : Sign → Sign defined as in Fig. 2. By fixing an enumeration of the elements in Sign we can lift the domain to a probabilistic domain. We consider the enumeration: 1. ∅, 2. 0, 3. ≤ 0,


, • Z ¸¸

sq(∅)= ∅
⎛⎜ 1 0 0 0 0 ⎞


≤ 0 •
,,,,,
¸¸¸¸
¸¸¸¸z˛
,,,•≥ 0
sq(0) = 0
sq(≤ 0) = ≥ 0
0 1 0 0 0 
S = ⎜ 0 0 0 1 0 ⎟


J	sq
• ∅ 

(Z) = ≥ 0
0 0 0 1 0 
⎝ 0 0 0 1 0 ⎠

Fig. 2. The lattice Sign, the concrete function sq and its matrix S
4. ≥ 0, and 5. Z, and define the vector space
V(Sign) =  Σ aii | i ∈ Sign, ai ∈ R 
which is isomorphic to the 5-dimensional real vector space R5. We can then define the matrix S as in Fig. 2 representing the linear operator on V(Sign) corresponding to the function sq.
The ortholattice P (V(Sign)) (or equivalently the ortholattice of all closed sub- space of V(Sign)) gives all possible PAI’s on the given concrete domain as depicted in Figure 3. The matrix representation of these projections is as follows:

1 1 1 1 1
    
5 5 5 5 5
1 1	1
 	
3 3	3
1 1  0 0 0 1
0 1 0 0 0 0 1

1 1
B 5  5
1 1 1
5 5 5 C
1 1	1
B 3  3	3	C
1 1
B 2  2
0 0 0
1 1
B	4 4
1 1
4 4 C

P1 = B 1
1 1 1
1 C , P2 = B 0 0 1
0 1 C , P3 = B 0 0 1
1 1 C , P4 = B 0 1 1 1	C
 	   

5 5 5 5 5
1 1 1 1 1
5 5 5 5 5
1 1	1
 	
3	3
0 CA
3 3 3
1 1 1
3 3 3
4 4 4 4
1 1 1 1
4 4 4 4

1 1 1 1 1
5 5 5 5 5
0 0 1	1
1 1 1
3 3
1 1 1 1
4 4 4 4

1 1 1
3 3 3
0 0 1
0 1 0 0 0 0 1
1 1 0 0 0 1
0 1 0 0 0 0 1

1 1 1
B 3 3 3
1	1
B	2	2	C
1 1
B 2  2
0 0 0
0 1 0 0 0

P5 = B 1 1 1
0 0 C , P6 = B 0 0 1
0 1 C , P7 = B 0 0 1
0 1 C , P8 = B 0 0 1 1	C

B@ 0 0 0 1
1 CA
B@ 0 1
1 0 CA
B@ 0 0 0 1 0 CA
B@ 0 0 1
1 1 CA

0 0 0 1 1
0 0 1	1
0 0 1	1
1 1 1
3 3

1 1 0 0 0 1
0 1 0 0 0 0 1
0 1 0 0 0 0 1
1 1 0 0 0 1

1 1
B 2  2
0 0 0
1 1
B	2 2
0 1 0 0 0
1 1
B 2  2
0 0 0 

P9 = B 0 0 1 0 0 C , P10 = B 0 1 1 0 0 C , P11 = B 0 0 1
0 1 C , P12 = B 0 0 1 0 0 C

B@ 0 0 0 1 1 CA	B@ 0 0 0 1
1 CA
B@ 0 0 0 1 0 CA	B@ 0 0 0 1 0 CA

0 0 0 1 1
0 0 0 1 1
0 0 1	1
0 0 0 0 1 

0 1 0 0 0 0 1
0 1 0 0 0
B	C
0 1 0 0 0 0 1
0 1 0 0 0 
B

1 1
 
2 2
0 0 0 1 1
0 0 0 1 0 C
0 0 0 0 1 


Precision of PAI’s
Based on the Birkhoff-von Neumann order we can compare probabilistic abstract interpretations by identifying them with the corresponding closed subspaces or or- thogonal projection operators. This allows us to say in some cases that one abstrac- tion is “better” or “more precise” than another, that is to specify a notion of relative precision. We base our approach on the idea originally introduced in [10] that mea-

,,,,, P1 ¸¸¸¸¸¸¸

,,,,,,,,,
¸¸¸¸ ¸¸¸¸¸¸¸¸¸

,¸,,c,
,	z 
 
¸¸¸z 

P2 ¸	P3 ¸¸¸ ¸ ¸ ¸ ¸ 
P4 ¸¸¸¸¸¸
P5 ¸

¸¸¸¸     ¸¸¸¸
¸¸¸ ¸ ¸¸¸
¸¸¸ ¸ ¸¸
¸¸¸

 ,  z , 
z , 
¸¸¸¸¸z ,  ¸¸¸¸¸zz 

P6 ¸¸ ,
P7 ¸¸¸
,, P8 ¸¸¸
,, P9
, P10

¸¸¸¸¸¸¸
¸¸,¸,¸,¸,,
¸¸,¸,¸,¸,,
,,,,,

¸¸zP˛J , ,, ,	¸¸¸zP˛ , ,, ,	¸¸¸zP˛J , ,, ,
11 ¸¸¸¸¸¸¸	12	,,,,,, 13
¸¸¸z˛J , ,, ,
14
Fig. 3. The Lattice P(V (Sign))
suring the precision of an abstraction is actually measuring the completeness with respect to the concrete problem.
Completeness is not an essential requirement of an analysis, but rather an ideal situation which does not occur very often in practice. Intuitively, it expresses the property of an abstraction which “makes no mistakes”. In a typical analysis frame- work, a function f : A '→B is given whose properties have to be analysed, and two different abstractions are specified on the input and the output domains respec- tively. In this case, a classical abstract interpretation is given by two abstraction functions, α and α', mapping the input domain A and the output domain B into
an abstract input domain A# and an abstract output domain B# respectively, and
by an abstract semantical functions f # : A# '→ B#. Then the abstract function f # : A# '→ B# is said to be a complete approximation of a concrete function f : A '→B if α' ◦ f = f # ◦ α.
This notion of completeness applies essentially unchanged in the PAI setting by simply replacing classical domains and functions with probabilistic domains and lin- ear functions. As shown e.g. in [5,18], the notion of completeness can be formulated in terms of closure operators by the equation
η ◦ f = η ◦ f ◦ ρ,
where ρ is a closure operator on A expressing the input property, and η is a closure operator on B expressing the output property. This translates in the PAI setting to the following definition of completeness.
Definition 4.3 Given two Hilbert spaces C and D and a bounded linear map F between them, i.e. F : C → D, then we say that a pair of projections P : D → D and R : C → C is complete for F if and only if
FP = RFP.
Obviously, this also includes the special case of a bounded linear operator F : C '→ C, where domain and codomain coincide (like e.g. for F being the fixpoint operator defining the semantics of a given program). In this case we have that a projection P on C is complete for F iff the pair (P, P) is complete for F, i.e. FP = PFP. It is obvious that for any F there always exist at least two complete abstractions as for the trivial projections – i.e. identity P = I and zero projection P = O – the equation FP = PFP is obviously fulfilled.

By using the equation defining the completeness of an abstraction (P, R) for a function F we can get an estimate of its precision by measuring the “difference” between FP and its optimal version RFP. Intuitively, this difference represents the information missing in the given abstraction to get completeness or, in other words, the quantity of “false positives” in the associated analysis. To this purpose we use the Hilbert space norm and define the function PrecF : P (7)2 → R for a given semantical function F by
PrecF(P, R) =  FP — RFP  =  (I — R)FP  =  R⊥FP  .
The function PrecF introduces a total ordering on all the approximations for F. This ordering is compatible with the Birkhoff-von Neumann ordering as shown by the following proposition.
Proposition 4.4 Let F : 71 '→ 72 be a bounded linear operator between two Hilbert spaces 71 and 72, and let P1, P2 ∈ P (72) and R1, R2 ∈ P (71). Then we have that if P1 ± P2 and R2 ± R1 then
PrecF(P1, R1) ≤ PrecF(P2, R2).
Note that, since PrecF(P, R) is a number expressing the loss in terms of com- pleteness of the given abstraction, the smaller it is the more precise is the resulting analysis. We will prove Proposition 4.4 based on the following two properties.
Lemma 4.5 (Order and ortho-complement) Let P and R be two orthogonal projections on a Hilbert space 7. Then
P ± R	⇒	R⊥ ± P⊥
Proof. By [13, Prop. 2.5.2] we have that P ± R iff Im(P) ⊆ Im(R). Since the following property holds for the ortho-complement sub-spaces of Hilbert spaces (e.g. [15])


we have that
A ⊆ B	⇒  B⊥ ⊆ A⊥,

Im(P) ⊆ Im(R)	⇒  (Im(R))⊥ ⊆ (Im(P))⊥.

By definition, R⊥ and P⊥ are the projections with ranges (Im(R))⊥ and (Im(P))⊥, respectively. Then apply again [13, Prop. 2.5.2].	 
Lemma 4.6 (Order and composition) Let P and R be two orthogonal projec- tions on a Hilbert space 7, then
P ± R	⇒	P = PR = RP
Proof. See [13, Prop. 2.5.2].	 
We now can prove Proposition 4.4.
Proof. By the hypothesis that R2 ± R1 and by Lemma 4.5 and Lemma 4.6 we have for all P ∈ P (72),
R⊥FP  =  R⊥R⊥FP  ≤  R⊥ · R⊥FP  ≤  R⊥FP ,
1	1	2	1	2	2

where the second inequality is simply due to the sub-multiplicativity of the norm and the last inequality is based on the fact that the norm of an orthogonal projection is always less or equal to 1.
On the other hand, since P1 ± P2 we have by Lemma 4.6,
 R⊥FP1  =  R⊥FP2P1  .
2	2
Therefore we have
 R⊥FP1  ≤ R⊥FP1  =  R⊥FP2P1  ≤ R⊥FP2  ,
1	2	2	2
where the last inequality is again due to the sub-multiplicativity of the norm and the fact that the norm of an orthogonal projection is always less or equal to 1. 
Proposition 4.4 establishes in particular that the (relative) precision PrecF(P, R) for any semantical function F is monotone in P and anti-monotone in R with respect to the Birkhoff-von Neumann order, i.e.  P1 ± P2 im- plies PrecF(P1, R) ≤ PrecF(P2, R) for any fixed R and R1 ± R2 implies PrecF(P, R1) ≤ PrecF(P, R2) for any fixed P, respectively.
Example 4.7 Consider again Example 4.2. The following table presents the pre- cisions PrecS(Pi, Pj) of the abstractions for the square function S defined by each pair of projections (Pi, Pj), i, j = 1, 2,..., 14 in the lattice of PAI’s on V(Sign).
P1  P2  P3  P4  P5  P6  P7 P8 P9  P10 P11 P12 P13 P14
We see from this table that with respect to the input property P3, the abstrac- tions P1, P2, P3, P5, P7, P9 and P12 are complete probabilistic abstract interpre- tations for the square function as the corresponding entries in the table are zero. This reflects the classical situation in the original example of [18]. Moreover, in our setting the non-zero entries indicate the degree by which a PAI fails to be complete with respect to the square function.

Examples
The aim of the following examples is to illustrate in very simple cases how the relationality improves the precision of abstractions and how to measure the quality of abstractions. The abstractions Ak we consider here are variations of k-cast, i.e.

arithmetics modulo k, in our case k = 2 (simple parity analysis) and k = 4. Besides this we also need to consider the forgetful abstraction A1 = Af which “ignores” the value of a given variable. For a single variable the three abstractions are represented by matrices with entries: (Ak)ij = 1 if i mod k = j — 1, and zero otherwise.
We then construct the abstractions of v variables x1, x2,..., xv as a linear map from the v-fold tensor product into some abstract domain. This domain is typically a direct sum of “small” vector spaces which record the value of the xi’s modulo 2 or 4. These abstractions are all of the form
v	i−1	v
S =   S(xi) with S(xi) = (  S¬i) ⊗ Si ⊗ (   S¬i)

where Si is the abstraction applied to the variable xi while S¬i represent the ab- stractions of the other variables. We compare three variations of S: (i) Sr with Si = S¬i = A4, (ii) Sw with Si = S4, S¬i = A2, and (iii) Sn with Si = S4, S¬i = A1, more concretely:
v	i−1	v

Sr =   S(xi)  with  S(xi) = (   A4) ⊗ A4 ⊗ ( 
A4)

i=1 v
k=1 i−1
k=i+1 v

Sw =   S(xi)  with  S(xi) = (  A2) ⊗ A4 ⊗ ( 
A2)

i=1 v
k=1 i−1
k=i+1 v

Sn =   S(xi)  with  S(xi) = (   A1) ⊗ A4 ⊗ ( 

A1)


The abstraction Sr is fully relational; in fact it is just v copies of the same fully
relational abstraction  v	A4. We thus can restrict ourselves to just one of the
components in the direct sum. We have Sr : V(n)⊗v → V(4)⊗v , i.e. the abstract
domain is also a tensor product, but a reduced one; for three variables the abstract
domain has 43 = 64 dimensions. This abstraction preserves the dependency between the “4ness” of all variables. For example, we record not only (the probability) that xi is a multiple of 4 (i.e. xi mod 4 = 0) but also (the probability) that xi and xj are multiples of 4 at the same time.
The abstraction Sn is non-relational or independent. In this abstraction we abstract only (the probabilities) that xi has a certain remainder when divided by 4, but the relationship between the “4ness” of variables is lost. We extract, for example, no information whether xi is multiple of 4 whenever xj is. This is achieved
by abstracting in each term in the direct sum  v	S(xi) only the “4ness” of one
variable, namely xi, while ignoring the value of all the others. The abstraction is of the type Sn : V(n)⊗v → V(4)⊕v = V(4)v ; for three variables the abstract domain is a 3 ∗ 4 = 12 dimensional abstract domain.
The abstraction Sw is weakly relational. We get in this case some infor- mation about the correlation between the values of two variables, but not, like with Sr modulo 4 but only about the parity (xi mod 2).  The dimensionality of this abstraction thus is a compromise between Sr and Sn , i.e.  we have

Sw : V(n)⊗v → (V(4) ⊗  v−1 V(2))⊕v . In the case of three variables this means that the abstract domain has 3 × 8 = 24 dimensions.
Regarding the precision of these three abstractions when used to analyse var- ious extremely short programs our experiments resulted in the following. For a program like var x:[0..10]; begin x:=k; stop (with k = 1 or k = 4) and var x:[0..10]; y:[0..10]; begin x:=y; stop we obtain the following relative precisions PrecT(P, R).
zz	R	zz	R




For another set of programs var x:[0..10]; y:[0..3]; begin x:=k*y; stop
we get for k = 2 and k = 3:
zz	R	zz	R




All these examples, despite their trivial nature, exhibit quite clearly how rela- tionality can improve the precision of an analysis. When we go from left to right,
i.e. from the trivial abstraction to Sn, to Sw, to Sr, and finally to the concrete semantics PrecT(P, R) decreases. In particular the last row (comparison with the concrete semantics) describes a kind of absolute precision measure. Similarily, the
first row describes the defect of the trivial abstraction with respect to what an other abstraction – increasing until we compare it with the concrete semantics. We also see that the precision measure gives different results for different types of programs. Sometimes the weakly relational analysis improves the analysis, sometimes it does not – in some cases the improvement could be considered to be relevant in other it is only marginal. In principle one could use this information to decide whether it is worth to apply Sn, Sw, or Sr by trading precision against the (sometimes substantial) additional overhead, i.e. dimensionality, of an analysis.

Conclusion
In this paper we have presented a framework for the relational analysis of a simple imperative language which is based on a linear operator semantics and probabilistic abstract interpretation. The main advantages of this are:
the exploitation of the tensor product operation for obtaining fully relational analyses: as both the concrete and the abstract semantics can be factorised, different properties can be analysed simultaneously;
the use of probabilistic domains (in the form of finite dimensional inner prod-

uct linear spaces) and the Moore-Penrose pseudo-inverse for a more speculative interpretation of the analysis results and in particular for obtaining a quanti- tative estimate of their precision.
The latter point could be achieved via the identification of each abstraction A with its associated orthogonal projection AA† and the use of the Birkhoff - von Neumann lattice of projections. Based on the lattice of PAI’s we have introduced a notion of relative precision expressing the degree of ‘incompleteness’ of a given abstraction via a number calculated as the norm of an appropriate linear opera- tor. We have applied this to a small example comparing the precision of various numerical analyses.
Other works have faced the problem of quantifying the precision of a static analysis. In [17], Rountev, Kagan and Gibas present an approach to evaluating the imprecision of a static analysis via the lower and upper bound of the set of false positive. This is a quite standard method in classical static analysis, but the authors suggest that for each fact a human experimenter should find a proof that a given result is not a false positive. This approach leads to know the exact percentage of false positive for one program and analysis. However, as it must be done by hand, there is a serious limitation to the size of the set to be tested and it could not be used inside a tool, for example to select the best analysis for a given program. To this aim our approach seems to be more promising: Given that we have a structured lattice of abstract interpretations, and a way to measure precision, we can think of a more systematic approach in the selection of the abstract domain. In particular, this can be achieved practically via a statistical interpretation of the number expressing the relative precision.
An important point in further investigations is that the representation of the semantics of a program via a tensor product establishes a connection between the notion of relational dependencies in program analysis and that of correlation and independence in statistics [11].

References
A. Ben-Israel and T.N.E. Greville. Generalised Inverses. Springer Verlag, 2nd edition, 2003.
G. Birkhoff and J. von Neumann. The logic of quantum mechanics. Annals of Mathematics, 37:823–843, 1936.
A. B¨ottcher and B. Silbermann. Introduction to Large Truncated Toeplitz Matrices. Springer Verlag, 1999.
John Conway. A Course in Functional Analysis, volume 96 of Graduate Texts in Mathematics. Springer Verlag, second edition, 1990.
P. Cousot and R. Cousot. Systematic Design of Program Analysis Frameworks. In Proceedings of POPL’79, pages 269–282, 1979.
A. Di Pierro, C. Hankin, and H. Wiklicky. Measuring the confinement of probabilistic systems.
Theoretical Computer Science, 340(1):3–56, 2005.
A. Di Pierro, C. Hankin, and H. Wiklicky. On probabilistic techniques for data flow analysis. In
Proceedings of QAPL’07, volume 190 of ENTCS, pages 59–77, 2007.
A. Di Pierro, C. Hankin, and H. Wiklicky. A systematic approach to probabilistic pointer analysis. Proceedings of APLAS’07, to appear.


A. Di Pierro and H. Wiklicky. Concurrent Constraint Programming: Towards Probabilistic Abstract Interpretation. In Proceedings of PPDP’00, pages 127–138, 2000.
A. Di Pierro and H. Wiklicky. Measuring the precision of abstract interpretations. In Proc. of LOPSTR’00, volume 2042 of LNCS, pages 147–164. Springer Verlag, 2001.
W. Feller. An Introduction to Probability Theory and Its Applications, Vol. 1. Wiley & Sons, New York, 3rd edition, 1970.
N.D. Jones and S.S. Muchnick. Complexity of flow analysis, inductive assertion synthesis and a language due to Dijkstra. In S.S. Muchnick and N.D. Jones, editors, Program Flow Analysis: Theory and Applications, pages 380–393. Prentice-Hall, 1981.
R.V. Kadison and J.R. Ringrose. Fundamentals of the Theory of Operator Algebras I, volume 15 of
Graduate Studies in Mathematics. AMS, 1997.
A. Min´e. Weakly Relational Numerical Abstract Domains. PhD thesis, E´cole Normale Sup´erieure, Paris, 2004.
J.M. Monier. Analyse MP. Dunod, 2004.
F. Nielson, H. Riis Nielson, and C. Hankin. Principles of Program Analysis. Springer Verlag, 1999.
A. Rountev, S. Kagan, and M. Gibas. Evaluating the imprecision of static analysis. In Workshop on Program Analysis for Software Tools and Engineering, 2004.
Francesca Scozzari. Domain theory in abstract interpretation: equations, completeness and logic. PhD thesis, University of Siena, 1999.
