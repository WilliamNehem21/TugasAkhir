The importance of memory efficiency is becoming increasingly significant as a determinant of performance for numerical algorithms, potentially surpassing the influence of the number of floating-point operations. The integration of memory efficiency considerations at the outset of algorithm design is facilitated by computational tools capable of quantifying memory traffic. The Sparse Linear Algebra Memory Model (SLAMM) is realized through a source-to-source translator that accepts a MATLAB algorithm specification and augments it with code to forecast memory traffic.

Our experimentation with various small computational components and comprehensive algorithmic solutions for solving sparse linear systems demonstrates that SLAMM accurately predicts the volume of data loaded from the memory hierarchy to the L1 cache within a 20% margin of error across three distinct computing platforms. SLAMM enables rapid evaluation of memory efficiency for specific design choices during the iterative algorithm design phase and offers an automated method for optimizing existing implementations, reducing the time required for a priori memory analysis from several days to just 20 minutes.

The creation of novel algorithms that exhibit both numerical and memory efficiency is a challenging endeavor that has not been extensively or systematically addressed. Unfortunately, the prevailing approach is to disregard memory efficiency until after the completion of the implementation, which can lead to daunting retroactive code modifications. It is advisable to incorporate memory efficiency considerations into the algorithm from the outset.

Both static and dynamic analyses of the prototype are essential to obtain pertinent information. SLAMM directly performs static analysis and generates a modified version of the prototype augmented with additional MATLAB code blocks, which is executed by the MATLAB interpreter to provide dynamic analysis.

Identification of an identifier in the prototype does not invariably signify that the corresponding variable must be loaded from the memory hierarchy. Fortunately, it is feasible to enhance the accuracy of the base identifier counts through a series of adjustments, capturing translation properties from MATLAB to a compiled language.

The process calculates the amount of data loaded from the memory hierarchy by the "if" expression and the fetching of new data, accumulating the result in the field "wsl" of the structure "slm foo." Similarly, the amount of data stored to the memory hierarchy (in the assignment to "b" at the beginning of the block) is recorded in the field "wss." Each of these assignments comprises two components: the total amount loaded or stored before the execution of the code in "b1" (e.g., "slm foo.wss"), and the amount loaded or stored during this execution (e.g., "slm new.bytes"). The former is initialized to zero in the header, while the latter is a constant determined by "whos."

Notably, this calculation only accounts for the accesses within the scope of the bindings of "b1." Both "new" and "b" have defining occurrences in "b2," making "b2" a gap in the scope of the "b1" bindings for those identifiers. Memory access information associated with the assignment in "b2" is accumulated by the exclusive memory analysis code block "b2."

In this instance, the original expression "t = sin(r) + cos(z)" is split into three separate statements. The temporary variables "slm l1c5" and "slm l1c14" represent the original output arguments of the "sin" and "cos" functions, respectively, and are utilized to calculate the expected result "t." The structures "slm sinl1c5" and "slm cosl1c14" contain the results of the memory analysis of the "sin" and "cos" functions, respectively.

Automated memory analysis furnishes both the capability to swiftly evaluate the memory efficiency of a specific design choice during the design phase and the ability to enhance the memory efficiency of an existing solver. We illustrate both applications by utilizing SLAMM to decrease the execution time of the Parallel Ocean Program (POP), a global ocean model developed at Los Alamos National Laboratory. POP utilizes a preconditioned conjugate gradient solver for updating surface pressure in the barotropic component and is extensively used as the ocean component of the Community Climate System Model. Parallelism on distributed memory computers is supported through the Message Passing Interface (MPI) standard. We examined data movement in POP version 2.0.1, which employs a three-dimensional computational mesh and decomposes the horizontal dimensions into logically rectangular two-dimensional (2D) blocks, distributed across multiple processors.

The primary advantage of the 2D data structure is the provision of regular stride-one access for the matrix-vector multiplication. However, the inclusion of land points leads to potentially a large number of grid points representing land, effectively adding explicitly stored zeros to the matrix. An alternative 1D data structure using compressed sparse row storage could circumvent the inclusion of land points but introduces indirect addressing. Additional details on the changes in data structures are provided in [Reference to the specific details].
