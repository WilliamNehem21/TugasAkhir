With the exponential increase in data volume, there is a growing interest in enabling people to derive value from their data, even when its quality is poor. A significant challenge related to data quality is the imbalanced distribution of different categories within the data, which can adversely affect the performance of data analysis and mining. This imbalance can have a detrimental impact on the effectiveness of traditional classification techniques. To address this issue, this paper introduces the TGT (Train Generate Test) method, which is an innovative oversampling technique designed to handle imbalanced datasets. Through various learning strategies, TGT ensures that the synthetic samples it generates are situated in minority regions of the data. Experimental results demonstrate a substantial improvement in the performance of different classification techniques when TGT is applied to five imbalanced datasets of various types.

Imbalanced data, characterized by a higher number of instances in one class compared to the other, poses challenges for classification as classifiers tend to be biased towards the majority class and exhibit low accuracy for the smaller classes. To address this issue, various techniques such as oversampling and undersampling have been proposed. Oversampling involves generating synthetic instances for the minority class, while undersampling reduces the number of instances in the majority class. This paper presents a novel oversampling technique called TGT, which aims to create a more balanced class distribution by generating synthetic minority class instances using different learning strategies.

Several other oversampling methods have been proposed to address the challenges of imbalanced datasets. For example, a modified version of SMOTE (Synthetic Minority Over-sampling Technique) called WK-SMOTE is designed to address the shortcomings of SMOTE on non-linear problems by producing synthetic instances in the feature space of the classifier. Additionally, RNS (Real value Negative Selection) generates synthetic minority instances when actual minority data is rare, and RBO (Radial Based Oversampling) identifies areas in which minority class artificial instances are to be produced based on the imbalance distribution estimate.

The effectiveness of the proposed TGT method is evaluated against baseline approaches such as SMOTE and its variations, as well as other recent data oversampling techniques, using different datasets and classifiers. The experimental results demonstrate the superior performance of TGT, particularly in datasets with high ratios between the majority and minority classes.

In conclusion, this paper presents an adversarial guided oversampling technique (TGT) for addressing imbalanced datasets. By leveraging two classifiers to model the knowledge about the minority class data and verify the generated samples, TGT demonstrates higher performance compared to standard and recent data oversampling techniques across various datasets and classifiers.