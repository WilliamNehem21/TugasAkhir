The execution time of ResNet-50 has decreased significantly, with Nvidia's results indicating that the task can now be completed in less than 30 seconds. However, this reduced runtime poses a challenge for thorough and reliable evaluation from a benchmarking standpoint. Additionally, the fixed computation load on the HPC system is not fully utilized as the system scales up, leading to underutilization of computing resources on each node.

Version 2.0 and MLPerf (HPC) fall short in addressing the scalability issue and instead prioritize the selection of typical HPC AI applications and parallel-based optimizations. On the other hand, HPL-AI and AIPerf achieve scalability but introduce other challenges. HPL-AI evaluates HPC systems by performing mixed-precision LU decomposition at the kernel level, similar to HPL.

Ensemble learning involves addressing common problems by combining predictions from a group of base models, thereby reducing prediction variance through collective decision-making, known as the "wisdom of crowds." Bagging (bootstrap aggregating) forms a fundamental paradigm of ensemble learning, involving two key components: bootstrapping and aggregating. In bootstrapping, data is sampled with replacement from the original dataset, yielding the bootstrapped dataset. The training process in bagging is highly parallel, as each base model in the ensemble is trained based on its corresponding bootstrapped dataset. Following training, the final decision is aggregated by averaging the predictions of all base models.

The overhead of data parallelism (DP) arises from the need to globally synchronize all model copies at the end of each training step. In contrast, the base model ensemble of HPC AI500 v3.0 is trained with high independence, circumventing synchronization and minimizing communication overhead.