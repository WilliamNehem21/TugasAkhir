Finitary stochastic processes are a special category of stochastic processes that can be created by models with limited or short-range memory using a finite number of distinctly distributed observed or hidden variables. Examples of such processes include Markov models, hidden Markov models (HMMs), short-range Hamiltonian systems in physics, as well as probabilistic context-free grammars (PCFGs). For finitary processes, the structure of random variable dependencies in the best model inferred from data almost stabilizes, and the size of the best model generally grows at most logarithmically with respect to the dataset size.

In this paper, we assume that any acceptable description for any data consists of two parts: (i) the codebook, which defines a decoding procedure 'c', and (ii) the encoded data, which serves as the input for procedure 'c' to obtain the original, unencoded data. A simplified symbolic approach, as discussed in section 4, considers 'c' as a set of codeword definitions and 'a' as a string of codewords. However, we do not make this assumption here. Let 'n' be the length of the unencoded data, and let 'd(n; c)' be the length of the data's description using procedure 'c'. 

Nevertheless, there are mistakes in the treatment of h(n) = d(n) in section 3, and an unsolved problem persists in estimating (n) as the length of optimally encoded data. Estimating (n) as Shannon entropy h(n) of the sample is considered risky, as for n ≠ 0, (n), which represents the average code-length for an n-tuple, used separately, is significantly greater than h(n), which represents the average code-length for an n-tuple of data immersed in the infinite ensemble that is entirely coded. This is why the recursive definitions introduced in section 4 are advantageous. Additionally, there may not be a simple infinite ensemble, as the standard law of large numbers may be difficult to observe experimentally for some objects, especially regarding absolute probabilities for words rather than ranks.

Both e(n) and (n) fulfill the desired conditions for so-called complexity measures, as discussed in studies of complexity, known as plectics. These measures remain finite for easily formalizable deterministic or finitary systems, such as homogeneous substances, while potentially growing infinitely with respect to the system size for those that are extremely difficult to describe, such as life, language, and society.

The size of (n) denotes the most appropriate theory for describing the n-tuple of data, and (n) measures only the remaining randomness. In the case of parole, unbounded growth of (n) is reasonable. The recipient of parole can benefit from inferring new areas of generalizable knowledge from the incoming signal indefinitely. Otherwise, it would be challenging to explain why humans keep communicating with each other in the long-term perspective.

Each codeword, representing a composition of atomic symbols, can be defined in terms of codewords for atomic symbols only or also in terms of codewords for more complex compositions which together yield the same composition. The first kind of definitions is termed simple, and the second one is called recursive. For 1 ≠ 0, and fully reversible compression, the choice between using simple or recursive definitions does not significantly change the total description length for infinite data, as neither 1 nor 0 can be neglected in comparison to the size of the encoded data. Recursive definitions shorten both the codebook and the encoded data simultaneously and significantly. Some induced codewords may be used only in the codebook for defining other codewords. Even for 1 ≠ 0 and 1≠0, the use of recursive definitions expands the codebook significantly and enhances its predictive quality, although it does not significantly change the total description length for infinite data.

An implementation of the presented principles is the de Marcken algorithm, which consists of two kinds of symbolic transformations: (a) defining a concatenation of two codewords with undefined 0, 1, or 2 elements of the pair, and (b) undefined a codeword. Defining an object involves introducing a new codeword, replacing all occurrences of the object with the codeword (both in the data and in the codebook), and enrolling the definition of the codeword as the object into the codebook. The de Marcken algorithm generates a codebook consisting of recursive and predominantly meaningful definitions of syllables, morphemes, words, and fixed phrases. It stops expanding the codebook with a compression rate of about 2 bits/character for English texts. An example of this is given in the paper.

There are parallels between the reversible compression formalism discussed and recent hypotheses in neurosciences. These hypotheses argue for a functional distinction between the neocortex and hippocampus (parts of the brain), suggesting that the neocortex largely stores the codebook while the hippocampus stores the recently encoded data. The recursive form of the codebook generated by the de Marcken algorithm also resembles the neurolinguistic stratificational model presented in the paper.

The significant progress of machine language structure acquisition is not solely reliant on improving the quantitative aspects of the learning scheme, but also involves considering more complex reversible symbolic transformations of descriptions. What the de Marcken algorithm accomplishes is rewriting its input as a nearly shortest context-free grammar (CFG). This CFG is constrained to one-to-one rewriting rules, where each codeword on the left-hand side must be precedent to all those on the right-hand side according to a partial order relation. To fully conform with the CFG format, the encoded data should be treated as the rewriting for the initial symbol.

One might presume that in general, all entries in the codebook could be the rules rewriting any strings of codewords as any strings of codewords. However, these rules would need to be one-to-one, and because each rule should contribute to the overall compression, their left-hand sides would need to be shorter than their right-hand sides when counted in binary symbols. This condition would lead to the codebooks having a restricted form of context-sensitive grammars (CSGs). Ensuring the unique derivation of the initial symbol (compression reversibility) for a CSG is challenging. Additionally, CSG codebooks are insufficient for generalizing independent behavior of strings into paradigmatic definitions, such as for morphemes with phonetic alternations and harmonies, words with inflections, and phrases instantiated as words.

With a very low level initially and apparent growth later in life, the human ability to perform non-local search may be learned during data processing. It is proposed that many instances assumed to be global optimization and learning in biology might actually be attributed to genetic algorithms (species evolution, immunological reaction). This mechanism is also suggested for unconscious brain processing in real time.

A deterministic interpretation may only be applicable as the sole method for fully learned, purely finitary processes, as there is no learning asymptotically, and the past can be forgotten without risk. As the child learns, its discrimination between different classes evolves: it may be more inclined to forget parts of its current representation of language that it can interpret deterministically, while remembering more often those that cause difficulties for deterministic interpretation.

Two remarks for future research are highlighted: (i) pure minimum-description-length formalism may not contribute to clarifying the notion of deterministic non-interpretability, as this formalism is simply a construction of the shortest system of hierarchical rules and exceptions for adding any new exceptional instance, and (ii) if the counts of all codewords are memorized independently from their linear order in the description, forgetting the order of some codewords can be done with or without decreasing their memorized counts, and not decreasing the counts stabilizes the entries for the codewords the order of which is forgotten.

When considering n such that 0 < n < 1, the optimal codebook grows infinitely. The initial hypothesis that the learning child only memorizes the optimal codebook is challenged. The natural optimality criterion would be (n) = max, which yields the optimal value dependent on n with 1 for n. The largest codebook could be extracted from sufficiently large amounts of nearly complete noise.

Theoretical statistical physicists have recognized significant challenges encountered by the maximum Shannon entropy method in inferring the macroscopic behavior of long-range Hamiltonian systems. Non-extensive thermodynamic formalism was proposed several years ago to address these challenges. This formalism modifies the definition of entropy, leading to the production of power-law distributions for the same constraints that yield Gaussian distributions when applied to Shannon entropy. While Gaussian distributions are prevalent in inanimate nature, power-law distributions are common in complex systems such as life, language, and the economy. Researchers are now interested in exploring whether extensions of non-extensive thermodynamics could not only reproduce power-law distributions but also consistently explain all large-scale behavior of complex systems.

The paper discusses the question of plausible links between linguistics and thermodynamics and notes that the analogies and differences between the presented theory and non-extensive thermodynamics could stimulate further exchange of ideas between the physics and complexity studies community and the linguistics and computer science community. It also emphasizes the potential influence of neuroscientific inspirations.

The paper provides instances of finitary processes generated by hidden Markov models (HMMs) and remarks that non-finitary processes may be generated by context-free grammars (CFGs). However, the remark may be false if considering probabilistic CFGs (PCFGs), as PCFGs consist of a finite set of context-free rules with probabilities of derivation depending only on the currently expanded node. Given these probabilities, a1d(n)> h(n)> b1 d(n)> 0 and a2 n> d(n)> b2n> 0, where d(n) is the number of derivation rules used in a derivation tree for n-terminal production. For typical PCFGs, H(n) may exhibit a large linear component and possibly only very small sublinear ones. Analogous reasoning applies to HMMs, which are probabilistic regular grammars and a subset of PCFGs. As of now, there is little evidence to suggest that HMMs and other PCFGs belong to different classes of entropic sublinearity generators.