The initial step in developing more advanced models to extract precise biological information related to crop health, weed presence, and phenological state involves the accurate and automated semantic segmentation of vegetation. While traditional models relying on normalized difference vegetation index (NDVI), near infrared channel (NIR), or RGB have been effective in indicating the presence of vegetation, they are not well-suited for accurately segmenting damaged vegetation. Therefore, this paper presents a comprehensive method for robust vegetation segmentation in RGB images that can handle damaged vegetation. The proposed method involves using a regression convolutional neural network to estimate a virtual NIR channel from an RGB image, computing two newly proposed vegetation indices, namely the infrared-dark channel subtraction (IDCS) and infrared-dark channel ratio (IDCR) indices, from the estimated virtual NIR, and then feeding both the RGB image and the estimated indices into a semantic segmentation deep convolutional neural network to train a model for segmenting vegetation irrespective of damage or condition. The model was tested on 84 plots containing thirteen vegetation species showing varying degrees of damage over 28 days, and the results indicated that the best segmentation was achieved when the input image was augmented with the proposed virtual NIR channel (F1=0.94) and with the proposed IDCR and IDCS vegetation indices (F1=0.95) derived from the estimated NIR channel, while the use of only the RGB image or RGB indices led to inferior performance (RGB F1=0.90, NIR F1=0.82, NDVI F1=0.89).

Previously, various studies have attempted to correlate NDVI and other vegetation indices based on light channels with vegetation coverage estimation. These studies typically involve acquiring experimental data on soil and leaf reflectance of different light channels and correlating those reflectance with combinations of such channels (i.e., vegetation indices), and further correlating these vegetation indices with actual biomass measurements through linear and non-linear regression analysis. More recent research integrates spatial information distribution from RGB pixels to infer the NIR channel. For instance, Khan et al. (2018) and Lima et al. (2019) proposed a method to estimate several vegetation indices using a neural network, but their approach did not estimate the vegetation indices at pixel-level resolution, only providing average vegetation index for the whole image.

To prevent bias, the distribution of images across the training, validation, and test datasets was determined by plots. This approach assigned each crop field (plot) an identification number, and all images from the same plot were assigned to the same subset of data (i.e., train, validation, or test), thus ensuring that images from the same plot taken on consecutive days were assigned to the same set, thereby avoiding contamination of the training, validation, and test sets. Eighty percent of the crop fields (plots) were randomly chosen for training, while the remaining 20% were distributed into validation and test sets, and all images were incorporated into the set determined by their crop field number, resulting in 24 plots for training, 2 for validation, and 3 for testing.

In this study, a semantic regression neural network was proposed to estimate a virtual NIR channel from an RGB image, which was then used to augment the original RGB image, resulting in a multi-channel image used to train a multispectral vegetation segmentation convolutional neural network. This multichannel image included not only the original red, green, and blue channels and the estimated virtual NIR channel, but also different multispectral indices derived from these four channels.

Additionally, another network with a similar structure as the one defined in the previous section was used for plant coverage estimation. The input layer of this network was defined as mxnxk, where m and n represent the height and width of the input image, and k represents the number of channels used. In this case, the number of input channels was k=7, which included the RGB channels of the original image and all the estimated channels from the previous methods (NIR, NDVI, IDCS, and IDCR). The final layer of this network consisted of two output channels of size mxn, with one channel mapping the estimation for the plant coverage segmentation and the other containing the other classes. This network was minimized over a categorical cross-entropy loss function.

The paper also introduced the use of a neural network to estimate the NIR channel and the calculation of additional vegetation indices (NDVI, IDCS, and IDCR) from the RGB and estimated NIR channels. After training, the validation subset of the dataset was used to determine optimal threshold values maximizing balanced accuracy, which were subsequently applied to the testing set. 

The study proposed a convolutional semantic regression network to estimate a virtual near infrared channel from an RGB image (RGB2NIR), with the option to incorporate an adversarial loss. The results demonstrated that the adversarial loss contributed to generating a more accurate estimation for the NIR channel, suggesting its efficacy over using just a convolutional semantic regression network.

Additionally, two novel vegetation indices, IDCS and IDCR, were introduced, and their capability to differentiate vegetation regardless of its damage was demonstrated. These vegetation indices have potential for independent use in vegetation segmentation purposes.