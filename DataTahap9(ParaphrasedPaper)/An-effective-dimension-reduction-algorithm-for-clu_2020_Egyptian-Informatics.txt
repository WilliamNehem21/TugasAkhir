Developing an effective and efficient text-clustering algorithm requires addressing the challenge of high dimensionality by employing an accurate technique for reducing dimensions. The presence of high dimensionality leads to the introduction of noise, which negatively impacts the accuracy of the clustering algorithm. Therefore, any efficient clustering algorithm must extract the primary concepts from the text by eliminating noise and reducing the dimensionality of the data. Dimension reduction techniques are employed to reduce the dimensionality of the data, with the aim of preserving the characteristics of the original data set. One method involves computing the principal components from the data matrix x by performing eigen decomposition of the covariance matrix. An alternative, faster approach involves using singular value decomposition (SVD) to factorize the matrix x into u r rvtr, providing the k sample principal components as the projection of utk on the data matrix x. This projected space achieves PCA after applying a whiten transformation to normalize variances to one and remove any noise. The features extracted using PCA, specifically the projected space after the whiten transformation, are then utilized in the clustering algorithm.

The corpus documents are represented using a term-document matrix, where each column represents a specific document and each row represents a unique term. A document within the corpus is denoted by a vector of terms, with the jth document symbolized as xj=[w1j, w2j,..., wmj]t, where wij represents the term weight and m represents the number of terms in the corpus.

The Reuters 21,578 dataset comprises a collection from the Reuters newswire, containing 21,578 documents across 135 topics. Ten categories were selected from this corpus, and any documents associated with more than one topic were removed, resulting in 7,293 documents (comprising 18,933 unique words) distributed across the ten categories.

Common pre-processing methods for both the Arabic and English corpora were applied, including stopword filtering, stemming using the light stemmer for Arabic and the Porter stemmer for English, as well as utilizing the TF-IDF term weighting formula to construct a term-document matrix for each corpus.