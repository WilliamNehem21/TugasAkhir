Ekman et al. identified six fundamental emotions: happiness, sadness, surprise, fear, disgust, and anger, and demonstrated that these emotions are universally perceived across different cultures. These emotions can be expressed using two orthogonal dimensions: valence, representing the pleasantness or unpleasantness of the emotion, and arousal, representing the level of excitement or calmness. Feldman et al. also noted that individuals may express these emotions in different ways, particularly when asked to express periodic emotions.

The paper is organized into several sections. Section 2 discusses previous related works, while section 3 outlines the main objective of the proposed approach and details the three core components for face, EEG, and fusion recognition approaches. Section 4 presents the evaluation of the proposed approach on the DEAP benchmark, along with a comparison against state-of-the-art approaches and a discussion and analysis of the experimental works. Finally, in section 5, conclusions for the proposed work and future research directions are provided.

The proposed 3D-CNN architecture comprises six basic layers, including the input layer, two convolution layers resulting in 3D feature maps, and subsequent max-pooling layers to down-sample the feature maps. The final layer is a fully connected layer for feature extraction. The input volume dimensions are 5*32*128, representing the temporal information, height, and width of the input frame from the EEG and face domains. For the EEG input, 32 denotes the number of channels, and 128 represents the samples in the frame segment. In the proposed network, the convolution filter has a shape of 3*3*3, with 8 feature maps in the first layer and a max-pooling layer resolution of 2*2*2.

Koelstra et al. developed the DEAP database for human emotion analysis using physiological signals, which includes 32-channel EEG signals and 12 peripheral physiological signals. The dataset has a sampling rate of 512 Hz and is pre-processed to have a sample rate of 128 Hz. Data augmentation is employed to increase the number of samples, with a Gaussian noise signal used to create new noisy versions of the EEG signals during the training phase.

Transfer learning is utilized in the proposed fusion system, where the knowledge learned from a previously trained model's weights is transferred to reduce the generalization error and improve performance. Ensemble learning is also employed to combine diverse learners (individual models) to enhance system stability and overall performance. Two score fusion methods, stacking and bagging, are adopted for the proposed work.

The proposed approach is evaluated using the DEAP dataset, which contains more multi-modal emotion signals and longer duration signals than other datasets. Tang et al. suggested two models for classifying emotions from EEG and peripheral physiological signals, while Liu et al. demonstrated a bi-modal auto-encoder for modeling the shared representation of EEG and peripheral physiological signals.

Two fusion methods, stacking and bagging, are proposed for blending facial expressions and EEG signals for emotion recognition, and both methods outperformed the single modalities. The 3D-CNN network is shown to extract shared representations with better performance in terms of accuracy, leading to improved performance in arousal and valence binary classes.

Finally, the paper concludes by discussing how the proposed scheme achieves better performance in classifying arousal and valence binary classes through an end-to-end deep learning framework that directly maps video data and EEG signals to emotion states without the need for manual feature extraction from input frames.