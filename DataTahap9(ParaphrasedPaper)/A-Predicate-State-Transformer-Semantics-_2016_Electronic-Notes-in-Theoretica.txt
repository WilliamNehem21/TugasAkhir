This paper establishes a connection between Bayesian inference and predicate and state transformer operations from programming semantics and logic. Specifically, it provides a general definition of backward inference through the application of a predicate transformer followed by conditioning, and analogously, defines forward inference as conditioning followed by the application of a state transformer. The paper illustrates these definitions through various examples in discrete and continuous probability theory, as well as in quantum theory.

The organization of the paper is as follows. It introduces the concepts of backward and forward inference in terms of predicate and state transformers, and presents their basic properties. The paper then focuses on demonstrating the impact and effectiveness of these definitions in various scenarios, particularly in discrete and continuous probability theory, and briefly in quantum theory. It elaborates on several examples of inference computations and their outcomes, emphasizing the application of the defined inference in Bayesian networks. The paper highlights the flexibility of the forward/backward distinction and how it can describe different points of inference within the network.

An example of backward inference is provided, illustrating how an observation on the test outcome affects the knowledge of disease occurrence. This example is presented in a uniform, abstract manner through calculations in (Kleisli) categories.

The paper considers initial conditions to estimate the time available for paper preparation (variable t) and the scientist's research skills (s). The obtained results (r) depend on both time and skill, while the paper's readability only depends on time. Both results and readability influence the likelihood of positive reviews (p), with results being more relevant. Additionally, these factors may lead a PC member to endorse the paper (m) independently of the reviewers' opinions, albeit rarely.

Unlike a?, the fuzzy predicate e? expresses the increased likelihood of error with less available time and skill. In such a situation, the scientist may still produce an accepted paper on time, but the chances decrease from 48% to 43%. This inference is expressed by the following calculation.

The abstract description of inference allows for transitioning from discrete to continuous approaches by switching from the Kleisli category kl(d) of the distribution monad to the Kleisli category kl(g) of the Giry monad on measurable spaces. An example is outlined where the function f in the inference situation (3) is the identity, with multiple predicates pi for successive learning, involving no predicate/state transformation. Details are briefly described, with references for further information.

The inference situations (3) and (4) are also interpretable in the effectus of von Neumann algebras for quantum computation. This interpretation uses the opposite vnaop of the category vna of von Neumann algebras, with normal completely positive unital maps between them as predicate transformers. The application to Bayesian networks allows for intriguing developments, presenting networks as arrows of kl(d) as part of a broader picture, which can be formulated in the language of props and their models. Understanding Bayesian inference as a categorical transformation on models of a prop is particularly worthwhile, mapping one network into another with the same topology but different probability distributions.