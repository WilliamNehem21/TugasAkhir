Support Vector Machines (SVMs) are a type of supervised learning model that can be enhanced through the use of unlabeled data. One approach, known as self-training, involves combining unlabeled data with labeled data in order to iteratively assign labels to the unlabeled data within the dataset. This process effectively expands the labeled training dataset at each iteration until all of the data is labeled. The self-training algorithm can be implemented as an extension to any existing supervised base learner. Additionally, the co-training method involves training two models, while self-training involves training just one model, making self-training a specific instance of co-training with a single model being trained.

Many semi-supervised learning algorithms rely on certain assumptions about the data, such as the smoothness assumption (similar inputs should have similar labels) or the cluster assumption (examples within the same cluster in the data manifold should have similar labels). These assumptions serve as the basis for most semi-supervised learning algorithms, which typically depend on one or more of these assumptions being satisfied, either explicitly or implicitly. Given sufficient unlabeled data and specific assumptions about the data distribution, the unlabeled data can contribute to the development of a more accurate predictive model. However, if these conditions are not met, the additional unlabeled data cannot enhance prediction accuracy. It is important to emphasize that blindly selecting a semi-supervised learning method for a specific task may not necessarily lead to improved performance over supervised learning, and in fact, using unlabeled data with incorrect assumptions can lead to worse performance.

The remainder of the paper is structured as follows: section 2 outlines the components and steps of the proposed geostatistical semi-supervised learning approach, section 3 demonstrates the method on synthetic spatial data, and section 4 presents an application example using real-world spatial data, including a comparison with classical supervised and semi-supervised learning methods. Concluding remarks are provided in section 5.

It is important to note that exposing a supervised machine learning model to as much data as possible can lead to improved performance. Pseudo-labeled spatial data offers contextual information that can aid in the training of a supervised machine learning model. Each pseudo training dataset contains the original training dataset. In this study, geostatistical conditional simulation is utilized as a label generator and data augmentation approach, implemented using the R package rgeostats. The simulated example illustrates a situation with a non-linear relationship between the target variable and auxiliary variables, as well as spatial autocorrelation in the target variable's distribution, and a non-Gaussian distribution.

The geostatistical semi-supervised learning approach, based on random forest, is initially applied to synthetic spatial data with known ground truth over the study region. A comparison is then conducted with classical random forest, random forest with unlabeled spatial data treated as additional covariates, and self-training random forest. The first two machine learning methods are supervised, while the last one is semi-supervised, and all methods are based on random forest with the same number of decision trees (1,000) and hyper-parameters optimized through cross-validation. The classical random forest is implemented in the R packages ranger and tuneranger, while the self-training random forest is implemented in the R package ssr.

Notably, the prediction uncertainty map resulting from the proposed geostatistical semi-supervised learning approach differs from those of the other methods, with lower prediction uncertainty observed at the original training data locations compared to other locations. This contrasts with the uncertainty predictions of the other methods.