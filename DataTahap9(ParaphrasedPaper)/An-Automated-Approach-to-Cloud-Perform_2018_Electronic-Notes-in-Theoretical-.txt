Cloud infrastructure has been provided as an on-demand service by various providers for many years. While these providers furnish different instance types with hardware specifications, they do not offer concrete performance metrics. One method to compare and contrast the services provided by multiple cloud providers is through benchmarking virtual machines. This paper introduces a tool that allows for the automated collection of performance information for cloud infrastructure, which can then be compared against the cost of these services.

The purpose of this research is to understand the variations in performance of cloud infrastructure among instance types and different providers. This aims to provide a thorough exploration of the actual performance of cloud offerings, rather than relying on assumptions based solely on listed hardware specifications and historical performance. By obtaining more accurate estimated execution times for jobs running on the cloud, it becomes possible to reduce delays while waiting for an instance to become available. With better insights into the execution time on higher performance instance types, job scheduling, especially for time-sensitive tasks, can be improved. Additionally, with improved estimates, it becomes feasible to allocate additional jobs to make the most of the available time on a provisioned instance. To achieve this objective, a tool has been developed to autonomously gather computational performance results from cloud instances across various providers. This tool is designed to handle the entire testing process, from provisioning new cloud instances to extracting results, without leaving any resources running, with a focus on ease of use. The initial implementation emphasizes the computational power of cloud infrastructure, especially CPU and memory performance, while the ultimate intention is for the tool to serve as a framework for any benchmark suite. While other similar systems exist, some are more tailored towards data-driven applications, and fewer are accessible for public use. Consequently, this tool is presented as an open-source alternative, available for others to utilize. Using this system, performance information has been collected for numerous instance types across Microsoft Azure and Amazon Web Services platforms.

The gathered performance information reveals significant differences in the performance of virtual machine series from different cloud providers. Some results demonstrate considerable variance in performance from one instance to another, whereas others exhibit consistent performance across multiple runs. Furthermore, different instance series generally exhibit varying levels of performance, even when the cost is not necessarily indicative of performance. For example, multi-core instance types within the basic a-series offered by Azure often perform worse than the other series, despite costing more per hour.

Several criteria are involved in collecting performance results from a cloud virtual machine. First, a new virtual machine must be acquired for benchmarking to ensure a clean state. Subsequently, the test suite is executed on the instance, and the results are collected. This paper focuses on the initial implementation using the specjvm2008 benchmarking suite, with the intention to support a range of testing suites in future iterations. Finally, the cloud service must be completely removed after extracting the result data to avoid incurring additional costs from leaving any remaining components.

To run specjvm2008, a command is executed from within the installation folder. The Python paramiko package is used to establish a secure shell (SSH) connection to the virtual machine, where the benchmark is then executed. However, to connect over SSH, the virtual machine must be configured to accept an SSH key and must be accessible on the default SSH port (22). To ensure the completion of the benchmark, a blocking call is made on the output stream from the execution, waiting for the command to exit or the channel to be closed. Upon completion, the results file is transferred to the original machine using an SFTP session. Finally, the virtual machine is released by deleting the hosted service and deployments, along with associated data disks.

The score of a basic a2 tier instance increases by more than a factor of 2 from the basic a1, even though the number of cores, memory, and price also double. This behavior is unusual, as it indicates that doubling the number of threads results in a performance increase slightly greater than double. Although the memory on the instance also increases, the amount of memory provided to the tested JRE remained at 1GB for both instances. This discrepancy may be an erroneous result, and it is uncertain whether the provisioned instances for basic a2 were simply slightly faster than those for basic a1, as further iterations are necessary to validate this claim.

T2 virtual machines receive CPU credits at the beginning of each hour, allowing them to exceed the baseline core usage. The full benchmark runtime is approximately 2.25 hours, during which near 100% utilization is likely maintained. The benchmark exhausts the available credits in the first two hours and subsequently operates at baseline for the remaining time before additional credits are provided at the hourly boundary. In the last hour slot, where only 25% of the time is used, the benchmark may be able to utilize the available credits for the rest of the duration. If, when bursting, the performance of a t2.large instance exceeds that of an m4.large, yet falls far below when no credits are available and operating at baseline, the expected performance aligns with observations. Specific tests such as startup and compiler tests, run when credits are available to a t2 instance, exhibit better results compared to the m4, whereas others, such as scimark and serial, demonstrate superior performance on the m4 instance. The scores would likely deteriorate if the benchmark ran over the entire VM hour slots provided with high utilization, due to credits being exhausted in the third hour and reverting to baseline for the remainder of the hour.

Some workloads may be single-threaded and therefore unable to benefit from the additional cores offered by a virtual machine. This limitation reduces the options for such tasks on current cloud offerings, as multicore systems are priced according to the performance gained from additional cores. However, it may be viable to batch load a number of single-threaded tasks onto a multicore instance type. If the performance of a single thread on a multicore instance is higher than that of a single-core system and the price per core is the same, loading a single-threaded job per core on the multicore system could prove advantageous. Specjvm2008 supports running in single-threaded mode, and results have been collected for the performance across the multicore variants of both the d-series and dv2-series. As the previously collected results were not greatly influenced by high-memory instance types, the d11-13 VM types have been excluded from the testing process.

The compression test results are consistent across the instances, with deviations ranging from 6% lower to 4% higher performance than the overall average. Some instances, such as numbers 3 and 7, remained relatively consistent across all runs, with maximum changes in results between runs at 0.84 and 0.81, respectively. In contrast, others, such as instance 5, exhibit more erratic performance, with the compression result increasing by 2.60 from the first iteration to the second. Instance 4 consistently performed poorly compared to the others, showing the lowest score in the third iteration and no scores above 48.81. The initial test iteration was the lowest score in 40% of cases, with performance increasing by more than 2.00 points in three instances. In contrast, in two instances, the initial test run was the highest result observed over the test period, with a decrease in performance by as much as 1.52.

The difference between the initial result and the average score for the VM across the results ranged from 0.04, where the initial result provided a good indication of the instance’s performance, to 1.60, where the average exceeded what the initial result had indicated. These differences underscore the challenge of judging an instance based on an initial reading. While this method may work well in some cases, the variability from one instance to the next makes it impractical to measure a VM’s performance before use. To enhance this approach, the initial test could be run for a longer duration to obtain a better representation of the instance's performance. Additionally, different VM types may exhibit less or more variability over time; as a result, a higher instance tier may yield more useful results.

Throughout this research, the aim has been to develop a benchmarking tool to collect performance information of cloud infrastructure, enabling more informed scheduling decisions. The tool has been designed to allow for a direct comparison of compute performance from one virtual machine to another, regardless of the service provider. While the current system only supports the specjvm2008 test suite, the ultimate goal is for the tool to serve as a framework for other benchmarking software. While there are similar systems that conduct similar experiments to those covered here, this tool is provided as an open-source system for others to use. The tool has been utilized to measure a wide variety of instance type offerings from two different cloud service providers, revealing that an increase in cost does not always correspond to higher compute performance.