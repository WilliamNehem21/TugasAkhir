In the past two decades, first-order methods have emerged as the primary approach for solving convex optimization problems, demonstrating significant progress in various applications such as machine learning, signal processing, imaging, and control theory. These methods offer the potential to deliver solutions with low accuracy at a low computational cost, making them particularly appealing for large-scale optimization problems. This survey covers several key developments in gradient-based optimization methods, including non-Euclidean extensions of the classical proximal gradient method and its accelerated versions, as well as recent advancements in projection-free methods and proximal versions of primal-dual schemes. The survey provides comprehensive proofs for various key results and emphasizes the unifying aspects of several optimization algorithms.

Traditionally, convex optimization problems were tackled by translating them into conic programs and employing primal-dual interior point methods (IPM) to solve them. This approach, championed by Nesterov and Nemirovski in 1994, paved the way for the development of robust technology for solving convex optimization problems, which forms the computational backbone of specialized solution packages such as MOSEK and SeDuMi. However, the inherent limitations of IPM, particularly in terms of scalability and iteration costs, have prompted the exploration of alternative methods such as conditional gradient (CG) optimization. The CG method is especially relevant for solving convex programming problems with complex geometries, where proximal operators are difficult to evaluate, as well as for preserving structural features of the desired solution, such as sparsity.

The proximal gradient method (PGM) has garnered considerable interest in optimization and its applications, particularly in the context of signal processing. Additionally, the mirror descent (MD) method has been proposed as a version of the proximal gradient method for convex composite non-smooth optimization. Furthermore, a classical method for solving linearly constrained optimization problems, building on the method of multipliers, has been demonstrated in the alternating direction method of multipliers (ADMM), which has garnered significant attention in various fields including PDEs, mixed-integer programming, optimal control, and signal processing.

Regarding step-size choices in optimization methods, while the ones analyzed in the survey are commonly used, there are additional step-size rules that provide similar guarantees. For example, new step-size rules have been proposed based on alternative analyses of the CG method, recursive step-size rules, and additional assumptions on problem structure.

In conclusion, while optimization problems in general are considered unsolvable, convex programming offers solutions for a significantly large class of model problems with important practical applications. Nonetheless, as modern optimization problems are typically large-scale, polynomial time methods are often impractical, leading to the widespread adoption of first-order methods in balancing inexpensive iterations with low solution accuracy. Many theoretical and practical advances have been made in the last 20 years in this area.