Machine learning algorithms for grammar inference have a wide range of applications in fields such as software engineering, syntactic pattern recognition, computational biology, computational linguistics, speech recognition, and natural language acquisition. For instance, in software engineering, recovering grammar from legacy systems can enable the automatic generation of various software analysis and modification tools. In application areas beyond software engineering, grammars serve as an efficient representation of structural and/or recursive artifacts, such as neural networks, structured data, and patterns.

Our work is particularly relevant to the renovation of legacy systems, as once a grammar is available, renovation tools can be rapidly developed. While current grammar inference techniques are unable to infer grammars of general-purpose programming languages, our approach presented in this paper can infer grammars for small domain-specific languages. The paper is organized as follows: Section 2 provides a brief overview of key findings in the grammar inference literature.

MARS, a semi-automatic inference system, operates in the domain-specific modeling (DSM) area of model-driven software engineering. DSM allows domain experts to use high-level specifications to describe solutions to domain-specific problems using domain concepts. The motivation behind the MARS project was to address the issue of metamodel drift, which occurs when instance models in a repository become disconnected from their defining metamodel. By leveraging existing tools and new grammar inference algorithms, the MARS system is able to recover metamodels that accurately define the mined instance models.

In our approach, we focus on obtaining grammars that accept positive samples, as this strategy leads to a more effective convergence to the desired grammar. Negative samples are only taken into consideration when a grammar is capable of accepting all the positive samples, as they are mainly used to prevent overgeneralization of grammars. To evaluate the fitness of each grammar, we define a fitness value between 0 and 1, where an interval of 0 to 0.5 indicates that the grammar did not recognize all positive samples, and an interval of 0.5 to 1 signifies that the grammar recognized all positive samples and did not reject all negative samples. A grammar with a fitness value of 1 indicates that the generated LR(1) parser successfully parsed all positive samples and rejected all negative samples.

Previous attempts at learning context-free grammars have had limited success on real examples. To address this, we have introduced grammar-specific heuristic operators and improved the construction of the initial population by leveraging knowledge from positive samples. Our future work involves exploring the use of data mining techniques in grammar inference, enhancing the brute force approach with heuristics, and investigating the support vector machine (SVM) classification technique for context-free grammar inference.