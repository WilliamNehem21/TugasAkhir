We aimed to develop an efficient algorithm suitable for a multi-core machine using shared memory communication among multiple processors. Our design objectives focused on maximizing memory efficiency while ensuring maximal parallelism. Specifically, our design sought to avoid storing predecessor edges, and aimed to be lock-free and almost wait-free. This imposed certain constraints on the heuristics used to select the next vertices to process, especially due to the lack of direct predecessor information.

For CTL*, we described a parallel implementation for multi-core systems, based on hesitant alternating games, which explores the induced and/or graph using a stack to detect cycles. The approach closely aligns with... The authors also highlighted potential issues with shared-memory implementations.

The least game parity progress measure is computed as the least simultaneous fix point of propagating the measure of each vertex when necessary, starting with the zero vector. In the context of distributed implementation, we allowed for lifting a whole set, u, at once, with the option of choosing singletons for u, corresponding to the original algorithm of... In section 4, we discussed heuristics for selecting u in a parallel algorithm. The complete algorithm is provided by...

It is sufficient to store at most |ui| + 1 different measure vectors in the indexed set for each worker i at a given time, although usually the number is lower, for example, in the initial situation where all measures are initialized to the zero vector. Therefore, this scheme also conserves memory.

However, the number of measures encountered during the algorithm's execution is often much higher. This suggests that the indexed set needs to be paired with a reference counting or garbage collection scheme to be feasible. Larger measures could potentially benefit from the vector folding described in a different context.

For our experiments, we implemented the algorithm for a multi-core setting using shared memory and thread-based implementation, utilizing Intel Threading Building Blocks (TBB) as a concurrency abstraction. It offers high-level operations such as parallel_for and parallel_reduce, which shield us from many threading details, allowing for a concise implementation closely reflecting the presented pseudo code.

In Algorithm 2, we abstracted from the order in which measures are lifted using a permutation h, with allowed repetition. In this section, we will discuss various possible choices for h, which are heuristics and may not always be optimal, but our experiments confirm their practical effectiveness.

All our experiments were conducted on a computer with two quad-core Intel Xeon E5320 processors (1.86 GHz) and 8 GB RAM, running Linux 2.6.18. The algorithm was run with varying numbers of worker threads (n=1, 2, 4, and 8) enabled, and each run was repeated five times. The average run time, representing the wall clock time (in seconds) needed to calculate mg and the actual computation of progress measures, was used. The generation time for parity game graphs is excluded, as these are assumed to be already present in memory, and their acquisition may vary in performance.

The onebit sliding window protocol is similar to the sliding window protocol, but with a limited window size of 1. While simple, this protocol offers more parallel behavior than the alternating bit protocol (ABP). Concentrating on the top half of Tab. 1, we observed improvement by increasing the number of worker threads (n) for most combinations of case studies and selection heuristics, although the gains were quite variable. However, the complete asynchronicity of workers for most of the run time had the drawback of causing extra iterations until a fix point was reached, particularly when the amount of work for each dropped below a certain threshold. Additionally, potential effects of false sharing were also considered as observed by Inggs and Barringer.