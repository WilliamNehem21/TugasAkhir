The need for training models on multiple data streams is becoming increasingly important as the analysis of online interactions on social media and shared systems grows in significance. For instance, variations of hidden Markov models (HMMs) have been used to identify behavioral trends in Twitter data by adapting K-means clustering and the Baum-Welch algorithm to simultaneously train on multiple users. This multi-input HMM, referred to as multihmm, represents an improvement over the computationally-intensive coupled HMM as it saves computational resources and allows for training on multiple users. However, further advances can be made through incremental parameter training to enhance online learning.

The use of workload benchmarking and scheduling for tasks distributed according to Markov modulated Poisson processes (MMPP) is valuable, but it is just one aspect of a broader objective. Modeling servers as part of queuing systems enables the understanding and prediction of waiting times, varying loads, system bottlenecks, and resource allocation in modern server applications. By incorporating MMPP into a first-come first-served (FCFS) queue, mean waiting times for varying loads can be obtained. Fluid input models, such as MMPP, share similarities with HMMs in terms of their ability to model discretized time variants and facilitate powerful traffic analysis. HMMs, in particular, are advantageous due to their parsimonious nature, allowing for the representation of time-varying, correlated traffic streams in workload models. The fundamental HMM algorithms are introduced in the subsequent section.

To comprehend the onlinehmm, it is essential to analyze its constituent processes. The first process is an adaptation of the sliding HMM (slidhmm), which utilizes a sliding window technique based on a simple moving average for data measurement. The second process is the multihmm, capable of training on multiple discrete traces simultaneously. The onlinehmm aims to integrate both techniques and establish a new online learning workload benchmark.

The concept of a sliding window to update datasets on which HMM parameters are trained in real-time is appealing in terms of run-time performance and online workload characterization. By updating the dataset with new arrivals and simultaneously discarding the oldest observations, time-variant processes can be measured parsimoniously. The sliding window effect enhances the incremental learning of inchmm, which accumulates an increasingly large observation set as outdated data points are included in iterative updates of HMM parameters.

The multihmm comprises a k-means clustering algorithm and a weighted Baum-Welch algorithm, enabling training on multiple discrete traces simultaneously and maintaining accuracy in comparisons of trace moments. The doubly clustered methodology and the full multihmm algorithm are presented.