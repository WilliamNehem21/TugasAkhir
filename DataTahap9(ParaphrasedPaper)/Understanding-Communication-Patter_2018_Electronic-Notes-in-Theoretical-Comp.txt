This brief paper outlines our preliminary investigation into the communication patterns found in the high performance conjugate gradient (HPCG) benchmark. Our analysis highlights patterns and characteristics that may necessitate further exploration to enhance the performance of conjugate gradient (CG) algorithms and applications that rely heavily on them. We conduct this investigation by capturing communication traces from HPCG benchmark runs across various processor counts, and subsequently examining the data to identify potential performance bottlenecks. Our initial findings indicate a decrease in network throughput as the number of communicating processes increases, attributed to network contention.

Benchmarking systems enables the comparison of performance for system designers to craft the most suitable system for their applications. Various benchmarks are accessible to evaluate the performance of an entire system and its subsystems, each with its own advantages and drawbacks. Benchmarking subsystems can be advantageous for system designers seeking to gain deeper insights into specific subsystems; for example, MPI benchmarks like SKaMPI and Intel MPI benchmarks. SKaMPI is a benchmark designed to test interconnects using the Message Passing Interface (MPI) developed by the University of Karlsruhe. Meanwhile, Intel MPI benchmarks offer a suite of tests for both point-to-point and global communication operations involving varying message sizes.

Our investigation revealed a throughput of 0.26 Gbps, significantly lower than the achievable 70.96 Gbps over the interconnect. One possible explanation for this discrepancy is external contention caused by network traffic from other applications running on the shared system. However, this factor alone does not account for the substantial performance gap we observed.

Rajovic, N., P. M. Carpenter, I. Gelado, N. Puzovic, A. Ramirez, and M. Valero. "Supercomputing with Commodity CPUs: Are Mobile SOCs Ready for HPC?" In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, ACM, 2013, p. 40.