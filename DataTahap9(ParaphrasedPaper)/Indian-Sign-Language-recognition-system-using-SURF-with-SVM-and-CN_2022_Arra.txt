Hand gestures are a form of interpersonal communication that holds potential for various applications. Typically employed by individuals with speech impairments, they have a substantial impact on about one percent of the Indian population. Therefore, developing a framework to interpret Indian Sign Language (ISL) would significantly benefit this demographic. In this study, we introduce a methodology that utilizes the bag of visual words model and background subtraction. We extract Speeded Up Robust Features (SURF) from images and create histograms to associate signs with corresponding labels. Classification tasks involve the usage of Support Vector Machine (SVM) and Convolutional Neural Networks (CNN), and we also develop an interactive graphical user interface (GUI) for user-friendly access.

Communication is an essential aspect of human life, and the ability to interact and express oneself is a fundamental necessity. However, factors such as upbringing, education, and societal influence can lead to significant variations in communication styles among individuals. Moreover, ensuring that one's intended message is accurately conveyed holds paramount importance.

Previously, Indian Sign Language (ISL) lacked standardization, limiting its use to short-term courses. However, in 2003, ISL became standardized, garnering the attention of researchers.

ISL encompasses both static and dynamic signs, including single and double-handed gestures. Additionally, regional variations lead to multiple signs for the same alphabet, further complicating attempts to introduce a unified system. The absence of a standard dataset further underscores the complexities associated with ISL.

Amid the demand for real-time systems, the adoption of deep learning technologies has facilitated automated image recognition. Notably, convolutional neural networks (CNNs) have made significant advancements in the realm of deep learning. Previous studies have utilized CNNs for feature extraction and classification of gestures to textual translation.

Given the lack of standard datasets for ISL, a significant challenge was encountered during data collection. Consequently, the project involved manual construction of a dataset to address this issue.

To capture variations in the dataset, two methods were employed for image capture, involving both plain and dynamic backgrounds. Subsequently, SURF features were chosen to reduce the measurement time and ensure system robustness to rotation. The issue of background dependency was also addressed to enable system operation in diverse environments.

Utilizing a codebook-based approach, random batches of fixed-size data were employed to reduce memory usage. Employing Support Vector Machine (SVM) with a linear kernel, we successfully trained the system on a set of 28,800 images and tested its performance on a set of 7,236 images. Canny edge detection was also utilized to enhance image processing.

The proposed recognition method based on SURF features offers fast computation and robustness against various factors, making it suitable for real-time applications. The model exhibits high accuracy in recognizing double-handed signs and translating visual information into text or speech. The system achieved an accuracy of 99%, marking an advancement in addressing the limitations of previous models.

The study aims to provide a real-time recognition utility that can be utilized in diverse settings. Efforts involved constructing a custom dataset, ensuring the system's rotation invariance, and addressing background dependency. The successful training of the system on all 36 ISL static alphabets and digits sets the stage for potential expansion of the dataset and broader real-time applications. The next steps involve incorporating signs from various languages to create a comprehensive framework for continuous and isolated recognition tasks, with an emphasis on enhancing response time.

Citations:
Shadman Shahriar, Ashraf Siddiquee, Tanveerul Islam, Abesh Ghosh, Rajat Chakraborty, Asir Intisar Khan, Celia Shahnaz, & Shaikh Anowarul Fattah. (Year). Real-time American Sign Language Recognition using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning. In TENCON, IEEE Region 10 International Conference.

(Note: The original citation was incomplete, and "Year" should be replaced with the relevant year of publication.)