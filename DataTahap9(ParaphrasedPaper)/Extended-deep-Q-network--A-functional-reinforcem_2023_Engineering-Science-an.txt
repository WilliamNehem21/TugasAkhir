Plug-in hybrid electric vehicles (PHEVs) have demonstrated substantial enhancements in fuel efficiency and reduction of CO2 emissions over the past decade. However, the effectiveness of such improvements hinges on the quality of the on-board energy management strategy (EMS). The increasing complexity in control objectives due to emerging technologies like connected vehicles and automated driving has made the traditional hand-crafted rule-based approaches insufficient in guaranteeing optimality within multi-domain, nonlinear, and time-varying systems. This necessitates the development of more intelligent controllers for future vehicles.

Recent approaches have shown the potential to achieve a 10% reduction in fuel consumption compared to conventional charge-depletion charge-sustaining (CDCS) strategies. While these approaches provide a viable solution for real-time hybrid control units (HCUs), achieving a balance between control objectives, real-time controller performance, tuning effort, and vehicle hardware capabilities presents a considerable challenge. Notably, reinforcement learning (RL)-based energy management approaches in PHEVs have garnered significant attention from the artificial intelligence (AI) community due to their ability to learn control policies through interaction without explicit programming.

The study presents three driving modes: conventional drive (CD), optimum generation (OG), and electric drive (ED) for the on-board EMS. An RL agent collected 50,000 experience tuples following a random policy, which were then postprocessed to calculate the episodic reward and q-values for each tuple. The study trained several neural network (NN) architectures on different datasets in a supervised learning fashion to calculate the root mean square error (RMSE) as training and validation errors, selecting the architecture with the minimum RMSE to efficiently learn and approximate the q-function.

To further improve the learning process and stabilize training, the double deep Q-network (DDQN) algorithm was employed to reduce the overestimation bias of observed q-values. Additionally, advancements in RL policy selection and mode segmentation were explored, with the study presenting an approach that allows the agent to have full observability and controllability over the environment.

It is emphasized that advanced AI-based strategies, such as RL-based EMSs, can significantly enhance the fuel economy of PHEVs. The study introduced an adaptive online learning RL agent into the existing HCU architecture and benchmarked the developed RL algorithms using dynamic programming (DP) results to achieve near-optimal solutions for EMS. It also highlighted the future need for distributed and multi-agent deep RL systems for cooperative learning between vehicles on the road, particularly in the context of intelligent transportation systems (ITSs) and the construction of smart cities or smart grids.

The paper concludes with acknowledgments to individuals who contributed to the research, including Prof. Thomas Schlechter, Prof. Stefan Winkler, the AVL DSP team, and the editors and anonymous reviewers who provided valuable feedback to improve the quality of the research paper.