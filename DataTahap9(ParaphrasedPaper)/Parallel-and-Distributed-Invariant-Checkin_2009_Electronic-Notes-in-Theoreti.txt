In the final stage, a counterexample is both created and optimized by removing loops and other unnecessary parts to facilitate its understanding. The counterexample is presented in assembly code, C code, the control flow graph of the assembly code, and as a state space graph.

The algorithms discussed in the remaining part of this section differ in the way the functions for accessing the state space are implemented. In our algorithms, two different approaches to load balancing and one partitioning function are utilized. Furthermore, communication between threads is achieved using different data structures and synchronization primitives.

For static load balancing, the assignment of a state to a thread is influenced only by the structure of the processed state. Stern and Dill suggest the use of an evenly distributed hash function for static load balancing. The static load balancing function implemented in [MC] square is the static partitioning function described in section 3.1.1.

A different approach to static load balancing is presented by Lerda and Sisto. In their approach, the load balancing function is based not on the complete state but only on small parts. The rationale behind this approach is that only small parts of a successor state are altered in a single transition. Developing an evenly distributed balancing function for this approach is challenging and sometimes impossible.

This section discusses the implementation of four different parallel algorithms. For the implementation, we have assessed the performance of the following Java containers for storing states: hashmap and treemap, accessed using explicit synchronization, as well as concurrenthashmap and concurrentskiplistmap. We observed results akin to the experiments of Goetz et al., wherein concurrenthashmap emerged as the fastest solution for parallel access.

The structure of a bidirectional producer-consumer pattern corresponds to the load balancing. Successor states are generated by n threads, which are consumed by the master thread and stored in the state space. The master thread uses the partitioning function part(s) to determine the thread that processes state s. In essence, the master thread populates the queues of the invariant checkers.

This section details an algorithm for the distributed generation of state space, which is based on the approaches of Stern and Dill, Lerda and Sisto, and Holzmann and Bosnacki. We use the term "node" for each process in the distributed network. A single master node is employed, which initiates the other nodes and detects the termination of the distributed algorithm. It fulfills a different role than the master thread for parallel invariant checking.

In the distributed algorithm, each node runs three threads. The main thread executes invariant checking, thus having exclusive access to the state space and performing load balancing. In our distributed approach, static load balancing and static partitioning are used, similar to the parallel case. Two threads, a sender and a receiver, facilitate the communication of states between nodes.

We utilized two programs for the Atmel ATmega16 microcontroller to evaluate the performance of the presented algorithms. The program adc.elf implements a distance measurement using an infrared controller and consists of 467 lines of assembly code, resulting in 434,756,686 states with all optimizations enabled in [MC] square. The program window lift.elf implements a controller for a powered window lift used in a car and comprises 288 lines of assembly code, resulting in 2,589,681 states without any optimizations. We employed different abstraction techniques to uncover any potential impact on the performance of the parallel and distributed algorithms.

We identified two reasons for these results. Firstly, we believe that they stem from the inefficient synchronization primitives in Java, as observed in all implemented parallel algorithms. Similar results were noted by Inggs and Goetz et al. Another issue is the structure of our multi-core system. Although the server has a shared-memory architecture, existing libraries. Ultimately, we retained the implementation of the best parallel algorithm, namely static load balancing and local access to the state space, alongside the distributed algorithm. Both algorithms can be customized by users by adjusting the number of threads or nodes utilized.

Having more than 5 processors barely proved beneficial and sometimes even led to a slowdown due to the synchronization overhead. Similar numbers were also observed by others such as Inggs or Goetz et al. in dealing with parallel Java programs. It could be the case that this observation is caused by the communication between the data structures used in Java. There are two potential solutions to this problem. First, we could implement the important methods in C or C++ and then use the Java Native Interface to use these methods. Another solution could be to use the new Java 7 version, as it will include several performance improvements.

State spaces for invariant checking can also be utilized for model checking. For model checking, we intend to enhance our global CTL model checking algorithm. We anticipate that this will enable the use of more than a single search front for state space building. In the local model checking algorithm, this is not efficient as it contradicts the local nature of the algorithm.