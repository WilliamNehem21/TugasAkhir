(wand and vu, 2018) conducted a study in which they utilized the grid dataset to train and test a deep neural network. They proposed a novel lipreading system called densely associated convolution network, which captures visual representations from color images. The authors focused on 3D lip physiological features based on the position and structure of the face and suggested a hybrid algorithm for audiovisual recognition. They applied connectionist temporal classification (CTC) and the attention architecture to the LRS2 database for audiovisual speech recognition.

In a separate study, (jadczyk, 2018) carried out a Polish audiovisual voice recognition project, dividing the work into three subcategories: audio, visual speech, and integration. For the audio component, they employed Mel-frequency cepstral coefficients (MFCC) to extract the features and used a hidden Markov model (HMM) to extract lip features. They also employed a multistream HMM for integration.

(shashidhar and patilkulkarni, 2021) worked on a traditional lipreading database using a custom 250-item dataset and applied a pre-trained model called VGG16 for improved accuracy. Meanwhile, (cornejo and pedrini, 2019) proposed a deep convolutional neural network (DCNN) for audiovisual voice detection. Their method involved separating audio from video, using two-dimensional CNN to extract audio data, and employing principal component analysis and linear discriminant analysis for visual speech extraction, ultimately merging the audio and visual features to enhance accuracy.

Additionally, (shashidhar and sudarshan, 2022) introduced a new approach called multimodal sparse transformer network (MMST), which uses a sparse self-attention mechanism to selectively focus on important data parts for improved global information processing.

In a different study, (dupont and luettin, 2000) demonstrated the high performance of their proposed system on a large database of continuously spoken digits involving multiple speakers. Their approach utilized a combination of the multistream hidden Markov model (MSHMM), denoised MFCCs, and visual features to enhance multimodal isolated word recognition accuracy.

The videos used to form the database were recorded at a resolution of 1080 pixels with a smooth frame rate of 60 frames per second, resulting in high-quality footage. Each video had an average duration of 1 to 1.20 seconds, with a file size typically around 10 megabytes.

Convolutional neural networks (CNNs) have proven to be effective in solving complex computer vision tasks and are often used in modern machine learning and artificial intelligence due to their efficacy. DCNNs comprise multiple layers, each playing a specific role in extracting and transforming visual features from input data.

Understanding and interpreting visual communication without audio input is a complex task. Deep learning models can predict speech content using visual information, primarily lip movements. Voice biometric applications, for example, can be used to verify individuals, and individuals with audio and verbal speech impairments can manage conversations using such applications. The audiovisual speech recognition method proposed in these studies involves a new dataset to address the computational challenges of CNNs and deep learning.

Moreover, the proposed audiovisual speech recognition method includes the use of a new dataset and a feed-forward neural network for addressing the computational challenges of CNNs and deep learning. The architecture incorporates a 1D CNN model for audio, an LSTM model for visual input, and a DCNN for integration. The authors achieved a training accuracy of 94.67% and testing accuracy of 91.75% using a custom dataset, indicating that combining audio and visual inputs yields superior performance compared to existing methods.