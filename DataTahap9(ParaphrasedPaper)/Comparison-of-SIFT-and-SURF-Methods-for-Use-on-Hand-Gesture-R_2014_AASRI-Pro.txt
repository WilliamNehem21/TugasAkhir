This paper presents a comparison between two widely used feature extraction methods: Scale-Invariant Feature Transform (SIFT) and Speeded Up Robust Features (SURF). The methods are tested on a set of depth maps containing ten defined gestures of the left hand, captured using the Microsoft Kinect camera. The Support Vector Machine (SVM) is utilized for classification, and the accuracy of SVM predictions on selected images is reported.

The paper begins with a discussion of SIFT and SURF, providing theoretical background and experimental details. A database of depth images is created, and SIFT and SURF are applied to the images to generate feature vectors, which are then divided into training and testing sets. The SVM model is trained using the training set and tested using the test set, with the prediction accuracies for SIFT and SURF reported.

SIFT, a popular local visual descriptor, operates in two steps: feature point detection and feature description. The method involves computing gradient magnitudes and orientations of pixels in the neighborhood of a key point, using the scale of the point to determine the choice of Gaussian kernel for image blurring. The feature vector is derived from the orientation histograms within sub-regions around the feature point and is normalized.

To capture the color image of the hand and conduct segmentation, significant computational resources may be required. Some segmentation methods focus on detecting the skin color to identify the hand region, but results may be affected by lighting conditions or an individual's skin tone. The Microsoft Kinect camera, which uses the infrared spectrum of light, offers the advantage of being invariant to lighting conditions and skin color, enabling it to track parts of the detected human body.

The results indicate that the invariance to rotation of the SIFT descriptor is a disadvantage in this context, as some hand gestures, such as those representing classes 1 and 5, have similar shapes but different orientations. Additionally, major errors occur for classes 7 and 2, where the shape of the hand is very similar in both images despite representing different gestures.