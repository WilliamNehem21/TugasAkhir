An empirical evaluation of machine learning algorithms can be effectively conducted in a controlled environment using synthetic datasets that are based on known properties of the dataset used by the learning algorithm. While there are publicly available frameworks for generating synthetic single-label datasets, there is a lack of such frameworks for multi-label datasets, which are often associated with sets of correlated labels. This paper introduces mldatagen, a publicly available framework for generating multi-label datasets, and describes two implemented strategies: hypersphere and hypercube. These strategies randomly generate geometric shapes for each label in the multi-label dataset, populate them with randomly generated instances, and then label each instance according to the shapes it belongs to.

The paper is organized as follows: Section 2 provides a brief overview of multi-label learning concepts and strategies for generating synthetic multi-label datasets, followed by the presentation of the mldatagen framework in Section 3 and its illustration in Section 4. Conclusions and future work are presented in Section 5.

Two main categories of multi-label learning methods are discussed: algorithm adaptation and problem transformation. Algorithm adaptation involves extending specific learning algorithms to handle multi-label data directly, while problem transformation methods transform the multi-label classification problem into several single-label classification problems. The evaluation of multi-label classifiers is also discussed, considering both example-based and label-based evaluation measures.

The paper details the generation of synthetic multi-label datasets using the mldatagen framework and presents the results of experiments using the multi-label brknn-b learning algorithm. It also discusses the performance of the algorithm for different synthetic datasets, highlighting the influence of the number of features and the presence of irrelevant features on the algorithm's performance.

The paper concludes by comparing the performance of datasets generated using the hypersphere and hypercube strategies and discusses plans for future work, including implementing more strategies into the mldatagen framework.