In this article, we examine the specific case of programs operating under a priority-based real-time scheduler, which is common in embedded systems. In such programs, higher priority threads cannot be interrupted by lower priority ones (except when explicitly waiting for a resource). Programmers take advantage of this characteristic to minimize their reliance on locks for protecting critical sections. We demonstrate how our analysis can be enhanced through partitioning to accommodate real-time considerations, eliminate unnecessary interleavings, and achieve more precise results for programs using priorities. Our analysis particularly encompasses dynamic priorities, allowing for the handling of explicit as well as implicit modifications of priorities through the priority ceiling protocol.

In a broader sense, a program is considered concurrent if it consists of multiple execution units, or threads, each with its own independent control flow. The overall execution, which thread runs at any given time, is managed by a scheduler. There are various scheduling strategies, some of which allow for arbitrary preemption of any thread by another, while others restrict thread switching to specific preemption points. Additionally, some schedulers use priorities to determine the order in which threads should run. Various communication methods exist for threads, and in the context we consider, all memory is shared, providing an implicit means for threads to communicate. While this model is the most general, it places significant demands on an analyzer to determine which variables are genuinely shared and which values are exchanged between threads. Threads can also synchronize and ensure mutual exclusion using locks.

We also introduce a specific technique known as the immediate priority ceiling protocol, which is commonly used in safety critical software. Our framework is capable of accommodating both variants of the priority ceiling protocol. For the purpose of presentation and to provide a comprehensive formal treatment, we define a simple, abstract language with a minimal set of constructs, focusing on those relevant to concurrency and real-time scheduling. However, our method has been implemented in a real language, C, as described in Section 6.

We present a concrete semantics for our language, initially considering an execution of the program as an arbitrary interleaving of thread executions, up to mutual exclusion enforced by locks. We initially overlook the impact of priorities and real-time scheduling, which we address in the subsequent section.

We specify a rule that explicitly prevents a yielding or blocked thread from blocking lower priority threads. If there are yielding threads, it may lead to non-deterministic behavior, with several simultaneously enabled threads (e.g., the yielding thread and a lower priority thread). Additionally, if multiple threads are waiting for a mutex to be unlocked, our semantics dictate that the highest priority thread waiting for the mutex will immediately run and acquire it, thereby blocking lower priority threads. We then define the trace semantics.

Although the interleaving semantics in Section 2.2 models multi-core systems, when considering priorities, we assume a single-core system where at most one thread runs at a time. There are different ways to extend mono-core real-time schedulers to multi-core systems, and while we believe our framework could effortlessly handle these cases through an adaptation of the "enbl" function, we leave the discussion of multi-core handling for future work.

When comparing with previous work, we note that our nested fixpoint formulation is similar to a previous model but has two main differences. Firstly, our model addresses priority-aware real-time semantics, whereas the previous model focuses on interleaving semantics. Secondly, we express a trace semantics, whereas the previous model presents a state semantics. Traces allow for history-sensitive abstractions, which can be beneficial for achieving path-sensitivity. While our implementation employs trace partitioning, we present a more traditional flow-sensitive but path-insensitive thread analysis in Section 4. The retention of traces in our analysis is primarily motivated by the need for history-sensitive abstractions of interference, allowing for the distinction between the most recent update of a variable within critical sections and previous updates, as only the most recent one can be observed by other threads.

While the presented example emphasizes the "yield" instruction, the same effect applies to any instruction that may prompt a thread to yield control to a lower priority thread, including "lock(m)," "unlock(m)," and "setpriority(p)." We refer to these instructions collectively as release points and refine the partitioning accordingly.

In concurrent programs, data races can be mitigated by protecting shared data accesses with mutexes. However, introducing lock instructions can potentially lead to deadlocks, where two or more threads mutually block each other indefinitely. Because of the severity of deadlocks, static analyzers may hesitate to correct reported data races for fear of introducing deadlocks. To address this, it is essential for the analyzer to accurately identify both data races and deadlocks. As the presence of deadlocks is a reachability property, it can be conservatively inferred using the information already gathered by our analysis.

We also present a program with three mutexes (a, b, and c), demonstrating a potential deadlock scenario. To further advance our research, we aim to enhance the deadlock analysis by considering priorities. Additionally, extending the deadlock analysis to include priority inversions, where a lower priority thread holding a mutex prevents a higher priority thread from executing, is another area of interest. This issue can also be addressed through the priority ceiling protocol.

Another avenue for future work involves developing history-sensitive abstractions for thread interference. Currently, our framework is capable, through locks or priorities, of detecting that two sections of two threads cannot interact, but it lacks information about the order in which threads execute. It would be beneficial to infer that one section of a thread always executes before a section of another thread, particularly for analyzing initialization processes.

Furthermore, extending and generalizing the real-time scheduling model is an interesting direction for future work. This expansion could encompass new synchronization primitives beyond locks, such as events and barriers, as well as support for various types of multi-core real-time schedulers.