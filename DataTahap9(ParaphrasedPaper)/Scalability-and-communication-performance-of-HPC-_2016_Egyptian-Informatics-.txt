Virtualization technology has brought about innovative methods for managing and consolidating computing resources, and has played a significant role in the emergence of cloud computing. Cloud computing has emerged as a feasible solution to bridge the gap between the increasing computational needs of researchers and the limited local computing capabilities. In addition to virtualization, cloud computing offers numerous advantages to users of high-performance computing (HPC) applications, such as resource elasticity and the elimination of setup time and costs for clusters. In a virtualized HPC system, computing nodes are deployed using individual virtual machines, communicating over Razi (Gigabit Ethernet) and Haitham (InfiniBand) clusters via applications like Skampi, IMB, and MPBench. The output results from these applications were compared and analyzed for validation, revealing that the architecture of the clusters may also influence the results irrespective of the interconnect type.

Belgacem et al. connected EC2-based cloud clusters in the USA to university clusters in Switzerland, and executed a tightly coupled, concurrent multi-scale MPI-based application on this infrastructure. They evaluated the overhead incurred by extending their HPC clusters with EC2 resources, finding that conducting multi-scale computation on cloud resources can yield subpar performance without proper adjustment of CPU power and workload. However, by implementing a load-balancing strategy, one can leverage the additional cloud resources.

The performance of the NPB kernels was evaluated using MOPS (Million Operations Per Second) and speedup metrics. An NPB class C workload was chosen because its performance is heavily influenced by the efficiency of the communication middleware and the support of the underlying networks.

The CG kernel exhibited optimal performance (both MOPS and speedup) on a cluster of 4 VMs (i.e., 32 cores), with degraded performance thereafter, pointing to a virtualized network performance bottleneck that could potentially be alleviated by improved NIC card virtualization implementations like SR-IOV or the use of faster network fabrics such as InfiniBand. The FT kernels showed similar peak performance on clusters of 1 and 16 VMs under MPICH2 and OpenMPI, respectively.

The analysis of the NPB kernel performance indicated that the evaluated libraries delivered favorable results when running entirely on shared memory (on a single VM) with up to 8 cores in 10 VMs, owing to the superior performance and scalability of intra-VM shared memory communications. However, when utilizing more than one VM, the evaluated kernels demonstrated poor scalability and experienced significant performance drawbacks due to network virtualization overhead. The IS kernel exhibited the poorest scalability, achieving its highest performance at 8 cores (i.e., one VM). Conversely, the MG kernel demonstrated the most scalability, attaining optimal performance on a cluster of 8 VMs (128 cores).