In the field of natural language understanding (NLU), several contemporaneous techniques have been suggested that rely on continuous prompts and aim to enhance knowledge exploration. Lester et al. (2021) demonstrated that when using large pre-trained models, adjusting continuous prompts while keeping the language model frozen resulted in improved performance.

This study introduces a method called p-tuning, which combines continuous prompts with discrete prompts. P-tuning enhances the performance and stability of training during the adaptation of pre-trained language models. P-tuning is effective for both tuned and frozen language models in both the few-shot and fully-supervised settings.