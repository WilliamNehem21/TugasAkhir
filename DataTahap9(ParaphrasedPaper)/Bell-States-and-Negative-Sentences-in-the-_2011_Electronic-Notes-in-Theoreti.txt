We utilize bell states to impart compositional distributed meaning to negative English sentences. The lexical meaning of each word in the sentence is represented as a context vector derived within a distributed model of meaning. The sentence's meaning resides within the tensor space of the word vector spaces. Mathematically, a sentence's meaning is the image of a quantizing functor from the compact closed category, which models the grammatical structure of the sentence using Lambek pregroups, to the compact closed category of finite-dimensional vector spaces that model the lexical meaning of the words. The meaning is computed by composing eta and epsilon maps to create bell states, enabling information flow among the words within the sentence through substitution.

Recently, both theoretical physicists and mathematical linguists have adopted the more expressive setting of compact closed categories over the monoidal structure of quantales. Lambek has employed the setting of a compact bi-category, known as a pregroup, which has been applied to analyze syntax in various natural languages including English, French, Japanese, Arabic, and Persian. Abramsky and Coecke have utilized compact closed categories to provide semantics for quantum protocols, setting a new foundation for quantum logic. Similarities between models of language and physics have emerged.

Furthermore, aside from syntax, these similarities extend to the semantic models of natural languages, encompassing both logical and distributed models of meaning. From a logical standpoint, a category-theoretical semantics for pregroup grammars has been proposed in the form of compact bi-categories. From a distributed perspective, vector spaces are employed to represent the lexical meaning of words. Additionally, the quantum axiomatic of Hilbert spaces has been used to model semantics of natural languages. These models have been applied to information retrieval from documents, including web documents, and to find synonymous meanings for words.

We take the initial step towards developing a logic for semantic derivations in natural languages. Inspired by the work of D. Clark, we introduce notation for graded implication and use it to quantify the degree of similarity between positive and negative sentences. Meanings of sentences can be derived from one another using this implication, and the degree of this implication represents how close the meanings of the sentences are to each other.

T(b) denotes the free compact 2-category generated over the partial order b. The details of this construction can be found in the joint work of the first author with J. Lambek. Each element (w, t) in dictionary d is referred to as a lexical entry in d.

In the distributed model of meaning, the lexical meaning of words is represented as vectors in a potentially high-dimensional vector space, with bases consisting of specific words from a dictionary. Given a text or a collection of texts and defining a neighborhood window of n words, the frequency of a certain word appearing in that window in the context of the bases is counted. This provides a vector representing the lexical meaning of that word.

As an example, let's consider the word "dog" in a vector space with bases "eat," "sleep," "pet," and "furry." If the word "dog" appears in the context of "eat" 6 times, "sleep" 5 times, "pet" 17 times, and "furry" 8 times in some text, then the vector for "dog" in this space is represented as (6, 5, 17, 8). Representing meanings in this way offers the advantage of providing a notion of distance between words, allowing the use of inner product or other measures to determine the closeness in meaning between words. For instance, one can construct the vector for "cat" in the same space as that of "dog" and observe their similar meanings in that context, as cats and dogs are both pets and share similar characteristics.

Computational models based on these principles have been developed utilizing large vector spaces (consisting of tens of thousands of context words/basis vectors) and large bodies of text (up to a billion words in some experiments). Experiments in constructing thesauri using these methods have shown relatively successful results. For instance, the top 10 most similar nouns to "introduction," according to the system of [referenced source], are "launch," "implementation," "advent," "addition," "adoption," "arrival," "absence," "inclusion," and "creation."