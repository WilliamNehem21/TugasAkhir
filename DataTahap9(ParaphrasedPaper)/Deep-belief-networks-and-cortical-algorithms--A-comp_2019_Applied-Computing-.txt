The paper discusses the specialization of neurons through training, with dendrites receiving inputs and axons transmitting signals. It also explores the organization of the cortex as an association of columnar units, and the role of plastic connections, repeated exposures, and neuron firing and inhibition in learning in the human brain.

Furthermore, the paper outlines the development of neural models from the simplified McCulloch-Pitts model to more complex architectures, and the transition from considering individual neurons as fundamental functional units to elevating the role of cortical columns in learning and forming invariant representations of sensory patterns.

Additionally, it delves into deep neural networks (DNN) as extensions of shallow artificial neural network (ANN) architectures, and their aim to achieve strong AI models through hierarchical abstraction of knowledge. It also discusses convolutional and recurrent neural networks as predecessors of DNN, and their training using back-propagation.

The paper also touches upon the concept of the cortical array (CA) model, which draws inspiration from the organization of cortical columns in the brain for computational training algorithms and invariant pattern representation. It then explains the computational cost and architecture differences between deep belief networks (DBN) and CA, highlighting their respective strengths and weaknesses.

In summary, the paper provides a comprehensive overview of the evolution of neural models, focusing on their biological inspirations, learning capabilities, and computational aspects.