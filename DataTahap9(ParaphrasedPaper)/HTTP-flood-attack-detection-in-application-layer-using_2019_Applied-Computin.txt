The internet is a global network of interconnected computers using standard protocols, which has become vital for various aspects of human life such as education, trade, socialization, and entertainment. However, with the increasing reliance on the internet, web threats have emerged, posing various risks including financial damages, identity theft, loss of confidential information, and erosion of consumer confidence in e-commerce and online banking.

In reflection/amplification-based flooding attacks, attackers initiate small DNS queries with forged source IP addresses, resulting in a large influx of network traffic directed towards the targeted system, causing it to become overwhelmed and incapacitated.

HTTP flood DDoS attacks focus on generating attack traffic that closely resembles legitimate human user activity, making it difficult for victims to distinguish between legitimate and attack traffic. As a result, servers become unavailable to legitimate users, leading to slow network performance, unavailability of websites, inability to access any websites, and a dramatic increase in spam emails received.

The increasing prevalence of application-layer DDoS attacks has prompted significant interest in research communities. These attacks often evade traditional network-based detection mechanisms, leading to a focus on specialized application-layer DDoS detection methods such as puzzle-based methods, digital signature analysis, and the use of bio-inspired algorithms like the bat algorithm.

The proposed bat algorithm, a bio-inspired algorithm based on swarm intelligence, is used for classification of attack and normal traffic in the detection of application-layer DDoS attacks. The algorithm involves initializing the bat population, calculating errors, and updating random numbers through multiple iterations to achieve accurate classification of attack and normal traffic based on the prepared dataset.

The proposed technique was tested using the CAIDA dataset 2007, which contains parameters such as server IP address, timestamp, time zone, object ID/URL of the web page, response code/status, and number of bytes sent, and aims to collect and share data for research and scientific analysis of internet traffic, topology, routing, performance, and security-related events.