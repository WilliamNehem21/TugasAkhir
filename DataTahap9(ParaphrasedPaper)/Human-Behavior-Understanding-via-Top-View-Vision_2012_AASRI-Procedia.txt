The paper proposes a method to address the occlusion problem in complex scenes by introducing human action recognition from a top-view perspective. However, the rotation of human behavior in this view can lead to misidentification. To mitigate this issue, the method utilizes the rotation invariance of the moments to represent human static postures using Hu moments and employs SVM as a trainer and classifier. Additionally, a semantic web of dynamic behavior is established based on the change in coordinate of the binary image centroid. Experimental results demonstrate that this approach accurately identifies human dynamic information with a high recognition rate.

The field of computer vision is actively engaged in understanding human behavior, and significant progress has been made due to advancements in technology. There is a shift from theoretical research to practical applications, especially in public scenarios where cameras are often placed in corners or at the top. By recognizing the limitations of a corner view, the paper proposes a method based on the top-view, which inherently addresses the occlusion problem. However, the rotation of people in the top-view renders common feature representation invalid. To overcome this, the method employs the translation and rotation invariance of Hu moments to represent gesture feature vectors and draws high-level semantic information based on changes in centroid coordinates.

The top-view necessitates that shape features remain consistent after rotation. Therefore, the method utilizes the invariance of geometric transformations to represent behavioral characteristics, as certain moments of the image area exhibit similar characteristics such as translation, rotation, and scale. The method uses training data to train an SVM classifier and employs kernel SVM to achieve classification.

The aim of the research is to accurately identify dynamic information rather than just static postures. The paper proposes using changes in centroid coordinates to identify higher-level semantic information in addition to recognizing static postures.

The experimental setup involves a camera placed at the top of a hall, with the lens pointing vertically downward and a distance of approximately 6m from the ground. The experimental environment utilizes vs2010 and opencv2.2, with the experimental set capturing video data at 30 frames per second.

The proposed top-view behavior recognition method can identify three static postures and eight high-level dynamic behaviors. Experimental results demonstrate significantly higher identification compared to existing literature. However, the recognition of behaviors such as arm lifting poses a serious challenge, and future research will focus on overcoming the impact of arm movements. This paper serves as the foundation for behavior recognition from the top-view, and future research will concentrate on other behaviors, as well as people interaction and tracking.