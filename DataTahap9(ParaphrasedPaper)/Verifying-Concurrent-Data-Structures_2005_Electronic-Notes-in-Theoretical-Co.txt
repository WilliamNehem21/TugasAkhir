Concurrent implementations of data structures are designed to enable multiple processes to carry out operations on a data structure simultaneously. To demonstrate the correctness of such implementations, it is necessary to prove that all potential interleavings of the individual steps of these operations will yield accurate results. While linearizability is widely recognized as the appropriate criterion for correctness in concurrent data structures, it does not seem to be extensively utilized in mechanical proofs.

The relationship between h and s can be clarified by enhancing h with linearization points that correspond to the points at which operations in h are considered to take place. Let x.do opp(args) signify the linearization point for an operation with invocation x.opp(args), which must occur after the invocation and before any corresponding response. If h' is an augmented history obtained by adding these linearization points to a history h, we can create a linearization of h from h' in the following manner:

The push by process 2 precedes the pop, but neither is ordered with respect to the push by process 1, and the pop is pending. This history can be linearized in five different ways by either leaving the pop incomplete or completing the pop by adding either s.popok2(a) or s.popok2(b), and then linearizing the push operations accordingly.

The actions from various processes can be interleaved in all possible ways that are consistent with the stack specification. Thus, the executions produced by the stack ioa are precisely the augmented histories described in section 2 and are linearizable for the reasons provided there. We can demonstrate that the canonical automata construction described above generates an ioa that produces exactly the linearizable histories for a given data type; refer to the details for further information.

The push operation creates a new node and stores the value to be pushed in its val field. It then attempts to link the new node into the list. It takes a snapshot of head in the local variable ss, sets the next field of the new node to this value, and then tries to make head point to the new node. This will yield the correct result only if head has not changed since the snapshot was taken, so a cas 9 is used to atomically test whether head is still the same as ss, and if so, change it to n, in which case the operation is complete. If head has changed, the operation loops back to line 3 and tries again.

If the stack was empty, pop copies the next and val fields from the node pointed to by ss and attempts to update head to point to the value in the next field of the node it points to. This will only produce the correct result if head has not changed since the snapshot was taken, so a cas is used to atomically test whether head is still the same as ss and if so, change it to ssn, in which case the operation is complete and lv can be returned as the popped value. If head has changed, the operation loops back to line 1 and tries again.

A subtle point, not evident in the code, is that the correctness of the algorithm relies on the fact that a successful pop operation does not free the memory used by the popped node, since other processes might also be attempting to pop that node. The algorithm would also be correct if memory was recycled by a garbage collection mechanism that only reclaims storage when there are no pointers to it, but then to claim that the implementation is lock-free, we would have to show that the garbage collector was lock-free (see). Other methods of recycling memory use version numbers to detect when storage has been recycled (e.g., see).

We are also interested in exploring ways to simplify our proofs by utilizing constructive approaches based on refinement. This might involve identifying common steps in the construction of nonblocking algorithms and perhaps employing a different formalism, such as action systems. A similar effort has been taken on by Abrial and Cansell, using Event-B (see).