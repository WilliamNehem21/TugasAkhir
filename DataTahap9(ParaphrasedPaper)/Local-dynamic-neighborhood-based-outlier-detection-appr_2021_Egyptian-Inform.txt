However, conventional local algorithms often yield subpar results and are sensitive to neighborhood parameters, such as the 'k' parameter. Many traditional algorithms use k-nearest neighbors (KNN) to measure the neighborhood of data objects. However, KNN quantifies a rounded or spherical local region, making it an imprecise measurement for datasets with non-spherical clusters. Furthermore, setting the value of 'k' becomes challenging as the dataset size increases, potentially leading to memory limitations in common computers. To illustrate, a dataset with 1 million samples would require 7,450.58 GB of memory for the affinity matrix, creating a bottleneck for the computer's memory capacity.

A commonly used partitioning method, due to its low computational and space complexity, is clustering-based local outlier factor (CBLOF). Another approach, called LDcof, first separates the dataset into clusters using k-means before computing the LDcof scores based on the distance of an object to its cluster center. Additionally, the histogram-based outlier score (HBOS) is an efficient anomaly detection algorithm that computes the feature probabilities of each data object. Zhao et al. proposed the scalable unsupervised outlier detection (SUOD) method for high-dimensional large datasets by integrating several classical detection methods.

In the subsequent section, we provide a detailed introduction to the proposed LDNOD algorithm and its framework, LDNOD-KM. LDNOD is capable of producing high-quality and robust detection results, while the integration of LDNOD with k-means in the LDNOD-KM framework allows for efficient handling of larger-scale datasets without compromising accuracy.

To address the issue of close data objects having similar or identical k-nearest neighbor (DRNN) neighborhoods, a common neighborhood is constructed for nearby data points. This means that all data objects within a DRNN neighborhood share a common neighborhood. It is important to note that different DRNN neighborhoods can share neighbors to retain their natural features. Thus, for a dataset 'X' containing 'n' data points, this approach ensures a consistent representation of neighborhoods for nearby data objects.

Typically, traditional detection algorithms use a top-n approach, where they output the top 'n' data objects with the highest scores as outliers. However, determining the parameter 'n' can be challenging without prior knowledge, and the detection quality is heavily dependent on it. In contrast, the LDNOD algorithm allows for an intuitive setting of the LNOF threshold value, such as 3 or 5, even though a consensus has not been reached.

Unlike traditional methods, LDNOD scores each local neighborhood instead of each data object, hence its name, Local Neighborhood-based Outlier Factor (LNOF). This approach can potentially enhance the efficiency of the algorithm by handling a local region at a time, rather than addressing individual data objects. Additionally, due to the similarity of outlier degrees among nearby data objects, there is no need to compute outlier scores for all data points. Moreover, LNOF can identify both single outliers and groups of outliers.

The purpose of this step is to obtain a small set of partitions that reflect the original data, as opposed to creating accurate clusters. In practical terms, 'm' is usually much larger than the actual number of clusters for large-scale datasets. Therefore, pure partitions can be obtained using k-means, containing only data objects from the same class apart from outliers.

In this paper, we propose a new local detection algorithm, LDNOD, and its LDNOD-KM framework. The LDNOD algorithm is insensitive to neighborhood parameters due to the stability of DRNN, which constructs the neighborhood of an instance based on dynamic reference objects. Additionally, the design incorporates the concept of sharing neighborhoods for nearby objects and the scoring of each local region, potentially reducing the runtime of LDNOD. By combining the advantages of LDNOD and k-means, LDNOD-KM efficiently handles large-scale datasets without sacrificing accuracy. Lastly, experimental results demonstrate the effectiveness of LDNOD and its framework. Future work will focus on further enhancing LDNOD-KM and applying it to larger scale and high-dimensional datasets.