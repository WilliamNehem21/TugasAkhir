The remainder of the paper is structured as follows. Section 2 presents an overview of the design of centralized or traditional search engines. Section 3 discusses the related work in this area. Section 4 describes the proposed architecture of the Distributed Web Crawling and Indexing System (DWCIS) and its workflow. Section 5 outlines the experimental setup and Section 6 presents the experimental results and discussion. Finally, Section 7 provides the concluding remarks.

Spiders, also known as web robots or crawlers, are programs used by search engines to retrieve web pages by recursively following URL links in pages using the standard HTTP. These spiders begin by reading from a list of starting-seed URLs and downloading the documents at these URLs. Each downloaded page is processed, and the URLs contained within it are extracted and added to the queue. Spiders usually connect simultaneously to multiple web servers to improve speed, either using multiple threads of execution or asynchronous I/O.

The documents retrieved by the spiders are stored in a web page repository, often in the form of a database or as compressed files, to reduce storage space. An indexer processes the pages in the repository and builds an underlying index for the search engine, which includes tokenizing each page into words and recording the occurrence of each word in the page. A query engine accepts search queries from users and performs searches on the indexes, ranking the search results according to content and link analysis scores.

Several authors have proposed mobile crawling techniques that distribute the data retrieval process across the network, bypassing the centralized architecture of current web crawling systems. However, these systems often ignore distributed indexing, index updating, and web page change detection.

Other proposed web page change detection techniques, such as checksum-based content level change detection and similarity computations between two versions of a web page based on HTML tags, have been discussed in the literature. These methods have drawbacks such as network overload, wasted resources, and storage space requirements.

In particular, the paper introduces a state-of-the-art indexing and retrieval functionalities and supports the rapid development and evaluation of large-scale retrieval applications. The paper also presents a performance evaluation of the proposed change detection and indexing techniques, comparing them against existing methods.

Additionally, the paper describes the Mobile Crawlers using a last modification date change detection technique, a page size change detection technique, a hash value change detection technique, and a document index change detection technique.

Furthermore, the paper outlines the simulation of non-significant page changes by altering specific HTML web pages to investigate the behavior of the proposed techniques.