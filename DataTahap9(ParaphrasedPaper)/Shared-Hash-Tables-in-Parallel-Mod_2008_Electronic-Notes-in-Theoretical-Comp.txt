We utilize a model that relies on shared memory, where threads have separate stacks in a shared address space and utilize thread-local storage for storing private data. Our working environment is POSIX, using lightweight processes for implementing threads. Context switching between threads is more cost-effective than switching between full-featured processes with separate address spaces, so utilizing more threads than the number of CPUs in the system only results in a minor performance penalty.

Processor cache: locality and coherence. Currently, there are two main architectures for level 2 cache. In one architecture, each processing unit has its own private level 2 cache (in the case of symmetric multiprocessing), while in the other, there is a shared level 2 cache for a package of 2 cores (designs with a level 2 cache shared among 4 cores are not commercially available at the time of writing). In larger shared-memory computer systems, split cache is common, as they often contain a significant number of cores attached to a single memory block. In recent hardware, the basic building units are dual-core CPUs with shared cache, and despite separate caches among different units, the caches are still separate. This architectural peculiarity has notable effects on performance, which will be discussed in more detail later.

Shared memory bus. In symmetric multiprocessing (SMP) machines, the memory is connected to a single shared memory bus, necessitating serialization of RAM access from different processors. This constrains the total memory throughput of the system, and eventually, the available memory bandwidth becomes the bottleneck of computation. This is an important consideration for memory-intensive workloads, such as model-checking.

The algorithm is an extended enumerative version of the "One Way Catch Them Young" algorithm. The algorithm's concept involves iteratively removing vertices from the graph that cannot lie on an accepting cycle, based on two removal rules. First, a vertex is removed from the graph if it lacks successors in the graph (indicating it cannot lie on a cycle). Second, a vertex is removed if it cannot reach an accepting vertex (signaling that a potential cycle the vertex lies on is non-accepting). The algorithm repeats removal steps until there are no vertices left in the graph (indicating the original graph contained an accepting cycle) or all vertices have been removed (indicating the original graph had no accepting cycles).

The theoretical advantages of the first approach are that the lock granularity and contention should remain minimal throughout program execution. Although a fixed number of locks heightens competition for any given lock, it should theoretically remain constant, as long as the number of competing threads is constant. The "squareroot" approach represents a compromise between these two. All methods are evaluated in the experimental section.

Shared queue. Another approach is to distribute states by placing them in a single shared breadth-first search (BFS) queue, rather than using a partition function. This approach is expected to achieve optimal load-balancing, although compromises may be needed to balance it with locking overhead and contention.

Both algorithms we have implemented are independent of the order of visits, hence they can be run in both BFS and DFS order. These algorithms are "reachability" and "owcty," and several other distributed algorithms share this property and could be used in this setting. We have not considered the parallel versions of nested DFS, as they do not use partitioning at all.

Our primary testing machine is a 16-way AMD Opteron 885 (comprised of 8 CPU units with 2 cores each). All timed programs were compiled using GCC 4.1.2 20060525 (Red Hat 4.1.1-1) in 32-bit mode, using -O2. This limits the addressable memory to 3GB, which was sufficient for our testing. The machine has 64GB of installed memory, meaning that none of the runs were impacted by swapping.

In an environment with relatively low communication overhead, the different schemes did not exhibit as much variation as we originally anticipated. Our research was motivated by the aim to enhance the performance and scalability of our parallel, shared-memory model checking platform based on Divine. However, the results have been less convincing than expected.