The development of machine learning (ML) and deep learning (DL) innovations, such as applications, datasets, frameworks, models, and software and hardware systems, is progressing rapidly. However, the current practice of sharing these innovations typically involves building ad-hoc scripts and writing manuals to describe the workflow. This approach makes it challenging to reproduce reported metrics and to transfer innovations to different environments and solutions. Therefore, the establishment of a standard benchmarking platform with an exchange specification and well-defined metrics to compare and benchmark these innovations is essential for the success of the ML/DL community.

MLCommons, previously known as MLPerf, is a platform that aims to address the needs of the emerging machine learning industry. It consists of two benchmark suites: MLCommons Training, which measures the speed at which systems can train models to a target quality metric, and MLCommons Inference, which measures the speed at which systems can process inputs and produce results using a trained model. Both benchmark suites aim to provide results across a range of computing services, from tiny mobile devices to high-performance computing data centers. This paper focuses specifically on benchmarking ML/DL inferences and thus concentrates on MLCommons Inference.

In addition to commonly used model metrics such as accuracy, MLCommons Inference includes a set of system-related metrics such as percentile-latency and throughput. These metrics cater to different use cases, such as the 99th percentile-latency for a data center to respond to a user query.

The System Under Test (SUT) in MLCommons Inference comprises the inference system under benchmarking, including ML/DL frameworks, models, software libraries, and the target hardware system. The SUT receives a query from the load generator (loadgen), completes an inference run, and reports the result to the loadgen.

This paper presents the utilization of MLModelScope, an inference platform with a defined exchange specification and an across-stack profiling and analysis tool, to replace components other than the loadgen in the MLCommons Inference. This extends the applicability of MLCommons Inference for models beyond its current scope.

While MLModelScope currently supports models for computer vision tasks, this paper implements user-defined pre-processing and post-processing interfaces to demonstrate usage on models with different modalities, such as question answering and medical 3D image segmentation.

In order to avoid using intermediate files, this paper embeds a Python interpreter into MLModelScope using Python/C APIs to execute Python functions. This enables the execution of Python functions within MLHarness directly, thereby bypassing the issues associated with the naive solution.

The paper introduces a method for efficient data transfer from Go to Python, addressing the challenge of exchanging data between the two languages.

The success of MLHarness in benchmarking ML/DL model inferences is demonstrated through two sets of experiments showcasing its support for modalities not covered in MLModelScope and its ability to report results for scenarios defined by MLCommons Inference. Additionally, MLHarness is shown to support models beyond MLCommons Inference and to report results in accordance with the defined metrics.

The paper also discusses the future potential of MLHarness in extending support for MLCommons Training and its broader impact on computing-related research.

Ultimately, the paper emphasizes the need for a standardized set of measures to benchmark and compare ML/DL models' quality and performance and highlights the role of MLHarness as a scalable benchmarking system that addresses this need.