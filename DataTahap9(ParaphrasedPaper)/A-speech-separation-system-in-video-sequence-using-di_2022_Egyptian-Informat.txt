As speech technologies such as Apple Siri and Amazon Alexa continue to advance, the issue of speech separation has gained increasing importance and interest. Speech separation, which is considered a crucial pre-processing step in various speech applications such as noise reduction in speech signals, automatic speech recognition, and speech database creation, has become more significant as machines struggle to accurately recognize speech in noisy and crowded environments. The progress in the field of speech separation heavily relies on the development of suitable speech databases.

In challenging acoustic environments like a cocktail party, following a single speaker amid other speakers and background noises is difficult. Researchers have demonstrated the advantage of incorporating visual information alongside audio signals to analyze mixed sounds in noisy environments. Due to limited resources, training the proposed model with vast datasets of recorded videos is challenging, so a subset of samples was extracted from the Oxford-BBC LRS2 dataset.

The remainder of the paper is structured as follows: section 2 provides a review of several related speech separation approaches, section 3 elaborates on the proposed model for sound separation, section 4 describes the experimental setup, reports the experimental results, and discusses findings, and finally, section 5 concludes the work.

Various researchers have attempted to address the speech separation problem using only audio information. Yannan et al. demonstrated an unsupervised co-channel speech separation using a deep neural network framework. Their system effectively separated speech from mixtures of two unseen speakers with different genders using log power spectral features as input. The proposed system outperformed Gaussian mixture model-based methods in terms of Perceptual Evaluation of Speech Quality (PESQ).

Some researchers have explored separating sounds using reference audio of the target speaker. Quan Wang et al. presented a system that separates the voice of a target speaker from multi-speaker signals by utilizing a reference signal from the target speaker. They achieved this through training two separate neural networks.

Ariel Ephrat et al. designed and trained a dilated convolutional neural network model that takes an audio of mixed sounds and detected faces from each video frame as input to split the mixed sounds into separate audio streams for all detected speakers. Their model utilized visual embedding features to enhance source separation quality and support tracking of all speakers in the video.

The proposed model in this paper introduces an architecture for speech separation in the presence of multiple speakers, with a focus on a two-speaker scenario. It leverages audio-visual models that separate speech based on face embedding features of each speaker and can achieve good performance even with limited dataset size.

The use of U-Net, which was originally developed for medical image segmentation, has been found to be effective in speech separation and enhancement due to its simple structure and high segmentation performance on various benchmarks, especially with limited data.

In the proposed model, a module is used to guide the overview model in the separation process by retrieving the voice of the target speaker based on their facial features. A preprocessing step applies a pretrained facenet network to scanned images of the recorded videos to extract embedding feature vectors that represent the most important features corresponding to each face.

To effectively cover a large input area and discover relations between the input elements, a dilated convolutional neural network (CNN) was utilized which facilitated easier discovery of input relations and reduced training time, enabling better performance in image classification and segmentation.

References:

- Quan Wang, Hannah Muckenhirn, Prashant Sridhar, Zelin Wu, John Hershey, Rif A. Saurous, Ron J. Weiss, Ye Jia, Ignacio Lopez Moreno, "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking," Proceedings of the International Speech Communication Association (INTERSPEECH), Graz, Austria, September 15-19, 2019.