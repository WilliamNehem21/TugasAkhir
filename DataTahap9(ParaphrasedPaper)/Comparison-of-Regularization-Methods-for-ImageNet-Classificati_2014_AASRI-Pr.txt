The application of large and deep convolutional neural networks has proven to be effective in image classification tasks, although the issue of overfitting needs to be addressed. This study compares the effectiveness of various regularization techniques in the context of the 2013 ImageNet Large Scale Visual Recognition Challenge. The empirical findings indicate that dropout outperforms dropconnect when applied to the ImageNet dataset.

Visual object recognition poses significant challenges in computer vision, especially in scenarios involving large-scale and real-world settings where high-resolution images and numerous object categories are involved. Historically, neural networks were not commonly utilized for this task due to the substantial requirement for labeled data and computational power. However, recent advancements in GPU technology and the availability of extensive labeled image datasets have facilitated the efficient use of neural networks, enabling them to outperform other methods. Neural networks have the potential for substantial learning capacity, which can be managed through the manipulation of network depth and layer size. Greater success can be achieved by employing deep neural networks, as depth is crucial for the acquisition of effective internal representations of input data. Large neural networks are susceptible to overfitting, necessitating the adoption of robust regularization techniques such as data augmentation, dropout, or the recently introduced dropconnect. Additionally, the incorporation of prior knowledge, such as an understanding of the 2D structure of input data, can enhance the performance of neural networks. Convolutional neural networks represent one category of such networks, exhibiting fewer learnable parameters than standard fully-connected neural networks, making them easier to train and less prone to overfitting.

The primary contribution of this research involves the comparison of dropconnect and dropout regularization techniques, which, to the authors' knowledge, have only been evaluated on small datasets, and the exploration of improved data augmentation in large-scale settings within the context of the 2013 ImageNet Large Scale Visual Recognition Challenge. The study began by examining an architecture similar to that proposed by previous winners of the challenge and evaluating its performance with alternative regularization methods in place of dropout. Various strategies for enhancing the results were also proposed.

The results of the study indicate that dropout regularization yields superior performance to dropconnect for the ImageNet classification task. Furthermore, it was observed that the network utilized in the study was too small, and that improved results could be achieved by employing a larger network with more effective regularization techniques. It is suggested that future improvements could be achieved through the incorporation of novel methods such as droppart (Tomczak, 2013), standout (Ba and Frey, 2013), maxout, stochastic pooling (Zeiler and Fergus, 2013b), dlsvm (Tang, 2013), lp units, channel-out (Wang and Jaja, 2013), and various data augmentation techniques.