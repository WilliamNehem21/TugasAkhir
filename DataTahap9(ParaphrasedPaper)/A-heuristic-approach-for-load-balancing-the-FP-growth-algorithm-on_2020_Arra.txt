The fp growth algorithm encounters performance and scalability challenges, particularly when dealing with very large datasets that cannot fit into memory, which hinders the processing of big data. To address this issue, a distributed and parallel implementation of the algorithm is necessary to accommodate large datasets. Hadoop has introduced parallelization techniques based on shared memory SMP architectures for data mining algorithms. These techniques involve parallelizing the fp growth algorithm and scaling it to handle datasets of several hundred megabytes. Additionally, two strategies, namely a cache-conscious fp-array and lock-free fp-tree construction, have been employed to parallelize the fp growth algorithm on multi-core processors, aiming to address issues such as poor data locality and insufficient parallelism of data mining algorithms on multi-core processors. Moreover, a multithreaded solution on multi-core CPU and GPU has been implemented, both of which are based on fp-arrays.

Fidoop-dp employs a Voronoi-based partitioning technique to distribute the load on reducer nodes, creating clusters of transactions and allowing for parallel mining of these clusters in different reducers. This approach not only aims to balance the load of the reducers but also avoids duplicate transactions in the intermediate data. However, it requires a pre-processing step, the overhead of which is not accounted for in the experimental results.

In the MPFP algorithm, grouping complexity is O(1) as it sorts the FP-list by the number of items per group. However, this method leads to the creation of unbalanced groups, which affects the performance of reduce tasks, slowing down the entire MapReduce job.

The FP-growth algorithm is implemented as a MapReduce job, similar to the parallel FP growth step of the MPFP algorithm. The FP-list and the map containing the groups are distributed to all map and reduce tasks using the distributed cache feature of the MapReduce framework, with every task reading from it. The group map is used in the map task to determine the group ID for a given item and in the reduce task to determine the items belonging to a given group ID.

The dataset used in the experiments was constructed from approximately 1.7 million web documents, with HTML tags and common words filtered out. Each transaction represents a web document, and the items in the transactions represent the distinct terms present in the document. The resulting dataset was 1.48 GB in size, comprising 1,692,082 transactions with 5,267,656 distinct items, and with a maximal transaction length of 71,472.

In terms of memory size, both BPFP and HBPFP successfully completed jobs for a reduce memory size of 8 GB, with HBPFP running faster than BPFP for most of the minimum support thresholds. MPFP struggled to handle large datasets at certain minimum support thresholds, even at a reduce memory size of 7 GB. Similarly, at a reduce memory size of 3 GB, both HBPFP and MPFP successfully ran at a minimum support threshold of 11%, with HBPFP running faster than both BPFP and MPFP.

The study also referenced the work by Vavilapalli et al. on Apache Hadoop YARN for resource negotiation in cloud computing.