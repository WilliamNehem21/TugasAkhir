As organizations increasingly seek accurate sentiment analysis (SA) for valuable information, it has become evident that opinions expressed on Arab social media are primarily in dialectal Arabic, with modern standard Arabic (MSA) being rarely used. Research focusing on dialectal Arabic is relatively new, and efforts to ensure accuracy are ongoing. Furthermore, dialectal Arabic is a dynamic language, with new words and terms continually introduced by new generations, alongside evolving word usages over time. To address these complexities, a hybrid approach is proposed, involving the construction of lexicons and their use in training a machine learning algorithm. This approach introduces a semi-automatic learning system, named "HILATSA" (Hybrid Incremental Learning Approach for Arabic Tweets Sentiment Analysis), that adapts to the dynamic nature of dialectal Arabic by crawling Twitter for new tweets and expanding the lexicon with newly extracted words. The primary contribution of this work is the introduction of a sentiment analysis tool for Arabic tweets that effectively handles the rapid changes in words and their usage over time. In line with this approach, essential lexicons, such as word lexicon, idioms lexicon, emoticon lexicon, and special intensified words lexicon, are built, along with two lists containing intensification and negation tools. These resources are made publicly available to address the shortage of dialectal Arabic resources. The paper also explores the effectiveness of employing the Levenshtein distance algorithm in SA to address variations in word forms and misspellings.

The remainder of the paper is organized as follows. Section two briefly outlines related work in the field of SA. Section three provides details of the methodology, including the employed datasets, lexicons, and classifiers. Section four presents the results achieved on different datasets and provides corresponding discussions. Finally, section five offers the conclusion of the entire work.

The development of a sentiment analysis tool for social media is an emerging task, given its widespread use and the wealth of information it generates. While significant work has been done in English, research on Arabic sentiment analysis is still in its early stages, with many resources not publicly available. Although some existing lexicons can be expanded, none have been proven capable, to the authors' knowledge, of adapting to the dynamic nature of the language over time without requiring retraining. This section provides a brief overview of some Arabic and English tools in this domain.

Ibrahim et al. developed a system for MSA and colloquial Arabic, creating two lexicons for words and idioms. They identified negation tools, intensifiers, wishes, and questions and employed a support vector machine (SVM) classifier to detect sentence polarity.

Duwairi et al. converted emotions such as "fx1" and "fx2" into their corresponding words and also converted both dialect sentences and Franco-Arabic words to their equivalent MSA. They achieved their highest accuracy using a naive Bayes (NB) classifier.

Zhang et al. proposed a new entity-level sentiment analysis method for Twitter without manual labeling. They first manually created a lexicon to determine tweet sentiment polarity, then extracted opinion indicators as words and tokens using chi-square based on the lexicon. Finally, they trained a classifier to identify tweet sentiment polarity.

Nodarakis et al. proposed a distributed parallel algorithm in the Spark platform. To avoid intensive manual annotation, tweets were labeled based on emotions and hashtags, with a bloom filter applied to improve performance after building the feature vector. They used all k-nearest neighbor (AKNN) queries to classify the tweets.

Poria et al. developed a seven-layer deep convolutional network for the aspect extraction sub-task. Their approach provides better accuracy than both linguistic patterns and conditional random fields (CRF).

In this work, SVM, L2 logistic regression, and recurrent neural network (RNN) classifiers are utilized. SVM is a linear classifier that creates a model to predict the class of the given data by finding a general hyperplane with the maximum margin. It supports various formulas for classifications and employs a penalty or cost parameter, denoted as "C," to limit the misclassification of training points of one class into the other side of the hyperplane, with the trade-off between overfitting and classification error. Logistic regression, on the other hand, is a discriminative classifier that extracts weighted features to maximize the probability of event occurrence and employs regularization to produce more general models and reduce overfitting.

Using 10-fold cross-validation with various training datasets, including ASTD, MASTD, ARSAS, GS, and the Syrian corpus, word lexicons are constructed for each fold excluding the testing part, and the classifiers are trained. The results are then presented in terms of accuracy and average F1 score, alongside comparisons with other existing work.

In summary, SVM generally demonstrates better accuracy, likely attributed to its kernel trick enabling improved accuracy with high-dimensional data, as well as its robustness to outliers and resistance to overfitting. Additionally, the 3-class sentiment analysis proves more challenging than the 2-class scenario, as expected. Moreover, the F1 score tends to be higher for balanced datasets compared to unbalanced datasets.

Finally, using the "ARTwitter" dataset, a classifier is trained using the same lexicons from the previous part, and 10-fold cross-validation is employed for training and testing. The word learner is then utilized to learn new words and update the lexicon, followed by the classification of tweets after the update.