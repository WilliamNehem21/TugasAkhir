Most software contains defects, which can range from easily identifiable errors to those that emerge infrequently or are not perceived as errors. Identifying significant static analysis functionality provided by tools and surveying underlying supporting technology is the main goal of this study. However, providing a ranking of the tools or a comprehensive survey of all functionality provided by the tools is not the objective. It is difficult to provide such rankings because static analysis is only part of the functionality provided by the tools and the tools provide largely non-overlapping functionality, even when seemingly providing the same functionality. 

Studying the internal details of commercial and proprietary tools presents challenges as it is difficult to obtain full technical information. Despite this, some technical information is publicly available in manuals and white papers, as well as in academic tools extensively described in research journals and conferences. However, descriptions of suggested technical solutions are subject to some speculation. 

Languages such as C and C++ provide little support to avoid or deal with runtime errors. Checks for boundary access in arrays, possible dereferencing of pointer variables, and well-defined type casting must be enforced by the programmer or ensured not to be needed in practice.

The term static analysis refers to automatic methods used to reason about runtime properties of program code without executing it. Although static analysis can check for issues such as premature termination or ill-defined program results, it does not address functional correctness errors. It can be contrasted with dynamic analysis, which concerns analyzing programs based on their execution.

The precision of static analysis determines the frequency of false positives reported. Path and context-sensitivity, value analysis, and aliasing are important considerations in static analysis to reduce false positives and make the analysis more precise.

Static analysis can be applied to incomplete code, but the quality of the analysis may be poor if it does not have information about the external use of the code or if it contains calls to unavailable procedures or functions.

Soundness can refer to assumptions made about possible executions, and a sound analysis may result in false positives but no false negatives.

The three tools studied - Polyspace, Klocwork, and Coverity - have different targets and areas of focus. For instance, Polyspace primarily targets embedded systems, Klocwork and Coverity target networked systems and applications, and each provides unique functionality such as security checkers and support for analyzing resource leaks.

Incremental analysis may take less time than analyzing the whole system from scratch, but the depth of analysis in an incremental approach may be limited.

Writing new, non-trivial checkers is challenging and may lead to errors. Current tools do not provide explicit guidelines for writing correct checkers or documented support for manipulation of abstract values. Additionally, there is limited support for dealing with concurrency in these tools.

Several users had differing expectations about the capabilities of the tools in finding defects and evaluating their severity. The tools found defects not discovered by manual testing, and in some cases, they highlighted potential problems that could lead to serious crashes if the system or usage were to change.

In evaluations of the tools, it was observed that the number of defects reported and the false positive rates varied significantly between different tools, and each tool found defects specific to its own analysis.

Overall, the study focused on identifying significant static analysis functionality and surveying underlying supporting technology, with a particular focus on Polyspace, Klocwork, and Coverity, while also noting the limitations and challenges associated with such analysis.