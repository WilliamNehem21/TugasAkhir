Mobile assisted language learning (MALL) allows learners to access learning materials quickly and conveniently, regardless of their location or time constraints. While there are numerous AI-based language learning applications available, such as Duolingo, Hello English, Babbel, Memrise, and Busuu, which cater to popular languages such as English, French, Spanish, Estonian, German, and Russian, there is a lack of such resources for indigenous languages like Chakma, Marma, and Saotal in Bangladesh. Therefore, there is a pressing need for a digital learning platform to preserve and promote these languages, and AI technology can be leveraged to improve reading, writing, speaking, and listening skills in these languages.

In the realm of contemporary computer vision research, one promising tool is heatmap generation, which illustrates the parts of an input image considered by a model when making predictions. This visualization technique could enhance users' understanding of how neural networks generate predictions.

In the context of data cleaning, the ISI Numerals and Ekush datasets underwent a rigorous cleaning process, resulting in a reduction of the ISI Numerals dataset to 19,392 images for training and validation, and 4,000 images for testing. The original dataset prior to cleaning contained 27,500 images. Similarly, the Ekush dataset was cleaned to yield 17,745 images for training, 5,053 images for testing, and 2,530 images for validation.

The self-ChakmaNet model displayed early notches in the validation and test error rates; however, as training progressed, the loss gradually decreased towards zero, indicating a well-fitting model akin to the state-of-the-art MobileNet_V2 for Chakma handwritten character recognition.

The predictions made by the self-ChakmaNet model are based on the stroke pattern of the character, as evidenced by the visualization outcomes showing hot or red mapping in the heatmaps. This illustrates the interpretability of the model, which focuses on important features such as the stroke pattern of handwritten characters. Overall, the self-ChakmaNet model demonstrates high accuracy in classifying instances and extracting relevant features, comparable to the MobileNet_V2.

In comparison to larger models with millions of parameters, the self-ChakmaNet model was trained on only 453,000 parameters, highlighting its computational efficiency and effectiveness.