Over the last ten years, the rate at which central processing unit (CPU) performance has improved has been relatively sluggish due to physical limitations. As the size of transistor circuits approaches the level of individual atoms, challenges stemming from physical constraints such as leakage have hindered the adherence to Dennard scaling law. In contrast, burgeoning computing domains like artificial intelligence (AI), big data, and the Internet of Things have flourished as computing resources have reached a critical scale. The burgeoning demand for computing resources in these domains has outpaced the capabilities of CPUs in data centers, leading to the adoption of specialized chips, such as graphics processing units (GPUs), tensor processing units (TPUs), and data processing units (DPUs), as a new trend in both academic and industrial settings.

Benchmarking is a commonly utilized research technique in computer science for assessing the performance of systems. Benchmarking evaluations can offer insights into the actual performance of the system being evaluated and can inform the future co-design and optimization of software and hardware. Given the development of DPUs and data centers, the creation of a DPU benchmark suite is imperative. However, at present, no comprehensive benchmark suite is available for DPUs.

We performed assessments on the NVIDIA BlueField-2 using DPUbench and provided suggestions for optimization. Our experiments showed that the NVIDIA BlueField-2 can effectively offload network applications from the CPU, especially in the realms of network storage protocol and deep packet inspection (DPI) applications. In our end-to-end assessment, we demonstrated that the NVIDIA BlueField-2 substantially reduces server CPU utilization in network applications, enabling the allocation of more CPU computing resources to computing applications.

The remainder of this paper is structured as follows: Section 2 presents the background and rationale, Section 3 introduces the DPUbench methodology, Section 4 outlines the operator set in DPUbench and the corresponding experimental results, Section 5 discusses the end-to-end evaluation programs in DPUbench with their respective experimental results, Section 6 concludes with a consideration of related work, and Section 7 lays out the conclusions and future work plans.

First, we will provide a background on DPUbench, including existing DPU benchmarks, DPU evaluation programs, and DPU characterization studies. Next, we will briefly introduce the NVIDIA BlueField-2 DPU. Based on the foregoing discussion, we will present the rationale for DPUbench.

The need for DPUbench arises from the lack of a DPU benchmark capable of effectively evaluating DPUs with different architectures, as well as the rapidly evolving DPU architecture due to the growing demand for offloading CPU computing resources in data centers. A scalable DPU benchmark that can assess DPUs of different architectures is essential, and it should be adaptable to accommodate the swift evolution of DPUs.

Another motivation behind DPUbench is ensuring the inclusiveness and coverage of the benchmark suite, along with the reliability and efficacy of the evaluation results. The benchmark programs should cover various network application scenarios without imposing excessive evaluation costs in terms of time and resource utilization. Furthermore, network-related metrics should be thoughtfully selected, and DPUs should be evaluated in real network environments within data centers to ensure the dependability of the evaluation results.

Our methodology facilitates the development, maintenance, and updating of DPUbench by providing a clear and detailed description of each step involved in its creation. In this section, we will offer a detailed introduction to the various steps entailed in constructing DPUbench.

The problem definition of DPUbench was formulated to guide the construction of the benchmark suite from an application perspective, specifically for network applications. Network-related metrics such as latency and throughput were chosen as the evaluation metrics for DPUbench, with the throughput acceleration ratio and CPU utilization ratio designated as the performance metrics. Additionally, DPUbench covers various aspects of network applications, encompassing network transmission protocols, compression and decompression algorithms, storage protocols, encryption and decryption algorithms, and other relevant factors.

We conducted performance evaluations on the NVIDIA BlueField-2 DPU and implemented optimizations commonly used in real DPU applications to ensure that the micro-benchmarks based on the operators in DPUbench were representative. Specifically, we evaluated the performance of offloading application recognition to the DPU, observing a decrease in process delay of 10% to 15%. However, this reduction was lower compared to the results obtained for the RXPMATCH operator due to the involvement of both server CPU and DPU processing in the packet processing procedure for the application recognition workload.

In summary, we have outlined the necessity and motivation behind the development of DPUbench, as well as our approach to constructing the benchmark suite and evaluating the performance of NVIDIA BlueField-2 using DPUbench. This represents an important step in ensuring the comprehensive assessment of DPUs with different architectures.
