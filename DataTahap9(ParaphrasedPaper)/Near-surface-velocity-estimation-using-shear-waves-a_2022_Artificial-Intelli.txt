The primary mode is the most prominent, with the energy information being disregarded. Nevertheless, this energy information is greatly influenced by the stratigraphy and thus encompasses supplementary data (Socco and Strobbia, 2004). The velocity of the Rayleigh wave is predominantly dependent on the shear wave velocity and layer thickness, as well as the bulk density and p-wave velocity.

In modal analysis, there is a possibility for higher modes to intersect and overlap, or to be more pronounced than the primary mode. Ideally, it would be preferable not to selectively choose the dispersion curves, but rather to provide the entire f-k or fvph gather to the inversion or prediction algorithm. This is the precise issue that full waveform inversion (FWI) aims to resolve. However, FWI presents its own challenges and is computationally intensive, particularly when applied to noisy land seismic data in an elastic context. As a result, our objective was to evaluate whether a deep neural network could bridge the gap between dispersion curve inversion and FWI and could learn.

While deep learning techniques are adept at learning from a large volume of supervised examples, they typically do not generalize well for smaller datasets. In such instances, transfer learning can be beneficial by enabling the transfer of knowledge from one task to another. This transfer of knowledge involves initializing the weights of the network with values obtained from a task that is usually more complex. A more complex dataset facilitates learning a more diverse set of discriminative features, thereby enriching the representation space. This is generally advantageous in computer vision, where the initial convolutional layers learn to generate high-level features common across tasks, and reusing these features facilitates faster and more effective optimization. In the geophysical context addressed in this paper, we utilize pre-trained neural network weights and biases from the ImageNet task, which involves categorizing natural images into 1000 categories. Although seismic data and natural images exhibit significant differences in distribution, we are interested in exploring whether their latent spaces have any overlap, and whether transfer learning from ImageNet can assist in our endeavor.

Deep neural networks heavily rely on extensive data to mitigate the risk of overfitting. However, labeled data is often limited for many applications, including geophysics. Data augmentation serves as a method to overcome this limitation by creating variations in existing data to artificially expand the dataset. Termed image augmentation in our context, this technique involves applying affine transformations to images without affecting the ground truth. The augmentation methods explored in this paper include random cropping, random translations, random dilations, random rotations, and random masking. These techniques improve the generalization of the deep learning model and, consequently, mitigate overfitting.

Another common issue with deep neural networks is their lack of generalizability when used with out-of-distribution datasets. Even a model trained on a large amount of training data can still struggle if the distribution of the validation set differs from that of the training set. To address this, we modify the ranges within which the network learns to fit the output curves, thereby enabling it to adapt to out-of-distribution samples as well when provided with additional information on the range.

The backbone architecture for this task is ResNet18. To incorporate prior knowledge on the bounds for certain variables, two meta-architectures are employed, each comprising a linear layer, a batch normalization layer, and a ReLU activation module. These meta-architectures encode the upper and lower bounds for certain variables into 50 neurons. The output from both architectures is concatenated with the output of the CNN backbone and fed into two linear heads to compute the required variables.

Notable improvements were observed in the out-of-distribution validation set. During inference, no augmentations are applied. The training and inference processes for the domain adaptation module differ, with the module being trained to tolerate noise in a specific manner and then applied accordingly during inference.

Overall, there is a better agreement between the predictions from the constrained network and the uphole velocities, as opposed to the unconstrained network.