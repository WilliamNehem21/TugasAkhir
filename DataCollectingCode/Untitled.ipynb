{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b4a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader \n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3387f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPaper/Short Text Classification Based on Wikipedia and Word2vec .pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92964a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(reader.pages)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7ea6365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 2nd IEEE International Conference on Computer and Communications \n",
      "Short Text Classification Based on Wikipedia and Word2vec \n",
      ". 1 C 1 W J 1 W x· ·2 Lm Wensen, ao Zewen, ang un, ang moy! \n",
      "1 Science and Technology on Information System Engineering Laboratory, National University \n",
      "of Defense Technology, Changsha, China \n",
      "2 College of Computer Science and Technology, Zhejiang University, Hangzhou, China e-mail: isssen@126.com \n",
      "Abstract-Different from long texts, the features of Chinese \n",
      "short texts is much sparse, which is the primary cause of the low accuracy in the classification \n",
      "of short texts by using \n",
      "traditional classification methods. In this paper, a novel method was proposed to tackle the problem by expanding \n",
      "the \n",
      "features of short text based on Wikipedia and Word2vec. Firstly, build \n",
      "the semantic relevant concept sets of Wikipedia. \n",
      "We get the articles that have high relevancy with Wikipedia concepts and use \n",
      "the word2vec tools to measure the semantic relatedness between \n",
      "target concepts and related concepts. And \n",
      "then we use the relevant concept sets to extend the short texts. \n",
      "Compared to traditional similarity measurement between concepts using statistical method, this method can get more \n",
      "accurate semantic relatedness. The experimental results show \n",
      "that by expanding the features of short texts, the classification accuracy can be improved. Specifically, \n",
      "our method appeared \n",
      "to be more effective. \n",
      "Keywords-short text; classification; wikipedia; Word2vec; semantic relatedness \n",
      "I. INTRODUCTION \n",
      "With the rapid development of Web2.0, netizens can exchange information on social network sites, such \n",
      "as \n",
      "Twitter, BBS, SNS, Weibo, and communicate with each other by the instant message applications such as MSN, Skype, QQ, WeChat. The widely used \n",
      "of those applications brings huge amount \n",
      "of short texts. Those short texts cover a wide range \n",
      "of topics, contain abundant information, which is important information source for governments and commercial organizations. Short texts classification is a fundamental task \n",
      "in short texts information processing. Short text classification is to categorize each short document \n",
      "in a document collection that without categorization information based on a known set \n",
      "of \n",
      "categories [1]. The traditional long texts classification methods can't work very well on short texts owing to its natural characters, such \n",
      "as data sparseness, immediacy, non-standardability [2]. Many researchers used external information to tackle the data sparseness problem and improved the classification accuracy. For example, Hu, Sun et al proposed a three tier architecture model based on Wikipedia and WordNet [3], made full use \n",
      "of the internal and external semantics to improve the accuracy for short texts clustering. M Sahami et \n",
      "al using the results returned by search engine to measure the similarity \n",
      "of short texts [ 4]; Liu Qun, \n",
      "Li Sujian et al extended the short texts based on \n",
      "978-l-4673-9026-2/16/$3l.00 ©20 16 IEEE 1195 WordNet and domain keywords to improve the classification performance [5]. In this paper, we propose a novel short text classification method based \n",
      "on Wikipedia and Word2vec to deal with data sparseness problem \n",
      "of massive short texts categorization. Firstly, based \n",
      "on the structure of Wikipedia and links information between Wikipedia articles, we get the article sets that has higher relevancy with the Wikipedia theme concepts. Secondly, we use the Word2vec tool to measure the semantic relatedness between target concepts and its' relevancy concepts, \n",
      "so we get the semantic relevant concept sets \n",
      "of Wikipedia. Thirdly, we extract the features of short text, extend the short text based on the features and the relatedness relevant concept sets ofWikipedia. Experimental results show that our method improves the classification performance. The rest \n",
      "of the paper is organized as follows. In section 2 \n",
      "we present the related work on short text classification. Section 3 shows the details \n",
      "of the classification method based on Wikipedia and Word2vec. Section 4 analyzes the experiments results. Finally, the paper is summarized in section \n",
      "5. \n",
      "II. RELATED WORK \n",
      "A. Short Text Classification Currently, the research on classification \n",
      "of long texts has been quite mature and the classification effect is good. Different from long texts, the features \n",
      "of short texts is much sparse, which makes the traditional text classification methods can't work well on short texts. Researchers try to improve the accuracy \n",
      "of classification by expanding the features \n",
      "of short texts. There are three common methods to extend features: based on topic model, based on the search engineer and based on the external Knowledge Base (KE). The feature expanding methods based on topic model extend the features by mining the latent topics \n",
      "of short text. The co-occurrence rate is improved after expanding the features belong to the latent topics, so the short classification accuracy is improved. In 2008, Phan et al mining the latent topics \n",
      "in large-scale data collections based on LDA mode [6]; Saurabh Kataria et al used the supervised topic models for Microblog classification [7]; The LDA topic model also help improving the performance \n",
      "of Chinese short text classification [8], [9]. The short text classification methods based on the search engineer is to measure the semantic similarity between features by leveraging web search results to provide greater \n"
     ]
    }
   ],
   "source": [
    "page = reader.pages[0]\n",
    "print(page.extract_text())\n",
    "parts = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "152d54bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfquery'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mv/8z6cl9lj4kl07_zpgwhqstlw0000gn/T/ipykernel_18126/1908001311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfquery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPaper/Short Text Classification Based on Wikipedia and Word2vec .pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfquery'"
     ]
    }
   ],
   "source": [
    "from pdfquery import PDFQuery\n",
    "\n",
    "pdf = PDFQuery('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPaper/Short Text Classification Based on Wikipedia and Word2vec .pdf')\n",
    "pdf.load()\n",
    "\n",
    "# Use CSS-like selectors to locate the elements\n",
    "text_elements = pdf.pq('LTTextLineHorizontal')\n",
    "\n",
    "# Extract the text from the elements\n",
    "text = [t.text for t in text_elements]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8aa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
