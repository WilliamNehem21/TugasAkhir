Array 15 (2022) 100212










Spatio-temporal aggregation of skeletal motion features for human motion prediction
Itsuki Ueda âˆ—, Hidehiko Shishido, Itaru Kitahara
University of Tsukuba, 1-1-1, Tennodai, Tsukuba, Ibaraki 305-8577, Japan


A R T I C L E  I N F O	A B S T R A C T

	

Keywords: Human motion Lie-algebra
Temporal aggregation Spatial aggregation Attention
This study proposes a human body motion prediction model that can adapt to the disorders in various human motion patterns and represent the kinematic constraints. In human motion prediction, the acquisition of features that capture inter-motion and inter-joint linkages is considered effective. To generate links that are adaptive to the reference time of dominant features and their crossing-over joints, we construct an attention- based network that aggregates motion sequences temporally and spatially. We evaluated the motion prediction results using the Human3.6M dataset with the indices of mean angle error and mean per joint position error and showed that our method outperforms other state-of-the-art methods.





Introduction

3D human motion prediction is a process that takes a numeric human body pose sequence as input and predicts the future pose sequence as shown in Fig. 1. Motion prediction is an essential tech- nology with many applications such as humanâ€“robot interaction [1], automated driving [2], pedestrian tracking [3]. Various input formats are possible for person motion prediction, including video [4], scene mesh [5], object information [6,7]. Especially motion capture informa- tion about a single person is widely used due to its high practicality in terms of sensing cost and accuracy. These motion predictions deal with numerical values of skeletal information obtained from motion captures such as joint positions and angles, as shown in Fig. 2. Since human motion involves many interlocking joints, the description of its characteristics requires high-dimensional information in both time and space. In addition, the prediction task is highly dynamic and nonlinear and inherently involves high-dimensional uncertainty as time passes. Therefore, there is a limit to the analytical acquisition of features required for motion prediction [8,9]. Recently, data-driven approaches such as [10â€“16] has become the mainstream. For example, in the case of walking, both hands and feet dominates in the same motion cycle, while the left and right hands and feet are linked in opposite phases. Thus, there is a strong bias in the continuity and regularity of natural human motion. Data-driven approaches have improved motion prediction performance by separating pose information into temporal and spatial axes and modeling feature aggregation with appropriate network structures, respectively [17]. In this paper, we propose a method to classify the features that have been generalized by the
conventional networks by temporal and spatial aggregation and to obtain high generalization performance by incorporating each aggre- gation model by adding appropriate information to the framework of attention [18]. We propose a method to achieve a high generalization performance within each aggregation model by adding appropriate information to the framework.
Temporal aggregation involves â€˜â€˜translation generalizationâ€™â€™ and â€˜â€˜time scale generalizationâ€™â€™ as shown in Fig. 3. The generalization of translation means acquiring features that do not depend on at which time in the input sequence a given action occurs. The gener- alization of time scales means acquiring features that do not depend on how long a behavior occurs and how fast the same behavior is performed. Previous methods adopt mainly four approaches, including convolutional neural network (CNN) [11], recurrent neural network (RNN) [10,13], discrete cosine transform (DCT) [12] and attention mechanism (Attention) [15,19]. CNN has excellent generalizability of temporal translation, but it is not easy to generalize the time scale because the kernel size fixes the reference time time-length. RNNs have the generalizability of time-scale in terms of the reference time-length stretch modeling. However, it belongs to the Markovian models, which is not suitable for translational generalization, due to the accumulation of errors when the dominant features of the motion appear in the forward part of the sequence. DCT can acquire features referring to the overall input time by transferring them to frequency space before processing them. The pose information transferred to the frequency space can be generalized for both translation and scale because the time shift and frequency are separated. However, DCT has difficulties


âˆ— Corresponding author.
E-mail address: ueda.itsuki@image.iit.tsukuba.ac.jp (I. Ueda).

https://doi.org/10.1016/j.array.2022.100212
Received 26 January 2022; Received in revised form 18 June 2022; Accepted 21 June 2022
Available online 23 June 2022
2590-0056/Â© 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).











/ig. 1. 3D motion prediction: The model takes the observation sequence on the left as input and outputs the prediction result sequence on the right. From top to bottom, the ground-truth value, the position-based and angle-based models of Traj-GCN [12], and the output of the proposed method are shown. The proposed method produces a pose sequence closer to the actual value.



with links and rotational joints. (Left) The offset ğ¥ğ‘– of each joint is set based on the /ig. 2. Examples of how to describe pose information. The human skeleton is modeled
ğ‘‡ pose. (Right) There are two ways to quantify the pose: a position-based description, such as the 3D position of joint ğ‘–, and an angle-based description, such as the rotation vector of joint angles, ğ«ğ‘– âˆˆ R3. There are two types of angle-based descriptions.



generalizing the translation because the time shift remains absolute due to the boundary condition in acyclic operation. Attention can provide feature descriptions referring to the entire time region by explaining the proximity between time points with Positional Encoding. Although Positional Encoding provides generalization for both translation and scale, it may be inferior to DCT for acquiring periodic motion features because it gives the proximity discontinuously for each time pair. In this method, we propose a new positional encoding in the attention architecture, which reproduces the operations in frequency space such as in DCT. This method provides frequency and time-shift information to Attentionâ€™s Query in the framework of relative position embedding (RPE). The model can acquire motion features with separate translation and time scales, which means it can interpret the same motion events with the same parameters when they appear at different times and speeds of occurrence.
Spatial aggregation critically involves generalizing motion features per joint and spatial dependencies between joints. Generalization of motion features per joint means acquiring independent features re- gardless of any joint. For example, the motion features observed in the right knee are also applicable to the left knee. Generalization of spatial dependence among joints means the acquisition of reference relationships among joints to provide each motion, for example, the rotation linkage of the shoulder, elbow, and wrist to produce a straight- line trajectory of the fingertips. Previous methods have focused on the expression of spatial dependency using network structures such as Graph Convolution and Graph Attention. Specifically, they generate a joint graph with each joint as a node, then detach the joint-wise features and the spatial dependencies across joints. For joint-wise fea- tures, they consider the feature dimensions retained by each node as
/ig. 3. Generalization of temporal features: (Left) The foot during walking is dominated by short-time features, while the center of gravity during a change of direction is dominated by long-time features. (Right) Features to be watched, such as a change in motion, are not always located behind the input.



identical and share the rotation matrix applied between joints. They also provide the importance that each node attaches to other nodes during aggregation, as adjacency matrices, to represent the spatial dependencies between joints.
The methods that use motion trees as adjacency matrices can de- scribe spatial proximity in link connection relationships, but they make it difficult to reference distant joints. Wei et al. [12] enables referencing of distant joints by learning the adjacency matrix. Due to the lack of information on the original connection relations of the motion tree, it was not sufficient for acquiring global features such as capturing the entire near joints of arms and legs. Li et al. [13] gives the closeness of joints to each other by constructing a multi-scale graph that integrates close joints. While wide-area features are easier to acquire, the choice of which joints to integrate relies on heuristics. The generalization of spatial dependence is still an unsolved problem.
We focus on the fact that the generalization issues are different between the two data description methods, position-based and angle- based, which have been evaluated separately as independent problems. Concretely, the position-based description can capture the features that represent a wide range of motions, such as the interlocking of distant joints in the kinematic tree due to the relative positional relationships of the joints. On the other hand, angle-based description has an ad- vantage in capturing features that represent local actions, such as local actions at a terminal joint, regardless of the posture of the root side in the kinematic tree. Therefore, we employ an approach in which the angle-based description gives the features held by each node of the joint graph, and the position-based description provides the spatial dependence during feature aggregation.
As shown in Fig. 6, we attempt to generalize both local and global motion features by building a predictor that handles angle-based fea- tures while incorporating position-based features suitable for represent- ing spatial dependencies. Since the rotation axis of each joint can vary from moment to moment as the human pose, Inverse Kinematics (IK), an algorithm that calculates the angle of each joint from the input posi- tion, requires the asymptotic computation of recursive equations. Since the inclusion of IK in the network degrades the prediction accuracy for actions far from the reference pose, adding a location-based description to the input does not directly improve accuracy. Therefore, previous methods use only angle-based or position-based descriptions for input and output, such as Fig. 5 (see Fig. 4).
The contribution of this work is to improve the performance of motion prediction by increasing the generalizability of motion feature acquisition, both in terms of temporal and spatial aggregation. For temporal aggregation, we introduce operations in frequency space in the framework of RPE to Self-Attention to achieve generalization in both translations of occurrence time and time width scale for the reference time of dominant motion features. For spatial aggregation, we derive position-based and angle-based pose descriptions that can be linearly mapped in the vicinity of the input final pose through the parameter space of Lie algebra, and assign position-based features to angle-based features by Cross-Attention, which allows us to both generalize the motion of individual joints and generalize the spatial





/ig. 4. (Left) IK is in general a nonlinear map. (Right) The Lie algebra associated with the tangent plane of the final pose can define a linear map between the position and the tangent plane.


/ig. 5. Example of a transformer that takes only angles as input and output: (Top) Pose described by angle is embedded into feature space. (Bottom) Output of the future
pose ğ‘‹ğ‘¡ described by angle, aggregated by the Transformer Decoder.



dependence between joints. We performed benchmark tests for motion prediction using the Human3.6M [20] and CMU-Mocap [21] datasets and showed that our model has superior performance compared to conventional methods.

Related works

Temporal aggregation

Modeling human motion prediction is difficult due to its high dimensionality, nonlinear dynamics, and the uncertainty of human motion. Analytical methods for short time forecasting have been pro- posed such as Gaussian process latent variable model [8], hidden Markov model [9], and random forest [9]. However, these methods have short applicability due to the modelsâ€™ limited complexity. Thus, the mainstream methods in recent years have shifted to using deep learning.
Previous research has mainly used four models, CNN, RNN, DCT, and Attention, as efficient ways to acquire motion features in time- series data processing by deep learning. Li et al. [11] built a Con- volutional Sequence-to-Sequence model using time series convolution. Since the range of spatial and temporal dependencies captured by this model is statically determined by the size of the convolutional filter,



































positions in the neighborhood of ğ‘‹ğ¿ , and generating a position-based description /ig. 6. Linking position and angle: (Top) Defining a mapping from joint angles to
with the necessary elements for the inverse mapping. (Middle) Feature extraction and transformation to angle-based features are represented by Transformer. (Bottom) Future poses are output by aggregating both position- and angle-derived features with the Transformer Decoder.



it cannot handle behaviors with different periods. Hence, regression- based methods, such as LSTM, are widely used because they can train a time-dependent range [13,22,23]. However, regression-based methods assume Markovianity, which reduces the robustness in the temporal direction. DCT transforms a time-discrete input sequence into a continuous sequence in the time axis and a discrete sequence in the frequency axis. DCT aggregates centrally in the temporal direc- tion and directly references distant times, which provides excellent generalization performance of the feature scale. On the other hand, the transformation from discrete values requires extrapolation, such as signal folding to form a periodic function. As a result, boundary condi- tions remain, and generalization of translation is challenging for acyclic transition behavior. Therefore, several works proposed a method using DCT to predict frequency space instead of pose space [12,14]. DCT aggregates centrally in the temporal direction and directly references distant times, which provides excellent generalization performance of the feature scale. On the other hand, the transformation from discrete values requires extrapolation, such as signal folding to form a periodic function. As a result, boundary conditions remain, and generalization of translation is challenging for acyclic transition behavior. Aksan et al. [15] proposed another approach that uses Attention to refer to features at distant times without distinction. In Attention, Positional Encoding assigns time proximity. The generalization performance for time translation of features is high, and the generalization performance to feature scales is also maintained due to the possibility of wide-area referencing. However, it is inferior to the method using DCT in periodic operation because the proximity exists for each time pair.



We attempt to reproduce the scale generalization performance of DCT with temporal attention by focusing on the frequency components dominant to the motion in the framework of Positional Encoding. There are two computational forms of Positional Encoding: absolute posi- tional encoding (APE) and relative positional encoding (RPE). Benyou et al. [24] has reported that the performance of BERT can be improved by using APE and RPE together. Wu et al. [25] proposed a multiplica- tive positional encoding as contextual RPE. In this method, we propose an RPE based on the contextual RPE [25] that can reproduce operations in frequency space, i.e., operations when using DCT, as a convolution in pose space.

Spatial aggregation

There are two essential aspects of spatial aggregation: generaliza- tion of the motion between individual joints, such as the typical motion of the right and left knees, and generalization of the spatial dependency between joints, such as the interlocking of the shoulder, elbow, and wrist, such as the smooth trajectory of the fingertips, which is indepen- dent of the posture of the whole body. In order to generalize these two characteristics, conventional methods focus on the representation of dependencies between joints by network structures. Graph Convolution and Graph Attention consider the motion tree a graph with each joint as a node and represent spatial dependencies by adjacency matrices. Li et al. [11] use the connection relation of the motion tree as the adjacency matrix of Graph Convolution. Graph Convolution can express spatial monotonicity such that the closer joints give more importance because it aggregates only the information of adjacent joints in each layer. On the other hand, it is difficult to refer to distant or strongly synchronized joints in the motion tree, such as the left and right wrists during hand-holding, despite their high spatial dependency. Wei et al. [12] introduce the adjacency matrix as a trainable parameter. They consider the connections between joints as complete graphs and describe the spatial dependence in edge weights, which enables dis- tance joint referencing. On the other hand, the original information of the connection relation is lost, such as which joints are adjacent to each other. Li et al. [13] constructs a multi-scale graph that integrates close joints to obtain global features such as focusing on the movement of the entire foot instead of independent joints such as the knee and ankle. While this method makes it easier to obtain global features by omitting similar joints, it relies on heuristics to select which joints to integrate. Emre et al. [15] uses Graph Attention to feed the adjacency matrix dynamically. In common with each method, the problem is generalizing the spatial dependence between joints.
We focus on generalizing spatial dependency by utilizing data structures instead of network structures. In general, networks utilize position-based or angle-based descriptions for data structures describ- ing input/output poses. The position-based description directly uses the 3D position of each joint acquired by motion capture [26], etc. The angle-based description determines a skeletal reference shape, such as T-pose or A-pose. It uses Euler angles and rotation vectors to describe the rotation of each joint from the reference shape.
In the position-based description, it is easy to obtain spatial con- straints on the end joints, such as the mutation of the ankle position into a zero vector when the axial foot stops during ground contact. The position-based description is also suitable for describing the rela- tionship between joints and selecting the joints to be gazed at from a global perspective. It can explicitly describe the positional relationship between joints distant from each other in the motion tree by using relative positions. On the other hand, the link length strongly affects the movement of the joint position, and the scale is different between the behavior of the hip joint and that of the toe. For example, even if the motion from the elbow to the toe is the same, the dynamic coordinate transformation depends on the shape of the root side, making it difficult to identify the motion features. Therefore, it is more difficult to obtain generalized features of individual joints and features of local joints in
comparison with angle-based descriptions. In addition, it is difficult to apply the constraint that the link length is invariant, such as the arm length does not change during the motion when the output is directly represented as a 3D position.
With angle-based description, it is easy to obtain local behavioral features such as gazing from the elbow to the tip of the elbow, regard- less of trunk behavior such as prone or crouching. Also, by describing the output as angle-based, fixed link length can be applied. The disad- vantage is that the link length information is lost from the data, making it difficult to describe the relationship between distant joints that are not directly connected. This makes it difficult to acquire global features and model spatial dependencies between joints dynamically. Wei et al. compared the performance of Res-Sup, ConvSeq2Seq, Traj-GCN, and Traj-GCN networks by using a position-based description for direct motion prediction and an angle-based description for transformation to each joint position after prediction. It is pointed out that the prediction performance of the angle-based description is inferior to that of the position-based description.
In the proposed method, features and spatial dependencies acquired from the position-based description are introduced into the intermedi- ate features of the motion prediction network using the angle-based description using Cross-Attention. The transformation from positional information to angular information, called inverse kinematics, requires recursive computation because the superposition of rotations cannot be decomposed. Since even a deep-learning-based predictor has a sig- nificant error, simply adding a position-based description to the input does not improve performance. We decompose the pose sequence into differences from the final input pose to isolate rough motion informa- tion, such as standing or walking, which facilitates the coordination of position and angles. In a similar approach, Liu et al. [16] proposed a joint trajectory space that can efficiently analyze motion context while preserving information on joint trajectories by decomposing the description of joint positions into the final pose and the velocity of each frame. We assume that the input and output poses are in the vicinity of the final input pose and achieve the linkage between position and angle via the Lie algebra around the rotation matrix of the final pose.

Our approach

Fig. 7 shows the entire proposed method. For input and output, we use the rotation of each joint in the form of a rotation matrix. As a pre- processing step, we convert the input to position-based and angle-based representations based on the final pose. Full connection and graph convolution gives the pose information as a feature vector for Self- Attention and provide the APE information. This network comprises a Transformer Encoder, which takes the position-based description as input and outputs the feature values, and a Transformer Decoder, which takes the angle-based description as input and performs forward prediction.
The transformer contains Temporal Attention for aggregation be- tween time points, Spatial Attention for aggregation between joints, Cross Attention for importing position-based features to the decoder side, and a Feed Forward network. The decoder outputs the data in rotation vectors in all coupling layers for each time and joint. The post-processing obtains the output pose sequence by finding the rota- tion matrix corresponding to the vector using the matrix exponential function and applying it to the final input pose.

Position-based and angle-based pose descriptions

In this method, position-based and angle-based features are ob- tained for spatial information aggregation to describe the coordination of distant joints and generalize each joint. The inverse kinematics prob- lem is converted from joint positions to angles, and it is difficult to solve the error even with recent deep learning techniques. In this study, we define a highly accurate mapping from the position description to the


The ğ‘Š can be defined to map to ğ‘†ğ‘‚(3) by the matrix exponential function. This means that we can compute the rotation matrix Mğ‘¡,ğ‘– for frame ğ‘¡, joint ğ‘– using the rotation vector Mğ‘¡,ğ‘– relative to the input final
frame as follows:
Mğ‘¡,ğ‘– = ğ‘’ğ‘¥ğ‘ ğ«ğ‘¡,ğ‘– âˆ§ Mğ¿,ğ‘–.	(3)
By using ğ‘…ğ‘¡ = [ğ«ğ‘¡,1, â€¦ , ğ«ğ‘¡,ğ‘ ]ğ‘‡ as the angle description, we can
quickly obtain the derivative for the angle of the joint position given
by the forward kinematics. The Lie algebraic parameter of rotation provides a stable property as the inputâ€“output space of the network because it avoids singularity and uniqueness problems such as gimbal lock, unlike angle descriptions such as Euler angles.
Next, we focus on the position-based description that can collabo-
the links are connected as 1, 2, â€¦ , ğ‘–, â€¦ , ğ‘— âˆ’ 1, ğ‘—, â€¦ , ğ‘ . The offset of link rate with the angle-based description. For simplicity, we assume that
ğ‘– at the reference pose is ğ¥ğ‘– âˆˆ R3, the rotation matrix from the reference pose is Mğ‘– âˆˆ R3Ã—3, the translation of the root is ğ¥0, and the rotation matrix M0 = I. The homogeneous coordinate transformation matrix ğ‘‡ğ‘– by link ğ‘– is shown in Eq. (4).
ğ‘‡ğ‘– = (Mğ‘–  ğ¥ğ‘–)	(4)

The angle of each joint provides the position ğ©(0) in the world coordinate system of joint ğ‘— by forward-kinematics in Eq. (2).

(ğ©
(0)
ğ‘—
= T0T1 â‹¯ T
â›0â
âœ âŸ


(5)

1
âœâ1âŸâ 
We derive the behavior when link ğ‘– is further rotated by ğ«ğ‘– in rotation vector notation based on M1, â€¦ , Mğ¾ . The rotation matrix of joint ğ‘– after
applying ğ«ğ‘– can be written as Mâ€² = ğ‘’ğ‘¥ğ‘([ğ«ğ‘–]âˆ§)Mğ‘–. The position ğ©(0)â€² of
ğ‘–	ğ‘—
joint ğ‘— after the transformation is as follows.

(ğ©
(0)â€²
ğ‘—
1
= T0T1 â‹¯
(ğ‘’ğ‘¥ğ‘
([ğ«ğ‘–]âˆ§)  0
0	1
Tğ‘– â‹¯
Tğ‘—âˆ’1
â›0â
âœ1âŸ


(6)



/ig. 7. Overall process diagram.
We consider a joint coordinate system ğ‘– whose coordinate transforma-
tion from the world coordinate system is given by Tâˆ’1 Tâˆ’1 â‹¯ Tâˆ’1 as the

ğ‘–âˆ’1 ğ‘–âˆ’2	0
coordinate system based on link ğ‘–. For the position ğ©(ğ‘–) of link ğ‘— in the joint coordinate system ğ‘–, the following is established from Eq. (4).

angle description and embed the necessary information in advance to
(ğ©(ğ‘–)) = Tâˆ’1 Tâˆ’1
Tâˆ’1 (ğ©(0))

avoid the accumulation of errors. Specifically, forward kinematics can provide a differentiable mapping from joint angles to positions using the offset and rotation matrices for each link in the reference pose. The rotation matrix takes a tangent plane at the reference pose and
ğ‘—
0
(ğ©(ğ‘–))
ğ‘–âˆ’1
= T T
ğ‘–âˆ’2 â‹¯ 0	ğ‘—
0
T	âœ âŸ
(7)





be interconverted by defining Jacobians with position descriptions. In this section, we first introduce the parameter notation of Lie algebra as the angle notation. Then, we show that the position change and


By assigning them toâ Eâ q. (6), we get the following:
(ğ©(ğ‘–)â€²) = (ğ‘’ğ‘¥ğ‘([ğ« ] )  0) (ğ©(ğ‘–))

described in terms of the local coordinate system of joint positions.
We denote the angular description of the pose at time ğ‘¡ as ğ‘‹ğ‘¡ =
In the neighborhood of M1, â€¦ , Mğ¾ âˆˆ ğ‘†ğ‘‚(3), a good approximation is
given by

[ğŒ
ğ‘¡,1
, â€¦ , ğŒ
ğ‘¡,ğ‘
]. Each ğŒ
ğ‘¡,ğ‘–
âˆˆ R3Ã—3 is an orthogonal matrix whose
ğ‘’ğ‘¥ğ‘([ğ«ğ‘–]âˆ§) âˆˆ ğ‘ ğ‘œ(3) using the source [ğ«ğ‘–]âˆ§ on the tangent plane.
The mapping between the source and the position description on the

determinant is 1, which is a member of the special orthogonal group
ğ‘†ğ‘‚(3). We consider the Lie algebra associated with the tangent plane in
tangent plane is as follows.

the input final frame Mğ¿,ğ‘–. The matrix ğ‘Š âˆˆ R3Ã—3 on the tangent plane
(ğ©(ğ‘–)â€²) = (I + [ğ« ]
0) (ğ©(ğ‘–))

is multiplied by the source of the Lie algebra [ğ«]âˆ§, represented by the
rotation vector ğ« = [ğ‘Ÿ1, ğ‘Ÿ2, ğ‘Ÿ3]ğ‘‡ , as follows.

(ğ‘–)â€²
ğ‘—
1
(ğ‘–)
ğ‘– âˆ§	ğ‘—
0	1	1
(ğ‘–)
(10)

B = Mğ¿,ğ‘– + [ğ«]âˆ§	(1)
ğ©ğ‘—  âˆ’ ğ©ğ‘—  = [ğ«ğ‘–]âˆ§ğ©ğ‘—	(11)
From the anti-commutativity of the Lie bracket product, the expression

[ğ«]
= â›âœ
0	âˆ’ğ‘Ÿ3
0
ğ‘Ÿ2 ââŸ

(2)
(11) is transformed as follows:

âœââˆ’ğ‘Ÿ2	ğ‘Ÿ1	0 âŸâ 
ğ©ğ‘—  âˆ’ ğ©ğ‘— = [ğ©ğ‘— ]âˆ§ğ«ğ‘–.	(12)

Similarly, when each joint is rotated by ğ«1, â€¦ , ğ«ğ¾ , the amount of change in position is related to the amount of change in angle as
follows:
ğâ€² âˆ’ ğ = (B âŠ— J)ğ‘	(13)


1
â‹®
âœğ©(0)âŸ
â›âœ M(0)[ğ©(0)]âˆ§	â‹¯	ğ‘€ (0)[ğ©(0)]âˆ§ ââŸ

(14)

J =	â‹®	â‹±	â‹®
âœâğ‘€ (0)[ğ©(ğ‘ )]âˆ§	â‹¯  ğ‘€ (0)[ğ©(ğ‘ )]âˆ§âŸâ 
(15)



R =  â‹® .	(16)
ğ«ğ¾
Note that B is the matrix representing the parentâ€“child relationship
matrix. The ğ½ can be computed from ğ‘ƒ using Eq. (2). Thus, by substi- between the joints, which in this example is the unit lower triangular
tuting ğ‘ƒ (0) for the position description at time ğ¿ in the reference pose






/ig. 8. (Top) Spatial Attention aggregates the information of different joints at the same time. (Bottom) Temporal Attention aggregates the information of the same joint at different times.

(0) ğ¿	â€²

ğ‘ƒ , ğ‘ƒğ‘¡  for the position description at time ğ‘¡ in the destination pose ğ‘ƒ ,
and ğ‘ğ‘¡ for the rotation ğ‘, we can associate the angle description with
Using the weights to be learned for each joint [Wğº, â€¦ , Wğº ], Wğº âˆˆ

the position description ğ‘ P âˆ’ P
= (B âŠ— J )ğ‘ .
ğ·Ã—ğ¶
0	ğ‘ Ã—ğ·
0	ğ‘	ğ‘–

ğ‘¡ ğ‘¡	ğ¿
ğ¿  ğ‘¡
R	, the middle layer output ğ¸ğ‘¡  âˆˆ R
with rotation applied is

(0)
(0)
calculated as follows.

â› ğ©ğ‘¡,1 â
â›ğ©ğ¿,1 â	â›	â

âœğ©(ğ‘ )âŸ
âœğ©(ğ‘ )âŸ
ğ¸ğ‘¡ =
(19)

â ğ‘¡,1 â 	â ğ¿,1 â 
Wğº G

and since the skeletal model is a tree structure with joint 1 as the root, we can transform B into a lower triangular matrix by exchanging
ğ‘¡
using the adjacency matrix ğ´ğ‘™ âˆˆ R
ğ‘ Ã—ğ‘
, the weights ğ‘Šğ‘™ âˆˆ R
ğ·Ã—ğ·
, and

columns. The inverse of the mapping by the lower triangular matrix can be easily represented in the network. By adding the information of J to
the activation function ğœ(â‹…) as follows:
Eğ‘™ = ğœ(Ağ‘™Eğ‘™âˆ’1Wğ‘™)	(20)

ğ‘¡	ğ‘¡
the encoderâ€™s input, the transformation to the angle can be embedded

in the network. This method uses Cross-Attention to combine angle- based features, so the rotation matrix is further separated at the encoder side, and the combination of joint position and joint coordinate system
Pğ‘¡ âˆˆ Rğ‘Ã—3ğ‘ is used as the input position-based description.
with edge weights instead of a kinematic tree, and ğ´ğ‘™ is the trainable Note that we assumed that the adjacency matrix is a complete graph
parameter for the reference of distant joints.

Attention

(0)	(ğ‘ ) ğ‘‡

â›âœ ğ©ğ‘¡,1	â‹¯  ğ©ğ‘¡,1 ââŸ

(18)

In this method, we use the Self-Attention mechanism to efficiently

ğ‘¡
(0)
ğ‘¡,ğ‘
(ğ‘ )
ğ‘¡,ğ‘
aggregate information from distant joints and times. The Cross-Attention mechanism incorporates position-based features into angle-based features. As shown in Fig. 8, we use two models for Self-

Embedding of human pose information
Pose-Embedding generates ğ·-dimensional feature vectors of poses
that can be aggregated by Self-Attention from pose sequences with position-based and angle-based descriptions transformed in Section 3.1.
Let the input and output pose sequences be the tensors of G âˆˆ
Attention: Spatial Attention, which aggregates the information of a different joint at each time, and Temporal Attention, which aggregates the information of a different joint at each time. Attention, which aggregates information for each joint at different times.
Spatial Attention is an independent process at each time. The in-
put/output to the layer at time ğ‘¡ is ğ¸ğ‘¡ = [ğğ‘¡,1, â€¦ , ğğ‘¡,ğ‘ ], ğ¸â€² = [ğâ€² , â€¦ , ğâ€²  ]

ğ‘¡	ğ‘¡,1
ğ‘¡,ğ‘

Rğ¿Ã—ğ‘ Ã—ğ¶ , E âˆˆ Rğ¿Ã—ğ‘ Ã—ğ· , respectively. C is the number of elements in each description, e.g., 3ğ¾ for the position-based description in Section 3.1
âˆˆ Rğ‘Ã—ğ·. To apply the Scale Dot-product Attention [18], we calculated Query Qğ‘¡, Key Kğ‘¡, and Value Vğ‘¡ from the linear transformation of ğ¸ğ‘¡

and 3 for the angle-based description. G is the number of elements in each description, e.g., 3ğ¾ in the position-based description and 3 in
by the learnable weights Wğ‘„
training target as follows:
ğ¾
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
ğ‘‰
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
âˆˆ Rğ·Ã—ğ· of the

axis for time, specifically, the feature values ğ ğ‘¡1,ğ‘–, ğ ğ‘¡2,ğ‘– at time ğ‘¡1, ğ‘¡2 the angle-based description. G is described by a common evaluation and joint ğ‘– are directly comparable. On the other hand, for space, the feature values ğ ğ‘¡,ğ‘–1, ğ ğ‘¡,ğ‘–2 for time ğ‘¡ and joints ğ‘–, ğ‘— are taken to different
evaluation axes. For example, the elbow joint and the knee joint have
ğ‘„
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
ğ¾
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
ğ‘‰
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
,	(21)
,	(22)
.	(23)

two axes of motion: flexionâ€“extension in the offset vertical direction and adductionâ€“abduction in the offset direction. They have similar ranges of motion, but the directions of the axes of motion are different.
Therefore, the input ğºğ‘¡ âˆˆ Rğ‘ Ã— ğ¶ at each time is rotated for each joint,
output    the    feature    vector    ğ¸ğ‘¡    âˆˆ   Rğ‘Ã—ğ·. and then static graph convolution is performed on the joint graph to
To ensure that the tight joints of ğ‘„, ğ¾ are strongly referenced, we compute the Attention map Ağ‘¡ âˆˆ Rğ‘Ã—ğ‘ as follows:
Ağ‘¡ = ğœ(Qğ‘¡Kğ‘‡ ).	(24)
Note that ğœ(â‹…) is the activation function, which generally uses a
normalization such that the sum of the directions is one after applying

the ReLU or exponential function. By aggregating ğ‘‰ğ‘¡ according to the Attention map and projecting it to the representation space with

ğ‘‚
ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
Eâ€² = Ağ‘¡Vğ‘¡Wğ‘‚
âˆˆ Rğ·Ã—ğ·, the output ğ¸â€² is calculated as follows:
.	(25)

ğ‘¡	ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™
Eq. (25) gives the interpretation that the adjacency matrix is a dynam- ically acquired graph convolution.
joint. The input/output to the layer at joint ğ‘– is as follows: Temporal Attention is similarly an independent process for each
Eğ‘– = [ğğ‘–,1, â€¦ , ğğ‘–,ğ¿], Eâ€² = [ğâ€² , â€¦ , ğâ€² ] âˆˆ Rğ¿Ã—ğ·.	(26)

ğ‘–	ğ‘–,1	ğ‘–,ğ¿
Using the weights Wğ‘„	, Wğ¾	, Wğ‘‰
, Wğ‘‚	âˆˆ Rğ·Ã—ğ·, the tempo-

ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™	ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™	ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™	ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™

ral aggregation at joint ğ‘– is calculated as follows:
ğ‘„
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
ğ¾
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
ğ‘‰
ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
(27)
(28)
(29)

Ağ‘– = ğœ(Qğ‘–Kğ‘‡ + M)	(30)

Eâ€² = ğ´ğ‘–ğ‘‰ğ‘–Wğ‘‚
(31)

ğ‘–	ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™
Note that Qğ‘–, Kğ‘–, Vğ‘– âˆˆ Rğ¿Ã—ğ·, M, Ağ‘– âˆˆ Rğ¿Ã—ğ¿. The ğ‘€ is a mask to avoid referring to the future during inference, which is ğ‘‚ for the encoder and
âˆ’âˆ for the upper triangular component for the decoder.

Embedding time and joint proximity


The Attention mechanism aggregates all of the time and joint infor- mation in the input indistinctly. For this purpose, the proximity of time and joint is assigned to the feature vector by Positional Encoding. Our method uses Absolutely Positional Encoding (APE) to obtain features that depend on absolute time and specific joints and Relative Positional Encoding (RPE) to obtain generalized features that do not distinguish the timing of occurrence.
APE assigns proximity to the feature vectors generated in Sec-







/ig. 9. RPE gives the product of query and cosine when computing Temporal Attention.

system calculated by forward kinematics is [ğ©(0), â€¦ , ğ©(0) ], and their

ğ‘¡,1
ğ‘¡,ğ‘

tion 3.2 just before the networkâ€™s input. Proximity on the time axis
respective true values are [Mğºğ‘‡ , â€¦ , Mğºğ‘‡ ][ğ©ğºğ‘‡ , â€¦ , ğ©ğºğ‘‡ ]. The angular

ğ‘ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ = [ğ‘§ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™] âˆˆ R(ğ¿+ğ‘†)Ã—ğ·, proximity on the spatial axis ğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ =
error ğœƒ
for joint ğ‘–
ğ‘¡,1
ğ‘¡,ğ‘
ğ‘¡,1
ğ‘¡,ğ‘

ğ‘¡,ğ‘
ğ‘¡,ğ‘–
is defined by the rotation angle of the rotation matrix

[ğ‘§ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™] âˆˆ Rğ‘Ã—ğ·, the feature of the pose embedding at time ğ‘¡, joint
Mğ‘‡ Mğºğ‘‡ , which is the residual. The rotation angle is given by a variant

ğ‘–,ğ‘
ğ‘¡,ğ‘–  ğ‘¡,ğ‘–

ğ‘–, and feature dimension ğ‘ is ğ‘’ğ‘™
assignment is as follows:


, then the value ğ‘’ğ‘¡,ğ‘–,ğ‘ after APE
of Rodriguezâ€™s formula as follows
( tr(Mğ‘‡ Mğºğ‘‡ ) âˆ’ 1 )
	


parameters ğ‘ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™, ğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ to ensure monotonicity over a wide area. In this method, we use a fully learnable APE with the learnable
The overall angle error ğ¿ğ‘Ÿğ‘œğ‘¡ is as follows.
âˆšâˆš

 	

feature dimension ğ‘, if the proximity between times ğ‘¡ and ğ‘¡
is ğ‘§ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’,
ğ‘¡=ğ¿+1	ğ‘–=1

1	2
then Eq. (24) can be replaced by the following equation
ğ‘¡1 âˆ’ğ‘¡2 ,ğ‘
The position error ğ¿
ğ‘ğ‘œğ‘ 
summarizes the Euclidean distance of the posi-

Ağ‘¡ = ğœ(Qğ‘¡Kğ‘‡ + Qğ‘¡Zğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’)	(33)
tion for each joint as follows:

ğ‘¡	ğ‘¡
ğ¿+ğ‘†  ğ‘

â› ğ‘§ğ‘¡âˆ’1,1	â‹¯	ğ‘§ğ‘¡âˆ’ğ¿,1 â
Lğ‘ğ‘œğ‘  =
âˆ‘ âˆ‘ |ğ©(0) âˆ’ ğ©ğºğ‘‡ |	(38)

Zğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ =
ğ‘¡	âœ
â‹®	â‹±	â‹®
(34)
ğ‘¡=ğ¿+1 ğ‘–=1
ğ‘¡,ğ‘
ğ‘¡,ğ‘




ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’
ğ‘¡1 âˆ’ğ‘¡2 ,ğ‘–,ğ‘
as follows:
Datasets

ğ‘§ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ = ğ‘ğ‘œğ‘ (  ğ‘¡1 âˆ’ ğ‘¡2   )	(35)
We verify the performance of the model on the motion capture

ğ‘¡1 âˆ’ğ‘¡2 ,ğ‘–,ğ‘
Training
100002ğ‘âˆ•ğ·
dataset Human3.6M [20]. The Human3.6M dataset contains 15 dif-
contains the Euler angles of each joint concerning the ğ‘‡ pose for 32 ferent classes of motion captured by seven subjects. Each sequence

In the training, we use ğ¿ğ‘Ÿğ‘œğ‘¡ + ğœ†ğ¿ğ‘ğ‘œğ‘  as the objective function, which is a composite of the angle error ğ¿ğ‘Ÿğ‘œğ‘¡ and the position error ğ¿ğ‘ğ‘œğ‘  with the hyperparameter ğœ† âˆˆ R+. The output for time ğ‘¡(ğ¿ + 1 â‰¤ ğ‘¡ â‰¤ ğ¿ + ğ‘†) is
ğ‘Œğ‘¡âˆ’ğ¿ = Mğ‘¡,1, â€¦ , Mğ‘¡,ğ‘ , and the joint position in the world coordinate
joints at 50 fps. We applied downsampling method from 50 fps to 25 fps for all sequences for fairness. We used the data of six subjects to train the model and tested it on the movement class of another subject. The test data traditionally used eight randomly selected sequences for each

Evaluation results of position error in H3.6M: â†“ MPJPE [mm].
Scenarios	Walking	Eating	Smoking	Discussion


movement class. However, the number of evaluation sequences was small, and the chosen sequences were biased toward easily predictable ones. Lingwei et al. [14] pointed out the inaccuracy of the evaluation and evaluated the entire sequence for each action class. We inherit this benchmark and evaluate it using the entire test data.
We also evaluate the same on the CMU-Mocap dataset [21]. For the sake of fairness, we use the same data as in Res-sup [10] with the anthropometric differences removed and validate it on eight classes of test cases.

Evaluations

Following the standard evaluation metrics used in Res-sup [10], the Mean Per Joint Position Error (MPJPE), which describes the av- erage distance from the ground-truth value of each joint position in millimeters, is used to evaluate the results. The prediction times were compared for 80, 160, 320, and 400 [ms] as in the previous study. We measured the MPJPE for the output angle converted to the position by forward-kinematics. As a baseline, we compared the results of our method with those of four recent methods: Res-sup, Traj- GCN, DMGNN, and MSR-GCN. We used the results reported in each paper for each error directly. Since DMGNN [13] did not report the
error of 3D positions, only the results of angular error were compared. For Human3.6M, we conducted comparative experiments on whether it has Encoder and RPE modules and uses position-based and angle- based descriptions. We omitted the cross-attachment of the decoder and used only the self-attachment for the angle descriptions for the experiments without the encoder. For the experiments without RPE, we configured each Temporal Attention as a regular self-attention.
We replaced the decoderâ€™s initial input ğ‘…1, â€¦ , ğ‘…ğ¿ with a single zero
vector for the only position-based description experiment. For the only
with angle informations ğ‘…1, â€¦ , ğ‘…ğ¿ instead of positions ğ‘ƒ1, â€¦ , ğ‘ƒğ¿. We rotation-based description experiment, we replaced the encoderâ€™s input
implemented the architecture of our method using Pytorch [27], and
learning rate is ğ‘™ğ‘Ÿ = 0.0001, ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ğ‘™ ğ‘Ÿ = 1.0, ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦ = 0.0005, trained the model using AdaBound [28] for the gradient method. The
mini-batch is trained with random extraction of batch size 256, input sequence length is 32 frames, and output sequence length is 10 frames.

Results

Table 1 shows the experimental results of the position error in Human3.6M [20]. In Human3.6M, all the motion classes outperformed the conventional method in prediction below 320 [ms]. In ablation,


Table 2
Evaluation results of position error in CMU-Mocap: â†“ MPJPE [mm].



compared to the case without encoders or position-based input, there is a remarkable improvement in the whole-body movement classes such as Discussion, Sitting, SittingDown, and WalkingTogether. We considered that the position-based description, directly referring to the positional relationship of distant joints, worked effectively.
On the other hand, there is no significant improvement in the long-time prediction of 400 [ms]. The association between position and angle is assumed to be near the last input pose as the reference pose. This may have made it challenging to integrate the position- based features into the angle-based features when the movement is significant. In addition, the performance of short-time prediction is particularly significant for motion classes that include many periodic motions, such as Walking, Directions, Greeting, and Phoning. The con- siderable improvement compared to the experimental results without RPE suggests that the dominant frequency component enhancement by RPE plays a role similar to that of DCT and improves the prediction performance for periodic motions.
In the ablation of data description, the performance of long-term prediction without position-based and short-term prediction without angle-based are lower than our original. We interpret this result that the position-based description affects long-time prediction since it detects global features of the whole body. In contrast, the angle-based involves short-time prediction since it detects local features of each joint unit.
Table 2 shows the experimental results of position error in CMU- Mocap [21]. Similar to Human3.6M, we can find an improvement in the performance of CMU-Mocap for predictions below 320 [ms]. In ad- dition, the improvement is more significant in the motion classes with complex motion patterns near the final input pose, such as basketball. We believe that this method extracts effective motion characteristics because it can refer to both position-based and angle-based features.

5. Discussion and future works

This paper proposes a method for extensive temporal and spatial information aggregation using Self-Attention. For temporal aggrega- tion, the dominant frequencies are emphasized in the framework of relative positional encoding and used together with absolute positional encoding to achieve aggregation that incorporates the characteristics of conventional models using DCT. For spatial aggregation, we focus on the fact that position-based and angle-based descriptions are suitable for global and local feature extraction, respectively, and constructed a structure that incorporates the position-based features extracted by the encoder into the angle features by Cross-Attention. We also show that a linear mapping can be defined between the parameter space of Lie algebra and the position space of the joint coordinate system. We introduce a position- and angle-based description in the neigh- borhood coordinates of the final input pose. At the same time, we realized the expression of conditions such as fixation and contact of end joints by using the position as input and the expression of
constraints such as connection relation and link length invariance by using angle as output. By selecting the appropriate architecture for ex- tracting the features of position and angle, we create an inference that can predict with high accuracy even for non-periodic and difficult-to- generalize motion classes. The results show that our model outperforms the state-of-the-art methods in short-term prediction.

CRediT authorship contribution statement

Itsuki Ueda: Conceptualization, Methodology, Software, Val- idation, Formal analysis, Investigation, Data curation, Writing â€“ original draft, Writing â€“ review and editing, Visualization. Hidehiko Shishido: Supervision. Itaru Kitahara: Conceptualization, Methodology, Resources, Writing â€“ review and editing, Supervision, Project administration, Funding acquisition.

Declaration of competing interest

One or more of the authors of this paper have disclosed potential or pertinent conflicts of interest, which may include receipt of payment, either direct or indirect, institutional support, or association with an entity in the biomedical field which may be perceived to have potential conflict of interest with this work. Itsuki Ueda reports a relationship with Preferred Networks Inc that includes: employment.

Acknowledgments

This work was supported by JSPS KAKENHI Grant Number JP19H00806, JP21KK0070 and JP22H01580.

References

Akhter I, Sheikh Y, Khan S, Kanade T. Nonrigid structure from motion in trajectory space. In: Proceedings of the neural information processing systems (NeurIPS); 2009.
Paden B, ÄŒÃ¡p M, Yong SZ, Yershov D, Frazzoli E. A survey of motion planning and control techniques for self-driving urban vehicles. IEEE Trans Intell Veh 2016;1(1):33â€“55.
Gong H, Sim J, Likhachev M, Shi J. Multi-hypothesis motion planning for visual object tracking. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV). IEEE; 2011, p. 619â€“26.
Zhang JY, Felsen P, Kanazawa A, Malik J. Predicting 3D Human Dynamics from Video. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2019.
Wang J, Xu H, Xu J, Liu S, Wang X. Synthesizing long-term 3d human motion and interaction in 3d scenes. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR); 2021, p. 9401â€“9411.
Adeli V, Ehsanpour M, Reid I, Niebles JC, Savarese S, Adeli E, Rezatofighi H. Tripod: Human trajectory and pose dynamics forecasting in the wild. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2021, p. 13390â€“13400.



Corona E, Pumarola A, Alenya G, Moreno-Noguer F. Context-aware human motion prediction, In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR); 2020, p. 6992â€“7001.
Wang JM, Fleet DJ, Hertzmann A. Gaussian process dynamical models. In: Proceedings of the neural information processing systems (NeurIPS), vol. 18. Citeseer; 2005, p. 3.
Lehrmann AM, Gehler PV, Nowozin S. Efficient nonlinear markov models for human motion. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2014, p. 1314â€“1321.
Martinez J, Black MJ, Romero J. On human motion prediction using recurrent neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2017, p. 2891â€“2900.
Li C, Zhang Z, Lee WS, Lee GH. Convolutional sequence to sequence model for human dynamics. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2018, p. 5226â€“5234.
Mao W, Liu M, Salzmann M, Li H. Learning trajectory dependencies for human motion prediction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV); 2019.
Li M, Chen S, Zhao Y, Zhang Y, Wang Y, Tia Q. Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR); 2020.
Dang L, Nie Y, Long C, Zhang Q, Li G. MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2021, p. 11467â€“11476.
Aksan E, Kaufmann M, Cao P, Hilliges O. A Spatio-temporal Transformer for 3D Human Motion Prediction. In: Proceedings of the international conference on 3D vision (3DV); 2021.
Liu Z, Su P, Wu S, Shen X, Chen H, Hao Y, Wang M. Motion Prediction Using Trajectory Cues. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2021.
Aksan E, Kaufmann M, Hilliges O. Structured Prediction Helps 3D Human Motion Modelling. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2019.
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I. Attention is all you need. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R, editors. Proceedings of the neural information processing systems (NeurIPS), vol. 30. Curran Associates, Inc.; 2017.
Wang J, Xu H, Narasimhan M, Wang X. Multi-person 3D motion prediction with multi-range transformers. In: Advances in neural information processing systems (NeurIPS), vol. 34. 2021.
Ionescu C, Papava D, Olaru V, Sminchisescu C. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Trans Pattern Anal Mach Intell 2014.
CMU. Carnegie-mellon mocap database. 2003, http://mocap.cs.cmu.edu.
Fragkiadaki K, Levine S, Felsen P, Malik J. Recurrent network models for human dynamics. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2015, p. 4346â€“4354.
Jain A, Zamir AR, Savarese S, Saxena A. Structural-rnn: Deep learning on spatio- temporal graphs. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2016, p. 5308â€“5317.
Wang B, Shang L, Lioma C, Jiang X, Yang H, Liu Q, Simonsen JG. On Position Embeddings in BERT. In: Proceedings of the international conference on learning representations (ICLR); 2021.
Wu K, Peng H, Chen M, Fu J, Chao H. Rethinking and Improving Relative Position Encoding for Vision Transformer. In: Proceedings of the IEEE/CVF international conference on computer vision (ICCV); 2021, p. 10033â€“10041.
Horiuchi Y, Makino Y, Shinoda H. Computational Foresight: Forecasting Human Body Motion in Real-time for Reducing Delays in Interactive System. In: Proceedings of the 2017 ACM international conference on interactive surfaces and spaces; 2017.
Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. PyTorch: An imperative style, high-performance deep learning library. In: Advances in neural information processing systems (NeurIPS). Curran Associates, Inc.; 2019,
p. 8024â€“35, URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative- style-high-performance-deep-learning-library.pdf.
Luo L, Xiong Y, Liu Y, Sun X. Adaptive Gradient Methods with Dynamic Bound of Learning Rate. In: Proceedings of the 7th International Conference on Learning Representations (ICLR); 2019, New Orleans, Louisiana.
