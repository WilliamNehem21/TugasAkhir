Array 15 (2022) 100232










An improved probability propagation algorithm for density peak clustering based on natural nearest neighborhood
Wendi Zuo a, Xinmin Hou a,b,c,âˆ—
a School of Data Science, University of Science and Technology of China, Hefei, Anhui 230026, China
b School of Mathematical Sciences, University of Science and Technology of China, Hefei, Anhui 230026, China
c CAS Key Laboratory of Wu Wen-Tsun Mathematics, University of Science and Technology of China, Hefei, Anhui 230026, China


A R T I C L E  I N F O	A B S T R A C T

	

Keywords: Clustering Density peaks
Natural nearest neighborhood Probability propagation
Clustering by fast search and find of density peaks (DPC) (Since, 2014) has been proven to be a promising clustering approach that efficiently discovers the centers of clusters by finding the density peaks. The accuracy
of DPC depends on the cutoff distance (ğ‘‘ğ‘ ), the cluster number (ğ‘˜) and the selection of the centers of clusters.
Moreover, the final allocation strategy is sensitive and has poor fault tolerance. The shortcomings above
make the algorithm sensitive to parameters and only applicable for some specific datasets. To overcome the limitations of DPC, this paper presents an improved probability propagation algorithm for density peak clustering based on the natural nearest neighborhood (DPC-PPNNN). By introducing the idea of natural nearest neighborhood and probability propagation, DPC-PPNNN realizes the nonparametric clustering process and makes the algorithm applicable for more complex datasets. In experiments on several datasets, DPC-PPNNN is shown to outperform DPC, K-means and DBSCAN.





Introduction

Clustering, also known as unsupervised classification, aims to divide datasets into subsets or clusters according to the similarity measure of the data sample (physical or abstract) such that the data samples within the subset or cluster have a high degree of similarity and that the data samples belonging to different subsets or clusters have a high degree of dissimilarity [1]. Currently, cluster analysis plays an important role in many fields such as social sciences, biology, pattern recognition, information retrieval and so on [2]. It is so useful in machine learning and data mining that many researchers have paid much attention to it. Over the past few decades, a number of excellent clustering algorithms have been developed for different types of applications. Typical algorithms include K-means [3] and K-medoids [4] based on partitioning, CURE [5] and BIRCH [6] based on hierarchy, DBSCAN [7] and OPTICS [8] based on density, WaveCluster [9] and STING [10] based on grids and statistical clustering [11] based on models.
In 2014, a density-based clustering algorithm DPC was given by Rodriguez and Laio [12] (Clustering by fast search and find of density peaks, Science, 344 (2014) 1492). The main idea of the algorithm DPC
is as follows: For each data point ğ‘–, we compute two quantities: its
local density ğœŒğ‘– and its distance ğ›¿ğ‘– from points of higher density. By mapping all the data points to the decision graph which takes ğœŒ and ğ›¿
as the two axes, we can recognize density peak points (cluster centers)
as points for which the values of ğœŒğ‘– and ğ›¿ğ‘– are anomalously large. After
the cluster centers have been found, each remaining point is assigned
to the same cluster as its nearest neighbor of higher density. The DPC algorithm is simple and efficient, and it can quickly find the high density peak points (cluster centers) without iteratively calculating the objective function. Moreover, it is suitable for cluster analysis on large- scale data. Although the DPC algorithm has obvious advantages over other clustering algorithms, it also has some shortcomings: the accuracy
of DPC depends on the cutoff distance (ğ‘‘ğ‘ ), the cluster number (ğ‘˜) and
the selection of the centers of clusters. Moreover, the final allocation
strategy is sensitive and has poor fault tolerance. Last, the algorithm has its basis on the assumptions that cluster centers are surrounded by neighbors with lower local density and that they are at a relatively large distance from any point with higher local density.
To avoid the deficiencies of DPC, in this paper, we present an improved probability propagation algorithm for density peak clustering based on natural nearest neighborhood (DPC-PPNNN). By introducing the concept of natural nearest neighborhood, we can avoid setting
ğ‘‘ğ‘ manually. By calculating ğ›¾ = ğœŒ Ã— ğ›¿ and ğœƒ =  ğœŒ , we can select
cluster centers automatically. Finally, the clustering algorithm based
on probability propagation can help us allocate all the remaining data points. By doing all these, DPC-PPNNN is suitable for more complex datasets and can distinguish two clusters that are close to each other.


âˆ— Corresponding author at: School of Data Science, University of Science and Technology of China, Hefei, Anhui 230026, China.
E-mail addresses: zuowendi@mail.ustc.edu.cn (W. Zuo), xmhou@ustc.edu.cn (X. Hou).

https://doi.org/10.1016/j.array.2022.100232 Received 10 July 2022; Accepted 12 July 2022
Available online 18 July 2022
2590-0056/Â© 2022 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).



The rest of this paper is organized as follows. In Section 2, we intro- duce the research progress related to the DPC algorithm. In Section 3, we briefly review the basic definitions and processes of the traditional DPC algorithm and reveal some problems within them. In Section 4.1, we introduce the basic concept and the algorithm of the natural nearest neighborhood. In Section 4.2, we show the improvement on the local density and the method of selecting cluster centers automatically. In Section 4.3, we propose the allocation algorithm based on probability propagation. In Section 5, we compare the DPC-PPNNN algorithm with other classical clustering algorithms using various synthetic and real-world datasets. In Section 6, we summarize the advantages and disadvantages of the DPC-PPNNN algorithm and point out the direction of our future research.
DPC algorithm and analysis

DPC algorithm

The DPC algorithm presented by Rodriguez and Laio [12] has its ba- sis on the assumption that cluster centers are surrounded by neighbors with lower local density and that they are at a relatively larger distance from any point with higher local density. There are two quantities to
describe each data point ğ‘–: its local density ğœŒğ‘– and its distance ğ›¿ğ‘– from
points of higher density.
density for a data point ğ‘–: the cutoff distance method and the kernel The DPC algorithm provides two methods for calculating the local distance method. For a data point ğ‘–, its local density ğœŒğ‘– is defined as



Related works
ğœŒğ‘– =
âˆ‘
ğ‘–â‰ ğ‘—
ğœ’ (ğ‘‘ğ‘–ğ‘— âˆ’ ğ‘‘ğ‘ ),  ğœ’ (ğ‘¥) =
{1,  ğ‘¥ < 0
(1)
0,  ğ‘¥ > 0

with the cutoff distance method, or

DPC has been proven to be an effective clustering strategy but it also has many limitations such as the arbitrary selection of density estimation metrics, clustering centers and the risk of error propagation.
ğœŒğ‘– =
âˆ‘
ğ‘–â‰ ğ‘—
exp
[ (	)2]
â€“	ğ‘–ğ‘—
ğ‘‘ğ‘
(2)

Over the past few years, many optimized variants of DPC have been proposed considering the following aspects:
The first aspect is improving the density measure of the DPC al- gorithm. Du, Ding and Jia [13] proposed DPC-KNN, which introduces the concept of K-nearest neighbors (KNN) to DPC and provides another option for computing the local density. They also employ PCA to reduce the dimensionality of data. However, the method still suffers from the limitations of DPC because it applies the same procedure in determining
the cluster centers and assigning the rest of the points. Liu, Wang, and
with the kernel distance method, where ğ‘‘ğ‘–ğ‘— is the Euclidean distance
between data points ğ‘– and ğ‘—, ğ‘‘ğ‘ (> 0), the cutoff distance, is the radius
the local density ğœŒğ‘– is positively correlated with the number of points of a point for scanning its neighborhood, which is set by the user. Thus, with a distance from ğ‘– less than ğ‘‘ğ‘ . The most obvious difference be- tween the two methods is that for Eq. (1), ğœŒğ‘– is a discrete value, whereas for Eq. (2), it is a continuous value. However, for both methods, ğœŒğ‘– is sensitive to ğ‘‘ğ‘ .
Subsequently, ğ›¿ğ‘– in DPC is defined as

Yu [2] proposed SNN-DPC, which estimates the local density based on the idea of shared nearest neighbor similarity. However, it still
ğ›¿ = min
ğ‘—âˆ¶ğœŒğ‘— >ğœŒğ‘–
(ğ‘‘ )	(3)

suffers from the limitation of manually selecting the number of cluster centers. Mehmood et al. [14] proposed CFSFDP-HD, which applies a nonparametric density estimator (Kernel Density Estimation, KDE) to
eliminate the reliance of DPC on the cut-off distance ğ‘‘ğ‘ .
The second aspect is to automatically recognize the numbers of clusters and cluster centers. Liang and Chen [15] proposed 3DC, which introduces a divide-and-conquer strategy to determine the ideal number of clusters. However, it ignores the local structure of the datasets which may cause missing clusters. Xu, Wang and Deng [16] proposed DenPEHC, which could automatically detect all possible centers and build a hierarchy presentation for the dataset. Nevertheless, both 3DC and DenPEHC will aggravate the propagation of errors due to the hier- archical clustering strategy. Li, Ge, and Su [17] proposed an automatic clustering algorithm for determining the density of clustering centers. In this algorithm, it is considered that if the shortest distance between a potential cluster center and a known cluster center is less than the
cutoff distance ğ‘‘ğ‘ , then the potential cluster center is a redundant
center. Otherwise, it is regarded as the actual center of another cluster.
The third aspect is to improve the assignment strategy to reduce error propagation. In most of the DPC variants, the idea of K-nearest neighbors is hybridized in the aggregation strategies. For instance, Zhou et al. [18] constructed the veins of clusters by connecting pairs with the highest similarity from the high-density regions to the cluster borders. The rest of the points are then assigned to the nearest veins. Xie et al. [19] proposed a two-step procedure for label propagation. The first strategy assigns non-outliers based on the search of the K- nearest neighbors starting from the density peaks. The second strategy assigns the other points using the fuzzy KNN technique. Geng et al. [20] proposed a KNN graph-based label propagation strategy to assign the remaining points. Liu, Wang and Yu [2] introduced a two-step alloca- tion method based on inevitably and possibly subordinate allocation of the noncenter points.
For the data point ğ‘– that has the highest local density ğœŒğ‘–, ğ›¿ğ‘– is conven-
tionally defined as
ğ›¿ğ‘– = max (ğ‘‘ğ‘–ğ‘— )	(4)
As shown in Eq. (3), ğ›¿ğ‘– is the minimum distance between point ğ‘– and points ğ‘— with local densities ğœŒğ‘— > ğœŒğ‘–.
After calculating the two quantities: ğœŒ and ğ›¿, we can recognize
of ğœŒğ‘– and ğ›¿ğ‘– are anomalously large by mapping all the data points to density peak points (cluster centers) as points for which the values the decision graph which takes ğœŒ and ğ›¿ as the two axes. However,
sometimes we cannot select cluster centers from the decision graph correctly because they are too close to each other. Instead, we can sort
all the data points by ğ›¾ defined in Eq. (5) and choose the largest ğ‘˜ (the
cluster number) data points as the cluster centers.
ğ›¾ğ‘– = ğœŒğ‘– Ã— ğ›¿ğ‘–	(5)
Since the cluster centers have been found, each remaining point is assigned to the same cluster as its nearest neighbor of higher density.

Analysis

Although the experimental results obtained with DPC have shown that DPC performs well in many instances, the following drawbacks are obvious.
distance (ğ‘‘ğ‘ ). As shown in Fig. 1, there is a two-moon dataset. The red First, the accuracy of DPC depends on the setting of the cutoff
pictures in this article. When we change the value of ğ‘‘ğ‘ , the clustering points are the cluster centers we select and the same for all the other
result of DPC can be greatly affected even for this simple dataset.
Second, the cluster number (ğ‘˜) is difficult to determine. Actually,
time, not to mention choosing an ideal cluster number (ğ‘˜) for DPC. we have little idea about the distribution of the dataset most of the
Fig. 2 shows the results of the traditional DPC algorithm on the Donut3




/ig. 1. Results of the traditional DPC algorithm on the two-moon dataset. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)


/ig. 2. Results of the traditional DPC algorithm on the Donut3 dataset.





/ig. 3. Results of the traditional DPC algorithm on the dataset with two Gaussian clusters with very different densities.



dataset. In Fig. 2(a), we recognize the outer ring as noise and let the cluster number be 2. However, clearly, the result cannot satisfy us. In Fig. 2(b), we recognize the outer ring as a cluster and let the cluster number be 3. Even when we choose the correct cluster center in the outer ring, we cannot obtain the ideal result.
Third, it is difficult to recognize the centers of clusters. As shown
setting ğ‘‘ğ‘ manually. By calculating ğ›¾ = ğœŒ Ã— ğ›¿ and ğœƒ = ğœŒ , we can introducing the concept of natural nearest neighbors, we can avoid
select cluster centers automatically. Finally, the clustering algorithm based on probability propagation can help us allocate all the remaining data points. By doing all these, DPC-PPNNN can be suitable for more complex datasets and distinguish two clusters that are close to each other.

DPC-PPNNN

Natural nearest neighborhood

traditional ğ‘˜-nearest neighbors. The NNN is a scale-free concept and The natural nearest neighborhood (NNN) [21] is very different from
does not require parameters in the selection of neighbors. The size of NNN of every data point may be different according to the distribution of the dataset. The following is a precise description of the natural neighborhood method through the definition of the relevant concepts.
Let ğ‘‹ = {ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›} âŠ‚ Rğ‘‘ be a dataset. We use the Euclidean
distance ğ‘‘(ğ‘¥ğ‘–, ğ‘¥ğ‘— ) to measure the distance between points ğ‘¥ğ‘– and ğ‘¥ğ‘— . For a given integer ğ‘˜ > 0, let ğ‘›ğ‘›ğ‘˜(ğ‘¥ğ‘–) be the ğ‘˜th nearest neighbor of ğ‘¥ğ‘– in
ğ‘‹ â§µ{ğ‘¥ğ‘–}. Define the ğ‘˜-nearest neighborhood of ğ‘¥ğ‘– (ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–)) as the set of
ğ‘˜ nearest neighbors of ğ‘¥ğ‘–, i.e.

in Fig. 3, there is a dataset that has two clusters based on a Gaussian
ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–) = âˆªğ‘˜
{ğ‘›ğ‘›ğ‘— (ğ‘¥ğ‘–)}.

distribution with one of the densities much larger than the other. Even
The ğ‘˜-reverse nearest neighborhood of ğ‘¥ (ğ‘…ğ‘ğ‘ (ğ‘¥ )) is the set of points

when we choose the two data points with the largest ğ›¾ğ‘– as the cluster
centers, both of them will belong to the same cluster, which has a
ğ‘–
ğ‘¥ğ‘— âˆˆ ğ‘‹ â§µ {ğ‘¥ğ‘–} with ğ‘¥ğ‘– âˆˆ ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘— ), i.e.
ğ‘˜  ğ‘–

larger density. The reason for this phenomenon is that the difference in densities between the two clusters is so large that we can hardly recognize the centers of clusters regardless of no matter by the decision
graph or by sorting all the data points by ğ›¾.
Last, the final allocation strategy is sensitive and has poor fault
tolerance. As shown in Fig. 2(b), even when we choose the three cluster centers correctly, we cannot obtain the ideal result. There are still some data points that cannot be allocated to the correct cluster.
To overcome the deficiencies mentioned above, in this article, we present an improved probability propagation algorithm for density peak clustering based on natural nearest neighbors (DPC-PPNNN). By
ğ‘…ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–) = {ğ‘¥ğ‘— âˆˆ ğ‘‹ â§µ {ğ‘¥ğ‘–} | ğ‘¥ğ‘– âˆˆ ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘— )}.
The ğ‘˜-natural nearest neighborhood of ğ‘¥ğ‘– (ğ‘ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–)) is defined as
ğ‘ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–) = ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–) âˆ© ğ‘…ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–).
Clearly, ğ‘ğ‘ğ‘ğ‘›âˆ’1(ğ‘¥ğ‘–) = ğ‘‹ â§µ {ğ‘¥ğ‘–} â‰  âˆ… for any ğ‘¥ğ‘– âˆˆ ğ‘‹.
Definition 1 (Natural Nearest Neighborhood (ğ‘ğ‘ğ‘ )). The least integer
ğœ† with ğ‘ğ‘ğ‘ğœ†(ğ‘¥ğ‘–) â‰  âˆ… for every ğ‘¥ğ‘– âˆˆ ğ‘‹ is called the natural eigenvalue of ğ‘‹. We define ğ‘ğ‘ğ‘ğœ†(ğ‘¥ğ‘–) as the natural nearest neighborhood of ğ‘¥ğ‘–, denoted by ğ‘ğ‘ğ‘ (ğ‘¥ğ‘–), for every ğ‘¥ğ‘– âˆˆ ğ‘‹,.



Let ğ‘ğ‘ğ‘ğ‘˜(ğ‘‹) = {ğ‘ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘–) | ğ‘¥ğ‘– âˆˆ ğ‘‹}. Note that ğ‘ğ‘ğ‘ğ‘˜(ğ‘‹) maybe
a multiset. Let
Local density estimation and selection of cluster centers

ğ‘ğ‘ğ‘ 0(ğ‘‹) = {ğ‘¥ğ‘— | ğ‘¥ğ‘—
âˆˆ ğ‘‹ with ğ‘ğ‘ğ‘ğ‘˜(ğ‘¥ğ‘— ) = âˆ…}.
require parameters in the selection of neighbors. The parameter ğ‘‘ğ‘ in As shown in Section 4.1, NNN is a scale-free concept and does not

By Definition 1, the natural eigenvalue ğœ† represents the number of
iterations in the process of searching the natural nearest neighborhood
ğ‘ğ‘ğ‘ (ğ‘‹) = ğ‘ğ‘ğ‘ğœ†(ğ‘‹). The natural eigenvalue ğœ† is generally small
based on the NNN. For any data point ğ‘¥ğ‘–, the new local density is DPC can be avoided by changing the definition of the local density
defined as
âˆ‘	[ ( ğ‘‘ )2]

when there is no outlier or noise (the data points make ğœ† âˆˆ ğ›º(ln ğ‘›)). In
the extremal case, ğœ† can be the largest possible value ğ‘› âˆ’ 1 if there is
ğœŒğ‘– =
ğ‘¥ğ‘— âˆˆğ‘ğ‘ğ‘ (ğ‘¥ğ‘– )
exp âˆ’
ğ‘–ğ‘—
ğœğ‘–
,	(6)

an outlier ğ‘¥ğ‘— âˆ‰ ğ‘ğ‘ğ‘›âˆ’2(ğ‘¥ğ‘–) for any ğ‘¥ğ‘– â‰  ğ‘¥ğ‘— and ğ‘¥ğ‘– âˆˆ ğ‘‹.
where ğœ = max{ğ‘‘(ğ‘¥ , ğ‘¥ ) | ğ‘¥
âˆˆ ğ‘ğ‘ğ‘ (ğ‘¥ )}. We define

To decrease the effect of outliers or noise in dataset ğ‘‹, we in- troduce ln ğ‘› + ln ğœ† as the threshold to control the number of itera-
tions when ğ‘ğ‘ğ‘ 0(ğ‘‹) remains unchanged in its subsequent iteration,
ğ‘–
ğ›¿ = min
ğ‘—âˆ¶ğœŒğ‘— >ğœŒğ‘–
(  )
ğ‘–ğ‘—
ğ‘–  ğ‘—	ğ‘—	ğ‘–

ğ‘˜
i.e.	0	0
and for data point ğ‘– which has the highest local density ğœŒğ‘–, define

ğ‘ğ‘ğ‘ğ‘˜ (ğ‘‹) = ğ‘ğ‘ğ‘ğ‘˜+1(ğ‘‹). Formally, we define the logarithmic
natural nearest neighborhood as follows:
ğ›¿ = max
ğ‘—
(  )
ğ‘–ğ‘—

least        integer        ğœ†        with Definition 2 (Logarithmic Natural Nearest Neighborhood). We call the
|{ğ‘– | ğ‘– â‰¤ ğœ† âˆ’ 1 with ğ‘ğ‘ğ‘ 0(ğ‘‹) = ğ‘ğ‘ğ‘ 0 (ğ‘‹)}| â‰¥ ln ğ‘› + ln ğœ†
which are the same as in DPC.
Let ğ›¾ğ‘– = ğœŒğ‘– Ã— ğ›¿ğ‘– as defined in Eq. (5). We can choose the candidate
centers based on a standard normal distribution with a one-sided 95%
confidence interval. The candidate centers are defined as

ğ‘–	ğ‘–+1

the logarithmic natural eigenvalue and define ğ‘ğ‘ğ‘ğœ†(ğ‘¥ğ‘–) as the loga- rithmic natural nearest neighborhood of ğ‘¥ğ‘–, denoted by ğ‘ğ‘ğ‘ (ğ‘¥ğ‘–), for
ğ‘¥ğ‘– âˆˆ ğ‘‹.
By Definition 2, the logarithmic natural eigenvalue ğœ† represents the
nearest neighborhood ğ‘ğ‘ğ‘ (ğ‘‹), which will be bounded by ln ğ‘› + ln ğœ†. number of iterations in the process of searching the logarithmic natural The logarithmic natural nearest neighborhood ğ‘ğ‘ğ‘ (ğ‘‹) may contain
an empty set. The robust (logarithmic) natural nearest neighborhood
ğ¶ğ‘ğ‘›ğ¶ğ‘’ğ‘›ğ‘  = {ğ‘¥ğ‘– âˆˆ ğ‘‹ | ğ›¾ğ‘– > ğ¸(ğ›¾) + 1.65Var(ğ›¾)},	(7)
where ğ¸(ğ›¾) is the expectation of ğ›¾, Var(ğ›¾) is the variance of ğ›¾, 1.65 is
the 95% quantile of the standard normal distribution.
with low local density ğœŒğ‘– and small distance ğ›¿ğ‘–, but it may have some By the definition of CanCens, it does not contain the data points points with large ğœŒ and small ğ›¿ or small ğœŒ and large ğ›¿ which is not
suitable for being cluster centers. To eliminate those data points, we define

searching algorithm proposed in this paper, which is based on the NNN searching algorithm [21], is shown in Alg. 1.
ğœƒ = ğœŒğ‘–
ğ‘–	ğ›¿ğ‘–
(8)



Algorithm 1 (Logarithmic) Natural Nearest Neighborhood Search Algorithm (NNN)

Input: A set of points ğ‘‹ = {ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ‘›} âŠ‚ Rğ‘‘
Output: The (logarithmic) natural eigenvalue ğœ† = ğ‘Ÿ and ğ‘ğ‘ğ‘ (ğ‘¥ğ‘–) =
ğ‘ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘–) for every ğ‘¥ğ‘– âˆˆ ğ‘‹
1: âˆ€ğ‘¥ğ‘– âˆˆ ğ‘‹, initially set ğ‘ğ‘0(ğ‘¥ğ‘–) = âˆ…, ğ‘…ğ‘ğ‘0(ğ‘¥ğ‘–) = âˆ…, ğ‘ğ‘ğ‘0(ğ‘¥ğ‘–) = âˆ…
2: ğ‘Ÿ = 1, flag=0, ğ‘‡ = 0
3: while flag=0 do
4:	for âˆ€ğ‘¥ğ‘– âˆˆ ğ‘‹ do
5:	calculate ğ‘›ğ‘›ğ‘Ÿ(ğ‘¥ğ‘–) = ğ‘¥ğ‘—
6:	Reset ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘–) = ğ‘ğ‘ğ‘Ÿâˆ’1(ğ‘¥ğ‘–) âˆª {ğ‘¥ğ‘— }
7:	Reset ğ‘…ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘— ) = ğ‘…ğ‘ğ‘ğ‘Ÿâˆ’1(ğ‘¥ğ‘— ) âˆª {ğ‘¥ğ‘–}
8:	Reset ğ‘ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘–) = ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘–) âˆ© ğ‘…ğ‘ğ‘ğ‘Ÿ(ğ‘¥ğ‘–)
9:	end for
10:	calculate the set ğ‘ğ‘ğ‘ 0(ğ‘‹)
for every ğ‘¥ğ‘– âˆˆ ğ¶ğ‘ğ‘›ğ‘ğ‘’ğ‘›ğ‘ . Note that the gap between ğœŒğ‘– and ğ›¿ğ‘– is large
enough for those data points with large ğœŒ and small ğ›¿ or small ğœŒ and large ğ›¿. Therefore, we can select the final centers from Cancens based on standard normal distribution of ğœƒ with a two-sided 95% confidence
interval as shown in Eq. (9).
ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹) = {ğ‘¥ğ‘– âˆˆ ğ¶ğ‘ğ‘›ğ¶ğ‘’ğ‘›ğ‘  | |ğœƒğ‘– âˆ’ ğ¸(ğœƒ)| < 1.96Var(ğœƒ)},	(9)
where 1.96 is the 97.5% quantile of the standard normal distribution. In the Probabilistic Propagation Clustering Algorithm provided in Section 4.3, one cluster may have more than one center point in
ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹), so the allocation strategy of DPC cannot be used here. We
centers             in             ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹). need a new allocation strategy to cluster all the data points using the

Probabilistic propagation clustering algorithm

0	ğ‘Ÿ	The idea of the probabilistic propagation clustering algorithm is

11:	if ğ‘ğ‘ğ‘ğ‘Ÿ (ğ‘‹) remains unchanged then
12:	ğ‘‡ âˆ¶= ğ‘‡ + 1
13:	end if
14:	if ğ‘‡ â‰¥ ln ğ‘Ÿ + ln ğ‘› or âˆ… âˆ‰ ğ‘ğ‘ğ‘ğ‘Ÿ(ğ‘‹) then
15:	flag=1
16:	else
17:	ğ‘Ÿ âˆ¶= ğ‘Ÿ + 1
18:	end if
19: end while




Remrk. NNN searching algorithm proposed by Zhu et al. [21] does not take outliers or noise into consideration. In contrast, by introducing
ln ğ‘› + ln ğœ† as the threshold, the robust natural nearest neighborhood
searching algorithm we propose can recognize outliers and eliminate
their effect.
based on the spread of the epidemic in recent years. An infected man will infect the persons he contacts. Therefore, we first choose the data
point with the largest local density ğœŒ in ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹) as patient zero. In
nearest neighborhood (ğ‘ğ‘ğ‘ ). These neighbors will infect their natural addition, he will infect the persons he contacts with, that is his natural
nearest neighbors and the propagation will continue until there is no neighbor that can be infected. At this time, we recognize all the infected data points as a cluster. We call the processes of forming a cluster a round of propagations. Second, we select the data point that has the
largest local density among those data points not infected in ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹),
and repeat the process above until all the data points in ğ¶ğ‘’ğ‘›ğ‘  have been
infected.
In the real world, some people will not become infected even when
other reasons. Therefore, in the algorithm, a data point ğ‘¥ âˆˆ ğ‘‹ is they contact with infected persons because they have antibodies or
infected with a certain probability ğ‘ğ‘¥ = ğ¶ â‹… (ğ‘â€² + ğ‘â€²â€²), where ğ‘â€² (and ğ‘â€²â€²)

ğ‘¥	ğ‘¥	ğ‘¥	ğ‘¥



denotes the rank of ğ‘¥ in the current round of propagation. Formally, in the current round of propagation, if point ğ‘¥ (to be checked) and the infected points can be ordered ğ‘¦1, â€¦ , ğ‘¦ğ‘–âˆ’1, ğ‘¥, ğ‘¦ğ‘–+1, â€¦ , ğ‘¦ğ‘˜ with ğœŒ(ğ‘¦1) â‰¤
â€¦ â‰¤ ğœŒ(ğ‘¦ğ‘–âˆ’1) â‰¤ ğœŒ(ğ‘¥) â‰¤ ğœŒ(ğ‘¦ğ‘–+1) â‰¤ â€¦ â‰¤ ğœŒ(ğ‘¦ğ‘˜), then we define
ğ‘â€² = ğ‘– ;
Table 1
Synthetic datasets.

ğ‘¥	ğ‘˜
if point ğ‘¥ and the unchecked points in âˆªğ‘˜	ğ‘ğ‘ğ‘ (ğ‘¦ğ‘— ) can be ordered
ğ‘§1, â€¦ , ğ‘§ğ‘—âˆ’1, ğ‘¥, ğ‘§ğ‘—+1, â€¦ , ğ‘§ğ“ with ğœŒ(ğ‘§1) â‰¤ â€¦ â‰¤ ğœŒ(ğ‘§ğ‘—âˆ’1) â‰¤ ğœŒ(ğ‘¥) â‰¤ ğœŒ(ğ‘§ğ‘—+1) â‰¤
â€¦ â‰¤ ğœŒ(ğ‘§ğ“), then we define
ğ‘â€²â€² = ğ‘— .
ğ‘¥	ğ“


If point ğ‘¥ is not infected, we consider it having antibodies or
propagation process, we increase the probability ğ‘ğ‘¥ of being infected to dead and that it will no longer be infected. At the beginning of the
propagation process, we decrease the probability ğ‘ğ‘¥ of being infected ensure that the dissemination process will continue. At the end of the
to guarantee that the outliers or noise will not be classified into any cluster.

Algorithm 2 Probabilistic propagation clustering algorithm (PP)
Input: A set of points ğ‘‹ âŠ‚ Rğ‘‘ , ğœŒ = ğœŒ(ğ‘‹), ğ‘ğ‘ğ‘ (ğ‘‹)
Output: Clustering results ğ¶ğ¿ğ‘ˆ 1:  Initially set ğ¶ğ‘’ğ‘›ğ‘  = ğ¶ğ‘’ğ‘›ğ‘ (ğ‘‹) 2: ğ‘› = 1
3: ğ‘¥ = ğ‘“ ğ‘–ğ‘›ğ‘‘(max ğœŒ(ğ¶ğ‘’ğ‘›ğ‘ )); ğ¶ğ‘’ğ‘›ğ‘  = ğ¶ğ‘’ğ‘›ğ‘  â§µ {ğ‘¥};
ğ¶ğ¿ğ‘ˆ (ğ‘›) = âˆ…, ; ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› = ğ‘ğ‘ğ‘ (ğ‘¥);
4: while ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› â‰  âˆ… do
5:	for âˆ€ğ‘¦ âˆˆ ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› do
6:	calculate the probability of being infected for y: ğ‘ğ‘¦
7:	if ğ‘¦ is infected (with probability ğ‘ğ‘¦) then
8:	Reset ğ¶ğ¿ğ‘ˆ (ğ‘›) = ğ¶ğ¿ğ‘ˆ (ğ‘›) âˆª {ğ‘¦}
9:	Reset ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› = ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› âˆª ğ‘ğ‘ğ‘ (ğ‘¦)
10:	if ğ‘¦ âˆˆ ğ¶ğ‘’ğ‘›ğ‘  then
11:	Reset ğ¶ğ‘’ğ‘›ğ‘  = ğ¶ğ‘’ğ‘›ğ‘  â§µ {ğ‘¦}
12:	end if
13:	else
14:	y does not belong to any cluster
15:	end if
16:	Reset ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› = ğ¶ğ¿ğ‘ˆğ‘ğ‘ğ‘› â§µ {ğ‘¦}
17:	end for
18: end while
Table 2
Real-world datasets.




the K-means and DBSCAN algorithms are implemented in the sklearn library of Python and the DPC algorithm, without the â€˜â€˜Haloâ€™â€™ part, is based on the source code provided by the author since our datasets do not contain noise. All the results shown are the optimal results after argument tuning.
The synthetic datasets and real-world datasets used in the experi- ments are presented in Tables 1 and 2, respectively.

Preliminaries

To evaluate the performance of the clustering algorithm, three evaluation indices are introduced, namely, Adjusted Rand Index (ARI), Adjusted Mutual Information (AMI) and Fowlkesâ€“Mallows index (FMI). The upper bound of the three indicators is 1, in other words, larger values of the indicators indicate better clustering results.
Before the tests, all of the real-world datasets should be adjusted by minâ€“max normalization to eliminate the differences in the ranges of different dimensions, as shown in Eq. (10).

19: if ğ¶ğ‘’ğ‘›ğ‘  â‰  âˆ… then
20:	ğ‘› = ğ‘› + 1
â€²
ğ‘–ğ‘—
=	ğ‘¥ğ‘–ğ‘— âˆ’ ğ‘šğ‘–ğ‘›(ğ‘¥ğ‘— )
ğ‘šğ‘ğ‘¥(ğ‘¥ğ‘— ) âˆ’ ğ‘šğ‘–ğ‘›(ğ‘¥ğ‘— )
(10)

21:	goto Step 3
22: end if
where ğ‘¥
ğ‘–ğ‘—
is the original data in the ğ‘–th position of ğ‘¥ğ‘— .

23: Assign each remaining data point to the cluster that has the largest sum of local densities among its natural nearest neighborhood
To more objectively reflect the actual results of various algorithms, we perform argument tuning on each of the algorithms, thereby en-

		suring that their best performances are compared. For K-means, we
simply give the correct number of clusters. For DBSCAN, we use a grid

When the propagation process is over, there will be some data points that are not infected and are not outliers. We assign each remaining data point to the cluster that has the largest sum of local densities among the natural nearest neighborhoods containing it.
By introducing the idea of probabilistic propagation, we can make the cluster algorithm closer to reality. Moreover, we can distinguish
clustering algorithm is shown in Alg. 2. Denote ğœŒ(ğ´) = {ğœŒ(ğ‘¥) | ğ‘¥ âˆˆ ğ´}. two clusters that are close to each other. The probabilistic propagation

Experiment

We call the algorithms 1 and 2 together the DPC-PPNNN algo- rithm. To test the performance of the DPC-PPNNN algorithm, we use classical synthetic datasets and real-world datasets. Moreover, we take traditional DPC, K-means and DBSCAN as the control group, where
choose the optimal ğ‘‘ğ‘ from 1% to 2% and apply the Gaussian kernel in search to find the best parameter configuration. For traditional DPC, we
the density estimation process.

Synthetic datasets

In this part, we select a number of synthetic datasets that are widely used to test the performance of clustering algorithms. These datasets are different in terms of the distribution and numbers of points and clusters. They can simulate different situations to compare the performance of various clustering algorithms in different scenarios.
Table 3 shows the clustering results in terms of the ARI, AMI and FMI scores on all synthetic datasets listed in Table 1. The evaluation criterion with the highest score is marked in bold, and we can see that in most cases, the DPC-PPNNN obtains the highest score, except for




/ig. 4. The re-clustering results of the three counter examples in Section 3.2. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)


/ig. 5. The clustering results on 2d-4c-no9 by 4 algorithms.


/ig. 6. The clustering results on 3-spiral by 4 algorithms.


/ig. 7. The clustering results on Aggregation by 4 algorithms.


/ig. 8. The clustering results on Cassini by 4 algorithms.


the Complex9 (where the score of DPC-PPNNN is a bit less than the DBSCAN).
In Fig. 4, we recluster the three counter examples in Section 3.2 by DPC-PPNNN. Clearly, the results are much better than the original DPC which implies that our algorithm can overcome the deficiencies of DPC. Next, we will show some typical clustering results on the synthetic datasets by DPC-PPNNN and the algorithms of the control group. The points with different colors in the figures are assigned to different
clusters. The cluster centers obtained from DPC and DPC-PPNNN are marked in red.
In Fig. 5, we can see that the 2d-4c-no9 dataset has four clusters, of which one cluster has a very high density. DPC-PPNNN succeeds in detecting all of them, while the other algorithms of the control group fail to do so.
Fig. 6 shows the results of each algorithm on the 3-spiral dataset. DPC-PPNNN can identify the four clusters correctly. Furthermore, DPC




/ig. 9. The clustering results on Complex9 by 4 algorithms.


/ig. 10. The clustering results on Compound by 4 algorithms.


/ig. 11. The clustering results on Dartboard1 by 4 algorithms.


/ig. 12. The clustering results on Jain by 4 algorithms.


/ig. 13. The clustering results on R15 by 4 algorithms.


/ig. 14. The clustering results on Shapes by 4 algorithms.



and DBSCAN detect the four clusters mostly correctly with some wrong data points at the tail, perhaps because the arguments are not set
Table 3
Performances of different clustering algorithms on different synthetic datasets.







and DBSCAN can detect the clusters in the Cassini dataset. Although DPC can find the correct cluster centers, it fails to allocate the other remaining data points correctly. The three clusters are not uniform in shape which leads to the wrong cluster results by K-means.
The Fig. 9 shows the results of the four algorithms on the Complex9 dataset. DBSCAN can recognize all the clusters successfully. While for DPC-PPNNN, there are some misclassified points at the tail of a cluster because these points are not connected tightly enough. DPC finds the correct cluster centers but it fails to allocate the other remaining data points correctly. K-means has poor performance.
Fig. 10 displays the results on the Compound dataset. DPC-PPNNN can detect most of the clusters correctly and can also recognize noise. DBSCAN is good at recognizing noise but it also misclassifies many data points as noise. DPC and K-means fail to recognize the clusters with noise.
As shown in Fig. 11, the Dartboard1 dataset has four concentric rings. Both DPC-PPNNN and DBSCAN have perfect performance. DPC cannot find the correct cluster centers because the densities of the data points have little difference. Moreover, K-means has poor performance. For the Jain dataset shown in Fig. 12, only DPC-PPNNN correctly identifies all clusters. DPC has not found the correct cluster centers because the difference between the densities of the two clusters is too large. The reason is the same for the poor performance of DBSCAN.
K-means is not suitable for the datasets with streamline shapes.
Fig. 13 displays the results on the R15 dataset. The distribution of points makes it the most straightforward dataset for all the algorithms. Although there are some small defects among them, all the algorithms can recognize both the clusters and centers.
The clustering shown in Fig. 14 demonstrates the ability to address the datasets with clusters of different shapes. DPC-PPNNN, K-means and DBSCAN all perform well. Only DPC cannot detect the clusters because there is a cluster with manifold structure.

Read-world datasets

In this section, 8 UCI datasets, as shown in Table 2, are used to demonstrate the performance of the DPC-PPNNN clustering algorithm. These datasets are different in terms of sample number, feature number and cluster number. As shown in Table 4, DPC-PPNNN performs almost the best among the test cases.
Conclusions

In this paper, we proposed an improved probability propagation al- gorithm for density peak clustering based on natural nearest neighbors (DPC-NNN). The new algorithm does not require any parameters and can recognize cluster centers automatically. The final clustering process of the DPC-PPNNN motivated by the epidemic spread performs well especially for distinguishing two clusters that are close to each other.
However, since the algorithm is based on density and probability,
data points have similar ğ›¿ or similar densities ğœŒ. For future work, we its performance is not good and stable enough, especially when all the
will work on solving these problems.

Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.



Table 4
Performances of different clustering algorithms on different real-world datasets. Algorithm	ARI	AMI	FMI	ARI	AMI	FMI



Acknowledgments

The work was supported by National Natural Science Foundation of China (No. 12071453), the National Key R and D Program of China (2020YFA0713100), Anhui Initiative in Quantum Information Technologies, China (AHY150200), and the Innovation Program for Quantum Science and Technology, China (2021ZD0302904).

References

Tan PN, Steinback M, Kumar V. Introduction to data mining. Data mining. 2011.
Liu R, Wang H, Yu X. Shared-nearest-neighbor-based clustering by fast search and find of density peaks. Inform Sci 2018;450:200â€“26.



Macqueen JB. Some methods for classification and analysis of multivariate observations. In: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, 1967. 1967.
Kaufmann L, Rousseeuw PJ. Clustering by means of medoids. North-Holland; 1987.
Guha S, Rastogi R, Shim K. Cure: An efficient clustering algorithm for large databases. Inf Syst 1998;26:35â€“58.
Zhang T, Ramakrishnan R, Livny M. Birch: An efficient data clustering method for very large databases. ACM SIGMOD Rec 1996;25:103â€“14.
Ester M, Kriegel HP, Sander J, Xu X. A density-based algorithm for discovering clusters in large spatial databases with noise. AAAI Press; 1996.
Ankerst M, Breunig MM, Kriegel HP, Sander J. Optics: Ordering points to identify the clustering structure. In: SIGMOD 1999, Proceedings ACM SIGMOD international conference on management of data, June 1-3, 1999. Philadelphia, Pennsylvania, USA; 1999.
Sheikholeslami G, Chatterjee S, Zhang A. Wavecluster: A wavelet based clustering approach for spatial data in very large databases. VLDB J 2000;8:289â€“304.
Wang W, Yang J, Muntz R. Sting: A statistical information grid approach to spatial data mining. VLDB J 1997;25:186â€“95.
Dempster AP, Laird NM, Rubin DB. Maximum likelihood from incomplete data via the em algorithm. Proc R Stat Soc 1977;39:1â€“22.
Rodriguez A, Laio A. Clustering by fast search and find of density peaks. Science 2014;344:1492.
Du M, Ding S, Jia H. Study on density peaks clustering based on k-nearest neighbors and principal component analysis. Knowl-Based Syst 2016;99:135â€“45.
Mehmood R, Zhang G, Bie R, Dawood H, Ahmad H. Clustering by fast search and find of density peaks via heat diffusion. Neurocomputing 2016;208:210â€“7.
Liang Z, Chen P. Delta-density based clustering with a divide-and-conquer strategy. Pattern Recognit Lett 2016;73:52â€“9.
Xu J, Wang G, Deng W. DenPEHC: Density peak based efficient hierarchical clustering. Inform Sci 2016;373:200â€“18.
Tao LI, Hongwei GE, Shuzhi SU. Density peaks clustering by automatic determination of cluster centers. Front Comput Sci Technol 2016;10:1614â€“22.
Zhou Z, Si G, Zhang Y, Zheng K. Robust clustering by identifying the veins of clusters based on kernel density estimation. Knowl-Based Syst 2018;159:309â€“20.
Xie JY, Grant WX, Philip W, Liu XH, Gao HC. Robust clustering by detecting density peaks and assigning points based on fuzzy weighted k-nearest neighbors. Inform Sci 2016;354:19â€“40.
Geng YA, Li Q, Zheng R, Zhuang F, He R. Recome: a new density-based clustering algorithm using relative knn kernel density. Inform Sci 2018;436â€“437:13â€“30.
Zhu Q, Huang J, Feng J, Zhou X. A clustering algorithm based on natural nearest neighbor. J Comput Inf Syst 2014;10:5473â€“80.
