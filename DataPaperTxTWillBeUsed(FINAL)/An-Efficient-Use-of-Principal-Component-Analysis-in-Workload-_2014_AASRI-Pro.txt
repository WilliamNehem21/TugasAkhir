Available online at www.sciencedirect.com
ScienceDirect

AASRI Procedia 8 (2014) 68 – 74


2014 AASRI Conference on Sports Engineering and Computer Science (SECS 2014)
An Efficient Use of Principal Component Analysis in Workload Characterization-A Study
Jyotirmoy Sarkara, Snehanshu Sahab, Surbhi Agrawalb*
aBITS PILANI & TechMahindra,,Bangalore,560100,India
bCBIMMC & Dept. of Computer Science and Engineering,PESIT-BSC,Bangalore,560100,India




Abstract

PCA is a useful statistical technique that has found application in fields such as face recognition, image compression, dimensionality reduction, Computer System performance analysis etc. It is a common technique for finding patterns in data of high dimension. In this paper, we present the basic idea of principal component analysis as a general approach that extends to various popular data analysis techniques. We state the mathematical theory behind PCA and focus on monitoring system performance using the PCA algorithm. Next, an Eigen value-Eigenvector dynamics is elaborated which aims to reduce the computational cost of the experiment. The Mathematical theory is explored and validated. For the purpose of illustration we present the algorithmic implementation details and numerical examples over real time and synthetic datasets.
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
Keywords: PCA; Eigen Value; Eigen Vector,Workload Characterization..








* Corresponding author. Tel.: +91-080-66186622; fax: 91-80-.
E-mail address: snehanshusaha@pes.edu.










2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute doi:10.1016/j.aasri.2014.08.012


Introduction

Performance evaluation helps us to give an idea how well a system is performing as compared to other systems. Workload is the most crucial part of any performance evaluation process. The entire process can end up in a wrong conclusion if workload is not chosen in appropriate ways. Therefore, workload selection is an integral part of performance evaluation project. Computer architectures are evaluated by running a workload on the computer and measuring the execution time. New computers are designed the same way. As the newly designed computer does not exist, it is not possible to run any workload. This is where workload characterization comes into the fore. The goal of workload characterization is to describe the properties of a workload in terms of abstract performance metrics, called workload characteristics, which predict the final performance [1].
There are a couple of techniques to classify workload components. One of the widely used techniques is “weighted sum of the parameter values” that uses sum to classify the workload components into classes. But there are proper guidelines to decide the weight of parameters. Before PCA, an analyst running software used to assume the values of weight. Instead, one can use PCA, to calculate the value of weights. PCA is a procedure by which numbers of correlated variables are transformed into a smaller number of uncorrelated variables. It is a data analysis technique traced back to Pearson (1901). It can be used to compress a high dimensional dataset into a lower dimensional dataset. PCA can be derived from a number of starting points and optimization criteria. The most important of these are minimization of the mean square error in data compression, finding mutually orthogonal directions in the data having maximal variances and de-correlation of the data using orthogonal transformation. These uncorrelated variables are called Principal Components.

How PCA works:

For a given set of n parameters{x1, x2 ,...xn } , the Principal Component Analysis will produce a set of principal factors. The following conditions will hold true for the newly produced set -
The principal factor ( yi ) is a linear combination of initial parameters (xj ) .

y   aij x j j 1


Principal factor set is an orthogonal set.
 yi , y j   aik akj  0
k

It is an ordered set {y1, y2 ,...yn } in the decreasing order of the percentage of variance, with





y1 being the

highest percentage of variance and components.
yn the least. So first few factors can be used to classify the workload

We can find the application of Principal Component Analysis in many fields including data compression, image processing, visualization, pattern recognition and time series prediction [2].Sirvich and Kirby had efficiently used PCA in human faces representation [3-4].This approach leads to decomposition of any images into Eigen pictures so that the image can be reconstructed using a portion of the Eigen pictures and the corresponding projection onto the Eigen picture subspace [5]. The PCA method has also been used in handprint recognition, human made object recognition, industrial robotics and mobile robotics etc [6]. In a workload composition the choice of benchmark is very important. The selection of benchmark for inclusion in


benchmark suit is called workload composition. Smith [7] used a metric, which is based on dynamic program characteristics for the Fortran language..They used squared Euclidean distance to measure the difference between benchmarks. The shortcoming of this procedure is the use of Euclidean distance for measuring the difference. To overcome this Eeckhout et al. [8] proposed Principal Component Analysis (PCA) to get rid of the correlation and dependence between variables. A number of program characteristics are measured for a number of benchmarks on which PCA was applied.

Proposed Work

The most computationally expensive part of PCA is the calculation of Eigen values and Eigen vectors of the dataset. In this paper our main objective is to save the computational time of Principal Component Analysis (PCA) by skipping the Eigen vector calculation. Here we propose to bypass Eigen vector calculation, by rather inspecting the Eigen vectors. The understanding of the dynamics of Eigen values and Eigen vectors in the context of Linear transformations and vector spaces plays a crucial role in improving the efficiency of PCA in the workload characterization problem. This will require us to prove/cite important theorems in Linear Algebra.


Algorithm

Compute the mean and standard deviation of the parameters.




1  n	,	2	1	n



x 	 ai i 1


sx 	 ( xi  x )
i 1




Compute the correlation of the parameters. R
1  n
 n i 1

( xai


xa )( xb


xb )

xa xb 

s a s xb



Compute QR Decomposition of correlation matrix at every step
Ak  Qk Rk
(starting with k  0 ),

where Qk
is an orthogonal matrix and Rk
is an upper triangle matrix.
Ak 1  Rk QK

The matrix will converge to a triangular matrix, known as the Schur form. Find out the eigen values of the matrix from the diagonal.
Apply the results of the theorem to choose the eigen vectors by inspection. This is possible since the
matrices obtained are symmetric and the Eigen values are real and distinct.
Use the Eigen vectors to compute the principal factors.
Next, we prove a theorem related to eigen values and eigen vectors. Let us take a linear map T : u  v 
T (α x  β y)  αT (x)  βT ( y); where x, y  u ; u, v are vector spaces of certain dimensions, n & m
say where n  m necessarily; e.g. u  Rn , v  Rm λ  R  Tx  λx , then λ is an Eigen value of
T & x is a corresponding Eigen vector.

Proposition 1:
For the linear map T : u  v , if the Eigen values “ λ “are distinct, then T admits of linearly independent Eigen vectors.
Proof: Linear Independence: A set of vectors {v1, v2 , v3,....vn } is linearly independent if  scalars
n

(α1,α2 ,	αn ) 
 αivi  0 implies αi  0i  1,..n i.e. vi ; any vector in the set is NOT a linear
i 1


1	 1 
combination of any of the other vectors in the same set e.g. 1 & 1 are linearly independent.
 		
Proof of the theorem: (Using the Principle of Mathematical Induction)

Basis Step: n  2;
NTS a1v1  a2v2  0  a1  0  a2

Apply T  T (a1v1  a2v2 )  T (0) ; T linear map
 a1T (v1 )  a2T (v2 )  0;
 a1λ1v1  a2λ2v2  0






(1)



Also

a1λ1v1  a2λ1v2  0
Therefore (1)  (2)  a2 (λ2  λ1 )v2  0  a2  0




(' λ1  λ2; v2  0)


(2)


a2  0  a1  0 ; Induction hypothesis: Assume the proposition is true for n  m ; Induction steps: on

n  m 1 i.e. NTS
a1v1  ....  amvm  am1vm1  0 => a  a
 ....  a   a
 0.

1	2	m	m1

Let so,
a1v1 	 amvm  am1vm1  0
i.e. T (a1v1 	 amvm  am1vm1 )  T (0)

a1λ1v1 	 amλmvm  am1λm1vm1  0
(3)



Also,

a1λ1v1 	 amλ1vm  am1λ1vm1  0


(4)



(3)  (4) 
a2 (λ2  λ1)v2 	 am1 (λm1  λ1)vm1  0

By the hypothesis, {v1 ,...vm } linear independent  a1  a2  ...am  am1  0


 am1 (λm1  λ1 )vm1  0
('(λm1  λ1)  0 & vm1  0)


Therefore, the Eigen vectors {v1 ,...vm , vm1} are Linearly Independent.
Proposition 2: A real, symmetric linear map T (matrix) admits of orthogonal Eigenvectors.
Proof: Well established result [9].
Conclusion of proposition: The aforementioned matrix (or the linear map, T) has distinct eigen values (as


always the case will be) and is real, symmetric. Therefore, the corresponding eigenvectors will be linearly independent & orthogonal to each other. This enables us to find the eigenvectors by inspection rather than computing step by step via set of simultaneous equations. This saves O (n) computations, crucial computation cost!

Implication

The paper aims to improve time complexity of PCA algorithm. The following example will illustrate the principle behind PCA from initial parameters.We have collected synthetic data of the number of packets lost on two different network links. xa is the number of packets lost on link A and xb is the number of packets lost on link B
Table 1. Data for Principal Component Analysis Example 1


First we have to compute the mean and standard deviation using the formulas given in Algorithm 2


x    45 60   45 6
; x    4600  460 ; s2
= 248 3986  10  456 2	;

a	10 
2  34805.5
b
b	10	xa
 4495 8. 4
9

Correlation among the variables as Rx x   0.486 and hence the correlation matrix will be
 1 . 000	 0. 4 86 

C
(5)
   0. 486	1. 000 

Now we will compute the Eigen values from the above correlation matrix using characteristic equation

C  λ I 
1  λ	 0.486  0  (1 λ)2  0.4862  0

 0.486	1  λ


The Eigen values are 1.486 and 0.514.

Results and Discussion

Now, the correlation matrix in (5) is both real and symmetric and is a candidate for the theorems to be applied. Figure1 below shows the average execution time of PCA algorithm over a sample of 5 different datasets. The execution time has been recorded in milliseconds.

Conclusion

PCA is the simplest of the true Eigenvector-based multivariate analyses. It can be used to reveal internal structure of data in a way that best explains the variance in data. PCA is sensitive to outliers in the data that produce large number of errors. So, before applying PCA it is expected to remove outliers. As a limitation the result of PCA depend on the scaling of variables. The applicability of PCA constrained by certain assumption made in derivation. Our work explores the underlying principles of PCA and exploits the inherent mathematical theory for efficient computation. The figure below conclusively shows that computation time has been reduced to achieve the same results.


Fig.1. execution times of Eigen values and Eigen vectors and the time saved.


References

T. M. Conte and W. Hwu, Benchmark Characterization,"IEEE Computer, vol. 24, no. 1, pp. 48-56, Jan. 1991.
Raj Jain, The Art of Computer Systems Performance Analysis, Techniques for Experimental Design, Measurement, Simulation, and Modeling.
Kirby and Sirovich, 1990. Application of Karhunen-Loeve Procedure for the Characterization of Human Faces. IEEE
Taranpreet Singh, Face Recognition Based on PCA Algorithm.
S Ekhe, Y Chincholkar, Improved Face Recognition using PCA & LDA
R Gottumukkal, V K Asari, An Improved Face Recognition Technique Based on Modular PCA Approach.


R. H. Saavedra and A. J. Smith, “Analysis of Benchmark Characteristics and Benchmark Performance Prediction,” ACM TOCS, vol. 14, no. 4, pp. 344–384, Nov. 1996.
L. Eeckhout, H. Vandierendonck, and K. De Bosschere, “Quantifying the Impact of Input Data Sets on Program Behavior and its Applications,” JILP, vol. 5, Feb. 2003, http://www.jilp.org/vol5
Gilbert Strang, “Introduction to Linear Algebra”, 4th Edition, SIAM, 2009.
