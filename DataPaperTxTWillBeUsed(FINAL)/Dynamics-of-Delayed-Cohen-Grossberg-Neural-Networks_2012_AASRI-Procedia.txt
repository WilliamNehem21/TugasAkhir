Available online at www.sciencedirect.com



AASRI Procedia 3 (2012) 254 – 261




2012 AASRI Conference on Modeling, Identification and Control
Dynamics of delayed Cohen-Grossberg neural networks
Ancheng Chang   Changlin Peng and Chuangxia Huang*
College of Mathematics and Computing Science, Changsha University of Science and Technology, Changsha, Hunan 410114, China.




Abstract

This paper studies the boundedness of Cohen-Grossberg neural networks with discrete delays and distributed delays (CGNN). Applying Lyapunov function and linear matrix inequalities technique (LMI), some novel sufficient conditions on the issue of the uniformly ultimate boundness, the existence of an attractor and the globally exponential stability for CGNN are established, which can be easily checked by the effective LMI toolbox in Matlab in practice.

© 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and/or peer review under responsibility of American Applied Science Research Institute

Keywords: Linear matrix inequalities technique (LMI), Neural Networks, Distributed delay, Boundedness, Attractor, Stability.


Introduction

In recent years, much attention has been paid on neural networks since they have been fruitfully applied in signal and image processing [1,2,3]. These applications rely crucially on the analysis of the dynamical behavior [4,5,6,7]. Among them, CGNN [8] can be described as follows


xi (t)
i (xi (t))[
i (xi (t))
 aij f j (xj (t))]
j 1
Ji ,

(1)






* Corresponding author. Tel.:0-086-731-85258787; fax: 0-086-731-85258787.
E-mail: cxiahuang@126.com..







2212-6716 © 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and/or peer review under responsibility of American Applied Science Research Institute doi:10.1016/j.aasri.2012.11.042


where t  0, n  2 ; n corresponds to the number of units in a neural network; xi (t) denotes the potential
(or voltage) of cell i at time t ; f j (x(t)) denotes a non-linear output function; i (x(t))  0 represents an
amplification function;	i (x(t)) represents an appropriately behaved function; the n  n connection matrix
 (aij )nn denotes the strengths of connectivity between cells, and if the output from neuron j excites
(respectively, inhibits) neuron i , then aij  0 (respectively, aij  0) , Ji denotes an external input source.
During hardware implementation, time delays do exist due to finite switching speed of the amplifiers and communication time and may lead to an oscillation which is degenerate to the instability of networks furthermore. For model (1),  Ye et al. [9] introduced delays by considering the following delay differential

equations
xi (t)
i (xi (t))[
i (xi (t))
n
 aij f j (xj (t
j 1
j ))]
Ji , i
 1,  , n.

(2)

Although constant fixed delays in the models of delayed feedback systems serve as a good approximation in simple circuits consisting of a small number of cells, neural networks usually have a spatial extent due to the presence of an amount of parallel pathways with a variety of axon sizes and lengths [10, 12]. Therefore, there will be a distribution of conduction velocities along these pathways.
In this paper, we will consider the following CGNN model with mixed delays (discrete delays and distributed delays):

xi (t)
i (xi (t))[
i (xi (t))
n
 aij f j (xj (t))
j 1
n
 bij f j (xj (t
j 1
j ))



n	t
cij
f j (xj (s))ds  Ji ].

(3)

j 1	t hj

Over the past decades, the stability of neural networks has been intensively investigated. In fact, except for stability property, boundedness is also one of the foundational concepts of dynamical neural networks, which plays an important role in investigation for the uniqueness of equilibrium point (periodic solutions), global asymptotic stability, global exponentially stable and its synchronization and so on [14, 15]. To the best of the authors knowledge, few authors have considered on the ultimate boundedness and attractor for CGNN with interval time-varying delays and distributed time-varying delays.
As is well known, compared with linear matrix inequalities (LMI) result, algebraic result is more conservative, and criteria in terms of LMI can be easily checked by using the powerful MATLAB LMI toolbox. This motivates us to investigate the problem of the ultimate boundedness and attractor for CGNN in this paper.

Problem formulation

System (3) for convenience can be rewritten as the following vector form

x(t) (x(t))[
(x(t))
 AF(x(t))
 BF(x(t 
))  C	F(x(s))ds  J ]

(4)


(x(t))H (t). Where, x(t)  (x (t),  , x (t))T  Rn is the neural state vector; (x(t)) 
1	n
diag( (x (t)),  ,  (x (t)))  Rn n ;  (x(t)) , F(x(t)) are appropriate dimensions functions;
1	1	n	n
( ,  ,  )T , h  (h ,  , h )T ; J  (J ,  , J )T , H (t) (x(t))  AF (x(t))  BF (x(t ))
1	n	1	n	1	n


C	F(x(s))  J . The discrete delays and distributed delays are bounded: 0  ,	*  max{ };
i	1 i n	i

0  h ,	h*  max{h };  max{*, h*}, here *, h*,
are scalars. As usual, the initial conditions

i	1 i n	i

associated with system (4) are given in the form vector-valued function.
x(t) (t),
t  0, where (t) is a differentiable

Throughout this paper, we make the following assumptions.
(H1 ) We assume that the delay kernels satisfy f j (0)  0, j

1,  , n , and there exist constants l j and Lj ,

i 1, 2,  , n, such that


l


f j (x)


f j ( y)


L ,	x, y  R, x  y.

j	x  y	j


(H2 ) There exist positive constants
i , i  such that

i  ai (xi (t)) i ;

(H3 ) There exist positive constants bj , such that
x (t)	(x (t))  b x2 (t).
j	j	j	j  j
Remark 2.1 The constants l j and Lj can be positive, negative or zero. Therefore, the activation functions

f (x(t)) are more general than the forms| f j (u) |
K j | u |, K j
 0, j
 1, 2,  , n .

Definition 2.1 [13] System (4) is uniformly ultimately bounded, if there is


 0 , for any constant

 0 , there is t'
 t'(
)  0 , such that
x(t, t0 , )

B for all
t  t0
 t ',t0
 0,  ,
where

x(t, t0 , ) max sup | xi (t  s, t0 , ) | .
1 i n s  0

Lemma 2.1 [11] For any positive definite constant matrix W  Rn n , scalar r  0 , vector function


u(t) :[t  r, t]
 Rn ,
t  0 , then ( r u(s)ds)T W ( r u(s)ds)  r r uT
(s)Wu(s)ds.

0	0	0


Main Results

Theorem 3.1 For a given constant


a  0 , if there is positive-definite matrix P


 diag( p1, p2


 , pn ),

Di  diag(Di1, Di 2
 , Din ) , i
 1, 2 , Q, R such that the following condition holds



11	12	13	PB
0	PC	0

22	0	24
	33	0

44



(5)


			





where Q
Q11

Q12 Q22
0, S
S11

S12 S22
0, R
R11

R12 R22
0, Di

 0,i

 1, 2,

	 a P  2a P  P  Se a* / *  h*R  D , 	 Se a* / *,
11	1	2	11	3  1	12

*

13	12	12	4  1
a*	a*	*
	
22	11	3  2

a*

24	12	4  2
33 	22 	1 	22 ,
a*
	
44	22	2

R e ah* /h*, 
R e ah* /h*, 
R e ah* /h* , 	*2S ,

55	11
56	12
66	22	77

1  diag{1/ 1
 ,1/
n }, 2
 diag{b1,  , bn}, 3
 diag{l1L1,  , ln Ln},

4  diag{(l1
 L1)/2,  , (ln
 Ln )/2},

the symbol '*' within the matrix represents the symmetric term of the matrix, then System (4) is uniformly ultimately bounded.
Proof. Choosing the following Lyapunov functional


V (t) 
V1 (t) 
V2 (t) 
V3 (t) 
V4 (t),
(6)



where V (t)	2p eat	j	 s	ds, V (t)
x (t ) 
easT (s)Q(s)ds, (t) 
[ T	T
]T ,


1	j	0
j 1
j (s)
x (t), F
(x(t))



V3 (t)
eas xT (s)Sx(s)dsd ,
V4 (t)
0
h
easT (s)R
(s)dsd.


Computing the derivative of V1 (t) along the trajectory of system (4), one can get


V (t)	2[ap eat
x j (t )   s	ds
 p eat x (t)	(x (t))]

1	=	j
j 1
0	 j (s)
j	j	j	j



[2xT (t)PAF(x(t))  2xT (t)PJ 2xT (t)PBF (x(t
)) 2xT (t)PC	F(x(s))ds]eat .

(7)


According to Assumption ( H 2 ), we obtain the following inequalities


2ap
x j (t )   s	ds
a p x2 (t).

(8)

0	 j (s)	j

From Assumption ( H3 ), inequalities (7) and (8), we obtain




V1 (t) 
aeat[xT (t) 
Px(t)  2xT (t) 
Px(t)]
[2xT (t)PAF(x(t))

(9)



2xT (t)PBF (x(t
)) 2xT (t)PC	F(x(s))ds
xT (t)Px(t) 
J T PJ ]eat .


Similarly, computing the derivative of $V_2(t)$ along the trajectory of system (4), one can get




V2 (t) 
eat[xT (t), FT x(t))]Q[xT (t), FT x(t))]T




ea(t 
)[xT (t
(t)), FT x(t
))] Q[xT (t
(t)), FT x(t
))]T

(10)

= eat[xT (t)Q x(t)  FT (x(t))QT x(t) xT (t)Q F(x(t))  FT (x(t))Q F(x(t))]
11	12	12	22


ea(t  * )[xT (t
)Q11x(t
) FT (x(t
))QT x(t )



 xT (t
)Q12 F (x(t
)) FT (x(t
))Q22F(x(t
))].


Computing the derivative of V3 (t) along the trajectory of system (4), one can get


V (t) =	0 [eat xT (t)Sx(t) 

ea(t 
) xT (t
)Sx(t
)]d 




	
 *eat xT (t)Sx(t) 
ea(t  * )
xT (s)Sx(s)ds,
(11)



Where	*	max{ }. Denote
1 i n	i
Max{1, 2 ,  , n} , we obtain



*eat xT (t)Sx(t) = *eat[
(x(t))H (t)]T S
(x(t))H (t)
*2eat HT (t)SH (t).

(12)


Using Lemma 2.1, the following inequality is easily obtained



ea(t  * )

xT (s)Sx(s)ds 
ea(t  * )
*
(
x(s)ds)T S(
x(s)ds)

(13)




= ea(t  * ) /
*[xT (t)Sx(t)  2xT (t
)Sx(t)
xT (t
)Sx(t
)].

Similarly, computing the derivative of V4 (t) along the trajectory of system (4), one can get




V4 (t) 
h*eat[xT (t)R x(t)  2FT (x(t))RT x(t) 
FT (x(t))R F(x(t))]

(14)



(
(s)ds)T R(
(s)ds)ea(t h* ) /h*



From Assumption (H1 ) , we have

[ f j (xj (t))
xj (t) 

l j ][


f j (xj (t))
xj (t) 




Lj ]



0, j



 1, 2,  , n.



(15)



Then we obtain


x(t)	T
F (x(t))



3 D1	4 D1
D1


x(t) 
F (x(t))


eat  0,



(16)



and

x(t F (x(t

(t))
(t)))


T
3  2	4  2
D2

x(t F (x(t

(t))
(t)))



eat


 0 .	(17)



Denote
MT (t)  (xT (t), xT (t

), FT (x(t)), FT (x(t

)), (	(s)ds)T ,(

F(x(s))ds)T , HT (x(t)))T ,

combing with (9)-(17), we have

V (t) 

V (t)  V (t)  eat M T (t)	M (t) 

eat J T PJ.

(18)

Therefore, one obtains
1	2	1

K eat
x(t) 2
V (x(0))2
 a 1eat J T PJ ,
(19)

where K1
 min{anp j /
j}, which implies

1 j n 
x(t)2

 [e at
V (x(0))2
 a 1JT PJ ]/K .

(20)





If one choose B
 [(1 
a 1JT PJ )/K ]1/2
 0 , then for any constant  0 and   , there

is t'
 t'(
)  0 , such that
e at
V (x(0))2
 1 for all
t  t' . According to Definition 2.1, we have

x(t, 0, )


B for all
t  t' . That is to say, system (4) is uniformly ultimately bounded.


Theorem 3.2 If all of the conditions of Theorem 3.1 hold, then there exists an attractor	B for the

solutions of system (4), where	B
 {x(t) :
x(t)


B,t  t0}.



Proof. If one choose B
 [(1
a 1JT PJ )/K ]1/2
 0, Theorem 3.1 shows that for any  , there is t'  0 ,

such that
x(t, 0, )
B for all t  t' . Let	B denote by
 {x(t) :
x(t)


B,t  t0}. Clearly,

B is closed, bounded and invariant. Furthermore, lim sup inf
x(t; 0, )  y
0 . Therefore,	B is an

attractor for the solutions of system (4).
t 	y  B

Theorem 3.3 In addition to all of the conditions of Theorem 3.1 hold, if J  0 , then system (4) has a

trivial solution
Proof. If J
x(t)  0 and the trivial solution of system (4) is globally exponentially stable.
0 , then system (4) has a trivial solution x(t)  0 . From Theorem 3.1, one has



x(t; 0, )2  K e at for all	(21)

2
where K2	V (x(0))  /K . Therefore, the trivial solution of system (4) is globally exponentially stable.

Conclusions

In this paper, the dynamics of Cohen-Grossberg neural networks with mixed delays is investigated. Novel multiple Lyapunov-Krasovkii functionals are designed to get new sufficient conditions guaranteeing the uniformly ultimate boundedness, the existence of an attractor and the globally exponential stability. The derived conditions are expressed in terms of LMIs, which are more relax than algebraic formulation and can be easily checked by the effective LMI toolbox in Matlab in practice.


Acknowledgements

This work was supported in part by National Natural Science Foundation of China ( No.11101053), the Key Project of Chinese Ministry of Education (No.211118), the Excellent Youth Foundation of Educational Committee of Hunan Provincial (No.10B002), Science and Technology Project of Hunan of China (No. 2010FK3025, No. 2012SK3096).


References
Z. Yuan, L. Huang, D. Hu, B. Liu. Convergence of Nonautonomous Cohen-Grossberg-Type neural networks with variable delays. IEEE Trans. Neural Netw., 2008; 19, 140-147.
T. Roska, L.O. Chua. Cellular neural networks with nonlinear and delay-type template. Int. J. Circuit Theor. Appl., 1992; 20, 469-481.


D. Liu, A.N. Michel. Celular neural networks for associative memories. IEEE Trans. Circuits. Syst., 1993; 40, 119-121.
P. Venetianer, T. Roska. Image compression by delayed CNNs. IEEE Trans. Circuits. Syst. I, 1998; 45, 205-215.
T. Chen. . Neural Networks, 2001; 14,
977-980.
K. Lu, D. Xu, Z. Yang. Global attraction and stability for Cohen-Grossberg neural networks with delays. Neural Networks, 2006; 19, 1538-1549.
H. Chen, etl.. Image-processing algorithms realized by discrete-time cellular neural networks and their circuit implementations. Chaos, Solitons, Fractals, 2006; 29, 1100-1108.
M. Cohen, S. Grossberg. Absolute stability and global pattern formation and parallel memory storage by competitive neural networks. IEEE Trans. Syst. Man Cybern., 1983; 13, 815-821.
H. Ye, A. Michel, K. Wang. Qualitative analysis of Cohen-Grossberg neural networks with multiple delays. Phys. Rev. E, 1995; 50, 2611-2618.
J. Cao, K. Yuan, H. Li. Global asymptotical stability of gneralized recurrent neural networks with multiple discrete delays and distributed delays. IEEE Trans. Neural Networks, 2006; 17, 1646-1651.
K. Yuan, J. Cao, J. Li. Robust stability of switched Cohen-Grossberg neural networks with mixed time- varying delays. IEEE Trans. Syst. Man Cybern, 2006; 36, 1356-1363.
J.	Lian, K. Zhang. Exponential stability for switched Cohen-Grossberg neural networks with average dwell time. Nonlinear Dyn., 2011; 63, 331-343.
J. Cao, J. Liang. Boundedness and stability for Cohen-Grossberg neural networks with time-varying delays. J. Math. Anal. Appl., 2004; 296, 665-685.
H. Zhang, Y. Wang. Stability analysis of Markovian jumping stochastic Cohen-Grossberg neural networks with mixed time delays. IEEE Trans. Neural Netw., 2008; 19, 366-370.
C. Huang, L. Huang. Dynamics of a class of Cohen-Grossberg neural networks with time-varying delays. Nonlinear Anal. RWA, 2007; 8, 40-52.
