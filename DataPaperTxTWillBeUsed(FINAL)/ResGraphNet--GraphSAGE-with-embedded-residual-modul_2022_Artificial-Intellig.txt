Artificial Intelligence in Geosciences 3 (2022) 148â€“156







Original research articles
ResGraphNet: GraphSAGE with embedded residual module for prediction of global monthly mean temperature
Ziwei Chen a,b, Zhiguo Wang a,âˆ—, Yang Yang b, Jinghuai Gao b
a School of Mathematics and Statistics, Xiâ€™an Jiaotong University, Xiâ€™an, 710049, Shaanxi, PR China
b School of Information and Communications Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, 710049, Shaanxi, PR China


A R T I C L E  I N F O	A B S T R A C T

	

Keywords:
Graph neural network GraphSAGE
ResNet
Global temperature prediction
Data-driven prediction of time series is significant in many scientific research fields such as global climate change and weather forecast. For global monthly mean temperature series, considering the strong potential of deep neural network for extracting data features, this paper proposes a data-driven model, ResGraphNet, which improves the prediction accuracy of time series by an embedded residual module in GraphSAGE layers. The experimental results of a global mean temperature dataset, HadCRUT5, show that compared with 11 traditional prediction technologies, the proposed ResGraphNet obtains the best accuracy. The error indicator predicted by the proposed ResGraphNet is smaller than that of the other 11 prediction models. Furthermore, the performance on seven temperature datasets shows the excellent generalization of the ResGraphNet. Finally, based on our proposed ResGraphNet, the predicted 2022 annual anomaly of global temperature is 0.74722 â—¦C, which provides confidence for limiting warming to 1.5 â—¦C above pre-industrial levels.





Introduction

The Global monthly mean temperature is a key climate indicator, which is usually expressed as a temperature â€˜â€˜anomalyâ€™â€™ that is just the difference from the average over a fixed period. Since 1850, as a typical time series, temperature series has been measured at weather stations, by ships and buoys and by satellites. The Paris Agreement is a legally binding international treaty on climate change, and its goal is to limit global warming to well around 2 â—¦C, preferable to 1.5 â—¦C, compared to pre-industrial levels. Thus, as a typical prediction problem of time series, it is significant to predict global mean temperature, which demonstrates the megatrend of global warming and climate changing. In the past few decades, prevalent climate models of the Earth have been proposed and worked well for weather and climate forecasting (Gordon et al., 2000; Stott and Kettleborough, 2002; Smith et al., 2007). However, not all significant physical and chemical pro- cesses can be explicitly resolved by these deterministic models (Bauer et al., 2015). Instead, in this study, the data-driven models (Pathak et al., 2018; Arcomano et al., 2020; Taylor et al., 2022) are focused on the near-term climate prediction (Kushnir et al., 2019) using time series of the global monthly mean temperature.
Nowadays, there are myriad data-driven methods to predict time series including global mean temperature series, such as the autore- gressive integrated moving average (ARIMA) (Contreras et al., 2003), the seasonal autoregressive integrated moving average with exogenous
(SARIMAX) (Guin, 2006), the random forest (Breiman, 2001), the support vector regression (Smola and SchÃ¶lkopf, 2004), the recurrent neural networks (RNN) (Hopfield, 1982), temporal convolutional net- work (Li et al., 2021), etc. In particular, as a deep learning model, the RNN and its variants, such as long-short term memory (LSTM) (Hochre- iter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Chung et al., 2014), have been widely applied in time series prediction in the Euclidean domain. However, different from many other time series,
ing accumulation of CO2 in the atmosphere as a result of fossil fuel the global temperature generally shows a rising trend due to increas-
consumption, which leads to some bottlenecks in the performance of traditional data-driven methods. For example, because the global mean temperature series show rich relational structure in the long sequences, the inherent architecture of the RNN limits its representation ability with slow and complex training procedures. To improve the accuracy and efficiency of prediction, the graph neural network (GNN) (Scarselli et al., 2009) is an emerging approach to expand the representation of artificial neural network with the help of relevant concepts of graph theory in the non-Euclidean domain.
The current GNN models are mainly categorized as recurrent GNNs, convolutional GNNs, graph autoencoders, and spatialâ€“temporal GNNs (Wu et al., 2021). As a pioneering work, Kipf and Welling proposed a simplified graph neural network model, graph convolution network

âˆ— Corresponding author.
E-mail address: emailwzg@mail.xjtu.edu.cn (Z. Wang).

https://doi.org/10.1016/j.aiig.2022.11.001
Received 28 October 2022; Received in revised form 26 November 2022; Accepted 26 November 2022
Available online 8 December 2022
2666-5441/Â© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




/ig. 1. (a) The global surface temperature dataset, HadCRUT5, includes a global mean temperature series from 1850 to 2022 in every month, and (b) its temperature anomalies distribution.


(GCN), that operates directly on a graph and induces embedding vec- tors of nodes based on the properties of their neighborhoods (Kipf and Welling, 2016). The major limitation of GCN is inherently transductive, GraphSAGE is an inductive framework that can quickly generate em- beddings and predict for unseen information (Hamilton et al., 2017). Whereas, due to over-smoothing problem, it is difficult to improve the predictive performance with stacked up many GraphSAGE layers. Meanwhile, as a classical deep convolution neural network (CNN) with strong feature extraction, the residual neural network (ResNet) helps in tackling the vanishing gradient problem using identity mappings (He et al., 2016). Thus, the embedded ResNet module also can act as a buffer layer to reduce the over-smoothing problem of the GraphSAGE layers (Oono and Suzuki, 2020).
Therefore, we propose a specific deep neural network, called Res- GraphNet, for the prediction of the global monthly mean temperature series. The ResGraphnet is an end-to-end architecture that consists of two GraphSAGE layers, an embedded ResNet layer and a full-connected (FC) layer. Meanwhile, the path graph is selected to represent the temperature time series, as the underlying topology of GraphSAGE. The subsequent experimental results show that compared with simply using the GraphSAGE or the ResNet as the network layers, our proposed ResGraphNet has a stronger accuracy and a generalization ability in predicting time series, especially monthly mean temperature series. Our source code is available at https://doi.org/10.5281/zenodo.7213337.
Compared with the previous models which has been commonly used in prediction of time series, our contributions of the proposed ResGraphNet mainly includes the following points:
To the best of our limited knowledge, this is the first study to model a temperature series as a path graph and learn the characteristics of the global temperature between the connection weights of each node in the GNN with embedded ResNet.
Results on several temperature time series datasets demonstrate that our method outperforms state-of-the-art temperature series methods, with possessing a shorter training time and faster convergence speed.
Our proposed ResGraphNet predicts that the 2022 annual anomaly of global temperature is 0.74722 â—¦C, which provides a confidence for limiting warming to 1.5 â—¦C above pre-industrial levels.
The rest of this paper is organized as follows. The Chapter II briefly introduces the basic concepts of the ResNet and the GraphSAGE, and then elaborates the architecture of the proposed ResGraphNet. The Chapter III shows experiments of datasets that reveal the performance of the proposed ResGraphNet. Finally, conclusions are drawn in the Chapter IV.

Data and method

Datasets of temperature time series

For understanding and predicting global surface temperature changes, a public time series HadCRUT5 (Morice et al., 2021), the global surface temperature dataset from 1850.Jan to 2022.Mar in every
because of increasing accumulation of CO2 in the atmosphere as a result month is analyzed mainly in this study. As shown in Fig. 1, since 1936,
of huge fossil energy consumption, the global temperature is increasing gradually (Arrow, 2007). This is a challenge to those methods that use past data to predict the future information (Dougherty et al., 2005; Hastings et al., 2014; Terando et al., 2014), because it is difficult to learn the drastic changes in temperature data in the last few years. Therefore, to tackle this problem, we build the novelty architecture of ResGraphNet.
In additional, ERSSTv4 (Smith et al., 2008), ERSSTv3b (Smith et al., 2008), Berkeley-Earth (Rohde et al., 2013), HadSST3 (Kennedy et al., 2011), and ERA5 (Hersbach et al., 2020) also are temperature data measured in different ways and in different areas. These datasets also are applied to verify the generalization ability of the proposed ResGraphNet.

Residual neural network

ResNet is a type of neural networks that applies identity mapping. It is known that simply stacking up CNN layers cannot indefinitely improve the performance of the CNN model, due to the vanishing gradient problem (Glorot and Bengio, 2010). To solve this problem effectively, the concept of a highway network is proposed as (Srivastava et al., 2015)
ğ‘¦ = T(ğ‘¥, ğ‘¤ğ‘–) â‹… ğœ(ğ‘¥, ğ‘¤ğ‘¡) + ğ‘¥ â‹… C(ğ‘¥, ğ‘¤ğ‘ )	(1)


information of node ğ‘£ğ‘– is ğ’™ğ‘–, then GraphSAGE updates node information by sampling and aggregating adjacent points of each node ğ‘£ğ‘–, that is:
ğ’™â€² = ğ‘¾ 1ğ’™ğ‘– + ğ‘¾ 2 â‹… ğ‘šğ‘’ğ‘ğ‘›ğ‘—âˆˆğ‘ (ğ‘—)ğ’™ğ‘—	(4)
where ğ‘ (ğ‘—) is the set of adjacent points of a node ğ‘£ğ‘–, which can be obtained by the adjacency matrix ğ‘¨ of a graph ğº; ğ‘¾ 1 and ğ‘¾ 2 are the learnable network weights; ğ‘šğ‘’ğ‘ğ‘› demonstrates that the aggregation
method adopts the mean aggregation. Fig. 3 shows the running process of the GraphSAGE. For the random sampling process of nodes, the number of the first-order sampling nodes is 1 and the number of the second-order sampling nodes is 1. As shown in Fig. 3(b), the node aggregation takes the mean value after weighted summation of the information of adjacent points. Then, we update the information of the current central node, as shown in Fig. 3(c).

2.4. Proposed ResGraphNet

mension, we propose that the ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡ module is embedded in two To pay attention to rich correlation in the temperature time di-
network are ğ’™ and ğ’š, its forward propagation can be expressed as: GraphSAGE layers. Assuming that the input and the output of the
â§ğ‘“1 = ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸1(ğ’™, ğ‘¨)

âªâ¨ğ‘“2 = ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(ğ‘“1))
â© ğ’šÌ‚ = ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸2(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(ğ‘“2), ğ‘¨)
(5)



/ig. 2. The architecture of a ResNet module that is applied in this study.

where ğ‘¥ and ğ‘¦ are the input and output of the layers aimed, and the
ğœ(ğ‘¥, ğ‘¤ğ‘¡) and C(ğ‘¥, ğ‘¤ğ‘ ) are the transform gate and the carry gate, respec-
tively. For simplifying parameters and reducing the risk of over fitting,
the ResNet set these two gates as the identical mapping(i.e., ğœ(ğ‘¥, ğ‘¤ ) =
where ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸ğ‘– is the ğ‘–th GraphSAGE layer, ğ‘– = 1, 2. ğ‘¨ is the
adjacency matrix of the graph.
Here, notice that the GraphSAGE layers (including most other GNN
adjacency matrix ğ‘¨). For a specific time series, it is an open topic to layers) need a given topology structure of a graph in advance (i.e. the
build an available graph (Lacasa et al., 2008, 2009; Wang et al., 2019;
and causality of time series, we consider adjacency matrix ğ‘¨ğ‘ğ‘ğ‘¡â„ of the Chen et al., 2022). In this study, to preserve the inherent correlation

1	ğ‘¡	path graph which satisfies

, C(ğ‘¥, ğ‘¤ğ‘ ) = 1) (He et al., 2016), then the forward propagation formula
is
ğ‘¦ = T(ğ‘¥, ğ‘¤ğ‘–) + ğ‘¥	(2)
where the function T(ğ‘¥, ğ‘¤ğ‘–) represents the residual mapping to be
learned. As shown in Fig. 2, in this study, the ResNet module is mainly
composed of 4 CNN layers, which can be expressed as
0	1	0	â‹¯	0	0
0	0	1	â‹¯	0	0
ğ‘¨ğ‘ğ‘ğ‘¡â„ = â¢0	0	0	â‹¯	0	0â¥
â¢0	0	0	â‹¯	0	1â¥
â£0	0	0	â‹¯	0	0â¦




(6)

â§â„1  =  ğ¶ğ‘ğ‘1(ğ‘¥) â„2 = ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(â„1)
âªâ„3 = ğ¶ğ‘ğ‘2(â„2)
â¨â„4 = ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(â„3)
âªâ„5 = ğ¶ğ‘ğ‘3(â„3)

(3)
For the path graph, each node ğ‘£ğ‘– has an edge ğ‘’ğ‘–,ğ‘–+1 pointing to the next node ğ‘£ğ‘–+1, and the time series prediction task studied in this paper is to
utilize the past data to predict future data. Based on this idea, a node
ğ‘£ğ‘¡ can be constructed for each time ğ‘¡, set ğ’™ = [ğ‘¥0, ğ‘¥1, â€¦ , ğ‘¥ğ¿âˆ’1]ğ‘‡ , ğ‘¥ğ‘¡ âˆˆ Rğ‘€Ã—1, ğ‘¡ = 0, 1, â€¦ , ğ¿ âˆ’ 1, then ğ‘¥ğ‘¡ is the value of node ğ‘£ğ‘¡, ğ‘¨ğ‘ğ‘ğ‘¡â„ âˆˆ Rğ¿Ã—ğ¿. So the role of ğ‘¨ğ‘ğ‘ğ‘¡â„, is to make the value ğ‘¥ğ‘¡ of time series ğ’™ at time ğ‘¡

âªâ„6
âª
= â„5
+ â„1
(the value of node ğ‘£ğ‘¡) be related to the value ğ‘¥ğ‘¡+1 of next time ğ‘¡ + 1 (the
value of node ğ‘£ğ‘¡+1).

â© ğ‘¦ = ğ¶ğ‘ğ‘4(â„6)
where ğ‘¥ and ğ‘¦ is the input and output of our ResNet module (Fig. 2) and ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ is the dropout layer. In the following content, the ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡
also denotes the forward propagation process in Eq. (3).
Consequently, to further alleviate the gradient disappearance prob- lem, a FC layer is applied to mitigate the instability of results due to the residual connection. From Fig. 4, the proposed ResGraphNet can be written as
â§ğ‘“1 = ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸1(ğ’™, ğ‘¨)

GraphSAGE neural network
In this study, a graph is defined as ğº = (ğ‘‰ , ğ¸), where ğ‘‰  =
{ğ‘£0, ğ‘£1, â€¦ , ğ‘£ğ‘ âˆ’1} denotes the nodes and ğ¸ denotes the edges. ğ´ âˆˆ Rğ‘Ã—ğ‘
âªğ‘“2 = ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(ğ‘“1))
âªğ‘“3 = ğ¹ ğ¶(ğ‘“1 + ğ‘“2)
â© ğ’šÌ‚ = ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸2(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡(ğ‘“3), ğ‘¨)
(7)

is the adjacency matrix. If there is an edge from ğ‘£ğ‘– to ğ‘£ğ‘— , that is
âˆƒ(ğ‘£ğ‘–, ğ‘£ğ‘— ) âˆˆ ğ¸, then ğ´ğ‘–ğ‘— = 1 otherwise ğ´ğ‘–ğ‘— = 0. And then the degree matrix ğ· is defined as ğ·ğ‘–ğ‘— = âˆ‘ğ‘ âˆ’1 ğ´ğ‘–ğ‘— . Thus, the Laplacian matrix ğ¿ is defined as ğ¿ = ğ· âˆ’ ğ´. In addition, the normalized form of Laplacian matrix G = ğ·âˆ’1âˆ•2ğ¿ğ·âˆ’1âˆ•2 is often used in practical applications. If data
will be learned by the shallow layers (ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸), and more detailed Ideally, for the proposed ResGraphNet, the main features of time series features will be extracted by the deeper module (ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡). To verify
performance of the ResGraphNet, the experimental results are shown in Chapter 3.




/ig. 3. The operation process of the GraphSAGE, involves (a) the sampling process, (b) the aggregation process and (c) the updating nodes.
test dataset ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡ = [ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡, ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡, â€¦ , ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡	] can be constructed for ğ‘«ğ‘¡ğ‘’ğ‘ ğ‘¡,

ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
0	1	ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡ âˆ’1
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘¡ğ‘’ğ‘ ğ‘¡

where ğ‘š
and ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡ are the number of samples of ğ‘«
and ğ‘«	,

respectively. Meanwhile, ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡ can be divided into training
data ğ’™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, training labels ğ’šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, test data ğ’™ğ‘¡ğ‘’ğ‘ ğ‘¡ and test labels ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡. The
construction process of samples is shown in Fig. 5.

3.2. Parameters and indicator
For any sample ğ‘ ğ‘¡, ğ‘¥ğ‘¡ is the input of the model and ğ‘¦Ì‚ğ‘¡ denotes the corresponding output. The loss function of the proposed model is the
mean square error and the network weights are updated by the negative direction of the gradient decline of the loss function.
For regression problems, the Root Mean Square Error (RMSE) is generally selected as the performance evaluation indicator (Bellocchio et al., 2012). However, the RMSE is easily affected by the data itself. If there are great differences in the value distribution of different time series, the magnitude of their RMSE may be completely different, which makes it impossible to compare the testing results of these time series.
as R2 score) as our indicator. For any model, we denote the input data Therefore, we further apply the determination coefficient (also named and predicted results as ğ’™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ’šÌ‚ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› respectively, then we set ğ’™ğ‘¡ğ‘’ğ‘ ğ‘¡ and ğ’šÌ‚ğ‘¡ğ‘’ğ‘ ğ‘¡ for test set. Their R2 score is defined as rğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, rğ‘¡ğ‘’ğ‘ ğ‘¡:
â§	âˆ‘ (ğ‘¦Ì‚ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› âˆ’ ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›)2

âªrğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› = âˆ‘ğ‘–
ğ‘–
(ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› âˆ’ ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›)2

ğ‘–  ğ‘–
âª	(ğ‘¦Ì‚ğ‘¡ğ‘’ğ‘ ğ‘¡ âˆ’ ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡)2
(8)


/ig. 4. Architecture of the proposed ResGraphNet is defined in Eq. (7), where the
ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡ module is defined in Eq. (3).
rğ‘¡ğ‘’ğ‘ ğ‘¡ =	ğ‘–
â©	ğ‘–
ğ‘–
(ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡ âˆ’ ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡)2
âˆ‘ ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
âˆ‘ ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡

where ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› =   ğ‘–   , ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡ =   ğ‘–  are mean values of the ğ’šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and the
ğ‘šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›	ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡

Experiment and result

This chapter first shows the training dataset and the test dataset based on real time series data. Then, we build models based on different algorithms and set the parameters of these different models. Finally, the training dataset are applied to train each model, while the test dataset are used to evaluate the performance of models.

Dataset samples

CRUT5. At the beginning, if we set the length of time series as ğ‘™, then Now we build training datasets and test datasets based on Had- it can be denoted as ğ‘« = [ğ‘‘0, ğ‘‘1, â€¦ , ğ‘‘ğ‘™âˆ’1]. The first 50% of the data are selected as the training dataset. ğ‘«ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and the last 50% are selected as the test dataset ğ‘«ğ‘¡ğ‘’ğ‘ ğ‘¡, whose lengths are ğ‘™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ‘™ğ‘¡ğ‘’ğ‘ ğ‘¡ respectively.
use the data before time ğ‘¡ to predict the data at time ğ‘¡ and the error The target task of this paper is to predict time series. We hope to
possible. To achieve this purpose, for each time ğ‘¡, we set the input data between the prediction result and the real data will be as small as
ğ‘¥ğ‘¡ = [ğ‘‘ğ‘¡âˆ’ğ‘™ğ‘¥ , ğ‘‘ğ‘¡âˆ’ğ‘™ğ‘¥ +1, â€¦ , ğ‘‘ğ‘¡âˆ’1] and the input label ğ‘¦ğ‘¡ = [ğ‘‘ğ‘¡], and each pair of
ğ‘¥ğ‘¡ and ğ‘¦ğ‘¡ constitute a sample ğ‘ ğ‘¡, then ğ‘ ğ‘¡ = {ğ‘¥ğ‘¡, ğ‘¦ğ‘¡}, where ğ‘™ğ‘¥ is the length of input data ğ‘¥. In other words, we use ğ‘™ğ‘¥ data before time ğ‘¡ to predict the data at time ğ‘¡. According to the above steps, the training dataset
ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡, respectively. If ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ of one model is closer to 1, this model has a
stronger time series prediction ability.

Performances on different models

Now we train all the models based on ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, and use ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡ for performance testing where we set ğ‘™ğ‘¥ = 60 and ğ‘™ğ‘¦ = 1. In Table 1,
there are 12 models are applied, including the proposed ResGrapNet, the ResNet, the GraphSAGE, the GNN with Unified Message Passaging Model (UniMP) (Shi et al., 2020), the GCN (Kipf and Welling, 2016), the Graph Isomorphic Network (GIN) (Morris et al., 2019), the RNN with LSTM, the RNN with GRU, the Random Forest Regression (Forest), the Linear Regression (Linear), the ARIMA and the SARIMAX. We
calculate R2 score of each model, as shown in Table 1. Using climate
spirals, we draw the true data and predicted results of the ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, and the true data and predicted results of the ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡ of 12 models, as shown in Fig. 6. From Table 1, the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ of the proposed ResGraphNet is 0.9980,
which is the best in all of 12 models. As shown in Fig. 6, the predicted climate spirals of the proposed ResGraphNet is closest to the true data,
but the prediction of the Forest is the worst with the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ = 0.6397.
Moreover, because the time series is too long, it is not intuitive to directly observe the curve between the prediction results and the true
data. Hence, we define the error index ğ’† = [ğ‘’0, ğ‘’1, â€¦ , ğ‘’ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡ âˆ’1] as

ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› = [ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, â€¦ , ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
] can be constructed for ğ‘«ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and the
ğ’† = ğ’šÌ‚ğ‘¡ğ‘’ğ‘ ğ‘¡ âˆ’ ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡	(9)

0	1	ğ‘šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› âˆ’1



/ig. 5. Samples construction process of the time series (When ğ‘™ğ‘¥ = 3 and ğ‘™ğ‘¦ = 1).


Table 1
R2 scores calculated by different models.
Table 3
ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ in different ğ‘™ğ‘¦ and different models.



Table 2
Training time of different models.


Model	Time[s]
ResGraphNet	78.78
ResNet	123.57
GraphSAGE	11.87
LSTM	149.08
GRU	126.70
Forest	1.17
Linear	0.04





Then we can count and plot the frequency distribution of all elements
ğ‘’ğ‘– in the ğ’† as shown in Fig. 7. Most of the errors ğ‘’ğ‘– computed by the ResGraphNet are distributed within the interval [âˆ’0.03, 0.06], but errors
ğ‘’ğ‘– of other models are outside this interval. In other words, the error ğ‘’ğ‘–
computed by the ResGraphNet is smaller than that of other models.
Furthermore, to compare the computation time, we record the time spent on training different models, as shown in Table 2. From Table 2 it turns out that ResGraphNet can indeed converge faster than ResNet
and RNN with LSTM and GRU. Due to the addition of ğ‘…ğ‘’ğ‘ ğ‘ ğ‘’ğ‘¡ module,
the training time of ResGraphNet is longer than that of GraphSAGE. Because the traditional machine learning including Forest and Linear adopts a simpler calculation process than the Neural Network, the running time of them is also quite shorter. In this Scenario, the pro- posed ResGraphNet makes a certain and acceptable sacrifice for the computation time, in exchange for the highest prediction accuracy.
In the previous analysis, we set ğ‘™ğ‘¥ = 60 and ğ‘™ğ‘¦ = 1. Actually, we
the length of predicted time series is becoming longer. The results ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ hope that ResGraphNet can still maintain its good performance, when based on different ğ‘™ğ‘¦ and different models are shown in Table 3. ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ decreases with the increase of ğ‘™ğ‘¦, but ResGraphNet outperforms other
models. In other words, the length of time series, that can be predicted
of 12 months (i.e. ğ‘™ğ‘¦ = 12) in 2021 year is expected to be predicted, the by the ResGraphNet, is limited in a way. When the temperature change indicator ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ between ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡ and ğ’šÌ‚ğ‘¡ğ‘’ğ‘ ğ‘¡ can remain above 0.98. Therefore,
series     ğ‘™ğ‘¦      =    12     is     available. for the prediction of future temperature, the length of predicted time





Linear,         (l)         ARIMA,         (m)         SARIMAX         models         in         ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡. /ig. 6. Climate spirals of (a) true data and predicted results by (b) ResGraphNet, (c) ResNet, (d) GraphSAGE, (e) UniMP, (f) GCN (g) GIN, (h) LSTM, (i) GRU, (j) Forest, (k)


Prediction of future temperature

To show the prediction performance of the ResGraphNet for un- known future temperature, we firstly list the known true data and pre- dicted results of 2021 in Table 4. As shown in Table 4, for HadCRUT5 dataset, the predicted result of annual mean temperature (0.76863 â—¦C) is close to the measured value (0.76185 â—¦C). Similarly, the tempera- tures in northern hemisphere HadCRUT5-N and southern hemisphere HadCRUT5-S are also tested.
sponding ğ‘šğ‘ ğ‘’ between the predicted results and measured data in 2022 Next, Table 5 and Fig. 8 show the results in 2022 year. The corre-
January, February and March are 0.0015, 0.0109, 0.0048 respectively, as well as the high similarity of annual temperature. Thus, the results of prediction show the proposed ResGraphNet can accurately predict the unknown temperature data in the future to some extent. Note that,
based on our proposed model, the predicted 2022 annual anomaly of global temperature is 0.74722 â—¦C, which is smaller than that of 2021. The predicted results of 2022 provide more confident to limit global warming to well below 2 â—¦C, preferably to 1.5 â—¦C above pre-industrial levels (red circle in Fig. 8).

Other datasets

Finally, to verify the generalization ability of the proposed Res- GraphNet, we test the prediction performance of models based on other different public datasets, including HadCRUT5-N, HadCRUT5-S, ERSSTv4, ERSSTv3b, Berkeley-Earth, HadSST3 and ERA5. Meanwhile, we additionally consider some other time-series prediction models,
i.e. the GraphSAGE, the Linear, the GRU, the ARIMA and the SARIMAX,





/ig. 7. The distributions of error ğ‘’ğ‘– based on (a) ResGraphNet, (b) ResNet, (c) GraphSAGE, (d) UniMP, (e) GCN (f) GIN, (g) LSTM, (h) GRU, (i) Forest, (j) Linear, (k) ARIMA, (l) SARIMAX models, where marked red and green bars represent different ğ‘’ğ‘– intervals.




/ig. 8. The temperature of true data and predicted (a) HadCRUT5, (b) HadCRUT5-N, and (c) HadCRUT5-S results in 2022, where the white-dashed-lines are â€˜â€˜Predictâ€™â€™ and the yellow-solid-lines are â€˜â€˜Knownâ€™â€™. Annual predicted results provide more confident to limit warming to below 1.5 â—¦C (red circle).


as the comparison methods. Table 6 reveals that the performance of the proposed ResGraphNet is the best among six prediction models.
When applied to seven different datasets, the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ calculated by
using the proposed ResGraphNet is still closer to 1 than that of other models. Therefore, the proposed ResGraphNet has a strong general- ization ability in temperature prediction, and has potential to predict temperature data in many scenarios.
Conclusion

This paper proposes a specific ResGraphNet that consists of two GraphSAGE layers, an embedded ResNet layer and a FC layer, which is an end-to-end architecture for time series prediction of the global monthly mean temperature. Based on some global mean temperature datasets, the performance of the proposed ResGraphNet is evaluated.



Table 4
The true and predicted temperature values in 2021.


Month	HadCRUT5	HadCRUT5-N	HadCRUT5-S



Table 5
The true and predicted temperature values in 2022.


Month	HadCRUT5	HadCRUT5-N	HadCRUT5-S



Table 6
For seven datasets, the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡ scores are calculated by different models.



The results of the comparison methods show that the proposed Res- GraphNet has the best accuracy and generalization to predict monthly anomaly of temperature series. In addition, the predicted 2022 annual anomaly of global mean temperature is 0.74722 â—¦C, which helps us understand the trend of global warming of 1.5 â—¦C above pre-industrial levels.
However, there are still two problems that have not been well solved in this paper. First, there is still no better method to choose the best graph structure for specific data such as temperature time series. Second, this paper does not solve the problem of long-term prediction of temperature series. Therefore, We hope to further improve the performance of the current model.
Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.



/unding

This work was supported by the National Natural Science Founda- tion of China under Grant 41974137.
References

Arcomano, T., Szunyogh, I., Pathak, J., Wikner, A., Hunt, B.R., Ott, E., 2020. A machine learning-based global atmospheric forecast model. Geophys. Res. Lett. 47 (9), e2020GL087776.
Arrow, K.J., 2007. Global climate change: A challenge to policy. Econ. Voice 4 (3), http://dx.doi.org/10.2202/1553-3832.1270.
Bauer, P., Thorpe, A., Brunet, G., 2015. The quiet revolution of numerical weather prediction. Nature 525 (7567), 47â€“55.
Bellocchio, F., Ferrari, S., Piuri, V., Borghese, N.A., 2012. Hierarchical approach for multiscale support vector regression. IEEE Trans. Neural Netw. Learn. Syst. 23 (9), 1448â€“1460.
Breiman, L., 2001. Random forests. Mach. Learn. 45 (1), 5â€“32.
Chen, Z., Wang, Z., Wu, S., Wang, Y., Gao, J., 2022. MagInfoNet: Magnitude estimation using seismic information augmentation and graph transformer. Earth Space Sci. 9 (12), http://dx.doi.org/10.1029/2022EA002580, e2022EA002580.
Chung, J., Gulcehre, C., Cho, K., Bengio, Y., 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555. Contreras, J., Espinola, R., Nogales, F., Conejo, A., 2003. ARIMA models to predict next-day electricity prices. IEEE Trans. Power Syst. 18 (3), 1014â€“1020. http:
//dx.doi.org/10.1109/TPWRS.2002.804943.
Dougherty, M.R., Scheck, P., Nelson, T.O., Narens, L., 2005. Using the past to predict the future. Memory Cogn. 33 (6), 1096â€“1115.
Glorot, X., Bengio, Y., 2010. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings,
pp. 249â€“256.
Gordon, C., Cooper, C., Senior, C.A., Banks, H., Gregory, J.M., Johns, T.C., Mitchell, J.F., Wood, R.A., 2000. The simulation of SST, sea ice extents and ocean heat transports in a version of the Hadley centre coupled model without flux adjustments. Clim. Dynam. 16 (2), 147â€“168.
Guin, A., 2006. Travel time prediction using a seasonal autoregressive integrated moving average time series model. In: 2006 IEEE Intelligent Transportation Systems Conference. pp. 493â€“498. http://dx.doi.org/10.1109/ITSC.2006.1706789.
Hamilton, W., Ying, Z., Leskovec, J., 2017. Inductive representation learning on large graphs. Adv. Neural Inf. Process. Syst. 30.
Hastings, S.N., Whitson, H.E., Sloane, R., Landerman, L.R., Horney, C., Johnson, K.S., 2014. Using the past to predict the future: Latent class analysis of patterns of health service use of older adults in the emergency department. J. Am. Geriatr. Soc. 62 (4), 711â€“715.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog- nition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770â€“778.
Hersbach, H., Bell, B., Berrisford, P., Hirahara, S., HorÃ¡nyi, A., MuÃ±oz-Sabater, J., Nicolas, J., Peubey, C., Radu, R., Schepers, D., et al., 2020. The ERA5 global reanalysis. Q. J. R. Meteorol. Soc. 146 (730), 1999â€“2049.
Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8), 1735â€“1780. http://dx.doi.org/10.1162/neco.1997.9.8.1735.
Hopfield, J.J., 1982. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. 79 (8), 2554â€“2558.
Kennedy, J.J., Rayner, N.A., Smith, R.O., Parker, D.E., Saunby, M., 2011. Reassessing biases and other uncertainties in sea surface temperature observations measured in situ since 1850: 1. Measurement and sampling uncertainties. J. Geophys. Res.: Atmos. 116 (D14), http://dx.doi.org/10.1029/2010JD015218.
Kipf, T.N., Welling, M., 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.
Kushnir, Y., Scaife, A.A., Arritt, R., Balsamo, G., Boer, G., Doblas-Reyes, F., Hawkins, E., Kimoto, M., Kolli, R.K., Kumar, A., et al., 2019. Towards operational predictions of the near-term climate. Nature Clim. Change 9 (2), 94â€“101.
Lacasa, L., Luque, B., Ballesteros, F., Luque, J., Nuno, J.C., 2008. From time series to complex networks: The visibility graph. Proc. Natl. Acad. Sci. 105 (13), 4972â€“4975. Lacasa, L., Luque, B., Luque, J., Nuno, J.C., 2009. The visibility graph: A new method for estimating the Hurst exponent of fractional Brownian motion. Europhys. Lett.
86 (3), 30001.
Li, J., Wu, Y., Li, Y., Xiang, J., Zheng, B., 2021. The temperature prediction of hydro-generating units based on temporal convolutional network and recurrent neural network. In: 2021 40th Chinese Control Conference. CCC, pp. 8228â€“8233. http://dx.doi.org/10.23919/CCC52363.2021.9549853.
Morice, C.P., Kennedy, J.J., Rayner, N.A., Winn, J., Hogan, E., Killick, R., Dunn, R., Osborn, T., Jones, P., Simpson, I., 2021. An updated assessment of near-surface temperature change from 1850: The HadCRUT5 data set. J. Geophys. Res.: Atmos. 126 (3), e2019JD032361.
Morris, C., Ritzert, M., Fey, M., Hamilton, W.L., Lenssen, J.E., Rattan, G., Grohe, M., 2019. Weisfeiler and leman go neural: Higher-order graph neural networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, no. 01. pp. 4602â€“4609.
Oono, K., Suzuki, T., 2020. Graph neural networks exponentially lose expressive power for node classification. In: 8th International Conference on Learning Representations. ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.



Pathak, J., Hunt, B., Girvan, M., Lu, Z., Ott, E., 2018. Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach. Phys. Rev. Lett. 120 (2), 024102.
Rohde, R., Muller, R., Jacobsen, R., Perlmutter, S., Rosenfeld, A., Wurtele, J., Curry, J., Wickham, C., Mosher, S., 2013. Berkeley earth temperature averaging process. Geoinform. Geostat. An Overview 1 (2), 1â€“13.
Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G., 2009. The graph neural network model. IEEE Trans. Neural Netw. 20 (1), 61â€“80. http://dx.doi.org/ 10.1109/TNN.2008.2005605.
Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., Sun, Y., 2020. Masked label prediction: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509.
Smith, D.M., Cusack, S., Colman, A.W., Folland, C.K., Harris, G.R., Murphy, J.M., 2007. Improved surface temperature prediction for the coming decade from a global climate model. Science 317 (5839), 796â€“799.
Smith, T.M., Reynolds, R.W., Peterson, T.C., Lawrimore, J., 2008. Improvements to NOAAâ€™s historical merged landâ€“ocean surface temperature analysis (1880â€“2006).
J. Clim. 2283â€“2296.
Smola, A.J., SchÃ¶lkopf, B., 2004. A tutorial on support vector regression. Stat. Comput.
14 (3), 199â€“222.
Srivastava, R.K., Greff, K., Schmidhuber, J., 2015. Highway networks. arXiv preprint arXiv:1505.00387.
Stott, P.A., Kettleborough, J.A., 2002. Origins and estimates of uncertainty in predictions of twenty-first century temperature rise. Nature 416 (6882), 723â€“726. Taylor, J.A., Larraondo, P., de Supinski, B.R., 2022. Data-driven global weather predictions at high resolutions. Int. J. High Perform. Comput. Appl. 36 (2),
130â€“140. http://dx.doi.org/10.1177/10943420211039818.
Terando, A.J., Costanza, J., Belyea, C., Dunn, R.R., McKerrow, A., Collazo, J.A., 2014. The southern megalopolis: using the past to predict the future of urban sprawl in the southeast US. PLoS One 9 (7), e102261.
Wang, Z., Zhang, B., Gao, J., 2019. How to transform the seismic time series to a graph? In: SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists, pp. 3459â€“3463.
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Yu, P.S., 2021. A comprehensive survey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32 (1), 4â€“24. http://dx.doi.org/10.1109/TNNLS.2020.2978386.
