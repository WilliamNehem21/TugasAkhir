Applied Computing and Informatics 15 (2019) 81–93








Deep belief networks and cortical algorithms: A comparative study for supervised classification
Yara Rizk, Nadine Hajj, Nicholas Mitri, Mariette Awad ⇑
Department of Electrical and Computer Engineering, American University of Beirut, Beirut, Lebanon



a r t i c l e  i n f o 

Article history:
Received 23 March 2017
Revised 17 January 2018
Accepted 17 January 2018
Available online 3 March 2018

Keywords:
Deep learning
Deep belief networks Cortical algorithms
a b s t r a c t 

The failure of shallow neural network architectures in replicating human intelligence led the machine learning community to focus on deep learning, to computationally match human intelligence. The wide availability of increasing computing power coupled with the development of more efficient training algo- rithms have allowed the implementation of deep learning principles in a manner and span that had not been previously possible. This has led to the inception of deep architectures that capitalize on recent advances in artificial intelligence and insights from cognitive neuroscience to provide better learning solutions. In this paper, we discuss two such algorithms that represent different approaches to deep learning with varied levels of maturity. The more mature but less biologically inspired Deep Belief Network (DBN) and the more biologically grounded Cortical Algorithms (CA) are first introduced to give readers a bird’s eye view of the higher-level concepts that make up these algorithms, as well as some of their technical underpinnings and applications. Their theoretical computational complexity is then derived before comparing their empirical performance on some publicly available classification datasets. Multiple network architectures were compared and showed that CA outperformed DBN on most datasets, with the best network architecture consisting of six hidden layers.
© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Contents
Introduction	82
Artificial neural networks history	82
Concepts from neuroscience	82
Shallow beginnings	83
Shallow networks’ limitations	83
Deep architectures	83
Deep belief networks	84
Overview	84
Network structure	84
Restricted Boltzmann machines	84
Deep belief networks	84
Training algorithm.	85
Restricted Boltzmann machines	85
Deep belief networks	85

* Corresponding author.
E-mail addresses: yar01@aub.edu.lb (Y. Rizk), njh05@aub.edu.lb (N. Hajj), ngm04@aub.edu.lb (N. Mitri), mariette.awad@aub.edu.lb (M. Awad).
Peer review under responsibility of King Saud University.


https://doi.org/10.1016/j.aci.2018.01.004
2210-8327/© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Cortical algorithms	85
Overview	85
Network structure	85
Mathematical model	86
Training algorithm.	86
Random initialization	86
Unsupervised feed-forward learning	86
Supervised feedback learning	87
Theoretical computational complexity	87
Number of non-zero weights	88
DBN	88
CA	88
Number of operations per neuron	88
Summation	88
Activation function	88
Pruning	89
Overall computational complexity	89
Empirical comparison	89
Experimental setup	89
Classification results	91
Network connectivity	91
Effect of batch size	91
Statistical analysis	91
Conclusion	92
Acknowledgment	92
References	92




Introduction

In an endeavor to replicate human level intelligence, artificial intelligence (AI) research has fused insights from the fields of com- puter science, cognitive neuroscience, computational science, and a litany of others to produce algorithms that perform with increasing efficacy on what is arguably the core element of intelligence: learning.
Notable among the many learning algorithms in AI are artificial neural networks (ANN) and their many variants. ANN are collec- tions of interconnected artificial neurons that incrementally learn from their environment and attempt to mimic some of the basic information processing processes in the brain. Their function is defined by the processing performed at the neuron level, the con- nection strengths between neurons (synaptic weights), and net- work structure (organization and linkage of neurons) [1]. It is the latter that resides at the core of the discussion presented herein.
Throughout their evolution, discussed in more details in the next section, shallow ANN still suffer from multiple issues in the context of complex applications requiring a higher level of abstrac- tion. However, with the rapid increase in processing power, the opportunity to successfully implement the computationally demanding designs of deeper architectures has recently emerged. The development of efficient training algorithm such as Hinton et al.’s greedy algorithm [2] has also helped ANN’s resurgence. Furthermore, findings in computational neuroscience have led to increased interest in deep, biologically inspired architectures [3–5] which adhere more faithfully to neuro-scientific theories of the human brain’s topology.
In this paper, we limit the scope of our comparative study to two - nowadays popular - algorithms: Hinton et al.’s Deep Belief Networks (DBN) [2], and Cortical Algorithms (CA) [6]. While many other deep architectures have been developed, including long short-term memory for sequential data processing and convolu- tional neural networks for image processing, this comparative study compares feedforward architectures. Specifically, DBN, one of the more efficient deep architecture training algorithms is com- pared to CA, a feedforward architecture with more biologically faithful properties. Deep neural networks (DNN), specifically



DBN, is presented as the state of the art of ANN in their traditional forms with network topologies built from layers of neuron models but with more advanced learning mechanics and deeper architec- ture, without modeling the detailed biological phenomena consti- tuting human intelligence. Maintaining a high-level abstraction of the biological modeling, results in simpler mathematical models for DBN compared to CA. On the other hand, CA represents the shift towards incorporating more biologically inspired structures than DBN, like cortical columns and inhibiting and strengthening learn- ing rules, as outlined by Edelman and Mountcastle’s work [7].
The structure of the paper is such that Section 2 summarizes the history of ANN while Sections 3 and 4 review the fundamental con- cepts and learning schemes of DBN and CA, respectively. Section 5 derives both algorithms’ theoretical computational complexity. Finally, Section 6 presents an empirical comparison on classifica- tion tasks before concluding with closing remarks in Section 7.

Artificial neural networks history

Before delving into the deeper network structures presented in this paper, we will go over the evolution of neural networks from their shallow beginnings to the complex structures that have recently become popular.

Concepts from neuroscience

Despite the advances in neuroscience and technology that have allowed for a detailed description of the structure of the brain, the learning process in the brain is yet to be completely understood. Biologically, the brain mainly consists of the cerebrum, the cerebel- lum, and the brain stem [8].
The cerebral cortex, biologically defined as the outer layer of tis- sue in the cerebrum and believed to be responsible for higher order functioning, is an association of an estimated 25 billion neurons interconnected through thousands of kilometers of axons propa-
gating and spreading about 1014 synapses simultaneously [9], arranged in six layers and divided into regions, each performing a specific task [10].



Though it is not very clear how certain areas in the brain become specialized, it is known that multiple factors affect the functional specialization of the brain areas such as structure, con- nectivity, physiology, development and evolution [11]. Neurons, considered the basic element in the brain, have different shapes and sizes but are all variations of the same underlying scheme,
i.e. they start the same general-purpose function but become spe- cialized with training [12]. While dendrites are the site of reception of synaptic inputs, axons convey electrical signals over long dis- tances. Inputs to neurons cause a slow potential change in the state of the neuron; its characteristics are determined by the membrane capacitance and resistance allowing temporal summation [13].
Studies showed that the organization of the cortex can be regarded as an association of columnar units [14,15], each column being a group of nodes sharing the same properties. Learning in the human brain is mainly performed using plastic connections, repeated exposures and firing and inhibition of neurons. In a sim- plified manner, information flowing in the cortex causes connec- tions in the brain to become active, over time, with repeated exposures these connections are strengthened creating a represen- tation of the information processed in the brain. Moreover, inhibi- tion of neurons - physically defined as prohibiting neurons from firing - partly account for the forgetting process [16].

Shallow beginnings

At a nodal level, ANN started with the simplified McCulloch- Pitts neural model (1943) [17], which was composed of a basic summation unit with a deterministic binary activation function. Successors added complexity with every iteration. At the level of activation functions, linear, sigmoid, and Gaussian functions came into use. Outputs were no longer restricted to real values and extended to the complex domain. Deterministic models gave way to stochastic neurons and spiking neurons which simulated ionic exchanges. All these additions were made to achieve more sophis- ticated learning models.
At the network level, topologies started out with single layered architectures such as Rosenblatt’s perceptron (1957) [18], Widrow and Hoff’s ADALINE network (1960) [19] and Aizerman’s kernel perceptron (1964) [20]. These architectures suffered from poor performance and could not learn the XOR problem, a simple but non-linear binary classification problem. This led to the introduc- tion of more complex networks starting with the multilayer per- ceptron (Rumelhart, 1986) [21], self-recurrent Hopfield networks (1986) [22], self-organizing maps (SOM or Kohonen networks, 1986) [23], adaptive resonance theory (ART) networks (1980s)
[24] and various others which are considered shallow architectures due to the small number of hidden layers.
Successive iterations incrementally improved on their prede- cessors’ shortcomings and promised higher levels of intelligence, a claim that was made partially feasible due to the hardware’s improved computational capabilities [25] and due to the develop- ment of faster and more efficient training and learning algorithms. Learning mechanics, whether supervised (back propagation) or unsupervised (feed forward algorithms), matured in parallel and allowed for better performance in a varied set of specific tasks. Nonetheless, the compound effect of the innovation targeting all aspects of these shallow networks was not enough to capture true human intelligence while large computational needs throttled the progress of deeper networks.

Shallow networks’ limitations

Supervised learning presents many challenges including the curse of dimensionality [26] where the increase in the number of  features  and  training  samples  makes  learning  more
computationally demanding. Furthermore, non-linear data is more difficult to divide into classes due to the inherent feature overlap. Unable to position themselves as strong AI models - general intel- ligent acts as defined by Kurzweil - which can faithfully emulate human intelligence, ANN lagged Support Vector Machines (SVM)
[27] in the 1990–2000s.

Deep architectures

The early 2000s saw a resurgence in ANN research due to increased processing power and the introduction of more effi- cient training algorithms which made training deep architec- tures feasible. Hinton et al.’s greedy training algorithm [2] simplified the training procedure of Boltzmann machines while deep stacking networks broke down training to the constitut- ing blocks of the deep network to reduce the computational burden. Furthermore, Schmidhuber’s long short-term memory architecture [28] allowed the training of deeper recurrent neu- ral networks. While these architectures do not borrow biologi- cal properties from the brain beyond the neuron, deep architectures with neural network topologies that adhere more faithfully to neuro-scientific theories of the human brain’s topology are gaining traction in the connectionist community due in part to the momentum achieved in computational neuroscience.
One of the major and most relevant contributions in that field was made by Edelman and Mountcastle [7]. Their findings lead to a shift from positioning simplified neuron models as fundamen- tal functional units of an architecture to elevating that role to cor- tical columns, collections of cells characterized by common feed- forward connections and strong inhibitory inter connections. This provided a biologically feasible mechanism for learning and form- ing invariant representations of sensory patterns that earlier ANN did not.
Additionally, two supplementary discoveries were believed to be key in emulating human intelligence. The first was the sus- pected existence of a common computational algorithm in the neocortex [12]. This algorithm is pervasive throughout these regions irrespective of the underlying mental faculty. Whether the task is visual, auditory, olfactory, or other, the brain seems to deal with sensory information in very similar ways. The sec- ond was the hierarchical structure of the human neocortex [12]. The brain’s regions are hierarchically connected so that the bidirectional flow of information merges into more complex representations with every layer, further abstracting the sensory stimuli.
The combination of these two findings forms potential grounds for building a framework that replicates human intelli- gence; a hierarchy of biologically inspired functional units that implement a common algorithm. These novel insights from neu- roscience have been reflected in the machine learning (ML) and AI fields and have been implemented to varying layers in several algorithms.
While CA restructured the neurons and their connections as well as the learning algorithm [6] based on Edelman and Mountcastle’s finding [7], other algorithms modeled other biological theories of the brain’s workings. Symbolic architec- tures such as Adaptive Character of Thought (ACT-R) [29] modeled working memory coupled with centralized control that refers to long term memory when needed. Emergentist architectures such as Hierarchical Temporal Memory (HTM)
[30] are based on globalist memory models and use rein- forcement or competitive learning schemes to generate their models. Integrating both classes of architectures to form hybrid architectures also exist and include Learning Intelligent Distribution Agent (LIDA) [31].



Deep belief networks

Overview

DNN are deeper extensions of shallow ANN architectures that are composed of a simplified mathematical model of the biological neuron but do not aim to faithfully model the human brain as do CA or some other ML approaches. DNN are based on the Neocogni- tron, a biologically inspired image processing model [32], that attempt to realize strong AI models through hierarchical abstrac- tion of knowledge. Information representation is learned as data propagates through the network, shallower layers learn low-level statistical features while deeper layers build on these features to learn more abstract and complex representations. Lacking clear skills for logical inferences, DNN need more morphing to be able to integrate abstract knowledge in a human manner. Recurrent and convolutional neural networks, first introduced in the 1980s, can be considered predecessors of DNN and were trained using back-propagation which has been available since 1974.
ANN were first trained using back-propagation, an algorithm that updates the network weights by propagating the output error backwards through the network [33]. However, the propagated error vanishes to zero as the network depth increases, preventing early-layer weights from updating and significantly reducing the
[38,39], natural language processing [40–42], automatic speech recognition [43–46] and feature extraction and reduction [47– 49], to name a few.

Network structure

Restricted Boltzmann machines
RBM, first known as Harmonium by [50], are two-layer net- works where only inter-layer neuron connections are allowed. They are a special case of Boltzmann machines (BM) which allow both inter and intra-layer connections. RBM’s neurons form two disjoint sets (as indicated in Fig. 1 by the black boxes), satisfying the definition of bipartite graphs. Thus, training RBM is less com- plex and faster. The neuron connections in RBM may be directed or undirected; in the latter case, the network forms an auto- associative memory which is characterized by bi-directional infor- mation flow due to feedback connections [2].

Deep belief networks
DBN are stacked directed RBMs, except for the first RBM which contains undirected connections, as shown in Fig. 1. This network architecture significantly reduces the training complexity and makes deep learning feasible. Focusing on two layers of the net- work, the weighted edges connecting the various neurons are

performance of the network [34–37]. Thus, other training algo- rithms for DNN were investigated. In 1992, Schmidhuber proposed to train recurrent neural networks by pre-training layers in an unsupervised fashion then fine-tuning the network weights using
back-propagation [28]. Momentum further picked up in 2006
annotated using the variable notation n‘

Table 1
DBN nomenclature.
which implies that

when Hinton et al. proposed a greedy training algorithm for DBN specifically. In what follows, we restrict our discussion of DNN to DBN, a popular and widely used deep architecture, trained using
‘
n;m


r n
Weight of the edge connecting the nth neuron in the ‘th layer to the mth neuron in the ‘th + 1 layer; ‘ is suppressed when there are only 2 layers in the network
Vector of connection weights leaving the nth neuron in the ‘th layer

Hinton et al.’s algorithm.
While DBN is a type of deep ANN, back-propagation fails to pro- duce a suitable model that performs well on training and testing data due to DBN’s architectural characteristics [2]. This has been attributed to the ‘‘explaining away” phenomenon. Explaining away, also known as Berkson’s paradox or selection bias, renders the commonly held assumption of layer independence invalid and consequently adds complexity to the inference process. The hidden nodes become anti-correlated because their extremely
low probabilities make the chances of both firing simultaneously
n‘	Matrix of weights connecting the ‘th layer to the ‘th + 1 layer
l	Learning rate
j	Number of Gibbs sampling steps performed during CD
N	Hidden-layer neuron cardinality
M	Input-layer neuron cardinality
L	Number of hidden layers
t	Sampling step
Q (.|.)  Conditional probability distribution
h‘	Binary configuration of the ‘th layer
p(h‘)  Prior probability of h‘ the current weight values
x0	Input layer data point

impossible.
(t)
m
Binary configuration of mth input-layer neuron at sampling step t

To remedy this issue, Hinton et al. proposed a training algo- rithm based on the observation that DBN can be broken down to
F	Set of training points
√n	Binary configuration variable of neuron n in the hidden layer at sampling step t

sequentially stacked restricted Boltzmann machines (RBM), a
two-layer network inter-layer neuron connections only. This novel
(t)
n
Binary configuration value of neuron n in the hidden layer at sampling step t

approach rekindled the interest in these deep architectures and saw DBN applied to many problems from image processing
bm	mth input-layer neuron bias
cn	nth hidden-layer neuron bias






Fig. 1. DBN architecture [51].



neuron n in layer ‘ is connected to neuron m in layer ‘ + 1. An exhaustive list of the entire nomenclature adopted in this section is included in Table 1.

Training algorithm
log p(x0) P	Q (h0|x0)(log p(h0)+ log p(x0|h0))
6h0
—	Q (h0|x0) log Q (h0|x0)
6h0


(5)


Restricted Boltzmann machines
@ log p(x0)	X	0  0	0

Hinton et al. proposed the contrastive divergence (CD) algo- rithm to train RBM in both supervised and unsupervised scenarios.
@nn;m
=
6h0
Q (h |x ) log p(h )	(6)

CD estimates the log-likelihood gradient using a j Gibbs sampling steps which is typically set to 1. The optimal weight vector is obtained by maximizing the objective function in (1) through gra- dient descent [52]. CD’s pseudo-code is summarized in Table 2.
Gibbs sampling is a randomized MCMC algorithm that allows the sampling of approximate samples from a multivariate proba- bility distribution [53]. The generated samples are correlated and form a Markov chain. Eq. (2) describes the energy function that
represents the joint probability distribution, derived from Gibbs distribution and calculated using (3). nn;m, bm and cn are real valued weights; hn and xm can take values in the set {0; 1} [54].
maxnPxЗX P(x)	(1)
After iteratively learning the weights of the network, the up- down algorithm [2] fine-tunes the network weights. This algorithm is a supervised variant of the wake-sleep algorithm that uses CD to modify the network weights. The wake-sleep algorithm [60] is an unsupervised algorithm used to train neural networks in two phases: the ‘‘wake” phase is applied on the feed-forward path to compute the weights and the ‘‘sleep” phase is applied on the feed- back path. The up-down algorithm, described in Table 3, is applied to network to reduce under-fitting which is commonly observed in greedily-trained networks.
Specifically, in the first phase (up-pass) of the algorithm, the weights on the directed connections, termed generative weights or parameters, are modified by calculating the wake-phase proba- bilities, sampling the states, and updating the weights using CD. On

N  M	M	N

E(x; h)= —	n
n=1 m=1

1

n;m
hnxm —	bm
m=1
xm —	cnhn	(2)
n=1
the other hand, the second phase (down-pass) stochastically acti-
vates earlier layers through the top-down links, termed inference
weights or parameters. The sleep-phase probabilities are calcu- lated, the states are sampled and the output is estimated.

p(x; h)= 
P P e—E(x;h)
e—E(x;h)	(3)




Deep belief networks
A simple and efficient layer-wise training algorithm was pro- posed for DBN by Hinton et al. in 2006 [2]. It trains the layers sequentially and greedily by tying the weights of unlearned layers, using CD to learn the weights of a single layer and iterating until all layers are trained. Tying the weights not only allows us to use RBM’s training algorithm but also eliminates the ‘‘explaining away” phenomenon. Then, the network weights are fine-tuned using a two-pass ‘‘up-down” algorithm.
In general, deep networks are pre-trained using unsupervised learning before using labeled data to improve the model with supervised learning. This scheme almost always outperforms net- works learned without pre-training [56] since this phase acts as a regularizer [57,58] and aid [59] for the supervised optimization problem.
The energy contained in the directed model can be calculated using (4) where the maximum energy is upper bounded by (5) and achieves equality when the network weights are tied. At equality, the derivative is equal to (6) and is used to solve the now simpler maximization problem.
E(x0; h0)= —(log p(h0)+ log p(x0|h0))	(4)
Overview

CA are a deep artificial neural network model, which borrows several concepts and aspects from the human brain. The main inspiration is drawn from the findings of Edelman and Mountcastle [15,7], which state that the brain is composed of cortical columns arranged in six layers. He also uses the concept of strengthening and inhibiting to build a computational training algorithm capable of extracting meaningful information from the sensory input and creating invariant representations of patterns. Further description of the CA model and its biologically plausible aspects can be found in [61,51,62].

Network structure

A typical CA network consists of an association of columns grouped in layers or levels. A column is a collection of neurons associated with the same stimulus, as shown in Fig. 2. Hence, CA can be considered as a three-level hierarchy structure. The neu- rons, like in other neural network architectures, use an activation






Table 2
Contrastive divergence workflow used in training RBM [55].
Set the weights to zero: n‘ = 0;‘ = 1 .. . L
6x ЗF 
Propagate training instance through the network
For j sampling steps
Loop over N hidden-layer neurons and sample h(t) ~ p(hn |x(t))
Loop over M input-layer neurons and sample x(t) ~ p(xm |h(t))
Loop over input and hidden-layer neurons and compute
Dn	= Dn  + p(√ = 1|x(0))x(0) — p(√ = 1|x(j))x(j)
Table 3
Up-Down training algorithm workflow [2].

Bottom-up phase
Calculate wake-phase probabilities
Sample network states
Using wake-phase probabilities, calculate CD statistics
Perform Gibbs sampling for j iterations
Using (1.d), calculate sleep-phase CD statistics
Down-pass phase
Compute sleep-phase probabilities through top-down pass
Sample network states
Estimate network output

n;m
n;m	n	m	n	m
Re-compute generative weights

Dbm = Dbm + x(0) — x(j)
Re-compute network’s directed sub-graph weights

Dcn = Dcn + p(√n = 1|x(0))— p(√n = 1|x(j))
Re-compute inference weights


	


synapsing at each of its neurons as shown in Eq. (7). Epoch number is denoted by e, layer number by l, the receiving column index by c, the receiving neuron n and the sending column by s.
Defining Jl;e as the output vector of level l for epoch e and tl;e the output of column c, within the same level; for the same train- ing epoch, we can write (8). The output of a neuron, zl;e defined by
(9) is the result of the nonlinear activation function f (.) in (10) in response to the weighted sum of the input connections while the output of the column is the sum of the outputs of the column’s neurons. T is a constant (across layers) tolerance parameter empir- ically selected and the nonlinear activation function emulates the brain’s observed nonlinear activity.

÷l;e = h ÷l;e
.. .  ÷l;e	.. .  ÷l;e i

c	c;1

l;e c;1;1
c;n
l;e c;n;1
c;N
l;e c;N;1



Fig. 2. Illustration of a cortical network.
.
.
6  l;e
.	.
.	.
l;e
.	.
.	.
l;e	7
(7)

=	nc;1;s
6
...	nc;n;s
...	nc;N;s
7

function f (.) to compute their output from their input. This activa- tion function is common for all neurons in the architecture.
Columns in a layer connect to those in the subsequent level:
l;e c;1;Cl—1
.	.
.	.
l;e
c;n;Cl—1
.	.
.	.
l;e
c;N;Cl—1

these connections are referred to as vertical connections and exist only between consecutive levels. A synaptic input terminating at a column’s input is shared within neurons constituting said column, however, the learned weights of the connections are different lead- ing to distinct neuronal outputs. The latter are summed to form the
Tl;e = h tl;e

tl;e = Xzl;e
...  tl;e
.. . tl;e i	(8)


tion permits the column to act as a basic computational structure in CA as opposed to neurons in DBN and other neural network architectures.


l;e
c;n

Cl—1


c=1

l;e	l—1;e
c;n;s c

Furthermore, lateral connections or intra-level connections, are ‘‘communication means” employed to deliver inhibiting signals that modify a column’s weights based on the activity of other col-
	1	
)= 1 + es·(/(s)—T)
 —2; if s = 1

umns during training. Contrarily to their vertical counterpart, these connections do not transmit data and are hence are not explicitly shown in Fig. 2. Data can only flow in a bottom-up direction, from
/(s)= 
s;	otherwise

level to level, through vertical connections.

Mathematical model

The complete mathematical description based on the model of [6,63] can be found in [62,61], and is summarized below based on the adopted nomenclature in Table 4.
A column of N neurons receives connections from the output of columns in the previous layer and hence is represented by a 2D matrix concatenating vectors of weights of incoming connections


Table 4
CA nomenclature.

c	Destination column index n	Destination neuron index l	Destination level index
s	Origin column index
e	Training epoch
N	Number of nodes in a column Cl	Number of columns in level l T	Tolerance
Training algorithm

Random initialization
The network is assumed to be initially fully connected with ran- dom weak weights (with absolute values less than 0.1). This is a common ‘‘blank slate” approach to ensure that the network is not initially biased to any specific pattern. As learning proceeds, these weights are incrementally updated so that the connectivity of the network is modified. All weights that fall to zero represent disabled connections. Therefore, an initially fully connected net- work is not necessarily preserved.

Unsupervised feed-forward learning
The first stage in training a cortical network aims at creating input-specific representations via random firing and repeated exposure. An input propagating through the levels of the network causes certain columns to fire (i.e. generate a threshold crossing response) based on initially random weights. This activation is then reinforced by strengthening the active columns’ weights and inhibiting neighboring ones. Repeated exposure or batch
learning trains columns to identify (via activation) particular

l;e c;n;s

l;e
c;n
l;e c

Weight of connection between neuron n, column c, level l, and column
s in previous level, during epoch e
Vector of connection weights entering neuron n, of column c, level l
Matrix of weights entering column c, level l
aspects or patterns of the training data, extracting discriminatory features with increasing complexity through levels (lower levels recognize basic elements, higher levels in the hierarchy learn
higher order concepts and reasoning). The strengthening process

Jl;e	Output vector of level l
increases a column’s weights rendering it more receptive to

l;e c
l;e
c;n
Output of column c
Output of neuron n, column c of level l
stimulation, while inhibition weakens the weights diminishing activity of a certain column. Connections formed or weakened



are plastic, i.e. can be altered during the feedback learning phase and vice versa. Strengthening and inhibition rules are shown in Eqs. (11)–(13).
pattern (strengthening and inhibition are the only weight update rules adopted in opposition to gradient descent employed in the backpropagation learning of traditional artificial neural networks).

nl;e+1 = tl;e · (nl;e
— X(÷l;e))	(11)
After multiple exposures, the top level attains a stable state also

c;n;s
c	c;n;s	c
0B
1	1C
known as a ‘‘stable activation” in which columns are able to cor- rectly generate the desired firing scheme. The term ‘‘firing scheme” refers to the firing pattern of neurons for a given input.

nc;n;s = tc · @Bnc;n;s + ac;n;s + q ·



N Cl—1

1 + e
l;e	(12)
c;n;s
X( l;e )
back to the previous level which in turn executes a series of inhibition and strengthening to achieve the desired firing scheme. The same process in repeated for each of the layers
until  a  convergence  condition  (expressed  as  a  discrepancy

X(÷l;e )= XXal;e
nl;e

between the desired and actual activation) is met. The training



l;e c;n;s
1  if nl;e  > s
=
0  otherwise
(
all variations of the same pattern in the training data (within a certain tolerance).
A pseudo-code showing the implementation of the training
algorithm is shown in Table 5. An illustration of the feedback train-

Supervised feedback learning
The feed-forward learning phase relies only on aspects of the given data to train columns that identify significant features with no error propagation or label information. The supervised feedfor- ward stage aims at correcting misclassifications occurring when the network is exposed to variations of the same patterns (more accurately training class) which may result due to the absence of label information in the previous phase.
Following the unsupervised phase, a ‘‘unique representation” based on the average firing scheme observed for a particular pat- tern is stored, the feedforward learning fine-tunes the network’s weights to achieve this scheme (within certain bounds to avoid overfitting) for all instances of a known class. A misclassification hence triggers an error signal at the top-most or output layer (where the final output is produced) correcting misfiring columns (through inhibition) and strengthening weakened columns firing for the original pattern forcing the column firing for the original
ing process is shown in Fig. 3. The top row shows the firing scheme (blue squares represent active columns) obtained after the feedfor- ward propagation of two variations of the same pattern as well as the desired firing scheme (average activation obtained based on all variations of this pattern in the training set). The middle and bot- tom rows show a succession of training epochs (for pattern 1 and 2 respectively) in which the error signal is generated at the top level and stable activations are formed from top to bottom. One can see that at convergence both instances are represented with the same firing scheme in the network.


Theoretical computational complexity

In this section, we derive the theoretical computational com- plexity of each algorithm to assess the required resources when deploying such a network for real world problems. We investigate



Table 5
CA generic training workflow.

Random initialization
for l = 1 : 6




Feedforward phase
for c = 1 : Cl

÷l = randn(Cl—1; N)



Processing of training data
for all training instances
for l = 1 : 6
for l = 1 : 6






Store average representations of patterns for all classes




compute Jl;e

for c = 1 : Cl










if tl;e > s

else









for n = 1 : N apply (12) for n = 1 : N apply (11)

for l = 1 : 6
for c = 1 : Cl



tl;avg = tl

Feedback phase
while MSE > h
c	c



for all training instances
for l = 1 : 6

while Jl;e — Jl;e > h
for c = 1 : Cl









if tl;avg — tl > h

c	c


else
for n = 1 : N apply (11) for n = 1 : N apply (12)




Fig. 3. An example of the feedback process.


two aspects of the computational complexity: memory and com- putations. While the required memory storage depends on the number of non-zero weights in the network, the number of com- putations depends on the non-zero weights and the adopted acti- vation function. Comparing CA to DBN, the more computationally demanding network is data specific since each problem would result in a different number of non-zero weights. We empirically compare the network sizes in the next section.

Number of non-zero weights

DBN
DBN is formed of R layers with Mr neurons in layer r. During training, the network starts out fully connected leading to a total number of weights equal to Nw =  R Mr · Mr 1. Assuming not all
weights are non-zero when training is complete, with c З [0, 1] is
the fraction of non-zero weights or firing rate, the number of
non-zero weights is equal to NNZW = R c · Mr · Mr 1. Empirical results in Section 6 reveal that c is usually greater than 90%. Know-
ing that the weights are double precision floating point numbers which require 8 bytes of storage, an upper bound on the memory requirements during training are  R 8 · Mr · Mr 1. During testing,
the number of bytes is equal to PR 8 · c · Mr · Mr 1.

CA
Number of operations per neuron

Since a neuron is composed of a summation and an activation function, the number of operations performed to obtain the output of each neuron can be divided into the operations to compute the sum and the activation function computational complexity.

Summation
The number of floating point operations required to compute the summation depends on the number of input connections of a neuron. Assuming there are m connections, m multiplications and m additions (including the bias term) are required, which is of the order of O(m2). For an input layer neuron, m is equal to the number of features in the input vector. For a hidden layer neu- ron, m is at most equal to the number of neurons in the previous layer.

Activation function
Depending on the activation function, the computation of each neuron output will require a certain number of floating point and comparison operations. Some of popular activation functions and their computational complexity are summarized in Table 6.
The hard limit activation function, described by (14), requires one comparison. The linear function in (15) requires one multipli- cation and one addition whereas the piece-wise linear function in
(16) requires two comparisons and at most one multiplication and addition operation.

CA is formed of R = 6 levels; the first level contains L1
= I col-
 1  if xi P 0

umns with M neurons per column. We assume, without loss of generality, the number of columns is cut in half in each subsequent
level, as used in [6,63,64] i.e. level r contains Lr =  I . Each level has
/(xi)= 
0  otherwise
(14)

2r—1
/(x )= ax + b	(15)

a weight matrix with dimensions Lr × M. Therefore, the total num-	i	i

ber of weights is equal to Nw = PR  Lr · M · Lr 1. However, some of
—
8 b	if x P b

these weights could be equal to zero. Therefore, we denote by c З [0, 1] the firing rate of the network, the fraction of neurons that are non-zero, which leads to the number of firing neurons equal to

 
i
axi + b if — b 6 xi < b
>: —b	if xi < —b
(16)

tion of firing columns across levels will be assumed. Therefore, the number of firing columns per level is NFC/L = NFC . Finally, the total number of non-zero weights can be approximated by
Table 6
Computational complexity of some activation functions

Activation function	Equation	Computations (per neuron)

NNZW =	R
c do ble p age, a
are PR
M · Lr · NFC L. Empirical results in Section 6 reveal that

to P
· c · M · Lr · PR
Lr .



Computing the Gaussian activation function, described by (17), requires m multiplications and m(m — 1) additions to compute the magnitude term, where m represents the dimen- sionality of the vector xi and is equivalent to the number of input connections of a neuron. Dividing by the standard devi- ation requires 3 additional multiplication operations. In addi- tion, the calculation of an exponential, estimated using the Taylor series expansion with approximately 10 terms, requires approximately 81 operations. In total, m2(m — 1)+ 81 opera- tions are required.
 —xi — li 2 !
5.4. Overall computational complexity

In summary, the overall computational cost of DBN and CA depends on the architecture of the network: the number of layers and the number of neurons per layer which affect the number of connections. The activation function can be fixed for both DBN and CA. Sigmoid is a common activation function used in both algorithms. After training, some of these connections will have a weight of zero. Based on the previous sections, we notice that for a fixed network architecture, CA will have less non-zero weights due to the pruning algorithm. However, the depth of the best net- works for each algorithm vary based on the data. six-layer archi-


The tangent sigmoid function in (18) has two exponential terms, each requiring approximately 81 operations, in addition to two additions and one multiplication operations which leads to a total of approximately 165 operations. On the other hand, the logarithmic sigmoid in (19) has one exponential term and one addition and multiplication for a total of 83 operations.
1 — e—2xi
depth’s varied from 3 to 5 hidden layers.

Empirical comparison

In this section, we report on the experimental results of DBN and CA on multiple databases from the UCI ML repository [72]. We compare the classification accuracy, network complexity and computational complexity of both algorithms.

/(xi)= tanh(xi)= 
1 + e—2xi

  1	
/(xi)= 
1 + e—xi
(18)

(19)
Experimental setup

CA and DBN were empirically compared on classification prob- lems using datasets from the UCI machine learning repository,

Each exponential term in the softmax activation function, described by (20), requires approximately 81 operations. The numerator has one exponential term while the denominator has MR terms. In total, 81(MR + 1)+ 1 operations are required.
described in Table 7. The datasets contained real-valued features. A 4-fold cross validation was adopted, i.e. each database is ran- domly divided into 4 sets where 3 sets (or 75% of the data samples) are used in training and 1 set is used in testing. The results are averaged over four runs where each set is used for testing once

/( i	P
exi
exj	(
20)
and the remaining for training. The CA library is a set of Matlab
functions obtained from [73,74]; it was run on a Windows 7

jЗMR


Pruning

The term synaptic pruning refers to the procedure by which synaptic connections are terminated during the early ages of mammals’ lives [65]. Starting with approximately 86 8 billion neurons at birth, the human brain quintuples its size until adoles- cence after which the volume of synaptic connection decreases again [66]. This process of pruning is mainly thought of as the byproduct of learning. While the total number of neurons
machine with Intel Core i5. The DBN library is a set of Matlab func- tions modified from [75]; it was run on an Intel Core i7 processor machine. Multiple network architectures, summarized in Table 8, were tested on the datasets. The number of neurons for the hidden layers are displayed only. The input layer’s neurons are equal to the number of features and the output layer’s neurons are equal to the number of classes. The chosen architectures can be grouped into


Table 7
UCI dataset characteristics.



remains roughly unaltered, the distribution and number of con- nections are tailored by learning [67,68]: the structure of the
Dataset	Number of
instances
Number of features
Number of classes

brain moves from a stage where its primary function falls under
Planning relax	182	12	2

perception–action due to disconnected cortical hubs [69,70] to
distributed networks spanning the brain performing a variety of
Breast cancer diagnostic Wisconsin
569	32	2

complex cognitive tasks [71].
The training of a cortical network bears several resemblances to the synaptic pruning procedure: starting with a fully connected structure, the network goes through an unsupervised phase where connections are pruned through inhibition leaving only ‘‘signifi- cant” ones. In more accurate terms, the strengthening of firing col- umns ensures the establishment of selective connections while inhibition prunes irrelevant connections. This process plays a cru- cial role not only in extracting meaningful characteristics of the input stimuli but also in avoiding overfitting due to the especially complex structure of a cortical network. This is further demon- strated in the number of non-zero weights in different network architectures as shown in our theoretical and empirical analysis: the number of ‘‘alive” connections decreases with the increase of parameters, hence a minimal effect on performance as shown in our experiments. This property of CA’s structure and training is not shared with DBN.
Tic Tac Toe	958	9	2
Spambase	4601	57	2
Wilt	4889	5	2
White wine	4898	11	2
MNIST	70,000	784	10
Skin segmentation	245,057	3	2




Table 8
Nomenclature adopted for network architectures.
Network name	Network architecture (hidden layers) N1	[5, 5]
N2	[50, 50]
N3	[50, 50, 50]
N4	[500, 500, 500]
N5	[500, 500, 2000]
N6	[1000, 1000, 2000]
N7	[2000, 1000, 500, 250, 125, 62]




one of three sets based on the pattern of hidden layer neurons: net- works with an equal number of neurons in all hidden layers, net- works with an increasing (doubling) number of neurons as the layer depth increases and networks with a decreasing (halving) number of neurons as the layer depth increases. Furthermore, the number of hidden layers is increased from 2 to 6 hidden layers. For the results reported in Table 9, DBN’s unsupervised training and fine-tuning algorithms were each run for 50 epochs and the batch size was set to 10. The second column indicates the number of col- umns per layer for CA (there were 20 neurons per column) and the
number of neurons per column for DBN. The connectivity of a net- work is computed by taking the ratio of weights greater than 5% of the average value of weights to the total number of weights in the
network. The threshold is not set to a fixed value since the weights range of weights varies based on the input data, i.e. for some data- sets all the weights might be less than this set threshold even though this threshold might be very small. Furthermore, the threshold is not set to zero since some weights will not exactly zero but significantly smaller than the other weights in the net- work and their contribution is insignificant. The classification



Table 9
Classification results.

The bold values refer to the best accuracy obtained per dataset.



accuracy simply reports the percentage of correctly classified instances in the test set. Running time is not compared since each algorithm is written and run in a different environment.

Classification results

First, we compare the performance of DBN when varying the network architecture. For some databases, such as White wine, the performance does not significantly vary as the network archi- tecture varies, achieving approximately 96.4% accuracy. On the other hand, performance varied significantly across architectures for MNIST, ranging from 38.8% to 89.47%. Hence, DBN is either able to achieve good accuracy or bad accuracy on a specific database. This is mainly due to the lack of enough training points to allow DBN to learn a good model of the data. However, for other datasets such as Breast cancer diagnostic Wisconsin, the classification accu- racy varied significantly as the network architecture varied. Next, considering CA’s performance on the various datasets when vary- ing the network size, we notice that the six-layered network always outperforms other network sizes on all datasets, although it occasionally marginally outperforms some networks. For exam- ple, CA achieved 99.9% accuracy for N7 on skin segmentation com- pared to 94.5% for N1 (2 layers). Comparing CA to DBN, we notice that CA outperforms DBN on almost all datasets. In addition, a six- layered architecture seemed to always perform better than other architectures, reducing the burden of searching for the optimal network architecture by training multiple networks.

Network connectivity

DBN exhibit high connectivity for most datasets on various net- work sizes. Therefore, Hinton et al.’s greedy training algorithm does not eliminate a large number of connections resulting in an almost fully connected graph. This is evident in the experimental results reported in Table 9 which reports the percentage of non-
zero weights in a DBN network to be around 90%. Unlike DBN, CA generally results in a sparsely connected network, evident by the low percentage of non-zero weights obtained after training; CA networks had less than 50% of their connections in tact com- pared to almost 90% for DBN. Furthermore, we notice that CA’s connectivity tends to decrease as the network size increases which implies that if certain data does not require a large network, CA’s training algorithm will reduce the number of connections to pro- duce a sparsely connected network. For example, connectivity per- centage decreases from 98.7% to 40.8% for MNIST when increasing the network architecture from a two-hidden layer network to a six-layer architecture.

Effect of batch size

The DBN code randomly reorders and divides the data into mini batches. While training, each mini batch is loaded into memory and used to update the weights of the network connections. Increasing the size of a batch meant loading a larger chunk of data into memory. This renders training slower if the chunk was too large to fit into memory. However, decreasing the batch size meant more memory transfers per epoch and therefore, increasing train- ing time. On the other hand, CA does not randomly reorder and divide the data into mini batches. Therefore, the network is exposed to the data in the order it was presented. Theoretically, reordering or dividing the data has no effect on the performance of the algorithm as all patterns are employed and must achieve a stable activation.

Statistical analysis

Next, we perform the pairwise t-test [76] and Friedman test
[77] to gain further insight into the differences between CA and DBN. The N7 architecture is trained on the various databases. Table 10 summarizes the p-values of both tests and show that





Fig. 4. CA vs. DBN ranking based on Nemenyi post hoc test.



there is a statistical significance between CA and DBN on all data- bases; the p-value was less than 5%. Furthermore, the Nemenyi post hoc [78] was performed once a significant difference was observed, to rank the algorithms. The rankings revealed that CA outperformed DBN on most databases except for Skin Segmenta- tion, as shown in Fig. 4. The critical distance is summarized in Table 10.

Conclusion

In this work, we compared two DNN architectures on super- vised classification problems. While DBN can be easily seen as an established technique developed from within a traditional AI per- spective, CA are more biologically inspired and can be classified as theories in the making, solidly rooted in principles inherited from neuroscience research. A theoretical computational complex- ity analysis for both algorithms was presented before empirically comparing CA and DBN. Experiments were run on eight publicly available classification databases. Multiple CA and DBN network architectures were compared based on their classification accuracy and resulting network connectivity. CA achieved the best perfor- mance on most databases using a six-layer architecture with decreasing number of hidden neurons for deeper layers. Results showed that deeper CA networks had lower connectivity than shal- lower CA networks. Furthermore, DBN did not prune as many con- nections as CA’s training algorithm. On the tested databases, CA generally had a higher classification accuracy than DBN. In the span of this work, we attempted to provide the reader with enough background and technical details for each of the algorithms while understanding that the breadth of the topic necessitates the inclu- sion of more involved insights. We therefore urge the interested reader to use this paper as a foundation for further exploration of the rapidly expanding sub-field of deep learning.

Acknowledgment

This work has been partly supported by the National Center for Scientific Research in Lebanon and the University Research Board at the American University of Beirut.

References

S. Samarasinghe, Neural Networks for Applied Sciences and Engineering: From Fundamentals to Complex Pattern Recognition, CRC Press, 2006.
G. Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deep belief nets, Neural Comput. 18 (7) (2006) 1527–1554.
E.M. Izhikevich, Which model to use for cortical spiking neurons?, IEEE Trans Neural Netw. 15 (5) (2004) 1063–1070.
H. De Garis, C. Shuo, B. Goertzel, L. Ruiting, A world survey of artificial brain projects, part i: large-scale brain simulations, Neurocomputing 74 (1) (2010) 3–29.
B. Goertzel, R. Lian, I. Arel, H. De Garis, S. Chen, A world survey of artificial brain projects, part ii: biologically inspired cognitive architectures, Neurocomputing 74 (1) (2010) 30–49.
A.G. Hashmi, M.H. Lipasti, Cortical columns: building blocks for intelligent systems, in: IEEE Symposium on Computational Intelligence for Multimedia Signal and Vision Processing, IEEE, 2009, pp. 21–28.
G.M. Edelman, V.B. Mountcastle, in: The Mindful Brain: Cortical Organization and the Group-Selective Theory of Higher Brain Function, Masachusetts Inst of Technology Pr, 1978.
R.J. Baron, The Cerebral Computer: An Introduction to the Computational Structure of the Human Brain, Psychology Press, 2013.
J. Nolte, The Human Brain: An Introduction to its Functional Anatomy.
J.M. DeSesso, Functional anatomy of the brain, in: Metabolic Encephalopathy, Springer, 2009, pp. 1–14.
N. Geschwind, Specializations of the human brain, Scientific American 241 (3) (1979) 180–201.
R.C. O’Reilly, Y. Munakata, Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain, MIT Press, 2000.
M. Catani, D.K. Jones, R. Donato, et al., Occipito-temporal connections in the human brain, Brain 126 (9) (2003) 2093–2107.
J. Szentagothai, The ferrier lecture, 1977: the neuron network of the cerebral cortex: a functional interpretation, Proc. R. Soc. Lond. Ser. B. Biol. Sci. 201 (1144) (1978) 219–248.
V.B. Mountcastle, The columnar organization of the neocortex, Brain 120 (4) (1997) 701–722.
A.S. Benjamin, J.S. de Belle, B. Etnyre, T.A. Polk, The role of inhibition in learning, Human Learn.: Biol., Brain, Neurosci.: Biol., Brain, Neurosci. 139 (2008) 7.
W.S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in nervous activity, Bull. Math. Biophys. 5 (4) (1943) 115–133.
F. Rosenblatt, The perceptron: a probabilistic model for information storage and organization in the brain, Psychol. Rev. 65 (6) (1958) 386.
B. Widrow, et al., Adaptive adaline neuron using chemical memistors, 1960.
A. Aizerman, E.M. Braverman, L. Rozoner, Theoretical foundations of the potential function method in pattern recognition learning, Autom. Rem. Control 25 (1964) 821–837.
J.L. McClelland, D.E. Rumelhart, P.R. Group, et al., Parallel distributed processing, Explorations in the microstructure of cognition 2 (1986) 184.
J.J. Hopfield, Neural networks and physical systems with emergent collective computational abilities, Proc. Nat. Acad. Sci. 79 (8) (1982) 2554–2558.
T. Kohonen, Self-organized formation of topologically correct feature maps, Biol. Cybernet. 43 (1) (1982) 59–69.
S. Grossberg, Competitive learning: from interactive activation to adaptive resonance, Cognit. Sci. 11 (1) (1987) 23–63.
J. Misra, I. Saha, Artificial neural networks in hardware: a survey of two decades of progress, Neurocomputing 74 (1) (2010) 239–255.
L. Arnold, S. Rebecchi, S. Chevallier, H. Paugam-Moisy, An introduction to deep learning, in: ESANN, 2011.
V. Vapnik, The Nature of Statistical Learning Theory, Springer Science & Business Media, 2000.
J. Schmidhuber, Learning complex, extended sequences using the principle of history compression, Neural Comput. 4 (2) (1992) 234–242.
J.R. Anderson, Act: a simple theory of complex cognition, Am. Psychol. 51 (4) (1996) 355.
J. Hawkins, S. Blakeslee, On Intelligence, MacMillan, 2007.
S. Franklin, F. Patterson Jr., The lida architecture: adding new modes of learning to an intelligent, autonomous, software agent, pat 703 (2006) 764– 1004.
K. Fukushima, Neocognitron: a hierarchical neural network capable of visual pattern recognition, Neural Netw. 1 (2) (1988) 119–130.
P.J. Werbos, Backpropagation through time: what it does and how to do it, Proc. IEEE 78 (10) (1990) 1550–1560.
S. Hochreiter, Untersuchungen zu dynamischen neuronalen netzen, Master’s thesis, Institut fur Informatik, Technische Universitat, Munchen.
S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
G.E. Hinton, To recognize shapes, first learn to generate images, Prog. Brain Res. 165 (2007) 535–547.
Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning, in: Proceedings of the 26th Annual International Conference on Machine Learning, ACM, 2009, pp. 41–48.
V. Nair, G.E. Hinton, 3d object recognition with deep belief nets, in: Advances in Neural Information Processing Systems, 2009, pp. 1339–1347.
Y. LeCun, K. Kavukcuoglu, C. Farabet, Convolutional networks and applications in vision, in: Proceedings of the IEEE International Symposium on Circuits and Systems, IEEE, 2010, pp. 253–256.
R. Collobert, J. Weston, A unified architecture for natural language processing: Deep neural networks with multitask learning, in: Proceedings of the 25th Internatuional Conference on Machine Learning, ACM, 2008, pp. 160–167.
S. Zhou, Q. Chen, X. Wang, Active deep networks for semi-supervised sentiment classification, in: Proceedings of the 23rd International Conference on Computational Linguistics: Posters, Association for Computational Linguistics, 2010, pp. 1515–1523.
X. Glorot, A. Bordes, Y. Bengio, Domain adaptation for large-scale sentiment classification: a deep learning approach, in: Proceedings of the 28th International Conference on Machine Learning, 2011, pp. 513–520.
G.E. Dahl, D. Yu, L. Deng, A. Acero, Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition, IEEE Trans. Audio, Speech, Lang. Process. 20 (1) (2012) 30–42.
T.N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. Novak, A.-R. Mohamed, Making deep belief networks effective for large vocabulary continuous speech recognition, in: IEEE Workshop on Automatic Speech Recognition and Understanding, IEEE, 2011, pp. 30–35.
A.-R. Mohamed, D. Yu, L. Deng, Investigation of full-sequence training of deep belief networks for speech recognition, in: INTERSPEECH, 2010, pp. 2846– 2849.
A.-r. Mohamed, T.N. Sainath, G. Dahl, B. Ramabhadran, G.E. Hinton, M. Picheny, et al., Deep belief networks using discriminative features for phone recognition, in: International Conference on Acoustics, Speech and Signal Processing, IEEE, 2011, pp. 5060–5063.
A.-R. Mohamed, G.E. Dahl, G. Hinton, Acoustic modeling using deep belief networks, IEEE Trans. Audio, Speech, Lang. Process. 20 (1) (2012) 14–22.
P. Hamel, D. Eck, Learning features from music audio with deep belief networks, in: ISMIR, Utrecht, The Netherlands, 2010, pp. 339–344.
G.E. Hinton, R.R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science 313 (5786) (2006) 504–507.



P. Smolensky, Information processing in dynamical systems: Foundations of harmony theory, Department of Computer Science, University of Colorado, Boulder, 1986.
R. Khanna, M. Awad, Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers, Apress, 2015.
G. Hinton, A practical guide to training restricted boltzmann machines, Momentum 9 (1) (2010) 926.
S. Geman, D. Geman, Stochastic relaxation, gibbs distributions, and the bayesian restoration of images, IEEE Trans. Pattern Anal. Mach. Intell. (6) (1984) 721–741.
B. Aleksandrovsky, J. Whitson, G. Andes, G. Lynch, R. Granger, Novel speech processing mechanism derived from auditory neocortical circuit analysis, Proceedings of the 4th International Conference on Spoken Language, vol. 1, IEEE, 1996, pp. 558–561.
A. Fischer, C. Igel, An introduction to restricted boltzmann machines, in: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, Springer, 2012, pp. 14–36.
D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, S. Bengio, Why does unsupervised pre-training help deep learning?, J Mach. Learn. Res. 11 (2010) 625–660.
Y. Bengio, Learning deep architectures for AI, Found. Trends® Mach. Learn. 2
(1) (2009) 1–127.
D. Erhan, P.-A. Manzagol, Y. Bengio, S. Bengio, P. Vincent, The difficulty of training deep architectures and the effect of unsupervised pre-training, in: Internartional Conference on Artificial Intelligence and Statistics, 2009, pp. 153–160.
Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, et al., Greedy layer-wise training of deep networks, Adv. Neural Inf. Process. Syst. 19 (2007) 153.
G.E. Hinton, How neural networks learn from experience, Scient. Am. 267 (3) (1992) 145–151.
N. Hajj, Y. Rizk, M. Awad, A mapreduce cortical algorithms implementation for unsupervised learning of big data, Proc. Comp. Sci. 53 (2015) 327–334.
N. Hajj, M. Awad, Weighted entropy cortical algorithms for isolated arabic speech recognition, in: International Joint Conference on Neural Networks, IEEE, 2013, pp. 1–7.
A. Hashmi, M.H. Lipasti, Discovering cortical algorithms, in: IJCCI (ICFC-ICNC), 2010, pp. 196–204.
A. Hashmi, A. Nere, J.J. Thomas, M. Lipasti, A case for neuromorphic isas, ACM SIGARCH Computer Architecture News, vol. 39, ACM, 2011, pp. 145–158.
G. Chechik, I. Meilijson, E. Ruppin, Synaptic pruning in development: a computational account, Neural Comput. 10 (7) (1998) 1759–1777.
F.I. Craik, E. Bialystok, Cognition through the lifespan: mechanisms of change, Trends Cog. Sci. 10 (3) (2006) 131–138.
L. Steinberg, Cognitive and affective development in adolescence, Trends Cog. Sci. 9 (2) (2005) 69–74.
E. D’Angelo, A. Antonietti, S. Casali, C. Casellato, J.A. Garrido, N.R. Luque, L. Mapelli, S. Masoli, A. Pedrocchi, F. Prestori, et al., Modeling the cerebellar microcircuit: new strategies for a long-standing issue, Front. Cell. Neurosci. 10 (2016) 176.
G.M. Shepherd, The Synaptic Organization of the Brain, Oxford University Press, 2003.
P. Fransson, U. Åden, M. Blennow, H. Lagercrantz, The functional architecture of the infant brain as revealed by resting-state FMRI, Cereb. Cort. 21 (1) (2011) 145–154.
N. Gogtay, J.N. Giedd, L. Lusk, K.M. Hayashi, D. Greenstein, A.C. Vaituzis, T.F. Nugent, D.H. Herman, L.S. Clasen, A.W. Toga, et al., Dynamic mapping of human cortical development during childhood through early adulthood, Proc. Nat. Acad. Sci. USA 101 (21) (2004) 8174–8179.
M. Lichman, UCI Machine Learning Repository, 2013 <http://archive.ics.uci. edu/ml>.
J. Mutch, U. Knoblich, T. Poggio, CNS: A GPU-Based Framework for Simulating Cortically-Organized Networks, Massachusetts Institute of Technology, Cambridge, MA, Tech. Rep. MIT-CSAIL-TR-2010-013/CBCL-286.
J. Mutch, Cns: Cortical Network Simulator, 2017 <http://cbcl.mit.edu/jmutch/ cns/>.
R. Salakhutdinov, G. Hinton, Deep Belief Networks, 2015 <http://www. cs.toronto.edu/hinton/MatlabForSciencePaper.html>.
C. Nadeau, Y. Bengio, Inference for the generalization error, in: Advances in Neural Information Processing Systems, 2000, pp. 307–313.
M. Friedman, The use of ranks to avoid the assumption of normality implicit in the analysis of variance, J. Am. Statist. Assoc. 32 (200) (1937) 675–701.
P. Nemenyi, Distribution-Free Multiple Comparisons, Ph.D. thesis, Princeton University, NJ, 1963.
