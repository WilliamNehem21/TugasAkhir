

ORIGINAL ARTICLE

An inverse problem approach to pattern recognition in industry
Ali Sever *
University Of North Carolina at Charlotte, Charlotte, NC 28223, USA

Received 31 January 2013; revised 1 January 2014; accepted 27 February 2014
Available online 12 March 2014

Abstract Many works have shown strong connections between learning and regularization techniques for ill-posed inverse problems. A careful analysis shows that a rigorous connection between learning and regularization for inverse prob- lem is not straightforward. In this study, pattern recognition will be viewed as an ill-posed inverse problem and applications of methods from the theory of inverse problems to pattern recognition are studied. A new learning algorithm derived from a well-known regularization model is generated and applied to the task of reconstruction of an inhomogeneous object as pattern recognition. Particu- larly, it is demonstrated that pattern recognition can be reformulated in terms of inverse problems defined by a Riesz-type kernel. This reformulation can be employed to design a learning algorithm based on a numerical solution of a sys- tem of linear equations. Finally, numerical experiments have been carried out with synthetic experimental data considering a reasonable level of noise. Good recoveries have been achieved with this methodology, and the results of these simulations are compatible with the existing methods. The comparison results show that the Regularization-based learning algorithm (RBA) obtains a promis- ing performance on the majority of the test problems. In prospects, this method can be used for the creation of automated systems for diagnostics, testing, and

* Tel.: +1 704 687 8580, +1 704 463 3118; fax: +1 704 687 8580.
E-mail address: ali.sever@uncc.edu
URL: http://www.uncc.edu
Peer review under responsibility of King Saud University.

2210-8327 ª 2014 Production and hosting by Elsevier B.V. on behalf of King Saud University. http://dx.doi.org/10.1016/j.aci.2014.02.004

control in various fields of scientific and applied research, as well as in industry.
ª 2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.


Introduction

No patterns can be derived solely from empirical data (Yee and Haykin, 1993). Some hypotheses about patterns have to be chosen and, from among patterns satisfying these hypotheses, a pattern with a good fit to the data must be sought.
Neurocomputing brought a new terminology to data analysis: searching for parameters of their input/output functions is called learning, and samples of data training sets and a capability to satisfactorily process new data that have not been used for learning is called generalization.
The capability of generalization depends upon the choice of a hypothesis set of input/output functions, in which one searches for a pattern (a functional relation- ship) that matches the empirical data. So a restriction of the hypothesis set to only physically meaningful functions can improve generalization.
Inverse problems frequently arise in experimental situations when one is inter- ested in the description of the internal structure of a system and is given indirect, noisy data. Estimating the response of a system given a complete specification of the internal structure, on the other hand, is the forward problem.
The modeling problem arises when one is given noisy data, observed over irreg- ular intervals of space and time, and is asked to develop a reasonable model to fit those observed data (Vapnik, 1998).
With the advent of high-speed computers and artificial intelligence techniques, this modeling problem underwent a metamorphosis and emerged as a machine learning problem (Bauer et al., 2007; Gdawiec and Domanska, 2011). Tikhonov and Lanweber regularized that learning algorithms have recently received an increasing interest due to both theoretical and computational motivations (Abru- kov et al., 2006; Kurkova, 2012; Tiknonov and Arsenin, 1977). Fractal, optimiza- tion, and a two-dimensional functional relational model have been used as a feature in several pattern recognition methods (Chang et al., 2010; Lo Gerfo et al., 2008; Noureddine, in press). Considerable attention is currently being de- voted to new possibilities of using artificial neural networks (ANN) in view of their increasing importance for solving the problem of automated reconstruction of the inner structure of an object. Accompanying algorithms that effectively quantify uncertainties, deal with ill-posedness, and fully take the nonlinear model into account are needed Therefore, it is necessary to both look for possible ways to improve the classical learning algorithms already existent in the literature, and to identify new methods which can compete with the traditional ones in speed, robustness, and quality of results.

Inverse problems are often formulated by assuming that the underlying phe- nomenon is a dynamic system characterized by mathematical equations, although no such assumption is always essential. Often the goal is to build an algorithmic model of the underlying phenomena. In some contexts a model is only a means to an end. The ultimate goal in such cases is to test the validity of a hypothesis. In these cases, the model is used as a classifier (e.g., neural nets and decision trees), and it matters little whether the model is parametric or non-parametric; the clas- sification accuracy becomes more important. From this point of view the entire field of Machine Learning can be treated as an exercise in solving inverse problems (Bauer et al., 2007; Prato et al., 2007). By their very nature, inverse problems are difficult to solve. Sometimes they are ill-posed. A well-posed mathematical prob- lem must satisfy the following requirements: existence, uniqueness and stability. The existence problem is really a non-issue in many realistic situations because the physical reality must be a solution. However, due to noisy and/or insufficient measurement data, an accurate solution may not exist. More often, the major difficulty is to find a unique solution; this especially when solving a parameter identification problem. Different combinations of parameter values (including boundaries and boundary conditions) may lead to similar observations. One useful strategy to handle the non-uniqueness issue is to utilize a priori information as additional constraints. These constraints generally involve the imposition of requirements such as smoothness on the unknown solution or its derivatives, or positivity, or maximum entropy or some other very general mathematical prop- erty. A more aggressive approach would be the use of regularization. Given an observed data set, genetic algorithms and genetic programming can be used to search a hypothesis space.
In this paper, starting from a reformulation of the pattern recognition as an in- verse problem, we introduce an alternative learning algorithm derived by a well- known regularization method. We use a Riesz-type kernel to solve classification tasks by transforming the geometry of input space by embedding them into higher dimensional, inner product spaces, and introducing a regularization method which adds to the derived integral equation a new term, called stabilizer, which penalizes undesired input/output functions. We split the problem into a simpler, ill-posed problem (an integral equation with a Riesz-type kernel) and a well-posed problem. In this way, we isolate and better control the propagation of errors due to the ill-posedness (Noureddine, in press). Then we show that this reformulation can be employed to design a learning algorithm based upon a numerical solution of a system of linear equations.
The rest of the paper is organized as follows: The next Section describes our model and justifies its use. In Section 3, we formulate the proposed regularized learning algorithm. Section IV presents main simulation results. We compare our Regularization-based Algorithm (RBA) with the Support Vector Machine (SVM) and Semanteme-based Support Vector Machine (SSVM) in Section 5. Finally, we conclude the paper with a summary of the work in Section VI.

Generalization model as Regularization

Find a function r1 ∈ L∞(X), X ∈ ffin, given the function B(xk) = w(xk), xk ∈ Ok, Ok ∈ ffin. Therefore, we have the following integral equation of the first kind Let us formulate the generalized problem as regularization in the following way: Ar1(x)= B(x),	x ∈ Xk	(2.1)
where Ar1(x) = ∫X k(x, y)r1(y)dy and k(x, y) = (1/2p)2|x — y|—2 and A is consid- ered as an operator from L∞(O) into L∞(Ok). This integral equation is the Fred- holm integral equation of the first kind with a Riesz-type kernel.
First we need to show that Eq. (2.1) represents a severely ill-posed problem. Then we have to prove that a solution r1(a) to the Eq. (2.2) exists and is unique.

Theorem 1. Let us assume that O and Ok are nonintersecting domains in ffi3. Then the integral Eq. (2.1) with the Riesz-type kernel represents an ill-posed problem.

with the domains defined above. We then claim that whether r1 ∈ L2(O) is contin- Proof. We should notice that there are no singularities in the Riesz-type kernel uous or not, (Ar1)(x) is continuous in the usual sense. In fact,

|Ar1(x1)— Ar1(x2)| =  Z
|x — y|—2r (y)dy —
X
|x — y|—2r (y)dy

6 |r1(y)|||x1 — y|—2 — |x2 — y|—2|dy
6 r1  2.	||x1 — y|r — |x2 — y|a|2.dy
X
Since the integrand ||x1 — y|r — |x2 — y|a| is uniformly continuous, we have if
|x1 — x2| < d,
||x1 — y|r — |x2 — y|a|2 < e for ∀ y ∈ X.
Therefore,
|Ar1(x1)— Ar1(x2)| 6 r1  2.e.|X|,
|X| stands for a certain measure of X for any given e > 0.
Now it is clear that if we take any B(x) e L2(Xk) which is continuous, then there is no r1 e L2(X) such that Ar1 = B. So the existence requirement of the well- posedness is violated. Therefore the Eq. (2.1) is ill-posed.
For the integral Eq. (2.1), with a Riesz-type kernel and non-intersecting do- mains X and Xk, there is uniqueness in L2(X) Prato and Zanni, 2008. Djatlov’s work shows a logarithmic type of stability estimate (Kress, 1989).
We use the Tikhonov regularization method (Kress, 1989; Sever, 1999) to solve the ill-posed problem in Eq. (2.1). In this method, instead of Eq. (2.1), we solve the following regularized equation:

(A*A + aI)r1(x)= A*B(x)	(2.2)
where I is the identity operator, and a is a regularization parameter. Now we will
show the solution r1(a) and its convergence to the solution B when a→0, provided r1 exists and the uniqueness for the original Eq. (2.1). Now we need to prove the existence of the solution r1(a) and its convergence to the solution f when a→0, provided r1 exists and is unique.

Theorem 2. A solution r1(a) to the Eq. (2.2) exists and is unique. Also Ray defined as r1(a) is a regularizer to the Eq. (2.1) on X = XM, provided that the equation Ar1 = 0 has only zero solution.

Proof. First we assume that A is compact, so is A*A, but A*A is also self-adjoint. Thus A*A has a complete eigenfunction system denoted by {ek} with the corre- sponding eigenvalues {kk}.
Then, we have r1(a) =	kr1k(a)ek, A*y =	kBAkek using equation, we get
(kk + a)r1k(a)= BAk
which implies r1k(a)= BAk/(kk + a) is uniquely determined. Uniqueness can be
obtained without using the expression of the solution. In fact, since A*A is posi- tive, we have
((A*A + a)x, x)= (Ax, Ax)+ a(x, x) > 0 for ∀ x–0.
Therefore it cannot happen that there is some r* „ 0 such that
(A*A + a)r* = 0
which means
kernel(A*A + aI)= {0}.
Now we show that r1(a) is actually a regularizer. To this end, we may assume now
Ar1 = B where r1 e XM. Noticing that A*A + aI is also self-adjoint, we have
((A*A + a))r1(a), ek)= (A*B, ek)(A*Ar1, ek)
or
(r1(a), ((A*A + a)ek)= (r1, A*Aek)
((kk + a)(r1(a), ek)= kk(r1, ek)
with r1(a) =	k r1k(a) ek,r1 =	krkek, therefore, we get
r1k(a)= (kk/kk + a)r1

and
r1k(a)— r1k = (kk/kk + a)r1k — r1k = —(a/kk + a)r1k.

Therefore

||r1k
(a) — r1k
||2 =	|r (a)— r	2	2	+ a)2)r2 (k
1k
k	k
> 0)

< aXKK(kk + a)—2|r1k|2 + X|r1k|2

k6
Here we assume that kk P kk+1 P .. .
k>K

For all e > 0 since x ∈ L2,	k|r1k|2 converges. We first choose K such that
k|r1k|2 < e2/2.
Then, for the fixed K, we may chose d such that
a	(kk + a)—2|r1k|2 < e2/2.
k6K
Consequently,
||r1k(a)— r1k|| 6 e
It remains to prove that Ra, i.e., r1 is continuous. By observing that
r1k(a)— r1k(a0)= ((a0 — a)kk)/(kk + a)(kk + a0)r1k
And using a similar argument to what we had above, we get the continuity of r1k.
Spectral representation of self-adjoint operators in Hilbert space gives the gen- eral case. Assumption Ar1 = 0 if and only if r1 = 0 guarantees that the limit of r1(a) is unique.
Regularized learning algorithm

In this section we formulate a regularized learning algorithm based upon the Tikhonov regularization algorithm. For computational reasons, let O and Ok be the domain in ffi2 and we will regard the integral operator


Ar1(x)= 
X
r (y)/|x — y|2dy

as defined from L2(O) into L2(Ok). By using the definition of an adjoint operator in
L2, we have A*: L2(Ok) → L2(O) defined by
A*B(y)= ZXk	B(x)/|x — y|2dx	y ∈ X	(2.3)
and A*Ar1 (y) becomes
A*Ar1(y)= ZXk	ZX	r1(y')|x — y|2|x — y'|2dy'dx	(2.4)
where x ∈ Ok, and y, y' ∈ O. By discretizing Eq. (2.4), we have

Xn	2	2

j	j	j
k,j=1
n
(2.5)

ﬃ XHI,jr1(y')

where
j=1
Xn




and wj, and wk are the weight functions. By discretizing (2.3), we have
Xn



From (2.3)–(2.6) we have the discretized matrix equation in the form of
(aI + H)r1(y)= A*B	(2.7)
for some regularization parameters, a > 0. Now the problem is reduced to solving
systems of linear equations.

Simulation results

In this section we want to investigate the effectiveness of the regularized learning algorithm introduced in Section 3.
Let us denote ‘real object’ by rt, and ‘computed object’ by rc. To test the meth- od numerically, it is necessary to generate ‘B(x)’ in the integral Eq. (2.1). We do this by specifying r and evaluating the integral numerically. Once we have the numerical values of B(x), we use these as our data and recover the pattern inside the required region. The steps of test calculation are:
specify rt,
calculate the integral (2.1),
use (2.7) to find rc,
compare rt with rc.
Our test calculation used
smooth surface (Fig. 1)
rt = r0 + x1r1 + x2r2 (two objects)
where xi’s are characteristic functions of unknown objects, and r0 = ½ and rI =1 (Fig. 3). In the case of smooth surface (i), the numerical calculations have shown that when rt is a smooth polynomial, the reconstruction is a very good approximation of rc (Fig. 2).





8	A. Sever

















Figure 1	Smooth surface rt = x + y.




















Figure 2	Reconstruction of surface in Fig. 1.


tion: simply, a = 10—7 and a = 10—10. We provided the cross sections of rt and rc. In case (ii), we used two different regularization parameters for our reconstruc- The proposed model was able to distinguish the objects. We observed that the
computed rc was always smoothed. The location of the objects was well produced by rc, and also the shape of the rc was a fair indication of the objects (Figs. 4 and 5 and Table 1).
Equation (2.7) involves main parameters that must be adjusted for greatest effi- ciency: the regularization parameter and the number of grid points. Looking at the reconstructions, the numerical experiments described above have shown that the reconstructed surface is smooth and close to the true surface. The reconstruction was usually a fair representation of the shape of the r.
Summarizing, the simplicity and the reconstruction accuracy make the proposed regularized learning model well suited for the considered application.

Inverse problem approach to pattern recognition	9























Figure 3	Test domain with two objects.






















functions  of  unknown  objects,  and  r0  =  ½  and  rI  =  1,  and  a  =  10—7. Figure 4	Cross-section from the reconstruction of rt = r0 + x1r1 + x2r2 where xi’s are characteristic


Results and discussion

In this section, misclassification rate (Li and Wang, 2009) is used to evaluate the efficiency of our algorithm. Misclassification rate refers to the ratio of the number of misclassified exemplars to the total number of exemplars in the dataset. The ratio is computed using the Formulation (5.3). Correspondingly, the classification accuracy is determined by the Formulation (5.2).





10	A. Sever




















functions  of  unknown  objects,  and  r0  =  ½  and  rI  =  1,  and  a  =  10—10. Figure 5	Cross-section from the reconstruction of rt = r0 + x1r1 + x2r2 where xi’s are characteristic




cerror =	numberi/n	(5.1)
i=1,—1
cerror = 1 — c(error)	(5.2)
where numberi refers to the number of misclassified exemplars in the positive and
negative classes, and n is the total number of X. A smaller cerror value indicates higher classification accuracy and better classification efficiency. Conversely, a big- ger cerror value indicates worse classification efficiency.
Experiments are performed on the purely syntactic datasets from the UCI ma- chine learning repository (UCI, 1998). We compare our Regularization-based Algorithm (RBA) with the Support Vector Machine (SVM) and Semanteme-based Support Vector Machine (SSVM) in the Table 2. In each test, all patterns with missing attribute values are initially removed. Continuous Dataset (CDS), Discon- tinuous Dataset I (DDS-I), and Discontinuous Dataset II (DDS-II) have their fixed real and computed pattern sets. The 9% outliers existing in DDS-II’s training patterns are identified, and the dataset is classified as unbalanced. The continuous

Inverse problem approach to pattern recognition	11



attributes in datasets are preprocessed using the Formulation (5.3) Li and Wang, 2009.
xij = (xij — min{xij}))(max{xij}— min{xij})	(5.3)
i	i	i
A number of classification algorithms depend on the similarity or dissimilarity of exemplars, such as Euclidean distance and inner production, among others. How- ever, majority of these algorithms only process continuous-attributed data, not discontinuous data. Discontinuous data (surface in our examples) are extreme, having disordered and unbalanced distribution.
For parameter selection and optimization, regularized learning algorithm faces the problem of selecting parameter a. In many algorithms, the standardization of the selection method for a is rarely performed (Han and Zhao, 2009). The heuristic method or some optimization algorithm is used to select a. In this study, the heuristic method is used in each experiment to standardize datasets. Table 1 shows the relationship between classification accuracy and the value a in the RBA algorithm.

Concluding remarks and future work

In this section, we briefly discuss several results obtained and issues related to the proposed RBA learning and recognition technique. Some of these issues may be viewed as merits while others as limitations leading to open research problems for the future.
The implementation of the proposed algorithm shows that the method is reasonably accurate for the reconstruction of two objects, using artificially gener- ated data whose distributions are known. We have seen, both theoretically and experimentally, that pattern classification can be viewed as an ill-posed, inverse problem to which a method of regularization may be applied. As shown in Table 2, our proposed regularized learning algorithm has already shown promising perfor- mance in comparison with the state-of-the-art approaches, such as Support Vector Machines (SVMs), on benchmark datasets and real-life test problems.
The information obtained from a preliminary analysis is by no means exhaustive of the method discussed here and suggests several areas of additional investigation. We recognize the clear connection between regularization theory for inverse problems, and pattern recognition as learning, and this allow us to

12	A. Sever
introduce a new learning algorithm. On one front, improvements have to be done both on the algorithm (different regularizer properties must be investigated) and the applications (non-homogeneous 3-D object recognition). More detailed work is needed to improve the effectiveness of the numerics in general. In addition, the answer to exactly how sensitive the method is to moderate amounts of noise is an open question.

References
Abrukov, V.S., et al., 2006. Artificial Neural Networks and Optical Diagnostics. In: Sixth International Conference on Intelligence Systems Design and Applications.
Bauer, F., Preverzev, S., Rosasco, L., 2007. On regularization algorithms in learning theory. J. Complexity 35, 52–72.
Chang, Y.F., Lee, J.C., Mohd Rijal, O., Syed Abu Bakar, S.A.R., 2010. Efficient online handwritten Chinese character recognition system using a two-dimensional functional relationship model. Int. J. Appl. Math. Comput. Sci. 20 (4), 727–738. http://dx.doi.org/10.2478/v10006-010-0055-x.
Gdawiec, K., Domanska, D., 2011. Partitioned iterated function systems with division and a fractal dependence graph in recognition of 2D shapes. Int. J. Appl. Math. Comput. Sci. 21 (4), 757–767. http://dx.doi.org/ 10.2478/v10006-011-0060-8, ISSN (Print) 1641-876X.
Han, Xian-Pei, Zhao, Jun, 2009. The creation of semantic metadata based on Wikipedia. J. Chin. Inform.
Process. 23 (2), 108–114.
Kress, R., 1989. Linear integral equation. In: Applied Math. Sciences. Springer-Verlag, Berlin, New York. Kurkova, V., 2012. Complexity estimates based on integral transforms induced by computational units. Nueral
Netw. 33, 160–167.
Li, Zhi-Hua, Wang, Shi-Tong, 2009. Clustering with outliers-based anomalous intrusion detection[J]. Syst. Eng.
Electron. 31 (5), 1227–1230.
Lo Gerfo, L., Rosasco, L., Odone, F., De Vito, E., Verri, A., 2008. Spectral algorithms for supervised learning.
Neural Comput. 20 (7), 1873–1897.
Noureddine, S., in press. An optimization approach for the satisfiability problem, Appl. Comput. Inform. http:// dx.doi.org/10.1016/j.aci.2011.11.002.
Prato, M., Zanni, L., 2008. Inverse problems in machine learning: an application to brain activity interpretation.
J. Phys. 135.
Prato, M., Zanni, L., Zanghirati, G., 2007. On recent machine learning algorithms for brain activity interpretation. In: 23rd Annual Review of Progress in Applied Computational Electromagnetics, March 19– 23, 2007, Verona, Italy, pp. 1939–1946.
Sever, A., 1999. On uniqueness in the inverse conductivity problem. Math. Methods Appl. Sci. 22 (12), 953–966. Tiknonov, A.N., Arsenin, V., 1977. Solutions of Ill-posed Problems Transl from Russian. John Wiley Sons, New
York, Toronto.
UCI repository of machine learning database [EB/OL], 1998. Available from: <http://www.ics.UCI.edu/
~mlearn/MLRepository.html>.
Vapnik, V.N., 1998. Statistical Learning Theory. John Wiley and Sons, New York.
Yee, P., Haykin, S., 1993. Pattern classification as an ill-posed, inverse problem: a regularization approach. In: ICASSP’93 Proceedings of the 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing: Plenary, Special, Audio, Underwater Acoustics, VLSI, Neural Networks, vol. I.
