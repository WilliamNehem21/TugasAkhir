Interpreting atomization of agricultural spray image patterns using latent
Dirichlet allocation techniques
Hongfei Li a, Steven Cryer b,⁎, John Raymond b, Lipi Acharya b
a Department of Statistics, University of Illinois at Urbana-Champaign, Champaign, IL, USA
b Data Science, Corteva Agriscience, Indianapolis, IN, USA
a b s t r a c t
a r t i c l e
i n f o
Article history:
Received 30 January 2020
Received in revised form 24 October 2020
Accepted 25 October 2020
Available online 28 October 2020
Breakup patterns of agricultural formulations are explored using unsupervised learning techniques to elucidate
the mechanics of atomization for oil-in-water formulations. Previous researchers have shown these formulations
succumb to a different breakup mechanism than conventional formulations, beginning with inhomogeneities
within the liquid sheet that nucleate holes within the material being sprayed, beginning the mechanism respon-
sible for breaking the sheet into droplets. The Latent Dirichlet Allocation (LDA), a Bayesian hierarchical model, is
used to explore unsupervised learning relationships between image analysis metrics on spray video data and the
resulting atomization droplet size. Latent factors discovered by LDA were used for classiﬁcation of video seg-
ments and achieved 99.9% accuracy (3-fold cross validation). Seventy-ﬁve videos were used for regression
where each video had a unique measured droplet size distribution (D10, D50, and D90 values) for atomization.
Experiments using the features learnt by Latent Dirichlet Allocation used with regression have extremely good
results (R2 ~ 0.995 in 3-fold cross validation, R2 ~ 0.963 on never-seen videos), which serves as evidence for
the potential use of this model in image analysis of agricultural spray patterns. LDA has huge potential in both
learning and predicting atomization patterns [e.g., driftable ﬁnes (drops <150 μm)] when used with images
based on the breakup phenomena in agricultural spray. These small drop sizes that occur during atomization
have the greatest propensity for off-target movement through wind induced drift. LDA proved useful in charac-
terizing current and future formulation designs using only images as witnessed by observations and excellent
predictions summarized in this paper. In fact, these methods offer potential for use under ﬁeld conditions to ad-
dress spray performance based upon images of spray patterns at the nozzle without the need for expensive light
scattering equipment often used to measure this phenomenon.
© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
1. Introduction
Flat-fan and air-induction hydraulic nozzles, mounted on spray
booms, are the primary tools used to deliver agrochemicals to their tar-
get location. Nozzles are mounted on delivery booms for both ground
and aerial delivery systems. Pesticide mixtures being sprayed leave
the nozzle and begin as a liquid sheet which fans outward from the noz-
zle and eventually breaks up into liquid drops. Instabilities decompose
the sheet into droplets by a variety of likely mechanisms, a phenome-
non known as spray atomization (Dorman (1952)).
Agrochemical spray drift is an increasingly important ﬁeld of study
due to increased environment stewardship, legal liabilities, and the in-
direct costs of off-target effects (Palardy and Centner, 2017; Moeller,
2018; Viera et al., 2019). The resulting drop size greatly inﬂuences
how well the sprayed chemical is delivered to the target. The ultimate
size of the drops dictates both efﬁcacy and potential off target
movement (e.g., drift). Attempts at the mechanistic understanding
phenomena of spray drift can be found elsewhere (Squire, 1953;
Barlow et al., 2011; Altieri et al., 2014; Cryer and Altieri, 2017; Altieri
and Cryer, 2018).
Atomized drop size greatly inﬂuences the chemical effectiveness
when delivered to its target. Large (coarse) drops may not effectively
cover the target or may bounce upon impact with foliage. This is
contrasted by smaller drops that risk becoming entrained in ambient
air currents and carried off target, known as droplet drift. These small
drops that are susceptible to off target movement are known as driftable
ﬁnes. Driftable ﬁnes is not a standardized quantity but it is on the order
of drop diameters ≤150 μm (Cloeter et al., 2010).
Experimental observations for spray patterns for multiphase emul-
sions are also reported (Li et al., 2020; Altieri et al., 2014) and used for
comparison and training in unsupervised learning techniques. Latent
Dirichlet Allocation (LDA) models are used in this analysis which differ
from deep learning approaches and constitute a generative statistical
model that allows sets of observations to be explained by unobserved
groups that elucidate why parts of the data are similar.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
⁎ Corresponding author.
E-mail address: steven.cryer@corteva.com (S. Cryer).
https://doi.org/10.1016/j.aiia.2020.10.004
2589-7217/© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
2. Methods and materials
2.1. Data
The droplet size distribution emanating from the nozzle was ob-
tained using a Sympatec laser diffraction system shown in Fig. 1 (Fritz
and Hoffmann, 2016). This liquid sheet area, from the nozzle tip to the
distance where the sheet breaks up into drops, was imaged by video.
Spray images videos were taken by a high-speed camera (Photron®
Fastcam SA3 high-speed camera and a Nikon ED AF Nikkor®
80–200 mm lens) with 1024 × 1024 pixels resolution at 2000 frames
per second.
The dataset consists of 83 videos belonging to 44 classes. Videos of
the same class are recordings of repeated trials with exactly the same
formulations, pressure and nozzles. Each video contains around 3000
frames and a total of 263,951 frames were obtained across all videos.
Since videos were taken at different times with different nozzles,
shape, size, and position of nozzles as well as brightness of images
could change. To standardize those videos, every frame was
preprocessed. Nozzles and headers at the top of each frame were
aligned and then cropped from each image frame. The background of
the centered spray patterns was denoised. Finally, each frame was
turned into gray-scale fooled by conversion into black-and-white only
(i.e. intensity of a pixel is either 0 or 255) since intensity variance is
not relevant in our case. Example of a standardized frame is provided
in Fig. 2 (b) along with the raw frame in Fig. 2 (a).
2.2. Latent Dirichlet allocation
Deep learning models using the same dataset are reported in Li et al.
(2020). Better overall performance is anticipated when using averaging
methods if models are uncorrelated. Latent Dirichlet Allocation Models
differ from deep learning approaches, constituting a generative statisti-
cal model that allows sets of observations to be explained by unob-
served groups that elucidate why some part of the data are similar.
LDA was ﬁrst introduced for natural language processing by Blei et al.
(2003). It is a three-level hierarchical Bayesian model, in which each
item of a collection is modeled as a ﬁnite mixture over an underlying
set of topics, and each topic is modeled as an inﬁnite mixture over an
underlying set of topic probabilities. A graphic model of LDA is showed
in Fig. 3. Here, plate notation is a method of representing variables that
repeat in a graphical model in Bayesian inference. A plate (or rectangle)
is used to group variables into a subgraph that repeat together, and a
number is drawn on the plate to represent the number of repetitions
of the subgraph in the plate.
According to Blei et al. (2003), an easy way to understand LDA is to
look at the processes it assumes for each document within a
document-term matrix (e.g., corpus).
1. Choose N ~ Poisson (ξ)
2. Choose θ ~ Dir(α)
3. For each of the N words wn:
a. Choose a topic zn ~ Multinomial (θ)
b. Choose a word wn from p(wn|zn, β), a multinomial probability con-
ditions on the topic zn.
To put it simply, topics are distributions over words, and documents
are distributions over topics. Suppose we have information from many
documents that talk about news on different topics such as social,
sport, ﬁnance, politics, etc. Some words (e.g. football, win, score) are
more likely to appear in sport news than ﬁnancial news. It is natural
to assume that, each topic consists of different set of words, or more ac-
curately, each topic corresponds to a different probability distribution of
words. Moreover, a document might contain several topics such as
news that talks about an event that is related to both ﬁnance and law.
LDA captures this by assuming each document corresponds to a combi-
nation of topics (e.g., 20% political, 50% ﬁnancial, 30% social, and so on).
An image can also be treated as a document when it comes to image
processing. For example, an image of a natural scene is a document. This
scene may consist of 30% sky, 20% sea, and 50% sand, where sky, sea, and
sand are topics. The topic sky might assign higher probability to visual
Fig. 1. The experimental spray chamber, nozzle system, Sympatec laser, and high-speed video camera used to measure the atomization drop size distribution and imaging the spray sheet.
Experimental setup at Corteva Indianapolis site.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
254
words that are white or blue, while the topic sand might assign higher
probability of visual words that are yellow.
As a result, although originally a language model, LDA has been
brought into and widely used in computer vision. Applications of LDA in-
clude object recognition (Sivic et al., 2005, Fei-Fei and Perona, 2005,
Russell et al., 2006, Cao and Fei-Fei, 2010), natural scene classiﬁcation
(Fei-Fei and Perona, 2005), human action classiﬁcation (Niebles et al.,
2008), activity perception in complicated scene (Wang et al., 2007), etc.
Variants of LDA were developed for different tasks such as Spatial LDA
(Wang and Grimson, 2008) for capturing spatial structures and Corre-
spondence LDA (Blei and Jordan, 2003) for image annotation. In this
work, we apply LDA to spray pattern images to obtain interpretable un-
derlying features that can be subsequentially used in supervised learning
tasks.
2.3. Feature construction
2.3.1. Bag of words
Words, vocabulary, and documents are required to apply LDA. Thus,
visual words are extracted (which are local areas of images) in handling
image data. For example, in natural scenes, there is likely to be stones,
mountains, sky, sun, trees, and so on. These objects are characterized
as visual words, and they form the vocabulary of natural scenes. Fei-
Fei and Perona (2005) sample local regions from images by several
methods (including evenly grid sampling) and use a k-means algorithm
to cluster these local regions to get the vocabulary. In this way, a local
region of an image corresponds to a word, and an image, which is a col-
lection of words, becomes a document. Other approaches employ the
use of super-pixels to over-segment images in regions of related pixels
(Chen et al., 2017; Qiaojin et al., 2011).
A video is a sequence of images, and in some cases human action
recognition (e.g., spatial-temporal information) is critical. Niebles
et al. (2008) represented each video sequence as a collection of
spatial-temporal words by extracting space-time interest points.
However, human actions are highly structured and non-random,
while the spray videos consist of random, unstable image textures,
white dots and lines. Static instances of an image spray pattern tex-
tures are insufﬁcient to properly categorize the information content
of such dynamic, highly random phenomena. It is the temporal dis-
tribution of these image texture regions over the course of a video
stream that provide discriminability. Thus, we treat videos as bags
of visual words. Each frame is an unordered collection of visual
words, and sequence of frames are still an unordered collection of vi-
sual words. A sequence of frames are the collections of all visual
words that appear in any of these frames. We count how many
times a word appears in a document, and a document is then repre-
sented by the counts of words (known as a document-word matrix).
A bag-of-words model is often used in document classiﬁcation
where the frequency of occurrence for each word is a feature for
training a classiﬁer (McTear et al., 2016; Harris, 1954).
For example, suppose we have two documents. The ﬁrst document is
“James likes apples and Emma likes oranges.” And the second one is
“James hates oranges.” We can represent the two documents as two
bags of words (BoW):
BoW1 = {“James”:1, “likes”:2, “apples”:1, “and”:1, “Emma”:1,
“oranges”:1};
BoW2 = {“James”:1, “hates”:1, “oranges”:1};
and the document-word matrix is constructed as follows:
Fig. 2. Single raw frame example extracted from a video showcasing the header and nozzle requiring removal (red boundary) and centering image (green boundary) (a), followed by
denoising and conversion to a black-and-white only image (b).
Fig. 3. Latent Dirichlet Allocation graphic model (taken from Blei et al. (2003)). Nodes are
random variables where shaded nodes are observed and unshaded ones are unobserved.
The plates indicate repetitions. The outer plate represents documents, while the inner
plate represents the repeated choice of topics and words within a document.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
255
James
likes
hates
apples
and
Emma
oranges
Document1
1
2
0
1
1
1
1
Document2
1
0
1
0
0
0
1
2.3.2. Codebook
Similarly, we can construct a document-word matrix for images by
thinking of an image as a document and local regions of an image as
words. We cut images into small local regions, use k-means to get a dic-
tionary of visual words, and then form a bag of words representing a se-
quence of images. All images are downsampled to 500 × 500 pixels for
LDA. Each image is cut into 10 × 10 pixels small patches and we remove
all patches that are completely empty (i.e., if every pixel in a patch is 0, it
contains the black background only).
Fifteen hundred frames are randomly sampled (due to memory lim-
itations and the relative simplicity of the elements in our images) to per-
form k-means. A vocabulary size of 200 is selected (i.e., 200 cluster
centers for the k-means) and we get a dictionary containing 200 visual
words of our “language” following the k-means step. We also obtain an-
other 50 visual words using pyramid features extraction method
described in section 2.3.3. Fig. 4 illustrates the codebook that is learned
from the frames, and we use the 250 visual words in the codebook to
describe the images. We assign each local patch sampled from a frame
to the visual word that is most similar. In this way, an image becomes
a bag of visual words from the codebook.
2.3.3. Pyramid features
Since we resize each frame to 500 × 500 pixels and sample 10 × 10
pixels local patches from these frames, each patch contains a very
small portion of information in the original frame. One patch might
contain a dot or part of a line (i.e., a very low-level feature about the
image). However, we would also like to have features that describes
higher level information about the image, (such as the spray angle of
sheets), and we adopt the feature extraction method used by
Lazebnik et al. (2006). After we extract local patches from 500 × 500
pixels images, we shrink each image to 100 × 100 pixels and cut
them into 10 × 10 pixels local patches again. In this way, one patch
can contain more information in relation to the size of the image
(i.e., higher-level features such as the angle of sheets in addition to a
dot or a short line).
Fig. 4. Codebooks sorted by frequencies. Left (a) is the codebook with 200 low level visual words. Right (b) is the codebook with 50 high level visual words.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
256
Fig. 5 illustrates the approach of extracting spatial pyramid features
from an image of spray emanating from a nozzle in order to obtain our
codebook of high-level visual words (right side, Fig. 4). We extract low-
level features from high-resolution images and extract high-level
Fig. 5. Pyramid features extraction. Low level patches (a) from high resolution images of atomization process. High level patches (b) from low resolution images.
Fig. 6. Topic proportion of four example videos. Each subplot is a video that consists of many video segments. Each line in the plot is the topic proportion of a video segment. The top (a) and
bottom (b) video each belongs to different classes.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
257
features from low-resolution images. We ﬁt other k-means to these
higher-level patches, and learn a new codebook, where the vocabulary
size is selected as 50. These 50 words and the previous 200 words con-
sists of the complete vocabulary of size 250. Every frame is turned into a
bag of these visual words. Each word is counted to construct the
document-word matrix.
3. Results
3.1. Clustering results and insights
Data consists of 83 high speed videos belonging to 44 classes (where
most classes contain two videos). Most videos contain around 3000
frames. Each frame is transformed into black-and-white image. We
split each video into smaller video segments where each video segment
consists of 25 consecutive frames. We treat a video segment as a docu-
ment (instead of a frame as a document) where most videos contain
around 100 video segments. Most frames sizes are approximately
1000 × 1000 pixels, but we resize them into 500 × 500 pixels.
The dataset is split randomly into a small training set (for learning
the codebook) and a test set.
Approximately 200 low level visual words and 50 high level visual
words are learned in training. We set the number of topics of the Latent
Dirichlet Allocation model as 44. Although we use same number of
topics as classes, we do not expect one-to-one correspondence between
topics and classes. Usually, a class is deﬁned by multiple dominant
topics.
The unnormalized topic proportion for each document, or video seg-
ment, is obtained after ﬁtting the Latent Dirichlet Allocation model. A
class is deﬁned by its unique topic proportions, and this topic proportion
is used to analyze similarity and difference between classes. The topic
proportion for two example videos is provided in Fig. 6. Each line in
the plot is the topic proportion of a video segment. Each subplot con-
tains video segments that come from the same video. We can see that
video segments that come from the same video have very similar
topic proportions. A video class represents the formulation attributes
that lead to how the material breaks up into droplets and the resulting
drop size. And the topic proportion of video segments of this class are
almost the same. The top and bottom video each belongs to different
classes, Fig. 6. Thus, the topic proportions are different.
The top two principle components (PCA) of topic proportion of each
video segment are represented in Fig. 7, with different colors indicating
different classes. PCA produces a low-dimensional representation of a
dataset by ﬁnding a sequence of linear combinations of the variable hav-
ing maximum variance that are mutually uncorrelated and serves as a
tool for data visualization. A dot is a video segment. A cloud is a video.
From Fig. 7, one sees that video segments of the same class are mostly
grouped together.
Fig. 7. Principle Components Analysis of topic proportion of each video segment using the ﬁrst two principle components. A dot is a video segment. A cloud is a video. Different color
indicating different classes.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
258
Fig. 8 shows the dendrogram of classes by Euclidian distance of their
topic proportions (i.e., a tree diagram used to illustrate the arrangement
of the clusters produced by hierarchical clustering). A dendrogram is
useful to explore the numbers of various clusters that form between dif-
ferent video classes with similar formulations clustering close to one an-
other. However, often dramatically different formulations can cluster
together (similar results for PCA).
Topics are distributions over words and one can further examine
what these topics are. A crude idea of topic descriptions is determined
by looking at the high frequency words of a certain topic. In Fig. 9, the
majority of ﬁrst topic is dots, while polygons, or holes, appear in the sec-
ond topic more frequently. The third topic is about vertical lines, and the
last example shows lines of different orientations. If a class has a high
proportion of the ﬁrst topic and low proportion of the last topic, there
might be more dots than lines in the videos of this class.
3.2. Video segment classiﬁcation
Latent Dirichlet Allocation discovers the topic distribution which de-
ﬁne classes in an unsupervised fashion. Each class has a unique topic
proportion. Supervised models can be built if topic proportions are
taken as a feature vector that describes each class. Linear Discriminant
Analysis is chosen to perform classiﬁcation using topic distributions.
There is a total of 10,461 video segments, 83 videos, 44 classes for the
spray atomization videos. Video segments originating from the same
video share the same class label. Unique classes represent video obser-
vations that yield similar behavior.
The topic proportion of each video segment is used as its features.
Even when running videos at steady state, the dynamic nature for the
ﬂuid sheets suggest that it is possible for video segments to be
misclassiﬁed into different classes depending upon where in the process
the video segments is taken. Thus, the dataset is ﬁrst split into training
and testing sets by video segments (i.e., 2/3 of all video segments are
train data, 1/3 of all video segments are test data). A Linear Discriminant
Analysis model is trained via the training set, and predictions for the
class of video segments (25 frames) are analyzed in the test set. Accu-
racy of 3-fold cross validation is 99.9%. Table 1 shows the high cross-
validation accuracy of the classiﬁcation for this approach. A possible
explanation to the high accuracy may be due to having multiple video
segments for a single video.
For further insights into generalizability of these features, the dataset
is then split into train and test set by videos. There are 31 classes con-
taining two or more different videos. For those classes, video segments
coming from one video are in train set, and video segments coming from
another video are in test set. This approach results in 5742 video seg-
ments in train set and 4719 video segments in test set. Again, a Linear
Discriminant Analysis model is used to classify these video segments,
and it achieved 85.9% accuracy.
Table 1 summarizes the classiﬁcation results. The high classiﬁcation
accuracy shows that the topic distributions extracted by Latent Dirichlet
Fig. 8. Dendrogram of classes by Euclidian distance of their topic proportions.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
259
Allocation successfully captured and summarized each video segment
and proved to be good features for classiﬁcation.
3.3. Drop size prediction
Feature vectors are also used to do regression in addition to classiﬁ-
cation. Only 75 of the 83 videos utilized for Latent Dirichlet Allocation
had atomization drop size distributions summarized for three measure-
ments (i.e., D10, D50, and D90 values representing drop size particle di-
ameter corresponding to 10, 50 and 90% cumulative), leaving 9621
video segments (75 videos) for the regression task. The independent
variables are the topic proportion of each video segments (smaller sub-
set of main video) and are different for each video segments (although
the difference is extremely small if two video segments are derived from
the same video). The dependent variables are the three measurements,
D10, D50, and D90.
Data is split into a train and test set by video segments (e.g., 2/3 of all
the video segments are train data, and 1/3 of all video segments are test
data). A polynomial regression with degree 2 is ﬁt to the train set and
make predictions on test set. Table 2 summarizes the result. The average
Fig. 9. Top 25 words of 4 example topics. Topic in (a) mostly consists of dots. Topic in (b) consists of polygons and holes. Topic in (c) consists of vertical lines and dots. Topic in (d) consists
of horizontal lines, slashes and dots.
Table 1
Classiﬁcation accuracy with Linear Discriminant Analysis.
Split method
Average classiﬁcation accuracy
3-fold cross validation by video segments
99.9%
Split by videos
85.9%
Table 2
Drop Size Prediction.
Model
Mean
squared
error
Explained
variance
Latent Dirichlet Allocation + Polynomial Regression
(3-fold CV)
51.6
99.5%
CNN + RNN (Li et al., 2020)
39.5
99.6%
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
260
R-squared value is 99.5% for a 3-fold cross validation. We also list the re-
sult of a deep learning regression model that uses the same videos and
D10, D50, D90 data reported earlier for comparison (Li et al., 2020). The
train-test split ratio is 2:1 for both models.
In addition, similar to video segment classiﬁcation, the dataset is
also split into train and test sets by videos, where video segments com-
ing from the same video are either in train set or test set. This ap-
proach results in 5550 video segments in train set and 4071 video
segments in test set. A polynomial regression with degree 2 is used,
mean squared error of test set is 361.2, and R-squared value of test
set is 96.3%.
4. Conclusions
The mechanism for delivering many pesticides to the target site is
by atomization of spray using hydraulic nozzles. Atomization shows
rich diversity and is a function of formulation type, composition, the
number of phases within the formulation, spray pressure, nozzle ge-
ometry, and so on. Breakup patterns from spray nozzles are explored
using unsupervised learning techniques to elucidate the mechanics of
atomization for oil-in-water formulations. The Latent Dirichlet Alloca-
tion (LDA), a Bayesian hierarchical model, is used to perform unsuper-
vised learning on video spray data for agricultural formulations. The
LDA model discovers and learns about the latent factors for the data
to provide insight on information that is much more interpretable
than that obtained via black box methods. Latent factors discovered
by LDA were used for supervised learning tasks including classiﬁcation
and regression.
A Linear Discriminant Analysis model was able to classify video seg-
ments with 99.9% accuracy (3-fold cross-validation) based on those la-
tent factors. Seventy-ﬁve videos were used for regression where each
video had a unique measured droplet size distribution (D10, D50, and
D90 values) for atomization. The primary experiments using the fea-
tures learnt by Latent Dirichlet Allocation used with regression (3-fold
cross-validation) have extremely good results (R2 ~ 0.995). Further ex-
periments where never-before-seen videos were held out as test set
still give good results (R2 ~ 0.963), which serves as evidence for the po-
tential use of this model in image analysis of agricultural spray patterns.
The Latent Dirichlet Allocation model offers huge potential to learn and
predict atomization patterns (e.g., especially driﬁtable ﬁnes) when used
with images based on the multiphase breakup phenomena witnessed in
agricultural spray drift. Small drops of sizes <150 μm offer the greatest
propensity for off-target movement via wind induced drift, and LDA
methods can be used to offer insight into the drop sizes manifest with
spraying oil-in-water formulations. Images can be obtained from videos
obtained in spray chambers (this work) or from real time videos of
spray phenomena at the source along a boom in ﬁeld applications.
Thus, process control can come into play to keep drop sizes within a
targeted range (e.g., spray performance).
Author Statement
Hongfei Li was a principle author involved in all aspects of this
work. She was a Master's Student in Statistics at the University of Il-
linois Urbana Champaign at the time of this work. Hongfei was su-
pervised by Lipi Acharya and Steven Cryer of Corteva Agriscience
(Data Science and Informatics) when she was ﬁnishing her Master's
degree at UIUC. Dr. Acharya and Dr. Cryer provided the necessary
ideas, support, feedback, guidance, and interactions for this work.
Dr. John Raymond of Corteva Agriscience also provided technical
guidance.
Declaration of Competing Interest
No conﬂict of interest exists.
Acknowledgements
Dr. Navin Elango provided critical review and feedback of this man-
uscript. Hongfei Li was a master's student at the University of Illinois,
Urbana-Champaign (UIUC) working with Corteva Agriscience as an in-
tern at the time this work was undertaken. All research was funded by
Corteva Agriscience.
Appendix A. Supplementary data
Supplementary data to this article can be found online at https://doi.
org/10.1016/j.aiia.2020.10.004.
References
Altieri, A.L., Cryer, S.A., 2018. Break-up of sprayed emulsions from ﬂat-fan nozzles using a
hole kinematics model. Biosystems Engineering 169, 104–114.
Altieri, A., Cryer, S.A., Acharya, L., 2014. Mechanisms Experiment and Theory of Liquid
Sheet Breakup and Drop Size from Agricultural Nozzles. Atomization and Sprays 24
(8), 695–721.
Barlow, N.S., Helenbrook, B.T., Lin, S.P., 2011. Transience to instability in a liquid sheet.
J. Fluid Mech. 666, 358.
Blei, D.M., Jordan, M.I., 2003. Modeling annotated data. Proceedings of the 26th annual in-
ternational ACM SIGIR conference on Research and development in information re-
trieval, pp. 127–134.
Blei, D.M., Ng, A.Y., Jordan, M., 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3
(4–5), 993–1022.
Cao, L., Fei-Fei, L., 2010. Spatially coherent latent topic model for concurrent object seg-
mentation and classiﬁcation. Proceedings of IEEE Intern. Conf. in Computer Vision
(ICCV), p. 2010.
Chen, C., Zare, A., Trinh, H.N., Omotara, G.O., Cobb, J.T., Lagaunne, T.A., 2017. Partial Mem-
bership Latent Dirichlet Allocation for Soft Image Segmentation. IEEE Transactions on
Image Processing 26 (12), 5590–5602.
Cloeter, M.D., Qin, K., Patil, P., Smith, B., 2010. Planar Laser Induced Fluorescence (PLIF)
ﬂow visualization applied to agricultural spray nozzles with sheet disintegration; In-
ﬂuence of an oil-in-water emulsion. ILASS Americas 22nd Annual Conference on Liq-
uid Atomisation and Spray Systems, Cincinnati, OH, May 2010.
Cryer, S.A., Altieri, A.L., 2017. Role of large inhomogeneities in initiating liquid sheet
breakup in agricultural atomization. Biosyst. Eng. 1693, 103–115.
Dorman, R.G., 1952. The atomization of liquid in a ﬂat spray. Br. J. Appl. Phys. 3, 189.
Fei-Fei, L., Perona, P., 2005. A bayesian hierarchical model for learning natural scene cat-
egories. Computer Vision and Pattern Recognition, CVPR 2005. IEEE Computer Society
Conference. 2, pp. 524–531.
Fritz, B.K., Hoffmann, W.C., 2016. Measuring Spray Droplet Size from Agricultural Nozzles
Using Laser Diffraction. J. Vis. Exp. 115, e54533. https://doi.org/10.3791/54533.
Harris, Z., 1954. Distributional structure. Word 10 (23), 146–162.
Lazebnik, S., Schmid, C., Ponce, J., 2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. 2006 IEEE computer society con-
ference on computer vision and pattern recognition (CVPR’06). 2, pp. 2169–2178.
Li, H., Cryer, S., Acharya, L., Raymond, J., 2020. Video and image classiﬁcation using
atomisation spray image patterns and deep learning. Biosyst. Eng. 200, 13–22.
McTear, M.F., Callejas, Z., Griol, D., 2016. The conversational Interface, talking to smart de-
vices. 6, no. 94. Springer, Cham, p. 102.
Moeller, D.L., 2018. Superfund, pesticide regulation, and spray drift: rethinking the Fed-
eral Pesticide Regulatory Framework to provide alternative remedies for pesticide
damage. Iowa L. Rev. 104, 1523.
Niebles, J.C., Wang, H., Fei-Fei, L., 2008. Unsupervised learning of human action categories
using spatial-temporal words. Int. J. Comput. Vis. 79 (3), 299–318.
Palardy, N., Centner, T.J., 2017. Improvements in pesticide drift reduction technology
(DRT) call for improving liability provisions to offer incentives for adoption. Land
Use Policy 69, 439–444.
Qiaojin, G., Ning, L., Yubin, Y., Ganghan, W., 2011. Supervised LDA for Image Annotation.
2011 IEEE international conference on systems, Man, and Cybernetics.
Russell, B.C., Freeman, W.T., Efros, A.A., Sivic, J., Zisserman, J.A., 2006. Using multiple seg-
mentations to discover objects and their extent in image collections. 2006 IEEE Com-
puter Society Conference on Computer Vision and Pattern Recognition (CVPR’06).
Vol. 2, pp. 1605–1614.
Sivic, J., Russell, B.C., Efros, A.A., Zisserman, A., Freeman, W.T., 2005. Discovering object
categories in image collections. Computer Science and Artiﬁcial Intelligence Labora-
tory Technical Report, MIT-CSAIL-TR-2005-012, Feb 25 AIM-2005-005.
Squire, H.B., 1953. Investigation of the instability of a moving liquid ﬁlm. Brit. J. Appl. Phys.
4, 167–169.
Viera, B.C., Luck, J.D., Amundsen, K.L., Gains, T.A., Werle, R., Kruger, G.R., 2019. Response of
Amaranthus spp. following exposure to sublethal herbicide rates via spray particle
drift. PloS one 14 (7), e0220014.
Wang, X., Grimson, E., 2008. Spatial latent Dirichlet allocation. Adv. Neural Inf. Proces.
Syst. 20, 1577–1584.
Wang, X., Ma, X., Grimson, E., 2007. Unsupervised activity perception by hierarchical
bayesian models. 2007 IEEE conference on computer vision and pattern recognition.
IEEE., pp. 1–8.
H. Li, S. Cryer, J. Raymond et al.
Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
261
