Classiﬁcation random forest with exact conditioning for spatial prediction of
categorical variables
Francky Fouedjio
AngloGold Ashanti Australia Ltd., Growth and Exploration, 140 St. Georges Terrace, Perth, WA, 6000, Australia
A R T I C L E I N F O
Keywords:
Categorical variable
Classiﬁcation
Exact conditioning
Principal component analysis
Signed distance
Spatial prediction
Quadratic programming
A B S T R A C T
Machine learning methods are increasingly used for spatially predicting a categorical target variable when
spatially exhaustive predictor variables are available within the study region. Even though these methods exhibit
competitive spatial prediction performance, they do not exactly honor the categorical target variable's observed
values at sampling locations by construction. On the other side, competitor geostatistical methods perfectly match
the categorical target variable's observed values at sampling locations by essence. In many geoscience applica-
tions, it is often desirable to perfectly match the observed values of the categorical target variable at sampling
locations, especially when the categorical target variable's measurements can be reasonably considered error-free.
This paper addresses the problem of exact conditioning of machine learning methods for the spatial prediction of
categorical variables. It introduces a classiﬁcation random forest-based approach in which the categorical target
variable is exactly conditioned to the data, thus having the exact conditioning property like competitor geo-
statistical methods. The proposed method extends a previous work dedicated to continuous target variables by
using an implicit representation of the categorical target variable. The basic idea consists of transforming the
ensemble of classiﬁcation tree predictors' (categorical) resulting from the traditional classiﬁcation random forest
into an ensemble of signed distances (continuous) associated with each category of the categorical target variable.
Then, an orthogonal representation of the ensemble of signed distances is created through the principal
component analysis, thus allowing to reformulate the exact conditioning problem as a system of linear inequalities
on principal component scores. Then, the sampling of new principal component scores ensuring the data's exact
conditioning is performed via randomized quadratic programming. The resulting conditional signed distances are
turned out into an ensemble of categorical outputs, which perfectly honor the categorical target variable's
observed values at sampling locations. Then, the majority vote is used to aggregate the ensemble of categorical
outputs. The effectiveness of the proposed method is illustrated on a simulated dataset for which ground-truth is
available and showcased on a real-world dataset, including geochemical data. A comparison with geostatistical
and traditional machine learning methods show that the proposed technique can perfectly match the categorical
target variable's observed values at sampling locations while maintaining competitive out-of-sample predictive
performance.
1. Introduction
The spatial prediction of a categorical target variable when auxiliary
spatial information is available everywhere within the region under study
has become ubiquitous in geosciences. Typical examples include pre-
dictinglanduse classes,landcover categories, drainageclasses,vegetation
species, landslide types, rocktypes, soiltypes, lithofacies, hydrofacies, and
geological units. The mapping of categorical variables plays an essential
role in a wide variety of geoscience applications. It is used to aid in risk-
aware decision-making in many areas, such as environmental studies
and natural resource management. Various methods have been proposed
for spatially predicting categorical target variables when spatially
exhaustivepredictor variables areavailable withinthe study region.These
methods include geostatistical methods (Goovaerts, 2001; Hengl et al.,
2004, Hengl et al., 2007), generalized linear mixed models-based ap-
proaches (Cao et al., 2011, Cao et al., 2014), and classiﬁcation machine
learning techniques (Kanevski, 2008; Kanevski et al., 2009; Hengl et al.,
2018; Maxwell et al., 2018; Du et al., 2020; Giaccone et al., 2021).
Geostatistical methods for spatially predicting a categorical variable
in the presence of auxiliary spatial information available everywhere
E-mail addresses: ffouedjio@anglogoldashanti.com, francky.fouedjio@gmail.com.
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.11.003
Received 13 August 2021; Received in revised form 25 October 2021; Accepted 30 November 2021
Available online 11 December 2021
2666-5441/© 2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
within the region under study include indicator kriging with external
drift (IKED), regression-kriging of indicators (RKI), and regression-
kriging of memberships (RKfM). Indicator kriging with external drift
(IKED) assumes that auxiliary variables are linearly related to the class
occurrence of the categorical target variable (Goovaerts, 2001). The
auxiliary variables are incorporated into the indicator kriging system as
deterministic linear functions. Its implementation is challenging since it
is often problematic to simultaneously estimate the parameters of
external drift and the covariance function of the stochastic component.
Regression-kriging of indicators (RKI) combines multinomial logistic
regression of the categorical target variable on predictor variables with
kriging of the regression residuals (Hengl et al., 2004, Hengl et al., 2007).
Thus, the regression modeling is supplemented with the modeling of
variograms for regression residuals, which are then interpolated and
added back to the regression estimate. RKI has been adapted to
regression-kriging of memberships (RKfM) by substituting crisp indicator
values with continuous membership values (Hengl et al., 2007). Indeed,
under the RKI method, the interpolation of residuals might lead to values
outside the physical range (< 0 or > 1). Although easy to implement,
these indicator kriging-based methods have some well-known short-
comings. The predicted probabilities of the target variable's categories
are not guaranteed to belong to the [0, 1] interval and sum up to one.
Therefore, post-processing methods of the predicted probabilities are
required (Bogaert, 2004; Allard et al., 2011). Also, under these methods,
the outcome values of the conditional cumulative distribution function
may not be monotonic. A posterior correction of the resulting conditional
probabilities is often necessary either through a Gaussian transformation
or via a logistic regression model (Pardo-Igúzquiza and Dowd, 2005).
Another alternative for spatially predicting a categorical response
variable in the context of spatially exhaustive auxiliary information
available within the spatial domain of interest consists of using gener-
alized linear mixed models-based approaches. Cao et al. (2011) propose a
spatial multinomial logistic mixed model in which spatially correlated
latent variables are assumed to account for the spatial dependency in the
categorical target variable. The proposed model is represented as a
multinomial logistic function of spatial covariances between target and
sampling locations. The sought-after class occurrence probability func-
tion for a target location is written as a multinomial logistic linear
combination of covariance values between the target and source data
locations, which can be analogous to the dual form of kriging methods.
This method was later extended to incorporate heterogeneous auxiliary
information for spatial prediction of categorical variables (Cao et al.,
2014). These generalized linear mixed models-based approaches are free
of the aforementioned inherent problems of indicator kriging-based
methods. However, they are computationally intensive compared to in-
dicator kriging-based methods.
Machine learning techniques are increasingly used for spatially pre-
dicting a categorical response variable when auxiliary information is
available everywhere within the study region. Indeed, the number of
predictor variables that help explain the spatial variation in the target
variable has grown dramatically, making other methods cumbersome to
apply. Also, some machine learning methods are well-known for
handling complex non-linear relationships and interactions and require
less data pre-processing. Classiﬁcation machine learning methods have
proven relevant for spatially predicting categorical variables in many
research works, including Albrecht et al. (2021); Kumar et al. (2020);
Hengl et al. (2018); Latifovic et al. (2018); Kuhn et al. (2018); Sahoo and
Jha (2017); Othman and Gloaguen (2017); Cracknell and Reading (2015,
2014); Yu et al. (2012). Even though classiﬁcation machine learning
methods
(e.g.,
random
forest,
support
vector
machines)
exhibit
competitive spatial prediction performance, they do not exactly honor
the categorical target variable's observed values at sampling locations by
construction. On the other side, competitor geostatistical techniques
(such as regression-kriging of indicators) perfectly match the categorical
target variable's observed values at sampling locations by essence. In
many geoscience applications, it is desirable to perfectly match the
observed values of the categorical target variable at sampling locations,
especially when the categorical target variable's measurements can be
reasonably considered error-free (hard data).
This work addresses the problem of exact conditioning of machine
learning methods for the spatial prediction of categorical variables. It in-
troduces a classiﬁcation random forest-based method in which the cate-
gorical target variable is exactly conditioned to the data, thus having the
exact conditioning property like competitor geostatistical methods. The
random forest popularity for spatial prediction relies on its ability to efﬁ-
ciently deal with many predictor variables, handle complex nonlinear re-
lationships and interactions, and require less data pre-processing, and be a
non-parametric method. The proposed approach extends a previous work
dedicated to continuous target variables by using an implicit representation
of the categorical target variable (Fouedjio, 2020). The exact conditioning
to the data is achieved through a step-by-step approach. First, classiﬁcation
Fig. 1. Signed distance transform approach - (a) categorical spatial variable with two categories; (b) signed distance function associated with category 0; (c) signed
distance function associated with category 1.
Table 1
Simulated data example - simulation parameters.
Mean
Covariance function
Type
Scale
Sill
X1(⋅)
10
Gaussian
12
1
X2(⋅)
10
Exponential
7
1
X3(⋅)
10
Cardinal Sine
1
1
X4(⋅)
10
Cubic
20
1
ϵ(⋅)
0
Spherical
10
500
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
83
tree predictors' ensemble (categorical) resulting from the traditional clas-
siﬁcationrandom forest istransformedinto an ensemble of signed distances
(continuous) corresponding to each category of the categorical target var-
iable. Second, an orthogonal representation of the ensemble of signed dis-
tances is created through the principal component analysis. Third, the exact
conditioning problem is reformulated as a system of linear inequalities on
principal component scores, thus allowing the sampling of new principal
component scores (via the randomized quadratic programming), ensuring
the exact conditioning to the data. Fourth, the resulting conditional signed
distances are turned out into an ensemble of categorical outputs, which
perfectly honorthe categoricaltarget variable's observedvaluesatsampling
locations. Finally, the majority vote is used to aggregate the ensemble of
categorical outputs. The ﬁnal output also matches the target variable's
observed values atsampling locationsby construction. On the one hand, the
proposed method's effectiveness is illustrated on a simulated dataset for
which the ground truth is available. On the other hand, the proposed
techniqueisexhibitedonareal-worlddatasetcomprisinggeochemicaldata.
A comparison is also made with geostatistical and classical machine
learning methods (regression-kriging of indicators, random forest, and
support vector machines).
The remainder of the paper is structured as follows. Section 2 de-
scribes the different ingredients required to apply the proposed method.
Fig. 2. Simulated data example - (a), (b), (c), (d) predictor variables, (e) exhaustive categorical target variable, and (f) sampled categorical target variable.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
84
Fig. 3. Simulated data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machines; (c) cat-
egorical target variable’ observed values at training locations. The misclassiﬁcation rate in the training data is 18.20% and 35.00%, respectively, for the traditional
random forest and support vector machines.
Fig. 4. Simulated data example - B ¼ 10 000 unconditional ﬁrst PC scores and T ¼ 1000 conditional ﬁrst PC scores.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
85
Section 3 demonstrates the proposed approach's effectiveness on a syn-
thetic dataset as well as a real-world dataset. A comparison with geo-
statistical and classical machine learning methods is provided. Section 4
offers concluding remarks.
2. Methodology
Let {C(s): s 2 D} be the categorical target variable deﬁned on a ﬁxed
continuous spatial domain of interest D⊂Rd, with a ﬁnite set of possible
categorical outputs (categories) {c1, …, cK} which are mutually exclusive
and collectively exhaustive. There exist n categorical target variable's
observed values fCðsiÞgi¼1;…;n (hard data) corresponding to sampling
locations fsi 2 Dgi¼1;…;n. In addition to the categorical target variable,
there is a set of predictor variables {x1(s), …, xp(s): s 2 D} exhaustively
known in the spatial domain D. We address the problem of predicting the
categorical target variable over the spatial domain D represented by N
grid locations using the categorical target variable's observed values and
predictor variables' data. In addition, the categorical target variable's
predicted values at sampling locations must be the same as the categor-
ical target variable's observed values at sampling locations, i.e., ^CðsiÞ ¼
CðsiÞ; i ¼ 1;…;n. The description of the different ingredients needed to
implement the proposed exact conditioning method is given in this sec-
tion. The implementation is carried out in the R platform (R Core Team,
2020).
2.1. Random forest classiﬁer
The ﬁrst step of the proposed method consists of training the tradi-
tional random forest (RF) classiﬁer on the data. Random forest classiﬁer
is an ensemble method where several individual decision trees are
trained on various subsets of the training dataset (bootstrap samples)
using different subsets of available predictor variables, followed by an
aggregation (Breiman, 2001). The bootstrapping of the training data and
the random selection of subsets of predictor variables ensure that each
decision tree in the random forest is unique, which reduces the overall
variance of the random forest classiﬁer. For the ﬁnal decision, the RF
classiﬁer aggregates the decisions of individual trees through a voting
scheme such as the majority voting, i.e., for each observation, each de-
cision tree votes for one category, and RF chooses the category with the
highest number of votes.
Classiﬁcation random forest has some tuning parameters that can be
optimized. There are, among others, the number of trees, number of
predictor variables randomly selected at each node, proportion of ob-
servations to sample in each decision tree, and minimum number of
observations in a decision tree's terminal node. These hyperparameters
are optimized via cross-validation. In practice, there is no need to tune
the number of decision trees; it is usually recommended to set it to a large
number, allowing the convergence of the prediction error to a stable
minimum (Hengl et al., 2018). The implementation of the classiﬁcation
random forest is carried out using the R packages ranger (Wright and
Fig. 5. Simulated data example - prediction map for (a) traditional classiﬁcation random forest, (b) support vector machines, (c) regression-kriging of indicators, and
(d) classiﬁcation random forest with exact conditioning.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
86
Ziegler, 2017) and tuneRanger (Probst et al., 2018).
The training of the classical random forest classiﬁer results in an
ensemble of classiﬁcation tree predictors f~CbðsÞ : s 2 Dgb¼1;…;B, where B
is the number of decision trees. As the traditional random forest classiﬁer
is not explicitly designed to match the data perfectly, classiﬁcation tree
predictors and the aggregated classiﬁcation tree predictors do not
necessarily match the categorical target variable's observed values at
sampling locations. Since f~CbðsÞ : s 2 Dgb¼1;…;B do not match data
perfectly, they will be called “unconditional classiﬁcation tree pre-
dictors”. The next steps aim to generate conditional classiﬁcation tree
predictors that perfectly ﬁt the categorical target variable's observed
values at sampling locations.
2.2. Signed distance transform
The second step of the proposed method includes transforming the
unconditional classiﬁcation tree predictors’ ensemble (categorical) into
an ensemble of unconditional signed distance functions (continuous)
corresponding to each category of the categorical target variable. The
categorical target variable {C(s): s 2 D} with K categories fckgk¼1;…;K can
be viewed as a variable that creates distinct boundaries in the study re-
gion D. Each category ck(k ¼ 1, …, K) can be codiﬁed by a binary variable
indicating their presence or absence: Ik(s) ¼ 1 if C(s) ¼ ck, and Ik(s) ¼ 0 if
C(s) 6¼ ck, 8s 2 D. Each category ck can be represented by a signed dis-
tance function φk(⋅) such that ck ¼ {s 2 D, φk(s) � 0}. The signed distance
transform approach (Grevera, 2007; Davies, 2012) can be used to
transform each category ck ({Ik(s): s 2 D}) into a signed distance function
φk(⋅). Indeed, each category ck deﬁnes a p-dimensional binary image
{Ik(s): s 2 D} where each point (pixel) has either a value of 1 indicating
the presence of the category ck or a value of 0 indicating the absence of
the category ck. For every point (pixel) set to 1, a distance transform
assigns a value indicating the negatively signed distance from that point
(pixel) to the nearest point (pixel) set to 0. Similarly, for every point
(pixel) set to 0, a distance transform assigns a value indicating the
positively signed distance from that point (pixel) to the nearest point
(pixel) set to 1. An illustration of the signed distance transform method is
given in Fig. 1. Additionally, the signed distance transformation is
one-to-one. The bijectivity is obtained using the following rule:
CðsÞ ¼ argmin
c1;…;cK
ðφ1ðsÞ; …; φKðsÞÞ; 8s 2 D:
(1)
Thus, the ensemble of unconditional classiﬁcation tree predictors
f~CbðsÞ : s 2 Dgb¼1;…;B is converted to an ensemble of unconditional
Fig. 6. Simulated data example - prediction uncertainty (entropy) map for (a) traditional classiﬁcation random forest, (b) support vector machines, (c) regression-
kriging of indicators, and (d) classiﬁcation random forest with exact conditioning.
Table 2
Simulated data example - predictive performance statistics in the testing dataset
containing 39 500 observations.
Methods
Accuracy
Rand index
Random Forest
0.619
0.734
Support Vector Machines
0.602
0.729
Regression-Kriging of Indicators
0.623
0.746
Random Forest with Exact Conditioning
0.639
0.747
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
87
signed distance functions
�
~φb
kðsÞ : s 2 D
�
b¼1;…;B for each category ck(k
¼ 1, …, K) via the signed distance transform approach previously
described. The following idea uses principal component analysis to
create an orthogonal representation of the ensemble of unconditional
signed distance functions. Then, the exact conditioning problem is
reformulated as a linear inequality problem on the principal compo-
nent scores. Then, new principal component scores that ensure the
exact conditioning to the hard data are generated through the ran-
domized
quadratic
programming.
Since
the
principal
component
orthogonalization is bijective, conditional signed distance functions are
obtained by reconstruction. The combination rule deﬁned in Eq. (1) is
applied to obtain conditional classiﬁcation tree predictors (categorical)
that exactly match the categorical target variable's observed values at
sampling locations.
2.3. Principal component analysis
This step consists of performing principal component analysis (PCA)
on
each
ensemble
of
unconditional
signed
distance
functions
�
~φb
kðsÞ : s 2 D
�
b¼1;…;B ðk ¼ 1; …; KÞ. This results in the following
decomposition in ﬁnite dimensions:
~φb
kðsÞ ¼
XL
l¼1 αb
l;kψl;kðsÞ; 8s 2 D; b ¼ 1; …; B; k ¼ 1; …; K;
(2)
where fαb
l;kgl¼1;…;L are principal component scores (coefﬁcients) and
fψl;kðsÞ : s 2 Dgl¼1;…;L are principal components factors (eigen-functions);
L ¼ min(B, N).
For each category ck(k ¼ 1, …, K), PCA is applied to a matrix Γk(B �
N) arranged as a set of B row vectors, each representing a single un-
conditional signed distance function
�
~φb
kðsÞ : s 2 D
�
. PCA is paralleliz-
able for each matrix Γk(k ¼ 1, …, K). In Eq. (2), the ensemble
�
~φb
kðsÞ : s 2 D
�
b¼1;…;B can be viewed as a set of images and
�
~φb
kðsÞ : s 2 D
�
as
an
image.
Thus,
the
resulting
principal
components
factors
fψl;kðsÞ : s 2 Dgl¼1;…;L are images as well. Hence, Eq. (2) provides a
decomposition of the images into a set of eigen-images and a set of co-
efﬁcients. It is important to note that in the PCA framework, the eigen-
functions are considered ﬁxed, while the coefﬁcients are considered
Fig. 7. Real-world data example - some predictor variables: (a) elevation, (b) Landsat 8 band 6, (c) gravity survey high-pass ﬁltered Bouguer anomaly, (d) potassium
counts from gamma ray spectrometry.
Fig. 8. Real-world data example - (a) categorical target variable and (b) training and testing locations.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
88
random. The PCA is used here more as an orthogonal decomposition
method than a dimension reduction technique since all the principal
component factors are kept, as shown in Eq. (2). The bijective nature of
PCA allows the reconstruction of signed distance functions from co-
efﬁcients. In other words, an image can be reconstructed back once all
the principal component factors and scores are used.
2.4. Randomized quadratic programming
Given the PCA decomposition of the ensemble of unconditional
signed distance functions as described in Sect. 2.3, this step consists of
generating new principal component scores such that signed distance
functions deﬁned in Eq. (2) perfectly match the data. Let
φkðsÞ ¼
XL
l¼1 θl;kψl;kðsÞ;
8s 2 D;
k ¼ 1; …; K;
(3)
where fθl;kgl¼1;…;L are random coefﬁcients and fψl;kðsÞ : s 2 Dgl¼1;…;L are
principal component factors derived from the PCA decomposition of
unconditional signed distance functions as given in Eq. (2).
The categorical target variable's observed values at sampling loca-
tions (hard data) inform the sign of the signed distance function associ-
ated with a category at sampling locations. Thus, the set of hard data can
be converted into a set of inequality constraints using Eq. (3). Let assume
that at the sampling location s1, the category c2 is observed, i.e., C(s1) ¼
c2. This means that the signed distance function associated with the
category c2 should be negative at location s1 (φ2ðs1Þ � 0), and the signed
distance functions associated with other categories should be positive at
location s1 (φkðs1Þ � 0; 8k 6¼ 2; k ¼ 1;…;K). For each φkðk ¼ 1;…;KÞ, the
conditioning to all data locations is expressed by the following
inequalities:
8
<
:
�φkðs1Þ ¼ θ1;kψ1;kðs1Þþθ2;kψ2;kðs1Þþ⋯þθL;kψL;1ðs1Þ � 0 or � 0
…
�φkðsnÞ ¼ θ1;kψ1;kðsnÞþθ2;kψ2;kðsnÞþ⋯þθL;kψL;kðsnÞ � 0 or � 0
:
(4)
In Eq. (4), the n inequalities corresponding to n hard data can be
summarized as:
~Ψkθk � 0; k ¼ 1; …; K:
(5)
New PC scores vector θk ¼ ðθ1;k; …; θL;kÞT that matches data are
generated by solving the following randomized quadratic optimization
problem (Fouedjio et al., 2021a):
min
θk2RL
�
ðθk � βkÞTΣ�1
k ðθk � βkÞ
�
subject to
~Ψkθk � 0; k ¼ 1; …; K:
(6)
where βk � N ðμk;ΣkÞ, and the mean μk and the covariance matrix Σk are
computed using unconditional PC scores fαb
l;kgl¼1;…;L derived from the PCA
of unconditional signed distance functions given in Eq. (2). Speciﬁcally,
Fig. 9. Real-world data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machines; (c)
categorical target variable’ observed values at training locations.
μk ¼
"
1
B
XB
b¼1 αb
l;k
#
l¼1;…;L
; Σk ¼
1
B � 1
XB
b¼1ðαb
k � μkÞðαb
k � μkÞ
T;
with αb
k ¼
h
αb
l;k
i
l¼1;…;L:
(7)
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
89
For each sample βt
k � N ðμk;ΣkÞðt ¼ 1;…;TÞ, quadratic programming
(Goldfarb and Idnani, 1983) is performed to ﬁnd a solution θt
k that sat-
isﬁes the inequality constraints and minimizes the quadratic objective
function in Eq. (6). The covariance matrix Σk in Eq. (7) is a diagonal
matrix because the PC scores are uncorrelated by construction. Co-
efﬁcients θt
k can be also generated via the Gibbs sampling method
(Fouedjio et al., 2021b). However, this approach can be time-consuming
for very large datasets since Gibbs samples are highly correlated.
Given conditional PC scores fθt
kgt¼1;…;T, conditional signed distance
functions are obtained by reconstruction:
φt
kðsÞ ¼
XL
l¼1 θt
l;kψl;kðsÞ; 8s 2 D:
(8)
Fig. 10. Real-world data example - unconditional and conditional ﬁrst two PC scores associated with each category.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
90
Fig. 11. Real-world data example - prediction map provided by (a) traditional classiﬁcation random forest, (b) support vector machines, and (c) regression-kriging of
indicators, and (d) classiﬁcation random forest with exact conditioning.
Fig. 12. Real-world data example - prediction uncertainty (entropy) map provided by (a) traditional classiﬁcation random forest, (b) support vector machines, (c)
regression-kriging of indicators, and (d) classiﬁcation random forest with exact conditioning.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
91
Conditional classiﬁcation tree predictors are given by applying the
combination rule deﬁned in Eq. (1):
CtðsÞ ¼ argmin
c1;…;cK
�
φt
1ðsÞ; …; φt
KðsÞ
�
; 8s 2 D; t ¼ 1; …; T:
(9)
Since all the individual reconstructed signed distance functions
fφt
kðsÞ : s 2 Dgt¼1;…;Tðk ¼ 1; …; KÞ perfectly match the hard data, condi-
tional classiﬁcation tree predictors fCtðsÞ : s 2 Dgt¼1;…;T do also. The ag-
gregation of the conditional classiﬁcation tree predictors using the
majority vote rule leads to the ﬁnal outcome f^CðsÞ : s 2 Dg. This latter
coincides with the categorical target variable observed values at sam-
pling locations.
The number of unconditional classiﬁcation tree predictors B should
be large enough to allow good coverage of the solution space when
performing the exact conditioning. Indeed, the number of categorical
target variable’ observations deﬁnes the number of inequalities con-
straints as shown in Eq. (4). The larger is the number of unconditional
classiﬁcation tree predictors, the wider is the solution space of Eq. (6). So,
too many constraints (hard data) relative to too few unconditional clas-
siﬁcation tree predictors will lead to too small uncertainty. It is worth
mentioning that the number of conditional classiﬁcation tree predictors T
does not depend on the number of unconditional classiﬁcation tree
predictors B under randomized quadratic programming. That is to say, T
can be smaller or greater than B.
To summarize, the proposed classiﬁcation random forest with exact
conditioning is performed using the following pseudo algorithm:
3. Empirical examples
The proposed classiﬁcation random forest with exact conditioning is
illustrated using simulated and real-world datasets. A prediction perfor-
mance comparison is carried out with a geostatistical method (regres-
sion-kriging of indicators) and traditional machine learning techniques
(random forest and support vector machines). Hyper-parameters associ-
ated with each machine learning method have been optimized through
cross-validation. The proposed classiﬁcation random forest with exact
conditioning uses the same ensemble of decision trees generated by the
classical random forest.
The predictive performance of each method is assessed on a testing
dataset using the ﬁrst evaluation statistic, i.e., the accuracy. The accuracy
corresponds to the percentage of observations that are correctly classi-
ﬁed. It has a value between 0 and 1. The higher is the accuracy, the better
is the model. In addition to the accuracy, the Rand index is calculated.
The Rand index measures the similarity between the predicted classiﬁ-
cation and true classiﬁcation on the testing data by considering all pairs
of points and counting pairs that are assigned in the same or different
category in the predicted and true classiﬁcations. The Rand index has a
value between 0 and 1, with 0 indicating that the two classiﬁcations do
not agree on any pair of points and 1 indicating the same classiﬁcations.
3.1. Simulated data example
In this simulated case study, we consider a categorical target variable
with four categories, and four continuous predictor variables deﬁned
Table 3
Real-world data example - predictive performance statistics in the testing dataset
containing 140 observations.
Methods
Accuracy
Rand index
Random Forest
0.386
0.649
Support Vector Machines
0.342
0.575
Regression-Kriging of Indicators
0.336
0.672
Random Forest with Exact Conditioning
0.421
0.710
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
92
over the spatial domain [0,100]2. The categorical target variable is
generated according to the following model:
8s 2 ½0; 100�2;
CðsÞ ¼ ck
if
YðsÞ 2 ½qk; qkþ1½;
k ¼ 1; …; 4;
(10)
with YðsÞ ¼ 50sinðX1ðsÞÞ þ 3X1ðsÞX2ðsÞ þ X3ðsÞ2 þ 50sinðX4ðsÞÞ þ ϵðsÞ;
X1(⋅), X2(⋅), X3(⋅), and X4(⋅) are predictor variables; and ϵ(⋅) is a latent
(non-observed) variable; the limits qj's are taken as the 0, 0.25, 0.50,
0.75, and 1 quantiles of the random function Y(⋅), so that the complete
system of events ck (k ¼ 1, …, 4) can be deﬁned.
The four predictor variables and the latent variable are simulated on
the spatial domain [0,100]2 based on ﬁve independent Gaussian
isotropic stationary random functions (Chiles and Delﬁner, 2012) with
different speciﬁcation of means and covariance functions as given in
Table 1. The simulation is performing using the R package RGeostats
package (Renard et al., 2020). Fig. 2 presents the simulated data over a
200 � 200 regular grid. The map of the categorical target variable dis-
played in Fig. 2e is considered as the reference map.
To demonstrate the proposed method's ability to exactly match the
categorical target variable's observed values at sampling locations, n ¼
500 stratiﬁed random samples are taken as the training data (Fig. 2). The
set of n ¼ 500 stratiﬁed random samples amounts to 1.25% of total lo-
cations in the reference map and each category contains 125 samples.
The rest of data (39 500 samples) is kept aside for the testing. The goal is
to reconstruct the reference map of the categorical target variable
(Fig. 2e) using the sampled categories (Fig. 2f) with an aid of the
observed four spatial auxiliary variables (Fig. 2a - d) such that the cate-
gorical target variable's predicted values coincide with the categorical
target variable's observed values at sampling locations.
Fig. 3 shows the categorical target variable's observed values at
training locations and those predicted by the traditional random forest
and support vector machines. There is a signiﬁcant disagreement be-
tween the observed values and the predicted values of categorical target
variable at training locations. The misclassiﬁcation rate in the training
data is 18.20% and 35.00%, respectively, for the traditional random
forest and support vector machines. For the traditional random forest, the
number of decisions trees has been set to 10 000, and the hyper-
parameters have been optimized through cross-validation. Concerning
the support vector machines, the kernel function and the hyper-
parameters have been selected using cross-validation.
The traditional classiﬁcation random forest is performed on the
training data with a large number of decision trees set to B ¼ 10 000.
Thus, an ensemble of B ¼ 10 000 unconditional classiﬁcation tree pre-
dictors
f~CbðsÞ : s 2 ½0; 100�2gb¼1;…;10 000
is
constructed.
This
latter
ensemble is transformed into an ensemble of unconditional signed dis-
tance functions
n
~φb
kðsÞ : s 2 ½0; 100�2o
b¼1;…;10 000 for each category ck(k
¼ 1, …, 4), according to the methodology described in Sect. 2. Principal
component analysis is performed on this ensemble, followed by the
sampling of new PC scores ensuring the data's exact conditioning. T ¼
1000 new PC scores are generated, thus giving an ensemble of T ¼ 1000
conditional signed distance functions
n
φ
t
kðsÞ : s 2 ½0; 100�2o
t¼1;…;1000 (k
¼ 1, …, 4). This latter are turned into conditional classiﬁcation tree
predictors fCtðsÞ : s 2 ½0; 100�2gt¼1;…;1000 that perfectly match the cate-
gorical target variable's observed values at sampling locations. The ma-
jority vote scheme is then used to derive the predicted categorical target
variable f^CðsÞ : s 2 ½0; 100�2g. This latter also perfectly matches the cat-
egorical target variable's observed values at sampling locations by
construction.
Fig. 4 shows PC scores before the conditioning (unconditional PC
scores fαl
kgl¼1;…;10 000) and after the conditioning (conditional PC scores
fθt
kgt¼1;…;1000) for each category ck(k ¼ 1, …, 4). One can notice that the
points cloud of conditional PC scores is less scattered than those from
unconditional PC scores due effectively to the exact conditioning
expressed as a system of linear inequalities.
Fig. 5 presents prediction maps provided by the traditional classiﬁ-
cation random forest, support vector machines, regression-kriging of
indicators, and the proposed classiﬁcation random forest with exact
conditioning. One can notice the prediction maps provided by regression-
kriging of indicators (Fig. 5c) and the proposed classiﬁcation random
forest (Fig. 5d) are exactly conditioned to the training data (Fig. 2f),
which is not the case for the ones provided by traditional classiﬁcation
random forest (Fig. 5a) and support vector machines (Fig. 5b). The
general appearance of prediction maps resulting from the traditional
classiﬁcation random forest and the proposed classiﬁcation random
forest with exact conditioning looks similar. However, there are some
local differences due to the exact conditioning nature of the proposed
method. It is important to highlight that the proposed classiﬁcation
random forest with exact conditioning uses the ensemble of decision
trees generated by the classical random forest as starting point. Overall,
the prediction map of the proposed classiﬁcation random forest exhibits
more similar spatial patterns present in the reference map (Fig. 2e) than
the regression-kriging of indicators.
The traditional classiﬁcation random forest, support vector machines,
regression-kriging of indicators, and proposed classiﬁcation random
forest with exact conditioning provide the probabilities for each possible
outcome of the categorical target variable at any spatial location. Thus,
the prediction uncertainty can be quantiﬁed through information en-
tropy (Wellmann and Regenauer-Lieb, 2012). Fig. 6 presents the pre-
diction
uncertainty
maps
associated
with
each
method.
The
regression-kriging of indicators and the proposed classiﬁcation random
forest provide zero entropy at sampling location by construction while
the traditional classiﬁcation random forest and support vector machines
do not. The traditional classiﬁcation random forest, support vector ma-
chines, and proposed classiﬁcation random forest provide lower entropy
in local neighborhoods dominated by a single category compared to the
regression-kriging of indicators.
The predictive performance statistics in the testing dataset (contain-
ing 39 500 observations) for the traditional classiﬁcation random forest,
support vector machines, regression-kriging of indicators, and proposed
classiﬁcation random forest with exact conditioning are reported in
Table 2. In addition to exactly ﬁtting the categorical target variable's
observed values at sampling locations, the proposed classiﬁcation
random
forest
maintains
a
competitive
out-of-sample
predictive
performance.
3.2. Real-world data example
In this real case study, the categorical target variable is Tl (Thallium)
geochemical concentration transformed into ﬁve categories through
quantiles and observed at 568 locations over the study region in south-
west England (Kirkwood et al., 2016). Predictor variables include
elevation, gravity, magnetic, Landsat, radiometric, and their derivatives,
totaling 26 predictor variables. Some predictor variables are displayed in
Fig. 7. Fig. 8a shows the categorical target variables's observations. The
observations are partitioned into a training set (� 75%) and testing set
(� 25%) as shown in Fig. 8b. The testing set is built such that all the
categories have roughly the same number of observations.
Fig. 9 shows the categorical target variable's observed values at
training locations and those predicted by the classical random forest and
support vector machines. There is a considerable disagreement between
the observed values and the predicted values of categorical target vari-
able at training locations. The misclassiﬁcation rate in the training data is
33.80% and 52.58%, respectively, for the classical random forest and
support vector machines. For the classical random forest, the number of
decisions trees has been set to 10 000, and the hyper-parameters have
been optimized through cross-validation. Regarding the support vector
machines, the kernel function and the hyper-parameters have been
selected using cross-validation.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
93
The learned conventional random forest model consists of an
ensemble of B ¼ 10 000 classiﬁcation tree predictors. This latter
ensemble is transformed into an ensemble of unconditional signed dis-
tance functions for each category, following by PCA and randomized
quadratic programming as described in the methodology section (Sect.
2). This results to unconditional and conditional PC scores as shown in
Fig. 10; T ¼ 1000 conditional PC scores are generated. As mentioned in
the simulated data example, the points cloud of conditional PC scores is
less spread out than those from unconditional PC scores because of the
exact conditioning.
Prediction maps provided by the traditional classiﬁcation random
forest, support vector machines, regression-kriging of indicators, and
proposed classiﬁcation random forest with exact conditioning are
depicted in Fig. 11. The prediction map provided by each method differs
notably. In particular, the prediction map of the proposed classiﬁcation
random forest is different from the one provided by the traditional
classiﬁcation random forest. This is explained by the exact conditioning
nature of the proposed method. It is important to highlight that the
proposed classiﬁcation random forest with exact conditioning uses the
same ensemble of decision trees generated by the traditional classiﬁca-
tion random forest. Although the regression-kriging of indicators pro-
vides exact conditioning, its prediction map shows a noisier spatial
distribution of categories (Fig. 11c). In contrast, the prediction map
provided by the proposed classiﬁcation random forest with exacting
conditioning depicts more regular and continuous contours (Fig. 11d)
and is consistent with the training data shown in Fig. 9c.
Fig. 12 presents the prediction uncertainty (entropy) map under the
traditional classiﬁcation random forest, support vector machines,
regression-kriging of indicators, and proposed classiﬁcation random
forest with exact conditioning. The prediction uncertainty map resulting
from the proposed classiﬁcation random forest with exact conditioning
differs signiﬁcantly from the others. In particular, the prediction uncer-
tainty map provided by the traditional and proposed classiﬁcation
random forest differ notably due to the exact conditioning in the pro-
posed method and not in the conventional method. Under the proposed
classiﬁcation random forest, local neighborhoods dominated by only one
category show lower entropy than local neighborhoods dominated by
several categories of the target variable.
Table 3 provides the predictive performance of the traditional clas-
siﬁcation random forest, support vector machines, regression-kriging of
indicators, and proposed classiﬁcation random forest with exact condi-
tioning in the testing dataset (containing 140 observations). The pro-
posed classiﬁcation random forest shows better predictive performance
than the three other methods according to the accuracy. The cost of non-
using the proposed classiﬁcation random forest in this case is not negli-
gible. There is an accuracy improvement of 25% and 9% respectively,
compared to the regression-kriging of indicators and traditional classi-
ﬁcation random forest. Thus, the proposed approach can exactly match
the categorical target variable's observed values at sampling locations
while achieving good out of sample predictive performance.
4. Conclusion
This paper proposed a classiﬁcation random forest-based method for
the spatial prediction of categorical variables in which the categorical
target variable is exactly conditioned to the data. The exact conditioning
means that the predicted values of the categorical target variable at
sampling locations are the same as those observed at sampling locations.
This property is well-known in geostatistical methods. The proposed
method combines classiﬁcation random forest, signed distance functions,
principal component analysis, and randomized quadratic programming
to achieve the exact conditioning of the categorical target variable to the
data. The effectiveness of the proposed method has been demonstrated
on simulated and real datasets.
Typical characteristics of the proposed method are the following. It
can perfectly match the categorical target variable's observed values at
sampling locations while achieving good out-of-sample predictive per-
formance compared to competitor geostatistical methods such as
regression-kriging of indicators. It is easy to implement since it combines
well-known existing statistical and machine learning methods. It can
easily handle a large number of categories consistently through the
signed distance representation. The proposed method can provide real-
istic prediction uncertainties of the categorical target variable. It has the
advantage of not producing noisy spatial prediction maps, as one can
observe for regression-kriging of indicators. The proposed method is free
of the inherent problems of regression-kriging of indicators such as the
predicted probabilities of the target variable's categories that are not
guaranteed to belong to the [0, 1] interval and sum up to one. Updating
the categorical target variable predictive map when few observations are
added can be carried out quickly. Only the last part of the proposed
method, i.e., the randomized quadratic programming, should be
performed.
The proposed method is computationally intensive compared to
regression-kriging of indicators and conventional classiﬁcation random
forest. However, it comprises components that can be performed in
parallel according to the target variables' categories, including condi-
tional principal component scores generation. The proposed method re-
quires that the number of unconditional classiﬁcation tree predictors
should be large enough to allow good coverage of the solution space
when performing the exact conditioning. Indeed, the number of cate-
gorical target variable’ observations deﬁnes the number of inequalities
constraints. The larger is the number of unconditional classiﬁcation tree
predictors, the wide is the solution space for the exact conditioning. So,
too many constraints (hard data) relative to too few unconditional clas-
siﬁcation tree predictors will lead to too small uncertainty. Nonetheless,
it will always be possible to meet this constraint because the number of
unconditional classiﬁcation tree predictors is a free parameter. Although
the proposed method uses the random forest as the base learner, it can be
used with other ensemble machine learning methods (e.g., boosting).
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂuence
the work reported in this paper.
References
Albrecht, T., Gonz�alez-�Alvarez, I., Klump, J., 2021. Using machine learning to map
Western Australian landscapes for mineral exploration. ISPRS Int. J. Geo-Inf. 10.
Allard, D., D’Or, D., Froidevaux, R., 2011. An efﬁcient maximum entropy approach for
categorical variable prediction. Eur. J. Soil Sci. 62, 381–393.
Bogaert, P., 2004. Spatial prediction of categorical variables: the bme approach. In:
Sanchez-Vila, X., Carrera, J., G�omez-Hern�andez, J.J. (Eds.), geoENV IV —
Geostatistics for Environmental Applications. Springer Netherlands, Dordrecht,
pp. 271–282.
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.
Cao, G., Kyriakidis, P., Goodchild, M., 2011. A multinomial logistic mixed model for
prediction of categorical spatial data. Int. J. Geogr. Inf. Sci. 25, 2071–2086.
Cao, G., Yoo, E.H., Wang, S., 2014. A statistical framework of data fusion for spatial
prediction of categorical variables. Stoch. Environ. Res. Risk Assess. 28, 1785–1799.
Chiles, J.P., Delﬁner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley &
Sons.
Cracknell, M., Reading, A., 2015. Spatial-contextual supervised classiﬁers explored: a
challenging example of lithostratigraphy classiﬁcation. IEEE J. Select. Topics Appl.
Earth Observ. Remote Sens. 8, 1–14.
Cracknell, M.J., Reading, A.M., 2014. Geological mapping using remote sensing data: a
comparison of ﬁve machine learning algorithms, their response to variations in the
spatial distribution of training data and the use of explicit spatial information.
Comput. Geosci. 63, 22–33.
Davies, E., 2012. Chapter 9 - binary shape analysis. In: Davies, E. (Ed.), Computer and
Machine Vision, fourth ed. Academic Press, Boston, pp. 229–265.
Du, P., Bai, X., Tan, K., Xue, Z., Samat, A., Xia, J., Li, E., Su, H., Liu, W., 2020. Advances of
four machine learning methods for spatial data handling: a review. J. Geovisual.
Spatial Anal. 4.
Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.
Artiﬁ. Intel. Geosci. 1, 11–23.
Fouedjio, F., Scheidt, C., Yang, L., Achtziger-Zupan�ci�c, P., Caers, J., 2021a.
A geostatistical implicit modeling framework for uncertainty quantiﬁcation of 3D
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
94
geo-domain boundaries: application to lithological domains from a porphyry copper
deposit. Comput. Geosci. 157, 104931.
Fouedjio, F., Scheidt, C., Yang, L., Wang, Y., Caers, J., 2021b. Conditional simulation of
categorical spatial variables using Gibbs sampling of a truncated multivariate normal
distribution subject to linear inequality constraints. Stoch. Environ. Res. Risk Assess.
35, 457–480.
Giaccone, E., Oriani, F., Tonini, M., Lambiel, C., Mari�ethoz, G., 2021. Using data-driven
algorithms for semi-automated geomorphological mapping. Stoch. Environ. Res. Risk
Assess. 1–17.
Goldfarb, D., Idnani, A., 1983. A numerically stable dual method for solving strictly
convex quadratic programs. Math. Program. 27, 1–33.
Goovaerts, P., 2001. Geostatistical modelling of uncertainty in soil science. Geoderma
103, 3–26.
Grevera, G.J., 2007. Distance Transform Algorithms and Their Implementation and
Evaluation. Springer New York, New York, NY, pp. 33–60.
Hengl, T., Heuvelink, G.B., Stein, A., 2004. A generic framework for spatial prediction of
soil variables based on regression-kriging. Geoderma 120, 75–93.
Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr€aler, B., 2018. Random forest as a
generic framework for predictive modeling of spatial and spatio-temporal variables.
PeerJ 6, e5518.
Hengl, T., Toomanian, N., Reuter, H.I., Malakouti, M.J., 2007. Methods to interpolate soil
categorical variables from proﬁle observations: lessons from Iran. Geoderma 140,
417–427. Pedometrics 2005.
Kanevski, M., 2008. Advanced Mapping of Environmental Data: Geostatistics, Machine
Learning and Bayesian Maximum Entropy. John Wiley & Sons.
Kanevski, M., Pozdnoukhov, A., Timonin, V., 2009. Machine Learning for Spatial
Environmental Data: Theory, Applications, and Software. EPFL press.
Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016. Stream sediment geochemistry as a
tool for enhancing geological understanding: an overview of new data from south
west England. J. Geochem. Explor. 163, 28–40.
Kuhn, S., Cracknell, M.J., Reading, A.M., 2018. Lithologic mapping using random forests
applied to geophysical and remote-sensing data: a demonstration study from the
Eastern Goldﬁelds of Australia. Geophysics 83, B183–B193.
Kumar, C., Chatterjee, S., Oommen, T., Guha, A., 2020. Automated lithological mapping
by integrating spectral enhancement techniques and machine learning algorithms
using aviris-ng hyperspectral data in gold-bearing granite-greenstone rocks in Hutti,
India. Int. J. Appl. Earth Obs. Geoinf. 86, 102006.
Latifovic, R., Pouliot, D., Campbell, J., 2018. Assessment of convolution neural networks
for surﬁcial geology mapping in the South Rae geological region, Northwest
territories, Canada. Rem. Sens. 10.
Maxwell, A.E., Warner, T.A., Fang, F., 2018. Implementation of machine-learning
classiﬁcation in remote sensing: an applied review. Int. J. Rem. Sens. 39, 2784–2817.
Othman, A.A., Gloaguen, R., 2017. Integration of spectral, spatial and morphometric data
into lithological mapping: a comparison of different machine learning algorithms in
the Kurdistan region, NE Iraq. J. Asian Earth Sci. 146, 90–102.
Pardo-Igúzquiza, E., Dowd, P.A., 2005. Multiple indicator cokriging with application to
optimal sampling for environmental monitoring. Comput. Geosci. 31, 1–13.
Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and tuning strategies for
random forest. Wiley Interdiscipl. Rev.: Data Min. Knowl. Discov. https://doi.org/
10.1002/widm.1301.
R Core Team, 2020. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria. URL. https://www.R-project.o
rg/.
Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. RGeostats:
geostatistical package. URL: http://cg.ensmp.fr/rgeostats. r package version 12.0.1.
Sahoo, S., Jha, M.K., 2017. Pattern recognition in lithology classiﬁcation: modeling using
neural networks, self-organizing maps and genetic algorithms. Hydrogeol. J. 25,
311–330.
Wellmann, J.F., Regenauer-Lieb, K., 2012. Uncertainties Have a Meaning: Information
Entropy as a Quality Measure for 3-D Geological Models. Tectonophysics, vols.
526–529. Modelling in Geosciences, pp. 207–216.
Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for high
dimensional data in Cþþ and R. J. Stat. Software 77, 1–17.
Yu, L., Porwal, A., Holden, E.J., Dentith, M.C., 2012. Towards automatic lithological
classiﬁcation from remote sensing data using support vector machines. Comput.
Geosci. 45, 229–239.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 2 (2021) 82–95
95
