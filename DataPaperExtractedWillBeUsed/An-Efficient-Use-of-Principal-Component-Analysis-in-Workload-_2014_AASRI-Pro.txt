 AASRI Procedia  8 ( 2014 )  68 – 74 
Available online at www.sciencedirect.com
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.08.012 
ScienceDirect
2014 AASRI Conference on Sports Engineering and Computer Science (SECS 2014) 
An Efficient Use of Principal Component Analysis in Workload 
Characterization-A Study 
Jyotirmoy Sarkara, Snehanshu Sahab, Surbhi Agrawalb*
aBITS PILANI & TechMahindra,,Bangalore,560100,India 
bCBIMMC & Dept. of Computer Science and Engineering,PESIT-BSC,Bangalore,560100,India  
Abstract 
PCA is a useful statistical technique that has found application in fields such as face recognition, image compression, 
dimensionality reduction, Computer System performance analysis etc. It is a common technique for finding patterns in 
data of high dimension. In this paper, we present the basic idea of principal component analysis as a general approach that 
extends to various popular data analysis techniques. We state the mathematical theory behind PCA and focus on 
monitoring system performance using the PCA algorithm. Next, an Eigen value-Eigenvector dynamics is elaborated 
which aims to reduce the computational cost of the experiment. The Mathematical theory is explored and validated. For 
the purpose of illustration we present the algorithmic implementation details and numerical examples over real time and 
synthetic datasets. 
© 2014 . Published by Elsevier B.V. 
Selection and/or peer review under responsibility of American Applied Science Research Institute 
Keywords: PCA; Eigen Value; Eigen Vector,Workload Characterization.. 
* Corresponding author. Tel.: +91-080-66186622; fax: 91-80-. 
E-mail address: snehanshusaha@pes.edu. 
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
69
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
1. Introduction 
Performance evaluation helps us to give an idea how well a system is performing as compared to other 
systems. Workload is the most crucial part of any performance evaluation process. The entire process can end 
up in a wrong conclusion if workload is not chosen in appropriate ways. Therefore, workload selection is an 
integral part of performance evaluation project. Computer architectures are evaluated by running a workload 
on the computer and measuring the execution time. New computers are designed the same way. As the newly 
designed computer does not exist, it is not possible to run any workload. This is where workload 
characterization comes into the fore. The goal of workload characterization is to describe the properties of a 
workload in terms of abstract performance metrics, called workload characteristics, which predict the final 
performance [1]. 
There are a couple of techniques to classify workload components. One of the widely used techniques is 
“weighted sum of the parameter values” that uses sum to classify the workload components into classes. But 
there are proper guidelines to decide the weight of parameters. Before PCA, an analyst running software used 
to assume the values of weight. Instead, one can use PCA, to calculate the value of weights. PCA is a 
procedure by which numbers of correlated variables are transformed into a smaller number of uncorrelated 
variables. It is a data analysis technique traced back to Pearson (1901). It can be used to compress a high 
dimensional dataset into a lower dimensional dataset. PCA can be derived from a number of starting points 
and optimization criteria. The most important of these are minimization of the mean square error in data 
compression, finding mutually orthogonal directions in the data having maximal variances and de-correlation 
of the data using orthogonal transformation. These uncorrelated variables are called Principal Components. 
1.1. How PCA works: 
For a given set of n parameters
1
{ , 2
,...
n}
x x
x
, the Principal Component Analysis will produce a set of 
principal factors. The following conditions will hold true for the newly produced set - 
�
The principal factor (
iy )
 is a linear combination of initial parameters(
jx )
 .  
1
n
ij
j
j
y
a x
�
� �
�
Principal factor set is an orthogonal set. 
,
0
i
j
ik
kj
k
y
y
a a
�
��
�
�
It is an ordered set 
1
{ , 2
,...
n}
y y
y
 in the decreasing order of the percentage of variance, with 
1y  being the 
highest percentage of variance and 
ny  the least. So first few factors can be used to classify the workload 
components.  
We can find the application of Principal Component Analysis in many fields including data compression, 
image processing, visualization, pattern recognition and time series prediction [2].Sirvich and Kirby had 
efficiently used PCA in human faces representation [3-4].This approach leads to decomposition of any images 
into Eigen pictures so that the image can be reconstructed using a portion of the Eigen pictures and the 
corresponding projection onto the Eigen picture subspace [5]. The PCA method has also been used in 
handprint recognition, human made object recognition, industrial robotics and mobile robotics etc [6]. In a 
workload composition the choice of benchmark is very important. The selection of benchmark for inclusion in 
70  
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
benchmark suit is called workload composition. Smith [7] used a metric, which is based on dynamic program 
characteristics for the Fortran language..They used squared Euclidean distance to measure the difference 
between benchmarks. The shortcoming of this procedure is the use of Euclidean distance for measuring the 
difference. To overcome this Eeckhout et al. [8] proposed Principal Component Analysis (PCA) to get rid of 
the correlation and dependence between variables. A number of program characteristics are measured for a 
number of benchmarks on which PCA was applied.  
2. Proposed Work 
The most computationally expensive part of PCA is the calculation of Eigen values and Eigen vectors of 
the dataset. In this paper our main objective is to save the computational time of Principal Component 
Analysis (PCA) by skipping the Eigen vector calculation. Here we propose to bypass Eigen vector calculation, 
by rather inspecting the Eigen vectors. The understanding of the dynamics of Eigen values and Eigen vectors 
in the context of Linear transformations and vector spaces plays a crucial role in improving the efficiency of 
PCA in the workload characterization problem. This will require us to prove/cite important theorems in Linear 
Algebra. 
2.1. Algorithm 
�
Compute the mean and standard deviation of the parameters. 
1
1
n
i
i
x
a
n
�
�
�
,
2
1
1
(
)
1
n
x
i
i
s
x
x
n
�
�
�
� �
�
Compute the correlation of the parameters. 
1
1
(
)(
)
i
i
a
b
a
b
n
a
a
b
b
i
x x
x
x
x
x
x
x
n
R
s
s
�
�
�
�
�
�
Compute QR Decomposition of correlation matrix at every step 
k
k
k
A
� Q R
 (starting with
k � 0
 ), 
where 
k
Q  is an orthogonal matrix and 
k
R  is an upper triangle matrix. 
k 1
k
K
A
� � R Q
�
The matrix will converge to a triangular matrix, known as the Schur form. Find out the eigen values 
of the matrix from the diagonal. 
�
Apply the results of the theorem to choose the eigen vectors by inspection. This is possible since the 
matrices obtained are symmetric and the Eigen values are real and distinct. 
�
Use the Eigen vectors to compute the principal factors. 
Next, we prove a theorem related to eigen values and eigen vectors. Let us take a linear map 
:
T u
� v
�
(
)
( )
( );
T
x
y
T x
T y
�
�
�
�
�
�
�
 where ,x y
�u
;
,u v  are vector spaces of certain dimensions,
n &
m
say where n
� m
 necessarily; e.g.
n ,
m
u
R
v
R
�
�
R
Tx
x
�
�
� �
�
�
 , then �  is an Eigen value of 
T &
x  is a corresponding Eigen vector. 
2.2. Proposition 1:
For the linear map
:
T u
� v
, if the Eigen values “�  “are distinct, then T  admits of linearly independent 
Eigen vectors. 
Proof: Linear Independence: A set of vectors 
1
2
3
{ ,
,
,.... }
n
v v v
v
 is linearly independent if �  scalars
1
2
(
,
,....
n )
� �
�
�
1
0
n
i
i
i
v
�
�
�
�
 implies 
0
1,..
i
i
n
� � � �
 i.e.
iv ; any vector in the set is NOT a linear 
71
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
combination of any of the other vectors in the same set e.g. 1
1
� �
� �
� �
 & 
1
1
�
�
�
�� �
�
 are linearly independent. 
Proof of the theorem: (Using the Principle of Mathematical Induction) 
Basis Step: 
n � 2;
 NTS 
1 1
2
2
1
2
0
0
a v
a v
a
a
�
�
�
�
�
Apply
1 1
2
2
(
)
(0)
T
T a v
a v
T
�
�
�
; T  linear map 
1
1
2
2
( )
(
)
0;
a T v
a T v
�
�
�
1 1 1
2
2
2
0
a
v
a
v
�
�
�
�
�
                                                                                                                          (1) 
Also 
1 1 1
2
1 2
0
a
v
a
v
�
�
�
�
                                                                                                                               (2) 
Therefore (1)�  (2) 
2
2
1
2
2
(
)
0
0
a
v
a
�
�
�
�
�
�
�
1
2
2
(
;
0)
v
�
� �
�
�
2
1
0
0
a
a
�
�
�
; Induction hypothesis: Assume the proposition is true for n
� m
 ; Induction steps: on 
1
n
� m
�  i.e. NTS  
1 1
1
1
....
0
m
m
m
m
a v
a v
a
� v
�
�
�
�
�
=>
1
2
1
....
0.
m
m
a
a
a
a �
�
�
�
�
�
Let
1 1
1
1
....
0
m
m
m
m
a v
a v
a
� v
�
�
�
�
�
  i.e. 
1 1
1
1
(
....
)
(0)
m
m
m
m
T a v
a v
a
v
T
�
�
�
�
�
�
so,
1 1 1
1
1
1
....
0
m
m
m
m
m
m
a
v
a
v
a
v
�
�
� �
�
�
�
�
�
�
                                                                                           (3)
Also, 
1 1 1
1
1 1
1
....
0
m
m
m
m
a
v
a
v
a
v
�
�
� �
�
�
�
�
�
                                                                                                (4)
(3)�  (4)�
2
2
1
2
1
1
1
1
(
)
....
(
)
0
m
m
m
a
v
a
v
�
�
�
�
�
�
�
�
�
�
�
�
By the hypothesis, 
1
{ ,...
m}
v
v
 linear independent
1
2
1
...
0
m
m
a
a
a
a �
�
�
�
�
�
�
1
1
1
1
(
)
0
m
m
m
a
v
�
�
�
�
�
�
�
1
1
1
(
(
)
0&
0)
m
vm
�
�
�
�
�
�
�
�
Therefore, the Eigen vectors 
1
1
{ ,...
,
}
m
m
v
v
v �
 are Linearly Independent. 
Proposition 2:  A real, symmetric linear map T (matrix) admits of orthogonal Eigenvectors. 
Proof: Well established result [9]. 
Conclusion of proposition: The aforementioned matrix (or the linear map, T) has distinct eigen values (as 
72  
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
always the case will be) and is real, symmetric. Therefore, the corresponding eigenvectors will be linearly 
independent & orthogonal to each other. This enables us to find the eigenvectors by inspection rather than 
computing step by step via set of simultaneous equations. This saves O (n) computations, crucial computation 
cost!
2.3. Implication
The paper aims to improve time complexity of PCA algorithm. The following example will illustrate the 
principle behind PCA from initial parameters.We have collected synthetic data of the number of packets lost 
on two different network links. 
ax is the number of packets lost on link A and 
bx  is the number of packets 
lost on link B 
Table 1. Data for Principal Component Analysis Example 1 
Observation Number 
ax   (Variables) 
bx (Variables)
ay   (Principal factors)
by
(Principal factors)
1 
300 
400 
-0.0027 
-0.0014 
2 
510 
330 
-0.0014 
 0.0028 
3 
212 
547 
-0.0021 
 0.0049 
4 
309 
690 
 0.0028 
-0.0070 
5 
610 
410 
 0.0014 
 0.0028 
6 
910 
150 
 0.0007 
 0.0133 
7 
540 
320 
-0.0014 
 0.0042 
8 
440 
540 
 0.0007 
 -0.0021 
9 
219 
440 
-0.0037 
 -0.0023 
10 
510 
779 
 0.0070 
 -0.0054 
First we have to compute the mean and standard deviation using the formulas given in Algorithm 2 
4 5 6 0
4 5 6
1 0
xa
�
�
  ; 
4600
460
10
bx
�
�
;
2
axs
=
2
2 4 8 3 9 8 6
1 0
4 5 6
4 4 9 5 8 .4
9
�
�
�
 ; 
2
34805.5
bxs
�
Correlation among the variables as 
0.486
a
Rx xb
� �
and hence the correlation matrix will be  
1 .0 0 0
0 .4 8 6
0 .4 8 6
1 .0 0 0
C
�
�
�
� �
�
� �
�                                                                                                                        
(5) 
Now we will compute the Eigen values from the above correlation matrix using characteristic equation 
1
0.486
0
0.486
1
C
I
�
�
�
�
�
�
�
�
�
�
2
2
(1
)
0.486
0
�
�
�
�
�
73
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
The Eigen values are 1.486 and 0.514. 
3. Results and Discussion 
Now, the correlation matrix in (5) is both real and symmetric and is a candidate for the theorems to be 
applied. Figure1 below shows the average execution time of PCA algorithm over a sample of 5 different 
datasets. The execution time has been recorded in milliseconds. 
4. Conclusion 
PCA is the simplest of the true Eigenvector-based multivariate analyses. It can be used to reveal internal 
structure of data in a way that best explains the variance in data. PCA is sensitive to outliers in the data that 
produce large number of errors. So, before applying PCA it is expected to remove outliers. As a limitation the 
result of PCA depend on the scaling of variables. The applicability of PCA constrained by certain assumption 
made in derivation. Our work explores the underlying principles of PCA and exploits the inherent 
mathematical theory for efficient computation. The figure below conclusively shows that computation time 
has been reduced to achieve the same results. 
0
200
400
600
800
1000
Till�Eigen�Value
Till�Eigen�
Vector
Fig.1. execution times of Eigen values and Eigen vectors and the time saved. 
References 
[1] T. M. Conte and W. Hwu, Benchmark Characterization,"IEEE Computer, vol. 24, no. 1, pp. 48-56, Jan. 
1991. 
[2] Raj Jain, The Art of Computer Systems Performance Analysis, Techniques for Experimental Design, 
Measurement, Simulation, and Modeling. 
[3] Kirby and Sirovich, 1990. Application of Karhunen-Loeve Procedure for the Characterization of Human 
Faces. IEEE 
[4] Taranpreet Singh, Face Recognition Based on PCA Algorithm. 
[5] S Ekhe, Y Chincholkar, Improved Face Recognition using PCA & LDA 
[6] R Gottumukkal, V K Asari, An Improved Face Recognition Technique Based on Modular PCA Approach. 
74  
 Jyotirmoy Sarkar et al. /  AASRI Procedia  8 ( 2014 )  68 – 74 
[7] R. H. Saavedra and A. J. Smith, “Analysis of Benchmark Characteristics and Benchmark Performance 
Prediction,” ACM TOCS, vol. 14, no. 4, pp. 344–384, Nov. 1996. 
[8] L. Eeckhout, H. Vandierendonck, and K. De Bosschere, “Quantifying the Impact of Input Data Sets on 
Program Behavior and its Applications,” JILP, vol. 5, Feb. 2003, http://www.jilp.org/vol5 
[9] Gilbert Strang, “Introduction to Linear Algebra”, 4th Edition, SIAM, 2009. 
