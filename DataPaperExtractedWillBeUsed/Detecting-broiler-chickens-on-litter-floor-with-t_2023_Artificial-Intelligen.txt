Detecting broiler chickens on litter ﬂoor with the YOLOv5-CBAM deep
learning model
Yangyang Guo a,b, Samuel E. Aggrey b, Xiao Yang b, Adelumola Oladeinde c, Yongliang Qiao d, Lilong Chai b,⁎
a School of Internet, Anhui University, Hefei, Anhui 230039, China
b Department of Poultry Science, University of Georgia, Athens, GA 30602, USA
c U.S. National Poultry Research Center, USDA-ARS, Athens, GA 30605, USA
d Australian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW 2006, Australia
a b s t r a c t
a r t i c l e
i n f o
Article history:
Received 5 April 2023
Received in revised form 16 July 2023
Accepted 8 August 2023
Available online 9 August 2023
For commercial broiler production, about 20,000–30,000 birds are raised in each conﬁned house, which has
caused growing public concerns on animal welfare. Currently, daily evaluation of broiler wellbeing and
growth is conducted manually, which is labor-intensive and subjectively subject to human error. Therefore,
there is a need for an automatic tool to detect and analyze the behaviors of chickens and predict their wel-
fare status. In this study, we developed a YOLOv5-CBAM-broiler model and tested its performance for de-
tecting broilers on litter ﬂoor. The proposed model consisted of two parts: (1) basic YOLOv5 model for
bird or broiler feature extraction and object detection; and (2) the convolutional block attention module
(CBAM) to improve the feature extraction capability of the network and the problem of missed detection
of occluded targets and small targets. A complex dataset of broiler chicken images at different ages, multiple
pens and scenes (fresh litter versus reused litter) was constructed to evaluate the effectiveness of the new
model. In addition, the model was compared to the Faster R-CNN, SSD, YOLOv3, EfﬁcientDet and YOLOv5
models. The results demonstrate that the precision, recall, F1 score and an mAP@0.5 of the proposed
method were 97.3%, 92.3%, 94.7%, and 96.5%, which were superior to the comparison models. In addition,
comparing the detection effects in different scenes, the YOLOv5-CBAM model was still better than the com-
parison method. Overall, the proposed YOLOv5-CBAM-broiler model can achieve real-time accurate and
fast target detection and provide technical support for the management and monitoring of birds in commer-
cial broiler houses.
© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Keywords:
Poultry production
Deep learning
YOLOv5
Attention mechanism
1. Introduction
Poultry provides a valuable source of proteins but face a number
of challenges worldwide. Among the many challenges is the welfare
concerns of the birds under intensive management systems (Chai
et al., 2018, 2019). Due to the large number of chickens reared at
any given time in a house, accurate and efﬁcient monitoring of
birds can improve their health and welfare status (Li et al., 2021;
Okinda et al., 2020). Currently, most broiler houses are monitored
manually, however, this approach could be both laborious and
erroneous. Automatic broiler monitoring system could collect
individual bird data within a ﬂock and provide critical information
to aid digital management (Subedi et al., 2023a, 2023b; Yang
et al., 2022).
Computer vision technology (CVT) is widely used to monitor
farm animals because it is non-invasive (Qin et al., 2021; Wang
et al., 2022; He et al., 2016). The CVT together with machine vision
methods have achieved target detection based on the features of
the target area (e.g., color, shape, texture). The ability to effectively
acquire these visual features will affect the accuracy of target detec-
tion (Tharwat et al., 2014; Awad et al., 2013; Andrew et al., 2017).
The feature acquisition method, external environment (e.g., light
intensity and occlusion), shooting angle and image quality chosen
are crucial parameters that will affect target detection. Therefore,
it is important to innovate animal target detection algorithms that
are less affected by the natural environment.
Deep learning technology has powerful feature representation capa-
bilities, fast processing speed, and can resolve problems associated with
external interferences. Thus, deep learning algorithms are appropriate
models for developing an automatic, efﬁcient and intelligent tool for an-
imal farming (Qiao et al., 2021). Deep learning technologies have been
applied to the study of large animals (pigs, sheep, cattle), such as object
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
⁎ Corresponding author.
E-mail address: lchai@uga.edu (L. Chai).
https://doi.org/10.1016/j.aiia.2023.08.002
2589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
detection, individual tracking, behavior recognition and body condition
evaluation (Chen et al., 2021; Xue et al., 2021; Tian et al., 2019; Qiao
et al., 2019; Shen et al., 2020; Alvarez et al., 2019; Guo et al., 2021a,
2022). However, the size of the chicken and the sheer numbers that
are raised in a single house (e.g., 20,000–30,000 chickens on litter
ﬂoor of 2000–2500 m2) pose challenges in applying deep learning tech-
niques in monitoring individual chickens (Guo et al., 2020). Yang et al.
(2022) built a YOLOv5x-hens model, which detection is highly efﬁcient
and over 95% accurate. Fang et al. (2020) proposed poultry tracking al-
gorithm TBroiler tracker which has good performance in overlap rate,
pixel error and failure rate, and its hybrid tracking performance evalua-
tion (MTPE) is 0.730. Fang et al. (2021) analyzed the behavior of broiler
using DNNs.
The tests showed that the accuracy of standing, walking, running, eat-
ing, resting and tidying behavior recognition was 0.7511, 0.5135, 0.6270,
0.9361, 0.9623 and 0.9258, respectively. Although the above research has
made some progresses, however, there is a lack of poultry research at dif-
ferent ages, feeding environments, and densities. And object detection is
the premise of behavior recognition and target tracking, and it is also the
data basis for providing target area information. Therefore, target detec-
tion of broiler groups in multiple scenarios is of great signiﬁcance.
Among the deep learning algorithms, YOLO series is one of the fast and
high precision algorithms for multi-target detection at present (Subedi
et al., 2023a, 2023b; Ge et al., 2021;Bochkovskiy et al., 2020). When tar-
gets are small or occluded YOLO can result in missed or false detections.
There are attention mechanisms in deep learning that can reduce infor-
mation loss and improve the detection performance of occlusion and
small object.(Yang et al., 2023; Li et al., 2020; Fukui et al., 2019).
In the current study, we incorporated a convolutional block atten-
tion module (CBAM) into YOLOv5 to enhance the algorithm's ability
to extract image features. To do this, we used a complex dataset of
broiler images (e.g., birds at different ages, fresh and reused litter and
multiple pens) to train and test the model. The proposed YOLOv5-
CBAM improved the acquisition ability of small object features and the
accuracy of small target detection.
2. Material and methods
2.1. Data acquisition
The data for this study came from an experimental broiler house
at the Poultry Research Center of the University of Georgia, USA
(Guo et al., 2020; Guo et al., 2021b). Two different litter types
(fresh pine shavings and reused litter previously used to raise three
ﬂocks of broilers) were selected as application scenes for broiler de-
tection. For the two litter scenes, 70 images were selected from d2,
d9, d16, and d23, respectively, for a total of 560 images. In addition,
to evaluate the detection performance of the model under multiple
pens scenes, the image samples shown in Fig. 1c were constructed,
in which 70 images were selected for d16 and d23. Finally, 700 im-
ages were obtained and randomly assigned at a ratio of 5:2 into
training and testing set, respectively. Fig. 1 are examples of broiler
images from different scenes.
2.2. YOLOv5-CBAM model for broiler detection
In the current study, we propose a YOLOv5-CBAM-broiler model
(Fig. 2). This method added CBAM attention modules to the backbone
and neck layers of YOLOv5 to improve the feature representation
capability.
2.2.1. The YOLOv5 network
Jocher et al. (2020) developed the YOLOv5 algorithm and demon-
strated that it was more accurate and faster compared to the previous
YOLO model. The YOLOv5s network consists of three parts: backbone,
neck, and prediction. Broiler chicken images were used as input for
the backbone to obtain image features, the neck part was used to inte-
grate the extracted feature information and generate feature maps,
and the prediction part was used to generate bounding boxes and pre-
dict categories for the generated feature maps. The detailed process is
provided as a supplementary material.
Fig. 1. Examples of broiler images from different scenes.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
37
The Backbone part of the model is used to extracted different ﬁne-
grained features from images, as shown in Fig. 2, the original backbone
network is composed of Focus, Conv, C3 and Spatial Pyramid Pooling
(SPP). The bird image size in the YOLOv5s model was 416 × 416 × 3,
which became 208 × 208 × 12 through the focus slicing operation,
and the ﬁnal feature map size became 208 × 208 × 32 after convolution
with 32 convolution operation kernels. C3 is used to extract broiler
image features. In the Backbone part, the C3 module contains detailed
location information, but less semantics. The SPP is used to concatenate
feature maps of different sizes together as an output (He et al., 2015). In
the Neck part, the C3 module extracts features, which contains less loca-
tion information, but more semantics. After the feature information of
occluded or small targets are processed by C3 modules, the target posi-
tion information is rough, and the feature information can be easily lost.
The Head part predicts the processed broiler image features in three dif-
ferent scales, generates bounding boxes and predicts the class of objects.
The Head part in YOLOv3 was used as YOLOv5 Head.
To improve the detection accuracy of the original model for broiler
targets at different growth stages and feeding scenes, we herein propose
an improved YOLOv5 network model, as shown in Fig. 2. The CBAM
module was added to Backbone and Neck and placed after the C3 mod-
ule. The CBAM module can strengthen the learning of occlusion or small
target feature information during the network training process through
the channel and spatial attention modules.
2.2.2. The CBAM attention module
In the target detection task of broiler chickens at different growth
stages and in different scenes, occlusion or small targets occupy fewer
pixels, and their feature information is easily lost in the deep network,
which leads to missed and false detection of targets. The CBAM module
can effectively increase the weight of the occlusion or small targets in
the entire feature map through channel and spatial attention modules,
making the information easier to be learned by the network (Woo
et al., 2018). The broiler image features extracted from the C3 module
were denoted as F, and the channel attention map was generated by
using the channel relationship between features, which was multiplied
by F to form a new feature F′ to enhance the features related to the tar-
get area of bird. Then, the spatial attention feature map was generated
by using the internal spatial relationship between features, which mul-
tiplied with F′ to obtain F″, which strengthened the weight of broiler tar-
get area features from the channel and spatial relationship between
features. As shown in Fig. 3.
2.2.2.1. Channel attention module. The channel attention module infor-
mation is extracted using max pooling and average pooling, respec-
tively, and then ﬁltered, activated and normalized to improve the
ability to extract channel information.
As shown in Fig. 3a. First, the feature map F ∈ R(C×H×W) is inputted,
and the feature map of size C × H × W is transformed into C × 1 × 1
using maximum pooling and average pooling. Then the feature map is
entered into the neural network MLP, the number of neurons in the
ﬁrst layer is C/r, r is the decline rate, and the activation function is
Relu. The number of neurons in the second layer is C, and then the re-
sults are combined through the addition operation. The weight coefﬁ-
cient Mc∈ R(C×1×1) is obtained through the sigmoid function, as shown
in Eq. (1).
Mc F
ð Þ ¼ σ
W1 W2
Fc
avg
�
�
�
�
þ W1 W2
Fc
max
�
�
�
�
�
�
ð1Þ
where, σ is the sigmoid function; Fc
avg and Fc
max represent the feature
maps after average and maximize pooling; W1 and W2 represent the
weights of two layers of a multilayer perception. Then, the channel
attention feature map F′ is obtained by multiplying Mc with the
original feature map F.
2.2.2.2. Spatial attention module. The spatial attention mechanism fo-
cuses on local information. The information is ﬁltered by pooling, and
then the important information is extracted by convolution from the ﬁl-
tered information. As shown in Fig. 3b. Using F′ as input into the spatial
attention module, it is also pooled with maximum and average, stacked
by the Concat operation, and then the weight coefﬁcient Ms ∈ R(1×H×W) is
obtained by convolution operation and sigmoid, as shown in eq. (2).
Ms F′
ð
Þ ¼ σ
f 7�7
F′S
avg; F′S
max
h
i
�
�
�
�
ð2Þ
Fig. 2. The overall structure of YOLOv5-CBAM network.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
38
where, σ is the sigmoid function; F′s
avg and F′s
max represent the feature
maps of size 1 × H × W after average and maximize pooling; f 7�7 repre-
sents 7 × 7 convolution. Lastly, the Ms and F′ are multiplied to obtain the
ﬁnal attention feature map F″.
2.3. Performance evaluation
In the current study, precision, recall, F1 score, mean average preci-
sion (mAP) and Frames Per Second (FPS) were adopted as the metrics of
the detection accuracy, as shown in the following equations:
Precision ¼
TP
TP þ FP � 100%
ð3Þ
Recall ¼
TP
TP þ FN � 100%
ð4Þ
F1 ¼ 2 � Precision � Recall
Precision þ Recall � 100%
ð5Þ
AP ¼
Z 1
0
P rð Þdr
ð6Þ
mAP ¼ 1
n ∑
n
i¼1
AP ið Þ
ð7Þ
where, TP, FP and FN are the numbers of true positive samples, false pos-
itive samples and false negative samples, respectively. The mAP is the
mean of all classiﬁed AP (Average Precision). n represents the number
of object categories (n = 1). FPS refers to the number of images identi-
ﬁed within 1 s.
2.4. Network training parameters
In this study, all model tests were performed on a computer
equipped with a GeForce GTX 1080 Ti GPU, I9-7920× CPU@2.9 GHz.
Parameter settings: 416 × 416 × 3 input size, 1000 training period, 16
batch size, 0.0013 learning rate. Other parameters are their default
settings.
I Faster R-CNN (Ren et al., 2015), SSD (Liu et al., 2016), YOLOv3
(Redmon and Farhadi, 2018), EfﬁcientDet (Tan et al., 2020) and
YOLOv5s (Liu et al., 2021) serve as comparison models.
3. Results
3.1. Performance of new detection model
We used datasets consisting of broiler images at different ages,
raised on two types of litter and multiple pens to test the performance
of YOLOv5-CBAM. The detection results of broiler with different models
Fig. 3. The structure of the CBAM module.
Table 1
Performance comparison of different algorithms (%).
Method
Precision
Recall
F1
mAP@0.5
FPS (Frame/s)
Faster-rcnn
79.7
95.4
86.8
90.6
2.6
SSD
60.8
94.0
73.8
88.5
3.1
YOLOv3
83.7
83.0
83.3
70.6
18.9
EfﬁcientDet
97.0
47.0
64.0
59.6
36
YOLOv5
96.6
92.1
94.3
96.3
62
YOLOv5-CBAM
97.3
92.3
94.7
96.5
55
Table 2
Comparison of detection accuracy in different scenes.
Method Precision
Fresh pine shavings
Reused litter
Multiple pens
d2
d9
d16
d23
d2
d9
d16
d23
d16
d23
Precision of YOLOv5
95.1
99
98.6
92.8
94.3
99
99.1
98.7
92.2
98.6
Precision of YOLOv5-CBAM
96.1
99.3
99.3
94
95.2
98.9
99.3
99.3
92.5
98.8
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
39
are shown in Table 1. From Table 1, the precision, recall, F1 and mAP@
0.5 of YOLOv5-CBAM were 97.3%, 92.3%, 94.7% and 96.5%, which was
higher than that of YOLOv5 (96.6%, 92.1%, 94.3% and 96.3%), Faster R-
CNN (79.7%, 95.4%, 86.8% and 90.6%), SSD (60.8%, 94.0%, 73.8% and
88.5%), YOLOv3 (83.7%, 83.0%, 83.3% and 70.6%) and EfﬁcientDet
(97.0%, 47.0%, 64.0% and 59.6%). Adding the CBAM module to YOLOv5
network improved the performance of the broiler detection model. It
also showed that the model YOLOv5-CBAM was suitable for the detec-
tion of broilers at different growth stages, in different litters type and
multiple pens. IThe FPS of YOLOv5-CBAM was 55 Frame/s, which was
lower than YOLOv5 (62 Frame/s), but higher than Faster R-CNN (2.6
Frame/s),SSD (3.1 Frame/s), YOLOv3 (18.9 Frame/s) and EfﬁcientDet
(36 EfﬁcientDet). It can be seen that the accuracy of YOLOV5-CBAM
has also been improved while maintaining a high processing speed,
and it can be applied to target detection or small target detection of
birds at different feeding densities.
3.2. Detection results in different scenes
Table 2 lists the detection precision of YOLOv5 and YOLOv5-CBAM at
different growth stages, on different litter types and in multiple pens. In
this sample dataset, the precision of YOLOv5-CBAM in each scene was
slightly higher than that of YOLOv5 (Table 2). The precision of detection
at d2 in fresh and reused litter was lower than d9 and d16. This is be-
cause broilers on d2 were small, and the feature extraction was not suf-
ﬁcient for crowded and occluded targets. This may have resulted in
missed detections or false positive and negative detections. Neverthe-
less, the precision of YOLOv5-CBAM (96.1%, 95.2%) at d2 was slightly
higher than YOLOv5 (95.1%, 94.3%). The detection precision of hens on
d23 was lower in the scene of fresh pine shavings than the reused litter
scene (as shown in Fig. 1), which could be caused by changes in
chickens' crowding or pilling behaviors on different litter ﬂoors. When
overcrowded, the target information of broilers could be lost, and thus
Fig. 4. Detection results using YOLOv5 and YOLOv5-CBAM in fresh pine shavings.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
40
lead to missed or false detection. In addition, the selection of samples
will also affect the performance of the test, and the accuracy of the
model detection will be reduced when the birds in the sample are too
crowded or heavily occluded. This is also the reason for the reduced ac-
curacy of YOLOv5-CBAM in d23. Although the precision of YOLOv5-
CBAM in each scene was slightly higher than YOLOv5 (Table 2), the
overall performance was better than YOLOv5 (Table 1). YOLOv5-
CBAM has been improved at different ages, different litter and different
population densities, and the generalization performance of the model
has also been improved, which can be applied to object detection in dif-
ferent feeding environments. It also provides technical support for the
accurate detection of commercial broiler breeding.
Fig. 4, Fig. 5, and Fig. 6 shows detection results of YOLOv5 and
YOLOv5-CBAM in different scenes (ﬂoor types). In Figs. 4 to 6, the
ﬁrst column is the detection results of YOLOv5, the second column
is the original images, and the third column is the detection results
of YOLOv5-CBAM. i → j in the Figs. 4 to 6, i is the actual number of
broilers, and j is the number of broilers detected broilers. It can be ob-
served from Figs. 4 to 6 that in different scenes, YOLOv5-CBAM can
detect broilers better than YOLOv5, and in the case of crowded or
small targets, it can still provide better detection results. For example,
in Fig. 4, YOLOv5 will falsely detect broilers under crowded condi-
tions, while YOLOv5-CBAM performed better under crowded condi-
tion. However, when the broilers were overcrowded, that is, the
broilers overlap and block each other signiﬁcantly, YOLOv5-CBAM
also has false detection (d23 in Fig. 5), but it was lower than
YOLOv5. In the case of multiple pens, the edge of the sample image
is distorted, and the broiler appears smaller in the ﬁeld of view, and
the occlusion is more substantial and resulted in false detections by
the model (Fig. 6).
Fig. 4 (continued).
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
41
4. Discussion
The YOLOv5-CBAM-broiler modle in the current study has a preci-
sion of 97.3% for broiler detection on the litter ﬂoor, which is higher
than that of Faster R-CNN, SSD and YOLOv5 models. The introduction
of CBAM attention mechanism can suppress the general features and
enhance the important features, thus effectively reducing the missed
or false detections. From Table 2 and Figs. 4 to 6, it can be found that
adding the CBAM module to YOLOv5 network can improve the
detection performance of the model against small or blocked targets.
However, when the broilers are signiﬁcantly blocked or there is a sub-
stantial distortion in the image, YOLOv5-CBAM has the phenomenon
of false detection or missing detection.
The datasets samples used for model development consisted of dif-
ferent image scenes of broilers at different ages, raised on litter types
and multiple pens. Therefore, the overall sample contains broilers of dif-
ferent sizes, crowding, occlusion, equipment interference, etc., which
will affect the detection performance. In addition, broilers have multiple
angles and poses in the scene, which will also affect the detection accu-
racy, as shown in Figs. 4 to 6. To sum up, it can be concluded that the se-
lection and number of samples will also affect the results.
YOLOv5-CBAM has a small model, high detection accuracy, and FPS
of 55 frames/s. It has good real-time performance and can be installed
on portable embedded platforms to develop mobile object detection
equipment, such as mobile robots. In addition, this method has achieved
good detection results in different scenarios, and the samples of differ-
ent varieties can be further enriched to further train the model in the
later stage, which is expected to achieve multi-target detection under
different varieties.
5. Conclusions
To detect broilers in different scenes (e.g., different ages, raised on
different litter types and multiple pens), we proposed using the
Fig. 5. Detection results using YOLOv5 and YOLOv5-CBAM in reused litter.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
42
YOLOv5-CBAM-broiler model. The proposed approach integrates CBAM
into YOLOv5 and improved the overall detection performance, espe-
cially in the case of small targets or occlusions. In addition, the results
show that YOLOv5-CBAM could detect broilers of different ages effec-
tively and provides the basis for real-time target detection for intelligent
poultry management.
CRediT authorship contribution statement
Yangyang Guo: Data curation, Investigation, Writing - original draft.
Samuel E. Aggrey: Resources, Supervision. Xiao Yang: Investigation.
Adelumola Oladeinde: Resources, Supervision. Yongliang Qiao: Data
curation. Lilong Chai: Conceptualization, Resources, Supervision.
Fig. 5 (continued).
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
43
Declaration of Competing Interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂu-
ence the work reported in this paper.
Acknowledgements
This study was supported by a cooperative grant 58-6040-6-030
(Lilong Chai) and 58-6040-8-034 (S. E. Aggrey) from the United State
Department of Agriculture-Agriculture Research Service; USDA-NIFA
Hatch Project (GEO00895): Future Challenges in Animal Production
Systems-Seeking Solutions through Focused Facilitation; UGA CAES
Dean's Ofﬁce Research Fund; and Georgia Research Alliance - Venture
Fund.
References
Alvarez, J.R., Arroqui, M., Mangudo, P., Toloza, J., Jatip, D., Rodriguez, J.M., Teyseyre, A.,
Sanz, C., Zunino, A., Machado, C., Mateos, C., 2019. Estimating body condition score
in dairy cows from depth images using convolutional neural networks, transfer learn-
ing and model ensembling techniques. Agronomy 9 (2), 90. https://doi.org/10.3390/
agronomy9020090.
Andrew, W., Greatwood, C., Burghardt, T., 2017. Visual localisation and individual identi-
ﬁcation of Holstein friesian cattle via deep learning. Proceedings of the IEEE Interna-
tional Conference on Computer Vision Workshops, pp. 2850–2859.
Awad, A.I., Zawbaa, H.M., Mahmoud, H.A., Nabi, E.H.H.A., Fayed, R.H., Hassanien, A.E.,
2013. A robust cattle identiﬁcation scheme using muzzle print images. In 2013 Fed-
erated Conference on Computer Science and Information Systems. IEEE, pp. 529–534.
Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: optimal speed and accuracy
of object detection. arXiv preprint arXiv:2004.10934. Doi: https://doi.org/10.48550/
arXiv.2004.10934.
Chai, L., Zhao, Y., Xin, H., Wang, T., Soupir, M.L., 2018. Mitigating airborne bacteria gener-
ations from cage-free layer litter by spraying acidic electrolysed water. Biosyst. Eng.
170, 61–71.
Chai, L., Xin, H., Wang, Y., Oliveira, J., Wang, K., Zhao, Y., 2019. Mitigating particulate mat-
ter generation in a commercial cage-free hen house. Trans. ASABE 62 (4), 877–886.
Chen, C., Zhu, W., Norton, T., 2021. Behaviour recognition of pigs and cattle: journey from
computer vision to deep learning. Comput. Electron. Agric. 187, 106255. https://doi.
org/10.1016/j.compag.2021.106255.
Fang, C., Huang, J., Cuan, K., Zhuang, X., Zhang, T., 2020. Comparative study on poultry tar-
get tracking algorithms based on a deep regression network. Biosyst. Eng. 190,
176–183.
Fang, C., Zhang, T., Zheng, H., Huang, J., Cuan, K., 2021. Pose estimation and behavior clas-
siﬁcation of broiler chickens based on deep neural networks. Comput. Electron. Agric.
180, 105863.
Fukui, H., Hirakawa, T., Yamashita, T., Fujiyoshi, H., 2019. Attention branch network:
learning of attention mechanism for visual explanation. Proceedings of the IEEE/
CVF Conference on Computer Vision and Pattern Recognition, pp. 10705–10714.
Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. Yolox: exceeding yolo series in 2021. arXiv pre-
print. https://doi.org/10.48550/arXiv.2107.08430 arXiv:2107.08430.
Guo, Y., Chai, L., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., 2020. A machine vision-
based method for monitoring broiler chicken ﬂoor distribution. Sensors 20 (11),
3179. https://doi.org/10.3390/s20113179.
Guo, Y.Y., Qiao, Y.L., Sukkarieh, S., Chai, L.L., He, D.J., 2021a. Bigru-attention based cow be-
havior classiﬁcation using video data for precision livestock farming. Trans. ASABE 64
(6), 1823–1833.
Guo, Y., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., Chai, L., 2021b. A machine vision-
based method optimized for restoring broiler chicken images occluded by feeding
and drinking equipment. Animals 11 (1), 123. https://doi.org/10.3390/ani11010123.
Guo, Y., Aggrey, P., Wang, S.E., Chai, L., 2022. Monitoring behaviors of broiler chickens at
different ages with deep learning. Animals 12 (23), 3390. https://doi.org/10.3390/
ani12233390.
He, K., Zhang, X., Ren, S., Sun, J., 2015. Spatial pyramid pooling in deep convolutional net-
works for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37 (9),
1904–1916. https://doi.org/10.1109/TPAMI.2015.2389824.
He, D.J., Liu, D., Zhao, K.X., 2016. Review of perceiving animal information and be-
havior in precision livestock farming. Transactions of the Chinese Society for
Agricultural Machinery 47 (5), 231–244. https://doi.org/10.6041/j.issn.1000-
1298.2016.05.032.
Jocher, G., Nishimura, K., Mineeva, T., Vilariño, R., 2020. yolov5. Code repository.
Li, X., Jia, X., Wang, Y., Yang, S., Zhao, H., Lee, J., 2020. Industrial remaining useful life pre-
diction by partial observation using deep learning with supervised attention. IEEE/
ASME Transactions on Mechatronics 25 (5), 2241–2251. https://doi.org/10.1109/
TMECH.2020.2992331.
Fig. 6. Detection results using YOLOv5 and YOLOv5-CBAM in multiple pens.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
44
Li, G., Huang, Y., Chen, Z., Chesser Jr., G.D., Purswell, J.L., Linhoss, J., Zhao, Y., 2021. Practices
and applications of convolutional neural network-based computer vision systems in
animal farming: a review. Sensors 21 (4), 1492. https://doi.org/10.3390/s21041492.
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., 2016. Ssd: Single
shot multibox detector. European Conference on Computer Vision. Springer, Cham,
pp. 21–37. https://doi.org/10.1007/978-3-319-46448-0-2.
Liu, K., Tang, H., He, S., Yu, Q., Xiong, Y., Wang, N., 2021. Performance validation of YOLO
variants for object detection. Proceedings of the 2021 international conference on
bioinformatics and intelligent computing, pp. 239–243. https://doi.org/10.1145/
3448748.3448786.
Okinda, C., Nyalala, I., Korohou, T., Okinda, C., Wang, J., Achieng, T., Wamalwa, T., Manga,
T., Shen, M., 2020. A review on computer vision systems in monitoring of poultry: a
welfare perspective. Artiﬁcial Intelligence in Agriculture 4, 184–208. https://doi.org/
10.1016/j.aiia.2020.09.002.
Qiao, Y., Truman, M., Sukkarieh, S., 2019. Cattle segmentation and contour extraction
based on mask R-CNN for precision livestock farming. Comput. Electron. Agric. 165,
104958. https://doi.org/10.1016/j.compag.2019.104958.
Qiao, Y., Kong, H., Clark, C., Lomax, S., Su, D., Eiffert, S., Sukkarieh, S., 2021. Intelligent per-
ception for cattle monitoring: a review for cattle identiﬁcation, body condition score
evaluation, and weight estimation. Comput. Electron. Agric. 185, 106143. https://doi.
org/10.1016/j.compag.2021.106143.
Qin, Q., Liu, Z., Zhao, C., Zhang, C., Dai, D., Sun, J., Wang, Z., Li, J., 2021. Application of ma-
chine vision Technology in Livestock and Poultry. Agric. Eng. 11 (07), 27–33.
Redmon, J., Farhadi, A., 2018. Yolov3: An incremental improvement. arXiv preprint.
https://doi.org/10.48550/arXiv.1804.02767.
Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: towards real-time object detection
with region proposal networks. Adv. Neural Inf. Proces. Syst. 28.
Shen, W., Hu, H., Dai, B., Wei, X., Sun, J., Jiang, L., Sun, Y., 2020. Individual identiﬁcation of
dairy cows based on convolutional neural networks. Multimed. Tools Appl. 79 (21),
14711–14724.
Subedi, S., Bist, R.B., Yang, X., Chai, L., 2023a. Tracking pecking behaviors and damages of
cage-free laying hens with machine vision technologies. Comput. Electron. Agric. 204
(1), 107545.
Subedi, S., Bist, R.B., Yang, X., Chai, L., 2023b. Tracking ﬂoor eggs with machine vision in
cage-free hen houses. Poult. Sci. 102637.
Tan, M., Pang, R., Le, Q.V., 2020. Efﬁcientdet: scalable and efﬁcient object detection. Pro-
ceedings of the IEEE/CVF conference on computer vision and pattern recognition,
pp. 10781–10790.
Tharwat, A., Gaber, T., Hassanien, A.E., 2014. Cattle identiﬁcation based on muzzle images
using gabor features and SVM classiﬁer. International Conference on Advanced Ma-
chine Learning Technologies and Applications. Springer, Cham, pp. 236–247.
Tian, M., Guo, H., Chen, H., Wang, Q., Long, C., Ma, Y., 2019. Automated pig counting using
deep learning. Comput. Electron. Agric. 163, 104840. https://doi.org/10.1016/j.
compag.2019.05.049.
Wang, Z., Song, H., Wang, Y., Hua, Z., Li, R., Xu, X., 2022. Research progress on intelligent
morning methods of dairy Cow’s motion behavior. Smart Agriculture 1–18 (2022-06-
12).
Woo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. Cbam: convolutional block attention module.
Proceedings of the European Conference on Computer Vision (ECCV), pp. 3–19.
Xue, H., Qin, J., Quan, C., Ren, W., Gao, T., Zhao, J., 2021. Open set sheep face recognition
based on euclidean space metric. Math. Probl. Eng. 1-5. https://doi.org/10.1155/
2021/3375394.
Yang, X., Chai, L., Bist, R.B., Subedi, S., Wu, Z., 2022. A deep learning model for detecting
cage-free hens on the litter ﬂoor. Animals 12 (15), 1983. https://doi.org/10.3390/
ani12151983.
Yang, X., Bist, R., Subedi, S., Chai, L., 2023. A deep learning method for monitoring spatial
distribution of cage-free hens. Artiﬁcial Intelligence in Agriculture 8, 20–29. https://
doi.org/10.1016/j.aiia.2023.03.003.
Y. Guo, S.E. Aggrey, X. Yang et al.
Artiﬁcial Intelligence in Agriculture 9 (2023) 36–45
45
