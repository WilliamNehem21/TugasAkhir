Unilateral Alignment: An interpretable machine learning method for
geophysical logs calibration
Wenting Zhang a, Jichen Wang b, Kun Li c, Haining Liu e, Yu Kang a,c,d, Yuping Wu d,
Wenjun Lv a,c,*
a Department of Automation, University of Science and Technology of China, Hefei, 230027, China
b College of Control Science and Engineering, China University of Petroleum, Qingdao, 266580, China
c Institute of Advanced Technology, University of Science and Technology of China, Hefei, 230031, China
d Institute of Artiﬁcial Intelligence, Hefei Comprehensive National Science Center, Hefei, 230088, China
e Shengli Geophysical Research Institute, SINOPEC Group, Dongying, 257022, China
A R T I C L E I N F O
Keywords:
Interpretable machine learning
Geophysical logs calibration
Data distribution discrepancy
A B S T R A C T
Most of the existing machine learning studies in logs interpretation do not consider the data distribution
discrepancy issue, so the trained model cannot well generalize to the unseen data without calibrating the logs. In
this paper, we formulated the geophysical logs calibration problem and give its statistical explanation, and then
exhibited an interpretable machine learning method, i.e., Unilateral Alignment, which could align the logs from
one well to another without losing the physical meanings. The involved UA method is an unsupervised feature
domain adaptation method, so it does not rely on any labels from cores. The experiments in 3 wells and 6 tasks
showed the effectiveness and interpretability from multiple views.
1. Introduction
Artiﬁcial intelligence (AI) is increasingly becoming an important
scientiﬁc and technological tool in solving geosciences issues such as
earthquakes detection, spatial prediction, stratigraphic correlation,
seismic processing and interpretation, etc. (Magrini et al., 2020; Foued-
jio, 2020; Xu et al., 2022; Zhou et al., 2020; Birnie et al., 2021). It has
reported that the rapidly evolving ﬁeld of machine learning (ML), a
popular way to realize AI, will play a key role in geosciences (Bergen
et al., 2019). However, due to the black-box issue existing in many ML
models, we cannot know the way how an ML model works, so cannot
assess its reliability and have enough conﬁdence to apply it in produc-
tion. We believe that the interpretability of ML will play an extremely
important role in geosciences in which there are too many risk-aware
applications, e.g., exploration.
The geophysical logs interpretation means the prediction of borehole
geology information (e.g., lithofacies, porosity) according the logs, which
is a fundamental and worthy work to understand the undersurface earth.
The recent years have witnessed the development of the applications of
machine learning in logs interpretation. For example, a semi-supervised
model named Laplacian support vector machine is proposed to solve the
problems of scarce labels (Li et al., 2020). The smoothnesses implying in
the feature space and depth are utilized to propagate the labels form
labelled samples to unlabelled ones, therefore increasing the classiﬁca-
tion accuracy. Although the Laplacian works in semi-supervised learning,
the setting of Laplacian is still empirical so that the introduction of
unlabelled data might result in a worse performance compared with just
using labelled data. Hence, an ensemble mechanism is used to combine
some candidate Laplacians and calculate an optimal one to guarantee the
safety of using semi-supervision (Li et al., 2021). More similar work could
be found in the literatures (Dunham et al., 2020; Imamverdiyev and
Sukhostat, 2019; Zhu et al., 2018).
Although there has been a great deal of researches in this area, most
of them work under the assumption of independent and identically dis-
tribution (iid), i.e., the data from training wells should have similar
distribution with those data from testing wells. However, such an
assumption barely hold due to the differences in logging equipments,
borehole conditions, etc. As illustrated in Fig. 1c, there are a signiﬁcant
distribution discrepancy between the training dataset and testing dataset,
so iid does not hold anymore. As shown in Fig. 1a and b, the non-iid
problem causes an accuracy decrease in interpreting the logs from
another wells by the trained classiﬁer. Both supervised and semi-
* Corresponding author. Department of Automation, University of Science and Technology of China, Hefei, 230027, China.
E-mail address: wlv@ustc.edu.cn (W. Lv).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2022.02.006
Received 27 December 2021; Received in revised form 27 February 2022; Accepted 27 February 2022
Available online 14 March 2022
2666-5441/© 2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
supervised classiﬁcation methods cannot handle such a situation, so
some pre-processing work should be done in advance of interpretation. In
addition, data mining and processing techniques can be coupled with
machine learning algorithms to solve the problem of data imbalance
(Long et al., 2016; Zhong et al., 2020) Filtering techniques help us to
remove some of the random non-stratigraphic responses (Lv et al., 2018;
Ruckebusch, 1983), but do not have a signiﬁcant effect on the distribu-
tion alignment. For such a geophysical logs calibration issue, we can
resort to the Domain Adaptation (DA) methods which are very popular in
machine learning.
As a transfer learning method, DA tries to ﬁnd the relevance between
two datasets and then adopts the knowledge from one domain to another.
DA could works in an unsupervised manner which means that we only
need the samples from two datasets. DA generally falls into three cate-
gories: model-, sample-, and feature-based methods (Yang et al., 2020).
The sample-based DA re-weights the samples overlapped in the feature
space during the training phase to yield a more robust model in both
domain, but it could only handle the small distribution discrepancy issue
(Chang et al., 2021b; Chang et al., 2022). The model-based DA re-adjust
the source-domain classiﬁer to ﬁt the target domain (Liu et al., 2020). For
a neural network model, usually the last few layer are allowed to adjust,
so the transferability is not strong enough. The feature-based DA (fDA)
tries to map the samples to a feature space with higher dimension such
that two domains align (Li et al., 2018). As we know that a deep neural
network possesses multiple layers for feature extraction, so the fDA
usually has more feasibility. Most fDA methods map both domains, so the
physical meanings of the original features would loss (Chang et al.,
2021a). Instead, we could just align one domain to another in the original
feature space, therefore maintaining the physical meanings (Chen et al.,
2018; Wu et al., 2022). Such a fDA method could be named Unilateral
Alignment (UA) which might be an excellent tool for geophysical logs
calibration to our opinions.
Fig. 1. Illustration of data distribution discrepancy. (a) Logs, cores (i.e, labels), and predictions on Well A using the model trained on Well B; (b) Logs, cores, pre-
dictions on Well B using the model trained on Well A; (c) t-SNE (van der Maaten and Hinton, 2008) visualization of data distribution for Well A and Well B. Gray,
yellow, and green indicate mudstone, sandstone, and dolomite, respectively. Dots and boxes indicate the data from Well A and Well B, respectively.
Fig. 2. Illustration of the UA framework.
Table 1
Dataset description.
Wellsb
Lithologya
Mu
Si
Co
Sum
A
955
512
113
1580
B
514
396
108
1018
C
735
220
100
1055
a Mu, mudstone; Si, siltstone; Co, conglomeratic sandstone.
b Logging curves of each well: AC, acoustic log; CAL, caliper log; COND, con-
ductivity log; GR, gamma ray log; R25, 2.5 m bottom gradient resistivity; and SP,
spontaneous potential log resistivity.
Table 2
Macro-average F1-score (%) on 6 tasks.
Tasks
Methods
S.O.a
T.O.b
UA
B → A
82.7
98.5
95.9
C → A
80.0
99.3
92.7
A → B
82.9
99.9
97.2
C → B
79.7
99.5
93.5
A → C
92.3
99.4
96.1
B → C
73.3
98.4
93.7
a Weighted ELM classiﬁer trained with source-domain examples.
b Weighted ELM classiﬁer trained with target-domain examples.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
193
In this paper, we would exhibit an interpretable machine learning
method, i.e., UA, which could align the logs from one well to another
without losing the physical meanings. In addition, the involved UA
method is an unsupervised fDA method, so it does not rely on any labels
from cores. The contributions are threefold. First, we formulate the
geophysical logs calibration problem and give its statistical explanation.
Second, we present the derivation of UA and illustrate its application in
logs calibration. Third, we conduct massive experiments in 3 wells and 6
tasks to show the effectiveness and interpretability from multiple views.
The above three contributions correspond to Sec. 2, 3, and 4,
respectively.
2. Formulation of geophysical logs calibration problem
In the logs interpretation problem (e.g., lithofacies identiﬁcation), the
well logging feature vector (also named instance or sample) of d logs at a
certain depth is denoted by xi ¼ ½xi1; xi2; ⋯ ; xid� 2 Rd, including acoustic
log (AC), Gamma ray log (GR), caliper log (CAL), etc. The lithofacies
corresponding to the instance xi is denoted by
yi ¼ ½0; …; 0; 1
zﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄ{ k
; 0; …; 0
|ﬄﬄﬄ{zﬄﬄﬄ}
k0�k
�;
Fig. 3. F1-score of each class in six tasks.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
194
Fig. 4. Visualization of Well A logs in the task A → B before and after calibration.
Fig. 5. Visualization of Well B logs and the corresponding predicted lithologies in the task A → B before and after calibration.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
195
when xi belongs to the k-th (1 � k � k0) lithofacies type of the total k0
types. n examples (xi, yi) along the depth constitute a labelled dataset D ¼
fðxi; yiÞgn
i¼1 from the well.
In most cases, the source-domain instances and the target-domain
ones have different probability distributions. With a logs calibration,
the accuracy of lithofacies identiﬁcation might decrease when predicting
the target-domain instances by the model trained on source-domain ex-
amples. In this case, the expert-interpreted dataset is denoted as the
source-domain dataset Ds ¼ fðxs
i; ys
iÞgns
i¼1, and the dataset of an unin-
terpreted well is called target-domain dataset Dt ¼ fxt
jgnt
j¼1, where ns, nt
represent the number of the instances in Ds, Dt, respectively. Besides,
these instances of both domains belong to the same feature space X,
which means that xs;xt 2 X. The label space Y of Ds is also identical with
Dt's, i.e., ys;yt 2 Y.
Since the datasets of two domains are sampled from respective joint
distributions, the discrepancies of probability distributions between two
domains can be seen as the discrepancies of two joint distributions i.e.,
Ps(xs, ys) 6¼ Pt(xt, yt) (Vapnik, 1999). The joint distributions of Ds and Dt
are rewritten as Ps(xs, ys) ¼ Ps(xs)Ps(ys|xs), Pt(xt, yt) ¼ Pt(xt)Pt(yt|xt),
respectively, where Ps(⋅), Pt(⋅) are marginal probability distributions, and
Ps(⋅|⋅), Pt(⋅|⋅) are conditional ones. In general, the conditional probability
distributions are shared across domains (Stojanov et al., 2021). Under the
assumption of Ps(ys|xs) ¼ Pt(yt|xt), the problem turns to tackle the
unequality of marginal probability distributions, i.e., Ps(xs) 6¼ Pt(xt).
In order to improve the performance in the case of probability
discrepancy, we propose a calibration method to learn a unilateral cali-
brating function f compelling Ps(f(xs)) � Pt(f(xt)) and consequently
Ps(f(xs), ys) � Pt(f(xt), yt). In this way, the classiﬁer trained on Ds would
have a good performance in the interpretation task on a target-domain
well.
3. Unilateral Alignment for geophysical logs calibration
We ﬁrst give some preliminaries in Sec. 3.1 and 3.2, and then yields
the Unilateral Alignment (UA) algorithm in Sec. 3.3.
3.1. Extreme Learning Machine (ELM)
Extreme Learning Machine (ELM) can be used to address classiﬁca-
tion and regression problems, which is proposed by G.-B. Huang (Huang
et al., 2004, 2011). Its model training is efﬁcient compared with some
traditional learning algorithms, meantime completing regression and
multi-class classiﬁcation tasks effectively. For example, considering a
classiﬁcation task with training dataset fX; Yg ¼ fðxi; yiÞgn
i¼1, ELM cor-
responding to the training dataset with L hidden layer nodes is described
as
XL
i¼1 βigðw>
i xj þ biÞ ¼ oj; j ¼ 1; …; n;
(1)
where g(⋅) is the nonlinear piecewise continuous activation function (e.g.,
Sigmoid function), wi ¼ ½wi1; wi2; ⋯ ; wid� 2 Rd and bi are the randomly-
generated weight vector and bias of the i-th hidden layer node, βi is the
output weight of the i-th hidden layer node. oj is the prediction of the j-th
instance.
To minimize the predicted error, the error function can be deﬁned as
Xn
j¼1 koj � yjk ¼ 0;
(2)
where yj denotes the j-th label among the n distances. Combining (1) and
Table 3
The mean values of each log on the task A → B.
Domain
Mean
AC
CAL
COND
GR
R25
SP
Original Source
429.50
21.81
43.50
81.41
1.80
90.69
Aligned Source
429.07
21.18
43.45
81.32
1.79
90.62
Original Target
417.17
22.25
44.31
74.22
1.68
87.84
Aligned Target
429.07
21.18
43.45
81.32
1.79
90.62
Table 4
The mean values of each log on the task A → C.
Domain
Mean
AC
CAL
COND
GR
R25
SP
Original Source
429.50
21.81
43.50
81.41
1.80
90.69
Aligned Source
429.50
21.81
43.50
81.41
1.80
90.69
Original Target
438.92
22.03
44.96
74.37
1.83
96.41
Aligned Target
437.15
21.98
43.66
79.69
1.82
95.44
Fig. 6. Frequency distribution histogram of SP log in the task A → B.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
196
(2) yields Hβ ¼ Y, where H ¼ ½hðx1Þ; hðx2Þ; ⋯ ; hðxnÞ� 2 Rn�L represents
the mapped instance matrix, and β 2 RL�k0 denotes the output weight
matrix, hðxiÞ ¼ ½gðxiw1 þb1Þ; gðxiw2 þb2Þ; …; gðxiwL þbLÞ� 2 R1�L is the i-
th mapped instance. Hence, the loss function is
min
β
kHβ � Yk2;
(3)
where β could be calculated by the Moore-Penrose (MP) generalized
inverse, that is,
β* ¼ HyY;
(4)
where Hy is the MP generalized inverse matrix of H. To avoid the over-
ﬁtting issue, (3) could be augmented as
min
β
1
2kβk2 þ C
2kHβ � Yk2;
(5)
where C is the penalty factor to balance the two terms. Furthermore, we
have
β* ¼
8
>
>
>
>
<
>
>
>
>
:
H>
�
HH> þ I
C
��1
Y; n < L
�
H>H þ I
C
��1
H>Y; n � L
(6)
3.2. Maximum mean discrepancy (MMD)
Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) is
Fig. 7. Frequency distribution histogram of SP log in the task A → C.
Fig. 8. Frequency distribution histogram of AC log in the task A → B.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
197
generally used to measure the similarity between two probability dis-
tributions of the source-domain and the target-domain instances. The
distance is measured by mapping the instances into a Reproducing Kernel
Hilbert Space (RKHS). Given two distributions P and Q, the formula of
the distance based on the maximum mean discrepancy criterion between
them is
MMD2ðF; P; QÞ ¼ sup
f 2F
kEP½f ðxÞ� � EQ½f ðyÞ�k2
(7)
where H denotes an RKHS space, F denotes a class of functions. The
continuous function f(⋅) is the mapping function, sup(⋅) is an upper bound
function, and EP(x), EQ(x) are expectations. The empirical estimation of
MMD in an RKHS is
MMD2ðX; YÞ ¼ jj1
n
Xn
i¼1 φðxiÞ � 1
m
Xm
j¼1 φðxjÞjj2
H
(8)
where φ(⋅) is the kernel-induced feature mapping function.
3.3. Unilateral Alignment (UA)
For the case that Ps(xs) 6¼ Pt(xt), UA of geophysical logs calibration
aims to adjust the logs of one well to align it with another. To achieve this
goal, we propose the UA calibration method by combining ELM and
MMD.
We introduce the random mapping used in ELM to our method, i.e.,
the instance xi is mapped by hðxiÞ 2 RL, where L is the dimension of
mapped instance. Additionally, we deﬁne Hs 2 Rns�L and Ht 2 Rnt�L as
Fig. 9. Frequency distribution histogram of COND log in the task A → B.
Fig. 10. Frequency distribution histogram of GR log in the task A → B.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
198
the random mapping matrices of all the instances in datasets Ds, Dt,
respectively. In this case, MMD could be expressed by
MMD2ðDs; DtÞ ¼ jj 1
ns
Xns
i¼1 hðxs
iÞ � 1
nt
Xnt
j¼1 hðxt
jÞjj2
(9)
A transform matrix β operating to each mapped instance is used to
reduce the distribution discrepancy, so we have
min
β
jj 1
ns
Xns
i¼1 hðxs
iÞβ � 1
nt
Xnt
j¼1 hðxt
jÞβjj2 ¼ min
β
Trðβ>H>ΩHβÞ
(10)
where Tr(⋅) is the matrix trace, and Ω 2 Rn�n denotes the marginal
PMMD matrix, that is,
~Ωij ¼
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
1
n2
s
;
i; j � ns
1
n2
t
;
i; j > ns
� 1
nsnt
; others
(11)
Nevertheless, the random mapping function leads to the interaction of
logs, thereby the mapped features lose the physical meanings. In this
case, the regularization term of source domain maintenance (Chen et al.,
2018) is introduced in the process of calculating β, which could maintain
the physical meanings of aligned features. Hence, the process of logs
calibration is interpretable through our method. Speciﬁcally, by intro-
ducing the source domain maintenance term, the equation (10) can be
written as
min
β
λ
2 Trðβ>H>ΩHβÞ þ γ
2kXs � Hsβk2
(12)
where the second term denotes the regularization term of source domain
maintenance. Xs 2 Rns�L is the source-domain instance matrix. The
penalty factors λ and γ are used to control the contributions of each term.
To prevent the overﬁtting issue, (12) is augmented with a complexity
measure term as follows:
min
β
1
2kβk2 þ λ
2 Trðβ>H>ΩHβÞ þ γ
2kXs � Hsβk2:
(13)
Eventually, we obtain the solution of (13), that is,
β* ¼
8
>
>
>
>
<
>
>
>
>
:
H>
s
�In
γ þ HsH>
s þ λ
γ ΩHH>
��1
Xs; n < L
�IL
γ þ H>
s Hs þ λ
γH>ΩH
��1
H>
s Xs; n � L
(14)
where β* is the optimal transform matrix, and In 2 Rn�n, IL 2 RL�L are
both identity matrices. Speciﬁcally, the UA calibrating function f could be
deﬁned as fðxsÞ ¼ Hsβ ¼ ~Xs � Xs and fðxtÞ ¼ Htβ ¼ ~Xt, where ~Xs 2
Rns�d; ~Xt 2 Rnt�d are the calibrated instance matrices corresponding to
Hs, Ht, respectively. In this case, this method could compel the proba-
bility distributions Psð~xsÞ � Ptð~xtÞ, and consequently achieve Psð~xs;ysÞ �
Ptð~xt; ytÞ. Subsequently, we could use the source-domain calibrated
dataset ð~Xs; YsÞ to train an ELM and test on target-domain calibrated
dataset ~Xt with high accuracy, where Ys 2 Rns�k0 is the label matrix of
source-domain instances. The UA framework is illustrated in Fig. 2.
4. Experimental analysis
In this section, we conduct a large number of experiments using well-
logging data collected from multiple wells in Jiyang depression, Bohai
Bay Basin to verify the validity and interpretability of our method. First,
we describe the dataset and the experimental settings. Second, we
compare the performances of UA, S.O., and T.O., and present the accu-
racy of each lithology via the bar graphs. Finally, we show the visualized
results and discuss the interpretability of our method. Here, S.O., means
the model trained on source-domain dataset only, which could be seen as
the lower bound of UA. If UA has a lower accuracy than S.O., then
negative transfer happens. T.O. means the model trained on target-
domain dataset only, which could be seen as the upper bound of UA.
4.1. Experimental settings
The dataset consists of three wells, which is shown in Table 1. Wells
A, B, and C are from the same area and have the same types of logs and
lithologies, that is, they have the same input and output space. To verify
the calibration effect, a three-class ELM is trained to classify three types
of lithologies and evaluated by average F1-Score which considers both
Fig. 11. Frequency distribution histogram of R25 log in the task A → B.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
199
the accuracy and recall ratios of the classiﬁcation model. F1-Score could
be expressed by
averageF1 � Score ¼ F1 þ F2 þ ⋯ þ Fc
c
(15)
where c is the number of categories and the Fi is F1-Score of i-th category
which can be expressed by
Fi ¼ 2 � Pi � Ri
Pi þ Ri
(16)
where the Pi represents the i-th Precision and Ri is the Recall, which can
be computed by
Pi ¼
TP
TP þ FP
Ri ¼
TP
TP þ FN
(17)
where the TP, FP and FN represent true positive(an instance are positive
classes and are judged to be positive classes), false positive(a false class is
judged to be a positive class) and false negative(a false class is judged to
be a negative class), respectively.
Due to the wide range of logs, we use min-max normalization to
convert all logging data to those within 0 and 1. The number of hidden
neurons are set 800, the weights and bias are generated randomly by the
Gaussian distribution with variance equalling 3. The trade-off parameter
γ is searched from 10�3 to 104, and λ from 102 to 106. S.O. and T.O. are
realized by weighted ELM which could eliminate the inﬂuence of un-
balanced classes. All experiments are implemented by Python 3.8 on a
desktop computer with 2.90-GHz CPU and 16.00-GB RAM.
4.2. Comparative study
The performances of these methods are evaluated by different com-
binations of wells on the experimental dataset. For example, we set Well
A as the source domain for training and Well B as the target domain for
testing (i.e., task A → B). Therefore, there are a total of six combinations
to be compared, i.e., A → B, A → C, B → A, B → C, C → A, and C → B.
Table 2 lists the macro-average F1-scores of these methods on 6 tasks. We
have the following observations. (i) In all six tasks, the high F1-score was
achieved by using T.O. method, while the F1-score was signiﬁcantly
decreased by using S.O. method. This indicates that the data distribution
varies from well to well. (ii) The F1-scores of UA performing on all tasks
are higher than S.O., which shows the effectiveness of UA in the case of
data discrepancy. (iii) The F1-scores of UA is very close to those of the
T.O. baselines. Especially, it can reach 97.2% on the task A → B. (iv) In
the six tasks, the F1-scores of UA are improved by about 10% compared
with S.O. baseline method, and even improved by 20% on the task B → C.
(v) Although the F1-score of S.O. baseline is relatively high, UA still have
an improvement on task A → C. The F1-scores for each lithology are
shown in Fig. 3. It is easy to ﬁnd that S.O. usually overﬁts to the mudstone
so that its F1-score might be high on some tasks. UA eliminates the dis-
tribution discrepancies in the six tasks, so the classiﬁcation after logs
calibration could perform well on each lithology.
4.3. Deeper discussions
Taking experiment A → B as an example, Fig. 4 and Fig. 5 show the
changes of logs of Well A and Well B. It is obvious that each log in source
domain has a slight change as shown in Fig. 4. Most of the macroscopic
characteristics are preserved. Some microscopic characteristics are
weakened due to low-pass ﬁltering function of the regularization term of
source domain maintenance. As shown in Fig. 5, some logs has a great
change and the predictions are signiﬁcantly improved by UA Observing
areas a and b in Fig. 5, we think that UA smoothen R25 by integrating
CAL, so the calibrated R25 can be seen as that under the situation without
borehole expansion. Such a smoothness in R25 avoids many mis-
classiﬁcations. In areas of c and d, although the calibrated logs change
greatly, it keeps the basic variation trend of the original logs.
The following analysis are based on the statistic information of logs,
i.e., the differences of means and distribution histograms between two
domains. Taking A → B and A → C as an examples, it can be seen from
Table 3 and Table 4 that the mean of each log in the aligned (calibrated)
target domain and source domain are almost the same as those in the
original source domain. This indicates that the means are aligned
unilaterally. Subsequently, Fig. 6 presents the distribution histograms of
Well A SP and Well B SP in two domains. It can be seen that the SP log
distribution of target domain is obviously aligned to the one of source
domain, while the original SP log distribution of Well A and Well B is
quite different. This could be the reason why the F1-score in the task A →
B could be increased greatly as shown in Table 2. According to Fig. 7, it
can be seen that there is a small difference in SP log distribution between
Well A and Well C, so S.O. method used to predict Well C can achieve a
high F1-score of 92.3%. In this case, UA should align to Well A in the
high-dimensional space while preserving the original information of Well
C as much as possible. The frequency distribution histograms of other
logs are shown in Figs. 8–11 which all support the superiority of UA
5. Conclusions and further work
In this paper, we formulated the geophysical logs calibration problem
and give its statistical explanation, and exhibited an interpretable ma-
chine learning method, i.e., UA, which could align the logs from one well
to another without losing the physical meanings. Massive experiments in
3 wells and 6 tasks showed the effectiveness and interpretability from
multiple views. The involved UA method is an unsupervised feature
domain adaptation method, so it does not rely on any labels from cores.
Actually, the source-domain labels could be used to pre-train a model and
outputs some pseudo-labels for the target domain, such that we can
obtain a conditional MMD instead of the marginal MMD used in the
paper. With the conditional MMD, the conditional probability distribu-
tion of both domain could be aligned, which might have a better cali-
bration effect than the current work.
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂuence
the work reported in this paper.
Acknowledgements
This work was supported in part by the National Natural Science
Foundation of China under Grant 61903353, and in part by the SINOPEC
Programmes for Science and Technology Development under Grant
PE19008-8.
References
Bergen, K.J., Johnson, P.A., de Hoop, M.V., et al., 2019. Machine learning for data-driven
discovery in solid earth geoscience. Science 363 (6433).
Birnie, C., Ravasi, M., Liu, S., Alkhalifah, T., 2021. The potential of self-supervised
networks for random noise suppression in seismic data. Artif. Intell. Geosci. 2, 47–59.
Chang, J., Li, J., Kang, Y., Lv, W., Xu, T., Li, Z., Xing Zheng, W., Han, H., Liu, H., 2021a.
Unsupervised domain adaptation using maximum mean discrepancy optimization for
lithology identiﬁcation. Geophysics 86 (2), ID19–ID30.
Chang, J., Kang, Y., Zheng, W.X., Cao, Y., Li, Z., Lv, W., Wang, X.-M., 2021b. Active
domain adaptation with application to intelligent logging lithology identiﬁcation.
IEEE Trans. Cybern.
Chang, J., Kang, Y., Li, Z., Zheng, W.X., Lv, W., Feng, D.-Y., 2022. Cross-domain lithology
identiﬁcation using active learning and source reweighting. IEEE Geoscience and
Remote Sensing Letters 19, 1–5.
Chen, Y., Song, S., Li, S., Yang, L., Wu, C., 2018. Domain space transfer extreme learning
machine for domain adaptation. IEEE Trans. Cybern. 49 (5), 1909–1922.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
200
Dunham, M.W., Malcolm, A., Kim Welford, J., 2020. Improved well-log classiﬁcation
using semisupervised label propagation and self-training, with comparisons to
popular supervised algorithms. Geophysics 85 (1), O1–O15.
Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.
Artif. Intell. Geosci. 1, 11–23.
Gretton, A., Borgwardt, K., Rasch, M., Sch€olkopf, B., Smola, A., 2006. A kernel method for
the two-sample-problem. Adv. Neural Inf. Process. Syst. 19, 513–520.
Huang, G.-B., Zhu, Q.-Y., Siew, C.-K., 2004. Extreme learning machine: a new learning
scheme of feedforward neural networks. In: Proceedings of the IEEE International
Joint Conference on Neural Networks, vol. 2. Ieee, pp. 985–990.
Huang, G.-B., Zhou, H., Ding, X., Zhang, R., 2011. Extreme learning machine for
regression and multiclass classiﬁcation. IEEE Trans. Syst. Man Cybern. B
(Cybernetics) 42 (2), 513–529.
Imamverdiyev, Y., Sukhostat, L., 2019. Lithological facies classiﬁcation using deep
convolutional neural network. J. Petrol. Sci. Eng. 174, 216–228.
Li, S., Song, S., Huang, G., Wu, C., 2018. Cross-domain extreme learning machines for
domain adaptation. IEEE Trans. Syst. Man Cybern.: Systems 49 (6), 1194–1207.
Li, Z., Kang, Y., Feng, D., Wang, X.-M., Lv, W., Chang, J., Zheng, W.X., 2020. Semi-
supervised learning for lithology identiﬁcation using Laplacian support vector
machine. J. Petrol. Sci. Eng. 195, 107510.
Li, Z., Kang, Y., Lv, W., Zheng, W.X., Wang, X.-M., 2021. Interpretable semisupervised
classiﬁcation method under multiple smoothness assumptions with application to
lithology identiﬁcation. Geosci. Rem. Sens. Lett. IEEE 18 (3), 386–390.
Liu, H., Wu, Y., Cao, Y., Lv, W., Han, H., Li, Z., Chang, J., 2020. Well logging based
lithology identiﬁcation model establishment under data drift: a transfer learning
method. Sensors 20 (13), 3643.
Long, W., Chai, D., Aminzadeh, F., 2016. Pseudo density log generation using artiﬁcial
neural network. In: SPE Western Regional Meeting. OnePetro.
Lv, W., Kang, Y., Zhao, Y., 2018. Self-tuning asynchronous ﬁlter for linear Gaussian
system and applications. IEEE/CAA J. Automat. Sin. 5 (6), 1054–1061.
Magrini, F., Jozinovi�c, D., Cammarano, F., Michelini, A., Boschi, L., 2020. Local
earthquakes detection: a benchmark dataset of 3-component seismograms built on a
global scale. Artif. Intell. Geosci. 1, 1–10.
Ruckebusch, G., 1983. A kalman ﬁltering approach to natural gamma ray spectroscopy in
well logging. IEEE Trans. Automat. Control 28 (3), 372–380.
Stojanov, P., Li, Z., Gong, M., Ca, R., Carbonell, J., Zhang, K., 2021. Domain adaptation
with invariant representation learning: what transformations to learn? Adv. Neural
Inf. Process. Syst. 34.
van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9
(Nov), 2579–2605.
Vapnik, V., 1999. The Nature of Statistical Learning Theory. Springer science & business
media.
Wu, Y., Yang, Y., Lv, W., Chang, J., Li, Z., Feng, D., Xu, T., Li, J., 2022. Robust unilateral
alignment for subsurface lithofacies classiﬁcation. IEEE Trans. Geosci. Rem. Sens. 60,
1–13.
Xu, T., Chang, J., Kang, Y., Lv, W., Li, J., Han, H., Liu, H., 2022. Intelligent cross-well
sandstone prediction based on convolutional neural network. IEEE Geoscience and
Remote Sensing Letters 19, 1–5.
Yang, Q., Zhang, Y., Dai, W., Pan, S.J., 2020. Transfer Learning. Cambridge University
Press.
Zhong, R., Johnson, R.L., Chen, Z., 2020. Using machine learning methods to identify coal
pay zones from drilling and logging-while-drilling (lwd) data. SPE J. 25, 1241–1258,
03.
Zhou, R., Cai, Y., Zong, J., Yao, X., Yu, F., Hu, G., 2020. Automatic fault instance
segmentation based on mask propagation neural network. Artif. Intell. Geosci. 1,
31–35.
Zhu, L., Li, H., Yang, Z., Li, C., Ao, Y., 2018. Intelligent logging lithological interpretation
with convolution neural networks. Petrophysics 59, 799–810, 06.
W. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201
201
