Exact Conditioning of Regression Random Forest for Spatial Prediction
Francky Fouedjio
AngloGold Ashanti Australia Ltd., Growth and Exploration, 140 St. Georges Terrace, Perth, WA, 6000, Australia
A R T I C L E I N F O
Keywords:
Exact conditioning
Monte Carlo sampling
Multi-Gaussian
Spatial prediction
Principal component analysis
Random forest
A B S T R A C T
Regression random forest is becoming a widely-used machine learning technique for spatial prediction that shows
competitive prediction performance in various geoscience ﬁelds. Like other popular machine learning methods for
spatial prediction, regression random forest does not exactly honor the response variable’s measured values at
sampled locations. However, competitor methods such as regression-kriging perfectly ﬁt the response variable’s
observed values at sampled locations by construction. Exactly matching the response variable’s measured values
at sampled locations is often desirable in many geoscience applications. This paper presents a new approach
ensuring that regression random forest perfectly matches the response variable’s observed values at sampled
locations. The main idea consists of using the principal component analysis to create an orthogonal representation
of the ensemble of regression tree predictors resulting from the traditional regression random forest. Then, the
exact conditioning problem is reformulated as a Bayes-linear-Gauss problem on principal component scores. This
problem has an analytical solution making it easy to perform Monte Carlo sampling of new principal component
scores and then reconstruct regression tree predictors that perfectly match the response variable’s observed values
at sampled locations. The reconstructed regression tree predictors’ average also precisely matches the response
variable’s measured values at sampled locations by construction. The proposed method’s effectiveness is illus-
trated on the one hand using a synthetic dataset where the ground-truth is available everywhere within the study
region, and on the other hand, using a real dataset comprising southwest England’s geochemical concentration
data. It is compared with the regression-kriging and the traditional regression random forest. It appears that the
proposed method can perfectly ﬁt the response variable’s measured values at sampled locations while achieving
good out of sample predictive performance comparatively to regression-kriging and traditional regression random
forest.
1. Introduction
A common problem in geosciences is predicting over the entire study
region a physical quantity of interest measured at a few sampled loca-
tions within the study region. The spatial prediction is used for impactful
decision-making
in
many
geoscience
ﬁelds,
including
geology,
geophysics, and geochemistry. There is an increasing interest in using
regression random forest for spatial prediction in various geoscience
ﬁelds, when auxiliary information is available everywhere within the
study region. Regression random forest is a machine learning ensemble
method based on a collection of randomized regression trees, which are
combined through averaging (Breiman, 2001). Its popularity for spatial
prediction relies on its ability to efﬁciently deal with many predictor
variables, handle complex non-linear relationships and interactions, and
require less data pre-processing. Regression random forest has proven
relevant for spatial prediction in several research works, including
Fouedjio and Klump (2019), Szatm�ari and P�asztor (2019), Veronesi and
Schillaci (2019), Hengl et al. (2018), Vaysse and Lagacherie (2017),
Vermeulen and Niekerk (2017), Barzegar et al. (2017), Kirkwood et al.
(2016a), Taghizadeh-Mehrjardi et al. (2016), Ballabio et al. (2016), Khan
et al. (2016), Wilford et al. (2016), Hengl et al. (2015), Appelhans et al.
(2015), Li (2013), and Li et al. (2011).
Like other popular machine learning techniques for spatial predic-
tion, regression random forest does not perfectly match the response
variable’s observed values at sampled locations during the training stage.
This characteristic is often undesirable in many geoscience applications,
including mining and petroleum exploration, where it is of interest for
modeling ore bodies and petroleum reservoirs. Competitor methods like
regression-kriging perfectly ﬁt the response variable’s observed values at
sampled locations (Hengl et al., 2004; Chiles and Delﬁner, 2012). In this
article, a new methodology ensuring the exact conditioning of regression
random forest is presented. The proposed method achieves the exact
conditioning through a step-by-step approach. First, traditional regres-
sion random forest is performed on data as usual, and an ensemble of
E-mail address: ffouedjio@anglogoldashanti.com.
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.01.001
Received 18 November 2020; Received in revised form 7 January 2021; Accepted 9 January 2021
Available online 13 January 2021
2666-5441/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
regression tree predictors is produced. Second, the principal component
analysis (PCA) is carried out on the ensemble of regression tree pre-
dictors. Third, the exact conditioning problem is reformulated as a
Bayes-linear-Gauss problem on principal component scores. This prob-
lem has an analytical solution making it easy to carry out Monte Carlo
sampling of new principal component scores and then reconstruct
regression tree predictors that perfectly ﬁt the response variable’s
measured values at sampled locations. A synthetic dataset where the
ground-truth is available within the region under study and a real dataset
Table 1
Synthetic data example - simulation parameters.
Mean
Covariance function
Type
Scale
Sill
X1ð �Þ
5
Cubic
40
5
X2ð �Þ
5
Spherical
40
5
X3ð �Þ
5
Cardinal Sine
2
5
X4ð �Þ
5
K-Bessel (shape ¼ 1)
11
5
εð �Þ
0
Exponential
6.5
175
Fig. 1. Synthetic data example - (a), (b), (c), (d) explanatory variables, and (e) response variable.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
12
Fig. 2. Synthetic data example - training dataset of n ¼ 1000 observations.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
13
including southwest England’s geochemical concentration data are used
to illustrate the proposed approach’s ability to perfectly ﬁt the response
variable’s observed values at sampled locations.
The remainder of the article is organized as follows. Section 2 de-
scribes the different ingredients required to apply the proposed
approach. Section 3 illustrates the proposed method’s effectiveness on a
synthetic dataset as well as a real dataset. A comparison with the
regression-kriging and the traditional regression random forest is carried
out. Section 4 offers concluding remarks.
2. Methodology
Let fYðsÞ : s 2 D ⊂Rdg be a real-valued response variable deﬁned on a
spatial domain of interest D⊂ℝd. Let fx1ðsÞ; …; xpðsÞ : s 2 D ⊂ℝdg to be the
set of predictor variables exhaustively known in the spatial domain D.
The following terms are also used to refer to the response variable: target
variable, dependent variable, outcome variable, explained variable. A
predictor variable is usually also referred as explanatory variable or in-
dependent variable or covariate. Let fYðs1Þ; …; YðsnÞg be the response
variable’s measured values at sampled locations fs1;…;sng⊂ D. The goal
is to use the regression random forest for predicting the response variable
over the spatial domain D represented by a number of grid locations N
such that the response variable’s predicted values are the same as the
response variable’s measured values at sampled locations, i.e., bYðsiÞ ¼
YðsiÞ; i ¼ 1; …; n. Different ingredients required to implement the pro-
posed method are described in this section. The implementation is car-
ried out in the R platform (R Core Team, 2020).
2.1. Regression random forest
Regression random forest is an ensemble learning approach that
creates many regression tree models built from bootstrap samples of the
training data (Breiman, 2001). It also injects some randomness into the
tree-growing process by randomly selecting only a subset of predictor
variables to consider for split-point selection at each node. This operation
reduces the chance of the same strong predictor variables to be selected
when a split is to be carried out, thus avoiding regression trees from
becoming overly correlated. The multiple regression tree predictors are
knitted together to reduce the prediction variance and increase predic-
tion accuracy. The method predicts the value that is the mean prediction
of all individual regression tree predictors.
Like most machine learning techniques, regression random forest has
some free parameters that can be optimized. There are among others, the
number of trees, number of predictor variables randomly selected at each
node, proportion of observations to sample in each regression tree, and
minimum number of observations in a regression tree’s terminal node.
These free parameters are optimized through cross-validation. In prac-
tice, there is no need to tune the number of decision trees; it is usually
recommend to set it to a large number, allowing the convergence of the
prediction error to a stable minimum (Hengl et al., 2018). The imple-
mentation of the regression random forest is done using the R packages
ranger (Wright and Ziegler, 2017) and tuneRanger (Probst et al., 2018).
Let f~YbðsÞ : s 2 Dgb¼1;…;B be the ensemble of regression tree pre-
dictors resulting from the training of the traditional regression random
forest. At this stage, individual regression tree predictors and their
average do not necessarily match the response variable’s measured
Fig. 3. Synthetic data example - B ¼ 10000 unconditional ﬁrst PC scores and T ¼ 1000 conditional ﬁrst PC scores.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
14
values at sampled locations. The next steps consist of using this ensemble
of regression tree predictors f~YbðsÞ : s 2 Dgb¼1;…;B to reconstruct indi-
vidual regression tree predictors that perfectly match the response vari-
able’s observed values at sampled locations. By construction, their
average also exactly ﬁts the response variable’s observed values at
sampled locations.
2.2. Principal component analysis
After training of the traditional regression random forest model, the
next step consists of performing principal component analysis on the
ensemble of regression tree predictors f~YbðsÞ : s 2 Dgb¼1;…;B arranged as
a matrix of size ðB �NÞ whose each row vector represents a single
regression tree predictor f~YbðsÞ : s 2 Dg. This results in the following
decomposition in ﬁnite dimensions:
~YbðsÞ ¼
X
L
l¼1
αb;lψlðsÞ; 8s 2 D; b ¼ 1; …; B;
(1)
where fαb;lgl¼1;…;L are principal component scores (or coefﬁcients) and
fψlðsÞ : s 2 Dgl¼1;…;L are principal components factors (or eigenfunc-
tions); L ¼ minðB;NÞ.
f~YbðsÞ : s 2 Dg
can
be
interpreted
as
an
image
and
f~YbðsÞ : s 2 Dgb¼1;…;B as an ensemble of images as well. Thus, Eq. (1)
provides a decomposition of an ensemble of images into a set of eigen-
images fψlðsÞ : s 2 Dgl¼1;…;L and a set of coefﬁcients fαb;lgl¼1;…;L. It is
essential to highlight that the eigenfunctions are considered ﬁxed in the
PCA, while the coefﬁcients are considered random. It is also important to
emphasize here that the PCA is used more as an orthogonal decomposi-
tion method than a dimension reduction technique since all the eigen-
functions are kept, as shown in Eq. (1). The bijective nature of PCA allows
the reconstruction of regression tree predictors from coefﬁcients. In other
words, an image can be reconstructed back once all the principal
component factors and scores are used.
2.3. Bayes-linear-Gauss problem
Given the PCA decomposition of the ensemble of regression tree
predictors as shown the previous section, the next step consists of ﬁnding
new principal component scores such that the reconstructed regression
tree predictors perfectly match the response variable’s observed values at
sampled locations. Let
Y
^ðsÞ ¼
X
L
l¼1
βlψlðsÞ; 8s 2 D
(2)
where fβlgl¼1;…;L are random coefﬁcients; fψlðsÞ : s 2 Dgl¼1;…;L are
eigenfunctions deﬁned in Eq. (1). It is important to note that all the
eigenfunctions are considered; so there is no truncation.
Fig. 4. Synthetic data example - response’s observed values vs response’s predicted values in the training dataset, for (a) regression-kriging, (b) traditional regression
random forest, and (c) proposed regression random forest.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
15
The objective here is to sample the vector of coefﬁcients β ¼
ðβ1; …; βLÞ
0 such that Y
^ðsÞ ¼ PL
l¼1βlψlðsÞ perfectly ﬁts the response var-
iable’s observed values at sampled locations, i.e., Y
^ðsiÞ ¼ YðsiÞ; i ¼ 1;…;
n. This exact conditioning can be rewritten as follows:
Y ¼ Fβ
(3)
where Y ¼ ðYðs1Þ; …; YðsnÞÞ
0 is the response variable’s measured values
at sampled locations and F ¼ ½ψlðsiÞ� is a ﬁxed matrix of size ðn �LÞ whose
elements are eigenfunctions at sampled locations.
To explore in a stochastic and convenient way the solution space
associated with Eq. (3), the vector of coefﬁcients β ¼ ðβ1; …; βLÞ
0 is
assumed to follow a multivariate Gaussian distribution deﬁned by:
β e f ðβÞ ¼ ð2πÞ�L=2
����Σj�1=2exp
�
� 1
2ðβ � μÞ
0Σ�1ðβ � μÞ
�
(4)
where the mean μ ¼ E½β� and the covariance matrix Σ ¼ Var½β� are
computed using PC scores fαb;lg derived from the PCA of regression tree
predictors given in Eq. (1). Speciﬁcally,
μ ¼
"
1
B
X
B
b¼1
αb;l
#
l¼1;…;L
;
Σ ¼
1
B � 1
X
B
b¼1
ðαb � μÞðαb � μÞ
0;
with αb ¼ ½αb;l�l¼1;…;L:
(5)
Equations (3) and (4) deﬁne a Bayes-linear-Gauss problem (Scheidt
et al., 2018; Tarantola, 2013). This problem has a solution subject that
L ¼ minðB;NÞ > n. In practice, the number of grid locations is larger than
the number of sampled locations (N ≫ n). So there is a solution when
B > n. It is important to highlight that given the number of sampled lo-
cations n, one can always have B > n since B is free parameter in the
regression random forest. Let fðβjYÞ be the probability density distribu-
tion of βjY. According to Bayes rule, fðβjYÞ is also multivariate Gaussian
with mean and covariance given by (Scheidt et al., 2018; Tarantola,
2013):
E½βjY� ¼ μ þ ΣF
0ðFΣF
0Þ�1ðY � FμÞ
(6)
Var½βjY� ¼ Σ � ΣF
0ðFΣF
0Þ�1FΣ
(7)
Since fðβjYÞ is a multivariate Gaussian distribution, it easy to draw
Monte Carlo samples from this distribution. The generation of the sam-
ples fβc
tgt¼1;…;TefðβjYÞ is carried out using the R package rockchalk
(Johnson, 2019). Given the Monte Carlo samples fβc
t gt¼1;…;T, the recon-
structed regression tree predictors are given by:
Fig. 5. Synthetic data example - spatial prediction map for (a) regression-kriging, (b) traditional regression random forest, (c) proposed regression random forest, and
(d) ground-truth.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
16
Y⌣
t ðsÞ ¼
X
L
l¼1
βc
t;lψlðsÞ; 8s 2 D;
with βc
t ¼
�
βc
t;1; …; βc
t;L
�0
; t ¼ 1; …; T:
(8)
The prediction at an unsampled location s0 2 D is made by averaging
the predictions from all the individual reconstructed regression tree
predictors:
bYðs0Þ ¼ 1
T
X
T
t¼1
Y
^
tðs0Þ
(9)
Since all the individual reconstructed regression tree predictors
fY
^
tðsÞ : s 2 Dgt¼1;…;T perfectly match the response variable’s observed
values at sampled locations, their mean fbYðsÞ : s 2 Dg does also. PC
scores fαbgb¼1;…;B in Eq. (1) are called unconditional PC scores while
those fβc
t gt¼1;…;T in Eq. (8) are called conditional PC scores. It is important
to highlight that there no constraint on T relatively to B; T can be less or
greater than B.
3. Practical examples
The proposed method’s ability to perfectly match the response vari-
able’s observed values at sampled locations is illustrated using both
synthetic and real datasets. Prediction performance comparison is carried
out with the regression-kriging and the traditional regression random
forest using some well-known prediction accuracy criteria (Hengl et al.,
2018): the mean absolute error (MAE), the root mean square error
(RMSE), the coefﬁcient of determination (R-square), and Lin’s concor-
dance correlation coefﬁcient (CCC). The lower are MAE and RMSE, the
better is the model. The closer are R-square and CCC to 1, the better is the
model.
Fig. 6. Synthetic data example - response variable’s observed values vs response variable’s predicted values in testing dataset, for (a) regression-kriging, (b) traditional
regression random forest, and (c) proposed regression random forest.
Table 2
Synthetic data example - predictive performance statistics in the testing dataset.
Criteria
Regression-Kriging
Traditional Random Forest
Proposed Random Forest
MAE
12.69
12.01
11.05
RMSE
16.86
15.66
14.40
R-square
0.88
0.90
0.91
CCC
0.93
0.94
0.95
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
17
Fig. 7. Real data example - some predictor variables: (a) elevation, (b) landsat 8 band 6, (c) gravity survey high-pass ﬁltered Bouguer anomaly, (d) potassium counts
from gamma ray spectrometry.
Fig. 8. Real data example - (a) response variable (Ga concentration), and (b) training and testing locations.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
18
Fig. 9. Real data example - B ¼ 10000 unconditional ﬁrst PC scores and T ¼ 1000 conditional ﬁrst PC scores.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
19
3.1. Synthetic data example
The data-generating process is given by the following underlying
model:
YðsÞ ¼ X1ðsÞ2 þ 40tanhðX1ðsÞX2ðsÞ Þ þ X3ðsÞ2 þ 50sinðX4ðsÞ Þ þ εðsÞ;
8s 2 ½0; 100�2
(10)
where Yð �Þ is the response variable; X1ð �Þ, X2ð �Þ, X3ð �Þ, and X4ð �Þ are
predictor variables; and εð �Þ is a latent (non-observed) variable.
Predictor variables are simulated on the spatial domain ½0; 100�2 as
Gaussian isotropic stationary random ﬁelds with mean and covariance
function given in Table 1. For background on Gaussian random ﬁelds, see
Chiles and Delﬁner (2012). The simulation is performing using the R
package RGeostats package (Renard et al., 2020). This simulated data
example for which the ground-truth is available everywhere within the
study domain refers to a situation where there is a non-linear relationship
between the response variable and predictor variables with some in-
teractions between predictor variables. Also, the response variable shows
some spatial auto-correlation and its distribution is not Gaussian.
Fig. 1 presents the synthetic data over a 100 � 100 regular grid. n ¼
1000 observations are sampled randomly and taken as the training data
as shown in Fig. (2). The rest of data (9000 observations) is kept aside for
the testing. The regression random forest is performed on the training
data with a large number of regression trees set to B ¼ 10000. Thus, an
ensemble
of
B ¼ 10000
regression
tree
predictors
f~YbðsÞ : s 2 ½0; 100�2gb¼1;…;10000 is constructed. According to the meth-
odology described in Sect. 2, principal component analysis is performed
on this ensemble, followed by the Monte Carlo sampling of new PC scores
ensuring the exact conditioning. T ¼ 1000 new PC scores are generated,
thus giving an ensemble of T ¼ 1000 reconstructed (new) regression tree
predictors
n
Y⌣
t ðsÞ : s 2 ½0; 100�2 o
t¼1;…;1000 that perfectly match the
response variable’s observed values at sampled locations. The mean
prediction of reconstructed (new) regression tree predictors also exactly
matches the response variable’s observed values at sampled locations.
PC
scores
before
the
conditioning
(unconditional
PC
scores
fαbgb¼1;…;B)
and
after
the
conditioning
(conditional
PC
scores
fβc
t gt¼1;…;T) are presented in Fig. 3. In this ﬁgure, the points cloud of
conditional PC scores is less scattered than those from unconditional PC
scores due effectively to the conditioning. Indeed, unconditional PC
scores have no constraints on their possible values. They can be any point
in the whole Euclidean space ℝL. However, the set of linear constraints
deﬁned by Eq. (3) produces a convex feasible region of possible values for
conditional PC scores. So, conditional PC scores can not be any point in
the Euclidean space ℝL.
Fig. 4 shows the response variable’s observed values versus predicted
values in the training data, under the regression-kriging, the traditional
regression random forest, and the proposed one. One can effectively
notice that the traditional regression random forest does not ﬁt the
training data perfectly while the regression-kriging and the proposed
regression random forest do. Fig. 5 presents spatial prediction maps
Fig. 10. Real data example - response’s observed values vs response’s predicted values in the training dataset, for (a) regression-kigring, (b) traditional regression
random forest, and (c) proposed regression random forest.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
20
provided by the regression-kriging, the traditional regression random
forest, and the proposed one. The spatial prediction map resulting from
the regression-kriging differs from the ones provided by the traditional
regression random forest and the proposed regression random forest. In
particular, the spatial prediction map of regression-kriging is smoother
than the two others.
The overall look of spatial prediction maps resulting from the tradi-
tional regression random forest and the proposed regression random
forest looks very similar. However, there are some local differences due
to the exact conditioning of the proposed regression random forest. The
predictive performance in the testing set for the regression-kriging, the
traditional regression random forest, and the proposed one is shown in
Fig. 6 and Table 2. One can notice that the proposed regression random
forest performs better than the regression-kriging and the traditional
regression random forest.
3.2. Real data example
The real dataset of interest comprises geochemical concentration data
of the southwest England (Kirkwood et al., 2016b). We are interested in
the target variable Ga (Gallium) concentration which is measured at 568
locations over the spatial domain of interest. Predictor variables include
elevation, gravity, magnetic, Landsat, radiometric, and their derivatives,
totaling 26 predictor variables. Some predictor variables are shown in
Fig. 7. Fig. 8a presents measurements of the response variable. The data
are divided into a training set (� 80%) and testing set (� 20%) as shown
in Fig. 8b.
The estimated regression random forest model is an ensemble of B ¼
10000 regression tree predictors. PCA applied to this ensemble,
following by the Monte Carlo sampling result to unconditional and
conditional PC scores as shown in Fig. 9; T ¼ 1000 conditional PC scores
are generated. Fig. 10 shows the conditioning performance in the
training data of the regression-kriging, the traditional regression random
forest, and the proposed regression random forest. As noticed in the
simulated data example, the regression-kriging and proposed regression
random forest perfectly ﬁt the data while the traditional regression
random forest does not.
Spatial prediction maps provided by the regression-kriging, the
traditional regression random forest, and the proposed one are depicted
in Fig. 11. The spatial prediction map of the regression-kriging differs
from the two others. The general appearance of the spatial prediction
maps of these later looks very similar. However, one notes some local
differences due to the exact conditioning of the proposed regression
random forest. Fig. 12 and Table 3 present the predictive performance of
the regression-kriging, the traditional regression random forest, and the
proposed regression random forest on the testing data. The proposed
regression random forest shows better predictive performance than the
two other methods. Thus, the proposed approach can exactly match the
response
variable’s
measured
values
at
sampled
locations
while
achieving good out of sample predictive performance.
Fig. 11. Real data example - spatial prediction map for (a) regresssion-kriging, (b) traditional regression random forest, and (c) proposed regression random forest.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
21
4. Concluding remarks
This article presented a methodology guaranteeing that regression
random forest perfectly matches the response variable’s observed values
at sampled locations like competitor techniques such as regression-
kriging. Given the ensemble of regression tree predictors resulting from
the traditional regression random forest, the exact conditioning is ach-
ieved by combining principal component analysis and Monte Carlo
sampling. As a result, a new ensemble of regression tree predictors that
perfectly ﬁt the response variable’s observed values at sampled locations
is obtained. The average of this new ensemble of regression tree pre-
dictors also exactly matches the response variable’s observed values at
sampled locations by construction. The effectiveness of the proposed
approach has been demonstrated on both synthetic and real datasets. It
can perfectly ﬁt the response variable’s measured values at sampled lo-
cations while achieving good out of sample performance comparatively
to regression-kriging and traditional regression random forest. It is easy
to implement since it combines well-known existing machine learning
and Monte Carlo sampling methods.
As highlighted previously, the exact conditioning is performed sub-
ject that the number of regression trees is greater than the number of
sampled locations. Nonetheless, it will always be possible to meet this
constraint because the number of regression trees is a free parameter. The
number of regression trees should also be large enough to allow good
coverage of the solution space when performing the exact conditioning.
The proposed approach relies on the regression random forest to perform
the exact conditioning. Thus, one expects the proposed method to pro-
vide better predictive performance in situations favorable to regression
random forest like a non-linear relationship between the response vari-
able and predictor variables and some interactions between predictor
variables. In a situation where there is linear relationship between the
dependent variable and predictor variables, competitor techniques like
regression-kriging could perform better. The proposed method can be
used in any spatial dimension (e.g., 2D and 3D). It can be extended in the
case where the response variable is categorical. The exact conditioning
can be achieved following the idea developed in Fouedjio et al. (2020).
Fig. 12. Real data example - response variable’s observed values vs response variable’s predicted values in testing dataset, for (a) regression-kriging, (b) traditional
regression random forest, and (c) proposed regression random forest.
Table 3
Real data example - predictive performance statistics on the testing dataset.
Criteria
Regression-Kriging
Traditional Random Forest
Proposed Random Forest
MAE
2.87
2.74
2.63
RMSE
3.79
3.59
3.46
R-square
0.57
0.61
0.64
CCC
0.74
0.75
0.78
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
22
Declaration of competing interest
There is no conﬂict of interest.
Acknowledgments
The authors is grateful to the anonymous reviewers and the editor for
their helpful and constructive comments that greatly helped improve the
manuscript.
References
Appelhans, T., Mwangomo, E., Hardy, D.R., Hemp, A., Nauss, T., 2015. Evaluating
machine learning approaches for the interpolation of monthly air temperature at mt.
Kilimanjaro, Tanzania. Spatial Statistics 14, 91–113.
Ballabio, C., Panagos, P., Monatanarella, L., 2016. Mapping topsoil physical properties at
European scale using the Lucas database. Geoderma 261, 110–123.
Barzegar, R., Asghari Moghaddam, A., Adamowski, J., Fijani, E., 2017. Comparison of
machine learning models for predicting ﬂuoride contamination in groundwater.
Stochastic Environmental Research and Risk Assessment 31 (10), 2705–2718.
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.
Chiles, J.P., Delﬁner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley &
Sons.
Fouedjio, F., Klump, J., 2019. Exploring prediction uncertainty of spatial data in
geostatistical and machine learning approaches. Environmental Earth Sciences 78
(1), 38.
Fouedjio, F., Scheidt, C., Yang, L., Jef, C., 2020. Conditional simulation of categorical
spatial variables using Gibbs sampling of a truncated multivariate normal distribution
subject to linear inequality constraints. Stoch. Environ. Res. Risk Assess. https://
doi.org/10.1007/s00477-020-01925-7.
Hengl, T., Heuvelink, G.B., Stein, A., 2004. A generic framework for spatial prediction of
soil variables based on regression-kriging. Geoderma 120, 75–93.
Hengl, T., Heuvelink, G.B.M., Kempen, B., Leenaars, J.G.B., Walsh, M.G., Shepherd, K.D.,
Sila, A., MacMillan, R.A., Mendes de Jesus, J., Tamene, L., Tondoh, J.E., 2015.
Mapping soil properties of Africa at 250 m resolution: random forests signiﬁcantly
improve current predictions. PloS One 10, 1–26.
Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr€aler, B., 2018. Random forest as a
generic framework for predictive modeling of spatial and spatio-temporal variables.
PeerJ 6:e5518. https://doi.org/10.7717/peerj.5518.
Johnson, P.E., 2019. Regression estimation and presentation. URL. rockchalk. htt
ps://CRAN.R-project.org/package¼rockchalk. r package version 1.8.144.
Khan, S.Z., Suman, S., Pavani, M., Das, S.K., 2016. Prediction of the residual strength of
clay using functional networks. Geoscience Frontiers 7, 67–74.
Kirkwood, C., Cave, M., Beamish, D., Grebby, S., Ferreira, A., 2016a. A machine learning
approach to geochemical mapping. J. Geochem. Explor. 167, 49–61.
Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016b. Stream sediment geochemistry as
a tool for enhancing geological understanding: an overview of new data from south
west England. J. Geochem. Explor. 163, 28–40.
Li, J., 2013. Predictive Modelling Using Random Forest and its Hybrid Methods with
Geostatistical Techniques in Marine Environmental Geosciences, in: 11-th
Australasian Data Mining Conference (AusDM1́3). Australia, Canberra, pp. 73–79.
Li, J., Heap, A.D., Potter, A., Daniell, J.J., 2011. Application of machine learning methods
to spatial interpolation of environmental variables. Environ. Model. Software 26,
1647–1659.
Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and Tuning Strategies for
Random Forest. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery doi:10.1002/widm.1301.
R Core Team, 2020. R: a language and environment for statistical computing. R Foundation
for Statistical Computing. Vienna, Austria. https://www.R-project.org/. (Accessed 15
November 2020).
Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. Geostatistical
package. URL. RGeostats. http://cg.ensmp.fr/rgeostats. r package version 12.0.1.
Scheidt, C., Li, L., Caers, J., 2018. Quantifying Uncertainty in Subsurface Systems.
Geophysical Monograph Series. Wiley.
Szatm�ari, G., P�asztor, L., 2019. Comparison of various uncertainty modelling approaches
based on geostatistics and machine learning algorithms. Geoderma 337, 1329–1340.
Taghizadeh-Mehrjardi, R., Nabiollahi, K., Kerry, R., 2016. Digital mapping of soil organic
carbon at multiple depths using different data mining techniques in Baneh region,
Iran. Geoderma 266, 98–110.
Tarantola, A., 2013. Inverse Problem Theory: Methods for Data Fitting and Model
Parameter Estimation. Elsevier Science.
Vaysse, K., Lagacherie, P., 2017. Using quantile regression forest to estimate uncertainty
of digital soil mapping products. Geoderma 291, 55–64.
Vermeulen, D., Niekerk, A.V., 2017. Machine learning performance for predicting soil
salinity using different combinations of geomorphometric covariates. Geoderma 299,
1–12.
Veronesi, F., Schillaci, C., 2019. Comparison between geostatistical and machine learning
models as predictors of topsoil organic carbon with a focus on local uncertainty
estimation. Ecol. Indicat. 101, 1032–1044.
Wilford, J., de Caritat, P., Bui, E., 2016. Predictive geochemical mapping using
environmental correlation. Appl. Geochem. 66, 275–288.
Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for high
dimensional data in Cþþ and R. J. Stat. Software 77, 1–17.
F. Fouedjio
Artiﬁcial Intelligence in Geosciences 1 (2020) 11–23
23
