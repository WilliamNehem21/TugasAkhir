Artificial Intelligence in the Life Sciences 1 (2021) 100014
Contents lists available at ScienceDirect
Artificial Intelligence in the Life Sciences
journal homepage: www.elsevier.com/locate/ailsci
Research Article
Maxsmi: Maximizing molecular property prediction performance with
confidence estimation using SMILES augmentation and deep learning
Talia B. Kimber a ,‚àó , Maxime Gagnebin b, Andrea Volkamer a ,‚àó
a In silico Toxicology and Structural Bioinformatics, Institute of Physiology, Charit√©-Universit√§tsmedizin Berlin, Charit√©platz 1, 10117, Berlin, Germany
b Berlin, Germany
a r t i c l e i n f o a b s t r a c t
Keywords: Accurate molecular property or activity prediction is one of the main goals in computer-aided drug design. Quan-
Deep learning titative structure-activity relationship (QSAR) modeling and machine learning, more recently deep learning, have
Molecular property prediction become an integral part of this process. Such algorithms require lots of data for training which, in the case of
Data augmentation
physico-chemical and bioactivity data sets, remains scarce. To address the lack of data, augmentation techniques
Open-source
are increasingly applied in deep learning. Here, we exploit that one compound can be represented by various
Confidence assessment
SMILES
SMILES strings as means of data augmentation and we explore several augmentation techniques. Convolutional
and recurrent neural networks are trained on four data sets, including experimental solubility, lipophilicity, and
bioactivity measurements. Moreover, the uncertainty of the models is assessed by applying augmentation on the
test set. Our results show that data augmentation improves the accuracy independently of the deep learning
model and of the size of the data. The best strategies lead to the Maxsmi models, the models that max imize the
performance in SMI LES augmentation. Our findings show that the standard deviation of the per SMILES pre-
diction correlates with the accuracy of the associated compound prediction. In addition, our systematic testing
of different augmentation strategies provides an extensive guideline to SMILES augmentation. A prediction tool
using the Maxsmi models for novel compounds on the aforementioned physico-chemical and bioactivity tasks is
made available at https://github.com/volkamerlab/maxsmi .
1. Introduction 1. The gain of computational power through graphics processing
units (GPUs) and tensor processing units (TPUs). Platforms such
Drug design is a time-consuming and costly process [1,2] with high as Google Colaboratory [11] allow any user to exploit high perfor-
attrition rates [3] . It can be supported with in silico methods by guiding mance computing resources without any cost and such free and easy
the design process, optimizing compounds, and discarding those with access is unprecedented.
undesired properties at an early stage of development. In this context, 2. The ever growing amount of available data. More data are created
computer-aided drug design (CADD) has become central in the drug and stored in databases every day in various fields. Many processes
discovery pipeline and is widely adopted in research and development are automatized making data more accessible and usable either in-
in both academia and pharmaceutical companies. ternally, as in pharmaceutical companies, or publicly. For example
Over the last few decades, there has been a keen interest in ma- in academic research, in competitions, such as Kaggle [12] , or in
chine learning (ML) and more specifically deep learning (DL), which challenges, such as D3R ‚Äì the Drug Design Data Resource chal-
have been applied to a variety of areas, covering computer vision [4] , lenge [13] or the Tox21 challenge [14] .
speech recognition [5] , as well as the life sciences. Only to name a few, 3. The advances in algorithms, making models perform better than ever
AlphaFold 2 from DeepMind which predicts protein folding [6] , Poten- before. Deep learning algorithms may surpass human performance
tialNet which focuses on protein-ligand binding affinity [7] , de novo if trained on a data set containing over 10 million data points, as
molecular design suitable for compound optimization [8] , and cytotox- suggested by Goodfellow et al. [10] .
icity prediction as in the work by Webel et al. [9] . Such excitement in
DL may be explained by the main three following factors [10] . With the rise of ML/DL research, many applications have been ex-
tended to the field of molecular property and affinity prediction, even
though insufficient data remains a challenge in the field.
‚àó Corresponding author.
E-mail addresses: talia.kimber@gmail.com (T.B. Kimber), andrea.volkamer@charite.de (A. Volkamer).
https://doi.org/10.1016/j.ailsci.2021.100014
Received 4 October 2021; Received in revised form 13 November 2021; Accepted 15 November 2021
Available online 18 November 2021
2667-3185/¬©2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license
( http://creativecommons.org/licenses/by-nc-nd/4.0/ )T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Encoding molecular compounds in both human- and computer- tion of physico-chemical properties for novel molecules and assessing
readable formats is a necessary step in CADD. A convenient encoding the uncertainty of the prediction. To this end, all code is made freely
is SMILES, or simplified molecular-input line-entry system [15] . As the available at https://github.com/volkamerlab/maxsmi .
name suggests, SMILES is a linear notation of a molecule based on atom
and bond enumeration, as well as branch, ring closure, and disconnec- 2. Methods
tion specification. Several advantages arise from this compact represen-
tation. In this section, we first describe different augmentation strategies
which can be used for data augmentation when dealing with SMILES.
1. The printable characters make SMILES easily readable by computers
Second, we illustrate how SMILES augmentation can be viewed as an
and decipherable by humans.
ensemble learning technique when it comes to prediction. We then ex-
2. Being a single line, SMILES resemble words and are therefore cheap
amine the deep learning models that are trained in this study.
to store.
3. Such a notation is very popular and many open-source databases
store compounds in SMILES. 2.1. Augmentation strategies
However, there is a trade-offbetween readability and specification:
As discussed in the Introduction, one compound can have several
having a compact encoding means losing detailed information about the
valid SMILES, since both the starting atom and the path along the molec-
molecule such as 2D or 3D features. Moreover, subtle chemistry rules
ular graph used to generate the SMILES can differ. Here, the way a user
such as aromaticity do not have a standard way of being handled [16] .
can explore such random SMILES is detailed and five strategies to aug-
The implementation of a SMILES given a compound can be described
ment a single SMILES to multiple SMILES are described: no augmen-
as follows: from any starting atom in the molecule, enumerate the atoms
tation, augmentation with duplication, augmentation without duplica-
and bonds following a path in the molecular graph. Two aspects of this
tion, augmentation with reduced duplication, and augmentation with
construction lead to the non-uniqueness of SMILES: 1. the atom to start
estimated maximum. For the following sections, we assume that we are
the enumeration from, and 2. the path to follow along the graph. given a data set ùê∑, containing ùëÅpairs of {compound, label}. Label refers
Therefore, one molecule can have many different valid SMILES, sim-
to the measured property, such as lipophilicity or solubility. The imple-
ply by starting the enumeration from a different atom or by choosing
mentation of these strategies is based on the open-source cheminformat-
a different path. Nevertheless, in some settings, having a bijection be-
ics software RDKit [29] .
tween a molecule and its SMILES notation may be sought. For exam-
ple, when determining the overlapping molecules from two data sets.
2.1.1. No augmentation
In this context, most cheminformatics tools have their own algorithm
The level zero to augmentation is having no augmentation or, in
implemented allowing them to always retrieve the same SMILES given
other terms, augmentation of zero. This means that given a data set ùê∑
a molecular graph, such a SMILES is called canonical [17] .
with ùëÅ compounds, the ‚Äúno augmentation ‚Äùversion of ùê∑also contains
As mentioned previously, deep learning being data greedy and both
ùëÅSMILES. More specifically, in this setting, the SMILES associated with
physico-chemical and bioactivity databases being meager, elaborate
each compound is the canonical SMILES.
techniques have to be integrated to unleash the full potential of deep
neural networks. In this context, data augmentation in general [18,19] ,
2.1.2. Augmentation with duplication
and more specifically SMILES augmentation [20‚Äì23] , is a powerful as-
Generating random SMILES implies picking at random an initial
sistance in molecular prediction. From a machine learning perspective,
atom and following a random path along the molecular graph. Aug-
data augmentation allows the model to see the same object through
menting the data set ùê∑by ùëö means that for each of the N compounds in
different angles and has been successfully applied in image classifica-
ùê∑, ùëö instances of random SMILES are drawn and the associated labels
tion [4,24] , where images undergo transformations such as flipping,
are matched for each compound. In this case, augmenting ùê∑by ùëö would
coloring, cropping, rotating, and translating. From a computational per-
result in the augmented data set containing ùëÅ √óùëö data points. In this
spective, SMILES augmentation is advantageous because generating ran-
scenario, all molecules in ùê∑are multiplied by the same factor ùëö . Conse-
dom SMILES is fast and memory efficient, and even though training
quently, smaller molecules, with fewer SMILES variations, will contain
a model may be more computationally expensive, it remains cheap to
more duplicates whereas larger molecules are more likely to cover a di-
evaluate.
verse set of random SMILES. A disadvantage of such an augmentation
The first occurrence of SMILES augmentation in QSAR modeling
strategy is that SMILES corresponding to small molecules will be over-
was developed by Bjerrum [20] , where affinity against dihydrofolate
represented in the data set and could create a bias in model training.
reductase (DHFR) is predicted on a small data set of 756 compounds.
The model consists of long short-term memory (LSTM) layers and a
fully connected layer for the normalized log ùêºùê∂
50
value. Each molecule 2.1.3. Augmentation without duplication
in the data set is augmented on average 130 times. The model with Removing duplicated entries is common in data wrangling [30] . In
SMILES augmentation reaches a test correlation coefficient of 0.68, a the context of SMILES augmentation, this translates to discarding du-
0.12 increase with respect to the canonical model. From then on, sev- plicates after having generated a number of random SMILES. For data
eral studies have built on the same idea, applying SMILES augmenta- set ùê∑, the final number of data points after augmentation varies accord-
tion in QSAR modeling [21] . Moreover, convolutional neural networks ing to the augmentation number, i.e. the number of times a sample is
have successfully been applied in the context of SMILES augmentation, drawn from the valid SMILES space, and the size of the molecules in the
outperforming models using traditional molecular descriptors [22,23] . data set. A disadvantage of such an augmentation strategy is that small
Such augmentation techniques have also emerged in related fields, such molecules, which presumably possess fewer unique SMILES representa-
as retrosynthesis [25,26] and generative modeling [27,28] . While all tives, will be under-represented in the data set and could create a bias
these studies show the benefit of augmenting the data, none of them in model training.
focus, to the best of our knowledge, on a systematic analysis on how
to augment the data set best, and most decide a priori on an aug- 2.1.4. Augmentation with reduced duplication
mentation number. This study aims at filling this gap by offering a In order to find a compromise between keeping or removing all du-
systematic augmentation approach, both in the augmentation strate- plicates, the notion of augmentation with reduced duplication is intro-
gies and by how much the data should be augmented. Moreover, a duced. In this setting, only a fraction of the number of duplicates is kept.
command-line interface is available for users interested in the predic- Mathematically speaking, if the data set ùê∑is augmented by ùëö , then a
2T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
function ùëì( ùëö ) which grows slower than linear is used to control the num- 2.3. Deep learning models
ber of replicas kept for each ‚àöSMILES. Sensible functions would be the
squared root function ùëì( ùëö ) = ùëö or the natural logarithm ùëì( ùëö ) = ln ( ùëö ), Neural networks are powerful algorithms that allow accurate predic-
the former being used for the experiments in this study. tions on various tasks. In the case of QSAR/ML/DL modeling and more
A corner case of the former three augmentation strategies by aug- specifically the use of SMILES representation, two types of models can
mentation number ùëö is when ùëö = 1 . In this instance, a random SMILES be applied, convolutional [ 10 , Chapter 9] and recurrent [ 10 , Chapter
will be generated and the number of data points would still be ùëÅ, as in 10] neural networks.
the ‚Äúno augmentation ‚Äùcase, with the difference that ‚Äúno augmentation ‚Äù In this study, comparing deep learning models and how they perform
contains canonical SMILES only. with respect to data augmentation is one of the key focuses. To this end,
three types of models are architectured and trained, namely 1D and 2D
convolutional neural networks (CONV1D, CONV2D) as well as a recur-
2.1.5. Augmentation with estimated maximum rent neural network (RNN). The architecture of the recurrent network
The final strategy described is augmentation with estimated max- consists of an LSTM layer, followed by two fully connected layers of 128
imum, which aims to cover a wide range of the valid SMILES space and 64 units, respectively. It was inspired by Bjerrum [20] , in which an
for a given compound, or in other words, to generate a number of LSTM layer is followed by a single 64 unit fully connected layer. Using a
unique SMILES that depends on the compound. In our study, the im- similar approach, a single 1D convolutional layer of kernel size 10 and
plementation of this augmentation strategy randomly samples SMILES stride 1 is applied in the CONV1D model. Two fully connected layers
corresponding to a compound, and the sampling process is stopped follow the convolution. The CONV2D adheres to the same pattern but
once the same SMILES string has been generated a pre-defined num- instead of using a 1D convolution, a 2D convolution operation is per-
ber of times. The experiments of this study set 10 generations of formed using one single channel. Finally, all three architectured models
the same SMILES as a stopping criterion. It is noteworthy that the stay consistent in the depth of the network and remain shallow.
number of SMILES this method generates is highly dependent on the In this study, all deep learning models are trained for 250 epochs,
size of the compound, unlike the previous methods which always using mini-batches of size 16, where the mean squared error is the con-
generate a number of SMILES bounded by ùëö . For example, our im- sidered loss. Optimization is done with stochastic gradient descent and
plementation of this augmentation strategy generated 50 659 unique a learning rate of 0.001. Note that a fixed number of epochs is used in
SMILES variations for the compound given by the canonical SMILES this study, but for sake of completeness, three sample models with early
CC( = O)C1(C)CCC2C3C = C(C)C4 = CC( = O)CCC4(C)C3CCC21C, whereas stopping were also run. The results with and without early stopping did
only three were generated for the canonical SMILES C = CC = C, namely not change significantly (data not shown). Moreover, some models were
C( = C)C = C, C(C = C) = C, and C = CC = C. trained by adapting the number of epochs with respect to the augmenta-
tion number, but this only proved to overfit the training set and yielded
the same results on the test set as training with 250 epochs (data not
2.2. SMILES augmentation as ensemble learning for compound prediction shown).
and confidence measure
3. Data and experimental setup
The application of data augmentation strategies during training has
proven to be successful, as shown in previous works [4,24] . In QSAR
This section introduces the data sets used in this study, namely their
modeling particularly, SMILES augmentation is not only beneficial on
provenance as well as the required preprocessing. Furthermore, a step by
the training set [22] , but there are also advantages of augmenting the
step instruction for efficient SMILES augmentation is described. Finally,
test set, or more generally, an unlabeled data set, as explained in this
the evaluation design and experimental setup are covered.
section.
Let us assume that a model ùëÄ with a set of parameters Œò was
trained for a certain number of epochs. Let us consider an unlabeled 3.1. Provenance
data set containing ùëÅ compounds for which we want to make pre-
dictions. Each compound ùê∂ can be augmented using random SMILES: The data in this research come from two sources: MoleculeNet [34] ,
ùëÜ 1 ( ùê∂) , ùëÜ 2 ( ùê∂) , ‚Ä¶, ùëÜ ùëò ( ùê∂) , where ùëò depends on the strategy. The model ùëÄ Œò and the ChEMBL database [35] , chosen for two main reasons.
produces a prediction for each of those SMILES, i.e. for ùëñ ‚àà{1 , ‚Ä¶, ùëò }
1. They are freely available and easily downloadable or retrievable.
ùë¶ÃÇ ùëñ ( ùê∂) = ùëÄ Œò( ùëÜ ùëñ ( ùê∂)) , 2. They are often used as benchmarks for study comparison [22,36,37] .
leading to a per SMILES prediction rather than a per compound predic- For tasks in MoleculeNet, we focus on physico-chemical predic-
tion. Using an aggregation function ùê¥ ‚à∂ ‚Ñù ùëò ‚Üí‚Ñù , such as the mean, a tion tasks and retrieve the data from the three following sets of vary-
prediction for compound ùê∂can be computed as ing sizes, all available as part of DeepChem [38] at https://deepchem.
( ) readthedocs.io/en/latest .
ùë¶ÃÇ( ùê∂) = ùê¥ ùë¶ÃÇ 1 ( ùê∂) , ‚Ä¶, ùë¶ÃÇ ùëò ( ùê∂) .
1. Measured water solubility is referred to as the ESOL data set [39] .
Such aggregation can be viewed as a consensus among the SMILES pre- The raw data contains 1 128 data points. This data set is further
diction and interpreted as ensemble learning for a given compound. processed to only include molecules with at most 25 heavy atoms
Additionally, if the standard deviations of the predictions are com- for experimental setup and is referred to as ESOL_small.
puted, they can be interpreted as a confidence in the molecular property 2. The FreeSolv [40] data set consists of 642 pairs of SMILES and
or activity prediction. If the standard deviation is large, then there is a experimental hydration free energy of small molecules in water
high variation in the per SMILES prediction, and the model is uncertain (kcal/mol).
in its per compound prediction. An illustration of such a molecular pre- 3. The lipophilicity data set originates from ChEMBL [35] and contains
diction is shown in Fig. 1 . Following the rationale by Tagasovska and 4 200 pairs of SMILES and experimental values of octanol/water
Lopez-Paz [31] , the aleatoric and epistemic uncertainties are often in- distribution coefficient.
tertwined; the uncertainty computed in our work rather falls into the
aleatoric category, a type of uncertainty linked to the model predictions Bioactivity data can be found in large quantities in ChEMBL. To
and the randomness in the input data [31‚Äì33] . date, over 18 million activities are stored in the database, covering
3T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. 1. Compound prediction and confidence measure thanks to SMILES augmentation. Given a compound represented by its canonical SMILES, a set of
random SMILES are generated. The trained machine learning model produces a prediction for each of the SMILES variations. Aggregating these values leads to a per
compound prediction and computing the standard deviation is interpreted as an uncertainty in the prediction.
Table 1
Data sets for this study. Size of the data sets before and after preprocessing, as well as the size of the training and test sets before applying an augmentation
strategy, and the provenance of the data.
Dataset Size before preprocessing Size after preprocessing Train set 80%, before augmention Test set 20%, before augmention Provenance
ESOL 1 128 1 128 902 226 MoleculeNet a
ESOL_small 1 128 1 068 854 214 MoleculeNet a
FreeSolv 642 642 513 129 MoleculeNet a
Lipophilicity 4 200 4 199 3 359 840 MoleculeNet a
Affinity (EGFR) 6 026 5 849 4 679 1 170 Kinodata b
a https://deepchem.readthedocs.io/en/latest .
b https://github.com/openkinome/kinodata .
more than 14 000 targets and two million compounds [41] . Among tar- symbols. Finally, all inputs are padded up to the length of the longest
gets, kinases are a well studied protein family due to their involve- SMILES. The reader is kindly referred to the work by Kimber et al.
ment, among others, in cancer and inflammatory diseases [42] . Kin- [47] for further details on one-hot encoding and padding.
odata, from the Openkinome organization [43] , provides an already
curated data set of human kinase bioactivities, retrieved from one of
3.3. Important steps in SMILES augmentation
the latest versions to date of ChEMBL (version 28) and is freely avail-
able at https://github.com/openkinome/kinodata . Moreover, for this
When processing SMILES for augmentation, some technical aspects
study, Kinodata is further filtered for the epidermal growth factor re-
are essential. This section assumes a training and test split, but the ra-
ceptor (EGFR) kinase [44] , since it is known to be an important drug
tionale is the same in the presence of a validation set.
target. Its UniProt identifier is given by P00533 [45] . Affinity towards
Firstly, it is important that the data are first split and then aug-
the EGFR kinase is quantified using ùëùùêºùê∂
50
values, the negative base 10
mented, rather than augmented and split. In the latter case, one com-
logarithm of ùêºùê∂
50
[46] . Information about the data set provenance and
pound could have SMILES appearing in both training and testing leading
size are detailed in Table 1 .
to most probably excellent performance, but yet statistically incorrect.
Secondly, storing values such as the length of the longest SMILES or
3.2. Data preprocessing and input featurization the dictionary of characters should be done not before but after aug-
menting the data. Indeed, augmentation may lead to the extension of
In order to train a deep learning neural network on data containing the dictionary as well as the lengthening of SMILES. For example, the
molecular compounds, the data set undergoes preprocessing and com- canonical SMILES CCCC consists of the letter C solely and contains four
pounds encoding. characters. However, one of its possible random variations is C(C)CC,
Once the data sets are retrieved from their original source, invalid which not only introduces new characters, such as the opening ‚Äú( ‚Äùand
SMILES, detected by RDKit [29] , not available (NA) values and dis- closing ‚Äú) ‚Äù of branches but is composed of six characters. Therefore,
connected compounds, marked by a dot in a SMILES, are removed. critical values such as length and dictionary should be retained after
Molecules are transformed to the canonical SMILES representation, us- augmentation.
ing RDKit functionalities. Finally, these same values should be computed on the union of the
For model training, the SMILES are one-hot encoded, based on a training and the test set for the smooth training and evaluation of the
dictionary of unique symbols constructed from the SMILES in the data. model. Indeed, if the dictionary of characters is only built on the basis
Atoms represented by two letters, such as Br for bromine or Cl for chlo- of the SMILES in the training data, there might be additional atoms, or
rine, as well as @@ for chirality specification, are treated as if single characters in the test set that the model will not recognize and will not
4T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
be able to one-hot encode. Moreover, if the length of the longest SMILES ing systems. Unit tests are done with Pytest [61] , and code coverage is
is taken from the training set and not the union of the training and test measured via Codecov [62] .
sets, augmentation on the test set could produce a longer SMILES than
the longest one in the training set, leading to dimensionality errors. 4. Results and discussion
For all the above reasons, it is important for machine learning engi-
neers to abide by the steps as described in this section for statistically This section gives a thorough analysis of the results that are obtained
correct results, as well as programmatic error-free model training and using the experimental setup described in the previous section and pro-
evaluation. vides the reader with guidelines on data augmentation applicable to new
data and exemplified with affinity measurements towards the EGFR ki-
nase. An example of the user prediction for compounds through a simple
3.4. Experimental setup and model evaluation
command-line interface is described.
In order to draw a conclusion on the efficiency of data augmentation,
three data sets of varying sizes are considered, namely ESOL, FreeSolv, 4.1. SMILES augmentation improves model performance
and lipophilicity (see the Provenance section). For each of these sets, the
data are split once into 80% training and 20% test set, with a fixed ran- As mentioned previously, deep learning models are data greedy and
dom seed for testing to be consistent with the augmentation schemes. the findings of our study reinforce this statement by a systematic anal-
Given all possible combinations between the five augmentation strate- ysis of performance differences when augmenting the input data. Feed-
gies and different augmentation numbers, the three deep learning mod- ing a neural network with different SMILES representations of the same
els, and the various data sets, including cross-validation would have compound leads to better performing models, as shown in Figs. 2, A1 ,
added considerable computational costs and has therefore not been im- and A2 . Improvements are also visible with respect to the baseline
plemented in this study. model. These observations are made on all three physico-chemical data
For model evaluation, the root mean squared error (RMSE) [48] on sets, namely ESOL, FreeSolv, and lipophilicity, independently of the data
the test set is reported, so that the lower the RMSE value, the better the set size that ranges between approximately 600 and 4 000 compounds
model. However additional information such as the measure of goodness (see Table 1 ). For example, the ESOL performance with no augmenta-
of fit, also known as the R2 value [49] , on both training and test sets, tion has an RMSE value of 0.839 for the CONV1D model, whereas the
as well as the time required for model training and evaluation are also performance of the same model with reduced augmentation and ùëö = 70
stored. achieves an RMSE as low as 0.569, see Fig. 2 . As the number of augmen-
Five augmentations strategies are studied: No augmentation, which tation increases, the RMSE values become smaller, indicated by lighter
considers the canonical SMILES representation. The augmentations shades of purple in Figs. 2, A1 , and A2 . Note that at first, as the aug-
with, without, and with reduced duplication, for numerous augmen- mentation number increases in the single digits, there is a clear increase
tation numbers: a finer grid from 1 to 20 with a step size of 1, and a in the performance of the models. For example, for the lipophilicity
coarser grid from 20 to 100 with a step size of 10. Finally, the estimated data set augmented with duplication and the CONV2D model, the sin-
maximum strategy where a SMILES representation has to be generated gle random SMILES model has an RMSE value of 1.309 and reaches
10 times for the process to stop. For this last strategy, the ESOL_small values below 1 as of an augmentation number of 4 (see Fig. A2 ). On the
data set (see Table 1 ) is used to keep the augmentation to a reasonable ESOL data set, the RNN model without duplication starts at an RMSE
time-scale. For the same reason, the same augmentation strategy is not of 1.016 and reaches values below 0.8 after only an augmentation of 5
run on the lipophilicity data set. (see Fig. 2 ). However, the performance steadily reaches a plateau. For
The augmentation strategies are applied to both the training set and example, the RMSE of the CONV1D model trained on FreeSolv is slightly
the test set, so that for example, if the FreeSolv training data set is aug- above 1 as of 20 number of augmentation and fluctuates around this
mented 20 times without duplication, then so would the FreeSolv test value thereafter, as shown in Fig. 3 . Similar observations can be made
set. for ESOL and lipophilicity. Using the same model, the RMSE on ESOL
Ensemble learning is applied on each test set and the mean is used as reaches a plateau around 0.60 at 40 augmentation steps (see Fig. A4 )
aggregation. However, a user could easily adapt it to another function, and lipophilicity around 0.60 at 60 (see Fig. A5 ). This result suggests
such as the median. The standard deviation is stored for each compound the following:
in the test set.
1. There does not seem to be one optimal value that particularly stands
Moreover, a Random Forest (RF) model [50] is used as a baseline,
out.
with all default parameters from Scikit-learn [51] . The inputs to the
2. A trade-off between performance and computation time must be
model are the Morgan fingerprints of radius 2 and length 1024. Aug-
found. As expected, the computation time increases as the number
mentation strategies as discussed above are not applicable in the context
of data points increases, as shown in Fig. A6 .
of fingerprints.
Simulations are run on a GeForce GTX 1080 Ti, provided by the cen-
4.1.1. Deep learning model performance by architecture
tral HPC cluster of the Freie Universit√§t Berlin [52] .
Not only does augmenting the data set overall help the learning for
all three considered tasks, but so is the case for all three deep learn-
3.5. Code and documentation ing architectures. This leads to the observation that augmentation im-
proves performance independently of the deep learning model, suggest-
All code is written in Python 3 [53] following PEP8 style ing that for any future QSAR study for molecular property prediction
guide [54] and is freely available at https://github.com/volkamerlab/ using SMILES and deep learning, SMILES augmentation should be the
maxsmi . Results of this study can be found at the same link. Examples method of choice. However, in this particular study and these particular
and documentation, generated via Read the Docs [55] , can be found at deep learning architectures, results point to the fact that the CONV1D
https://maxsmi.readthedocs.io/en/latest/ . model tends to outperform the RNN model, which itself seems to out-
Package management is done with Anaconda [56] . RDKit [29] is perform CONV2D. As shown in Fig. 4 , on the ESOL data using augmen-
used for cheminformatics, PyTorch [57] for deep learning, and other tation with reduced duplication, as of an augmentation number of 40,
popular packages such as Scikit-learn [51] , NumPy [58] , and Pan- the RMSE value of the CONV2D model fluctuates around 0.7, the RNN
das [59] for general purposes. Continuous integration is deployed with model around 0.65 and the CONV1D around 0.6, promoting the latter
Github actions [60] ensuring runs on Linux, Mac, and Windows operat- model to best performing model. This exhibits the power of convolutions
5T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. 2. Test RMSE using data augmentation on the ESOL data set. The table shows the root mean squared error (RMSE) on the test set for three deep learning
models and five SMILES augmentation strategies, using various augmentation numbers, as well as a baseline consisting of a Random Forest (RF) model with Morgan
fingerprint as input. The lighter the purple color, the better the model. The overall best setting is highlighted in yellow, which for the ESOL data set is augmenting
the data set 70 times using a reduced number of duplicates and training a 1D convolutional neural network (CONV1D). For interpretation of the references to color
in this figure legend, the reader is referred to the web version of this article.
6T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. 3. Performance reaches a plateau independently of the augmentation Fig. 5. Generating a large portion of the SMILES space does not necessarily
strategy. The performance of the CONV1D model trained and evaluated on lead to the best performance. Even though the RNN model is presented with
the FreeSolv data set reaches a test RMSE value slightly above 1 as of 20 aug-
SMILES variations that cover a large portion of the SMILES space using the
mentation steps and fluctuates around this value thereafter, for all augmenta- augmentations strategy with estimated maximum, on the ESOL data set, this
tion strategies: with, without, and with reduced duplication. For the ESOL and strategy does not achieve the best results.
lipophilicity data, see Figs. A4 and A5 .
Fig. 5 . Although less obvious than in the ESOL case, a similar conclu-
sion can be made on the FreeSolv data set and for example the CONV1D
model, as shown in Fig. A3 . This suggests that there might be a point of
saturation, where the neural network stops learning, even though being
fed more data.
4.1.3. Maxsmi models: Best performing model per data set
From the results of the experiments, as mentioned previously, there
does not seem to be one augmentation strategy that fully stands out, nei-
ther does a particular model. However, from a purely numerical stand-
point, there is an optimal performance value and this value is high-
lighted in yellow in Figs. 2, A1 , and A2 . For the ESOL data set, the tuple
of (model, augmentation number, augmentation strategy) that yields
best performance is the CONV1D model, an augmentation number of
70 and keeping a reduced number of duplicates. For the FreeSolv data
set, the same model but generating 70 random SMILES keeping all dupli-
Fig. 4. The 1D convolutional (CONV1D) model outperforms the recur-
rent (RNN) and 2D convolution (CONV2D) models. The figure shows the
cates is the best setting. Finally, for lipophilicity, generating 80 random
evolution of the root mean squared error (RMSE) on the test set with respect SMILES and removing duplicates leads to the best performance. Given
to the number of augmentation using reduced duplication on the ESOL data. these three best models, we select them for further analysis, henceforth
CONV1D outperforms RNN, which outperforms CONV2D. calling them Maxsmi models and summarized in Table 2 .
and their ability to extract relevant features in compounds based on one- 4.1.4. Performance comparison between canonical and random SMILES
hot encoded SMILES input. This also implies that although applying 2D One interesting observation from this study is the performance com-
convolutions to SMILES is programmatically feasible, 1D convolutions parison between training a model with canonical SMILES versus train-
are better suited than 2D convolutions, the latter having shown great ing a model using one random SMILES representation, in other terms,
success in image classification. Indeed, when considering the one-hot augmentation of 1. The canonical model systemically outperforms the
encoded matrix, SMILES are more similar to words, in which the posi- model that uses a random SMILES. More specifically, for the ESOL data
tion of the atoms is important, rather than to images. set, the canonical model reaches an RMSE value of 0.839 using CONV1D,
whereas the random version 0.964 with the same model. In the Free-
4.1.2. There is no best augmentation strategy applying to all data sets Solv and lipophilicity cases, the canonical model yields an RMSE value
From an augmentation strategy point of view, conclusions are not of 1.963 and 0.994, versus 2.577 and 1.268 for random SMILES. A pos-
straightforward. The three augmentation strategies, namely with, with- sible explanation for such an outcome is the simplicity in the canoni-
out, and with reduced duplication, all perform similarly well, without cal SMILES representation. The algorithm in RDKit produces the more
one standing out. For example, the test RMSE on the FreeSolv data set readable SMILES representation, one that avoids branches, as well as
trained using the CONV1D model reaches values just above 1 for all nested branches. Table 3 shows some of these differences. For example,
three strategies, as shown in Fig. 3 . a random version might add brackets, where the canonical version has
Moreover, generating a large portion of the SMILES space using the none (see the first row in Table 3 ), it might add sets of brackets, where
strategy with estimated maximum surprisingly does not lead to the best the canonical version keeps them to a minimum (see the second row in
results. On the ESOL data set, this strategy reaches a test RMSE of 0.683 Table 3 ) and the random version even allows nested brackets where the
using RNN, whereas the same model but using strategies with, without, canonical version avoids them (see the last row in Table 3 ).
and with reduced duplication already outperforms the estimated max- To conclude with this observation, if SMILES augmentation cannot
imum as of an augmentation number of 19 and onward, as shown in be applied for future studies for any reason, practitioners are highly
7T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Table 2
The b est augmentation strategies define the Maxsmi models. After training three data sets (ESOL, FreeSolv, and lipophilic-
ity) on various deep learning models (CONV1D, CONV2D, and RNN), using different augmentation numbers and strategies,
the setting that yields the best performance, or lowest root mean squared error (RMSE) on the test set is selected and named
the Maxsmi model.
Data Model Augmentation number Augmentation strategy Test RMSE
ESOL CONV1D 70 With reduced duplication 0.569
FreeSolv CONV1D 70 With duplication 1.032
Lipophilicity CONV1D 80 Without duplication 0.593
Table 3
Models based on the canonical SMILES outperform the ones based on a single random SMILES. The test prediction for a
model trained and evaluated on the RDKit canonical SMILES systematically performs better than the same model trained and
evaluated on a single random SMILES. ESOL is the prediction task leading to the values in the table.
Canonical SMILES Random SMILES True value Canonical SMILES prediction (&error) Random SMILES prediction (&error)
CCCCCC C(C)CCCC ‚àí3 . 84 ‚àí2 . 87 (0.97) ‚àí2 . 77 (1.07)
CCCC( = O)CC C( = O)(CCC)CC ‚àí0 . 83 ‚àí1 . 37 (0.54) ‚àí1 . 65 (0.82)
CCCC( = O)OCC C(OC(CCC) = O)C ‚àí1 . 36 ‚àí1 . 14 (0.22) ‚àí0 . 55 (0.81)
recommended to consider the canonical SMILES representation rather
than a random one.
4.2. Ensemble learning for compound prediction and confidence measure
Using the Maxsmi models established above, we look into more de-
tails at the information gained from ensemble learning for molecular
prediction, and more specifically at the average and standard deviation
computed from the per SMILES prediction. Feeding different SMILES
representations to the model and aggregating the prediction for each
SMILES variation to obtain a single prediction per compound is valu-
able not only from a practical point of view where molecular prediction
is more informative than a SMILES prediction, but it also allows the
model to merge information coming from different perspectives of the
same compound. Moreover, the standard deviation associated with the
SMILES predictions allows to quantify the uncertainty of the prediction
of the model toward a given compound. The higher the standard devia-
tion for a molecule, the less concurrent are the predictions by the model, Fig. 6. Lower errors when evaluating the Maxsmi model using ensemble
and thus, less confident. learning. There are fewer errors in the evaluation of the trained Maxsmi models
when using ensemble learning (i.e. the averaged per SMILES prediction) vs. the
canonical prediction.
4.2.1. Difference between canonical vs. averaged prediction
Considering the Maxsmi models trained with their respective aug-
mentations, we analyze the difference in prediction on the test set when relationship between high confidence and small prediction error on the
using the canonical or the averaged prediction. More specifically, we test set for the Maxsmi models. A way of visually evaluating uncertainty
compare the prediction error of the Maxsmi models when evaluated on is to plot the confidence curve [32] , which displays how the error varies
the test set twice: once using the canonical SMILES for compound pre- with the sequential removal of compounds from lowest to highest confi-
diction and a second time averaging the per SMILES prediction using dence. Fig. 7 shows the confidence curve of the Maxsmi model used on
the same augmentation number and strategy which was used for train- the FreeSolv data. As shown in the figure, as molecules with low confi-
ing. For both evaluations, the error between the prediction and the true dence are sequentially removed, the mean prediction error decreases. In
value is computed. Fig. 6 shows the histogram of these errors on the other words, the error vanishes as only compounds with the highest cer-
ESOL data. As shown in the figure, more compounds have an error close tainty predictions are kept, demonstrating a relationship between high
to zero using the ensemble learning evaluation rather than the canon- confidence and small prediction error. Fig. A7 shows the confidence
ical, which incentives the use of ensemble learning for future studies. curves of the Maxsmi models for the ESOL and lipophilicity data. The
However, this gain is marginal and the canonical prediction performs general trend of the curve is decreasing in the ESOL case. Once the 10%
similarly well compared to the averaged prediction. In light of the over- of compounds with the highest confidence are kept, the error is below
all gain in the accuracy of the models, this indicates that augmentation 0.25. However, in the lipophilicity case, although the general trend is
during training is the more crucial step. As discussed in the following also decreasing, even when keeping the 10% of compounds with the
paragraph, an advantage of using augmentation on the test set is to es- highest confidence, the error is still above 0.3.
timate the confidence of the model in its prediction.
4.3. Comparison to other studies
4.2.2. More confident model implies smaller prediction error
As mentioned in the SMILES augmentation as ensemble learning for Given the results of the Maxsmi models, their performance is com-
compound prediction and confidence measure section, computing the pared to other studies, namely MoleculeNet [34] , CNF [22] , and MolP-
standard deviation of the per SMILES prediction provides a confidence MoFiT [21] , that are trained and evaluated on the same data sets as
measure in the compound prediction. In this section, we analyze the Maxsmi, see Table 4 .
8T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Table 4
The Maxsmi models reach state-of-the-art results. Comparison of four studies on the same data sets (ESOL, FreeSolv, and
lipophilicity). The Maxsmi model outperforms most of the other models with a lower RMSE on a randomly split test set.
Study Test RMSE ( ¬± std if available) Split Model
ESOL FreeSolv Lipophilicity Fold Ratio % (train:valid:test) Type
Maxsmi 0.569 1.032 0.593 Single 80 ‚à∂ 0 ‚à∂ 20 Random CNN
MoleculeNet [34] 0 . 58 ¬± 0 . 03 1 . 15 ¬± 0 . 12 0 . 655 ¬± 0 . 036 3 80 ‚à∂ 10 ‚à∂ 10 Random GNN
CNF [22] 0.62 1.11 0.67 5-fold CV NA Random CNN
MolPMoFiT [21] NA 1 . 197 ¬± 0 . 127 0.565 ¬± 0.037 10 80 ‚à∂ 10 ‚à∂ 10 Random RNN
Abbreviations: RMSE = root mean squared error, std = standard deviation, CNN = Convolutional Neural Network, GNN =
Graph Neural Network, RNN = Recurrent Neural Network, NA = not available, CV = cross-validation.
closer to 70 as in Maxsmi, yields better results than 10 times augmen-
tation as in CNF.
Finally, the Molecular Prediction Model Fine-Tuning (MolP-
MoFiT) [21] study builds an RNN model based on LSTM layers us-
ing SMILES augmentation with duplication. The lipophilicity data is
augmented 25 times whereas the FreeSolv data 50 times. MolPMoFiT
is trained and evaluated using 10 splits of ratio 80 ‚à∂ 10 ‚à∂ 10 for the
training, validation, and test sets. The model reaches an RMSE value
(and standard deviation) of 1 . 197 ¬± 0 . 127 on the FreeSolv data and
0 . 565 ¬± 0 . 037 on the lipophilicity data, see [ 21 , Figure 3, 4]. While
Maxsmi leads on the FreeSolv prediction problem, MolPMoFiT slightly
outperforms Maxsmi on the lipophilicity data ( Table 4 ).
Lastly, study comparison should be treated with utmost attention,
since results can not be compared blindly. For instance, if the data pre-
processing is done differently in each study, or the splits are not iden-
tical, or the parameters of the experiments are not set to be the same,
then the results are not fairly comparable.
Fig. 7. More confident Maxsmi model on FreeSolv implies smaller predic-
tion error. The general trend of the confidence curve is decreasing, showing that
as compounds with high uncertainty are removed, the error becomes smaller.
4.4. Test case: EGFR affinity data following the guideline
Given the results of the Maxsmi models on the physico-chemical data
The first considered study is MoleculeNet, where several molecular sets, we now discuss guidelines to apply SMILES augmentation on a new
encodings and models are trained and evaluated, but where no augmen- data set. The EGFR affinity data, discussed in the Provenance section and
tation is used. In MoleculeNet, the data is randomly split into training, henceforth simply referred to as affinity data, is used as a test case, but
validation, and test set, using an 80 ‚à∂ 10 ‚à∂ 10 ratio and run three times the idea can be applied to different data sets and broader use cases.
on different seeds. The best performing model on the test set for both Since the affinity data set contains 5 849 data points after prepro-
ESOL and FreeSolv is a message passing neural network with an RMSE cessing (see Table 1 ), the lipophilicity and affinity data are of a similar
and standard deviation of 0 . 58 ¬± 0 . 03 and 1 . 15 ¬± 0 . 12 , respectively [ 34 , order of magnitude, although the latter is somewhat larger. Therefore, a
Table S5]. On the lipophilicity data set, a slightly different graph model compromise between the size of the data set and the tuple that gives the
performs best with 0 . 655 ¬± 0 . 036 . On all three tasks, the Maxsmi results best results for the FreeSolv, ESOL, and lipophilicity data (see Table 2 )
perform better than MoleculeNet (see Table 4 ), suggesting that SMILES is found: for the affinity data, the CONV1D model is chosen (similarly
augmentation with shallow neural networks could perform at least as to lipophilicity, ESOL, and FreeSolv), the number of augmentation is set
well as, if not better than, graph neural networks (GNNs). to 70, as for ESOL and FreeSolv, and the augmentation strategy is set to
The second study we consider is the Convolutional Neural Finger- augmentation with reduced duplication for a less computational inten-
print (CNF) model [22,23] , in which SMILES augmentation is applied, sive training than augmentation with duplication. As comparison, the
generating unique representations for each compound, i.e. augmenta- Maxsmi, the canonical, and the baseline models on affinity are trained
tion without duplication. The CNF model is evaluated using five-fold and evaluated. The same experimental setup for splitting and evaluation
cross-validation (CV), however standard deviations are not reported. as mentioned in the Data and experimental setup section is applied. On
Test RMSE values are 0.62, 1.11 and 0.66 on the ESOL, FreeSolv, and the test set, the canonical model reaches an RMSE value and coefficient
lipophilicity data set, respectively, see [ 22 , Table S1]. Similar to Molecu- of correlation ùëÖ 2 value of 1.031 and 0.494, respectively. In compari-
leNet, the Maxsmi model slightly outperforms these results on all three son, the Maxsmi model shows great improvement with test RMSE and
tasks. This suggests that augmenting the data set by greater factors, e.g. ùëÖ 2 values of 0.777 and 0.712, respectively (see Table 5 ). Surprisingly,
Table 5
The Maxsmi models strike again! The Maxsmi model developed for the affinity against the EGFR kinase and the Random
Forest (RF) baseline model outperform the canonical model.
Name Model Augmentation number Augmentation strategy Test RMSE Test ùëÖ 2
Maxsmi CONV1D 70 Augmentation with reduced duplication 0.777 0.712
Canonical CONV1D 0 No augmentation 1.031 0.494
Baseline RF 0 No augmentation 0.758 0.726
9T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
the RF baseline model performs similarly to the Maxsmi model, with an As an outlook, we observe that strategies that keep all, or a fraction
RMSE of 0.758 and an ùëÖ 2 of 0.726. of duplicates, may help the model to learn inherent symmetry in a com-
pound. Indeed the same random SMILES representation will certainly
4.5. Maxsmi models available for user predictions be generated multiple times for a symmetric molecule even though the
initial atom and the path along the graph are different. In this sense,
Given the good performance of the Maxsmi models for all three SMILES duplication is not an artificial construction, and keeping repli-
physico-chemical tasks and for EGFR affinity, we retrained them on all cas could retain important information about the underlying symmetry
points in the data set as a final product. The aim is to offer a single of a compound.
command-line interface for prediction. A user can provide a SMILES as
input, choose a given task and they will receive an output file in the 6. Abbreviations
form of a CSV table with relevant information, such as 1. the user input
SMILES itself, 2. whether the compound was in the training set, 3. the List of abbreviations used in the paper.
canonical SMILES, and its associated variations 4. the per SMILES pre-
dictions, 5. the per compound prediction, 6. and the standard deviation. QSAR Quantitative Structure-Activity Relationship
A PNG file of the 2D molecular graph associated with the in- SMILES Simplified Molecular-Input Line-Entry System
put SMILES is also generated. For example, for the semaxanib drug,
CADD Computer-Aided Drug Design
taken from the PKIDB database [63] , and given by the SMILES
O = C2C(\1ccccc1N2) = C/c3c(cc([nH]3)C)C , lipophilicity is ML Machine Learning
predicted using the command-line DL Deep Learning
$ python maxsmi/prediction_unlabeled_data.py LSTM Long Short-Term Memory
--task = ‚Äùlipophilicity ‚Äù --smiles_prediction = ‚ÄùO =
EGFR Epidermal Growth Factor Receptor
C2C(\1ccccc1N2) = C/c3c(cc([nH]3)C)C ‚Äù
NA Not Available
The command above was used to generate the values in Fig. 1 .
RMSE Root Mean Squared Error
5. Conclusion CONV1D 1D Convolutional Neural Network
CONV2D 2D Convolutional Neural Network
In this study, SMILES augmentation applied to deep learning molec-
ular property and activity prediction is investigated. Five augmentation RNN Recurrent Neural Network
strategies that can be applied as SMILES augmentation are explored, RF Random Forest
together with three neural network architectures, and the performance GNN Graph Neural Network
thoroughly assessed on three molecular data sets: ESOL, FreeSolv, and
CNF Convolutional Neural Fingerprint
lipophilicity.
Our findings show that augmentation improves the performance of CV Cross-validation
deep learning models not only independently of the model, but also MolPMoFiT Molecular Prediction Model Fine-Tuning
with respect to the size of the data set. This suggests that the choice of
augmentation strategy can be viewed as hyper-parameter tuning.
Declaration of Competing Interest
The tuple consisting of (model, augmentation number, augmenta-
tion strategy) that maximizes the performance on the test set leads to
The authors declare that they have no known competing financial
the definition of the Maxsmi models. Our findings also show that the
interests or personal relationships that could have appeared to influence
model using canonical SMILES outperforms the one using single ran-
the work reported in this paper.
dom SMILES, thanks to the simplicity of the canonical notation.
Additionally, the Maxsmi models outperform, or perform at least as
Acknowledgments
well as state-of-the-art models such as MoleculeNet, CNF, and MolP-
MoFiT, on the three physico-chemical data sets. This suggests that apply-
The authors thank the HPC Service of ZEDAT, Freie Universit√§t
ing simple SMILES augmentation techniques can reach similar or even
Berlin, for computing time. Talia B. Kimber received funding from
better performance as sophisticated models such as graph-based neural
the Stiftung Charit√©in the context of the Einstein BIH Visiting Fellow
networks, as in the case of MoleculeNet. Moreover, we use our find-
Project. The authors thank Greg Landrum for beneficial discussions, as
ings to guide the application of SMILES augmentation on a new data set
well as Dominique Sydow for valuable advice on documentation.
and provide a test case with data on affinity against the EGFR kinase. Fi-
nally, we provide an easy to use framework for out-of-sample prediction
Appendix
on four tasks: ESOL, FreeSolv, lipophilicity, and affinity against EGFR,
which should be helpful to assess properties of novel compounds. The
Figures
open-source code allows to perform similar studies on different data sets
with minor programmatic adjustments.
Figures in the appendix.
10T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. A1. Test RMSE using data augmen-
tation on the FreeSolv data set. The table
shows the root mean squared error (RMSE)
on the test set for three deep learning mod-
els and five SMILES augmentation strate-
gies, using various augmentation numbers,
as well as a baseline consisting of a Ran-
dom Forest (RF) model with Morgan fin-
gerprint as input. The lighter the purple
color, the better the model. The overall best
setting is highlighted in yellow, which for
the FreeSolv data set is augmenting the
data set 70 times keeping all duplicates
and training a 1D convolutional neural net-
work (CONV1D). For interpretation of the
references to color in this figure legend, the
reader is referred to the web version of this
article.
11T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. A2. Test RMSE using data augmen-
tation on the lipophilicity data set. The
table shows the root mean squared er-
ror (RMSE) on the test set for three deep
learning models and five SMILES augmen-
tation strategies, using various augmenta-
tion numbers, as well as a baseline consist-
ing of a Random Forest (RF) model with
Morgan fingerprint as input. The lighter
the purple color, the better the model. The
overall best setting is highlighted in yel-
low, which for the lipophilicity data set is
augmenting the data set 80 times removing
duplicates and training a 1D convolutional
neural network (CONV1D). For interpreta-
tion of the references to color in this fig-
ure legend, the reader is referred to the web
version of this article.
12T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
Fig. A3. Generating a large portion of the SMILES space does not lead
Fig. A5. Performance reaches a plateau independently of the augmenta-
to the best performance. Even though the CONV1D model is presented with
tion strategy. The performance of the CONV1D model trained and evaluated
SMILES variations that cover a large portion of the SMILES space using the aug-
on the lipophilicity data set reaches a test RMSE value slightly below 0.6 as of
mentations strategy with estimated maximum, on the FreeSolv data set, this
60 augmentation steps and fluctuates below this value thereafter, for all aug-
strategy does not achieve the best results.
mentation strategies: with, without, and with reduced duplication.
Fig. A6. Trade-off between performance and computation time. As ex-
pected, the training time of the CONV1D model on the ESOL data increases
Fig. A4. Performance reaches a plateau independently of the augmenta- with the augmentation number for all augmentation strategies: with, without,
tion strategy. The performance of the CONV1D model trained and evaluated and with reduced duplication. Augmenting the training set by 100 and keeping
on the ESOL data set reaches a test RMSE value slightly below 0.6 as of 40 aug- duplicate leads to 90 200 data points (see Table 1 ). Training the model on a GPU
mentation steps and fluctuates below this value thereafter, for all augmentation takes approximately three hours and reaches a test RMSE of 0.580 (see Figure 2 ).
strategies: with, without, and with reduced duplication. However, augmenting the data by just 19 leads to a test RMSE of 0.605 in less
than 30 minutes.
Fig. A7. Confidence curves of the Maxsmi models on the ESOL and lipophilicity data. The general trend of the curve in the left plot (the ESOL data) is decreasing,
showing a relationship between high confidence and small mean prediction error. Although also generally decreasing, the mean prediction error in the right plot
(the lipophilicity data) is still above 0.3 when only keeping the 10% of compounds with the highest confidence.
13T.B. Kimber, M. Gagnebin and A. Volkamer Artificial Intelligence in the Life Sciences 1 (2021) 100014
References [30] Kazil J , Jarmul K . Data wrangling with python: tips and tools to make your life
easier. 1st. O‚ÄôReilly Media, Inc.; 2016 . ISBN 1491948817
[1] Paul SM, Mytelka DS, Dunwiddie CT, Persinger CC, Munos BH, Lindborg SR, et al. [31] Tagasovska N, Lopez-Paz D. Single-model uncertainties for deep learning. arXiv
How to improve r&d productivity: the pharmaceutical industry‚Äôs grand challenge. preprint arXiv:181100908 2019 . https://arxiv.org/abs/1811.00908
Nat Rev Drug Discovery 2010;9(3):203‚Äì14. doi: 10.1038/nrd3078 . [32] Scalia G, Grambow CA, Pernici B, Li Y-P, Green WH. Evaluating scalable uncertainty
[2] Scannell JW, Blanckley A, Boldon H, Warrington B. Diagnosing the decline estimation methods for deep learning-based molecular property prediction. J Chem
in pharmaceutical r&d efficiency. Nat Rev Drug Discovery 2012;11(3):191‚Äì200. Inf Model 2020;60(6):2697‚Äì717. doi: 10.1021/acs.jcim.9b00975 .
doi: 10.1038/nrd3681 . [33] Ayhan M.S., Berens P. Test-time data augmentation for estimation of
[3] Waring MJ, Arrowsmith J, Leach AR, Leeson PD, Mandrell S, Owen RM, et al. An heteroscedastic aleatoric uncertainty in deep neural networks. 2018.
analysis of the attrition of drug candidates from four major pharmaceutical compa- https://openreview.net/pdf?id = rJZz-knjz .
nies. Nat Rev Drug Discovery 2015;14(7):475‚Äì86. doi: 10.1038/nrd4609 . [34] Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, Leswing K,
[4] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolu- Pande V. Moleculenet: a benchmark for molecular machine learning. Chem Sci
tional neural networks. Commun ACM 2017;60(6):84‚Äì90. doi: 10.1145/3065386 . 2018;9:513‚Äì30. doi: 10.1039/C7SC02664A .
[5] Graves A, Mohamed A-r, Hinton G. Speech recognition with deep recurrent neural [35] Gaulton A, Hersey A, Nowotka M, Bento AP, Chambers J, Mendez D, et al.
networks. In: 2013 IEEE International Conference on Acoustics, Speech and Signal The ChEMBL database in 2017. Nucleic Acids Res 2016;45(D1):D945‚Äì54.
Processing; 2013. p. 6645‚Äì9. doi: 10.1109/ICASSP.2013.6638947 . doi: 10.1093/nar/gkw1074 .
[6] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. [36] Mayr A, Klambauer G, Unterthiner T, Steijaert M, Wegner JK, Ceulemans H,
Highly accurate protein structure prediction with alphafold. Nature 2021;596:583‚Äì Clevert D-A, Hochreiter S. Large-scale comparison of machine learning
9. doi: 10.1038/s41586-021-03819-2 . methods for drug target prediction on ChEMBL. Chem Sci 2018;9:5441‚Äì51.
[7] Feinberg EN, Sur D, Wu Z, Husic BE, Mai H, Li Y, et al. Potentialnet for molec- doi: 10.1039/C8SC00148K .
ular property prediction. ACS Cent Sci 2018;4(11):1520‚Äì30. doi: 10.1021/acs- [37] Zhang Y, Lee AA. Bayesian semi-supervised learning for uncertainty-calibrated pre-
centsci.8b00507 . diction of molecular properties and active learning. Chem Sci 2019;10:8154‚Äì63.
[8] Sattarov B, Baskin II, Horvath D, Marcou G, Bjerrum EJ, Varnek A. De novo doi: 10.1039/C9SC00616H .
molecular design by combining deep autoencoder recurrent neural networks [38] Ramsundar B , Eastman P , Walters P , Pande V , Leswing K , Wu Z . Deep learning for
with generative topographic mapping. J Chem Inf Model 2019;59(3):1182‚Äì96. the life sciences. O‚ÄôReilly Media; 2019 . ISBN 9781492039839
doi: 10.1021/acs.jcim.8b00751 . [39] Delaney JS. Esol: estimating aqueous solubility directly from molecular structure. J
[9] Webel HE, Kimber TB, Radetzki S, Neuenschwander M, Nazar√©M, Volkamer A. Re- Chem Inf Comput Sci 2004;44(3):1000‚Äì5. doi: 10.1021/ci034243x .
vealing cytotoxic substructures in molecules using deep learning. J Comput Aided [40] Mobley DL, Guthrie JP. FreeSolv: a database of experimental and calculated hydra-
Mol Des 2020;34(7):731‚Äì46. doi: 10.1007/s10822-020-00310-4 . tion free energies, with input files. J Comput Aided Mol Des 2014;28(7):711‚Äì20.
[10] Goodfellow I, Bengio Y, Courville A. Deep learning. MIT Press; 2016 . doi: 10.1007/s10822-014-9747-x .
http://www.deeplearningbook.org [41] ChEMBL. https://www.ebi.ac.uk/chembl/ [Online; accessed 27-August-2021];
[11] Bisong E . Google Colaboratory. Berkeley, CA: Apress; 2019. p. 59‚Äì64 . ISBN 2021.
978-1-4842-4470-8 [42] Kooistra AJ, Volkamer A. Kinase-centric computational drug development.
[12] Kaggle. https://www.kaggle.com/ ; 2021. [Online; accessed 27-August-2021]. In: Annual Reports in Medicinal Chemistry. Elsevier; 2017. p. 197‚Äì236.
[13] Parks CD, Gaieb Z, Chiu M, Yang H, Shao C, Walters WP, et al. D3R Grand doi: 10.1016/bs.armc.2017.08.001 .
challenge 4: blind prediction of protein‚Äìligand poses, affinity rankings, and [43] OpenKinome. http://openkinome.org/ [Online; accessed 27-August-2021]; 2021.
dre ol ia :t 1i 0v .e
1
0b 0i 7n /d si 1n 0g
8
2fr 2e -e
0
2e 0n -0er 0g 2i 8es 9.
-
yJ
.
Comput Aided Mol Des 2020;34(2):99‚Äì119. [44] JH oe ur rb ns at
l
R oS f. RR ae dv ii ae tw
io
no f Oe np ci od le or gm ya
‚àó
Bl iogr loo gw yt ‚àóh
P
hf ya sc it co sr 2r 0ec 0e 4p ;5to 9r
(
2b
,
io Sl uo pg py l.
e
mIn et ner t)n :a St 2i 1on ‚Äì6al
.
[14] Huang R, Xia M. Editorial: Tox21 challenge to build predictive models of nuclear doi: 10.1016/j.ijrobp.2003.11.041 .
receptor and stress response pathways as mediated by exposure to environmental [45] Consortium TU. Uniprot: the universal protein knowledgebase in 2021. Nucleic
toxicants and drugs. Front Environ Sci 2017;5:3. doi: 10.3389/fenvs.2017.00003 . Acids Res 2020;49(D1):D480‚Äì9. doi: 10.1093/nar/gkaa1100 .
[15] Weininger D. Smiles, a chemical language and information system. 1. introduc- [46] IC50 Values. Offermanns S, Rosenthal W, editors. Berlin, Heidelberg: Springer Berlin
tion to methodology and encoding rules. J Chem Inf Comput Sci 1988;28(1):31‚Äì6. Heidelberg; 2008. doi: 10.1007/978-3-540-38918-7_5943 . ISBN 978-3-540-38918-7
doi: 10.1021/ci00057a005 . [47] Kimber TB, Chen Y, Volkamer A. Deep learning in virtual screening: recent applica-
[16] O‚ÄôBoyle NM. Towards a universal SMILES representation - a standard method tions and developments. Int J Mol Sci 2021;22(9). doi: 10.3390/ijms22094435 .
to generate canonical SMILES based on the inchi. J Cheminform 2012;4(1). [48] Mean squared error. Sammut C, Webb GI, editors. Boston, MA: Springer US; 2010.
[17]
d Wo ei:
i
1 n0 in.1 g1 er8 6 D/1
,
7 W58 e- i2 n9 i4 n6 ge-4
r
-2 A2
,
.
Weininger JL. Smiles. 2. algorithm for genera- [49]
d Ko vi √•:
l
1 s0 e. t1 h0 07 T/ O9 .7 8- C0 a-3 u8 ti7 o- n3 a0 r1 y6 4- n8 o_ t5 e2 8 .
a
I bS oB uN
t
97 ùëü8
2
.- 0-3 A8 m7 -30 S1 ta6 t4 -8
1 985;39(4):279‚Äì85.
tion of unique smiles notation. J Chem Inf Comput Sci 1989;29(2):97‚Äì101. doi: 10.1080/00031305.1985.10479448 .
doi: 10.1021/ci00062a008 . [50] Breiman L. Random forests. Mach Learn 2001;45(1):5‚Äì32.
[18] Hemmerich J, Asilar E, Ecker GF. COVER: Conformational oversam- doi: 10.1023/A:1010933404324 .
pling as data augmentation for molecules. J Cheminform 2020;12(1). [51] Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blon-
doi: 10.1186/s13321-020-00420-z . del M, a Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A,
[19] Li Y, Rezaei MA, Li C, Li X. Deepatom: A framework for protein-ligand binding Cournapeau D, Brucher M, Perrot M, Duchesnay E. Scikit-learn: machine
affinity prediction. In: 2019 IEEE International Conference on Bioinformatics and learning in python. Journal of Machine Learning Research 2011;12:2825‚Äì30 .
Biomedicine (BIBM); 2019. p. 303‚Äì10. doi: 10.1109/BIBM47256.2019.8982964 . http://jmlr.org/papers/v12/pedregosa11a.html
[20] Bjerrum EJ. Smiles enumeration as data augmentation for neural net- [52] Bennett L., Melchers B., Proppe B. Curta: A general-purpose high-performance
work modeling of molecules. arXiv preprint arXiv:170307076 2017 . computer at Zedat, Freie Universit√§t Berlin. 2020. https://doi.org/10.17169/
https://arxiv.org/abs/1703.07076 refubium-26754 .
[21] Li X, Fourches D. Inductive transfer learning for molecular activity pre- [53] Van Rossum G , Drake FL . Python 3 reference manual. Scotts Valley, CA: CreateSpace;
diction: next-gen QSAR models with molpmofit. J Cheminform 2020;12(1). 2009 . ISBN 1441412697
doi: 10.1186/s13321-020-00430-x . [54] van Rossum G, Warsaw B, Coghlan N. Style guide for Python code. PEP; 2001 .
[22] Kimber TB, Engelke S, Tetko IV, Bruno E, Godin G. Synergy effect be- https://www.python.org/dev/peps/pep-0008/
tween convolutional neural networks and the multiplicity of smiles for im- [55] Read the Docs. https://readthedocs.io/en/stable/ [Online; accessed 30-July-2021];
provement of molecular prediction. arXiv preprint arXiv:181204439 2018 . 2021.
https://arxiv.org/abs/1812.04439 [56] Anaconda software distribution. 2020. https://anaconda.com/ .
[23] Tetko IV, Karpov P, Bruno E, Kimber TB, Godin G. Augmentation is what you need!. [57] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z,
In: Tetko IV, K ≈Ø rkov√°V, Karpov P, Theis F, editors. Artificial Neural Networks and Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M, Tejani A,
Machine Learning ‚ÄìICANN 2019: Workshop and Special Sessions. Cham: Springer Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. Pytorch: an imperative style,
International Publishing; 2019. p. 831‚Äì5. doi: 10.1007/978-3-030-30493-5_79 . ISBN high-performance deep learning library. In: Wallach H, Larochelle H, Beygelzimer A,
978-3-030-30493-5 d‚ÄôAlch√©-Buc F, Fox E, Garnett R, editors. Advances in Neural Information Processing
[24] Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learn- Systems 32. Curran Associates, Inc.; 2019. p. 8024‚Äì35 . http://papers.neurips.cc/
ing. J Big Data 2019;6(1). doi: 10.1186/s40537-019-0197-0 . paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
[25] Tetko IV, Karpov P, Deursen RV, Godin G. State-of-the-art augmented NLP trans- pdf
former models for direct and single-step retrosynthesis. Nat Commun 2020;11(1). [58] Harris CR, Millman KJ, van der Walt SJ, Gommers R, Virtanen P, Courna-
doi: 10.1038/s41467-020-19266-y . peau D, et al. Array programming with numpy. Nature 2020;585(7825):357‚Äì62.
[26] Sumner D, He J, Thakkar A, Engkvist O, Bjerrum EJ. Levenshtein augmentation doi: 10.1038/s41586-020-2649-2 .
improves performance of smiles based deep-learning synthesis prediction. ChemRxiv [59] The pandas development team. pandas-dev/pandas: Pandas. 2020. 10.5281/zen-
2020. doi: 10.26434/chemrxiv.12562121.v2 . odo.3509134
[27] Ar√∫s-Pous J, Johansson SV, Prykhodko O, Bjerrum EJ, Tyrchan C, Reymond J-L, et al. [60] GitHub Actions. https://docs.github.com/en/actions [Online; accessed 30-July-
Randomized SMILES strings improve the quality of molecular generative models. J 2021]; 2021.
Cheminform 2019;11(1). doi: 10.1186/s13321-019-0393-0 . [61] Pytest. https://docs.pytest.org/ [Online; accessed 30-July-2021]; 2021.
[28] van Deursen R, Ertl P, Tetko IV, Godin G. GEN: Highly efficient SMILES explorer [62] Codecov. https://docs.codecov.com/docs [Online; accessed 30-July-2021]; 2021.
using autodidactic generative examination networks. J Cheminform 2020;12(1). [63] Carles F, Bourg S, Meyer C, Bonnet P. PKIDB: a curated, annotated and up-
doi: 10.1186/s13321-020-00425-8 . dated database of protein kinase inhibitors in clinical trials. Molecules 2018;23(4).
[29] RDKit: Open-source cheminformatics. http://www.rdkit.org [Online; accessed 01- doi: 10.3390/molecules23040908 .
July-2021]; 2021.
14