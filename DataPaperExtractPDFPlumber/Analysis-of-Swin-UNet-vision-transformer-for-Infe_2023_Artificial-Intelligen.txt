ArtificialIntelligenceintheLifeSciences4(2023)100084
ContentslistsavailableatScienceDirect
Artificial Intelligence in the Life Sciences
journal homepage: www.elsevier.com/locate/ailsci
Research Article
Analysis of Swin-UNet vision transformer for Inferior Vena Cava filter
✩
segmentation from CT scans
Rahul Gomesa,∗, Tyler Phamb, Nichol Hea, Connor Kamrowskia, Joseph Wildenbergc,∗∗
aDepartment of Computer Science, University of Wisconsin-Eau Claire, Eau Claire, 54701, WI, United States
bDepartment of Computer Science, University of Minnesota-Twin Cities, Minneapolis, 55455, MN, United States
cInterventional Radiology, Mayo Clinic Health System, Eau Claire, 54703, WI, United States
A R T I C L E I N F O A B S T R A C T
Dataset link: https:// Purpose: The purpose of this study is to develop an accurate deep learning model capable of Inferior Vena Cava
github .com /rahulgomes19 /IVC _3D (IVC) filter segmentation from CT scans. The study does a comparative assessment of the impact of Residual
Networks (ResNets) complemented with reduced convolutional layer depth and also analyzes the impact of
Keyw ords:
using vision transformer architectures without performance degradation.
Deeplearning
Medical imaging Materials and Methods: This experimental retrospective study on 84 CT scans consisting of 54618 slices involves
Convolutional neural networks design, implementation, and evaluation of segmentation algorithm which can be used to generate a clinical report
SWIN transformer for the presence of IVC filters on abdominal CT scans performed for any reason. Several variants of patch-based
UNet 3D-Convolutional Neural Network (CNN) and the Swin UNet Transformer (Swin-UNETR) are usedto retrieve
ResNet the signature of IVC filters. The Dice Score is used as a metric to compare the performance of the segmentation
IVCfilter models.
Results: Model trained on UNet variant using four ResNet layers showed a higher segmentation performance
achieving median Dice = 0.92 [Interquartile range(IQR): 0.85, 0.93] compared to the plain UNet model with
four layers having median Dice = 0.89 [IQR: 0.83, 0.92]. Segmentation results from ResNet with two layers
achieved a median Dice = 0.93 [IQR: 0.87, 0.94] which was higher than the plain UNet model with two layers
at median Dice = 0.87 [IQR: 0.77, 0.90]. Models trained using SWIN-based transformers performed significantly
better in both training and validation datasets compared to the four CNN variants. The validation median Dice
was highest in 4 layer Swin UNETR at 0.88 followed by 2 layer Swin UNETR at 0.85.
Conclusion: Utilization of vision based transformer Swin-UNETR results in segmentation output with both low
bias and variance thereby solving a real-world problem within healthcare for advanced Artificial Intelligence
(AI) image processing and recognition. The Swin UNETR will reduce the time spent manually tracking IVC filters
by centralizing within the electronic health record. Link to GitHubrepository.
1. Introduction usually due to a high risk of major bleeding [15]. The National Hospital
Discharge Survey recorded a total of 803,00 IVC filters placed between
Inferior Vena Cava (IVC) filters are medical devices placed inside 1985 to 2006, and about 259,000 filters were estimated to have been
the IVC to reduce the morbidity and mortality of Venous Thromboem- placed in 2012 [3].
bolism (VTE), specifically Pulmonary Embolism (PE). In the United Currently, the two types of IVC filters in use within the United
States, there are about 60,000 to 100,000 deaths per year due to VTE States are permanent filters and retrievable filters. Retrievable filters
[3]. IVC filtration is an alternative option for managing these conditions originated in the 1990s and were designed with the option of being
when anticoagulation, the first line treatment for VTE, cannot be used – retrieved or left in place after the risk of PE subsided [18]. When re-
✩ This document is the results of the research project funded by the Mayo Clinic-UW Eau Claire Research Innovation Council, and National Science Foundation-
Research Experience for Undergraduates OAC-2150191. The computational resources of the study were provided by the Blugold Center for High-Performance
Computing under National Science Foundation grant CNS-1920220.
* Corresponding author.
** Principal corresponding author.
E-mail addresses:gomesr@uwec.edu(R.Gomes), Wildenberg.Joseph@mayo.edu(J.Wildenberg).
https://doi.org/10.1016/j.ailsci.2023.100084
Received 8 June 2023; Received in revised form 25 July 2023; Accepted 9 August 2023
Availableonline18August2023
2667-3185/©2023TheAuthor(s).PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
modified final fully connected layer [24]. While this study was able to
identify different types of IVC filters, it was not applicable to detecting
IVC filters that were already inside the patients. Another study made
an effort to improve on this strategy by developing an architecture that
can categorize three different IVC filter kinds without requiring the ra-
diographs to be cropped and adjusted to be centered on the filter [25].
An image-processing-only approach created in the past by Dr. Wilden-
berg can identify a filter from CT images with sensitivity and specificity
of roughly 80% each. However, this study used a smaller dataset and
mostly relied on a linear approach rendering it impossible to adjust
to the heterogenous nature of patient CT scans. Given the low preva-
lence of IVC filters in the general community and the daily average
of 400 suitable CT scans in Mayo Enterprise, this accuracy would put
an unreasonably heavy burden on clinician analysis of the inevitable
false positive results. A workable answer is offered by an automated
deep learning method, and to our knowledge, real-time IVC filter pres-
ence/absence detection from segmentation is an uncharted area.
The success of the transformer architecture in the domain of natural
Fig.1.(a) An example of IVC filter used in patients. (b) shows a 3D rendering
language processing (NLP) has led researchers to examine its appli-
of CT scan with the position of IVC filter in the patient.
cability in the domain of computer vision. The usage of attention in
transformer models allows it to learn long-range relationships between
image elements that are difficult to attain through convolutions. The
trieved at the appropriate time, retrievable IVC filters provide an added Vision Transformer (ViT) is the first architecture that fully replaced
benefit of reducing the occurrence of long-term complications associ- convolutions with the original transformer model and garnered signifi-
ated with their permanent counterpart [5,26]. While retrievable IVC cant research attention [11,20]. However, inherent differences between
filters were advertised with the option of having it remain in a pa- NLP and computer vision presents significant challenges in adapting
tient’s body, serious long-term complications can occur. Leaving IVC the transformer architecture. Visual elements are more variable in scale
filters in the body for more than 30 days increases the risk of Deep compared to words in a passage and images of higher resolution scales
Vein Thrombosis (DVT), filter migration/embolization, filter fracture, complexity beyond what is expected in NLP. The Swin Transformer uti-
and IVC perforation [10,4,2]. To address these potential complications, lizes shifting windows of attention and hierarchical layers to overcome
the FDA in 2010 recommended that physicians consider the removal these differences and achieve strong performance while maintaining lin-
of retrievable IVC filters as soon as the risk for PE subsided. Then, in ear computational complexity [21].
2014, the FDA issued guidance that, once the patient is no longer at risk This research explores the feasibility and performance of a Swin
for PE, the risks of keeping filters in the patient begin to outweigh the UNet transformer to create a segmentation backbone for identification
benefits between 29 to 54 days after implantation [8]. and tracking of patients with IVC filters and facilitate timely removal,
Although removal of these IVC filter is as simple as setting up an ap- even when they transition their care into or throughout a health system.
pointment with the clinician within 30 days of the procedure, most stud- In addition to the reduction in complications related to unintentionally
ies reported an average retrieval rate between 20% and 30% [28,23]. long filter dwell times, timely removal will also result in shorter proce-
There are three factors relating to the underwhelming rate of IVC filter dural times during retrieval [9,12].
retrieval: (a) patient, (b) system, and (c) technical factors. Patient fac-
tors include socioeconomic status and medical comorbidities. Patients 2. Materials and methods
could have limited resources, such as healthcare coverage and trans-
portation, which results in poor clinical follow-up. Patients with many 2.1. Image datasets
comorbidities may have a higher periprocedural risk than the risks of
leaving the filter in place. The system factors consist of poor tracking All data processing was retrospective and IRB-exempt in the need for
and patient follow-up [8]mostly arising due to patients moving to a dif- consent. Using an internal list of known patients with filters, a total of
ferent health care system due to job, education, and other factors and 84 CT scans having 54618 slices along the axial plane were provided by
not transferring records from their past. The technical factors include the health system. By eliminating any attached protected health infor-
the challenges encountered during retrievals, such as filter tilt or frag- mation (PHI), these scans were anonymized. To make the model scanner
mentation. An easy way to track these IVC filters at a much later date agnostic, CT scans were purposely chosen from multiple different scan-
is to have a lightweight filter detection algorithm that runs whenever ners used throughout the health system. A distribution of Hounsfield
a person gets a CT scan for other health reasons thereby uncovering Units (HU) for scans is shown in Fig.2. Fig.2a shows the slice thickness
patients who would otherwise be lost to follow-up. Fig.1shows an ex- and Fig.2b shows the slice spacing of all the CT scans used for analysis.
ample of IVC filter used during the procedure. With these three issues These metrics show the heterogeneity in the CT scans used for training
leading to underwhelming retrieval rates, it is crucial that the process of the deep learning models. Fig.2c shows how many slices are available
IVC tracking, and retrieval be integrated in the general CT scan pipeline per CT scan used in this research. The bar graphs in blue resemble a re-
to enable rapid diagnosis. The proposed research explores the develop- duced version of CT scans to further remove slices not relevant to our
ment of the segmentation algorithm which will be an integral part of research. This significantly reduces the dataset as it can be observed in
this pipeline. There is also a need to ensure that this algorithm is very the graph compared to the dimensions of the original scans shown in
accurate with significantly less false positives thereby saving valuable red. A further explanation about data preprocessing is discussed in the
time for clinicians. next section.
Research has already been conducted on classifying the type of IVC
filter found in radiographs using CNNs in the context of medical image 2.2. Data preprocessing
analysis. In one study, radiographic pictures that had been manually
cropped were utilized to classify 14 different IVC filters. This catego- All CT scans were spatially cropped before training to remove as
rization was carried out using a 50-layer ResNet architecture with a much background thereby giving the deep learning model less data for
2R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
The training set consisted of 58 CT scans of size (256 × 256 × 128).
This was 70% of the total CT scans split randomly. These scans were
further split into smaller 3-D patches to be used in the training process.
The selected patch size was (64 × 64 × 32). The desired patches were
obtained using a function view_as_blocks [29] made available through
the Scikit-image library. A total of 3712 patches were generated along
with corresponding 3712 segmentation masks. The patches were split
into IVC and non-IVC based on the presence of IVC filters in the mask.
Hence, a total of 3557 patches and masks were generated with no IVC
filters while 155 patches and masks were generated with IVC filters for
training. The validation set consisted of 26 CT scans (30%) resulting in
1591 patches and masks with no IVC filters along with 73 patches and
masks with IVC filters.
These IVC filter voxels constitute an extremely small portion of the
entire volume of a CT scan. To remedy the bias towards background
data, the ratio of IVC filter patches to normal patches were restricted to
1:1 while training. Thus, the total input size was 155 patches with IVC
filters and 155 patches without IVC filters in each sub-epoch. A total of
22 sub-epochs were run each containing a different sequence of non-
IVC filter CT scan patches. This procedure was repeated for 25 epochs
resulting in 550 total iterations thereby training on the entire dataset.
To address overfitting that may arise for using same IVC patches, dy-
namic augmentation techniques such as random crop, zoom, flipping,
and rotation were applied on 3D-patches.
2.3. Deep learning segmentation
2.3.1. UNet
UNet is a Convolutional Neural Network (CNN) designed for image
segmentation [27]. CNNs have been extensively used in healthcare for
Fig. 2. Metrics of 84 CT scans used for model training. (a) Shows the slice creating diagnostic and prognostic models [1,32,19,13]. The underlying
thickness in mm, (b) Shows the slice pixel spacing in mm, and (c) shows the structure is made up of two paths: a contracting path and an expan-
number of slices per scan before (blue) and after (red) application of 40 cm sion path. The contracting path is an encoder consisting of convolution
spatial cropping. It is observed that the number of slices reduces significantly layers and pooling layers. The expansion path is a decoder consisting
thereby removing redundant data.
of transposed convolution layers and concatenations with the feature
maps from the encoder. The 3D-UNet is an extension of the basic UNet
structure, allowing for 3D volumetric processing [7]. The 3D-UNet still
consists of the contraction and expansion paths, but all its convolution
and pooling operations are implemented on a data cube instead of a
2D image. Changing the basic UNet model to fit 3D patches allows it
to fully take advantage of spatially dependent features across all three
dimensions [30].
2.3.2. ResUNet
ResUNets are similar to UNet in terms of the UNet architecture.
However, the convolution blocks are replaced with residual blocks also
known as ResNets [17]. ResNets use “skip connections” that allow infor-
mation to bypass one or more layers in the network, and thus propagate
more easily through the network. It is therefore able to address the van-
ishing gradient problem during backpropagation for higher accuracy. A
Fig.3.Slices(a)beforeand(b)afterHUnormalization.
comparison of a convolution block and a ResNet block used in this re-
search is shown in Fig.5. The concatenation of the input with the output
better performance. The 20% spatial cropping was done equally to each from convolution layers increased prediction performance of ResNets.
of the four edges. As a result, the original (512 × 512) CT scan slices
were downsized to (307 × 307) before resampling to (256 × 256). 2.3.3. SWIN UNet
Following the spatial cropping, slices of each CT scan 40 cm below the The Swin UNet Transformer (Swin UNETR) is a novel architecture
cranial-most slice were discarded. A 40 cm cut-off was chosen because that integrates the Swin Transformer into the traditional UNet convo-
almost all IVC filters are placed within the upper to mid abdomen. The lution architecture. Swin UNETR replaces the convolutional encoder
cut-off was able to decrease CT slices by 19.01% as shown in Fig. 2c of the original UNet design with the hierarchical design of the Swin
followed by resampling to have 128 slices per scan. The minimum and Transformer. The outputs of the Swin Transformer are reconnected to
maximum HU for the study was set at 1 HU and 2500 HU respectively. the standard UNet decoder through residual block skip connections.
These scans were then normalized between 0 and 1 before deep learn- Swin UNETR was originally designed for accurate and reproducible seg-
ing was applied. This hard normalization scheme works well for IVC mentation of brain tumors in MRI images [16]. The integration of the
filter detection as shown in [14]. The segmentation masks were created Swin Transformer into the UNet architecture was intended to allow the
under the supervision of a board-certified radiologist. Fig. 3 shows a model to learn long-range dependencies for accurate segmentation of
slice before and after application of hard normalization scheme. tumors that can appear in varying locations, shapes, and sizes. Model
3R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
Fig.4.Overview of the entire training and validation process of the IVC prediction pipeline. After data normalization, slices are split into training and validation.
This is followed by 3D-patch extraction. Patches are fed to 3D-UNet variants for IVC filter segmentation. Finally, image processing algorithms remove artefacts to
classify a scan having IVC filter.
on 5050 CT scans from publicly available datasets and transfer learned
to the HECKTOR dataset. The model achieved an average of 0.707 and
0.582 aggregated Dice similarity coefficient on the primary tumors and
lymph node gross tumor volume (GTV) respectively. The model per-
formed at a more stable 0.633 (primary tumor) and 0.673 (lymph node)
during the evaluation on the test dataset [6]. In a comparative study
conducted by Wei etal.[33], Swin UNETR was used among other ar-
chitectures to evaluate the effectiveness of the proposed high-resolution
Swin Transformer network. In the BraTS 2021 dataset, the Swin UN-
ETR achieved the best Hausdorff score amongst the transformer-based
models while maintaining a comparable DICE score. Swin UNETR also
achieved the best average validation DICE performance of 0.787 on the
BTCV multi-organ segmentation task. Maurya etal.[22]applied Swin
UNETR to the segmentation of pulmonary arteries using 200 samples
from the PARSE 2022 Grand Challenge. All versions of the Swin UN-
ETR trained and examined performed better than all versions of UNet,
although by a small margin of around 0.1 DICE score. The authors ob-
served that Swin UNETR was able to better segment arterial branches
while UNet was able to better detect the main artery.
Fig.5.Overview of a) ResNet blocks and b) UNet blocks used in the training
process.
2.4. Training process
was trained on the dataset provided as part of the 2021 Multi-modal
Brain Tumor Segmentation Challenge (BraTS). Compared to the win- Fig. 4 summarizes this entire process. Six variants of UNet model
ning methodologies of the 2021 BraTS, Swin UNETR outperformed all were implemented. The plain UNet models had two convolutional lay-
other models with at least 0.7%, 0.6%, and 0.5% greater Dice scores ers per block. One model utilized four blocks and the other utilized two
on Enhancing Tumor (ET), Whole Tumor (WT), and Tumor Core (TC) blocks. A similar two and four-block approach was used by replacing
semantic classes respectively. The model also achieved a highly com- basic UNet blocks with ResNets. Batch normalization was applied to
petitive performance in the testing phase. Swin UNETR has also been the output of the convolution layers. The output of the decoder blocks
applied to head and neck primary tumors and lymph node segmenta- is fed through SoftMax activation to create a (64 × 64 × 32 × 2) patch
tion using FGD-PET/CT images. 524 samples provided by the Head and containing prediction probabilities for background and IVC filter. The
Neck Tumor (HECKTOR) 2022 challenge. Swin UNETR was pre-trained remaining two UNet models utilized SWIN transformers.
4R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
Fig.6.OverviewofSWIN-UNetusedinthetrainingprocess.
Fig.7.BoxplotshowingtheDiceScoredistributionsofthesixmodelvariantsusedinthisstudy.
Table1
Dice scores for different model variants derived from training and validation datasets.
UNetSimple ResNetblocks SwinUNETR
2Layer 4Layer 2Layer 4Layer 2Layer 4Layer
Train Median 0.87 0.89 0.93 0.92 0.89 0.90
IQR 0.77-0.90 0.83-0.92 0.87-0.94 0.85-0.93 0.84-0.91 0.87-0.92
Valid Median 0.6 0.61 0.64 0.63 0.85 0.88
IQR 0.55-0.63 0.58-0.63 0.62-0.65 0.59-0.65 0.75-0.89 0.79-0.92
The two variants of the Swin UNETR architecture was implemented model is (64 x 64 x 32 x 2) patches of logits, which are then processed
to allow for comparison with the four UNet variants. One utilizes the into inferences using the Argmax function. The 4-stage Swin UNETR
four stage design detailed by Hatamizadeh etal.[16]. The model was contained 35,072,552 trainable parameters and completed training in 4
implemented using the MONAI library, which limits the starting num- hours 40 minutes. The 2 stage Swin UNETR contained 6,612,392 train-
ber of filters to a multiple of 12. Therefore, both variants are imple- able parameters and completed training in 4 hours 28 minutes. The
mented with a starting feature size of 36 to best match the other UNet diagram of the 4-stage Swin UNETR is shown in Fig.6.
variant implementations. Each subsequent stage doubles the number of
features, to a max of 576 features in the 4-stage variant and 144 fea- 3. Results
tures in the 2-stage variant. The output of the Swin UNETR at each stage
is fed into a residual block and then concatenated with the deconvolu- Application of different UNet variants showed significant variation
tion of the previous stage in a standard UNet decoder. The output of the in segmentation performance. Dice Score was used as the evaluation
5R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
Fig.8.OverviewofIVCDicefortrainingdatafora)Twostageandb)Fourstagemodelsandvalidationdataforc)Twostageandd)Fourstagemodels.
highest median Dice = 0.64 [IQR: 0.62, 0.65] while the UNet model
with two layers achieved the lowest median Dice = 0.60 [IQR: 0.55,
0.63].
The 4-stage Swin UNETR training recorded a median Dice = 0.90
[IQR: 0.87, 0.92]. The 2-stage Swin UNETR training recorded a me-
dian Dice = 0.89 [IQR: 0.84, 0.91]. Compared to results from plain
UNet and UNet with ResNet blocks, it is noticeable that Swin UNETR
training returns a more consistent and better performance. The effects
are also visible with Swin UNETR performance on validation datasets.
The 4-stage model returns median Dice = 0.88 [IQR: 0.79, 0.92] and
2-stage model reported median Dice = 0.85 [IQR: 0.75, 0.89]. These
metrics are significantly higher than the other CNN-based implementa-
tions used earlier. The IVC Dice during training process along with the
Fig.9.Figure showing calculation of Dice Score adopted from image under a Dice distributions is shown in Fig.7and summarized in Table1.
CC BY-SA 4.0 license. The training accuracy for all six model variants for the desired num-
ber of epochs were similar as noted in Fig.8a and 8b. The graphs denote
metric which is a common tool in computer vision tasks. In the con- performance of 2-layer compared to 4-layer models. The validation ac-
text of image segmentation, Dice is used to assess the accuracy of a curacy showed a major improvement while using Swin UNETR across
segmentation model’s predictions compared to the ground truth seg- both two and four stage models. The validation accuracy is shown in
mentation masks as shown in Fig.9. The Dice score ranges from 0 to Fig. 8c and 8d. The loss values of the training and validation process
1 with 1 being a complete match between segmentation and ground are shown in Fig.10.
truth. Considering the highly imbalanced nature of IVC pixels, the Dice
provided a simple and intuitive way to investigate the segmentation 4. Discussion
performance across different models which would otherwise have been
challenging if sensitivity and specificity were used due to such a large A 3D-patch based segmentation solution has been proposed to IVC
amount of background class. On the training dataset, the ResNet models filter detection from CT scans along the axial plane. The use of 3D
performed significantly better compared to plain UNet versions. ResNet patches enabled retention of the spatial component of these filters
with two layers achieved the highest median Dice = 0.93 [IQR: 0.87, thereby addressing the limitations of a 2D approach [14]. A smaller
0.94] while the UNet model with two layers achieved the lowest me- patch size also reduced the bias introduced by the sparseness of IVC fil-
dian Dice = 0.87 [IQR: 0.77, 0.90]. Results from applying the model ters in the data even though the scans used for training only constituted
on validation data also revealed a similar trend with ResNet variants about 0.08% of IVC filters. The effect of this imbalance was further re-
achieving a higher accuracy. ResNet with two layers again achieved the duced during the training process when using a batch size with equal
6R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
Fig.10.OverviewofIVCLossfortrainingdatafora)Twostageandb)Fourstagemodelsandvalidationdataforc)Twostageandd)Fourstagemodels.
distribution of filter and no filter patches complemented with augmen- eters. While these numbers are higher than traditional UNet variants at
tation during training process. Six different segmentation approaches 1.3 million (1,357,954) and 22 million (22,587,138) for two and four
were compared to further ensure sparsity of filter signature would not layer versions respectively, this research established that a transformer-
impact the outcome. It was observed that using ResNet blocks outper- based model with similar parameters as a CNN can produce better
formed basic convolution blocks in segmentation. The ResNet blocks training and validation outcomes when integrated with existing medi-
also reduced the rate of false-positives. These false positives were com- cal information technology. The data augmentation strategies including
prised mostly of bones and calcification around the spine. A probable random crop, random resize, random flip, and random rotation ensured
cause for this issue was the lack of skip-connections in basic UNet blocks that there was significant variation in the training dataset that consisted
that is present in ResNet. The skip connections enable a network to of 155 IVC and 155 non-IVC patches per sub-epoch. This augmentation
learn residual mappings, i.e., the difference between the input and out- strategy ensures that even if the same image is used repeatedly, there
put. Since the IVC filter signature is very low compared to background, are significant variations introduced by randomness thereby reducing
the kernels used in deep layers find it challenging to detect features any chances of any model overfitting.
unique to filters. As skip connections bring back the input from shallow
layers, the network learns more easily the underlying filter patterns.
5. Conclusion
Another very significant outcome of this research indicates that deep
learning models like Swin UNETR with transformers as their backbone
are capable of maintaining consistent performance across both training Future work in the IVC filter detection incorporates algorithm val-
and validation datasets. This is in contrast to CNN based segmenta- idation and classification pipeline on an estimated 1,500 clinical CT
tion which shows a drop in segmentation accuracy for validation data. scans predicted to be acquired from the health system. IVC filter pre-
Fig. 11 further corroborates this fact. Notice how both Swin UNETR diction pipeline will be developed to provide a report of the presence
models are able to conform the segmentation to the actual shape of of an IVC filter and its location. After receiving a predicted 3D mask
the filters whereas the CNN-derived UNet and ResUNet models lose from the Swin UNETR model, the program will use scikit-image image
the edges during the segmentation process. Also, the four layer mod- processing library [31] in Python to analyze the region properties of
els achieve better segmentation hence less post-processing as their are the segmented image, removing any spurious segmentation that may
no spurious segmentation regions around the scans. occasionally arise. The entire algorithm will be tested on clinical CT
A comparative layer analysis resulted in an optimized model with scans performed in the health system to assess performance, with man-
a smaller footprint. While ResNet blocks increase the segmentation ual review of each scan. This may be supplemented with additional
performance, they also require more parameters. For example, the known-positive test cases, separate from the training set, if the number
two layer ResNet model used in this research utilized around three of true-positive clinical cases is too low to generate meaningful statis-
million parameters (3,155,810) compared to 53 million parameters tics. The results and statistics from this review will then be used to
(53,154,018) in the four layer ResNet models. The Swin UNETR also retrain the Swin UNETR, if necessary, with rapid re-deployment and
provided better results with very comparable usage of trainable param- re-evaluation.
7R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
Fig.11.Overview of results a) UNet 2 Layer b) UNet 4 layer, c) ResUNet 2 Layer, d) ResUNet 4 layer, e) Swin UNETR 2 Layer, and f) Swin UNETR 4 Layer. Results
indicate Swin UNETR 4 Layer is able to conform to the shape of IVC Filters without any false positives.
8R.Gomes,T.Pham,N.Heetal. ArtificialIntelligenceintheLifeSciences4(2023)100084
CRediT authorship contribution statement times for the Günther tulip and celect retrievable filters. Cardiovasc Interv Radiol
2012;35:299–308.
Rahul Gomes:Conceptualization, Software, Methodology, Supervi- [13]GomesR,Kam rowski C, Langlois J,Rozar io P, Di rcksI,G rottodden K,etal.A
sion, Funding acquisition, Writing – original draft, Writing – review &
comprehensivereviewofmachinelearningusedtocombatCovid-19.Diagnostics
2022;12:1853.
editing. [14]GomesR, KamrowskiC, MohanPD, SenorC, LangloisJ, WildenbergJ. Application
Tyler Pham: Data curation, Software, Investigation, AI methodol- of deep learning to ivc filter detection from ct scans. Diagnostics 2022;12:2475.
ogy, Writing – original draft. [15]HannCL, StreiffMB. The role of vena caval filters in the management of venous
Nichol He:Data curation, AI methodology, Visualization, Writing –
thromboembolis m.Blood Rev200 5;19:179 –202.
[16]HatamizadehA,NathV,TangY,YangD,RothHR,XuD.Swinunetr:swintrans-
original draft.
formersforsemanticsegmentationofbraintumorsinmriimages.In:Brainlesion:
Connor Kamrowski:Data curation, Algorithm validation, Software, glioma, multiple sclerosis, stroke and traumatic brain injuries: 7th international
Visualization. workshop,BrainLes2021,heldinconjunctionwithMICCAI2021,virtualevent,
Joseph Wildenberg:Conceptualization, Software, Validation, Fund- revisedselectedpapers,PartI.Springer;2022.p.272–84.
ing acquisition, Project administration, Writing – review & editing.
[17]HeK,Zhang X ,Re nS, SunJ.Deep re siduallear ningfo rim agerec ognition.In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition;
2016.p.770–8.
Declaration of competing interest [18]HeitJA, CohenAT, AndersonJrFA. Estimated annual number of incident and re-
current,non-fatalandfatalvenousthromboembolism(vte)eventsintheus.Blood
The authors declare that they have no known competing financial
2005;106:91 0.
[19]Ismael SAA, Mohammed A, Hefny H. An enhanced deep learning approach for
interests or personal relationships that could have appeared to influence
brain cancer mri images classification using residual networks. Artif Intell Med
the work reported in this paper. 2020;102:101779.
[20]KhanS,NaseerM,HayatM,ZamirSW,KhanFS,ShahM.Transformersinvision:a
Data availability survey.ACMComputSurv2022;54:1–41.
[21]LiuZ,LinY,CaoY,HuH,WeiY,ZhangZ,etal.Swintransformer:hierarchicalvision
transformerusingshiftedwindows.In:ProceedingsoftheIEEE/CVFinternational
https://github .com /rahulgomes19 /IVC _3D. conference on computer vision; 2021. p.10012–22.
[22]MauryaA,PatilKD,PadhyR,RamakrishnaK,KrishnamurthiG.Parsechallenge
References 2022:pulmonaryarteriessegmentationusingswinu-nettransformer(swinunetr)
andu-net.ArXivpreprint.arXiv:2208.09636,2022.
[23]MismettiP,Rivron-GuillotK,QuenetS,DécoususH,LaporteS,EpinatM,etal.A
[1]AhsanM,GomesR,DentonA.Applicationofaconvolutionalneuralnetworkusing
prospectivelong-termstudyof220patientswitharetrievablevenacavafilterfor
transferlearningfortuberculosisdetection.In:2019IEEEinternationalconference
secondarypreventionofvenousthromboembolism.Chest2007;131:223–9.
onelectroinformationtechnology(EIT).IEEE;2019.p.427–33.
[24]NiJC,ShpanskayaK,HanM,LeeEH,DoBH,KuoWT,etal.Deeplearningfor
[2]AngelLF,TapsonV,GalgonRE,RestrepoMI,KaufmanJ.Systematicreviewofthe
automatedclassificationofinferiorvenacavafiltertypesonradiographs.JVasc
useofretrievableinferiorvenacavafilters.JVascIntervRadiol2011;22:1522–30.
IntervRadiol2020;31:66–73.
[3]AyadMT,GillespieDL.Long-termcomplicationsofinferiorvenacavafilters.JVasc
[25]ParkBJ,SotirchosVS,AdlebergJ,StavropoulosSW,CookTS,HuntSJ.Feasibility
SurgVenousLymphatDisord2019;7:139–44.
andvisualizationofdeeplearningdetectionandclassificationofinferiorvenacava
[4]CaplinDM,NikolicB,KalvaSP,GanguliS,SaadWE,ZuckermanDA,ofInterven-
filters.2020.medRxiv,2020–06.
tionalRadiologyStandardsofPracticeCommitteeS,etal.Qualityimprovement
[26]Ray CE, Mitchell E, Zipser S, Kao EY, Brown CF, Moneta GL. Outcomes with
guidelinesfortheperformanceofinferiorvenacavafilterplacementforthepreven-
retrievable inferior vena cava filters: a multicenter study. J Vasc Interv Radiol
tionofpulmonaryembolism.JVascIntervRadiol2011;22:1499–506.
2006;17:1595–604.
[5]Charles HW, Black M, Kovacs S, Gohari A, Arampulikan J, McCann JW, et
[27]Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomed-
al. G2 inferior vena cava filter: retrievability and safety. J Vasc Interv Radiol
ical image segmentation. In: Medical image computing and computer-assisted
2009;20:1046–51.
intervention–MICCAI2015:18thinternationalconference,proceedings,PartIII18.
[6]ChuH,DelaArévaloLR,TangW,MaB,LiY,etal.Swinunetrfortumorand
Springer;2015.p.234–41.
lymphnodesegmentationusing3dpet/ctimaging:atransferlearningapproach.In:
[28]SarosiekS,CrowtherM,SloanJM.Indications,complications,andmanagementof
Headandnecktumorsegmentationandoutcomeprediction:thirdchallenge,HECK-
inferiorvenacavafilters:theexperiencein952patientsatanacademichospital
TOR2022,heldinconjunctionwithMICCAI2022,proceedings.Springer;2023.
withalevelItraumacenter.JAMAInternMed2013;173:513–7.
p.114–20.
[29]Scikit-image.skimage0.21.0documentation.https://scikit-image.org/docs/stable/
[7]ÇiçekÖ,AbdulkadirA,LienkampSS,BroxT,RonnebergerO.3du-net:learning
api/skimage.util.html#skimage.util.view_as_blocks,2023.[Accessed22July2023].
densevolumetricsegmentationfromsparseannotation.In:Medicalimagecomput-
[30]Siddique N, Paheding S, Elkin CP, Devabhaktuni V. U-net and its variants for
ingandcomputer-assistedintervention–MICCAI2016:19thinternationalconfer-
medical image segmentation: a review of theory and applications. IEEE Access
ence,proceedings,PartII19.Springer;2016.p.424–32.
2021;9:82031–57.
[8]CrumleyKD,HyattE,KalvaSP,ShahH.Factorsaffectinginferiorvenacavafilter
[31]VanderWaltS,SchönbergerJL,Nunez-IglesiasJ,BoulogneF,WarnerJD,YagerN,
retrieval:areview.VascEndovascSurg2019;53:224–9.
etal.scikit-image:imageprocessinginpython.PeerJ2014;2:e453.
[9]DesaiKR,LawsJL,SalemR,MouliSK,ErreaMF,KarpJK,etal.Definingprolonged
[32]WangG,LiW,OurselinS,VercauterenT.Automaticbraintumorsegmentationusing
dwelltime:whenareadvancedinferiorvenacavafilterretrievaltechniquesneces-
convolutionalneuralnetworkswithtest-timeaugmentation.In:Brainlesion:glioma,
sary?Ananalysisin762procedures.CircCardiovascInterv2017;10:e003957.
multiplesclerosis,strokeandtraumaticbraininjuries:4thinternationalworkshop,
[10]DesoSE,IdakojiIA,KuoWT.Evidence-basedevaluationofinferiorvenacavafilter
BrainLes2018,heldinconjunctionwithMICCAI201,revisedselectedpapers,Part
complicationsbasedonfiltertype.In:Seminarsininterventionalradiology.Thieme
II4.Springer;2019.p.61–72.
MedicalPublishers;2016.p.093–100.
[33]WeiC,RenS,GuoK,HuH,LiangJ.High-resolutionswintransformerforautomatic
[11]DosovitskiyA,BeyerL,KolesnikovA,WeissenbornD,ZhaiX,UnterthinerT,etal.
medicalimagesegmentation.Sensors2023;23:3420.
Animageisworth16x16words:transformersforimagerecognitionatscale.ArXiv
preprint.arXiv:2010.11929,2020.
[12]Durack JC, Westphalen AC, Kekulawela S, Bhanu SB, Avrin DE, Gordon RL,
et al. Perforation of the ivc: rule rather than exception after longer indwelling
9