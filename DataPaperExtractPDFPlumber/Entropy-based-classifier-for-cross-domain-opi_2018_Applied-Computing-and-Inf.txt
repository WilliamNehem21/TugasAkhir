AppliedComputingandInformatics14(2018)55–64
ContentslistsavailableatScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
Original Article
Entropy based classifier for cross-domain opinion mining
Jyoti S. Deshmukha, Amiya Kumar Tripathyb,⇑
aDepartmentofComputerEngineering,PAHERUniversity,Udaipur,India
bDepartmentofComputerEngineering,DonBoscoInstituteofTechnology,Mumbai,India
a r t i c l e i n f o a b s t r a c t
Articlehistory: Inrecentyears,thegrowthofsocialnetworkhasincreasedtheinterestofpeopleinanalyzingreviews
Received30August2016 andopinionsforproductsbeforetheybuythem.Consequently,thishasgivenrisetothedomainadap-
Revised11February2017 tationasaprominentareaofresearchinsentimentanalysis.Aclassifiertrainedfromonedomainoften
Accepted20March2017
givespoorresultsondatafromanotherdomain.Expressionofsentimentisdifferentineverydomain.The
Availableonline22March2017
labelingcostofeachdomainseparatelyisveryhighaswellastimeconsuming.Therefore,thisstudyhas
proposedanapproachthatextractsandclassifiesopinionwordsfromonedomaincalledsourcedomain
Keywords:
andpredictsopinionwordsofanotherdomaincalledtargetdomainusingasemi-supervisedapproach,
Datamining
whichcombinesmodifiedmaximumentropyandbipartitegraphclustering.Acomparison ofopinion
Opinionmining
classificationonreviewsonfourdifferentproductdomainsispresented.Theresultsdemonstratethat
Knowledgediscovery
Expertsystems the proposed method performs relatively well in comparison to the other methods. Comparison of
Informationsystems SentiWordNet of domain-specific and domain-independent words reveals that on an average 72.6%
Machinelearning and88.4%words,respectively,arecorrectlyclassified.
(cid:1)2017TheAuthors.ProductionandhostingbyElsevierB.V.onbehalfofKingSaudUniversity.Thisisan
openaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).
1.Introduction Opinionminingisconstantlygrowingduetotheavailabilityof
views,opinionsandexperiencesaboutaproduct/serviceonline,as
Opinionated text has created a new area of research in text people are shedding their inhibition to express their opinions
analysis. Traditionally, fact and information-centric view of text online. However, automatic detection and analysis of opinions
wasexpandedtoenablesentiment-awareapplications.Nowadays, about products, brands, political issues, etc. is a daunting task.
increaseduseoftheInternetandonlineactivitiesliketicketbook- Opinion mining involves three chief elements: feature and
ing, online transactions, e-commerce, social media communica- feature-of relations, opinion expressions and the related opinion
tions, blogging, etc. has led to the need for the extraction, attributes(e.g.,polarity),andfeature-opinionrelations.Anopinion
transformationandanalysisofhugeamountofinformation.There- lexiconisalistofopinionexpressionsorasetofadjectives,which
fore, new approaches need to applied to analyze and summarize areusedtoindicateopinion/sentimentpolaritylikepositive,nega-
theinformation[14]. tiveandneutral.ThislexiconarisesfromsynonymsintheWord-
Organizations take the review of product given by users seri- Net, while antonyms are used to expand lexicon in the form of
ously,asitadverselyaffectsthesalesoftheproduct.Consequently, graphs. Such a dictionary-based approach has been used to par-
organizationstaketheefforttorespondtothereviews,aswellas tially disambiguate the results of parts of speech tagger. Further,
monitor the effectiveness of its advertising campaigns. In this fuzzylogicisusedtodetermineopinionboundariesandtoadopt
regard, sentiment analysis, a popular method, is used to extract syntactic parsing to learn and infer propagation rules between
andanalyzesentiments[5,4]. opinionsandfeatures[24,13].
Medhatetal.[18]conductedasurveyonsentimentalgorithms
⇑ and its applications and found that sentiment classification and
Corresponding author at: School of Science, Edith Cowan University, Perth,
Australia. feature selection are more prominent areas in recent research.
E-mail addresses: jyoja2007@gmail.com (J.S. Deshmukh), amiya@dbit.in (A.K. Theyalsoreportedthat Supportvectormachineand NaïveBayes
Tripathy). algorithms are the generally used algorithms to classify senti-
PeerreviewunderresponsibilityofKingSaudUniversity. ments, and English is the language used in many resources like
WordNet. Opinions and reviews given on social networking sites
areusedtogeneratedatasetsfortheexperiments.
The WordNet is a generalized lexicon and cannot be used for
Production and hosting by Elsevier
sentiment analysis; therefore, a need arose for the development
http://dx.doi.org/10.1016/j.aci.2017.03.001
2210-8327/(cid:1)2017TheAuthors.ProductionandhostingbyElsevierB.V.onbehalfofKingSaudUniversity.
ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).56 J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64
ofsentimentlexicon. SentiWordNet evolved outof WordNetwas niques yielded in the ensemble framework was proposed by Xia
createdasalexicalresourceforopinionmining.Itassignstoeach etal. [26]. Theyusedtwo typesof featuresets,namely,Parts-of-
synsetofWordNetthreesentimentscores:positive,negativeand speech information and Word-relations and Naïve Bayes, Maxi-
neutral[19,11]. mumEntropyandSupportVectorMachinesclassifiers.Forbetter
Manufacturers, as well as consumers, require opinion mining accuracy, ensemble approaches like fixed combination, weighted
toolstocollectopinionsaboutacertainproduct.Theopinionanal- combination and Meta-classifier combination, were applied. Li
ysis tools can be used by manufacturers to decide a marketing etal.[29]proposedactivelearninginwhichsourceandtargetclas-
strategy for estimating production rate. On the other hand, con- sifiers were trained separately. Using Query By Committee(QBC)
sumerscanusethesetoolstomakedecisiononbuyinganewpro- selectionstrategy,informativesampleswereselected,andclassifi-
ductortakeatriptovacationlocations,orselecthotel,etc. cationdecisionweremadebycombiningclassifiers.Labelpropaga-
Labeledopinionsareusedtoanalyzetheclassifier.Practically, tion was used to train both classifiers. The result demonstrated
labeledopinionsfor every domainisnotpossible,as it delimited thatsignificantlyoutperformedthebaselinemethods.
by time and cost, while domain adaptation or transfer learning Most often, opinions are given in the natural language. One
couldbeusedtocircumventthislimitation.Inthispaper,wepro- majorissuewithnaturallanguageistheambiguityofwords.Fer-
posetheapproachofdomainadaptablelexiconwhichpredictsthe sinietal.[10]appliedBayesianensemblemodelinwhichuncer-
polarityoflexiconofonedomainusingasetoflabeledlexiconof tainty and reliability was taken care. Greedy approach was used
another domain using a modified entropy algorithm. This algo- for classifier selection, while gold standard datasets were used
rithm uses enhanced entropy with modified increment quantity for experimental analysis. However, classification performance is
insteadoftraditionalentropyalgorithm.Datasetofdifferenttypes frequentlyaffectedbythepolarityshiftproblem.Polarityshifters
of products containingtextualreviews has been used for evalua- are words and phrases that can change sentiment orientation of
tion. Multiple experiments were carried out to analyze the algo- texts.Xiaetal.[28]addressedthisissueusingthree-stagemodels
rithm using accuracy and F-measure. We designed the approach whichincludedetectionofpolarityshift,removalofpolarityshifts
intwophases:(i)preprocessingofdatasetand(ii)applyingclassi- and sentiment classification. Onan et al. [2] proposed the weight
fierandclusteringondataset. based ensemble classifier, in which weighted voting scheme was
Therestofthepaperisstructuredasfollows.InSection2,we usedtoassignweighttoclassifier.AsabaselearnerBayesianlogis-
describe the related work on domain adaptation approaches. In tic regression, Naïve Bayes, linear discriminant analysis, logistic
Section 3, we introduce our new improved entropy based semi- regressionandSupportvectormachineareused.Adifferenttype
supervisedapproach.InSection4,weevaluateourapproachusing of experimental analysis shows better result than conventional
cross-domain sentiment classification tasks, and compare it with ensemble learning. Da Silva et al. [8] used classifier ensembles
otherbaselinemethods.Finally,inSection5wedrawconclusions formedbydifferentclassifierwhichisapplicabletofindproducts
ontheproposedapproachandsetdirectionsforfuturework. ontheweb.Augustyniaketal.[16]demonstratedTwitterdataset
tohavegoodaccuracyonlyforpositiveandnegativequeries.They
foundthatBagofWordswithensembleclassifierperformsbetter
2.Relatedwork thansupervisedapproach.
Identificationoffeatureandweightingisanimportantstepin
The text documents containing opinions or sentiments were opinion mining. Khan et al. [12] proposed a new approach that
classifiedbasedontheirpolarity,i.e.whetheradocumentiswrit- identified features and assigned term label using SentiWordNet.
ten with a positive approach or a negative approach. Although In this method, point wise mutual information and chi square
machine learning approach uses a word’s polarity as a feature, approaches were used to select features to SentiWordNet that
thepolarityofsomewordscannotbedeterminedwithoutdomain were weighted. Support vector machine was used as classifier.
knowledge.Hence,thereusabilityoflearnedresultofadomainis Experimental evaluation on benchmark dataset shows effective-
essential.Transferlearning,alsoknownasdomainadaptation,can nessofapproach.
be used to address this challenge. Transfer learning utilizes the Socialnetworkingsitescontainstextdatainlongformataswell
results learned in a source domain to solve a similar problem in asshortmessageswithsymbols,emoticonsetc.Opiniondetection
another target domain [22]. Approaches used to classify single inlongreviewsiseasythanshortreviews,asshortreviewscontain
and cross-domain polarity opinions are usually a bag of words, fewer features, and more symbols, idioms etc. hence difficult to
n-gramsorlexicalresource-basedclassifiers. extract opinion. Lochter et al. [15] proposed ensemble approach
The main aim of domain adaptation is to transfer knowledge totacklethisissue.Thisapproachusedtextnormalizationmethods
across domains or tasks. Tagging the opinion word and building to improve the quality of features. The features thus filtered and
a classifier is time consuming and expensive, as opinions are enhancedservedastheinputformachinelearningalgorithms.Pro-
domaindependent.Normally,usersexpresstheiropinionsspecific posedframeworkwasevaluatedusingrealandnon-codeddatasets
toaparticulardomain.Anopinionclassifiertrainedinonedomain andconcludedthatthisapproachwas superiortoothermethods
maynotworkwellwhendirectlyappliedtoanotherdomaindueto with a 99.9% confidence level. However, this approach was sug-
mismatchbetweendomain-specificwords.Thus,domainadapta- gestedtobeexpensivefor offlineprocessesdue tohighercost of
tionalgorithmsareextremelydesirabletoreducedomaindepen- computingpower.Hence, parallelizationof thisprocesshas been
dency and labeling costs. Sentiment classification problem are statedasfutureworkbytheauthors.
consideredas afeatureexpansionproblem,in whichrelatedfea- Sparseness is another issue in short text data. Word co-
tures are appended to reduce mismatch of features between the occurrenceandcontextinformationapproachesaregenerallyused
twodomains.Toovercomethisproblem,sentiment-sensitivethe- forsolvingsparsenessissue.Theseapproachesarelessefficient.To
saurus,whichcontainsdifferentwordsandtheirorientationindif- addressthisproblem,Chutaoetal.[32]consideredprobabilitydis-
ferentdomains,hasbeencreated.Bollegalaetal.[7]usedlabeled, tributionoftermsastheweightofterms.
aswellasunlabeleddata,forevaluation.Theresultssuggestedthat Similar to ensemble classifiers, graph-based methodology are
methodperformssignificantlywellcomparedtobaseline. alsousedfor domainadaptation. Dhillonetal. [25]proposedthe
To overcome domain adaptation issue, various adaptation graph-based domain adaptation method. Similarity graphs were
methodshavebeenproposedinthepast,e.g.,ensembleofclassi- constructed between features from all domains, if these features
fiers. Combination of various feature sets and classification tech- weresimilar then it demonstrated the presenceof edge betweenJ.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64 57
them.Alllabeledfeatureswereusedinmetric-learningalgorithms. well as the generalization of the information. Hence, generalized
Graph was constructed using data-dependent metric and the domainadaptablealgorithmsareneededfortheautomaticidenti-
weightwascalculatedforeachedge.Experimentalresultsdemon- ficationandclassificationofopinionlexicons.
stratedthereductionofclassificationerror. Domain adaptability is a major issue in sentiment analysis or
PanandYang[22]andWangandShi[31]focusedonbridging opinionmining,whichhasbeenaddressedintheproposedframe-
the gap between domain-specific and domain-independent lexi- work.Therearemanyresourcesandtrainingcorporaavailablein
cons,astheseapproachesdonotworkwellwhenappliedtotwo Englishwithprovenresults.Aproposedmodelwillbetrainedfrom
extremelydifferentdomains.SinghandHusain[30]presenteddif- atrainingdataset,whichwillbeusedforsentimentclassification.
ferentdatasetsusedinsentimentanalysisaswellasclassification SentiWordNetresourcewillbeusedforthisresearchasitisapub-
andclusteringmethods.Thisreviewrevealsthatsamemethodis liclyavailableforopinionlexiconswithpolarity.
notapplicableforalldomains.Fromtheanalysisoftheliterature, Anopinionlexiconisoneormorewordswithpositiveornega-
itcanbesummarizedthatNaïveBayesworkswellforortextclas- tiveorientation.Lexiconsareusedwhennotrainingdataareavail-
sification,clusteringforconsumerservices,andSVMforbiological ablebecausethetrainingdatacontainpriorknowledgeaboutthe
reviewandanalysis. sentimentofafeature.Itisavitalcomponentofunsupervisedsen-
Training and testing data from same feature space and same timentclassificationmethods.Theconstructionofalargesizedlex-
distribution has been reported to give good results for machine icon is an expensive and time-consuming task. Hence, building
learning algorithms. Estimatingthe effectof distributionchanges automated methods that influence existing resources to expand
through statistical models is reported to be very expensive, as it existinglexiconsareneeded.
hastoberebuiltfromscratch.Inmanyrealworldapplications,it Domain adaptation of sentiment models from a domain with
is expensive or impractical to recollect the needed training data sufficient labeled data to a new domain with less labeled data is
andrebuildthemodels.Insuchcases,domainadaptationortrans- achallengethatrequiresnewandefficientalgorithmstosolveit.
ferlearningbetweentaskdomainswouldbedesirable. Theproposedsystemhasconstructedadomainadaptablelexicon
Toovercometheproblemoffeaturedistributionvarianceacross which can adapt seamlessly from one domain to another. The
domains,XiaoandGuo[21]proposedafeaturespaceindependent expectedoutcomewouldbeasetoflexiconswithpolaritiesfordif-
semi-supervised kernel matching method, based on a Hilbert- ferentdomainswithdevelopmentofrobustmodel.
Schmidt Independence Criterion. Two kernel matrices were cre- Labeledsetofdocumentsfromsourceandlabeledorunlabeled
ated over the instances in the source domain and the instances documents from target domain is taken as input (Fig. 1). Prepro-
in the target domain. Each labeled instance in the target domain cessing is done to eliminate unnecessary words called as stop
wasdefinitelymappedintoasourceinstancewiththesameclass words.Theirrelevantdatawouldbeeliminatedbythisprocess.
label through prediction function. Evaluation of the proposed MostoftheEnglishsentencesincludewordslike‘‘a,an,of,the,I,
methodperformedonAmazonproductreviewsandReuters’mul- it,you,etc.”Suchwordsdonotcarryanyparticularmeaning.Infor-
tilingualnewswirestoriesshowedreductioninhumanannotation mation extraction from natural language can be done effectively
efforts. and clearly by avoiding those words which occur frequently. To
The Lexicon based approach works with the polarities of the
opinion-oriented words and relies on a lexicon. A collection of
known terms that contribute to the sentiment of a text is called
sentimentlexicon.Manyopensourcelexiconsareavailablewhich
serve as a database for extracting the polarity values of opinion
words.Butthesegenericpolaritylexiconsreflectthemostgeneric
sentimentofopinionwords.Anopinionwordneednotexpressthe
samesentimenteverywhere,i.e.,opinionwordscouldbecontext-
dependent or domain-specific. The word ‘‘small” in ‘‘room is too
small”indicatesanegativeopinion,whereasin‘‘smallscreensize”
asseeninthemobiledomainindicatesapositiveopinion.
The variation of opinion found for the same word in different
domains restricts the usage of generic lexicons as it generalizes
the polarity of a word. Therefore, lexicons with updated polarity
valuesthatcangivepolarityofasamewordindifferentdomains
using same lexicon database will have to be built. The proposed
workattemptsinbuildingsuchanenhancedpolaritylexiconusing
themaximumentropyalgorithmwithmodificationbeingmadein
incrementquantitywhichhelpsinrefiningtheclassificationgran-
ularityfromdocumenttowordlevel.Theknowledgegainedfrom
onedomainisusedtopredictandclassifythepolarityofopinion
words from another domain, resulting in an improved lexicon
using semi-supervised approach. The common words from all
domainshavingsamepolarityorientationaretreatedasdomain-
independentwordsandremainingasdomain-specificwords.
3.Proposedframework
3.1.Genericprocesses
Most of the existing research regarding opinion mining is
domain dependent, which limits the scope of the application as Fig.1. Workflowofproposedsystem.58 J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64
P
removestopwordsfromsentences,atextfilethatconsistsoflistof Pðcjd;kÞd¼efP exp iPk if iðc;dÞ ð2Þ
Englishstopwordsisused. exp kf ðc0;dÞ
c02C i i i
Aftertheremovalstopword,partsofspeechlikenoun,adjec-
where k¼k þd ðkiscalculatedbyaniterativescalingalgorithmÞ
tive,adverb,verb,etc.areextractedusingtheparser.Parsingisa i i
d isincrementquantity
vitalstepasitgivesopinionwordsasanoutput.Sentenceparsing i
As the granularity of classification is refined from document-
involves assigning different parts of speech tags to a given text.
level to word level, the increment quantity (d) is modified. The
ThisprocessisknownasPart-Of-Speech(POS)tagging.Forinfor- i
modifiedquantity(dm)isdefinedas
mationextraction,POStaggingisimportantbecauseeachcategory i
!
plays a specific role within a sentence. Nouns give names to
1
Xk
objects, or entities from reviews. An adjective describes opinion. dm ¼ log idf ; ð3Þ
i M i
Also, some verbs and adverbs can play an important role as an i
adjective. P
whereM¼max kf ðd;cÞ.
i i
Eq. (3) calculates inverse document frequency of each word,
Examples:.
whichisapopularmeasureofwords’importance.Itisdefinedas
the logarithmicratio of the number of documents in a collection
tothenumberofdocumentscontainingthegivenword.Thissug-
the/DTbattery/NNlife/NNon/INthe/DTiphone/JJ4S/CDis/VBZ geststhatuncommonwordshavehigheridf iandcommonfunction
amazing/JJ words have lower idf i, where idf i is inverse document frequency.
this/DTphone/NNis/VBZvery/RBslow/JJ Thisisusefultomeasurethewordsabilitytodiscriminatebetween
documents.Misthesumofallfeaturesintraininginstance.Fea-
turevalueistakenastfidf.
i
In pre-processing step, text review is first divided into sen- Algorithmworksonwordlevel.POStaggedwordsareextracted
tences.StanfordparserisusedtogeneratethePOStaggingofeach frompreprocessingstepsareused.Eachwordactsasfeature.Fea-
wordpresentinthesentence[9],asitisessentialtofindgeneral turevaluefiðd;cÞiscalculatedastfidf.ofeachword.AsperEq.(3)
i
languagepatterns.
inversedocumentfrequencyofeachPOStaggedwordiscalculated.
Adjectivesandadverbsaregoodindicatorsofopinion,henceare
Algorithm is executed for total number of features provided in
extracted from each review. Some verbs are also considered as
input dataset. According to this weight words are classified into
opinion, e.g., like, love, recommend, etc. Two consecutive words,
two categories. Classified words are having POS tag, polarity tag,
i.e., adverb-verb, adverb-adjective also extracted from processed
and weight value. Fromthis list, common and uncommonwords
tagged reviews as a verb alone does not indicate opinion. Nouns
arepickedandusedforbipartitegraphclusteringexplainedbelow.
arenotconsideredinframework.
Probability distribution of class c is calculated based on term
AlltaggedwordsafterPOStaggingphasetaggedwordsareclas-
frequency.Classificationprocesspredictsthepolarityofthetarget
sifiedusinganalgorithmexplainedinSection3.2.
domainslexiconfromsourcedomain.Theclusteringalgorithmis
appliedonclassifiedwordlistsanddocumentsuntilitreachescon-
3.2.Algorithm
vergence.Allextractedwordsfromsourcedomainaretagged,and
weightiscalculatedforeachwordusingmutualinformationavail-
Classification of opinions can be done using a modified maxi-
ableforwords.Targetwordsareextractedandcomparedwiththe
mum entropy algorithm. The increment quantity is modified source. If they match then they will be categorized as domain-
accordingtotheimportanceofthemeasureofwordsasspecified
independent, otherwise domain-specific. Domain-independent
in Eq. (3). The maximum entropy classifier is closely related to words are from both source and target domains; whereas,
theNaïveBayesclassifier,exceptthatitusesasearch-basedopti-
domain-specific are from target domain only. A graph is con-
mizationtofindweightsforthefeaturesthatmaximizethelikeli- structed between domain dependent and domain-independent
hoodofthetrainingdata.Itcanhandlemixtureofboolean,integer,
words. Co-occurrence relationship between these words repre-
andreal-valuedfeatures[17].Itisalsousedwhentheconditional sents edge. Occurrence of domain-specific word along with
independenceofthefeaturescannotbeassumed,i.e.,inproblems
domain-independentwordmeansthatbotharelatedtoeachother
liketextclassificationwherefeaturesarewordsandarenotinde- and assign edge. Using domain-independent words weight is
pendent[1].
assignedtodomain-specificwordsandclassifiedaccordingly.Each
The main aim of the study is to construct a stochastic model fileformtargetdomainisassignedscoreonwhichbasisitisclas-
that accurately represents the behavior of the random process.
sifiedaspositiveornegative.Eachwordhasweightassignedtoit.
Letdbedocumentinadataset;w,thewordpresentindocument; Summation of weights of words in each sentence gives score to
andc,theclass.
sentence.Thenadditionofallsentencescoreisnothingbutscore
offile.Onthebasisofthis,fileisclassified.
1. Foreachwordwandclassc2C,ajointfeaturepðw;cÞ¼f(w,c)
Clustering helps in reducing mismatch between domain-
=Nisdefined,whereNisthenumberoftimesthatwoccursina
specificwordsofsourceandtargetdomains.Twosetsoflexicons
documentinclassc.(Ncouldalsobeboolean,registeringpres-
areextractedasanoutputwithpolaritywhichiscomparedwith
encevs.absence.)
SentiWordNet(Fig.2).
2. Empirical distribution is used to build the statistical model of
therandomprocess,whichdistributestexttospecificclass.
4.Resultanddiscussions
Z
1
f iðd;cÞ¼ if c¼c i&dcontainsw k otherwise ð1Þ 4.1.Experiment1
0
Above indicator function called as feature. Via iterative optimiza- The dataset from John et al. [6] was used for experiments. It
tion, assign a weight to each joint feature so as to maximize the contains a collection of product reviews from Amazon.com. This
log-likelihoodofthetrainingdata. dataset contains three types of files positive, negative and unla-
3. Theprobabilityofclasscgivenadocumentdandweightskis beledinXMLformat.Eachlineinformof:feature:<count>....feaJ.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64 59
Table2
Comparativeanalysisofaccuracyofproposedmethodandbaselinemethods.
Source!Target Accuracy(%) Accuracy Accuracy Accuracy
ProposedMethod (%)SS-FE (%)SFA (%)SWC
B!D 82.45 79.10 82.55 81.66
B!E 78 74.24 72 77.04
B!K 78.65 78.07 78 82.26
D!B 74.35 80.38 77 79.95
D!E 79.78 77.07 77 76.98
D!K 84.21 77.82 81 82.13
E!B 82.15 72.86 75.5 72.11
E!D 87.8 74.60 77 73.81
E!K 81.44 84.87 87.1 85.33
K!B 81.05 72.94 74 75.78
K!D 70 75.70 77 76.88
K!E 88.35 82.93 84.6 84.78
Fig.2. Flowofproposedalgorithm.
ture:<count>#label#:<label>, e.g., old_boy:1 i_am:1 the: 1 boy_-
had:1 so_i:1 #label#: negative. These files were extracted using
XML file splitter and reviews were converted into text file. The
datasetcontains1000positivefilesand1000negativefilesforeach
domain. The reviews are about four product domains: Books (B),
Fig.3. Accuracyanalysis.
DVDs(D),Electronics(E)andKitchenappliances(K)andarewrit-
ten in English language. For experiment, labeled dataset of 1000
positive and 1000 negative files was used. An instance in each firstitclassifiesthewordsanddocuments,andthenclustersthem.
domainisrecordedinTable1.Exceptbookdomainotherdomains After classification step, opinionated words are extracted with
hadmorenumberofpositiveinstances. weightvalueaswellaspolarity.Theseweightsareveryimportant
From this dataset, 12 cross-domain sentiment classification factorasitincreasestheimportanceofdiscriminativeterms.Pro-
tasks were constructed: B!D;B!E;B!K;D!B;D!E; posed approach also identifies domain-independent and specific
D!K;E!B;E!D;E!K;K!B;K!D;K!E, where the features which are used for clustering. All classified words clus-
word beforeanarrow correspondstothe sourcedomainand the tered again leads to acceptable results. One of the reasons for
wordafteranarrowcorrespondstothetargetdomain. low accuracy for some domain is imbalance of class labels and
FromTable2,itisevidentthatBookandDVD,ifconsideredasa thepresenceofworddisambiguation.
sourcedomain,achieveagoodcompatibilitywithelectronicsand Accuracyisusedasanevaluationmeasure.Accuracyisthepro-
kitchen domain, which is considered as target domain. Besides, portion of correctly classified examples to the total number of
electronicandkitchenarecompatibledomains. examples;ontheotherhand,errorratereferstoincorrectlyclassi-
Baselinemethodsuse in this studyare Feature Ensemble plus fiedexamplestocorrectlyclassifiedexamples.F-measureorpreci-
Sample selection (SS-FE) [27], Spectral feature alignment (SFA) sionandrecallcanbeusedasevaluationmeasures.
[23], and Supervised word clustering (SWC) [20]. SFA achieved F-measure is only defined in terms of true positive (TP), false
was between 72.5% and 86.75%, SS-FE was between 72.94% and positive (FP) and false negative (FN), while true negative (TN) is
84.87%andSWCwasbetween72.11%and85.33%,whereasaccu- notconsidered.AccuracyandF-measureiscomparedforproposed
racyofproposedalgorithmwasbetween70%and88.35%. approach which shows that, in general, F-measure is similar to
OnlytheDVD,electronics,andkitchenwereconsideredasthe accuracy.ButonlysingleclassisconsideredinF-measureasposi-
sourcedomain,whilebook,kitchenandDVDas atargetdomain, tive class (Fig. 4). On the other hand, when calculating accuracy
producing comparatively less accurate results than the baseline equalweightisgiventoboththeclasses.
method(Fig.3).Therearetwokeypointsinproposedframework: Classified words are used to find domain-independent and
domain-specific words from the respective domains. Domain-
independentanddomain-specificwordsarecomparedtotheSen-
Table1 tiWordNet[3], in orderto findouthow manywordsmatchwith
Negativeandpositiveinstancesformulti-domaindataset. them(Tables3–6). Fromthetables, ithas beenobservedthat on
DomainName NegativeInstances PositiveInstances an average 72.6% domain-specific words are correctly classified
for different domains; while88.4% words from domain-
Book 73,500 72,794
independent word list are correctly classified. Domain-
DVD 66,126 76,759
Electronics 43,806 44,321 independentwordstypicallyoccurineverydomain;hence,match-
Kitchenappliances 36,106 36,733 ingpercentageismorethanthematchingpercentageofdomain-60 J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64
Fig.4. AnalysisofaccuracyvsF-measure.
Table3
Comparisonofdomain-specificanddomain-independentwordsagainstSentiWordNetconsideringbook(B)asasourcedomain.
Domains Domain-specificwords Domain-independentwords No.ofwordsmatchingSentiWordNet
Domain-specificwords Domain-independentwords
B?D 11,503 9744 8934 8972
B?E 5250 4796 4077 4395
B?K 4200 4325 3262 3979
Table4
Comparisonofdomain-specificanddomain-independentwordsagainstSentiWordNetconsideringDVD(D)asasourcedomain.
Domains Domain-specificwords Domain-independentwords No.ofwordsmatchingSentiWordNet
Domain-specificwords Domain-independentwords
D?B 11,130 9744 8644 9009
D?E 5238 4781 4068 4418
D?K 4205 4320 3266 3980
Table5
Comparisonofdomain-specificanddomain-independentwordsagainstSentiWordNetconsideringElectronics(E)asasourcedomain.
Domains Domain-specificwords Domain-independentwords No.ofwordsmatchingSentiWordNet
Domain-specificwords Domain-independentwords
E?B 16,105 4769 12,508 4430
E?D 16,466 4781 12,789 4462
E?K 4802 3723 3729 3454
Table6
Comparisonofdomain-specificanddomain-independentwordsagainstSentiWordNetconsideringKitchen(K)asasourcedomain.
Domains Domain-specificwords Domain-independentwords No.ofwordsmatchingSentiWordNet
Domain-specificwords Domain-independentwords
K?B 16,549 4325 12,853 3980
K?D 16,927 4320 13,147 3987
K?E 6296 3723 4890 3449
specificwords.AsSentiWordNethasgeneralizedopinionlexicons, tronics,kitchenandDVDdomainsarecompatiblewitheachother
percentageofmatchingdomain-specificwordsisrelativelyless. duetotheirmoresimilarfeatures.Ingeneral,thefeaturesofthese
threedomainsaremoreorlesssimilar.Alsokitchenassourceand
electronics as target and vice versa gives better accuracyas both
4.2.Experiment2 domainssharemorecommonfeatures.Kitchenappliancesdomain
also shares electronics appliances; hence, major information of
Toevaluateaccuracyofproposedalgorithmforunlabeledtarget bothdomainsissimilar.ButkitchenassourceandDVDastarget
dataset, we have performed 12 cross-domain tasks with labeled doesnotgivegoodresultsforbothlabeledandunlabeleddataset,
source dataset and unlabeled target dataset. Fig. 5 illustrates the becausekitchenandDVDarenotsimilartoeachother,asthatof
accuracy analysis. Accuracy achieved by proposed method for kitchenandelectronics.Usually,iftwodomainsaremoresimilar
unlabeledtargetliesbetween65.65%and98.0%;whereas,labeled then a number of features transferred from source to target are
targetachievesaccuracybetween70.0%and88.35%.Performance alsomorebecausesourcedataisusedastrainingdatasetandtar-
of classifier, therefore, increases significantly. It shows that elec-J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64 61
forsome domains.Theresultshowsthat theproposedalgorithm
wasbetterthanbothSVMandNaïveBayes.
Figs.6and7providesaccuracyandF-measureanalysisforeach
of12cross-domainsentimentclassificationtasksonAmazonpro-
duct reviews. For this study, 1000 positive and 1000 negative
reviewsweretakenasthesourceandtargetdomains.Comparison
of proposed approach with baseline approaches shows that
domainadaptationfrombookassourcedomaintotheDVDastar-
getdomainratherthankitchenassourcetoDVDastargetdomain
ismorefeasible.Italsoshowsthatrelatednessbetweensourceand
targetdomainreviewsaremoreimportantfactorsfortheeffective-
nessofdomainadaptation.
Electronicsasasourcedomainismorecompatiblewithevery
domain.This suggeststhat more numberof features are relevant
in both source and target domains. The proposed approach pro-
videsthehighestaccuracyforelectronicsascomparedtobaseline
aswellasSVMandNaïveBayes.Comparedtoaccuracy,F-measure
resultsareimprovedbuttheseareonlyrelatedwithpositivedoc-
uments. Results show that the proposed approach have better
accuracy and F-measure. Further, Naïve Bayes provides better
Fig.5. Accuracyanalysisofunlabeledandlabeledtarget. resultsthan SVM. Some ofthe majordrawback ofNaïve Bayes is
assumptionofindependentattributesanddifficultyininterpreta-
get features are derived from it. The impact of unlabeled target tions of SVM results. In proposed framework, maximum entropy
data is more than that of labeled target data. It states that unla- thatisusedprovidesanaturalmechanismofmulticlassclassifica-
beled target data can be used for the accuracy gain, as well as tion.Theresultsarebetterasincrementquantityfocusesonterm
reducetheannotationcostsignificantly. frequencyandinversedocumentfrequency.Itconcentratesonfea-
turesanditspresencewhichisnotfocusedinNaïveBayesorSVM.
4.3.Experiment3 Random Trees area collection of individual decision trees, in
which each tree is generated from different samples and subsets
Generally, Naïve Bayes and SVM algorithms are used for text of the training data. Classification of dataset based on random
classification. Experiment 3 was conducted to evaluate proposed sub selection of training samples result in many decision trees,
framework. using Rapid Miner 5.3.015 software for Naïve Bayes hencethismethodiscalledRandomtrees.Eachtreecanbevoted
and SVM algorithm,. The software contains text mining plug-in tomakefinaldecision.
whichconvertsnon-structuredtextualdataintostructuredformat AnexperimentwascarriedusingRapidMiner5.3.015software.
forfurtheranalysis.ThisstudyadoptedimplementedlinearSVMas Results are recorded in Fig. 8, which shows the comparison
mostofthetextclassificationproblemsarelinearlyseparable.Fur- between the proposed approach and the Random tree. The pro-
ther,astextclassificationcontainslargenumberoffeatures,linear posed approachgives better accuracythan the Random tree. The
kernelwasfoundtobebettersuitedforthispurpose.Resultswere randomtreesclassifiertakestheinputfeaturevector,classifiesit
obtained from Blitzer dataset for four different domains of uni- with every tree in the forest, and produces the class label that
gramsandforapplyingwordfrequencyindocumentandinentire received the majority of ‘‘votes” as output. Using bootstrap
corpus. approach, training sets are generated. Vectors are randomly
NaïveBayesclassifierisaprobabilisticclassifierbasedonprob- selected, hence some vectors will occur more than once or some
abilitymodelsthatincorporatestrongindependenceassumptions willbeabsent.Allvariablesarenotusedtofindthebestsplit.
amongthefeatures.Independenceassumptionoffeaturesisasub- Incontrast,theproposedapproachworksatwordlevelwhere
tleissuewithNaïveBayes.Ifcertainfeatureandclasslabelvalue eachwordactsasafeature.Later,importanceofwordisanalyzed
do not occur together then the frequency-based probability esti- using term frequency and inverse document frequency of each
matewillbecomezero.Whenalltheprobabilitiesaremultiplied, wordconsequentlyproducingbetteraccuracy.UsingRandomtree
the answer will be zero and this affects the posterior probability higher accuracy achieved in Electronics as source and kitchen as
estimate. Hence, it provides poor accuracy compared to the SVM target domain wherein the proposed approach it is reverse way.
Fig.6. Accuracyanalysisfor12cross-domainclassificationtasks.62 J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64
Fig.7. F-measureanalysisfor12cross-domainclassificationtasks.
Fig.8. AccuracycomparisonwithRandomTree.
Therefore,itisclearthatelectronicsandkitchenaremorecompat- into six topic subdirectories: Books, Camera, DVD, Health, Music
ibledomainsastheyarehavingsimilarfeatures. andSoftware.Thedocumentineachtopicdirectoryisdividedinto
positiveandnegativesubdirectories.Fromthisdataset,30classifi-
4.4.Experiment4 cationtaskswereconstructed.For1000positiveand1000negative
filesofeachdomain,theaccuracywasachievedbetween70%and
For testing the model, Amazon’s balanced 6cats dataset col- 97%(Fig.9).
lected by Mark Drezde and processed by Richard Johansson in Highestaccuracyachievedinsoftwareassourceandcameraas
2012hasbeenusedforthisstudy.Thereviewcollectionisdivided target domain. B!C;C!B;D!M; H!S;M!B;S!C clas-
Fig.9. AccuracyanalysisforAmazonbalanced6catsdataset.J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64 63
Table7 toeachotherorrelatednessbetweenthesedomainsisless,hence
Domain-independent,Domain-specificandSWNmatchedwordsforBookassource theresultsarelower.
domain.
Domains Domain-specific Domain-independent SWNmatched
words words words 5.Conclusions
B?C 3942 4459 4533
B?D 12,044 9896 10,358 Opinionminingisapopularresearcharea;yet,researchershave
B?H 4356 4468 4444 mainly focussed on domain adaptation. This work addressed the
B?M 10,304 7438 7910
major issue of domain adaptation. In this work, semi-supervised
B?S 4712 5222 5356
approachwasusedwhichholdsmaximumentropyclassifierwith
modifiedincrementvalueandbipartiteclustering.Labeledaswell
asunlabeledsetoflexiconsfromdifferentdomainswerecollected
Table8
fromAmazonareusedfor experimental analysisoftheproposed
Domain-Independent,Domain-SpecificandSWNmatchedwordsforDVDassource
domain. approach. Pre-processing step was used to remove noise from
dataset.Eachwordfromdatasetistaggedforpartsofspeechusing
Domains Domain-specific Domain-independent SWNmatched
theStanfordparser.Thistaggeddataisusedbyclassifierwhichis
words words words
basedonfeaturestfidf andidf,.valuewhichisusefultomeasure
D?B 10,519 9896 10,259 i i
the words’ ability to discriminate between documents. Domain-
D?C 3770 4631 4670
D?H 4306 4518 4481 specificanddomain-independentlexiconsareusedforclustering.
D?M 9305 8437 8883 Classified lexicons are compared with SentiWordNet 3.0 to find
D?S 4741 5193 5342 matchingpercentageasSentiWordNetispubliclyavailablelexicon
resource.
Thisworkwasabletoproducerelativelygoodresultsforsome
Table9 ofthedomain,anditwasabletohandleonlytwoclasseswithan
Domain-independent, Domain-specific and SWN matched words for Camera as acceptable accuracy. Domain-specific and domain-independent
sourcedomain. words compared to SentiWordNet 3.0 shows average matching
Domains Domain-specific Domain-independent SWNmatched percent 68.25%. The experimental results of proposed approach
words words words haveshownasignificantincreaseinaccuracyfordifferentdomains
C?B 15,956 4459 5049 overbaselineapproachastheproposedframeworkemphasizeson
C?D 17,309 4631 5264 granularity of the word. This is the major change in classifier in
C?H 5186 3638 3652 comparisontotraditionalapproach.Importanceofeachwordthat
C?M 13,845 3897 4410
has more impact on results of classifier was classified. Testing of
C?S 6029 3905 4142
approachcarriedonAmazoncat6dataset,whichshowsasignifi-
cant improvement in accuracy ranging from 3 to 6 points com-
pared to dataset from Blitzer. In comparison to SVM and Navie
sification tasks are giving high accuracy. From the results, it was
Bayes, we have proposed an algorithm that could provide better
foundthatbook,cameraandmusicdomainsarehavingmorecom-
accuracy. It shows that relatedness between domains is a major
monfeatures.
factorforeffectivenessofdomainadaptation.
Tables 7–9 present domain-specific, domain-independent and
Intheproposedsystem,bipartitegraphclusteringwasusedto
matchingwordswithSentiWordNet. Domain-independent words
reduce the mismatch between domain specific words of source
arefrombothsourceandtargetdomains.Domain-specificareonly
domainandtargetdomain.Domain-independentwordswereused
from target domains. With SentiWordNet matching percent is
toclusterdomain-specificwordsfromsourceandtargetdomains.
average55.7%.
To train classifier for target domain, clustering was used as it
reduced the gap between domain-specific words of different
4.5.Discussion domains. Future studies can be taken up to determine the co-
clustering of words and documents from different domains. The
For our experiments, two different datasets were used. Each proposed system focuses on only words, in futurenon-word fea-
datasetconsistedoflabeledpositiveandnegativetextreviewdoc- turesliketheageofdocument,therecommendationcountsofdoc-
uments. All the results from above sections reveal that the pro- ument can be considered. At present, framework considers only
posed approach gives better accuracy than baseline methods. unigramsandreviewsareinEnglishlanguage.Alsoinfuturethis
Word is important entity as it indicates sentiment or opinion of workcanbeextendedforotherlanguagesaswellasn-grams.
object.Theproposedframeworkisbasedonmodifiedentropyclas-
sifier. Opinionated words are extracted based on the term fre-
References
quency and inverse document frequency. Increment quantity is
modified as granularity refined from document to word level
[1] Abinash Tripathy, Ankit Agrawal, Santanu Kumar Rath, Classification of
which shows drastic difference between traditional maximum sentiment reviews using n-gram machine learning approach, in: Expert
entropyandmodifiedentropy.Bipartitegraphclusteringisapplied SystemswithApplications,vol.57,2016,pp.117–126.
onclassifieddatawhichhasenhancedtheresults. [2] AytugOnan,SerdarKorukoglu,HasanBulut,Amultiobjectiveweightedvoting
ensemble classifier based on differential evolution algorithm for text
As compared to baseline methods, moderate accuracy was sentimentclassification,in:ElsevierExpertSystemsWithApplications,vol.
achieved by the proposed method. Relatedness between source 62,2016,pp.1–16.
andtargetdomainisimportantfactorindomainadaptation.Also [3] BaccianellaStefano,EsuliAndrea,SebastianiFabrizio,SentiWordNet3.0:An
Enhance Lexical Resource for Sentiment Analysis and Opinion Mining, in:
forunlabeledtargetdatasetbetteraccuracywasachieved.Itmeans
Proceedingsofthe7thLanguageResourcesandEvaluationConference(LREC
thatitcansignificantlyreducetheannotationcostalso.F-measure 2010),Valletta,Malta,May17–23,2010,pp.2200–2204.
isalsotakenasevaluationmeasurewhichshowsbetterresultsof [4] A.Bermingham,M.Conway,L.McInerney,N.O’Hare,A.Smeaton,Combining
Socialnetworkanalysisandsentimentanalysistoexplorethepotentialfor
proposedframeworkoverbaselinemethods.Butitdoesnotcon-
onlineradicalisation, in:Proc.ofInt’lConf.onAdvancesinSocialNetwork
siderthetruenegativefeatures.Somedomainsarenotcompatible AnalysisandMining,Athens,Greece,July20–22,2009,pp.231–236.64 J.S.Deshmukh,A.K.Tripathy/AppliedComputingandInformatics14(2018)55–64
[5] Bing Liu, Sentiment Analysis & Opinion Mining, Kindle Edition., Morgan & [18] W. Medhat, A. Hassan, H. Korashy, Sentiment analysis algorithms and
ClaypoolPublishers,2012. applications:asurvey,AinShamsEng.J.5(4)(2014)1093–1113.
[6] JohnBlitzer,MarkDredze,FernandoPereira,Biographies,Bollywood,boom- [19] G.A.Miller,WordNet:alexicaldatabaseforEnglish,Commun.ACM38(11)
boxes and blenders: domain adaptation for sentiment classification, in: (1995)39–41.
AssociationofProceedingsofthe45thAnnualMeetingoftheComputational [20] MinXiao,FeipengZhao,YuhongGuo,Learninglatentwordrepresentationsfor
Linguistics(ACL),Prague,CzechRepublic,June2007,pp.440–447. domainadaptationusingsupervisedwordclustering,in:Proceedingsofthe
[7] D.Bollegala,D.Weir,J.Carroll,Cross-domainsentimentclassificationusinga 2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages
sentiment sensitive thesaurus, in: Knowledge and Data Engineering, IEEE 152–162,Seattle,Washington,USA,18–21October,2013.
Transactions,vol.25(8),2013,pp.1719–1731. [21] MinXiao,YuhongGuo,Featurespaceindependentsemi-superviseddomain
[8] N.F. Da Silva, E.R. Hruschka, E.R. Hruschka, Tweet sentiment analysis with adaptationviakernelmatching,IEEETrans.PatternAnal.Mach.Intelligence37
classifierensembles,in:ElsevierDecisionSupportSystems,vol.66,2014,pp. (1)(2015)52–66.
170–179. [22] S.Pan,Q.Yang,Asurveyontransferlearning,IEEETrans.KnowledgeEng.22
[9] M.C.DeMarneffe,B.MacCartney,C.D.Manning,Generatingtypeddependency (10)(2009)1345–1359.
parsesfromphrasestructureparses,in:ProceedingsofLREC,vol.6,2006,pp. [23] PanSinnoJialin,XiaochuanNi,Jian-TaoSun,QiangYang,ZhengChen,Cross-
449–454. domain sentiment classification via spectral feature alignment, in:
[10] E. Fersini, E. Messina, F.A. Pozzi, Sentiment analysis: Bayesian Ensemble Proceedings of the 19th International World Wide Web Conference, ACM,
Learning,ElsevierDecisionSupportSyst.68(2014)26–38. Raleigh,USA,April26–30,2010.
[11] A.Esuli,F.Sebastiani,Senti-WordNet:APubliclyAvailableLexicalResourcefor [24] B.Pang,L.Lee,Opinionminingandsentimentanalysis,Found.TrendsInform.
Opinion Mining, in: Proc. of the 05th Conf. on Language Resources and Retrieval2(1–2)(2008)1–135.
Evaluation,GenovaItaly,May22–28,2006,pp.417–422.Availableat<http:// [25] Paramveer S. Dhillon, Partha Talukdar, Koby Crammer, Metric Learning for
sentiwordnet.isti.cnr.it/>. Graph-BasedDomainAdaptation,UniversityofPennsylvaniaDepartmentof
[12] FarhanHassanKhan,UsmanQamar,SabaBashir,SWIMS:semi-supervised Computer and Information Science Technical Report No. MS-CIS-12-17,
subjective feature weighting and intelligent model selection for sentiment January2012.
analysis,Knowledge-BasedSyst.100(2016)97–111. [26] Rui Xia, Chengqing Zong, Shoushan Li, Ensemble of feature sets and
[13] Haiqing Zhang, Aicha Sekhari, Yacine Ouzrout, Abdelaziz Bouras, Jointly classification algorithms for sentiment classification, in: Information
identifying opinion mining elements and fuzzy measurement of opinion Sciences,vol.181(6),15March2011,pp.1138–1152.
intensity to analyze product features, Eng. Appl. Artificial Intelligence 47 [27] RuiXia,ChengqingZong,XueleiHu,E.Cambria,Featureensembleplussample
(2016)122–139. selection:domainadaptationforsentimentclassification,IEEEIntelligentSyst.
[14] Kumar Ravi, Vadlamani Ravi, A survey on opinion mining and sentiment 28(3)(2013)10–18.
analysis:tasks,approachesandapplications,KnowledgeBasedSyst.89(2015) [28] RuiXia,FengXu,JianfeiYu,YongQi,ErikCambriac,Polarityshiftdetection,
14–46. eliminationandensemble:athree-stagemodelfordocument-levelsentiment
[15] J.V.Lochter,R.F.Zanetti,D.Reller,T.A.Almeida,ShortTextopiniondetection analysis,Inform.Process.Manage.52(2016)36–45.
using ensemble of classifiers and semantic indexing, Expert Syst. Appl. 62 [29] ShoushanLi,YunxiaXue,ZhongqingWang,GuodongZhou,Activelearningfor
(2016)243–249. cross-domainsentimentclassification,in:ProceedingsofIJCAI’13theTwenty-
[16] Lukasz Augustyniak, Tomasz Kajdanowicz, Piotr Szyma´nski, Włodzimierz ThirdInternationalJointConferenceonArtificialIntelligence,2013,pp.2127–
Tuligłowicz,PrzemyslawKazienko,RedaAlhajj,BoleslawSzymanski,Simpler 2133.
isbetter?lexicon-basedensemblesentimentclassificationbeatssupervised [30] P.K. Singh, M.S. Husain, Methodological study of opinion mining and
methods,in:InternationalWorkshoponCurbingCollusiveCyber-gossipsin sentimentanalysistechniques,Int.J.SoftComput.5(1)(2014)11.
Social Networks (C3-2014), August 17, 2014 Proc. IEEE/ACM Int. Conf. [31] M. Wang, H. Shi, Research on sentiment analysis technology and polarity
Advances in Social Network Analysis and Mining, ASONAM, Beijing, China, computation of sentiment words, in: Proc. of Int’l Conf. on Progress in
2014. InformaticsandComputing,Shanghai,vol.1,December10–12,2010,pp.331–
[17] McCallum Andrew, Freitag Dayne, Pereira Fernando, Maximum entropy 334.
markov models for information extraction and segmentation, in: 17th [32] ZhengChutao,LiuCheng,WongHau-San,Iterativetermweightingforshort
InternationalConf.onMachineLearning,StanfordUniversity,June29-July2, text data, in: Proceedings of the 2015 IEEE International Conference on
2000. Systems,Man,andCybernetics,HongKong,9–12October2015.