Array11(2021)100072
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
Emotion recognition from EEG-based relative power spectral topography
using convolutional neural network
Md. Asadur Rahmana, Anika Anjumb, Md. Mahmudul Haque Miluc, Farzana Khanamc,
Mohammad Shorif Uddind,*, Md. Nurunnabi Mollahe
aDepartmentofBiomedicalEngineering,MilitaryInstituteofScienceandTechnology(MIST),Dhaka,1216,Bangladesh
bDepartmentofBiomedicalEngineering,KhulnaUniversityofEngineering&Technology(KUET),Khulna,9203,Bangladesh
cDepartmentofBiomedicalEngineering,JashoreUniversityofScienceandTechnology(JUST),Jashore,7408,Bangladesh
dDepartmentofComputerScienceandEngineering,JahangirnagarUniversity(JU),Dhaka,1342,Bangladesh
eDepartmentofElectricalandElectronicEngineering,KhulnaUniversityofEngineering&Technology(KUET),Khulna,9203,Bangladesh
A R T I C L E I N F O A B S T R A C T
Keywords: Emotionrecognition,achallengingcomputationalissue,findsinterestingapplicationsindiversefields.Usually,
Electroencephalography(EEG) feature-basedmachine-learningmethodshavebeenusedforemotionrecognition.However,theseconventional
Emotionrecognition shallow machine learning methods often find unsatisfactory results as there is a tradeoff between feature di-
Relativepowerspectraldensity mensionsandclassificationaccuracy.Besides,extractionandselectionoffeaturesfromthespatialandfrequency
Convolutionalneuralnetwork(CNN)
domainscouldbeanadditional issue.Thisworkproposesamethod thattransforms EEG(electroencephalog-
SEEDdataset
raphy)signalstotopographicimagesthatcontainthefrequencyandspatialinformationandutilizesaconvolu-
tional neural network (CNN) to classify the emotion, as CNN has improved feature extraction capability.
Accordingtotheproposedmethod,thetopographicimagesarepreparedfromtherelativepowerspectraldensity
ratherthanpowerspectraldensitythatshowsremarkableimprovementinclassificationaccuracy.Theproposed
methodisappliedtothewell-knownSEEDdatabaseandhasgivenoutperformingresultsthanthecurrentstate-of-
the-art.
1. Introduction communicational pathway between humans and machines. Among
variousmodalities,thismethodis verymuchpromisingbecauseofits
Emotionisacomplexstateofmindthatisassociatedwithsomeone's highaccuracyandobjective evaluation comparingwithotherexternal
surroundings, thoughts, feelings, and circumstances and it results in appearances like facial expression and gesture [7]. But, the raw EEG
physicalandpsychologicalchanges.Someresearchershavefoundthat signaliscontaminatedwithartifactsandthissignaliscomplexduetoits
emotion is a cognitive task [1–3]. A human can understand emotion variationwithtimeandspace.Thefeatureextractionandclassificationof
throughthebehavior,mood,andtemperamentofanotherperson,butit EEG signal have two important aspects which must be ensured in
ishardforamachinetounderstandhumanemotionunlessthehuman emotionalstaterecognition.Specifieddomainknowledgeisrequiredfor
makesthemachineintelligentenoughtodecodeemotions.Thismethod conventionalmanualfeatureextractionandfeatureselection.However,
for the machinescan benamedas humanemotionrecognition.Inthe the cost of conventional feature selectionincreasesat aquadraticrate
caseofthetreatmentofpatientswithexpressionproblems,recognitionof withtheincreaseofthenumberoffeatures[8].Inmostofthestudiesfor
a real emotional state by a machine is helpful for providing better feature extraction, researchers have only focused on time [9–14], fre-
medical treatment. There are some psychophysiological studies [4–6] quency[15–17],ortime-frequency[18,19]domainsoftheEEGsignal,
wheretheyfoundastrongcorrelationbetweenemotionrecognitionand andrarelyfocusedonthespatialdimension.AmultichannelEEGsystem
brainactivities. acquiresdatafromthedifferentspatiallocationsofthehumanbrain.So,
In recent years, EEG modality has gained much attention for themotivationofthisworkistocombinetime,frequency,andspatial
measuring brain activity in a precise manner along with setting a domains to classify the emotional state from the multichannel EEG
* Correspondingauthor.
E-mailaddresses:bmeasadur@gmail.com(Md.A.Rahman),anikaanjum123@gmail.com(A.Anjum),mahmudhmilu@gmail.com(Md.M.H.Milu),farzanabme@
just.edu.bd(F.Khanam),shorifuddin@gmail.com,shorifuddin@juniv.edu(M.S.Uddin),nurunnabim12@gmail.com(Md.N.Mollah).
https://doi.org/10.1016/j.array.2021.100072
Received24October2020;Receivedinrevisedform7May2021;Accepted31May2021
Availableonline10June2021
2590-0056/©2021TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).Md.A.Rahmanetal. Array11(2021)100072
signalswithhighaccuracy. SEED dataset [32]. In the data acquisition, there are fifteen subjects,
Severalstudieshaveconsideredbodymovement,voicetoneorfacial amongthemeightarefemalesandsevenaremales(Age¼23.27(cid:3)2.37
expressions,andphysiologicalactivitiestorecognizeemotions[20–23]. years). All participants are right-handed native Chinese students of
Thesephysicalchangeshadsomecommoncomplicationsastheydidnot ShanghaiJiaoTongUniversity.Theyarereportedasnormalorcorrected
showreliableemotion.Ourstateofmindcouldonlybeareliablewayto tonormalvisionandnormalhearing.
understandtheemotionalstatethatcouldhavebeenrevealedbythebrain Variousstimuli can beutilized in emotion-relatedresearchsuch as
signallikeEEG.Someresearchworks[24–26]areaccomplishedsofarto movieclips,music,andverbalcommand,etc.Amongthem,themovie
construct machine learning-based predictive models to classify the cliphasgreaterefficiencyandreliability[33]asitcontainsbothaudio
emotionalstatesfromtheEEGsignals.AstudyledbyWangetal.[24] andvideo.Inthisexperiment,Chinesemovieclipswereselectedsinceall
reported the alpha, beta, and gamma bands as features to classify four theparticipantswerenativeChinese.Thevideosareof4minlongand
emotions(joy,relax,sad,andfear)andfound66.5%(inaverage)accuracy canbecategorizedas:positive,negative,andneutral.Thefollowingse-
bysupportvectormachine(SVM)usingdifferentkindsofemotionalvideo lectioncriteriaweremaintained-
asstimuli.AnotherstudydonebyLinetal.[25]achieved82.29%accuracy
usingthesameclassifier.Theirexperimentincluded26subjectsandused (cid:1) Thetotaltimeoftheexperimentshouldbesmallotherwisesubjects
30slongmusicsamplesasstimuli.Toachieveahighlyaccuratepredictive mightbecomefatigued.
model,recentstudiespreferthedeepneuralnetworkasafeatureextractor (cid:1) Themovieclipsshouldbewellunderstoodwithoutclarification.
and classifier. Zheng et al. [26] extracteddensity entropyof the multi- (cid:1) Theclipsshouldstimuliasingletargetemotion.Itshouldnotcontain
channelEEGsignalsusingashort-timeFouriertransformanduseddeep mixedemotions.
belief network (DBN) as a classifier to classify positive, negative, and
neutralemotionsandobtained86.65%accuracy.Arecentstudy[27]has Eachfilmiseditedtocreatecoherentemotion-elicitingandmaximize
proposed to construct topographic images from the multichannel EEG emotionalmeanings.Thedetailsofthefilmclipsusedintheexperiments
signalstofeedthemintothedeepneuralnetworkwhichprovestoachieve aregiveninTable1.
ahighclassificationaccuracythanthecurrentstateofthearts.Themethod Eachsubjectexperiencedthreeexperiments:positive,negative,and
describes in Ref. [27] used power spectral density (PSD) to form the neutral.So,15subjectsexperienced45experimentsintotal.Thereisa
topographicimagewhichisslightlyquestionabletoattainhigh-levelac- totalof15trialsforeachexperiment.Eachtrialisstartedwith5shintof
curacy,asthevalueofthePSDcanvarywiththeindividuals,time,and starting.Then,filmclipsareplayedfor4minand45secisallottedafter
system.Therefore,usingrelativePSD(RPSD)ismoretechnicallysoundto eachclipforfeedback.Withinthisperiod,participantsareaskedtoreport
ensurethepowerlevelvariationmeasurementoftheconcernedbandwith their emotional reactions toward the shown video clips. The order of
respecttothefullbandpowerincaseofEEGsignals[27,28].Therefore, presentationisarrangedinsuchawaythattwoclipstargetingthesame
featureextraction,featureselection,andsettingoftheproperclassifierare emotionarenotshownconsecutively.Thedetailedprotocolisgivenin
themostimportantissuestoconstructamachinelearning-basedpredictive Fig.1.
modelforemotionalstateclassification.
Toovercometheseissues,thepresentworkproposesamechanismto 2.2. Dataacquisitionandprocessing
combine the time, frequency, and spatial domain of the multichannel
EEGemotionalsignalsintotopographicimages.Thetopographicimage Theexperimentswereperformedinthreesessionsforeachsubject.
representstheRPSDofeachtrialofanemotionalstatethatneedstime, The time interval between each session is one week or longer. This
frequency, and spatial information. There may arise an argument for process ensuredstablepatterns ofneuralactivitiesacross sessionsand
usingtheRPSDinsteadofPSD.Althoughabeautifulexplanationisgiven individuals. Facial videos and EEG data are recorded simultaneously.
inRefs.[27,28],agraphicalrealizationispresentedintheResultsand Subjectsareseatedinfrontofabigscreenwheremovieclipsareshown.
DiscussionsSectiontoexplaintheeffectofPSDandRPSDthroughpower EEGdataisrecordedusinganESINeuroScanSystematasamplingrateof
levelvariation.ThisgraphicalpresentationclarifieshowRPSDimproves 1000Hzfroma62-channelactiveAgClelectrodecapfollowingtheIn-
thefeaturequalityoftheEEGsignalpowerdistribution. ternational 10–20 System [34]. The SEED dataset contains a down-
Manyrecentresearchworks[29–31]recommendthattheconvolu- sampled, preprocessed, and segmented version of EEG data. It is
tionalneuralnetwork(CNN)methodisveryeffectiveinfeatureextrac- downsampled to 200 Hz. A bandpass filter of 0–75 Hz is applied to
tion and classification tasks. For this, utilizing the automated feature removeanyartifacts.Thereare45matfilesintotal,oneperexperiment.
extraction and classification facilities of the CNN, the topographical Each subject file contains 15 arrays. In this work, we have used a
imagesareclassified.Themaincontributionsofthisworkareasfollows: band-passfilter(3–30Hz)toremoveunnecessaryfrequencies.IntheEEG
signal,upto3HzrepresentstheDeltaBand,whichispresentindeep
(cid:1) AmethodisproposedthatutilizestheRPSDofeachspatialpositionof sleeporcoma.Assubjectswereawakeandalertduringtrials,DeltaBand
EEGdataonthebrain(channel)topreparethetopographicimages. is rejected in our processing. Frequencies higher than 30 Hz are also
(cid:1) A CNN has been applied for the feature extraction and emotion removedasitdoesnotrepresentanybrainactivityregardingemotional
classificationfromthetopographicimages. effects.
(cid:1) We compared our work with the other recent works on emotion
recognitionusingtheSEEDdataset. 2.3. BaseconstructionoftheEEGtopography
The article is organized as follows: Section 2 describes the data Anillustrationof62-electrodeplacementregardingtheSEEDdataset
collection, representation, and the proposed methodology, Section 3 isgiveninFig.2.Itcanbegeneralizedthroughamatrixofm(cid:4)n.Here,m
presentstheresultsofthisresearchworkandcomparesitsfindingswith andnrepresentthemaximum-pointnumberinhorizontaltestpointsand
thefindingsoftherecentworks,finally,Section4concludesthepaper vertical test points, respectively. In this work, m (cid:4) n equals to 9 (cid:4) 8
withfutureresearchdirections. matrix.TheredcircledpointsinFig.3aretheEEGelectrodesoftheSEED
dataset.ThesepointsindicatethevalueofrelativePSDoftheEEGsignals
2. Materialsandmethods ofcorrespondingelectrodes.Thegraypointsareaddedtoformacom-
plete9(cid:4)8matrix.Thesepointsaretheinterpolationofthesurrounding
2.1. Datasetdescription redpoints.ThemethodofEEGtopographyconstructionisshownstep-
wise in Fig. 4. A pseudo-code for the topographic image formation
Theproposedmethodologyofthisworkhasbeenconductedonthe fromthemultichannelEEGsignalsisaddedinAppendixA.
2Md.A.Rahmanetal. Array11(2021)100072
Table1
Exemplarymovieclipsforthepositive,negative,andneutralemotionalstimulationusedintheexperiment.
Serialno Emotionlabel Filmclips'sources
01 Negative TangshanEarthquake
02 Negative Backto1942(Warmovie)
03 Positive LostinThailand
04 Positive FlirtingScholar
05 Positive JustAnotherPandora'sBox
06 Neutral WorldHeritageinChina
ExamplesofthemovieclipsofPositive,Negative,andNeutralemotions
PositiveClips NegativeClips NeutralClips
FlirtingScholar Backto1942 WorldHeritageinChina
LostinThailand TangshanEarthquake WorldHeritageinChina
The construction of some new points using specific neighborhood ThefrequencycontentofthesignalxðtÞis_ xðωÞwhichiscalculatedby
discretedatapointsisfoundbyinterpolation.Tomake72pointsofthe9 Fouriertransformation.Then,thePSDcanbecalculatedasfollows[35,
(cid:4)8matrix,10additionalpointsarecalculatedthroughinterpolatingthe
36]:
neighborhood points. The interpolation process of gray points can be (cid:2) (cid:3)
calculatedas: S ðωÞ¼limE jbxðωÞj2 (3)
xx T→∞
Γðm;nÞ¼Γ0ðmþ1;nÞþΓ0ðm(cid:5)1;nÞþΓ0ðm;nþ1ÞþΓ0ðm;n(cid:5)1Þ
;
TheratioofthePSDofthebandofinterest(PSDBOI)andthePSDof
K (1) thetotalfrequencyband(PSDtotal)iscalledRPSD.ThevalueofthePSD
ð0(cid:6)m;n(cid:6)8;m;m2NÞ variesfrompersontopersonandinthecaseofthesameindividual,it
variesfromtimetotime.ThusPSDisnotareliablesourceofinformation
where,Γ(m,n)presentstherequiredvalueofthegraypoints,Γ’
(m,n) irrespective of person and time [27,28]. RPSD solves this problem by
arethevaluesofthepointsurroundingΓ(m,n).Here,Kisthenumberof comparingthePSDoftheconcernedbandwithrespecttothePSDvalue
non-zeroelementsinthenumerator. ofthetotalfrequencyrangeofthesignal[27,28].Therefore,RPSDcanbe
presentedas(4).
2.4RPSDcalculationandnormalization
PSD
RPSD¼ BOI (4)
PSD
PSDofasignalmeansthe distributionofpoweroveritsfrequency total
components that represents the impact of the frequency components Toreduceinter-participantvariability,theRPSDofeachsubjectcan
includedinthesignal.Let,Pistheaveragepowerofasignalx(t),thenthe benormalizedbyscalingbetween0and1[34]asgivenin(5).
powerforthetotaltimeperiodTis, π(cid:5)π
π0¼ min (5)
ZT π max(cid:5)π min
1
P¼lim jxðtÞj2dt (2)
T→∞T Whereπ0is the normalized value of the feature; π max , π min are the
0
maximumandminimumvalueofthesubjectfeatures,respectively.Using
Fig.1. Eachsubjectfaced45trialsintotal.Noconsecutivetrialwastargetedatthesameemotion.
3Md.A.Rahmanetal. Array11(2021)100072
Fig.2. Thepositionsofthe62electrodesofthedataacquiringdeviceaccordingtotheinternational10–20method.
Fig.3. FeaturematrixofEEGismadeconsideringRPSDasafeature.Theshowngraypointsareinterpolatedfromthesurroundingredpoints.Tennewpointsare
interpolatedduringthistopographicimageconstruction.
4Md.A.Rahmanetal. Array11(2021)100072
normalized RPSD values of 72 positions (as shown in Fig. 3) the EEG work.CNNisatypeofmachinelearningsysteminwhichamodellearns
topographicimageswereconstructed. automatically to classify objects from images, numerical values, or
videos.Itishighlycapableoflearningfromtheinputdatabyoptimizing
2.5. ConstructionofCNN-basedclassifier the weight parameters of each filter by minimizing the classification
error. CNN consists of an input layer and an output layer, along with
ForautomaticfeatureextractionandclassificationCNNisusedinthis multiplehiddenlayers.Ittakesanimageasaninput,thenprocessesit
Fig.4. FlowdiagramoftheproceduretoconstructthetopographicimagefrommultichannelEEG.
5Md.A.Rahmanetal. Array11(2021)100072
Fig.5. Differentlayersofaconvolutionalneuralnetworkconsistingofanimageasaninputlayer,differentfiltersinconvolution,ReLUandpoolinglayer,andfinally
outputlayerwiththefinalscore.
throughmultiplehiddenlayers.Itgivestheoutputasaprobableclass Inthiswork,topographicimagesareconsideredastheinputofthe
name.ThehiddenlayersofCNNtypicallyconsistofconvolutionallayers, CNN.Acolorimagecanberepresentedasm(cid:4)n(cid:4)cwheremisthewidth
ReLUlayers,poolinglayers,afullyconnectedlayer,abatchnormaliza- andnistheheightoftheimageandcisthenumberofchannelse.g.RGB
tionlayer[37].AgeneralizedpatternofthelayersinaCNNispresented image has c ¼ 3. Here, the input image size is 192 (cid:4) 192 (cid:4) 3. The
inFig.5.Thedesigndetailsofthedifferentlayersregardingthisworkare convolutionallayerworksforfeatureextraction.Itisconsideredthecore
discussedbrieflyinthefollowing. building block of the CNN architecture [38]. Convolutional layers
Fig.6. ThevaluesofthedifferentparametersofthelayersintheproposedCNNalongwiththeclarificationoftheregardinglayers.
Fig.7. ComparisonbetweentherawEEGdataandthefilteredEEGsignal.EEGsignalisfilteredwithaband-passfilterofband8–32Hz.
6Md.A.Rahmanetal. Array11(2021)100072
Fig.8. AnexampleofanEEGtopographicimage.AF7,AF5,AF6,AF8,PO9,PO10,CB5,CB3,CB4,andCB6aretheinterpolatedpointsfromsurroundingpoints.
Fig.9. Comparisonamongtopographicimagesof(a)negative,(b)neutral,and(c)positiveemotions.Thecenterpartofthebrainwhichcontainsalimbicsystemis
alwaysactiveduringtheseemotionsasthispartisresponsibleforcreatingtheemotions.
7Md.A.Rahmanetal. Array11(2021)100072
Fig.10. RPSD-andPSD-basedtopographspresentingthevariationofEEGsignals'spatialpowerleveldistributionforpositive,negative,andneutralemotions.These
arethecorrespondingresultsofSubject3.
transformtheinputdatabyusingapatchoflocallyconnectingneurons Here,grepresentstheinputimagematrixtobeconvolvedwiththe
fromthepreviouslayer.Thisconvolutionallayerconvolvesthe192(cid:4) kernelmatrixhtoresultinanewmatrixG.
192(cid:4)3imagebymovingafilteralongwiththeverticalandhorizontal InourproposedCNNstructure,theinputchannelisnormalizedbya
inputimage.Whilecreatingaconvolutionaltwo-dimensionallayer,the batchnormalizationlayer.Itisgenerallyusedbetweentheconvolutional
filtersizeoftheinputargumentisspecifiedas8(cid:4)8.Strideisknownas layerandtheReLUlayertospeedupthetrainingprocessofCNNand
the step size with which the filter moves. For 8 (cid:4) 8 filter scanning reduce the sensitivity to network initialization. A batch normalization
throughtheinputimage,thestrideof1isusedinthisresearch. layernormalizesitsinputsbycalculatingthemeanandvarianceovera
In dilatedconvolution,the filtersare expanded by insertingspaces mini-batch and each input channel. Then it calculates the normalized
betweentheelementswithoutincreasingthenumberofparametersor activationsas[39],
computation.Weapply([3,3,4,4])zeropaddingtoinputimageborders
p (cid:5)μ
to add zero values vertically and horizontally. The output size of this bp ¼piffiffiffiffiffiffiffiffiffiBffiffiffiffi (8)
i σ2þε
designedconvolutionallayercanbeequatedby: B
1 Here, p i¼ inputs; μ B ¼ mean; σ2 B¼ variance; ε improves numerical
Output Size ¼ Sþ1ðIS(cid:5)ððFS(cid:5)1Þ*DFþ1Þþ2PÞ (6) stabilitywhenthemini-batchvarianceisverysmall.Therectifiedlinear
unitorReLUwasusedasanelement-wiseactivationfunctionoverthe
Here, IS ¼ Input size of image; S ¼ Stride; FS ¼ Filter size; DF ¼ inputdatathresholding.RunningReLUovertheinputvolumechanges
Dilationfactor;P¼Padding.So,themathematicalformulaofthetwo-
thepixelvaluesbutdoesnotchangethespatialdimensionoftheinput
dimensionalconvolutionallayeris: dataintheoutput.ReLUismorepreferredthanotherfunctionsbecauseit
X∞ X∞ makestheneuralnetworkfaster[39].
G½m;n(cid:7)¼ h½i;j(cid:7):g½m(cid:5)i;n(cid:5)j(cid:7) (7) ThedetailsoftheselayersinourproposedCNNmodelareshownin
i¼(cid:5)∞j¼(cid:5)∞ Fig.6.Thefullyconnectedlayerisusedtocomputeclassscoresthatcan
Fig.11. (a)Accuracyand(b)lossofthetrainingandvalidationoftheproposedCNNstructurewithrespecttoiterations.
8Md.A.Rahmanetal. Array11(2021)100072
Table2 amongtopographicimagesofnegative,neutral,andpositiveemotionsis
Classificationaccuracyfor15subjectsusingCNN. showninFig.9.Onethingtonoticeinthoseimagesisthatthecentral
partofthebrainisalwaysactiveinallthreeemotions.Thecentralpartof
Subject Accuracywith25%datafor Accuracywith50%datafor
trainingand75%datafor trainingand50%datafor thebrainormedialtemporallobeisconsistsofthelimbicsystem[40].It
testing(%) testing(%) hasbeensaidthatthelimbicsystemisresponsibleforemotionformation
1 82.79 93.48 [41].Thus,despitetheemotiontype,thelimbicsystemisalwaysactive.
2 90.91 100 AnotherimportantissueistousetheRPSDinsteadofPSDformaking
3 84.85 92.48 thetopographicalimages.Itisahypothesisthatpowerleveldistribution-
4 90.85 95.24
basedimagesarebetterinthecaseofRPSDthanthatofthePSD.Based
5 84.70 100
onthisargument,wehavepresentedPSD-andRPSD-basedtopograph-
6 90.91 90.71
7 83.82 90.71 icalimagesinFig.10.Itisclearlyobservablethatpowerleveldistribu-
8 84.85 95.24 tionismorepreciseinthecaseofRPSDthanthatofthePSD.
9 88.82 100 The topographic images are given as input to the CNN for feature
10 93.94 90.48 extractionandemotionclassification.TofeedtheCNN,therewerethree
11 92.91 95.24
12 85.73 88.67 typesofEEGdatacorrespondingtothreeemotionalstatesofthebrain.
13 90.88 94.48 Sinceeachparticipanttookpartin3differentemotionalstates(positive,
14 96.97 94.48 negative,andneutral)andthereare15trialsforanindividualstate.So,
15 92.91 98.24 thereare15(cid:4)3¼45multichannelEEGdatafromeachparticipant.
Average(cid:3) 89.056(cid:3)4.32 94.63(cid:3)3.68
TheaccuracyandlossoftheproposedCNNstructurewithrespectto
Standard
Deviation iterationsarepresentedinFig.11(a)andFig.11(b),respectively.Here,
themaximumepochisconsidered15.Wehaveselectedthefiltersizeof
theconvolutionallayerbasedonthetrialanderrormethod.Eachresultis
beusedastheoutputofthenetwork.Thedimensionoftheoutputvol-
checked using the Matlab simulator several times to select the best
umeis 1 (cid:4)1 (cid:4)N, whereNis the numberofoutputclasses(herethe
pattern.Theconvolutionallayer,batchnormalizationlayer,ReLUlayer,
numberofclassesN¼3).
poolinglayerareusedtwiceinthiswork.Thefinalresultsaretheclas-
sification accuracies against the three classes: positive, negative, and
3. Resultsanddiscussions
neutralemotionalstates.Therefore,thefullyconnectedlayerproduces3
outputclasses.
Allstepsoftheprocessing,imageformation,featureextraction,and
WehavedoneexperimentationwiththeproposedCNNintwosce-
dataclassificationwereperformedinMatlab2018a.Datautilizedinthis
narios.Inthefirstscenario,25%ofdataareusedfortraining,andtherest
research work has been collected from the SEED dataset. Some pre-
75%areusedfortesting.Similarly,inthesecondscenario,50%ofdata
processing was already applied to the dataset e.g. data were down-
areusedfortraining,andtherest50%areusedfortesting.Thetraining
sampledto200Hz,bandpassfilterwasappliedfrom0to75Hztoremove
andtestingdatawerechosenrandomlyfivetimesforeachparticipant
artifacts. Despite this preprocessing, some other processing were also and the resultant classification accuracy is tabulated from the average
done.Abandpassfilterwasappliedfrom8to32Hztoremovenoise.A
value of the corresponding five classification accuracy of each partici-
graphical representation of raw and bandpass filtered EEG signals is pant.Theclassificationaccuracyoftheproposedmethodispresentedin
showninFig.7. Table2.Wefoundthattheaverageclassificationaccuracyachievedby
Fig.8showsonesampleofEEGtopographicimage,where10new ourproposedworkis89%forthefirstscenarioand94%forthesecond
data pointsareinterpolatedfrom the neighborpoints: AF7,AF5,AF6, scenario.Thisoutcomeisthehighestclassificationaccuracywithrespect
AF8,PO9,PO10,CB5,CB3,CB4,andCB6.TheRPSDofeachelectrodeis
to the other recent methods applied to the SEED dataset so far. The
calculatedandnormalizedwithintherangeof0–1.EachRPSDvalueis
comparisonofthemethodsandtheirclassificationaccuraciesareshown
mapped in a two-dimensional plot according to the locations of each
inTable3.
electrode.Theselocationsaregivenspecificnames.Acolorbarisalso
showninFig.8torepresenttheactivitylevelofthedifferentpositionsof
4. Conclusion
ourbrainduringdifferentemotions.
Differentemotionscreatedifferenteffectsonthehumanbrain.Asad
In this research work, human emotion has been detected from
moviecanmakesomeoneemotionallyunstablewhileanaturalscenecan
multichannelEEGsignals.ThemultichannelemotionalEEGsignalswere
createasoothingeffectonourbrain.Thestateofanemotionalcondition
mapped into two-dimensional tomographic images using RPSD. These
changestheEEGresulttakenduringdifferentemotions.Acomparison
imagescombinedfrequencyandspatialdomaininformationoftheEEG
Table3
ComparisonwithotherrecentmethodswithSEEDdataset.
Author&Study Method Classifier AverageAccuracy(%)
W.Zhengetal.,2017[13] GroupSparseCanonicalCorrelationAnalysis 86.65
W.L.Zhengetal.,2017[14] DifferentialEntropyasFeatures DiscriminativeGraphregularizedExtremeLearning 79.28
Machine
W.L.Zhengetal.,2015[17] CriticalFrequencyBandInvestigation DeepBeliefNetwork 86.08(cid:3)8.34
Y.M.Jinetal.,2020[30] DifferentialEntropy DomainAdaptationNetwork 79.19
Y.Yang,2018[42] DifferentialEntropyasFeatures HierarchicalNetworkwithSubnetworkNodes 86.42
M.A.Rahman,2019etal. PCAandt-statistics-BasedFeatureSelection SVMandANN 85.85(cid:3)5.72and86.57(cid:3)4.08
[43] Method
ProposedMethod RPSD-BasedTopographicImageforEmotionalEEG CNN 89.056(cid:3)4.32(25%datafor
Data training)
94.63(cid:3)3.68(50%datafor
training)
SVM:SupportVectorMachine[44,45],ANN:ArtificialNeuralNetwork[44,45].
9Md.A.Rahmanetal. Array11(2021)100072
signals. For feature extraction and classification CNN was used in this dataset. Proper acknowledgments with citation guidelines are main-
workandfoundasignificantenhancementintheclassificationaccuracy. tainedfortheuseofthisdataset.
ComparedwiththeotherrecentmethodsthatwereappliedontheSEED
dataset,theproposedmethodachievedthehighestclassificationaccu-
References
racies89.056%(cid:3)4.32(using25%dataintrainingandtherest75%in
testing)and94.63%(cid:3)3.68(using50%dataintraining,andrest50%in [1] LazarusR.Emotionandadaptation.USA:OxfordUniversityPress;1991.
testing).Fromthisconvincingoutput,itis expectedthatthe proposed [2] MuellerSC.Theinfluenceofemotiononcognitivecontrol:relevancefor
developmentandadolescentpsychopathology.FrontPsychol2011;2:327.https://
expertsystemwouldworkefficientlyinothertypesofEEGsignalclas-
doi.org/10.3389/fpsyg.2011.00327.
sification,whichwillbeournextfocus. [3] TyngCM,AminHU,SaadMNM,MalikAS.Theinfluencesofemotiononlearning
andmemory.FrontPsycholAugust2017;8(1454):24.https://doi.org/10.3389/
fpsyg.2017.01454.
Declarationofcompetinginterest
[4] SammlerD,GrigutschM,FritzT,KoelschS.Musicandemotion:
electrophysiologicalcorrelatesoftheprocessingofpleasantandunpleasantmusic.
Theauthorshavenoconflictofinterestregardingthispublication. Psychophysiology2007;44(2):293–304.https://doi.org/10.1111/j.1469-
8986.2007.00497.x.
[5] KnyazevGG,Slobodskoj-PlusninJY,BocharovAV.Genderdifferencesinimplicit
FundingAcknowledgement andexplicitprocessingofemotionalfacialexpressionsasrevealedbyevent-related
thetasynchronization.Emotion2010;10(5):678–87.https://doi.org/10.1037/
a0019175.
Nofundingorganization/institutesupportedthisresearchwork.
[6] MathersulD,WilliamsLM,HopkinsonPJ,KempAH.Investigatingmodelsofaffect:
relationshipsamongEEGalphaasymmetry,depression,andanxiety.Emotion2008;
AppendixA 8(4):560–72.https://doi.org/10.1037/a0012811.
[7] AhernGL,SchwartzGE.Differentiallateralizationforpositiveandnegativeemotion
inthehumanbrain:EEGspectralanalysis.Neuropsychologia1985;23(6):745–55.
PseudoCode: https://doi.org/10.1016/0028-3932(85)90081-8.
EEGtopographicimage(usedinMATLABR2018a): [8] DashM,LiuH.Featureselectionforclassification.IntellDataAnal1997;1(3):
131–56.https://doi.org/10.1016/S1088-467X(97)00008-5.
[9] MurugappanM,RamachandranN,SazaliY.Classificationofhumanemotionfrom
data¼loadtheRPSDvaluesoftotalestimatedchannel;%Loadyour EEGusingdiscretewavelettransform.JBiomedSciEng2010;3:390–6.https://
2DEEGdata doi.org/10.4236/jbise.2010.34054.
xc¼[horizontalaxiscoordinates];%getthex-axis [10] PetrantonakisPC,HadjileontiadisLJ.EmotionrecognitionfromEEGusinghigher-
ordercrossings.IEEETransInfTechnolBiomed2010;14:186–97.0.1109/
yc¼[verticalaxiscoordinates];%gety-axispoints
TITB.2009.2034649.
trlen¼ normalized value of each electrode; % find the normalized [11] PetrantonakisPC,HadjileontiadisLJ.Emotionrecognitionfrombrainsignalsusing
value
hybridadaptivefilteringandhigher-ordercrossingsanalysis.IEEETransactionon
xi¼linspace(min(xc),max(xc),30);
AffectiveComputing2010;1:81–97.https://doi.org/10.1109/T-AFFC.2010.7.
[12] MurugappanM,RizonM,NagarajanR,YaacobS.Inferringofhumanemotional
yi¼linspace(min(yc),max(yc),30); statesusingmultichannelEEG.EurJSciRes2010;48(2):281–99.
[XI,YI]¼meshgrid(xi,yi);%Createameshwithxiandyi [13] ZhengW.MultichannelEEG-basedemotionrecognitionviagroupsparsecanonical
zc ¼ griddata(xc,yc,trlen,XI,YI,'natural’); % relative power of each c So er pr te .l 2at 0i 1o 7n ;9a (n 3a )l :y 2s 8is 1. –I 9E 0E .E hT ttr pa sn :/sa /c dt oio i.n os rgo /n 10C .o 1g 1n 0i 9ti /v Te Ca Dn Sd .2D 0e 1v 6e .2lo 5p 8m 72en 9t 0a .lSystems
electrode [14] ZhengWL,ZhuJY,LuBL.“Identifyingstablepatternsovertimeforemotion
[cs,hh]¼contourf(XI,YI,ZI,20,'LineStyle','none’); % contour plot of recognitionfromEEG.IEEETransactiononAffectiveComputing2017;1.https://
doi.org/10.1109/TAFFC.2017.2712143.
thedata
[15] ThammasanN,MoriyamaK,FukuiK,NumaoM.Continuousmusic-emotion
set(hh,'EdgeColor','none') recognitionbasedonelectroencephalogram99.”IEICETransactiononInformation
shadinginterp System;2016.p.1234–41.https://doi.org/10.1587/transinf.2015EDP7251.
set(gca,'Visible','off’);colormap(jet); [16] JirayucharoensakS,Pan-NgumS,IsrasenaP.““EEG-basedemotionrecognition
usingdeeplearningnetworkwithprincipalcomponentbasedcovariateshift
set(gcf,'PaperUnits','inches','PaperPosition',[0022]); adaptation.SciWorldJ2014:1–10.https://doi.org/10.1155/2014/627892.2014,
print(1,'-dpng’,'.png','-r00); 627892.
[17] ZhengWL,LuBL.InvestigatingcriticalfrequencybandsandchannelsforEEG-based
emotionrecognitionwithdeepneuralnetworks.IEEETransactionsonAutonomous
Authorstatement MentalDevelopment2015;7(3):162–75.https://doi.org/10.1109/TAMD.2015.
[18] YinZ,WangY,LiuL,ZhangW,ZhangJ.Cross-subjectEEGfeatureselectionfor
Md.AsadurRahman:Conceptualization,Methodology,Formalanal- emotionrecognitionusingtransferrecursivefeatureelimination.FrontNeurorob
2017;11:19.https://doi.org/10.3389/fnbot.2017.00019.
ysis,Software,Investigation,Resources,Draftpreparation.AnikaAnjum:
[19] LiX,QiXY,SunXQ,XieJL,FanMD,KangJN.Animprovedmulti-scaleentropy
Methodology, Formal analysis, Software, Investigation, Draft prepara- algorithminemotionEEGfeaturesextraction.JournalofMedicalImaging&Health
tion. Md. Mahmudul Haque Milu: Formal analysis, Data curation,
Informatics2017;7:436–9.https://doi.org/10.3772/j.issn.1002-0470.2015.10-
11.001.
Investigation,Draftpreparation.FarzanaKhanam:Formalanalysis,Data
[20] SchirmerA,AdolphsR.Emotionperceptionfromface,voice,andtouch:
curation, Investigation, Draft preparation. Mohammad Shorif Uddin: comparisonsandconvergence.TrendsCognitSci2017;21(3):216–28.https://
Validation, Visualization, Review & Editing. Md. Nurunnabi Mollah: doi.org/10.1016/j.tics.2017.01.001.
Supervision,Review&Editing [21] R guig ido eu slo vt isS u, aP le all ttM enD ti. oS nee toin fg ace em s.o Pti lo on Sw Oi nt eh 2y 0o 1u 2r ;e 7a (r 1s ): :1e –m 1o 1t .io hn tta pl sp :/r /o dso od i.y ori gm /plicitly
10.1371/journal.pone.0030740.
Funding [22] BanzigerT,GrandjeanD,SchererKR.Emotionrecognitionfromexpressionsinface,
voice,andbody:themultimodalemotionrecognitiontest(MERT).Emotion2009;
9(5):691–704.https://doi.org/10.1037/a0017088.
Nofundingwasreceivedforthisresearch. [23] StathopoulouIO,TsihrintzisGA.“Emotionrecognitionfrombodymovementsand
gestures,”IntelligentInteractiveMultimediaSystemsandServices.July2011.
Informedconsent
p.295–303.https://doi.org/10.1007/978-3-642-22158-3_29.
[24] WangXW,NieD,LuBL.EEGbasedemotionrecognitionusingfrequencydomain
featuresandsupportvectormachines.NeuralInformationProcessing2011;7062:
The studies reported in this work did not require any informed 734–43.https://doi.org/10.1007/978-3-642-24955-6_87.
consent. [25] LinYP,WangCH,JungTP.EEG-basedemotionrecognitioninmusiclistening.IEEE
(InstElectrElectronEng)TransBiomedEng2010;57(7):1798–806.https://
doi.org/10.1109/TBME.2010.2048568.
Ethicalapproval [26] LiY,HuangJ,ZhouH,ZhongN.Humanemotionrecognitionwith
electroencephalographicmultidimensionalfeaturesbyhybriddeepneural
networks.ApplSci2017;7(10).https://doi.org/10.3390/app7101060.
Thisarticledoesnotcontainanystudieswithhumanparticipantsor
[27] RahmanMA,RashidMMO,KhanamF,AlamMK,AhmadM.EEGbasedbrain
animals performed by any of the authors. It used a publicly available alertnessmonitoringbystatisticalandartificialneuralnetworkapproach.IntJAdv
10Md.A.Rahmanetal. Array11(2021)100072
ComputSciApplJanuary2019;10(1).https://doi.org/10.14569/ [37] KrizhevskyA,SutskeverI,HintonGE.Imagenetclassificationwithdeep
IJACSA.2019.0100157. convolutionalneuralnetworks.AdvNeuralInfProcessSyst2012:1097–105.
[28] KhanamF,RahmanMA,AhmadM.EvaluatingalpharelativepowerofEEGsignal https://doi.org/10.1145/3065386.
duringpsychophysiologicalactivitiesinSalat.In:Internationalconferenceon [38] CunYL,BengioY.“Convolutionalnetworksforimages,speech,andtimeseries,”
innovationsinscience,engineeringandTechnology2018(ICISET).Bangladesh: Thehandbookofbraintheoryandneuralnetworks.1995.
InternationalIslamicUniversityChittagong(IIUC);2018.p.1–6.https://doi.org/ [39] AcharyaUR,OhSL,HagiwaraY,TanJH,AdeliH.Deepconvolutionalneural
10.1109/ICISET.2018.8745614.27-28October. networkfortheautomateddetectionanddiagnosisofseizureusingEEGsignals.
[29] MahmudM,KaiserMS,HussainA,VassanelliS.Applicationsofdeeplearningand ComputBiolMed2018;100:270–8.https://doi.org/10.1016/
reinforcementlearningtobiologicaldata.inIEEETransactionsonNeuralNetworks j.compbiomed.2017.09.017.
andLearningSystemsJune2018;29(6):2063–79.https://doi.org/10.1109/ [40] MorganePJ,GallerJR,MoklerDJ.Areviewofsystemsandnetworksofthelimbic
TNNLS.2018.2790388. forebrain/limbicmidbrain.ProgNeurobiol2005;75(2):143–60.https://doi.org/
[30] NoorMBT,ZeniaNZ,KaiserMS,MamunSA,MahmudM.“Applicationofdeep 10.1016/j.pneurobio.2005.01.001.
learningindetectingneurologicaldisordersfrommagneticresonanceimages:a [41] PessoaL.“Emotionandcognitionandtheamygdala:from“whatisit?”to“what'sto
surveyonthedetectionofAlzheimer'sdisease,Parkinson'sdiseaseand bedone”.Neuropsychologia2010;48(12):3416–29.https://doi.org/10.1016/
schizophrenia.BrainInformatics2020;7(11).https://doi.org/10.1186/s40708- j.neuropsychologia.2010.06.038.
020-00112-2. [42] YangY,WuQMJ,ZhengW,LuB.EEG-basedemotionrecognitionusinghierarchical
[31] RahmanMA,UddinMS,AhmadM.Modelingandclassificationofvoluntaryand networkwithsubnetworknodes.IEEETransactionsonCognitiveand
imagerymovementsforbrain-computerinterfacefromfNIRandEEGsignals DevelopmentalSystemsJune2018;10(2):408–19.https://doi.org/10.1109/
throughconvolutionalneuralnetwork.HealthInfSciSyst2019;7(22).https:// TCDS.2017.2685338.
doi.org/10.1007/s13755-019-0081-5. [43] RahmanMA,HossainMF,HossainM,AhmmedR.EmployingPCAandt-statistical
[32] EmotionalEEGDataset.Availablein:http://bcmi.sjtu.edu.cn/home/seed/index.ht approachforfeatureextractionandclassificationofemotionfrommultichannel
ml. EEGsignal.EgyptianInformaticsJournal2019.https://doi.org/10.1016/
[33] JinY,LuoY,ZhengW,LuB.EEG-basedemotionrecognitionusingdomain j.eij.2019.10.002.
adaptationnetwork.InternationalConferenceonOrangeTechnologies.Singapore: [44] RahmanMA,KhanamF,AhmadM,UddinMS.“MulticlassEEGsignalclassification
ICOT);2017.p.222–5.https://doi.org/10.1109/ICOT.2017.8336126. utilizingR(cid:1)enyimin-entropy-basedfeatureselectionfromwaveletpacket
[34] HomanRW,HermanJ,PurdyP.“Cerebrallocationofinternational10–20system transformation.BrainInformatics2020;7(7).https://doi.org/10.1186/s40708-020-
electrodeplacement.ElectroencephalogrClinNeurophysiol1987;66(4):376–82. 00108-y.
https://doi.org/10.1016/0013-4694(87)90206-9. [45] RahmanMA,RashidMA,AhmadM,KuwanaA,KobayashiH.Modelingand
[35] RiekeF,BialekW,WarlandD.Spikes:ExploringtheNeuralCode(Computational classificationofvoluntaryandimagerymovementsfromtheprefrontalfNIRS
Neuroscience).MITPress;1999,ISBN978-0262681087. signals.IEEEAccess2020;8:218215–33.https://doi.org/10.1109/
[36] ScottMillers,ChildersDonald.Probabilityandrandomprocesses.AcademicPress; ACCESS.2020.3042249.
2012.p.370–5.
11