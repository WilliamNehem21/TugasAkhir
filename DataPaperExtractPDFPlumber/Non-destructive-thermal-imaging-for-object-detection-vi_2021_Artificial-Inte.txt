ArtificialIntelligenceinAgriculture5(2021)102–117
ContentslistsavailableatScienceDirect
Artificial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
Non-destructivethermal imagingforobjectdetectionvia advanceddeep
learning for robotic inspection and harvesting of chili peppers
⁎
StevenC.Hespeler,HamidrezaNemati,EhsanDehghan-Niri
IntelligentStructuresandNondestructiveEvaluation(ISNDE),CivilEngineeringDepartment,NewMexicoStateUniversity,LasCruces,NM,USA
a r t i c l e i n f o a b s t r a c t
Articlehistory: DeepLearninghasbeenutilizedincomputervisionforobjectdetectionforalmostadecade.Real-timeobjectde-
Received15March2021 tectionforroboticinspectionandharvestinghasgainedinterestduringthistimeasapossibletechniqueforhigh-
Receivedinrevisedform11May2021 qualitymachineassistanceduringagricultureapplications.WeutilizeRGBandthermalimagesofchilipeppersin
Accepted11May2021
anenvironmentofvariousamountsofdebris,pepperoverlapping,andambientlighting,trainthisdataset,and
Availableonline15May2021
compareobjectdetectionmethods.Resultsarepresentedfromthereal-timeandlessthanreal-timeobject
detectionmodels.Twoadvanceddeeplearningalgorithms,Mask-RegionalConvolutionalNeuralNetworks
Keywords:
(Mask-RCNN)andYouOnlyLookOnceversion3(YOLOv3)arecomparedintermsofobjectdetectionaccuracy
Deeplearning
Youonlylookonce(YOLO)v3 andcomputationalcosts.WhenutilizingtheYOLOv3architecture,anoveralltrainingmeanaverageprecision
Objectdetection (mAP)valueof1.0isachieved.Mosttestingimagesfromthismodelscorewithinarangefrom97to100%con-
Chilipepperfruit fidencelevelsinnaturalenvironment.ItisshownthattheYOLOv3algorithmhassuperiorcapabilitiestothe
Mask-RCNNwithover10timesthecomputationalspeedonthechilidataset.However,someoftheRGBtestim-
agesresultedinlowclassificationscoreswhenheavydebrisispresentintheimage.Asignificantimprovementin
thereal-timeclassificationscoreswasobservedwhenthethermalimageswereused,especiallywithheavyde-
brispresent.WefoundandreportimprovedpredictionscoreswithathermalimagerydatasetwhereYOLOv3
struggledontheRGBimages.Itwasshownthatmappingtemperaturedifferencesbetweenthepepperand
plant/debriscanprovidesignificantfeaturesforobjectdetectioninreal-timeandcanhelpimproveaccuracyof
predictionswithheavydebris,variantambientlighting,andoverlappingofpeppers.Inaddition,successfulther-
malimagingforreal-timeroboticharvestingcouldallowtheharvestingperiodtobecomemoreefficientand
openupharvestingopportunityinlowlightsituations.
©2021TheAuthors.PublishingservicesbyElsevierB.V.onbehalfofKeAiCommunicationsCo.,Ltd.Thisisanopen
accessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).
1.Introduction thecropincreasedby10milliontonnesperyear,from28to38million
tonnes(FAO,2019).Fig.2visualizestheworldwidetrend.Whilede-
Duetoitslightlypungent,crisp,andsmokytaste,theNewMexico mandforthechilipepperhasincreased,harvestproductionhasroom
chilipepper(Capsicumannuum)iswidelypopularintheSouthwestre- forgrowth.Agricultureproducerprices(pricesforcropsattheinitial
gionoftheUSA.Sometimesreferredtoaschiles,thepepperinNew salepoint)donotindicatequantityvaluesofproduction.Although
Mexico is a cash crop with an annual harvesting of approximately oneofthesmallertotalproducingcountrieswiththeleastamountof
8000to 10,000 acresandis used for consumption,processing into landharvested,Netherlandsachievedthehighestsellingpointperhect-
driedspice,ordecorations(strungonristras)(Boslandetal.,1991). are(ha)ofchilipeppersamongtheentiregrouphighlighted.Table1
Theagriculturalproductionofchilipeppersisthemostconsumed displaysdatafrom2019thatshowcasestheproductionamount,land
spicycropthroughouttheworld(Jiangetal.,2018).Throughoutthe areaharvested,producerpriceattheinitialsale,andhowmuchprice
world,theUSAandothercountrieslackintotalCapsicumannuumpro- eachhectareproducesforthesecountries(FAO,2019).Usingthisdata
ductioncomparedtoagriculturalproductionachievementsmadeby asablueprint,countrieswithlesslandharvestedcouldincreaseproduc-
Chinain2019.Fig.1displaystheproductionquantitiesofthetop11 tiontobecomemorecompetitiveanddriveuptheirproductionyield
countriesworldwide. perhectare.Weaimtoilluminateandprovideinsightintothisissue.
Totalworldproductionandareaharvestedhavesteadilyincreased Nondestructiveimagingforroboticharvestinghasbeenaneffective
eachyearsince1994.From2009to2019,worldwideproductionof toolforassistanceintheagricultureindustrytoharvestthefruitwith
debrispresentwhilenotharmingtheplant(Gaoetal.,2020).Dueto
⁎ Correspondingauthor.
thecomplexityofthistask,anefficientobjectdetectionandinspection
E-mailaddress:niri@nmsu.edu(E.Dehghan-Niri). algorithmarenecessaryforaroboticplatformtobeusedinpepper
https://doi.org/10.1016/j.aiia.2021.05.003
2589-7217/©2021TheAuthors.PublishingservicesbyElsevierB.V.onbehalfofKeAiCommunicationsCo.,Ltd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://
creativecommons.org/licenses/by-nc-nd/4.0/).S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
allwhilethemachineisdetectingthefruit.WithadvancesinConvolu-
tionNeuralNetwork(CNN)techniques,objectdetectionperformance
hasrapidlyincreasedinrecentyearsandisasuitablesolutionforreal-
timeroboticharvesting.Guetal.showcasetherecentgrowthofCNNs
throughabroadscopeofliterature(Guetal.,2018).Thesurveyhigh-
lights several improvements that have propelled the expansion of
CNNapplications.Acriticaldevelopmentisapplyingseveralnewactiva-
tion functions that have increased in model performance. One ex-
tremely effective activation function is the Rectified Linear Unit
(ReLU).ThetransitiontoReLUfromsigmoidfunctionsolvedthefamous
issueofvanishinggradient,whichwasimpedingtraditionalNeuralNet-
works(NN)fromlearningonlargedatasets.ReLUcanbemathemati-
callyrepresentedas:
(cid:1) (cid:3)
a ¼ max z ,0 ð1Þ
i,j,k i,j,k
wherez istheinputoftheactivationfunctionlocatedat(i,j).Other
i,j,k
variantsincludeLeakyReLU(andmanyothervariants),ELU,Maxout,
andProbout.Wereferthereadertothefollowingreferenceforanin-
depthanalysisofCNNcomponents(Guetal.,2018).
Thermalimagingisusedtoconvertinvisibleradiationpatternsofa
particularobjecttovisibleimagingforfeatureextraction(Vadivambal
and Jayas, 2011). This type of imaging can be utilized as a non-
destructivemethodduringreal-timeroboticharvestingsinceitisnon-
contact,non-invasive,andrapid(VadivambalandJayas,2011).Webe-
lievemappingtemperaturedifferencesbetweenthepepperandplant/de-
Fig.1.Chilipepperproductionin2019(FAO,2019). briscanprovidesignificantfeaturesforobjectdetectioninreal-timeand
canhelpimproveaccuracypredictionswithheavydebris,variantambient
lighting,andoverlappingofpeppers.Inaddition,successfulthermalimag-
harvesting.Nondestructivequalityassessmentofpeppershasbeen ingforreal-timeroboticharvestingcouldallowtheharvestingperiodto
studiedsuccessfullyusingnear-infrared(NIR)hyper-spectralimaging becomemoreefficientandopenupharvestingopportunityintheevening
tonotharmtheplant(Jiangetal.,2018). hoursorlowlightsituations.Fig.3highlightshowprominentthermalim-
Real-timeroboticharvestingisapromisingdevelopmentinagricul- agingcanbeforextractingfeaturesthananRGBimage.
turewithsomechallenges(Kangetal.,2020).Challengesinthisareare- Modernarchitectures(one-stageandtwo-stage)forobjectdetec-
sultfromcomplexmachinetasksthatrequirepowerfulcomputation tionisimplementedonacustomdatasettodetectchilipeppersin
abilities,complexmodelarchitectures,andnon-destructivemeasures, real-timeforroboticharvesting.Inthisinvestigation,we1)compare
Fig.2.Chiliareaharvested(blue)inhaandchiliproduction(red)intonnes(FAO,2019).
Table1
Chilipepperdata2019(FAO,2019).
Country Production(tonnes) AreaHarvested(ha) ProducerPrice(USD/t) Priceperha
Netherlands 375,000 1500 1177 $294,150.00
Tunisia 443,632 20,103 382 $8425.54
USA 624,982 19,627 941 $29,964.24
Algeria 675,168 21,767 643 $19,953.86
Nigeria 753,116 99,715 1077 $8130.47
Egypt 764,292 40,422 127 $2395.62
Spain 1,402,380 21,430 927 $60,662.91
Indonesia 2,588,633 300,377 1206 $10,388.93
Turkey 2,625,669 92,089 432 $12,308.76
Mexico 3,238,245 149,577 525 $11,370.24
China 19,007,248 798,877 748 $17,801.52
103S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.3.(a)RGBimage,(b)Thermalimage.
imageryfeaturesonRGBandthermaldatasets,2)comparetwomodern windowintocells(spatialregions)andcombininglocalizedhistograms
objectdetectiontechniquesonanRGBdatasetforlocatingandidentify- ofgradientsdirectionsandedgeorientationsovereverypixel.Afterthe
ingchilifruit,and3)developathermalimagingtechniquetoimprove HOGdescriptorsarecalculated,theyarefedtoalinearSupportVector
objectdetectiononimageswithhighdebris,variousambientlighting, Machine(SVM)forclassificationpurposes.
andoverlappingpeppersusingYOLOv3. The Deformable Part-based Model (DPM) was developed by
Felzenszwalb et al. (2008) and builds off of the architecture from
(DalalandTriggs,2005).Usingtheunderlyingblocks,themodelestab-
2.Literaturereview
lishesHOGdescriptorsviahistogramgradientmagnitudeswithineach
1Dpixel.Filtersareusedtodictateweightsinthedetectionwindow.
2.1.Objectdetection
LearningisaccomplishedonthePASCALtrainingdatawithalatent
SVM.UsingDPM,theauthorswon2007,2008,and2009PascalVisual
Objectdetectionhasarichhistoryofliteraturewithvastimprove-
mentsmaderecentlythatutilizecomputervisiontoidentifywhere ObjectClasses(VOC)detectionchallenge(Everinghametal.,2007).
andwhatanobjectisinspace.Literaturegenerallyacceptsthenotation
thattwoerasexistfortheprogressionofobjectdetection;traditional
2.1.2.Deeplearningforobjectdetection
objectdetectionbefore2014andtheintroductionofDeepLearning
For a comprehensive survey of Convolutional Neural Networks
(DL)forobjectdetectionafter2014(Zouetal.,2019).BeforeDLwasuti-
(CNN),wereferthereaderto(Guetal.,2018).DLapproachesadvanced
lizedforobjectdetection,manyinvestigationswerelimitedduetocom- significantlyin2012withtheintroductionofAlexNetthatwasbasedon
putationpowerandtrainableimagerepresentation.Wereferthereader
LeNet-5architecture,justwithadeeperstructure(Krizhevskyetal.,
to(Zouetal.,2019)foracomprehensivesurveyonobjectdetection.
2012).
Two-step-baseddetectorsexaminetheinput(image),applyre-
2.1.1.Traditionalobjectdetection
gionalproposals,anddetectobjects.Then,thosedetectedobjectsare
ViolaandJones(2001),introducedtheViola-Jonesdetectorwiththe
croppedandprocessedbyanentirelyseparatenetworktoestimate.
purposeofrobustrapidandreal-timedetection.Inthiswork,theau-
Two-stepmethodstypicallyrequiremorecomputationtimebecause
thorsfocusonsimplefeaturesratherthanpixelstospeedthedetection
eachsteprequiresresampling(threeintotal)(Poirsonetal.,2016).Typ-
processup.Threemaincontributionsweremadeinthestudy.1)Integral
ically,one-stagedetectornetworksdivideregionsandapplyprediction
Imagewhichisamethodforusingintermediateimagerepresentation
boundingboxeswithprobabilitiesinsideeachregionsimultaneously.
forrapidrectanglecomputation,2)analgorithmbasedonAdaBoost
Fig.4illustratesthetwo-stage(4a)andone-stageprocess(4b).
forfeatureextractionbycombingsomefeaturestoformaneffective
classifier,and3)acascademethodforclassificationtodiscardorpass
onviableinformationforfurtheranalysisandultimately,increasedde- 2.1.3.CNNtwo-stagebaseddetectors
tectionperformance.ViolaandJones(2004)demonstratedtherobust- Girshicketal.(2014)introducedtheRegionalCNN(R-CNN).They
nessofthismethodbyapplyingthetechniquestofacialrecognition. reporteddrasticimprovements(atthetime)tomeanprecisionaver-
Violaetal.(2005),utilizedthecascademethodformovingpersonde- age (mPA) metrics compared to the best performing method on
tectionalongwithAdaBoostforincreasinglycomplexrejectionregions VOC2012. Itbeginswith extracting around2000bottom-upregion
basedonarulesystem. proposals (object candidate boxes) by Selective Search (Uijlings
AnothercornerstoneinobjectdetectionwasintroducingtheHisto- et al., 2013). Next, each regional proposal was rescaled to fixed
grams of Oriented Gradient (HOG) detection system presented in image size (227 × 227) and used for model training via a CNN on
Dalal andTriggs(2005).In this investigation,Dalalet al. created a animagedatabaselikeImageNet(Dengetal.,2009)toextractfea-
methodthatutilizesedgedirectionandgradientintensitytodetermine tures.Lastly,abinarySVMclassifierisusedtopredict/detectanob-
theappearanceofanimage.Thistaskisaccomplishedbyseparatingthe ject(s)withineachregion.
104S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.4.Two/One-stagedetectors(Poirsonetal.,2016):(a)Two-stage,(b)One-stage.
Fig.5.HeadarchitectureforMaskR-CNN(Heetal.,2017).
Fig.6.YOLOarchitecture(Redmonetal.,2016.
Girshick(2015),improvedontheR-CNNbysimplifyingthelearning Poolingnetworks(SPPorSPPnet)wereintroducedbyHeetal.(2015)
process.TheauthorsrealizedthattheR-CNNisslowduetothenetwork inanattempttospeeduptraining.Guetal.(2018),utilizedSPPtoup-
performinga“forwardpassforeachobjectproposal”.SpatialPyramid dateallnetworklayersduringfine-tuning.Renetal.(2015),made
105S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Table2 2.1.4.CNNone-stagebaseddetectors
YOLOperformance(Redmonetal.,2016). YouOnlyLookOnce(YOLO)isareal-timedetectingsystemcreated
Real-TimeDetectors Train mAP FPS by Redmon et al. (2016) with several incremental improvements
throughtheyears(RedmonandFarhadi,2017;RedmonandFarhadi,
100HzDPM(SadeghiandForsyth,2014) 2007 16.0 100
2018).MostrecentimprovementshavebeenmadebyBochkovskiy
30HzDPM(SadeghiandForsyth,2014) 2007 26.1 30
FastYOLO 2007+2012 52.7 155 etal.(2020).Theoriginalsystemdesigngetsitsnamefromonlylooking
YOLO 2007+2012 63.4 45 atanimageonce,thereforetreatingeveryimage(input)asaregression
LessThanReal-Time problem.ComparedtoR-CNNarchitecturesthatarecomplexpipelines,
FastestDPM(Yanetal.,2014) 2007 30.4 15
thisimprovementinsimplicitymightbethemostimportantaspectof
R-CNNMinusR(LencandVedaldi,2015) 2007 53.5 6
FastR-CNN(Girshick,2015) 2007+2012 70.0 0.5 theYOLOsystem.Thesystemconsistsof3mainsteps.First,inputsare
FasterR-CNNVGG-16(Renetal.,2015) 2007+2012 73.2 7 resizedto448×448.Next,asingleCNNisrun.Finally,theNon-max
FasterR-CNNZF(Renetal.,2015) 2007+2012 62.1 18 Suppression(NMS)techniqueattemptstocorrectmultipledetections
YOLOVGG-16 2007+2012 66.4 21
onthesameimage.
TheYOLOarchitectureconsistsof24convolutionallayersfollowed
by2fullyconnectedlayers.Fig.6showsthedetailsofeachlayerin
thebasicYOLOarchitecture.Thenetworkoutputsa7×7×30tensor
ofpredictions.
In2018Redmonetal.establishedanincrementalimprovementto
thenetworkdesigntoinclude53convolutionallayers.Thisversion
demonstratedexceptionalaccuracyduetothemorerobustdesignand
is3.8timesfasterthanRetinaNet.Atthetimeofpublication(2016),
theYOLOsystemachievedmorethantwicethemPAcomparedtothe
nextbestreal-timesystem,63.4%mAPto26.1%,respectively.Italsoob-
servestheentireimage,unlikeregionproposal-basedarchitectures.
Table2summarizestheperformanceofYOLOcomparedtootherreal-
timeandlessthanreal-timesystems(Redmonetal.,2016).
AnothercriticalaspectofYOLOperformanceisthatwhencompared
toFastR-CNN,YOLOhadasmallerpercentageofbackgrounderrorsbut
moresignificantpercentageoflocalizationerrors.DL,especiallyCNNs,
havebeenthrustupontheroboticharvestingresearchareaforassis-
tanceduringagricultureapplications.Theresearcharea hasseen a
surgeininvestigationsin2019and2020(Naranjo-Torresetal.,2020).
InHameedetal.(2018),investigatorspresentacriticalcomparisonof
Fig.7.HOGfilter:(a)Horizontalkernel,(b)Verticalkernel. multiple modern computer vision techniques investigated by re-
searcherstoclassifyfruitandvegetables.Inthestudy,SupportVector
Machine(SVM),K-NearestNeighbour(KNN),DecisionTrees,Artificial
improvementsonthefastR-CNNdesigninthesameyeartomovethe NeuralNetworks(ANN),andCNN'sarereviewedforavarietyoffruit
algorithmclosertoreal-timedetectionspeeds.Thisversionbecame andvegetableclassification.However,theMaskR-CNNandYOLOv3
thefirstend-to-endandclosetoreal-timeobjectdetector(Zouetal.,
methodsarenotevaluatedintheinvestigation.
2019).Inthefirststage,aRegionProposalNetwork(RPN)considersa
Lietal.(2019),reviewednon-destructiveopticaltechniquesfor
candidateboundingbox.Inthesecondstage,featureextractioniscom- berries,mainlystrawberriesandblueberries.Theresearchersidentified
pletedusingRegionofInterest(RoI)pooling(RoIPool)fromeachcandi- andreviewed13methods,oneofwhichwascomputervision.There-
datewhichexecutesclassificationandboundingboxregression(Ren viewdidnotdetailspecificcomputervisiontechniques;however,sev-
etal.,2015). eral studies are mentioned that detail computer vision as a viable
MaskR-CNNisanextensionofthefastR-CNNarchitectureandgen- methodforberryclassification.Theresearchersstatethatonemain
eratesahigh-qualitysegmentationmaskforeachinstancewhileeffi- issuewithcomputervisionis“inconsistentambientilluminationand
cientlydetectingobjectsinanimage(Heetal.,2017).Usingvarious complex background.” For these reasons, we have selected the
FeaturePyramidNetwork(FPN)frameworks,R-50-FPN,R-101-FPN, YOLOv3forreal-timeobjectdetection.
X-101-FPN,andX-152-FPN,APperformancewasmeasuredat40.8
and70.4forenhanceddetectionandenhancedkeypointresults,respec-
tively.Fig.5detailstheheadarchitectureofthealgorithmandhigh- 2.2.Objectdetectioninagriculture
lightstheexactimprovementsmadetotheFasterR-CNN.
Here,weseethealgorithmappliedwiththeResNet(left)andFPN Inthepastdecade,differentobjectdetectionmethodshavebeen
backbones. successfullyadoptedinseveralagriculturalapplications.Hameedetal.
Fig.8.MaskR-CNNblockdiagram.
106S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.9.YOLOv3blockdiagram.
Fig.10.RGBHOGimages:(a)OriginalRGBimage,(b)ResizedRGBimage,(c)HOGRGB.
highlightedthecomplexitiesofclassifyingfruitandvegetables.Specifi-
cally,theylookedattheissueofidentifyingfruitsandvegetablesatsu-
permarketself-checkoutsystems(Hameedetal.,2020a).Theauthors
applied pre-trained weights via transfer learning and an ensemble
methodtoweightsofGoogleNetandMobileNetforanoptimizedaver-
ageweight.Resultsdemonstratedthatthesepre-trainedweightsposi-
tively affected the model, with the ensemble method reaching
accuracy measurements higher than training and testing datasets.
Hameedetal.(2020b),proposedanoveltechniquetothesupermarket
fruitandvegetableclassificationissuebyincorporatingaprogressive
fruitandvegetableclassifierwithAdaBoost-basedoptimizationofa
CNN.15classes,1000imagesineachclass,offruitsandvegetables
wereconsideredfortraining.Themethodbeganbycoarselyclassifying
fruitsandvegetablesintothreecategoriesviatheJenksNaturalBreaks
classificationmethod.Fromthere,theclasseswereimplementedto
threeCNNs(GoogleNet,MobileNet-v2,andacustom-designednet-
work)formoredetailedclassification.TheoptimizedAdaBoostCNN
outperformedthecustomCNN.AccuracyoftheCNNwithAdaBoost
optimizationrangedfrom97.60%(navelorange)to99.87%(ladyfinger
banana)withlessthan3%errorforallclasses.Thecustom15layer
CNNachievedtestaccuracyrangesof80.13%(10epochs)to93.97%
Fig.11.Gradientsofimages.
(22epochs).
107S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.12.ThermalHOGimages:(a)Originalthermalimage,(b)Resizedthermalimage,(c)HOGthermal.
Severalrecentstudieshaveattemptedtoimproveortestmodern FastR-CNN,andFasterR-CNNintermsofdetectionspeedandmAP%.
objectdetection methodstouse real-timerobotic harvesting.Wan WhentheauthorsfedtheirdatasettoYOLOv3theyachieveda40ms/
andGoudos(2020),utilizedanimprovedFasterR-CNNarchitectureto imagedetectionspeedandmAP%of90.03.So,theywereabletosoften
detectfruits(apples,mango's,oranges)forrobotpickingapplications. the gap of performance between the Faster R-CNN and YOLOv3
Intheinvestigation,theauthorsreportedon1)howtheirmodelup- architectures.
dates hyperparameters during the training phase of their model, Therehavebeenstudiesthatfocusonlowharvestingratesandhow
2)howthemodelaugmentsdataonhigh-qualityimages,and3)optimi- toimprovepickingpointaccuracyinreal-time(Yuetal.,2020).Thisin-
zationoftheconvolutionalandpoolinglayersoftheirFasterR-CNN vestigation,completedbyYuetal.,involvedcreatinganend-effectoras-
model. Specifically, feature extraction was completed with a CNN sembledonaservocontrolsystemofaroboticharvestingmachine.This
modelVGG-16using13convolutionallayers,13ReLulayers,andfour mechanismeffectivelyallowedtherobottoemitandreceivealaser
pooling layers. Two loss functions were added to optimize the beaminsteadofmeasuringthedepthdistanceinreal-time.Arotated
convolutionalandpoolinglayers,allowingtheparameterstoadjustau- YOLO(R-YOLO)wasproposedtoimprovetotheYOLOv3architecture
tomaticallyduringtraining.Theyfoundthatautomaticallytuningpa- tobecompatiblewiththemechanism.AhighlightofR-YOLOwasthe
rameters resulted in 58 ms/image detection speed with mAP % of processofrotatingtheannotationboundingboxesandemployingaro-
90.72.TheseresultswerecomparedtoYOLO,YOLOv2,YOLOv3,FastR- tationalparameter,α,duringthelabelingphase.Thisprocessaidsinthe
CNN,andFasterR-CNN,of whichtheir model outperformedYOLO, localizationofthepickingpointofthefruit.Theyreportedprecisionand
recallratesof94.4and93.4%,respectively.Theaveragecomputational
speedona640×480imagewithR-YOLOwas0.056s.
Robustandefficientnon-destructivetestingmethodsplayanessen-
tialroleinroboticharvestingandqualitycontrolofagriculturalprod-
ucts. Throughout literature, many novel techniques have been
implementedforaccomplishingthistask.Jiangetal.(2018),employed
near-infraredhyperspectralimagingfornon-destructivequalityassess-
mentofchilipeppers.Incombinationwithregressionmodels,NIRwas
usedforevaluatingcapsaicinoidconcentrationsandthewatercontent
ofpeppers.Besides,aradialbasisfunctionneuralnetwork(RBFNN)
wasusedforclassifyingpungentandnon-pungentpeppers.Theinves-
tigationdemonstratedthatcapsaicinanddihydrocapsaicinconcentra-
tionsofchilipeppersarevisuallydifferent.Pungencywasclassified
withanaccuracyof98.7and98.0%(fullspectraandsuccessiveprojec-
tionsalgorithm,respectfully).Gaoetal.(2020),implementedareal-
timehyperspectralimagingforfieldworkinclassifyingtheripenessof
fruit.Thisinvestigationreportedatechniquecreatedbytheauthors
Fig.13.Gradientsofthermalimages.
108S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
thatutilizedaportablehyperspectralimagerymachinetoestimatethe ThedatasetsinthisstudywereconstructedasthermalandRGB
ripenessofstrawberries.Twowavelengthswereselectedbasedona colorimagestodetectthechilipeppers.Tothisend,wemonitored
forwardSFSalgorithmandfeedtoanSVM.Finally,aCNNwasimple- twodifferentchiliplantsduringtheirgrowthperiod.Thefirstchilipep-
mentedtopredictdeepspatialfeatures,andthemodelachievedanac- perwasaraw,greenone,andthesecondwasmature.TheRGBimages
curacyof98.6%. wereacquiredfromconventionaldigitalcameraswithnopreprocessing
andsavedonastandardJPEGfileformat.Atthesametime,thermalim-
3.Methodology agesofchilipepperswerecollectedusingaForwardLookingInfrared
camera(FLIRA615).Thecameraishighlyaccuratewitheasysetup
3.1.Imagerepresentation andautomatedinspectionsoftware.Itissensitiveenoughtodetecttem-
peraturechangesassmallas50mK.Thespectralrangeofthethermal
Vision-basedfruitdetectionisacriticalcomponentforin-fieldauto- camerais7.5−14μmwithanabsolutethermalaccuracyof±2°Cor
mationandroboticfruitharvesting.However,thistechnologycould ±2%,andthermalsensitivityof<0.05°at30°C.Thecameraproduces
alsobeleveragedinotherapplicationssuchasdiseasedetection,matu- thermalimageswithupto640by480resolutionat60framesper
ritydetection,crophealthstatusmonitoring(Pateletal.,2011). second.
Fig.14.Imageswithmasks:(a)Mask1,(b)Mask2,(c)Mask3,(d)Mask4.
109S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Bothcameras were mounted over the chili plants at a fixeddis- Where,TPistruepositive,FPisfalsepositive,andFNisfalseneg-
tance of 130 cm with a light background. The thermal camera was ative.Truepositivesarewhenthemodelcorrectlyclassifiestheobject
fully controlled with the FLIR Tools; In fact, pre or post-processing detection.Falsepositiveoccurswhenthereisincorrectobjectdetec-
could be done on an individual image, using ResearchIR software. tion. Falsenegative occurs when groundtruth bounding boxes are
UsinganIRlenswithafocallengthof13.1mm,112thermalimages presentintheimageandthemodelfailstodetecttheobjectinthe
(Besides 112 RGBs) were captured every other day over three image.Truenegative(TN)isnotconsideredforthisapplicationsince
months.Oneachday,fourdifferentimagesweretakenfromdifferent it would be correctly not labeling parts of the image that do not
directionsforasingleplant.Someoftheimageswerecapturedinan havetruthboundingboxes.
outdoor setting and in natural light to validate the proposed
technique. 3.3.Implementationofobjectdetectionmodels
3.2.Performancemetrics AnnotationfilesweregeneratedforeachimagewiththeLabelImg
software(tzutalin,2017).MaskR-CNNwasimplementedinaCondaen-
UnlikeotherMLapplications,objectdetectionclassificationisnotbi- vironmentandoperatedwithPython3.6,Tensorflow1.14,andKeras
nary,asintrueorfalse.Therefore,theperformancemetricsusedhave 2.24packages.Next,weclonedtheGithubrepositoryMaskR-CNN
beentailoredfortheseexactoperations.IntersectionofUnion(IoU) from(Abdulla,2017).
givestheusertheabilitytomeasurehowwellboundingboxesare TheYOLOv3modelwascomputedonGPUwithCUDAversion10.1,
being predicted with respect to truth boxes greater than a user- V10.1.243.ThearchitecturewasclonedfromtheGithubrepositorycre-
identifiedthreshold.Inotherwords,IoUisameasurementoftheobject atedbyBochkovskiy(2020).
localizationaccuracybetweenthetruthboxesandpredictedboxes(Zou
etal.,2019).Eq.2definesIoU: 3.3.1.HOGimplementationinPython
AprerequisiteforusingtheHOGmethodisthatallimagesmustbe
AreaofOverlap
IoU¼ ð2Þ anarrayofshape(128,64).Therefore,allimagesweredroppedthen
AreaofUnion
transformedusingskimageresizetoolinpython(vanderWaltetal.,
2014).TheHOGdescriptorsworkbyapplyingafiltertotheimage.
Here, we are measuring the magnitude of overlap between the
Theverticalandhorizontalgradientsarecalculatedwiththekernels
groundtruthboxandpredictedbox.Inliterature,anacceptedthreshold
showninFig.7.
tendstobe0.5.Thatis:
MagnitudeanddirectionarecalculatedbyEqs.5and6,respectively:
(cid:4)
IoU≥0:5 TP qffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
IoU<0:5 FP g¼ g2 xþg2 y ð5Þ
So,ifthemeasurementisatorhigherthan0.5theobjectwillbesuc- θ¼ arctang y ð6Þ
cessfullyidentified.Precisionandrecallareusedasametrictoevaluate g
x
performance.WedefineprecisionandrecallinEqs.3and4below.
TP
Precision¼ ð3Þ 3.3.2.Implementationofoneandtwo-stagedetectionmodels
TPþFP
Weselecttwotop-performingalgorithmsfromtwo-stageandone-
TP stagedetectorsforimplementationonourdataset.Afterthedatawas
Recall¼ ð4Þ
TPþFN prepared,wesplitourtest/trainsetsto20/80%totrainonasmanyim-
agesaspossibleduetothedatasetbeingsmall.AsstatedinSection2.1.2,
theMaskR-CNNutilizesFPNandaResNet101backbone.FortheMask
R-CNN model, we first loaded bounding boxes and then masks for
eachimage.Weutilizedapythontooltoensurethatthemaskswereap-
pliedcorrectlytothedataset.Then,wedefinedandfitthemodelusing
theMaskRCNNfunctionsuppliedfromAbdulla(2017).Fig.8depicts
howourimagedatasetwalksthroughtheMaskR-CNN.
ForourimplementationoftheMaskR-CNNarchitecture,weutilize
FPNforfeatureextractionwhichisleveragedfromaConvNet'spyrami-
dalfeaturehierarchicalsystem(Linetal.,2017).Todothis,aregion-
basedobjectdetectororRegion-of-Interest(RoI)poolingextractsfea-
turesfromthematrix.TheRoIisassignedas:
(cid:6)pffiffiffiffiffiffiffi (cid:7)
⌊ ⌋
k¼ K þlog wh=224
0 2
wherewandharethewidthandheight(respectively),ontheinputim-
agetothenetworkandK isthetargetlevel.Duringthefirststage,re-
0
gionsareproposedwithanchors.Thentheobjectclassispredictedin
thesecondstage.Duringthesecondstage,anchorsarenotused.Instead,
ROIAlign is performed to fix misalignments (He et al., 2017). The
ROIAligngeneratesmasksforeachobjectatthepixellevelwhilethe
RoIclassificationbranchisusedtopredictcategories(Heetal.,2017).
ResultsaredisplayedinSection4.2.
YOLOv3consistsof53convolutionallayersthatcontainabatchnor-
malizationlayerwithaReLUactivationfunction.Poolingisnotutilized
Fig.15.MaskR-CNNpredictionbox. forthemodel.Sincethismodel is invarianttoimagesize, cropped
110S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
images were similar in size but not the same. Feature maps are
b ¼σðt Þþc
downsampledviaaconvolutionallayerwithastrideof2.Fig.9walks x (cid:1)x(cid:3) x
throughtheprocessofhowourdataisimplementedintoYOLOv3. b y¼σ t y þc y
Once the dataset is prepared, the YOLOv3 model extracts features b w¼p wetw
withDarknet-53.Duringfeatureextraction,threefeaturematricesare b h¼p heth
createdofsizes1)52×52,2)26×26,and3)13×13.Thesefeaturema-
tricesarefedtothemultiscaleconvolutionaldetectorwherethefea- where,b,b,b ,andb areboundingboxcentercoordinatesatx,yand
x y h w
turesareconcatenatedthroughmulti-stepconvolutions. widthandheightofthepredictionbox.t arenetworkoutputs.
x,y,w,h
Webatchandinputimageswithsize(m,416,416,3)andoutput Objectscoresarecalculatedastheprobability(between0and1)that
boundingboxeslabeled(p,b,b ,b ,b ,c).Theboundingboxpredic- anobjectisinsideaboundingboxfromasigmoidactivationfunction.
c x y h w
tionsarecalculatedby: WereportallfindingsinSection4.3.
Fig.16.ActualvspredictedplotsforMask-RCNN:(a)Actualplotimage1,(b)Predictedplotimage1,(c)Actualplotimage2,(d)Predictedplotimage2.
111S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
4.Resultsanddiscussion directionsin Fig.10c.When plotting the normalized histogramsin
Fig.10c,thereisvisualevidence(intensity)oftheHOGdetectorrecog-
Thissectionprovidestheresultsofallexperimentationcompleted nizingtheshapeoftheentireplantandevendemonstratingaconnec-
duringtraining.Hereweshowtheviabilityofcertaintechniquesap- tiontothechilipeppercentralizedintheoriginalimage.
pliedtoachilipepperdatasetinanenvironmentwithcomplexback- Gradientintensityofthehistogramsisvisuallyrepresentedasthey
groundsandgoodambientillumination.Section4.1detailstheHOG increaseincolorintheimagefromthecolormapinFig.11.Gradient
algorithmresults,Section4.2providesresultsfortheMaskR-CNNarchi- valuesrangefrom0to0.35andvisualizedistinctfeaturesofthechili
tecture,andSection4.3showsresultsfromreal-timedetectionusing plantandpepper.Edgesofthepepperandleaveshavelowergradient
theYOLOv3architecture.TheRGBdatasetwasconsideredforexperi- valueswhencomparedtothebackgroundoftheimage.Thesevalues
mentationontheHOG,MaskR-CNN,andYOLOv3models. areclosetozero(purple),whilethecenterofthepepperdisplaygradi-
Computervisionisahighlyresearchedareawithmanynewand entsbetween0.30and0.25.Thebackgrounddisplaysgradientvalues
cutting-edgetechnology.Asaresult,terminologycansometimesget rangingbetween0.20and0.35.Hereweshowtheregionallocationof
confusing.APwasfirstintroducedbyVOC2007(Everinghametal., wherethechilipepperiswithinthenormalizedhistogramplot.Aplot
2007)andthecalculationisappliedtoeveryimagewithinthelabeled ofthegradientsdemonstratesevidencethattheHOGmethodcanout-
set and measures the average AP of the entire set. We follow the linethechilipepperinFig.11.
COCOdefinitionformAPterminologyas: Whenwemovetoathermalimage,HOGdetectorsappeartohave
APisaveragedoverallcategories.Traditionally,thisiscalled“mean lessvisualevidenceofdetectingapepper.Fig.12depictsfeaturesofa
averageprecision”(mAP).WemakenodistinctionbetweenAPand thermalimage.Thefeatureextractionprocessisrepresentedwiththe
mAP(andlikewiseARandmAR)andassumethedifferenceisclear originalimageinFig.12a,theresizedthermalimageinFig.12b,and
fromcontext(CommonObjectsinContext(COCO)(2021)). thegradientdirectionsinFig.12c.Gradientdirectionsfromthe12c
Therefore,wedefinemAPas: plotdoesnotshowthedistinctregionofthepepper,butitdoesdemon-
stratetheshapeoftheentireplant.
1 N Gradientsrangefrom0to0.25inFig.13anddisplaythegradient
mAP¼ N∑ i¼1AP i ð7Þ valuesofthethermalimagefromFig.12a.Theplotofgradientvalues
doesnotdemonstrateaclearfeaturepatternandthevariationbetween
gradientvaluesismuchlessinFig.13comparedtoFig.11(<0.1).The
where,AP istheaverageprecisionatimageiandNisthetotalnumber
i HOGalgorithmwasaneffectivetechniquetovisualizehowfeatures
ofimages.ThemAPmetricisusuallyusedasafinalmetrictocompare
mayberepresentedincomputervision.Figs.11and13demonstrate
performancethroughoutallobjectcategories(Zouetal.,2019),which
throughgradientintensitiesthatRGBimageshavestrongerfeatures
weusetoevaluateexperimentswithboththeMaskR-CNNandYOLOv3
whencomparedtothermalimages.
models.
4.1.HOGresults
4.2.MaskRCNN
TheHOGmethodallowsforvisualfeatureextractionandcompari-
son, which can be seen in Figs. 10, 11, 12, and 13. The purpose of DuetoitsabilitytoproducehighlyaccuratemAPresponses,weuti-
usingtheHOGmethodinthisexperimentwastovisualizegradientdi- lizedtheMaskR-CNNarchitecturetoanalyzeandcompareresultsfrom
rectionandintensitybetweenRGBandthermalimagesofchilipeppers. theMaskR-CNNtotheYOLOv3model.Beforetraining,weensurethat
InFig.10,weshowtheprocessoftransformingtheoriginalimagein imageandmaskarrayshaveidenticalwidthandheight.Fig.14shows
Fig.10a,resizingtheimageinFig.10b,andfinallyplottinggradient four images with masks overlaid to confirm masks were loading
Fig.17.Predictionsfrompre-trainedYOLOWeights:(a)Out-of-Boxchilipepperprediction,(b)TRAINEDchilipepperprediction.
112S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
properly.Forexample,Fig.14doutputanimageshapeof(881,562,3) algorithm has identified and located all chili peppers in the image
andamaskshapeof(881,562,1).Weutilizethefollowingtool: (withdifferentcolors),assignedapredictionboundingbox,andlabeled
itwiththeappropriatelabel(chilipepper).AscoveredinSection3.2,
visualize.display_instance precision and recall are determined by evaluating truth bounding
ThispythontoolisusedtocalltheplotinFig.15.Here,wedemon- boxesandpredictedboundingboxes.WhiletrainingontheRGBdataset,
stratetheabilityofthisparticulararchitecturetodetectchilipeppers theMaskR-CNNperformedwithatrainmAPof0.872andtestmAPof
in an environmentwith debris.We visualizethattheMask R-CNN 0.896withanoverallcomputationaltimeof40.79spertestimage.
Fig.18.AdditionalpredictionsonvalidationdatasetwithYOLOv3:(a)Predictiontestimage2,(b)PredictionTestImage3,(c)PredictionTestImage4,(d)Predictiontestimage5.
113S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.19.PredictionsongreenchilitestsetviaYOLOv3:(a)Predictiontestimage2,(b)Predictiontestimage3,(c)Predictiontestimage4.
Fig.16visualizesthetruthandpredictionboundingboxesfor2im-
agesofchilipeppersduringtesting.Overall,theMaskR-CNNmodelap-
pearstoplacepredictingboxeswell.Fig.16b,showsthatthemodel
failedtodetect(FalseNegative)achilipeppercoveredindebris.
4.3.YOLOv3forreal-timedetection
WeutilizetheYOLOv3architectureforreal-timeapplicationdueto
itshighperformanceandfastcomputingcapabilities.Wenoticedthat
Table3
MaskR-CNNmAPvalues.
RGB Thermal
Train Test Train Test
0.872 0.896 0.391 0.298
Fig.20.Computationaltime(sec)fortwo-stageandone-stagealgorithms.
114S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Fig.21.ThermalimageswithMasks:(a)Mask1,(b)Mask2,(c)Mask3,(d)Mask4.
traininglossdropsbelow2.0roughlyatthe250thepoch.Aftertraining real-timeroboticharvestingwheretheenvironmentmaybedensewith
wascomplete,weevaluatedthemodelusingmAPperformancemetrics. debris.
Bothtrainingandtestingresultedin100%precisionaccuracy(mAP) Fig.20comparesthecomputationalcapabilitiesofMask-RCNN(red)
withacomputationaltimeof3.64spertestimage. toYOLOv3(blue).TheYOLOv3algorithmhassuperiorcapabilitiestothe
Aninitialtestwasconductedusingout-of-boxpre-trainedweights Mask-RCNNwithovertentimesthecomputationalspeedonthechili
andclassesonthechilidataset.Fig.17comparestheout-of-boxlikeness dataset.Forthisreason,webelievetheYOLOv3algorithmwillbesuit-
scoresofachilipeppertothatofachilipepperimageaftertrainingwas ableinaharvestingrobotforreal-timeobjectdetection.Computational
complete. speedsfortheYOLOv3andMaskR-CNNarethesameontheRGBand
Fig.18displaysconfidencelevelsofadditionaltestimages.Asstated thermalimages.ThefollowingsectionreportsmAPvaluesandcom-
earlierinthetext,confidencescoresmostlyevaluatetestimagesbe- paresclassificationperformanceforthetwomodelswiththeRGBand
tween97and100%asthepredefinedclassof‘chilepepper’. thermaldatasets.
ConfidencescoressuggestthattheYOLOv3modelisfunctioning Inthissection,usingtheRGBdataset,itwasshownthataccuracyof
withahighlevelofprecision.Theredoesappeartobeanissueofdebris predictioncansignificantlydecreasewhenthereareheavydebrisand
inFig.19a).Here,wecanseethatoneofthepeppersisonlydetected overlappingofpeppers.Inaddition,thevariantambientlightingcan
withaconfidenceof33%whichindicatesapossibleimpedimentduring hampertheperformanceofdetectionalgorithmsbasedonRGBdata.
Fig.22.ComparisonofthermalMaskR-CNN:(a)Actualplot,(b)Predictedplot.
115S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
Table4 PerformanceoftheMaskR-CNNduringthisportionofourinvestiga-
YOLOv3mAPvalues. tionleadsustodoubtthepossibilityofusingthemodelforreal-timeap-
RGB Thermal plicationswiththermalimagery.TheYOLOv3model,howevershowed
promisingresults.Table4displaysRGBandthermalmAPvaluesfor
Train Test Train Test
theYOLOv3architecture.Thereisaslightdeclineinperformancefrom
1.0 1.0 0.99 0.97 RGBtothermal;howevercomparedtothedeclinethatoccurredwith
theMaskR-CNNarchitecture,thisdeclineisacceptable.
Fig.23.ThermalchilipredictionsonvalidationdatasetwithYOLOv3:(a)Predictiontestimage2,(b)Predictiontestimage3,(c)Predictiontestimage4,(d)Predictiontestimage5.
Inthefollowingsection,non-destructivethermalimagingwillbeused Fig.23demonstrateshowthermalimagingcanbeusefulforpredic-
toresolvetheseissuesaswellashighlightresultsoftheMaskR-CNN tionsmadewithdebris.Whenwecomparetestimages2,3,and5in
andYOLOv3modelsusedwiththethermaldataset. Figs.18and23wecanseethatpredictionscoresdonotsignificantly
change. However, test image 4 shows a dramatic improvement in
Fig.23.InFig.18(c)wherethereisoverlappingofpeppers,prediction
4.4.Thermalimaging scoresforthepeppersarereportedas1.00and0.67.Comparedtothe
samepepperconfiguration,Fig.23(c)showsthatbothpeppersarepre-
ThesameprocessconductedwiththeRGBdatasetwasusedwiththe dictedatscoresof0.97.
thermaldataset.80%oftheimageswereusedfortrainingwith20%held PredictionperformanceofYOLOv3withthethermalimagedataset
fortesting.PredictionsfromtheMaskR-CNNmodeldidnotbenefit
showcasesthepossibilityofutilizingobjectdetectionforrobotichar-
fromthermalimagingandPredictionperformancewasdecreaseddras- vestingwithdebris,paperoverlapping,andlowlightpresent.Further
tically.Table3displaysthecomparisonofmAPvaluesfromtheRGBand testingshouldbeconductedtodeterminethepossibilityofusingobject
thermalimagerydatasetsshowingprecisionreducedoverhalf.Wesee detectionforroboticharvestingduringeveningandnighthours.
testingmAPvaluesdropbyalmost0.6fortheMaskR-CNN,whichis
concerningsincethesemodelsonlypredictononeclass.
Fig.21showshowthemaskswereappliedduringtrainingtoavari- 5.Conclusion
etyofdifferentimages.Fig.21ademonstratesanimageinheavydebris
duetothepepperlocationdirectlybehindtheplantstem.Figs.21b Inthisstudy,utilizedRGBandthermalimagesofchilipeppersinan
through21ddemonstratepeppersinlightdebris.Weseehowdebris environmentofvariousamountsofdebris,pepperoverlapping,andam-
negativelyaffectspredictionfortheMasR-CNNinFig.22.Adetailedin- bientlighting,trainthisdataset,andcompareobjectdetectionmethods.
spectionofthefigurereflectsthetabularresultsbycomparingactual ResultsoffeaturevisualizationwithHOGandobjectdetectionwith
andpredictedplotsofatestimage.InFig.22a,theimageshowsone MaskR-CNNandYOLOv3architecturesforanovelchilipepperimage
boundingboxappliedtoasinglepepperhowever,theMaskR-CNN datasetwerereported.HOGhelpeddisplaygradientedgedirections,
modelpredicted7boundingboxesonthetestimagewhichdecreases andtheMaskR-CNNmodelprovidedsuitablepredictionperformance
themAPvaluesignificantly. for less than real-time detection on the RGB dataset. The YOLOv3
116S.C.Hespeler,H.NematiandE.Dehghan-Niri ArtificialIntelligenceinAgriculture5(2021)102–117
algorithmhassuperiorcapabilitiestotheMask-RCNNwithoverten Hameed,K.,Chai,D.,Rassau,A.,2018.Acomprehensivereviewoffruitandvegetableclas-
sificationtechniques. ImageVis.Comput. 80,24–44. https://doi.org/10.1016/j.
times the computational speed on the chili dataset. The YOLOv3
imavis.2018.09.016.
modelintroducesaspikeinmodelperformanceforreal-timedetection Hameed,K.,Chai,D.,Rassau,A.,2020a.Aprogressiveweightedaverageweightoptimisa-
(~3spredictions)anddemonstratesthepossibilityofbeingutilized tionensembletechniqueforfruitandvegetableclassification.202016thInternational
ConferenceonControl,Automation,RoboticsandVision(ICARCV).IEEE,pp.303–308.
duringreal-timeroboticharvesting.However,wereportedalackof
Hameed,K.,Chai,D.,Rassau,A.,2020b.Asampleweightandadaboostcnn-basedcoarse
confidenceindetectionwhenheavyamountsofdebrisandpaperover- tofineclassificationoffruitandvegetablesatasupermarketself-checkout.Appl.Sci.
lappingarepresentinRGBimages.Inthisstudy,itwasshownthatnon- 10,8667.https://doi.org/10.3390/app10238667.
destructivethermalimagingcanresolvetheseissues.Inparticular, He,K.,Zhang,X.,Ren,S.,Sun,J.,2015.Spatialpyramidpoolingindeepconvolutionalnet-
worksforvisualrecognition.IEEETrans.PatternAnal.Mach.Intell.37,1904–1916.
YOLOv3demonstratedanabilitytodetectpeppersinareaswithhigh https://doi.org/10.1109/TPAMI.2015.2389824.
debrisandpepperoverlappingwiththethermalimagery;however He,K.,Gkioxari,G.,Dollár,P.,Girshick,R.,2017.Maskr-cnn.ProceedingsoftheIEEEInter-
theMaskR-CNNmodeldidnotdetectwellonthethermalimagery.It
nationalConferenceonComputerVision,pp.2961–2969.https://doi.org/10.1109/
ICCV.2017.322.
wasshownthatmappingtemperaturedifferencesbetweenthepepper
Jiang,J.,Cen,H.,Zhang,C.,Lyu,X.,Weng,H.,Xu,H.,He,Y.,2018.Nondestructivequality
andplant/debriscanprovidesignificantfeaturesforobjectdetectionin assessmentofchilipeppersusingnear-infraredhyperspectralimagingcombined
real-timeandcanhelpimproveaccuracypredictionswithheavydebris,
withmultivariateanalysis.PostharvestBiol.Technol.146,147–154.https://doi.org/
10.1016/j.postharvbio.2018.09.003.
variantambientlighting,andoverlappingofpeppers.Inaddition,suc-
Kang,H.,Zhou,H.,Wang,X.,Chen,C.,2020.Real-timefruitrecognitionandgraspinges-
cessfulthermalimagingforreal-timeroboticharvestingcouldallow timationforroboticappleharvesting.Sensors20,5670.https://doi.org/10.3390/
theharvestingperiodtobecomemoreefficientandopenupharvesting s20195670.
Krizhevsky,A.,Sutskever,I.,Hinton,G.E.,2012.Imagenetclassificationwithdeep
opportunityintheeveninghoursorlowlightsituations.
convolutionalneuralnetworks.In:Pereira,F.,Burges,C.J.C.,Bottou,L.,Weinberger,
Furtherinvestigationshouldbepursuedtoimproveobjectdetection K.Q.(Eds.),AdvancesinNeuralInformationProcessingSystems.CurranAssociates,
whendensedebrisandpoorambientlightarepresent.Oneissueinour Inc,pp.1097–1105.https://doi.org/10.1145/3065386.
Lenc,K.,Vedaldi,A.,2015.R-cnnminusr.arXivhttps://doi.org/10.5244/C.29.5preprint
experimentationisthatonlyoneclasswasusedduringtraining.This
arXiv:1506.06981.
issuecouldleadtooverfittingandcouldbethereasonthatYOLOv3per-
Li,S.,Luo,H.,Hu,M.,Zhang,M.,Feng,J.,Liu,Y.,Dong,Q.,Liu,B.,2019.Opticalnon-
formedsowellonthedatasets.Additionalclassesandvariationsinim- destructivetechniquesforsmallberryfruits:areview.Artif.Intel.Agric.2,85–98.
agery type should be utilized during training and testing to avoid https://doi.org/10.1016/j.aiia.2019.07.002.
Lin,T.Y.,Dollár,P.,Girshick,R.,He,K.,Hariharan,B.,Belongie,S.,2017.Featurepyramid
overfitting.Weplantoexpandthenumberandtypeofimagesinthe
networksforobjectdetection.ProceedingsoftheIEEEConferenceonComputerVi-
datasetanddetermineiftheYOLOv3(orothermodels)canexceedper- sionandPatternRecognition,pp.2117–2125.
formanceexpectationsduringreal-timeroboticharvestingindensede- Naranjo-Torres,J.,Mora,M.,Hernández-García,R.,Barrientos,R.J.,Fredes,C.,Valenzuela,
A.,2020.Areviewofconvolutionalneuralnetworkappliedtofruitimageprocessing.
bris and poor lighting environments with peppers in natural
Appl.Sci.10,3443.https://doi.org/10.3390/app10103443.
environment. Patel,H.N.,Jain,R.,Joshi,M.V.,etal.,2011.Fruitdetectionusingimprovedmultiplefeatures
basedalgorithm.Int.J.Comput.Appl.13,1–5.https://doi.org/10.5120/1756-2395.
Poirson,P.,Ammirato,P.,Fu,C.Y.,Liu,W.,Kosecka,J.,Berg,A.C.,2016.Fastsingleshotde-
tectionandposeestimation.2016FourthInternationalConferenceon3DVision
DeclarationofCompetingInterest (3DV).IEEE,pp.676–684.https://doi.org/10.1109/3DV.2016.78.
Redmon,J.,Farhadi,A.,2017.Yolo9000:better,faster,stronger.ProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition,pp.7263–7271.https://
Theauthorsdeclarethattheyhavenoknowncompetingfinancial
doi.org/10.1109/CVPR.2017.690.
interestsorpersonalrelationshipsthatcouldhaveappearedtoinflu- Redmon,J.,Farhadi,A.,2018.Yolov3:Anincrementalimprovement.Proceedingsofthe
encetheworkreportedinthispaper.
IEEEConferenceonComputerVisionandPatternRecognition(CVPR).779–788.
arXivpreprintarXiv:1804.02767.
Redmon,J.,Divvala,S.,Girshick,R.,Farhadi,A.,2016.Youonlylookonce:unified,real-
References timeobjectdetection.ProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pp.779–788.https://doi.org/10.1109/CVPR.2016.91.
Abdulla,W.,2017.Maskr-cnnforObjectDetectionandInstanceSegmentationonKeras Ren,S.,He,K.,Girshick,R.,Sun,J.,2015.Fasterr-cnn:towardsreal-timeobjectdetection
andTensorflow.https://github.com/matterport/MaskRCNN. withregionproposalnetworks.arXivhttps://doi.org/10.1109/TPAMI.2016.2577031
Bochkovskiy,A.,2020.Yolov3WithDarknetFramework.https://github.com/AlexeyAB/ preprintarXiv:1506.01497.
darknet. Sadeghi,M.A.,Forsyth,D.,2014.30hzobjectdetectionwithdpmv5.EuropeanConference
Bochkovskiy,A.,Wang,C.Y.,Liao,H.Y.M.,2020.Yolov4:optimalspeedandaccuracyofob-
onComputerVision.65–79.Springer.https://doi.org/10.1007/978-3-319-10590-1-5.
jectdetection.arXiv2004.10934(preprintarXiv:2004.10934). tzutalin,2017.GraphicalImageAnnotationTool.https://github.com/tzutalin/labelImg.
Uijlings,J.R.,VanDeSande,K.E.,Gevers,T.,Smeulders,A.W.,2013.Selectivesearchforob-
Bosland,P.W.,Bailey,A.L.,Cotter,D.J.,1991.GrowingChilesinNewMexico.GuideH-New
jectrecognition.Int.J.Comput.Vis.104,154–171.https://doi.org/10.1007/s11263-
MexicoStateUniversity,CooperativeExtensionService(USA).
013-0620-5.
CommonObjectsinContext(COCO),2015.DetectionEvaluate.https://cocodataset.org/
Vadivambal,R.,Jayas,D.S.,2011.Applicationsofthermalimaginginagricultureandfood
detection-eval.Online;accessed19January2021. industry—areview.FoodBioprocessTechnol.4,186–199.https://doi.org/10.1007/
Dalal,N.,Triggs,B.,2005.Histogramsoforientedgradientsforhumandetection.2005 s11947-010-0333-5.
IEEEcomputersocietyconferenceoncomputervisionandpatternrecognition Viola,P.,Jones,M.,2001.Rapidobjectdetectionusingaboostedcascadeofsimplefea-
(CVPR’05). 886–893.IEEE.https://doi.org/10.1109/CVPR.2005.177.
tures.Proceedingsofthe2001IEEEComputerSocietyConferenceonComputerVi-
Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.,2009.ImageNet:alarge-scalehierar- sion and Pattern Recognition. CVPR 2001. IEEE https://doi.org/10.1109/
chicalimagedatabase.CVPR09,pp.248–255. CVPR.2001.990517Pp.I–I.
Everingham,M.,VanGool,L.,Williams,C.K.I.,Winn,J.,Zisserman,A.,2007.ThePASCAL Viola,P.,Jones,M.J.,2004.Robustreal-timefacedetection.Int.J.Comput.Vis.57,137–154.
VisualObjectClassesChallenge2007(VOC2007)Results.http://www.pascal-net- https://doi.org/10.1023/B:VISI.0000013087.49260.fb.
work.org/challenges/VOC/voc2007/workshop/index.html. Viola,P.,Jones,M.J.,Snow,D.,2005.Detectingpedestriansusingpatternsofmotionand
FAO,2019.FoodandAgricultureOrganizationoftheUnitedNationsStatisticsDivision. appearance. Int. J. Comput. Vis. 63, 153–161. https://doi.org/10.1109/
(ACCESSED23FEBRUARY2021).http://www.fao.org/faostat/en/data/QC/visualize/. ICCV.2003.1238422.
Felzenszwalb,P.,McAllester,D.,Ramanan,D.,2008.Adiscriminativelytrained,multiscale, vanderWalt,S.,Schönberger,J.L.,Nunez-Iglesias,J.,Boulogne,F.,Warner,J.D.,Yager,N.,
deformablepartmodel.2008IEEEConferenceonComputerVisionandPatternRec- Gouillart,E.,Yu,T.,thescikit-imagecontributors,2014.scikit-image:imageprocess-
ognition. 1–8.IEEE.https://doi.org/10.1109/CVPR.2008.4587597. inginPython.PeerJ2,e453.https://doi.org/10.7717/peerj.453.
Gao,Z.,Shao,Y.,Xuan,G.,Wang,Y.,Liu,Y.,Han,X.,2020.Real-timehyperspectralimaging Wan,S.,Goudos,S.,2020.Fasterr-cnnformulti-classfruitdetectionusingaroboticvision
forthein-fieldestimationofstrawberryripenesswithdeeplearning.Artif.Intell. system.Comput.Netw.168,107036.https://doi.org/10.1016/j.comnet.2019.107036.
Agric.https://doi.org/10.1016/j.aiia.2020.04.003. Yan,J.,Lei,Z.,Wen,L.,Li,S.Z.,2014.Thefastestdeformablepartmodelforobjectdetec-
Girshick,R.,2015.Fastr-cnn.ProceedingsoftheIEEEInternationalConferenceonCom- tion.ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-
puterVision,pp.1440–1448.https://doi.org/10.1109/ICCV.2015.169. tion,pp.2497–2504.https://doi.org/10.1109/CVPR.2014.320.
Yu,Y.,Zhang,K.,Liu,H.,Yang,L.,Zhang,D.,2020.Real-timevisuallocalizationofthepick-
Girshick,R.,Donahue,J.,Darrell,T.,Malik,J.,2014.Richfeaturehierarchiesforaccurateobject
ing points for a ridge-planting strawberry harvesting robot. IEEE Access 8,
detectionandsemanticsegmentation.ProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pp.580–587.https://doi.org/10.1109/CVPR.2014.81.
116556–116568.https://doi.org/10.1109/ACCESS.2020.3003034.
Zou,Z.,Shi,Z.,Guo,Y.,Ye,J.,2019.Objectdetectionin20years:asurvey.arXiv
Gu,J.,Wang,Z.,Kuen,J.,Ma,L.,Shahroudy,A.,Shuai,B.,Liu,T.,Wang,X.,Wang,G.,Cai,J.,
1905.05055preprintarXiv:1905.05055.
etal.,2018.Recentadvancesinconvolutionalneuralnetworks.PatternRecogn.77,
354–377.https://doi.org/10.1016/j.patcog.2017.10.013.
117