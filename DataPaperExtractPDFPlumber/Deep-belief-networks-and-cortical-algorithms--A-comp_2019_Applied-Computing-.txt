AppliedComputingandInformatics15(2019)81–93
ContentslistsavailableatScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
Deep belief networks and cortical algorithms: A comparative study
for supervised classification
⇑
Yara Rizk, Nadine Hajj, Nicholas Mitri, Mariette Awad
DepartmentofElectricalandComputerEngineering,AmericanUniversityofBeirut,Beirut,Lebanon
a r t i c l e i n f o a b s t r a c t
Articlehistory: Thefailureofshallowneuralnetworkarchitecturesinreplicatinghumanintelligenceledthemachine
Received23March2017 learningcommunitytofocusondeeplearning,tocomputationallymatchhumanintelligence.Thewide
Revised17January2018 availabilityofincreasingcomputingpowercoupledwiththedevelopmentofmoreefficienttrainingalgo-
Accepted17January2018
rithmshaveallowedtheimplementationofdeeplearningprinciplesinamannerandspanthathadnot
Availableonline3March2018
beenpreviouslypossible.Thishasledtotheinceptionofdeeparchitecturesthatcapitalizeonrecent
advances in artificial intelligence and insights from cognitive neuroscience to provide better learning
Keywords:
solutions. In this paper, we discuss two such algorithms that represent different approaches to deep
Deeplearning
learning with varied levels of maturity. The more mature but less biologically inspired Deep Belief
Deepbeliefnetworks
Network(DBN)andthemorebiologicallygroundedCorticalAlgorithms(CA)arefirstintroducedtogive
Corticalalgorithms
readersabird’seyeviewofthehigher-levelconceptsthatmakeupthesealgorithms,aswellassomeof
their technical underpinnings and applications. Their theoretical computational complexity is then
derivedbeforecomparingtheirempiricalperformanceonsomepubliclyavailableclassificationdatasets.
MultiplenetworkarchitectureswerecomparedandshowedthatCAoutperformedDBNonmostdatasets,
withthebestnetworkarchitectureconsistingofsixhiddenlayers.
(cid:1)2018TheAuthors.ProductionandhostingbyElsevierB.V.onbehalfofKingSaudUniversity.Thisisan
openaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).
Contents
1. Introduction.......................................................................................................... 82
2. Artificialneuralnetworkshistory......................................................................................... 82
2.1. Conceptsfromneuroscience........................................................................................ 82
2.2. Shallowbeginnings............................................................................................... 83
2.3. Shallownetworks’limitations ...................................................................................... 83
2.4. Deeparchitectures ............................................................................................... 83
3. Deepbeliefnetworks................................................................................................... 84
3.1. Overview....................................................................................................... 84
3.2. Networkstructure................................................................................................ 84
3.2.1. RestrictedBoltzmannmachines.............................................................................. 84
3.2.2. Deepbeliefnetworks...................................................................................... 84
3.3. Trainingalgorithm................................................................................................ 85
3.3.1. RestrictedBoltzmannmachines.............................................................................. 85
3.3.2. Deepbeliefnetworks...................................................................................... 85
⇑
Correspondingauthor.
E-mailaddresses:yar01@aub.edu.lb(Y.Rizk),njh05@aub.edu.lb(N.Hajj),ngm04@aub.edu.lb(N.Mitri),mariette.awad@aub.edu.lb(M.Awad).
PeerreviewunderresponsibilityofKingSaudUniversity.
Production and hosting by Elsevier
https://doi.org/10.1016/j.aci.2018.01.004
2210-8327/(cid:1)2018TheAuthors.ProductionandhostingbyElsevierB.V.onbehalfofKingSaudUniversity.
ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).82 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
4. Corticalalgorithms..................................................................................................... 85
4.1. Overview....................................................................................................... 85
4.2. Networkstructure................................................................................................ 85
4.3. Mathematicalmodel.............................................................................................. 86
4.4. Trainingalgorithm................................................................................................ 86
4.4.1. Randominitialization...................................................................................... 86
4.4.2. Unsupervisedfeed-forwardlearning.......................................................................... 86
4.4.3. Supervisedfeedbacklearning ............................................................................... 87
5. Theoreticalcomputationalcomplexity..................................................................................... 87
5.1. Numberofnon-zeroweights....................................................................................... 88
5.1.1. DBN.................................................................................................... 88
5.1.2. CA ..................................................................................................... 88
5.2. Numberofoperationsperneuron................................................................................... 88
5.2.1. Summation.............................................................................................. 88
5.2.2. Activationfunction........................................................................................ 88
5.3. Pruning......................................................................................................... 89
5.4. Overallcomputationalcomplexity................................................................................... 89
6. Empiricalcomparison .................................................................................................. 89
6.1. Experimentalsetup............................................................................................... 89
6.2. Classificationresults.............................................................................................. 91
6.3. Networkconnectivity............................................................................................. 91
6.4. Effectofbatchsize ............................................................................................... 91
6.5. Statisticalanalysis................................................................................................ 91
7. Conclusion ........................................................................................................... 92
Acknowledgment...................................................................................................... 92
References ........................................................................................................... 92
1.Introduction DBN,ispresentedasthestateoftheartofANNintheirtraditional
formswithnetworktopologiesbuiltfromlayersofneuronmodels
In an endeavor to replicate human level intelligence, artificial butwithmoreadvancedlearningmechanicsanddeeperarchitec-
intelligence(AI)researchhasfusedinsightsfromthefieldsofcom- ture,withoutmodelingthedetailedbiologicalphenomenaconsti-
puterscience,cognitiveneuroscience,computationalscience,anda tutinghumanintelligence.Maintainingahigh-levelabstractionof
litanyofotherstoproducealgorithmsthatperformwithincreasing the biological modeling, results in simpler mathematical models
efficacy on what is arguably the core element of intelligence: forDBNcomparedtoCA.Ontheotherhand,CArepresentstheshift
learning. towards incorporating more biologically inspired structures than
NotableamongthemanylearningalgorithmsinAIareartificial DBN,likecorticalcolumnsandinhibitingandstrengtheninglearn-
neural networks (ANN) and their many variants. ANN are collec- ingrules,asoutlinedbyEdelmanandMountcastle’swork[7].
tionsofinterconnectedartificialneuronsthatincrementallylearn ThestructureofthepaperissuchthatSection2summarizesthe
from their environment and attempt to mimic some of the basic historyofANNwhileSections3and4reviewthefundamentalcon-
information processing processes in the brain. Their function is ceptsandlearningschemesofDBNandCA,respectively.Section5
definedbytheprocessingperformedattheneuronlevel,thecon- derives both algorithms’ theoretical computational complexity.
nection strengths between neurons (synaptic weights), and net- Finally,Section6presents an empirical comparison onclassifica-
workstructure(organizationandlinkageofneurons)[1].Itisthe tiontasksbeforeconcludingwithclosingremarksinSection7.
latterthatresidesatthecoreofthediscussionpresentedherein.
Throughout their evolution, discussed in more details in the
2.Artificialneuralnetworkshistory
nextsection, shallow ANNstill suffer frommultiple issues in the
contextofcomplexapplicationsrequiringahigherlevelofabstrac-
Beforedelvingintothedeepernetworkstructurespresentedin
tion. However, with the rapid increase in processing power, the
thispaper,wewillgoovertheevolutionofneuralnetworksfrom
opportunity to successfully implement the computationally
their shallow beginnings to the complex structures that have
demandingdesignsofdeeperarchitectureshasrecentlyemerged.
recentlybecomepopular.
The development of efficient training algorithm such as Hinton
et al.’s greedy algorithm [2] has also helped ANN’s resurgence.
Furthermore, findings in computational neuroscience have led to 2.1.Conceptsfromneuroscience
increased interest in deep, biologically inspired architectures
[3–5]whichadheremorefaithfullytoneuro-scientifictheoriesof Despitetheadvancesinneuroscienceandtechnologythathave
thehumanbrain’stopology. allowedforadetaileddescriptionofthestructureofthebrain,the
In this paper, we limit the scope of our comparative study to learning process in the brain is yet to be completely understood.
two - nowadays popular - algorithms: Hinton et al.’s Deep Belief Biologically,thebrainmainlyconsistsofthecerebrum,thecerebel-
Networks(DBN)[2],andCorticalAlgorithms(CA)[6].Whilemany lum,andthebrainstem[8].
other deep architectures have been developed, including long Thecerebralcortex,biologicallydefinedastheouterlayeroftis-
short-term memory for sequential data processing and convolu- sueinthecerebrumandbelievedtoberesponsibleforhigherorder
tional neural networks for image processing, this comparative functioning, is an association of an estimated 25 billion neurons
study compares feedforward architectures. Specifically, DBN, one interconnected through thousands of kilometers of axons propa-
ofthemoreefficientdeeparchitecturetrainingalgorithmsiscom- gating and spreading about 1014 synapses simultaneously [9],
pared to CA, a feedforward architecture with more biologically arranged in six layers and divided into regions, each performing
faithful properties. Deep neural networks (DNN), specifically aspecifictask[10].Y.Rizketal./AppliedComputingandInformatics15(2019)81–93 83
Though it is not very clear how certain areas in the brain computationallydemanding.Furthermore,non-lineardataismore
become specialized, it is known that multiple factors affect the difficulttodivideintoclassesduetotheinherentfeatureoverlap.
functionalspecializationofthebrainareassuchasstructure,con- UnabletopositionthemselvesasstrongAImodels-generalintel-
nectivity, physiology, development and evolution [11]. Neurons, ligentacts as definedby Kurzweil- whichcan faithfullyemulate
considered the basic element in the brain, have different shapes human intelligence, ANN lagged Support Vector Machines (SVM)
and sizes but are all variations of the same underlying scheme, [27]inthe1990–2000s.
i.e.theystartthesamegeneral-purposefunctionbutbecomespe-
cializedwithtraining[12].Whiledendritesarethesiteofreception 2.4.Deeparchitectures
of synaptic inputs, axons convey electrical signals over long dis-
tances.Inputstoneuronscauseaslowpotentialchangeinthestate The early 2000s saw a resurgence in ANN research due to
oftheneuron;itscharacteristicsaredeterminedbythemembrane increased processing power and the introduction of more effi-
capacitanceandresistanceallowingtemporalsummation[13]. cient training algorithms which made training deep architec-
Studies showed that the organization of the cortex can be tures feasible. Hinton et al.’s greedy training algorithm [2]
regardedasanassociationofcolumnarunits[14,15],eachcolumn simplified the training procedure of Boltzmann machines while
beingagroupofnodessharingthesameproperties.Learninginthe deep stacking networks broke down training to the constitut-
human brain is mainly performed using plastic connections, ing blocks of the deep network to reduce the computational
repeatedexposuresandfiringandinhibitionofneurons.Inasim- burden. Furthermore, Schmidhuber’s long short-term memory
plified manner, information flowing in the cortex causes connec- architecture [28] allowed the training of deeper recurrent neu-
tions in the brain to become active, over time, with repeated ral networks. While these architectures do not borrow biologi-
exposurestheseconnectionsarestrengthenedcreatingarepresen- cal properties from the brain beyond the neuron, deep
tationoftheinformationprocessedinthebrain.Moreover,inhibi- architectures with neural network topologies that adhere more
tion of neurons - physically defined as prohibiting neurons from faithfully to neuro-scientific theories of the human brain’s
firing-partlyaccountfortheforgettingprocess[16]. topology are gaining traction in the connectionist community
due in part to the momentum achieved in computational
2.2.Shallowbeginnings neuroscience.
Oneofthemajorandmostrelevantcontributionsinthatfield
At a nodal level, ANN started with the simplified McCulloch- was made by Edelman and Mountcastle [7]. Their findings lead
Pitts neural model (1943) [17], which was composed of a basic toashiftfrompositioningsimplifiedneuronmodelsasfundamen-
summation unit with a deterministic binary activation function. talfunctionalunitsofanarchitecturetoelevatingthatroletocor-
Successorsadded complexity withevery iteration. At the levelof ticalcolumns,collectionsofcellscharacterizedbycommonfeed-
activationfunctions,linear,sigmoid,andGaussianfunctionscame forwardconnectionsandstronginhibitoryinterconnections.This
into use. Outputs were no longer restricted to real values and providedabiologicallyfeasiblemechanismforlearningandform-
extendedtothecomplexdomain.Deterministicmodelsgaveway inginvariantrepresentationsofsensorypatternsthatearlierANN
tostochasticneurons and spikingneuronswhichsimulatedionic didnot.
exchanges.Alltheseadditionsweremadetoachievemoresophis- Additionally, two supplementary discoveries were believed to
ticatedlearningmodels. be key in emulating human intelligence. The first was the sus-
Atthenetworklevel,topologiesstartedoutwithsinglelayered pected existence of a common computational algorithm in the
architecturessuchasRosenblatt’sperceptron(1957)[18],Widrow neocortex [12]. This algorithm is pervasive throughout these
and Hoff’s ADALINE network (1960) [19] and Aizerman’s kernel regions irrespective of the underlying mental faculty. Whether
perceptron (1964) [20]. These architectures suffered from poor the task is visual, auditory, olfactory, or other, the brain seems
performance and could not learn the XOR problem, a simple but to deal with sensory information in very similar ways. The sec-
non-linearbinaryclassificationproblem.Thisledtotheintroduc- ond was the hierarchical structure of the human neocortex
tion of more complexnetworks starting withthe multilayer per- [12]. The brain’s regions are hierarchically connected so that
ceptron (Rumelhart, 1986) [21], self-recurrent Hopfield networks the bidirectional flow of information merges into more complex
(1986) [22], self-organizing maps (SOM or Kohonen networks, representations with every layer, further abstracting the sensory
1986) [23], adaptive resonance theory (ART) networks (1980s) stimuli.
[24]andvariousotherswhichareconsideredshallowarchitectures The combination of these two findings forms potential
duetothesmallnumberofhiddenlayers. grounds for building a framework that replicates human intelli-
Successive iterations incrementally improved on their prede- gence; a hierarchy of biologically inspired functional units that
cessors’ shortcomings and promised higher levels of intelligence, implement a common algorithm. These novel insights from neu-
a claim that was made partially feasible due to the hardware’s roscience have been reflected in the machine learning (ML) and
improvedcomputationalcapabilities[25]andduetothedevelop- AIfieldsandhavebeenimplementedtovaryinglayersinseveral
mentoffasterandmoreefficienttrainingandlearningalgorithms. algorithms.
Learning mechanics, whether supervised (back propagation) or While CA restructured the neurons and their connections
unsupervised (feed forward algorithms), matured in parallel and as well as the learning algorithm [6] based on Edelman
allowed for better performance in a varied set of specific tasks. and Mountcastle’s finding [7], other algorithms modeled other
Nonetheless, the compound effect of the innovation targeting all biological theories of the brain’s workings. Symbolic architec-
aspectsoftheseshallownetworkswasnotenoughtocapturetrue tures such as Adaptive Character of Thought (ACT-R) [29]
humanintelligencewhilelargecomputationalneedsthrottledthe modeled working memory coupled with centralized control
progressofdeepernetworks. that refers to long term memory when needed. Emergentist
architectures such as Hierarchical Temporal Memory (HTM)
2.3.Shallownetworks’limitations [30] are based on globalist memory models and use rein-
forcement or competitive learning schemes to generate their
Supervised learning presents many challenges including models. Integrating both classes of architectures to form
thecurseofdimensionality[26]wheretheincreaseinthenumber hybrid architectures also exist and include Learning Intelligent
of features and training samples makes learning more Distribution Agent (LIDA) [31].84 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
3.Deepbeliefnetworks [38,39], natural language processing [40–42], automatic speech
recognition [43–46] and feature extraction and reduction [47–
3.1.Overview 49],tonameafew.
DNN are deeper extensions of shallow ANN architectures that 3.2.Networkstructure
arecomposedofasimplifiedmathematicalmodelofthebiological
neuronbutdonotaimtofaithfullymodelthehumanbrainasdo 3.2.1.RestrictedBoltzmannmachines
CAorsomeotherMLapproaches.DNNarebasedontheNeocogni- RBM, first known as Harmonium by [50], are two-layer net-
tron, a biologically inspired image processing model [32], that works where only inter-layer neuron connections are allowed.
attempttorealizestrongAImodelsthroughhierarchicalabstrac- TheyareaspecialcaseofBoltzmannmachines(BM)whichallow
tion of knowledge. Information representation is learned as data both inter and intra-layer connections. RBM’s neurons form two
propagatesthroughthenetwork,shallowerlayerslearnlow-level disjointsets(asindicatedinFig.1bytheblackboxes),satisfying
statistical features while deeperlayers build on these features to thedefinitionofbipartitegraphs.Thus,trainingRBMislesscom-
learn more abstract and complex representations. Lacking clear plex and faster. The neuronconnectionsin RBMmay be directed
skills for logical inferences,DNN need more morphing tobe able or undirected; in the latter case, the network forms an auto-
to integrate abstract knowledge in a human manner. Recurrent associativememorywhichischaracterizedbybi-directionalinfor-
andconvolutionalneuralnetworks,firstintroducedinthe1980s, mationflowduetofeedbackconnections[2].
can be considered predecessors of DNN and were trained using
back-propagationwhichhasbeenavailablesince1974. 3.2.2.Deepbeliefnetworks
ANN were first trained using back-propagation, an algorithm DBNarestackeddirectedRBMs,exceptforthefirstRBMwhich
thatupdatesthenetworkweightsbypropagatingtheoutputerror containsundirectedconnections,asshowninFig.1.Thisnetwork
backwards through the network [33]. However, the propagated architecture significantly reduces the training complexity and
errorvanishestozeroasthenetworkdepthincreases,preventing makes deep learning feasible. Focusing on two layers of the net-
early-layer weights from updating and significantly reducing the work, the weighted edges connecting the various neurons are
performance of the network [34–37]. Thus, other training algo- annotated using the variable notation n‘ which implies that
rithmsforDNNwereinvestigated.In1992,Schmidhuberproposed
n;m
to train recurrent neural networks by pre-training layers in an
Table1
unsupervisedfashionthenfine-tuningthenetworkweightsusing
DBNnomenclature.
back-propagation [28]. Momentum further picked up in 2006
n‘ Weightoftheedgeconnectingthenthneuroninthe‘thlayertothe
whenHintonetal.proposedagreedytrainingalgorithmforDBN n;m
mthneuroninthe‘thþ1layer;‘issuppressedwhenthereareonly2
specifically.Inwhatfollows,werestrictourdiscussionofDNNto
layersinthenetwork
DBN, a popular and widely used deep architecture, trained using nr Vectorofconnectionweightsleavingthenthneuroninthe‘thlayer
n
Hintonetal.’salgorithm. n ‘ Matrixofweightsconnectingthe‘thlayertothe‘thþ1layer
WhileDBNisatypeofdeepANN,back-propagationfailstopro- l Learningrate
j NumberofGibbssamplingstepsperformedduringCD
duce a suitablemodelthatperformswell ontrainingand testing
N Hidden-layerneuroncardinality
data due to DBN’s architectural characteristics [2]. This has been
M Input-layerneuroncardinality
attributed to the ‘‘explaining away” phenomenon. Explaining L Numberofhiddenlayers
away,alsoknownas Berkson’sparadoxor selectionbias,renders t Samplingstep
the commonly held assumption of layer independence invalid Qð:j:Þ Conditionalprobabilitydistribution
and consequently adds complexity to the inference process. The
h‘ Binaryconfigurationofthe‘thlayer
hidden nodes become anti-correlated because their extremely
pðh‘Þ Priorprobabilityofh‘thecurrentweightvalues
x0 Inputlayerdatapoint
low probabilities make thechances ofboth firingsimultaneously
impossible.
xmðtÞ Binaryconfigurationofmthinput-layerneuronatsamplingstept
X Setoftrainingpoints
To remedy this issue, Hinton et al. proposed a training algo- Hn Binaryconfigurationvariableofneuronninthehiddenlayerat
rithmbasedontheobservationthatDBNcanbebrokendownto samplingstept
sequentially stacked restricted Boltzmann machines (RBM), a hðtÞ Binaryconfigurationvalueofneuronninthehiddenlayeratsampling
n
two-layernetworkinter-layerneuronconnectionsonly.Thisnovel stept
approach rekindled the interest in these deep architectures and cb m m ntt hh hin idp du et n-l -a lay ye er rn ne eu uro ron nb bia ias
s
n
saw DBN applied to many problems from image processing
Fig.1. DBNarchitecture[51].Y.Rizketal./AppliedComputingandInformatics15(2019)81–93 85
X
neuron n in layer ‘ is connected to neuron m in layer ‘þ1. An logpðx0ÞP Qðh0jx0Þðlogpðh0Þþlogpðx0jh0ÞÞ
exhaustivelistoftheentirenomenclatureadoptedinthissection X8h0
ð5Þ
isincludedinTable1. (cid:2) Qðh0jx0ÞlogQðh0jx0Þ
8h0
3.3.Trainingalgorithm
@logpðx0Þ X
3.3.1.RestrictedBoltzmannmachines ¼ Qðh0jx0Þlogpðh0Þ ð6Þ
@n
Hinton et al. proposed the contrastive divergence (CD) algo- n;m 8h0
rithmtotrainRBMinbothsupervisedandunsupervisedscenarios.
j After iteratively learning the weights of the network, the up-
CDestimatesthelog-likelihoodgradientusinga Gibbssampling
downalgorithm[2]fine-tunesthenetworkweights.Thisalgorithm
steps which is typically set to 1. The optimal weight vector is
isasupervisedvariantofthewake-sleepalgorithmthatusesCDto
obtainedbymaximizingtheobjectivefunctionin(1)throughgra-
modifythenetworkweights.Thewake-sleepalgorithm[60]isan
dientdescent[52].CD’spseudo-codeissummarizedinTable2.
unsupervised algorithm used to train neural networks in two
Gibbs sampling is a randomized MCMC algorithm that allows
phases: the ‘‘wake” phase is applied on the feed-forward path to
thesamplingofapproximate samplesfrom amultivariate proba-
computetheweightsandthe‘‘sleep”phaseisappliedonthefeed-
bilitydistribution[53].Thegeneratedsamplesarecorrelatedand
backpath.Theup-downalgorithm,describedinTable3,isapplied
form a Markov chain. Eq. (2) describes the energy function that
tonetworktoreduceunder-fittingwhichiscommonlyobservedin
represents the joint probability distribution, derived from Gibbs
d wi est igri hb tu st ;io hn aa nn ddc xalc cu al nat te ad keus vi an lg ue(3 s) i. nn n t; hm e,b sm etan f0d ;c 1n ga [r 5e 4r ]e .alvalued gre Se pd eil cy i- fit cra ai ln lye ,d inne tt hw eo firk rss t. phase (up-pass) of the algorithm, the
n m weights on the directed connections, termed generative weights
maxnP x2XPðxÞ ð1Þ orparameters,aremodifiedbycalculatingthewake-phaseproba-
bilities,samplingthestates,andupdatingtheweightsusingCD.On
Eðx;hÞ¼(cid:2)XN XM
n n;mh nx
m(cid:2)XM
b mx
m(cid:2)XN
c nh
n
ð2Þ t vh ae teo st eh ae rr lih ea rn ld ay, eth rse ts he rc oo un gd hp th ha es te o( pd -o dw own- npa lis ns k) ss ,t to ec rh ma est dic ia nl fl ey rea nct ci e-
n¼1m¼1 m¼1 n¼1
weights or parameters. The sleep-phase probabilities are calcu-
1 lated,thestatesaresampledandtheoutputisestimated.
pðx;hÞ¼P P e(cid:2)Eðx;hÞ ð3Þ
e(cid:2)Eðx;hÞ
x h
4.Corticalalgorithms
3.3.2.Deepbeliefnetworks
A simple and efficient layer-wise training algorithm was pro- 4.1.Overview
posed for DBN by Hinton et al. in 2006 [2]. It trains the layers
sequentiallyandgreedilybytyingtheweightsofunlearnedlayers, CA are a deep artificialneural network model,which borrows
usingCDtolearntheweightsofasinglelayeranditeratinguntilall several concepts and aspects from the human brain. The main
layers are trained. Tying the weights not only allows us to use inspirationisdrawnfromthefindingsofEdelmanandMountcastle
RBM’s training algorithm but also eliminates the ‘‘explaining [15,7],whichstatethatthebrainiscomposedofcorticalcolumns
away” phenomenon. Then, the network weights are fine-tuned arranged in six layers. He also uses the concept of strengthening
usingatwo-pass‘‘up-down”algorithm. andinhibitingtobuildacomputationaltrainingalgorithmcapable
In general, deep networks are pre-trained using unsupervised ofextractingmeaningfulinformationfromthesensoryinputand
learning before using labeled data to improve the model with creatinginvariantrepresentationsofpatterns.Furtherdescription
supervisedlearning.Thisschemealmostalwaysoutperformsnet- oftheCAmodelanditsbiologicallyplausibleaspectscanbefound
works learned without pre-training [56] since this phase acts as in[61,51,62].
aregularizer[57,58]andaid[59]forthesupervisedoptimization
problem. 4.2.Networkstructure
The energy contained in the directed model can be calculated
using (4) where the maximum energy is upper bounded by (5) A typical CA network consists of an association of columns
and achieves equality when the network weights are tied. At grouped in layers or levels. A column is a collection of neurons
equality, the derivative is equal to (6) and is used to solve the associatedwiththesamestimulus,asshowninFig.2.Hence,CA
nowsimplermaximizationproblem. can be considered as a three-level hierarchy structure. The neu-
Eðx0;h0Þ¼(cid:2)ðlogpðh0Þþlogpðx0jh0ÞÞ ð4Þ rons,likeinotherneuralnetworkarchitectures,useanactivation
Table3
Up-Downtrainingalgorithmworkflow[2].
Table2
ContrastivedivergenceworkflowusedintrainingRBM[55]. 1. Bottom-upphase
a. Calculatewake-phaseprobabilities
1. Settheweightstozero:n ‘¼0;‘¼1...L b. Samplenetworkstates
2. 8x2X c. Usingwake-phaseprobabilities,calculateCDstatistics
a.Propagatetraininginstancethroughthenetwork d. PerformGibbssamplingforjiterations
b.Forjsamplingsteps
e. Using(1.d),calculatesleep-phaseCDstatistics
i.LoopoverNhidden-layerneuronsandsampleh nðtÞ(cid:3)pðhnjxðtÞÞ 2. Down-passphase
ii.LoopoverMinput-layerneuronsandsamplexmðtÞ(cid:3)pðxmjhðtÞÞ a b.
.
C Sao mm pp lu ete nes tl wee op r- kph sta as te esprobabilitiesthroughtop-downpass
c.Loopoverinputandhidden-layerneuronsandcompute
i.Dn n;m¼Dn n;mþpðHn¼1jxð0ÞÞxð m0Þ(cid:2)pðHn¼1jxðjÞÞxð mjÞ
3.
c R. e-compuE ts etim gea nt ee ran te it vw eo wrk eio gu ht tp sut
ii.Db m¼Db mþxmð0Þ(cid:2)xð mjÞ 4. Re-computenetwork’sdirectedsub-graphweights
iii.Dc n¼Dc nþpðHn¼1jxð0ÞÞ(cid:2)pðHn¼1jxðjÞÞ 5. Re-computeinferenceweights86 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
synapsingateachofitsneuronsasshowninEq.(7).Epochnumber
isdenotedbye,layernumberbyl,thereceivingcolumnindexbyc,
thereceivingneuronnandthesendingcolumnbys.
Defining!l;e astheoutputvectoroflevellforepocheandtl;e
c
theoutputofcolumnc,withinthesamelevel;forthesametrain-
ingepoch,wecanwrite(8).Theoutputofaneuron,zl;e definedby
c;n
(9)istheresultofthenonlinearactivationfunctionfð:Þin(10)in
responsetotheweightedsumoftheinputconnectionswhilethe
output of the column is the sum of the outputs of the column’s
neurons.Tisaconstant(acrosslayers)toleranceparameterempir-
icallyselectedandthenonlinearactivationfunctionemulatesthe
brain’sobservednonlinearactivity.
h i
Nl;e¼ Nl;e ... Nl;e ... Nl;e
c c;1 c;n c;N
2 3
nl;e ... nl;e ... nl;e
6 c;1;1 c;n;1 c;N;1 7
6 . . . . . 7
6 . . . . . . . . . . 7
Fig.2. Illustrationofacorticalnetwork. 6 6 7 7 ð7Þ
¼6 nl;e ... nl;e ... nl;e 7
6 c;1;s c;n;s c;N;s 7
6 6 .. .. .. .. .. 7 7
functionfð:Þtocomputetheiroutputfromtheirinput.Thisactiva- 4 . . . . . 5
tionfunctioniscommonforallneuronsinthearchitecture. nl;e ... nl;e ... nl;e
Columns in a layer connect to those in the subsequent level:
c;1;Cl(cid:2)1 c;n;Cl(cid:2)1 c;N;Cl(cid:2)1
h i
theseconnectionsarereferredtoasverticalconnectionsandexist !l;e¼ tl;e ... tl;e ... tl;e ð8Þ
onlybetweenconsecutivelevels.Asynapticinputterminatingata 1 c Cl
column’sinputissharedwithinneuronsconstitutingsaidcolumn,
however,thelearnedweightsoftheconnectionsaredifferentlead-
tl;e¼XN
zl;e
ingtodistinctneuronaloutputs.Thelatteraresummedtoformthe c c;n
column’soutputbeingforwardedtothenextlayer.Suchconfigura- n¼ 1 ! ð9Þ
tionpermitsthecolumntoactasabasiccomputationalstructure zl;e ¼f
XCl(cid:2)1
nl;e tl(cid:2)1;e
in CA as opposed to neurons in DBN and other neural network c;n c;n;s c
c¼1
architectures.
Furthermore,lateralconnectionsorintra-levelconnections,are s 1
‘
t
u‘ hc mo atm nsmm dou udn
ri
ifi nc
y
ga at ti ro
c
aon
il
num
im
ne gna
.’
Cn
s
os w”
ne
te rigm
ah
rp
it
lsl yo
b
ty oae sd
te hd
eto
io rn
vd ee
t
rhl ti ev ice
a
ar
c lt
ci in
ov
uh
it
ni yb ti eot rfin pog
at rh
ts
e
,i trg hn
c
eoa sl
l
es
-
/fð ðsÞ Þ¼ ¼1 (cid:2)þ
s(cid:2)
;e 2s ;(cid:4)ð/ðs oifÞ t(cid:2) hsTÞ
e¼ rw1
ise
ð10Þ
connectionsdonottransmitdataandarehencearenotexplicitly
showninFig.2.Datacanonlyflowinabottom-updirection,from
leveltolevel,throughverticalconnections. 4.4.Trainingalgorithm
4.3.Mathematicalmodel 4.4.1.Randominitialization
Thenetworkisassumedtobeinitiallyfullyconnectedwithran-
Thecompletemathematicaldescriptionbasedonthemodelof dom weak weights (with absolute values less than 0.1). This is a
[6,63] can be found in [62,61], and is summarized below based common ‘‘blank slate” approach to ensure that the network is
ontheadoptednomenclatureinTable4. not initially biased to any specific pattern. As learning proceeds,
AcolumnofNneuronsreceivesconnectionsfromtheoutputof theseweightsareincrementallyupdatedsothattheconnectivity
columns in the previous layer and hence is represented by a 2D ofthenetworkismodified.Allweightsthatfalltozerorepresent
matrixconcatenatingvectorsofweightsofincomingconnections disabled connections. Therefore, an initially fully connected net-
workisnotnecessarilypreserved.
Table4 4.4.2.Unsupervisedfeed-forwardlearning
CAnomenclature.
The first stage in training a cortical network aims at creating
c Destinationcolumnindex input-specific representations via random firing and repeated
n Destinationneuronindex exposure.Aninputpropagatingthroughthelevelsofthenetwork
l Destinationlevelindex
causes certain columns to fire (i.e. generate a threshold crossing
s Origincolumnindex
response) based on initially random weights. This activation is
e Trainingepoch
N Numberofnodesinacolumn then reinforced by strengthening the active columns’ weights
Cl Numberofcolumnsinlevell and inhibiting neighboring ones. Repeated exposure or batch
T Tolerance learning trains columns to identify (via activation) particular
nl;e Weightofconnectionbetweenneuronn,columnc,levell,andcolumn
c;n;s aspectsorpatternsofthetrainingdata,extractingdiscriminatory
sinpreviouslevel,duringepoche
Nl;e Vectorofconnectionweightsenteringneuronn,ofcolumnc,levell features with increasing complexity through levels (lower levels
c;n
Nl;e Matrixofweightsenteringcolumnc,levell recognize basic elements, higher levels in the hierarchy learn
c higher order concepts and reasoning). The strengthening process
!l;e Outputvectoroflevell
tl;e Outputofcolumnc increases a column’s weights rendering it more receptive to
zl cc ; ;e
n
Outputofneuronn,columncoflevell s at ci tm ivu itl yati oo fn, aw ceh ril te aininh coib luit mio nn
.
w Coe na nke en cts iot nh se fw ore mig eh dts ordi wm ein ai ks eh nin edgY.Rizketal./AppliedComputingandInformatics15(2019)81–93 87
areplastic,i.e.canbealteredduringthefeedbacklearningphase pattern(strengtheningandinhibitionaretheonlyweightupdate
and vice versa. Strengthening and inhibition rules are shown in rules adopted in opposition to gradient descent employed in the
Eqs.(11)–(13). backpropagationlearningoftraditionalartificialneuralnetworks).
After multiple exposures, the top level attains a stable state also
nl;eþ1¼tl;e(cid:4)ðnl;e (cid:2)XðNl;eÞÞ ð11Þ
c;n;s c c;n;s c knownasa ‘‘stableactivation”inwhichcolumnsareabletocor-
0 1 rectlygeneratethedesiredfiringscheme.Theterm‘‘firingscheme”
referstothefiringpatternofneuronsforagiveninput.
B C
nl c; ;e nþ ;s1¼tl c;e(cid:4)B @nl c; ;e n;sþal c; ;e n;sþq (cid:4) 1
nl c; ;e
n;s(cid:2)TC A ð12Þ bacA kft te or s thta ebi pli rz ei vn ig out she let vo ep
l
l way he ir c, hth ine e turr ro nr es xig en ca ul tei ss p aro sp era ig ea ste od
f
1þeXðNl c;eÞ inhibition and strengthening to achieve the desired firing
scheme. The same process in repeated for each of the layers
XðNl;eÞ¼XN XCl(cid:2)1al;e
nl;e
until a convergence condition (expressed as a discrepancy
c c;n;s c;n;s between the desired and actual activation) is met. The training
(n¼1c¼1 ð13Þ exits when all layers are stable and the network can recognize
al;e
¼
1 if nl c; ;e n;s>(cid:2) all variations of the same pattern in the training data (within a
c;n;s
0 otherwise certain tolerance).
A pseudo-code showing the implementation of the training
algorithmisshowninTable5.Anillustrationofthefeedbacktrain-
4.4.3.Supervisedfeedbacklearning ingprocessisshowninFig.3.Thetoprowshowsthefiringscheme
The feed-forward learning phase relies only on aspects of the (bluesquaresrepresentactivecolumns)obtainedafterthefeedfor-
givendatatotraincolumnsthatidentifysignificantfeatureswith wardpropagationoftwovariationsofthesamepatternaswellas
noerrorpropagationorlabelinformation.Thesupervisedfeedfor- thedesiredfiringscheme(averageactivationobtainedbasedonall
ward stage aims at correcting misclassifications occurring when variationsofthispatterninthetrainingset).Themiddleandbot-
the network is exposed to variations of the same patterns (more tomrowsshowasuccessionoftrainingepochs(forpattern1and2
accuratelytrainingclass)whichmayresultduetotheabsenceof respectively)inwhichtheerrorsignalisgeneratedatthetoplevel
labelinformationinthepreviousphase. andstableactivationsareformedfromtoptobottom.Onecansee
Following the unsupervised phase, a ‘‘unique representation” thatatconvergencebothinstancesarerepresentedwiththesame
basedontheaveragefiringschemeobservedforaparticularpat- firingschemeinthenetwork.
tern is stored, the feedforward learning fine-tunes the network’s
weights to achieve this scheme (within certain bounds to avoid
overfitting) for all instances of a known class. A misclassification 5.Theoreticalcomputationalcomplexity
hence triggers an error signal at the top-most or output layer
(wherethefinaloutputisproduced)correctingmisfiringcolumns In this section, we derive the theoretical computational com-
(through inhibition) and strengthening weakened columns firing plexity of each algorithm to assess the required resources when
for the original pattern forcing the column firing for the original deployingsuchanetworkforrealworldproblems.Weinvestigate
Table5
CAgenerictrainingworkflow.
1. Randominitialization
forl¼1:6
forc¼1:Cl
Nl c¼randnðCl(cid:2)1;NÞ
2. Feedforwardphase
a.Processingoftrainingdata
foralltraininginstances
forl¼1:6
compute!l;e
forl¼1:6
forc¼1:Cl
iftl c;e>(cid:2)
forn¼1:Napply(12)
else
forn¼1:Napply(11)
b.Storeaveragerepresentationsofpatterns
forallclasses
forl¼1:6
forc¼1:Cl
tl c;avg¼tl
c
3. Feedbackphase
whileMSE>h
foralltraininginstances
forl¼1:6
(cid:3) (cid:3)
while(cid:3) (cid:3)!l;e(cid:2)!l;e(cid:3)
(cid:3)>h
forc¼1:Cl
(cid:3) (cid:3)
if(cid:3) (cid:3)tl c;avg(cid:2)tl c(cid:3) (cid:3)>h
forn¼1:Napply(11)
else
forn¼1:Napply(12)88 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
Fig.3. Anexampleofthefeedbackprocess.
twoaspectsofthecomputational complexity: memoryandcom- 5.2.Numberofoperationsperneuron
putations. While the required memory storage depends on the
numberofnon-zeroweightsinthenetwork,thenumberofcom- Sinceaneuroniscomposedofasummationandanactivation
putationsdependsonthenon-zeroweightsandtheadoptedacti- function,thenumberofoperationsperformedtoobtaintheoutput
vationfunction.ComparingCAtoDBN,themorecomputationally ofeachneuroncanbedividedintotheoperationstocomputethe
demanding network is data specific since each problem would sumandtheactivationfunctioncomputationalcomplexity.
resultin adifferentnumberofnon-zeroweights.Weempirically
comparethenetworksizesinthenextsection. 5.2.1.Summation
The number of floating point operations required to compute
thesummationdependsonthenumberofinputconnectionsofa
5.1.Numberofnon-zeroweights
neuron. Assuming there are m connections, m multiplications
and m additions (including the bias term) are required, which is
5.1.1.DBN
of the order of Oðm2Þ. For an input layer neuron, m is equal to
DBN is formed of R layers with M neurons in layer r. During
r thenumberoffeaturesintheinputvector.Forahiddenlayerneu-
training,thenetworkstartsoutfullyconnectedleadingtoatotal
P ron,misatmostequaltothenumberofneuronsintheprevious
n wu em igb he tsro arf ew ne oig nh -zts ere oqu wa hl et no tN rw ain¼ ingR r¼ is1M cor m(cid:4)M plr e(cid:2) t1 e. ,A wss itu hm cin 2g ½n 0;o 1t (cid:5)a il sl layer.
the fraction of non-zero weights or firing rate, the number of
P 5.2.2.Activationfunction
non-zero weights is equal to cN NZW ¼ R r¼1c (cid:4)M r(cid:4)M r(cid:2)1. Empirical Dependingontheactivationfunction,thecomputationofeach
resultsinSection6revealthat isusuallygreaterthan90%.Know- neuronoutputwillrequireacertainnumberoffloatingpointand
ing that the weights are double precision floating point numbers comparisonoperations.Someofpopularactivationfunctionsand
whichrequire8bytesofstorage,anupperboundonthememory theircomputationalcomplexityaresummarizedinTable6.
P
requirementsduringtrainingare R 8(cid:4)M (cid:4)M .Duringtesting, The hard limit activation function, described by (14), requires
P r¼1 r r(cid:2)1
thenumberofbytesisequalto R 8(cid:4)c (cid:4)M (cid:4)M . onecomparison.Thelinearfunctionin(15)requiresonemultipli-
r¼1 r r(cid:2)1
cationandoneadditionwhereasthepiece-wiselinearfunctionin
(16)requirestwocomparisonsandatmostonemultiplicationand
5.1.2.CA additionoperation.
CAisformedofR¼6levels;thefirstlevelcontainsL ¼Icol- (cid:2)
1 1 if x P0
umns with M neurons per column. We assume, without loss of /ðxÞ¼ i ð14Þ
generality,thenumberofcolumnsiscutinhalfineachsubsequent i 0 otherwise
level,asusedin[6,63,64]i.e.levelrcontainsL r¼ 2rI (cid:2)1.Eachlevelhas /ðxÞ¼a x þb ð15Þ
aweightmatrixwithdimensionsL (cid:6)M.Therefore,thetotalnum- i i
Pr
8
berofweightsisequaltoN w¼ R r¼1L r(cid:4)M(cid:4)L r(cid:2)1.However,someof ><b if x iPb
cth 2es ½e 0;w 1(cid:5)e ti hg eht fis ric no gu rld atebe ofe tq hu ea nl et to woz re kr ,o t. hT eh fe rare cf to ior ne, ow fne ed ue ron no ste thb ay
t
/ðx iÞ¼ >:a x iþb if (cid:2)b6x i<b ð16Þ
arenon-zero,whichleadstothenumberoffiringneuronsequalto
(cid:2)b if x i<(cid:2)b
P
N ¼c R L. To simplify the computations, a uniform distribu-
FC r¼1 r
tion of firing columns across levels will be assumed. Therefore, Table6
the number of firing columns per level is N FC=L¼N RFC. Finally, the Computationalcomplexityofsomeactivationfunctions
total number of non-zero weights can be approximated by Activationfunction Equation Computations(perneuron)
P
N
NZW
¼ R r¼1M(cid:4)L r(cid:4)N FC=L.EmpiricalresultsinSection6revealthat Hardlimit (14) Oð1Þ
c doesnotusuallyexceed50%.Knowingthattheweightsaredou- Linear (15) Oð2Þ
Piecewiselinear (16) Oð4Þ
bleprecisionfloatingpointnumberswhichrequire8bytesofstor-
Gaussian (17) Oðm3Þ
age, Panupperboundonthememoryrequirementsduringtraining Sigmoid,Tangent (18) Oð165Þ
are R 8(cid:4)L (cid:4)M(cid:4)L .Duringtesting,thenumberofbytesisequal Sigmoid,Logarithm (19) Oð83Þ
P r¼1 r r(cid:2)P1
to R r¼18(cid:4)c (cid:4)M(cid:4)L r(cid:4) R r¼1L Rr. Softmax (20) Oð83MRÞY.Rizketal./AppliedComputingandInformatics15(2019)81–93 89
Computing the Gaussian activation function, described by 5.4.Overallcomputationalcomplexity
(17), requires m multiplications and mðm(cid:2)1Þ additions to
compute the magnitude term, where m represents the dimen- In summary, the overall computational cost of DBN and CA
sionality of the vector x and is equivalent to the number of dependsonthearchitectureofthenetwork:thenumberoflayers
i
input connections of a neuron. Dividing by the standard devi- andthenumberofneuronsperlayerwhichaffectthenumberof
ation requires 3 additional multiplication operations. In addi- connections. The activation function can be fixed for both DBN
tion, the calculation of an exponential, estimated using the and CA. Sigmoid is a common activation function used in both
Taylor series expansion with approximately 10 terms, requires algorithms. After training, some of these connections will have a
approximately 81 operations. In total, m2ðm(cid:2)1Þþ81 opera- weightofzero.Basedontheprevioussections,wenoticethatfor
tions are required. a fixed network architecture, CA will have less non-zero weights
! duetothepruningalgorithm.However,thedepthofthebestnet-
/ðx iÞ¼exp (cid:2)kx 2i(cid:2) r 2l ik2 ð17Þ w teo ctr uk rs ef sor are eac ch omal mgo or nit lh ym usv ea dry inba ts he ed lo itn ert ah te urd eat foa. rs Cix A-l way he ir lear Dc Bh Ni-
depth’svariedfrom3to5hiddenlayers.
The tangent sigmoid function in (18) has two exponential
terms, each requiring approximately 81 operations, in addition
6.Empiricalcomparison
to two additions and one multiplication operations which
leads to a total of approximately 165 operations. On the other
In this section, we report on the experimental results of DBN
hand, the logarithmic sigmoid in (19) has one exponential
and CA on multiple databases from the UCI ML repository [72].
term and one addition and multiplication for a total of 83
We compare the classification accuracy, network complexity and
operations.
computationalcomplexityofbothalgorithms.
/ðxÞ¼tanhðxÞ¼1(cid:2)e(cid:2)2xi
ð18Þ
i i 1þe(cid:2)2xi 6.1.Experimentalsetup
/ðxÞ¼ 1 ð19Þ CAandDBNwereempiricallycomparedonclassificationprob-
i 1þe(cid:2)xi lems using datasets from the UCI machine learning repository,
describedinTable7.Thedatasetscontainedreal-valuedfeatures.
Each exponential term in the softmax activation function,
A 4-fold cross validation was adopted, i.e. each database is ran-
described by (20), requires approximately 81 operations. The
domlydividedinto4setswhere3sets(or75%ofthedatasamples)
numerator has one exponential term while the denominator has
are used in training and 1 set is used in testing. The results are
M terms.Intotal,81ðM þ1Þþ1operationsarerequired.
R R averaged over four runs where each set is used for testing once
/ðxÞ¼Pexi
ð20Þ
and the remaining for training. The CA library is a set of Matlab
i exj functions obtained from [73,74]; it was run on a Windows 7
j2MR
machinewithIntelCorei5.TheDBNlibraryisasetofMatlabfunc-
tionsmodifiedfrom[75];itwasrunonanIntelCorei7processor
5.3.Pruning machine.Multiplenetworkarchitectures,summarizedinTable8,
weretestedonthedatasets.Thenumberofneuronsforthehidden
The term synaptic pruning refers to the procedure by which layersaredisplayedonly.Theinputlayer’sneuronsareequaltothe
synaptic connections are terminated during the early ages of numberoffeaturesandtheoutputlayer’sneuronsareequaltothe
mammals’ lives [65]. Starting with approximately 86(cid:7)8 billion number of classes. The chosen architectures can be grouped into
neuronsatbirth,thehumanbrainquintuplesitssizeuntiladoles-
cence after which the volume of synaptic connection decreases
again [66]. This process of pruning is mainly thought of as the Table7
byproduct of learning. While the total number of neurons UCIdatasetcharacteristics.
remains roughly unaltered, the distribution and number of con- Dataset Numberof Numberof Numberof
nections are tailored by learning [67,68]: the structure of the instances features classes
brain moves from a stage where its primary function falls under Planningrelax 182 12 2
perception–action due to disconnected cortical hubs [69,70] to Breastcancerdiagnostic 569 32 2
distributed networks spanning the brain performing a variety of Wisconsin
TicTacToe 958 9 2
complex cognitive tasks [71].
Spambase 4601 57 2
Thetrainingofacorticalnetworkbearsseveralresemblancesto
Wilt 4889 5 2
the synaptic pruning procedure: starting with a fully connected Whitewine 4898 11 2
structure,thenetworkgoesthroughanunsupervisedphasewhere MNIST 70,000 784 10
connections are pruned through inhibition leaving only ‘‘signifi- Skinsegmentation 245,057 3 2
cant”ones.Inmoreaccurateterms,thestrengtheningoffiringcol-
umns ensures the establishment of selective connections while
inhibitionprunesirrelevantconnections.Thisprocessplaysacru- Table8
cial role not only in extracting meaningful characteristics of the Nomenclatureadoptedfornetworkarchitectures.
inputstimulibutalsoinavoidingoverfittingduetotheespecially
Networkname Networkarchitecture(hiddenlayers)
complex structure of a cortical network. This is further demon-
N1 [5,5]
strated in the number of non-zero weights in different network
N2 [50,50]
architectures as shown in our theoretical and empirical analysis: N3 [50,50,50]
thenumberof‘‘alive”connectionsdecreaseswiththeincreaseof N4 [500,500,500]
parameters, hence a minimal effect on performance as shown in N5 [500,500,2000]
our experiments. This property of CA’s structure and training is N6 [1000,1000,2000]
N7 [2000,1000,500,250,125,62]
notsharedwithDBN.90 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
oneofthreesetsbasedonthepatternofhiddenlayerneurons:net- numberofneuronspercolumnforDBN.Theconnectivityofanet-
workswithanequalnumberofneuronsinallhiddenlayers,net- workiscomputedbytakingtheratioofweightsgreaterthan5%of
works with an increasing (doubling) number of neurons as the theaveragevalueofweightstothetotalnumberofweightsinthe
layer depth increases and networks with a decreasing (halving) network.Thethresholdisnotsettoafixedvaluesincetheweights
numberofneuronsasthelayerdepthincreases.Furthermore,the rangeofweightsvariesbasedontheinputdata,i.e.forsomedata-
numberofhiddenlayersisincreasedfrom2to6hiddenlayers.For sets all the weights might be less than this set threshold even
the results reported in Table 9, DBN’s unsupervised training and though this threshold might be very small. Furthermore, the
fine-tuningalgorithmswereeachrunfor50epochsandthebatch threshold is not set to zero since some weights will not exactly
sizewassetto10.Thesecondcolumnindicatesthenumberofcol- zero but significantly smaller than the other weights in the net-
umnsperlayerforCA(therewere20neuronspercolumn)andthe work and their contribution is insignificant. The classification
Table9
Classificationresults.
DBN CA
Dataset Net.size Acc.(%) Connec.(%) Acc.(%) Connec.(%)
Planning N1 69.4 91.1 71.4 86.4
relax N2 68.8 49.6 65.2 72.5
N3 69.4 55.0 70.8 68.7
N4 70.6 9.1 75.6 55.1
N5 68.8 3.8 80.8 43.4
N6 68.1 3.2 82.5 30.9
N7 66.3 6.0 86.1 29.1
Breast N1 62.1 99.1 97.0 95.8
cancer N2 88.4 98.7 97.1 90.7
diagnostic N3 88.4 98.5 96.8 86.4
Wisconsin N4 87.5 97.3 97.5 77.1
N5 87.5 97.3 98.2 68.3
N6 87.0 97.2 98.2 56.5
N7 67.9 96.9 99.0 35.6
Tic N1 65.1 100.0 67.0 86.3
tac N2 65.1 94.9 79.3 83.5
toe N3 65.1 97.0 80.1 79.8
N4 65.1 99.4 89.4 68.1
N5 65.1 99.4 90.5 56.4
N6 65.1 98.9 92.7 45.9
N7 65.1 99.1 93.1 34.7
Spambase N1 89.9 98.1 90.5 89.7
N2 90.5 98.7 91.8 87.6
N3 91.4 98.9 92.8 79.9
N4 92.2 96.7 93.7 66.4
N5 91.3 96.1 94.3 58.1
N6 91.8 95.3 95.6 46.8
N7 88.5 93.5 97.5 36.3
Wilt N1 94.6 100.0 94.6 88.4
N2 94.6 99.9 95.5 80.7
N3 94.6 99.9 96.5 77.5
N4 94.6 99.9 97.8 66.9
N5 94.6 99.4 98.2 55.3
N6 94.6 99.2 98.0 45.5
N7 94.6 97.8 98.2 35.8
White N1 96.4 94.1 95.1 81.7
wine N2 96.4 99.7 96.0 80.3
N3 96.4 98.6 97.5 73.2
N4 96.4 70.1 98.1 67.9
N5 96.1 73.0 98.9 47.3
N6 96.3 60.0 99.5 36.8
N7 96.4 62.8 99.7 26.1
MNIST N1 38.8 88.1 96.2 89.8
N2 84.1 91.8 97.1 85.2
N3 84.2 98.8 98.5 79.1
N4 89.0 47.2 98.5 58.7
N5 88.3 54.9 99.2 43.5
N6 89.4 53.9 99.8 37.6
N7 86.5 40.8 99.8 28.5
Skin N1 79.6 80.6 94.5 83.2
segmentation N2 79.6 94.4 94.8 79.5
N3 79.6 95.8 94.8 75.3
N4 79.6 97.2 95.7 64.3
N5 79.6 97.1 97.8 53.8
N6 79.6 97.1 97.8 50.1
N7 79.6 97.3 99.9 33.5
Theboldvaluesrefertothebestaccuracyobtainedperdataset.Y.Rizketal./AppliedComputingandInformatics15(2019)81–93 91
accuracy simply reports the percentage of correctly classified zero weights in a DBN network to be around 90%. Unlike DBN,
instancesinthetestset.Runningtimeisnotcomparedsinceeach CA generally results in a sparsely connected network, evident by
algorithmiswrittenandruninadifferentenvironment. the low percentage of non-zero weights obtained after training;
CA networks had less than 50% of their connections in tact com-
pared to almost 90% for DBN. Furthermore, we notice that CA’s
6.2.Classificationresults
connectivitytendstodecreaseasthenetworksizeincreaseswhich
impliesthatifcertaindatadoesnotrequirealargenetwork,CA’s
First, we compare the performance of DBN when varying the
trainingalgorithmwillreducethenumberofconnectionstopro-
network architecture. For some databases, such as White wine,
duceasparselyconnectednetwork.Forexample,connectivityper-
theperformancedoesnotsignificantlyvaryasthenetworkarchi-
centagedecreasesfrom98.7%to40.8%forMNISTwhenincreasing
tecture varies, achieving approximately 96.4% accuracy. On the
the network architecture from a two-hidden layer network to a
other hand, performance varied significantly across architectures
six-layerarchitecture.
forMNIST,rangingfrom38.8%to89.47%.Hence,DBNiseitherable
to achieve good accuracy or bad accuracy on a specific database.
This is mainly due to the lack of enough training points to allow 6.4.Effectofbatchsize
DBNtolearnagoodmodelofthedata.However,forotherdatasets
suchasBreastcancerdiagnosticWisconsin,theclassificationaccu- TheDBNcoderandomlyreordersanddividesthedataintomini
racyvariedsignificantlyasthenetworkarchitecturevaried.Next, batches. While training, each mini batch is loaded into memory
consideringCA’sperformanceonthevariousdatasetswhenvary- and used to update the weights of the network connections.
ing the network size, we notice that the six-layered network Increasingthesizeofabatchmeantloadingalargerchunkofdata
alwaysoutperformsothernetworksizesonalldatasets,although into memory. This renders training slower if the chunk was too
itoccasionallymarginallyoutperformssomenetworks.Forexam- largetofitintomemory.However,decreasingthebatchsizemeant
ple,CAachieved99.9%accuracyforN7onskinsegmentationcom- morememorytransfersperepochandtherefore,increasingtrain-
paredto94.5%forN1(2layers).ComparingCAtoDBN,wenotice ing time. On the other hand, CA does not randomly reorder and
thatCAoutperformsDBNonalmostalldatasets.Inaddition,asix- divide the data into mini batches. Therefore, the network is
layeredarchitectureseemedto alwaysperformbetterthanother exposed to the data in the order it was presented. Theoretically,
architectures, reducing the burden of searching for the optimal reordering or dividing the data has no effect on the performance
networkarchitecturebytrainingmultiplenetworks. ofthealgorithmasallpatternsareemployedandmustachievea
stableactivation.
6.3.Networkconnectivity
6.5.Statisticalanalysis
DBNexhibithighconnectivityformostdatasetsonvariousnet-
work sizes. Therefore, Hinton et al.’s greedy training algorithm Next, we perform the pairwise t-test [76] and Friedman test
doesnoteliminatealargenumberofconnections resultinginan [77] to gain further insight into the differences between CA and
almostfullyconnectedgraph.Thisisevidentintheexperimental DBN. The N7 architecture is trained on the various databases.
results reported in Table 9 which reports the percentage of non- Table 10 summarizes the p-values of both tests and show that
Table10
Statisticalsignificancetests.
Pairwiset-test Friedmantest
Database p-value Statisticallysignificant p-value Statisticallysignificant Nemenyicriticaldistance
Planningrelax 2.88E(cid:2)4 Y 3.64E(cid:2)02 Y 0.1
BreastcancerdiagnosticWisconsin 0 Y 9.26E(cid:2)126 Y 0.1
Tictactoe 5.28E(cid:2)10 Y 1.92E(cid:2)09 Y 0.1
Spambase 0 Y 0 Y 0.0
Wilt 4.65E(cid:2)79 Y 4.74E(cid:2)75 Y 0.0
Whitewine 0 Y 0 Y 0.0
MNIST 9.11E(cid:2)84 Y 6.09E(cid:2)61 Y 0.0
Skinsegmentation 0 Y 0 Y 0.0
Fig.4. CAvs.DBNrankingbasedonNemenyiposthoctest.92 Y.Rizketal./AppliedComputingandInformatics15(2019)81–93
thereisastatisticalsignificancebetweenCAandDBNonalldata- [14] J.Szentagothai,Theferrierlecture,1977:theneuronnetworkofthecerebral
bases; the p-value was less than 5%. Furthermore, the Nemenyi cortex: a functionalinterpretation, Proc. R. Soc. Lond. Ser. B. Biol. Sci. 201
(1144)(1978)219–248.
post hoc [78] was performed once a significant difference was
[15] V.B.Mountcastle,Thecolumnarorganizationoftheneocortex,Brain120(4)
observed, to rank the algorithms. The rankings revealed that CA (1997)701–722.
outperformed DBN on most databases except for Skin Segmenta- [16] A.S. Benjamin, J.S. de Belle, B. Etnyre, T.A. Polk, The role of inhibition in
learning, Human Learn.: Biol., Brain, Neurosci.: Biol., Brain, Neurosci. 139
tion, as shown in Fig. 4. The critical distance is summarized in
(2008)7.
Table10. [17] W.S.McCulloch,W.Pitts,Alogicalcalculusoftheideasimmanentinnervous
activity,Bull.Math.Biophys.5(4)(1943)115–133.
[18] F.Rosenblatt,Theperceptron:aprobabilisticmodelforinformationstorage
7.Conclusion andorganizationinthebrain,Psychol.Rev.65(6)(1958)386.
[19] B.Widrow,etal.,Adaptiveadalineneuronusingchemicalmemistors,1960.
[20] A. Aizerman, E.M. Braverman, L. Rozoner, Theoretical foundations of the
In this work, we compared two DNN architectures on super- potential function method in pattern recognition learning, Autom. Rem.
visedclassificationproblems.WhileDBNcanbeeasilyseenasan Control25(1964)821–837.
[21] J.L. McClelland, D.E. Rumelhart, P.R. Group, et al., Parallel distributed
establishedtechniquedevelopedfromwithinatraditionalAIper-
processing,Explorationsinthemicrostructureofcognition2(1986)184.
spective, CA are more biologically inspired and can be classified [22] J.J.Hopfield,Neuralnetworksandphysicalsystemswithemergentcollective
as theories in the making, solidly rooted in principles inherited computationalabilities,Proc.Nat.Acad.Sci.79(8)(1982)2554–2558.
fromneuroscienceresearch.Atheoreticalcomputationalcomplex- [23] T.Kohonen,Self-organizedformationoftopologicallycorrectfeaturemaps,
Biol.Cybernet.43(1)(1982)59–69.
ityanalysisforbothalgorithmswaspresentedbeforeempirically [24] S. Grossberg, Competitive learning: from interactive activation to adaptive
comparing CA and DBN. Experiments were run on eight publicly resonance,Cognit.Sci.11(1)(1987)23–63.
available classification databases. Multiple CA and DBN network [25] J. Misra, I. Saha, Artificial neural networks in hardware: a survey of two
decadesofprogress,Neurocomputing74(1)(2010)239–255.
architectureswerecomparedbasedontheirclassificationaccuracy
[26] L.Arnold,S.Rebecchi,S.Chevallier,H.Paugam-Moisy,Anintroductiontodeep
and resulting network connectivity. CA achieved the best perfor- learning,in:ESANN,2011.
mance on most databases using a six-layer architecture with [27] V. Vapnik, The Nature of Statistical Learning Theory, Springer Science &
BusinessMedia,2000.
decreasing number of hidden neurons for deeper layers. Results
[28] J.Schmidhuber,Learningcomplex,extendedsequencesusingtheprincipleof
showedthatdeeperCAnetworkshadlowerconnectivitythanshal- historycompression,NeuralComput.4(2)(1992)234–242.
lowerCAnetworks.Furthermore,DBNdidnotpruneasmanycon- [29] J.R.Anderson,Act:asimpletheoryofcomplexcognition,Am.Psychol.51(4)
(1996)355.
nections as CA’s training algorithm. On the tested databases, CA
[30] J.Hawkins,S.Blakeslee,OnIntelligence,MacMillan,2007.
generally had a higher classification accuracy than DBN. In the [31] S. Franklin, F. Patterson Jr., The lida architecture: adding new modes of
spanofthiswork,weattemptedtoprovidethereaderwithenough learningtoanintelligent,autonomous,softwareagent,pat703(2006)764–
1004.
backgroundandtechnicaldetailsforeachofthealgorithmswhile
[32] K.Fukushima,Neocognitron:ahierarchicalneuralnetworkcapableofvisual
understandingthatthebreadthofthetopicnecessitatestheinclu- patternrecognition,NeuralNetw.1(2)(1988)119–130.
sion of more involved insights. We therefore urge the interested [33] P.J.Werbos,Backpropagationthroughtime:whatitdoesandhowtodoit,
readertousethispaperasafoundationforfurtherexplorationof Proc.IEEE78(10)(1990)1550–1560.
[34] S.Hochreiter,Untersuchungenzudynamischenneuronalennetzen,Master’s
therapidlyexpandingsub-fieldofdeeplearning. thesis,InstitutfurInformatik,TechnischeUniversitat,Munchen.
[35] S.Hochreiter,Y.Bengio,P.Frasconi,J.Schmidhuber,Gradientflowinrecurrent
nets:thedifficultyoflearninglong-termdependencies,2001.
Acknowledgment [36] G.E.Hinton,Torecognizeshapes,firstlearntogenerateimages,Prog.Brain
Res.165(2007)535–547.
ThisworkhasbeenpartlysupportedbytheNationalCenterfor [37] Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning, in:
Proceedingsofthe26thAnnualInternationalConferenceonMachineLearning,
ScientificResearchinLebanonandtheUniversityResearchBoard ACM,2009,pp.41–48.
attheAmericanUniversityofBeirut. [38] V.Nair,G.E.Hinton,3dobjectrecognitionwithdeepbeliefnets,in:Advances
inNeuralInformationProcessingSystems,2009,pp.1339–1347.
[39] Y.LeCun,K.Kavukcuoglu,C.Farabet,Convolutionalnetworksandapplications
References invision,in:ProceedingsoftheIEEEInternationalSymposiumonCircuitsand
Systems,IEEE,2010,pp.253–256.
[40] R.Collobert,J.Weston,Aunifiedarchitecturefornaturallanguageprocessing:
[1] S.Samarasinghe,NeuralNetworksforAppliedSciencesandEngineering:From
Deepneuralnetworkswithmultitasklearning,in:Proceedingsofthe25th
FundamentalstoComplexPatternRecognition,CRCPress,2006.
InternatuionalConferenceonMachineLearning,ACM,2008,pp.160–167.
[2] G.Hinton,S.Osindero,Y.-W.Teh,Afastlearningalgorithmfordeepbeliefnets,
[41] S. Zhou, Q. Chen, X. Wang, Active deep networks for semi-supervised
NeuralComput.18(7)(2006)1527–1554.
sentiment classification, in: Proceedings of the 23rd International
[3] E.M.Izhikevich,Whichmodeltouseforcorticalspikingneurons?,IEEETrans
Conference on Computational Linguistics: Posters, Association for
NeuralNetw.15(5)(2004)1063–1070.
ComputationalLinguistics,2010,pp.1515–1523.
[4] H.DeGaris,C.Shuo,B.Goertzel,L.Ruiting,Aworldsurveyofartificialbrain
[42] X.Glorot,A.Bordes,Y.Bengio,Domainadaptationforlarge-scalesentiment
projects,parti:large-scalebrainsimulations,Neurocomputing74(1)(2010)
classification: a deep learning approach, in: Proceedings of the 28th
3–29.
InternationalConferenceonMachineLearning,2011,pp.513–520.
[5] B.Goertzel,R.Lian,I.Arel,H.DeGaris,S.Chen,Aworldsurveyofartificialbrain
[43] G.E.Dahl,D.Yu,L.Deng,A.Acero,Context-dependentpre-traineddeepneural
projects,partii:biologicallyinspiredcognitivearchitectures,Neurocomputing
networksforlarge-vocabularyspeechrecognition,IEEETrans.Audio,Speech,
74(1)(2010)30–49.
Lang.Process.20(1)(2012)30–42.
[6] A.G. Hashmi, M.H. Lipasti, Cortical columns: building blocks for intelligent
[44] T.N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. Novak, A.-R.
systems,in:IEEESymposiumonComputationalIntelligenceforMultimedia
Mohamed, Making deep belief networks effective for large vocabulary
SignalandVisionProcessing,IEEE,2009,pp.21–28.
continuous speech recognition, in: IEEE Workshop on Automatic Speech
[7] G.M.Edelman,V.B.Mountcastle,in:TheMindfulBrain:CorticalOrganization
RecognitionandUnderstanding,IEEE,2011,pp.30–35.
andtheGroup-SelectiveTheoryofHigherBrainFunction,MasachusettsInstof
[45] A.-R.Mohamed,D.Yu,L.Deng,Investigationoffull-sequencetrainingofdeep
TechnologyPr,1978.
belief networks for speech recognition, in: INTERSPEECH, 2010, pp. 2846–
[8] R.J. Baron, The Cerebral Computer: An Introduction to the Computational
2849.
StructureoftheHumanBrain,PsychologyPress,2013.
[46] A.-r.Mohamed,T.N.Sainath,G.Dahl,B.Ramabhadran,G.E.Hinton,M.Picheny,
[9] J.Nolte,TheHumanBrain:AnIntroductiontoitsFunctionalAnatomy.
et al., Deep belief networks using discriminative features for phone
[10] J.M.DeSesso,Functionalanatomyofthebrain,in:MetabolicEncephalopathy,
recognition, in: International Conference on Acoustics, Speech and Signal
Springer,2009,pp.1–14.
Processing,IEEE,2011,pp.5060–5063.
[11] N.Geschwind,Specializationsofthehumanbrain,ScientificAmerican241(3)
[47] A.-R. Mohamed, G.E. Dahl, G. Hinton, Acoustic modeling using deep belief
(1979)180–201.
networks,IEEETrans.Audio,Speech,Lang.Process.20(1)(2012)14–22.
[12] R.C. O’Reilly, Y. Munakata, Computational Explorations in Cognitive
[48] P. Hamel, D. Eck, Learning features from music audio with deep belief
Neuroscience:UnderstandingtheMindbySimulatingtheBrain,MITPress,
networks,in:ISMIR,Utrecht,TheNetherlands,2010,pp.339–344.
2000.
[49] G.E. Hinton, R.R. Salakhutdinov, Reducing the dimensionality of data with
[13] M.Catani,D.K.Jones,R.Donato,etal.,Occipito-temporalconnectionsinthe
neuralnetworks,Science313(5786)(2006)504–507.
humanbrain,Brain126(9)(2003)2093–2107.Y.Rizketal./AppliedComputingandInformatics15(2019)81–93 93
[50] P.Smolensky,Informationprocessingindynamicalsystems:Foundationsof [64] A.Hashmi,A.Nere,J.J.Thomas,M.Lipasti,Acaseforneuromorphicisas,ACM
harmony theory, Department of Computer Science, University of Colorado, SIGARCHComputerArchitectureNews,vol.39,ACM,2011,pp.145–158.
Boulder,1986. [65] G. Chechik, I. Meilijson, E. Ruppin, Synaptic pruning in development: a
[51] R. Khanna, M. Awad, Efficient Learning Machines: Theories, Concepts, and computationalaccount,NeuralComput.10(7)(1998)1759–1777.
ApplicationsforEngineersandSystemDesigners,Apress,2015. [66] F.I.Craik,E.Bialystok,Cognitionthroughthelifespan:mechanismsofchange,
[52] G. Hinton, A practical guide to training restricted boltzmann machines, TrendsCog.Sci.10(3)(2006)131–138.
Momentum9(1)(2010)926. [67] L.Steinberg,Cognitiveandaffectivedevelopmentinadolescence,TrendsCog.
[53] S. Geman, D. Geman, Stochastic relaxation, gibbs distributions, and the Sci.9(2)(2005)69–74.
bayesian restoration of images, IEEE Trans. Pattern Anal. Mach. Intell. (6) [68] E.D’Angelo,A.Antonietti,S.Casali,C.Casellato,J.A.Garrido,N.R.Luque,L.
(1984)721–741. Mapelli, S. Masoli, A. Pedrocchi, F. Prestori, et al., Modeling the cerebellar
[54] B.Aleksandrovsky,J.Whitson,G.Andes,G.Lynch,R.Granger,Novelspeech microcircuit:newstrategiesforalong-standingissue,Front.Cell.Neurosci.10
processing mechanism derived from auditory neocortical circuit analysis, (2016)176.
Proceedingsofthe4thInternationalConferenceonSpokenLanguage,vol.1, [69] G.M. Shepherd, The Synaptic Organization of the Brain, Oxford University
IEEE,1996,pp.558–561. Press,2003.
[55] A. Fischer, C. Igel, An introduction to restricted boltzmann machines, in: [70] P.Fransson,U.Åden,M.Blennow,H.Lagercrantz,Thefunctionalarchitecture
Progress in Pattern Recognition, Image Analysis, Computer Vision, and oftheinfantbrainasrevealedbyresting-stateFMRI,Cereb.Cort.21(1)(2011)
Applications,Springer,2012,pp.14–36. 145–154.
[56] D.Erhan,Y.Bengio,A.Courville,P.-A.Manzagol,P.Vincent,S.Bengio,Why [71] N.Gogtay,J.N.Giedd,L.Lusk,K.M.Hayashi,D.Greenstein,A.C.Vaituzis,T.F.
doesunsupervisedpre-traininghelpdeeplearning?,JMach.Learn.Res.11 Nugent, D.H. Herman, L.S. Clasen, A.W. Toga, et al., Dynamic mapping of
(2010)625–660. humancorticaldevelopmentduringchildhoodthroughearlyadulthood,Proc.
[57] Y.Bengio,LearningdeeparchitecturesforAI,Found.Trends(cid:3)Mach.Learn.2 Nat.Acad.Sci.USA101(21)(2004)8174–8179.
(1)(2009)1–127. [72] M. Lichman, UCI Machine Learning Repository, 2013 <http://archive.ics.uci.
[58] D. Erhan, P.-A. Manzagol, Y. Bengio, S. Bengio, P. Vincent, The difficulty of edu/ml>.
training deeparchitecturesandtheeffect ofunsupervised pre-training, in: [73] J.Mutch,U.Knoblich,T.Poggio,CNS:AGPU-BasedFrameworkforSimulating
Internartional Conference on Artificial Intelligence and Statistics, 2009, pp. Cortically-Organized Networks, Massachusetts Institute of Technology,
153–160. Cambridge,MA,Tech.Rep.MIT-CSAIL-TR-2010-013/CBCL-286.
[59] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, et al., Greedy layer-wise [74] J.Mutch,Cns:CorticalNetworkSimulator,2017<http://cbcl.mit.edu/jmutch/
trainingofdeepnetworks,Adv.NeuralInf.Process.Syst.19(2007)153. cns/>.
[60] G.E.Hinton,Howneuralnetworkslearnfromexperience,Scient.Am.267(3) [75] R. Salakhutdinov, G. Hinton, Deep Belief Networks, 2015 <http://www.
(1992)145–151. cs.toronto.edu/hinton/MatlabForSciencePaper.html>.
[61] N.Hajj,Y.Rizk,M.Awad,Amapreducecorticalalgorithmsimplementationfor [76] C.Nadeau,Y.Bengio,Inferenceforthegeneralizationerror,in:Advancesin
unsupervisedlearningofbigdata,Proc.Comp.Sci.53(2015)327–334. NeuralInformationProcessingSystems,2000,pp.307–313.
[62] N. Hajj, M. Awad, Weightedentropycortical algorithms forisolated arabic [77] M.Friedman,Theuseofrankstoavoidtheassumptionofnormalityimplicitin
speech recognition, in: International Joint Conference on Neural Networks, theanalysisofvariance,J.Am.Statist.Assoc.32(200)(1937)675–701.
IEEE,2013,pp.1–7. [78] P.Nemenyi,Distribution-FreeMultipleComparisons,Ph.D.thesis,Princeton
[63] A.Hashmi,M.H.Lipasti,Discoveringcorticalalgorithms,in:IJCCI(ICFC-ICNC), University,NJ,1963.
2010,pp.196–204.