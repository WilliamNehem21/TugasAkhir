Artificial Intelligence in the Life Sciences 1 (2021) 100009
Contents lists available at ScienceDirect
Artificial Intelligence in the Life Sciences
journal homepage: www.elsevier.com/locate/ailsci
Conceptual Analysis
Chemistry-centric explanation of machine learning models
Raquel Rodríguez-Pérez 1, Jürgen Bajorath 2, ∗
Department of Life Science Informatics, B-IT, LIMES Program Unit Chemical Biology and Medicinal Chemistry, Rheinische Friedrich-Wilhelms-Universität,
Friedrich-Hirzebruch-Allee 6, Bonn D-53115, Germany
Artificial intelligence (AI) is increasingly being considered across pear [16] but other claimed advances remain often questionable [17] ,
chemical disciplines, just as in many other areas of science, often with partly due to the lack of generally accepted evaluation criteria and stan-
high expectations for “revolutionary ” advances. Medicinal chemistry dards for assessing chemical novelty [17] . Even in successful design ap-
and drug design are among the focal points of these developments [1– plications, newly generated or repurposed chemical entities are often
4] and there is rising interest in deep neural network (DNN) architec- viewed controversially from a medicinal chemistry perspective. Clearly,
tures and deep learning (DL) for the generation of novel compounds and the field is in flux but ML/DL in medicinal chemistry and drug design
prediction of various molecular properties [ 3 , 4 ]. is still largely dominated by methodological considerations (what could
In chemoinformatics, medicinal chemistry, and drug design, the term be done?) rather than successful practical applications (what has been
AI is typically used synonymously with machine learning (ML), which done?), which are still far from being routine [18] .
represents only a part of the methodological AI spectrum [5] . However, One of two particularly relevant aspects concerning ML in medicinal
ML already has a long history in chemoinformatics and medicinal chem- chemistry emerging from the discussion above is that a variety of meth-
istry, spanning more than two decades, and is widely applied for molecu- ods are being employed for property prediction and compound design,
lar property predictions as well as the search for novel active compounds ranging from simple decision tree-based algorithms and probabilistic
[6] . Neural networks (NNs) were popular early on in chemoinformatics, modeling to complex DNNs, often with comparable success. The other
but have been increasingly replaced over time with other approaches important aspect relates to the rationalization of predictions, as further
[7] such as the support vector machine (SVM) [8] and random forest discussed in the following.
(RF) [9] algorithms or Bayesian modeling [10] , mostly due to the in- Given the often cited “black box ” character of most ML models
trinsic tendency of NNs to overfit property prediction models trained on [ 12 , 19 ], their predictions are difficult to rationalize. In the practice
moderately sized data sets. With DL, NNs have re-emerged in this field, of medicinal chemistry, lack of transparency of predictions and model
for the most part as DNN architectures [11] . rationalization continue to hinder the acceptance of ML and limit the
In computer science, it is often observed that methodological com- impact of predictive modeling on experimental programs [ 6 , 18 ], de-
plexity does not scale with predictive performance [12] and the same spite the long tradition of ML in this field. Similar considerations ap-
applies to medicinal chemistry and drug design [6] . Although “big data ” ply to other chemical disciplines that are primarily experimentally
trends are also beginning to emerge in medicinal chemistry [13] , ML driven. Ideally, one would like to know how certain and reliable a
predictions are typically based upon relatively small data regimes and given prediction is and also understand it in intuitive chemical terms.
well-defined molecular representations. These conditions do not play While a few studies have presented approaches for uncertainty es-
into the strengths of DNNs compared to other areas such as image anal- timation of ML predictions [ 20 , 21 ], robust and generally applicable
ysis or natural language processing where DL has made a large impact in methods are currently not yet available. However, in computer sci-
recent years [ 14 , 15 ]. Accordingly, standard ML methods often perform ence and other fields, the potential of explainable or interpretable
comparably well or better than DNNs in predicting biological activity ML is a much discussed topic [ 12 , 22 , 23 ], which also is of high rel-
and other molecular properties [6] . evance for ML applications in chemistry. In computer science, inter-
On the other hand, DL offers new opportunities in chemoinformat- pretable ML refers to algorithms whose predictions can be directly
ics and medicinal chemistry that were hardly possible to address in a reconciled (such as decision trees) and explainable ML to methods
comparable way until recently such as, for example, in chemical re- enabling the rationalization of black box models [12] . However, in
action modeling [1] or large-scale generative de novo compound de- chemistry-related publications, these terms have been used more or
sign [3] . In generative design and DNN-based compound reposition- less interchangeably concerning the rationalization of models and their
ing, successful applications charting new territory are beginning to ap- decisions.
∗ Corresponding author.
E-mail address: bajorath@bit.uni-bonn.de (J. Bajorath).
1 Present address: Novartis Institutes for Biomedical Research, Novartis Campus, CH-4002 Basel.
2 Given his role as Editor in Chief, Jürgen Bajorath had no involvement in the peer-review of this article and has no access to information regarding its peer-review.
Full responsibility for the editorial process for this article was delegated to Mingyue Zheng.
https://doi.org/10.1016/j.ailsci.2021.100009
Received 20 September 2021; Received in revised form 28 September 2021; Accepted 28 September 2021
Available online 30 September 2021
2667-3185/Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )R. Rodríguez-Pérez and J. Bajorath Artificial Intelligence in the Life Sciences 1 (2021) 100009
In medicinal chemistry, there is an urgent need for ML model ra-
tionalization. Although the reluctance of medicinal chemists to rely on
black box predictions in their compound design and optimization efforts
is a widespread phenomenon, only a limited number of investigations
have addressed the explanation or interpretation of ML predictions in
the context of structure-activity relationship (SAR) analysis [24] . With
increasing use of complex DNN architectures in the field [ 3 , 4 ], this
widens the gap between model availability and acceptance and further
limits the impact of ML.
For activity prediction models, available rationalization strategies
typically aim to identify molecular representation features that de-
termine individual predictions, mostly in a model-dependent manner.
These approaches predominantly employ feature weighting techniques
to rationalize predictions of kernel-based [ 25 , 26 ] or Bayesian [27] clas-
sification models. In addition, weight gradients across different NN lay-
ers can also be determined to trace determinants of predictions and ex-
plain (D)NN models. However, these gradients tend to be unstable, fre-
quently resulting in different interpretations of very similar predictions
[28] .
Fig. 1. Interpretation of predictions. SHAP feature importance values are rep-
In addition to model-dependent techniques, model-independent
resented as sequential arrows recording positive (red) and negative (blue) con-
methods can also be considered, which are principally preferred be-
tributions to the prediction of activity, resulting in a cumulative output proba-
cause they are applicable to any ML model, regardless of the com- bility. For ML, the given compound was represented using a topological finger-
plexity of the underlying algorithm. Moreover, model-independent ap- print comprising overlapping atom environment features. Top-ranked features
proaches typically do not require balancing performance and inter- making positive or negative contributions are mapped onto the structure. The
pretability across different models [29] . However, with sensitivity anal- activity of the compound was only correctly predicted using a DNN but not an
ysis [30] , only one model-independent approach has until recently been SVM model. The figure was adapted from Rodríguez-Pérez and Bajorath [39] .
applied to property predictions in chemoinformatics. This methodol- (For interpretation of the references to color in this figure legend, the reader is
ogy was adapted about a decade ago to study the influence of sys- referred to the web version of this article).
tematic feature value changes on activity predictions using ML mod-
els [ 31 , 32 ]. Sensitivity analysis generally relies on feature perturbation
to study ensuing effects. In particular, partial derivatives were used putationally increasingly expensive. Therefore, to render the approach
to assess the impact of local perturbations of fragment descriptors on practical for ML, a locally interpretable explanatory model is derived that
model predictions [32] . Sensitivity analysis becomes rapidly infeasi- limits required Shapely value calculations. This methodology is termed
ble with increasing model dimensionality, which limits its applicability. Shapley Additive exPlanations ( SHAP ) [36] and can be perceived as an
Hence, this methodology has been more or less abandoned in the field extension of the Local Interpretable Model-agnostic Explanations (LIME)
or substituted by approximations that better summarize perturbations approach [37] . A brief account of the underlying theory is presented in
effects. the Appendix below.
Our group has considered alternative approaches with potential for We have adapted the SHAP methodology for explaining compound
ML model-independent rationalization from different viewpoints includ- activity predictions and SARs [38] . To these ends, SHAP was combined
ing broad applicability, quantitative assessment of predictions, and ease with molecular feature mapping and feature classification according to
of visual interpretation. In light of these criteria, the concept of Shap- quantitative contributions to correct or incorrect ML model predictions
ley values [ 33 , 34 ] from cooperative game theory [ 34 , 35 ] has become [38] and successfully applied to different compound activity prediction
our methodology of choice. Shapley values were originally introduced tasks including classification, regression, or multi-task learning [ 38 , 39 ].
in 1953 [33] to quantitatively account for contributions of individual Fig. 1 shows exemplary results of molecular SHAP analysis.
players forming a team. These values provide a quantitative assessment Shown is an active compound whose biological activity that was
of cooperative contributions to a team’s ultimate success (total gain). correctly predicted by a DNN model whereas an SVM classifier pre-
Accordingly, they specify a partition of merit among individual players dicted this compound to be inactive. The SHAP contribution plots of
by calculating the average of all contributions made by a given player these predictions reveal that multiple features were assigned compa-
in different team constellations. rable importance for predictions with both models. However, the SVM
The Shapley value concept is readily transferable to ML by applying model yielded negative contributions for a number of features that were
the following analogies: The game a team engages in can perceived as not prioritized by the DNN. Mapping of highly weighted positive (red)
a prediction task for a single instance (e.g., a compound). The merit for and negative (blue) features from the SVM prediction on the compound
this task is given by the difference between its prediction and the aver- structure revealed that these features formed overlapping substructures,
age prediction of all instances. The players participating in the game are thus detecting and explaining a model error.
features values of the instance that cooperate (act in concert) to obtain Recently, an algorithm was introduced for the exact calculation of
the merit for a given prediction. The resulting Shapley value of a given local SHAP values specifically with decision tree-based methods, taking
feature is then obtained as the average contribution of a feature over advantage of the tree distribution [40] . By comparing predictions using
all possible feature combinations. Accordingly, Shapley values account SHAP and this algorithm directly, high correlation of greater than 80%
for the partition of contributions over individual features comprising a was observed between approximated and exactly determined local fea-
feature vector or set (such as a molecular representation). A key aspect ture importance values, both for decision tree-based classification and
of the Shapley value concept that sets it apart from model-dependent regression models [39] . These findings further supported the reliability
feature weighting methods is that not only the contribution of feature of SHAP’s local kernel approximation.
presence to a given prediction can be quantified, but also the contribu- We also note that there are conceptually distinct yet complementary
tion of feature absence. approaches to SHAP for model interpretation. For example, the Simi-
For feature sets of increasing size, systematic calculations of Shapley larity Maps approach depends on the removal of individual atoms from
values on the basis of all possible feature combinations become com- test compounds and estimation of their importance by quantifying the
2R. Rodríguez-Pérez and J. Bajorath Artificial Intelligence in the Life Sciences 1 (2021) 100009
resulting change of a prediction [41] . This is in contrast to SHAP that LIME generates the explanation 𝜉 of an instance x according to
does not rely on atom removal (i.e., introduction of a perturbation at Eq. (3) :
the structural level), but assesses collaborative (i.e., combination-based)
( )
𝜉( 𝑥 ) = argmin  𝑓, 𝑔, 𝜋 𝑥 + Ω( 𝑔 ) (3)
contributions of representation features. The Similarity Maps approach 𝑔∈𝐺
does not require feature mapping but cannot quantify contributions of where 𝐺is a class of interpretable (linear) models,  is the loss function
feature absence. In addition, single-atom perturbations might not signif- to minimize, 𝜋 𝑥 the proximity measure between an instance z and x
icantly modify a model’s prediction but affect structural integrity. (kernel defining locality), and Ω( 𝑔) an optional regularization term to
Another conceptually distinct approach involves feature weighting
limit model complexity.
of multiple models generated with the same ML algorithm, providing
For the explanation of a given test instance x , the following proce-
the basis for the determination of feature importance correlation [42] .
dure is applied:
For target-based compound activity classes, this measure yielded model-
internal data set signatures and revealed similar compound binding (i) Artificial samples are obtained by permuting features of the test
characteristics of proteins as well as functional relationships. Strong instance x .
feature importance correlation between compound activity prediction (ii) These samples are weighted by the value of a kernel calculated
models indicated functional similarities between different targets that for them and x .
were not related to compound binding [42] . For RF models derived for (iii) A model g is trained to predict f(x) with coefficients estimating
more than 200 targets, the Gini impurity (GI) criterion [43] served as feature importance.
a measure of node-based recursive partitioning quality. GI is a metric
Hence, LIME builds a linear model g in a feature region proximal to
from information theory defined in Eq. (1) :
the test instance, with ML model f typically being non-linear. The LIME
∑𝑛 ( ) approach provides the basis for the development of the kernel SHAP
GI = 𝑝 𝑖 1 − 𝑝 𝑖 (1) methodology, as explained in the following.
𝑖 =1
Shapley values account for the distribution of feature contributions
Here, 𝑝 𝑖 is the frequency for class i at a given node, and n is 2 for to a model’s prediction for a given test instance. To determine the contri-
binary classification. Accordingly, GI for a given feature is equivalent bution of a feature i , all operations by which a feature might be added to
to the mean decrease in GI, i.e., the normalized sum of all impurity the set ( 𝑁! ) and a summation over all possible sets ( 𝑆) must be carried
decrease values for nodes in the RF where splitting was based on that out. For any feature sequence, the marginal contribution by adding fea-
feature. Thus, increasing values indicate increasing feature importance ture i is given by [ 𝑓( 𝑆 ∪{ 𝑖 }) − 𝑓( 𝑆)] . Theresulting quantityis weighted
for the RF model [43] . Correlation or statistical association across fea- by the number of combinations available to form the set prior to addi-
ture importance values from different models is then quantified using tion of feature I , i.e., ( |𝑆|! ), and the order in which remaining features
correlation coefficients. might be added, i.e., (( |𝑁|− |𝑆|− 1)! ). Hence, the importance of a given
Feature importance correlation analysis only requires model-internal feature i is defined by Eq. (4) :
information, but does not depend on model explanations. Feature ∑
weights or importance values can be extracted using multiple strategies, 𝜙 𝑖 = 𝑁1
!
|𝑆 |! ( |𝑁 |− |𝑆 |− 1) ! [ 𝑓 ( 𝑆 ∪{ 𝑖 }) − 𝑓 ( 𝑆 )] (4)
depending on the underlying ML algorithm. Moreover, in contrast to 𝑆⊆𝑁∖ { 𝑖 }
SHAP-based analysis of individual predictions, feature importance cor- It follows that Shapley values represent a unique way of dividing a
relation analysis is based upon global model assessment, taking many model’s output among feature contributions satisfying three axioms: lo-
compounds into account. While SHAP and feature importance correla- cal accuracy (or additivity), consistency (or symmetry), and nonexistence
tion analysis are conceptually distinct, these methods are complemen- (or null effect).
tary and can be combined for ML model assessment. It is anticipated Additive feature attribution methods typically do not consider two
that increasing efforts will be expanded in chemoinformatics to develop properties that are of high relevance for assessing feature importance,
methods for model explanation in order to further increase the accep- i.e., local accuracy and consistency . The SHAP formalism was specifically
tance of ML in the practice of medicinal chemistry. devised to take these axiomatic properties into account [36] . The prop-
erty local accuracy ensures that the sum of individual feature attributions
Declaration of Competing Interest is equal to the original prediction because SHAP allocates the model pre-
diction across contributing features. Furthermore, consistency ensures
The authors declare that they have no known competing financial that feature importance correctly accounts for different models on a rel-
interests or personal relationships that could have appeared to influence ative scale. Hence, if a change in a feature value has larger impact on
the work reported in this paper. model A than model B , feature importance should be larger in A . These
properties can be accounted for by representing feature importance as
Appendix A Shapley values [36] .
A weighting procedure for artificial samples is a key aspect for con-
SHAP theory necting Shapley values to the LIME approach. In LIME, heuristic choices
are made to select  , Ω( 𝑔) , and 𝜋 𝑥 . By contrast, the SHAP method in-
The principal goal of an explanation model g is to locally approx- troduces a special kernel function that is related to the Shapley value
imate and thereby simplify a complex model f that is difficult to un- definition, assuming that feature weights follow the two axioms of in-
derstand. Additive feature attribution methods generate an explanation terpretability. Specifically, SHAP uses the following procedure for inter-
model via a linear function of binary variables, given by Eq. (2) : preting an instance x :
( ) ∑𝑀 (i) Training data are organized by k -means clustering and the k sam-
𝑔 𝑥 ′ = 𝜙 0 + 𝜙 𝑖 𝑥 ′ 𝑖 (2) ples are weighted by the number of training instances they rep-
𝑖 =1 resent. These samples constitute a background data set of given
where 𝑥 ′∈{ 0 , 1} 𝑀 , M is the number of input features, and 𝜙 𝑖 ∈ℝ . The feature values.
presence or absence of a feature value impacting the prediction repre- (ii) Artificial samples are obtained by replacing features of the test
sents a feature contribution ( 𝜙 𝑖 ). Accordingly, a weight must be assigned instance x with the values from the background data set.
to each variable for which the LIME methodology [37] can be applied (iii) These artificial samples are weighted by the value of the SHAP
and further extended. kernel calculated for each sample and x .
3R. Rodríguez-Pérez and J. Bajorath Artificial Intelligence in the Life Sciences 1 (2021) 100009
(iv) A weighted linear regression model g is trained to predict f(x) . [19] Castelvecchi D . Can we open the black box of AI? Nature 2016;538:20–3 .
The model coefficients are Shapley values corresponding to fea- [20] Hirschfeld L , Swanson K , Yang K , Barzilay R , Coley CW . Uncertainty quantifica-
tion using neural networks for molecular property prediction. J Chem Inf Model
ture importance estimates.
2020;60:3770–80 .
[21] Soleimany AP , Amini A , Goldman S , Rus D , Bhatia SN , Coley CW . Evidential deep
Sampling all possible feature subsets is avoided through permuta-
learning for guided molecular property prediction and discovery. ACS Cent Sci
tion of the feature vector by setting features on and off. A feature is 2021;7:1356–67 .
assigned a large weight if its replacement with an artificial value leads [22] Molnar C , Casalicchio G , Bischl B . Interpretable machine learning – a brief
history, state-of-the-art and challenges. In: Proceedings of the Joint European Con-
to a significant change in model output. Weights of artificial samples
ference on machine learning and knowledge discovery in databases. Springer; 2020.
are determined as the number of feature addition sequences of a given p. 417–31 .
subset by the SHAP kernel. Coefficients from local linear regression pro- [23] Rudin, C.; Chen, C.; Chen, Z.; Huang, H.; Semenova, L.; Zhong, C. Interpretable
vide feature weights as Shapley values, which indicate how important
machine learning: fundamental principles and 10 grand challenges. arXiv preprint
arXiv:2103.11251 , 2021.
a feature is for a given prediction including the direction (sign) of fea- [24] Polishchuk P . Interpretation of quantitative structure-activity relationship models:
ture influence. The expected explanatory value is calculated as the mean past, present, and future. J Chem Inf Model 2017;57:2618–39 .
of the model output probability (numerical value) over training set in- [25] Hansen K , Baehrens D , Schroeter T , Rupp M , Müller KR . Visual interpretation of
kernel-based prediction models. Mol Inf 2011;30:817–26 .
stances. For a given instance, the model output is then calculated as the [26] Balfer J , Bajorath J . Visualization and interpretation of support vector machine ac-
sum of the expected (base) value and all SHAP feature values. tivity predictions. J Chem Inf Model 2015;55:1136–47 .
[27] Balfer J , Bajorath J . Introduction of a methodology for visualization and graphical
References interpretation of Bayesian classification models. J Chem Inf Model
2014;54:2451–68 .
[28] Ghorbani A , Abid A , Zou J . Interpretation of neural networks is fragile. In: Proceed-
[1] Struble TJ , Alvarez JC , Brown SP , Chytil M , Cisar J , DesJarlais RL , Engkvist O , ings of the AAAI Conference on artificial intelligence, 33; 2019. p. 3681–8 .
Frank SA , Greve DR , Griffin DJ , Hou X , Johannes JW , Kreatsoulas C , Lahue B , [29] Johansson U , Sönströd C , Norinder U , Boström H . Trade-off between accuracy
Mathea M , Mogk G , Nicolaou CA , Palmer AD , Price DJ , Robinson RI , Salentin S , and interpretability for predictive in silico modeling. Future Med Chem
Xing L , Jaakkola T , Green WH , Barzilay R , Coley CW , Jensen KF . Current and fu- 2011;3:647–63 .
ture roles of artificial intelligence in medicinal chemistry synthesis. J Med Chem [30] Iooss B , Saltelli A . Introduction to sensitivity analysis. In: Ghanem R, Higdon D,
2020;63:8667–82 . Owhadi H, editors. Handbook of uncertainty quantification. Cham: Springer; 2016.
[2] Yang X , Wang Y , Byrne R , Schneider G , Yang S . Concepts of artificial intelligence p. 1–20 .
for computer-assisted drug discovery. Chem Rev 2019;119:10520–94 . [31] Baskin II , Ait AO , Halberstam NM , Palyulin VA , Zefirov NS . An approach to the in-
[3] Walters WP , Barzilay R . applications of deep learning in molecule generation and terpretation of backpropagation neural network models in QSAR studies. SAR QSAR
molecular property prediction. Acc Chem Res 2020;54:263–70 2020 . Environ Res 2002;13:35–41 .
[4] Mater AC , Michelle LC . Deep learning in chemistry. J Chem Inf Model [32] Marcou G , Horvath D , Solov’ev V , Arrault A , Vayer P , Varnet A . Interpretabil-
2019;59:2545–59 . ity of SAR/QSAR models of any complexity by atomic contributions. Mol Inf
[5] Pannu A . artificial intelligence and its application in different areas. Artif Intell 2012;31:639–42 .
2015;4:79–84 . [33] Shapley LS . A value for N-person games. Contributions to the theory of games.
[6] Bajorath J . State-of-the-art of artificial intelligence in medicinal chemistry. Future Kuhn HW, Tucker AW, editors, Princeton: Princeton University Press; 1953. Annals
Sci OA 2021;7:FSO702 . of Mathematical Studies, pp. 307-317 .
[7] Varnek A , Baskin I . Machine learning methods for property prediction in chemoin- [34] Young HP . Monotonic solutions of cooperative games. Int J Game Theory
formatics: quo vadis? J Chem Inf Model 2012;52:1413–37 . 1985;14:65–72 .
[8] Vapnik VN . The nature of statistical learning theory. 2nd editor. New York: Springer; [35] Osborne MJ , Rubinstein A . A course in game theory. Cambridge: MIT Press; 1994 .
2000 . [36] Lundberg SM , Lee S . A unified approach to interpreting model predictions. Adv Neu-
[9] Breiman L . Random forests. Mach Learn 2001;45:5–32 . ral Inf Process Syst (NIPS) 2017;30:4766–75 .
[10] Alpaydin E . Introduction to machine learning. 2nd ed. Cambridge, USA: MIT Press; [37] Ribeiro MT , Singh S , Guestrin C . “Why should I trust you? ”Explaining the pre-
2010 . dictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International
[11] Baskin I , Winkler D , Tetko IV . A renaissance of neural networks in drug discovery. Conference on knowledge discovery and data mining; 2016. p. 1135–44 .
Expert Opin Drug Discov 2016;11:785–95 . [38] Rodríguez-Pérez R , Bajorath J . Interpretation of compound activity predictions from
[12] Rudin C . Stop explaining black box machine learning models for high stakes deci- complex machine learning models using local approximations and Shapley values.
sions and use interpretable models instead. Nat Mach Intell 2019;1:206–15 . J Med Chem 2020;63:8761–77 .
[13] Bajorath J . Foundations of data-driven medicinal chemistry. Future Sci OA [39] Rodríguez-Pérez R , Bajorath J . Interpretation of machine learning models using
2018;4:FSO320 . shapley values: application to compound potency and multi-target activity predic-
[14] Shen D , Wu G , Suk HI . Deep learning in medical image analysis. Ann Rev Biomed tions. J Comput Aided Mol Des 2020;34:1013–26 .
Eng 2017;19:221–48 . [40] Lundberg SM , Erion G , Chen H , DeGrave A , Prutkin JM , Nair B , Katz R , Himmel-
[15] Deng L , Liu Y . Deep learning in natural language processing (editors). New York: farb J , Bansal N , Lee S . From local explanations to global understanding with ex-
Springer; 2018 . plainable AI for trees. Nat Mach Intell 2020;2:56–67 .
[16] Stokes JM , Yang K , Swanson K , Jin W , Cubillos-Ruiz A , Donghia NM , McNair CR , [41] Riniker S , Landrum GA . Similarity maps –a visualization strategy for molecular
French S , Carfrae LA , Bloom-Ackermann Z , Tran VM , Chiappino-Pepe A , Badran AH , fingerprints and machine-learning methods. J Cheminf 2013;5:43 .
Andrews IW , Chory EJ , Church GM , Brown ED , Jaakkola TS , Barzilay R , Collins JJ . [42] Rodríguez-Pérez R , Bajorath J . Feature importance correlation from machine learn-
A deep learning approach to antibiotic discovery. Cell 2020;180:688–702 . ing indicates functional relationships between proteins and similar compound bind-
[17] Walters WP , Murcko M . Assessing the impact of generative AI on medicinal chem- ing characteristics. Sci Rep 2021;11:14245 .
istry. Nat Biotechnol 2020;38:143–5 . [43] Zwillinger D , Kokoska S . Standard probability and statistics tables and formulae.
[18] Bajorath J , Kearnes S , Walters WP , Meanwell NA , Georg GI , Wang S . Artificial intel- New York: CRC Chapman & Hall; 2000 .
ligence in drug discovery: into the great wide open. J Med Chem 2020;63:8651–2 .
4