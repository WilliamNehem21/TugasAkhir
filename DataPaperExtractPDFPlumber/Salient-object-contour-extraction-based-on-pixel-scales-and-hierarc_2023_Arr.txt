Array17(2023)100270
ContentslistsavailableatScienceDirect
Array
journalhomepage:www.elsevier.com/locate/array
Salientobjectcontourextractionbasedonpixelscalesandhierarchical
convolutionalnetwork
XixiYuana,YouqingXiaob,ZhanchuanCaib,âˆ—,LeimingWuc,âˆ—
aCollegeofElectronicsandInformationEngineering,ShenzhenUniversity,Shenzhen518061,China
bSchoolofComputerScienceandEngineering,MacauUniversityofScienceandTechnology,999078,MacaoSpecialAdministrativeRegionofChina
cSchoolofInformationEngineering,GuangdongUniversityofTechnology,Guangzhou510006,China
A R T I C L E I N F O A B S T R A C T
Keywords: Image salient object contours are helpful for many advanced computer vision tasks, such as object segmen-
Salientobjects tation,actionrecognition,andsceneunderstanding.Weproposeanewcontourextractionmethodforsalient
Objectcontours objectsbasedonpixelscaleknowledgeandhierarchicalnetworkstructure,whichimprovestheaccuracyof
Adaptivelossfunction objectcontours.First,adeephierarchicalnetworkisdesignedtocapturerichfeaturedetails.Then,anewloss
Hierarchicalnetwork
functionwithadaptiveweightedcoefficientsisdeveloped,whichcanreducetheunevendistributioninfluence
Pixelscales
ofcontourpixelsandnon-contourpixelsintrainingdatasets.Next,theobjectcontoursareclassifiedbasedon
thescaleinformationofcontourpixels.Byimportingthepriorknowledgeofscalecategoriesintothenetwork
structure,themodelrequiresasmallnumberoftrainingsamples.Finally,theregressiontaskofcontourscale
predictionisaddedtothenetwork,andtheprecisecontourscalesofforegroundobjectsareperformedasprior
knowledge or an auxiliary task. The experimental results demonstrate that compared with related methods,
theproposedmethodachievessatisfactoryresultsfromprecision/recallcurvesandF-measurescoreestimation
onthreedatasets.
1. Introduction thediscontinuityofcharacteristicdistribution(e.g.,pixelgrayscaleand
texture)inanimage.Itisacollectionofpixelswithstepchangesinthe
The salient object (or foreground object) in a natural image is image.Asoneoftheprimaryimagefeatures,imageedgedetectionis
alwaysinterestedtohumanbeings,anditisaresearchtrendintheera mainlyusedtoenhancethecontouredges,detailsandgrayscalejumps
ofartificialintelligence.Thecontoursareessentialstructurecharacter- inanimage.
isticsofobjects,whichprovidesposture,size,andcategoryinformation Many researches have been conducted around image contour ex-
of objects in an image [1â€“3]. Therefore, it is significant to study traction. Firstly, the early image processing methods basically locate
approachesofsalientobjectcontourextraction. contours by extracting texture features [8,9]. However, due to the
Object contours play an important role in semantic recognition
massivecomputationoftraditionalmethods,itisdifficulttodealwith
tasks,suchasobjectextraction,actionrecognition,andbehavioranaly-
complexscenariosinanimage.Then,withthespringofdeeplearning
sis[4â€“6].Infact,thecontourreferstotheperipheryofanobjectorthe
methods, image contour extraction has been further developed. For
outer frame of a figure. The contour detection aims to extract curves
instance, the holistically-nested edge detection (HED) method [10] is
thatcanreflecttheshapesoftheobjectinanimage.Inaddition,there
based on â€˜â€˜VGG16â€™â€™ (i.e., Visual Geometry Group-16) model [11] and
aresomedifferencesamongedges,boundariesandcontours.Theedges
side output strategy to extract contours from natural images, which
and boundaries generally means the discontinuities of objects in an
is illustrated in Fig. 1(a). The edge detection method based on richer
image in terms of photometrical, geometrical, and physical features.
convolutional features (RCF) [12] concatenates adjacent convolutions
Firstly, in [7], the boundary is defined as the contour of an image,
ateachstageofVGG16,anditeffectivelyimprovesfeatureextraction
and it reflects the changes of pixel ownership from one object to
performance,whichisrepresentedinFig.1(b).However,theexisting
another. Other researchers tend to regard contours as boundaries of
end-to-endmethodsextractalledgesintheimage,andtheycannotrec-
interestingregions.Whenthecontoursarenotregionboundaries,the
closedcontourscannotbegeneratedbycontourdetectors,norcanthe ognizethesalientobject[13].Theedgeguidancenetwork(EGNet)[14]
image be divided into different regions. Secondly, the image edge is adopts the step-by-step method to detect salient object: in the first
âˆ— Correspondingauthors.
E-mailaddresses: yuanxixi@szu.edu.cn(X.Yuan),xiaoyq11@vanke.com(Y.Xiao),zccai@must.edu.mo(Z.Cai),leiming_wu@gdut.edu.cn(L.Wu).
https://doi.org/10.1016/j.array.2022.100270
Received10October2022;Receivedinrevisedform2December2022;Accepted5December2022
Availableonline7December2022
2590-0056/Â©2022TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-
nc-nd/4.0/).X.Yuanetal. Array17(2023)100270
step, the end-to-end method is used to obtain salient objects; in the
secondstep,thecontoursareextractedbasedongradientinformation
ofsalientobjects.However,thismethodwithtwostepsiscomplicated
in computation. At the same time, the error is expanded, including
the salient object extraction error and the contour extraction error
with gradient information. In the deep category-aware semantic edge
detection network (CASENet) [15], each contour pixel is associated
withmorethanonecategory.Thepixelsappearincontoursorjunctions
belonging to two or more semantic categories. A saliency detection
method based on salient contour-aware with twice learning strategy
is proposed in [16], but it cannot always extract saliency contours
accurately.Liuetal.[17]presentedasimultaneousdetectionmethod
of salient object, edge, and skeleton using the dynamic feature inte-
gration(DFI)andthetask-adaptiveattentionmodule.Itproducesmost
importantedgesintheimage,butitcannotdistinguishobjectcontours
exactly.
Anewsalientobjectcontourextractionmethodisproposedinthis
paper, which can recognize the foreground object as illustrated in
Fig.1. Briefillustrationoftheproposedmethodandtworelatedmethods.(a)theHED
Fig.1(c).Inthismethod,animprovedhierarchicalnetworkisdesigned
method[10]fusesside-outputfeaturesatdifferentstagestoextractalledgesfromthe
tointegraterichfeatures.Then,anewSoftmaxlossfunctionwithadap- original image. (b) the RCF method [12] fuses convolutional layers at one stage to
tive coefficients is proposed to balance unevenly distributed samples, conductonesideoutput,andthenfusessideoutputsatallstagestogeneratethefinal
which is used to improve the accuracy of contour extraction. Finally, edges.(c)intheproposedmethod,thehierarchicalintegration,regression,andthenew
Softmaxfunctionareadoptedtoexactsalientobjectcontoursfromthenaturalimage.
inordertoevaluateobjectcontourextractionperformance,threenew
datasetsarepresentedbasedonSK-SMALL[18],SK-LARGE[19],and
WH-SYMMAX [20] datasets, and they have skeleton scales originally.
Inthispaper,thesedatasetsareregeneratedforsalientobjectcontour differentsupervisionaccordingtoreceptivefieldsatdifferentstagesof
position,andthentheyareusedintrainingandtestingexperiments.In thenetworkstructure.IntheRCFmethod[12],allconvolutionallayers
at one stage are concatenated, and then the side outputs at different
addition,therearefourmaincontributionsoftheproposedmethod:
stages are fused. This strategy extracts richer convolutional features
(1) Different from popular edge extraction methods of HED, RCF,
thantheHED.Inaddition,theconvolutionalorientedboundaries(COB)
andDFI,theproposedmethodextractssalientobjectcontours,which
isproposedin[23].DifferentfromtheHEDthatusestheside-outputsto
is more significant than image edges for intelligent industry. Besides,
obtaincoarsecontourmaps,theCOBusesmultiplesmallsub-networks
thisisanend-to-endmethodwithoutcalculatinggradientinformation
to predict oriented contour maps, and then the max response of the
tosavecomputingresources.
sub-networkisoutputtedtodecidethefinalorientationofeachpixel.
(2) The extracted contour pixels contain scale information, which
However, all of these methods extract whole edge features in the
canberegardedasapriorknowledgeandanauxiliarytasktoimprove
image, which cannot be used to recognize salient objects. The fully
therecognitionaccuracy.
convolutional encoderâ€“decoder network (CEDN) [24] focuses on de-
(3) A new loss function with adaptive weighted coefficients is
tecting higher-level object contours based on the dataset of PASCAL-
proposed. The new loss function is suitable for images with uneven VOC. But it still outputs some boundaries of background objects. The
distributionofsamplecategories,whichhelpstoimprovetheextraction instance-levelsalientobjectsegmentationnetwork(MSRNet)[25],the
accuracyofcontourpixels.WhilethelossfunctionusedintheHEDcan awaresalientobjectdetectionnetwork(BASNet)[26],andthedynamic
onlybeusedforbinaryclassificationproblems. feature integration network [17] are salient instance segmentation
(4)Furthermore,ahierarchicalnetworkforcontourextractionisde- methods,buttheycannotgenerateclearobjectcontours.
signedtoextractrichfeatures,andnewsalientobjectcontourdatasets Multi-task hierarchical network: In the network of HED, the loss
withscaleinformationarebuilt. generated by multiple side-outputs is directly conveyed back to the
The remainders of this paper are organized as follows. Section 2 corresponding convolutional layer, avoiding the gradient disappear-
introduces related works of contour extraction and the basic network ance. At the same time, different scales of features are learned at
structure. The principle of the salient object contour extraction are different convolutional layers. Based on the idea of side-outputs, the
describedinSection3.Section4demonstratestheeffectivenessofthe RCF makes full use of multi-scale and multi-level information of the
proposed method. At last, the conclusion and future works are stated object, and completes image-to-image prediction by fusing all mean-
inSection5. ingful convolutional features. The FSDS classifies image pixels to five
categories according to the size of the skeleton scale; different stages
2. Relatedwork of network structure have different perception fields; and different
perception fields can supervise different skeleton pixel categories. In
the network of learning multi-task deep side outputs (LMDS) [19],
Different from the existing contour extraction methods, the pro-
some regression tasks are added to the FSDS network structure to
posed method can extract salient object contours in an image. The
predict pixel scales. The hierarchical feature integration network (Hi-
developmentofcontourextractionmethodsandthemulti-taskhierar-
Fi)[27]canfurtherextractricherfeaturesandpromotetherecognition
chicalnetworkareintroducedbelow.
accuracy.Therefore,anewhierarchicalnetworkisdesignedforsalient
Contourextraction:Inthepastdecades,manyresearchesonimage
objectcontourextractioninthispaper.
contourextractionhaveemerged.Thesearemainlyclassifiedintotwo
categories.Oneistheearlytraditionalimageprocessingmethods,such 3. Methodology
as Roberts operator, Sobel operator, and Canny operator. The other
oneisthemachinelearningmethods.Inrecentyears,machinelearning In order to improve the extraction accuracy of salient object con-
has been widely used in intelligent industries [17,21,22]. In the HED tours, a new network structure is designed in this paper, including
method [10], the â€˜â€˜side outputâ€™â€™ convolution is used to improve the thescale-basedhierarchicalstructure,theweightedSoftmaxlossfunc-
edge extraction accuracy and computational efficiency. Afterwards, tion,thecontourpixelscaleclassification,andthecontourpixelscale
thescale-associatedside-output(FSDS)isproposedin[18].Itrealizes predictionwithregressiontasks.
2X.Yuanetal. Array17(2023)100270
Fig.2. Theproposednetworkstructureofsalientobjectcontourextraction.
Fig.3. ThehierarchicalnetworkstructurewithfivelevelsbasedontheVGG16network
structure.
3.1. Hierarchicalnetworkstructure
TheproposednetworkstructureisshowninFig.2,andthenewhi-
erarchicalnetworkstructureisdesignedbasedonVGG16.TheVGG16
has five stages, and the pooling layer is connected at each stage to
adjustthereceptivefieldsizes.Onlythelastconvolutionallayerateach
stageperformshierarchicaloutputs.ItisknownthattheVGG16-based Fig.4. Theintegrationprincipleofdifferentlevels.(a)and(b)representthehierar-
chicalstructureofH1andH2,andtheiroutputsareH1-outandH2-out.(c)illustrates
networkstructurelosesfeaturesinpoolingprocesseswiththeincrease
thefusedoutputofH1-outandH2-out.(d)theSoftmaxLossContouroutputlayermeans
ofreceptivefields.Theside-outputstrategycanreducefeaturelossesin thattheSoftmaxlossfunctionhasadaptivecoefficients.
featureextraction,whichisadoptedinthedesignednetworkstructure.
The hierarchical network structure is shown in Fig. 3, which con-
tainsfivelevels(i.e.,H0,H1,H2,H3,andH4),andtheperformanceof
themareanalyzedinexperiments.ThemethodwiththreelevelsofH0,
H1,andH2areexplainedspecificallyinthefollowing.Fig.4(a)and(b)
represents the hierarchical structure of H1 and H2, and their outputs
areH1-outandH2-out.Fig.4(c)illustratesthefusedoutputofH1-out
andH2-out,whichiscalledH1H2-out(whenthenetworkstructureonly
contains H0, H1, and H2 levels). It should be noted that there is no
outputlayerattheH0level,whereeachoutputlayercorrespondstoa
lossfunction.Inaddition,anewSoftmaxlossfunctioniscustomizedto
dealwithimbalancedistributionofcontourandnon-contourpixels.In Fig.5. ThenewnetworkstructuresofH1andH2levelswithregressiontasks.
thispaper,theregressiontasksareonlyusedasauxiliarytaskstoreduce
lossesoffeatureinformation,whicharenotfusedinclassificationtasks.
The contour pixels of a salient object have scale features, which 3.2. Contourpixelandcontourmap
is denoted as a real number. The linear regression is used to fit a
prediction model for the values of ğ‘Œ and ğ‘‹ in the observed dataset. In order to extract object contour pixels in convolutional neural
Therefore,thelinearregressiontaskisusedtoobtaincontourscales.In network, the image pixels confront with classification problems. The
ordertoreducefeaturelosses,theregressiontasksareaddedintothe pixel scales are classified by piecewise quantization. The number of
network structure named as â€˜â€˜Ours1ARâ€™â€™ and â€˜â€˜Ours2ARâ€™â€™. The biggest categories depends on the sizes of receptive fields in the network
differencesbetweenFigs.4and5lieinthelinearregressiontasksand structure. For example, from the first stage to the fifth stage of the
lossfunctions. VGG16,astheextractedfeaturesbecomemoreandmoreabstract,the
3X.Yuanetal. Array17(2023)100270
Table1
Thecontourscalecategoriesandtheranges.
Scales â‰¥150âˆ¥<1 [1,10) [10,26) [26,60) [60,150)
Category 0 1 2 3 4
sizes of receptive fields increase gradually, so the counter pixels are
definedintofivecategoriesinthispaper.
Contourpixelscalepiecewisequantization:AsshowninFig.6(c),
the scale of a contour pixel refers to the radius of the maximum
inscribed circle that is centered on the skeleton point and is tangent
to the contour of the object. In other words, the scale is the distance
betweentheskeletonpointandthecontourpointoftheobjectthatis
closesttotheskeletonpoint.
Intheclassificationtaskofcontourpixels,thepixelsaredividedinto
ğ‘˜categories.Thecategory0representsnon-contourpixels.TheVGG16
networkstructurehasfivestages,andthelastconvolutionofeachstage
correspondstothesizesofreceptivefieldswith5,14,40,92,and196,
respectively[19].Accordingtotherangesofdifferentreceptivefields,
thecontourscalequantizationcategorytableisabletobeobtainedin
Fig.6. Theillustrationofcontourpixelscale.(a)isthecontourofthehorse.(b)isthe
Table1.Forexample,thesecondcategoryofcontourpixelsrepresents skeletonofthehorse.(c)includesthecontourandskeleton.Theredcircleindicatesthe
the scales of pixels between the range of [12, 26). In Fig. 6(d), the maximuminscribedcirclefromtheskeletonpixeltothecontour,andthegreenscaleof
contours with different scales are shown in four kinds of colors, and thepointoftangencyoncontourpixelistheradiusoftheinscribedcircle.In(d),the
contourswithdifferentscalesareshowninfourkindsofcolors,andthecorresponding
they are red, yellow, blue, and green from big scales to small scales.
skeletonsareshowninlightercolors.(Forinterpretationofthereferencestocolorin
Then,thecorrespondingskeletonsareshowninfourlightercolors. thisfigurelegend,thereaderisreferredtothewebversionofthisarticle.)
Contour map generation: One image can be defined as a non-
âˆ‘ âˆ‘
contour pixel map and a contour pixel map . The
ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘›ğ‘¡ğ‘œğ‘¢ğ‘Ÿ ğ‘ğ‘œğ‘›ğ‘¡ğ‘œğ‘¢ğ‘Ÿ
finalcontoursoftheimagearecomposedofdifferentscales.ğ‘† (non-
0
contour pixels) is defined as category 0, and ğ‘† (ğ‘– âˆˆ {0,1,â€¦,ğ‘˜âˆ’1})
ğ‘–
is the predicted category of scales. For instance, ğ‘† represents the
2
pixel set with the scales belonging to category 2, i.e., the scales of
those pixels are lie in [10,26). More details are shown in the con-
tour scale piecewise quantization section. Then, it can be found that
the ğ‘† â‹ƒğ‘† â‹ƒğ‘† â‹ƒ â‹¯â‹ƒğ‘† is the contour map. Consequently, the
1 2 3 ğ‘˜âˆ’1
followingequationsareobtained: Fig.7. Thecontourmapgenerationillustration.(a)istheoriginalimage.(b)isthe
âˆ‘
=ğ‘†
â‹ƒ
ğ‘†
â‹ƒ
ğ‘†
â‹ƒ â‹¯â‹ƒ
ğ‘† (1)
contourmapgeneratedbyğ¼âˆ’ğ‘† 0.(c)isthecontourmapofğ‘† 0.
1 2 3 ğ‘˜âˆ’1
ğ‘ğ‘œğ‘›ğ‘¡ğ‘œğ‘¢ğ‘Ÿ
âˆ‘
=ğ‘† (2)
0
ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘›ğ‘¡ğ‘œğ‘¢ğ‘Ÿ
Afterğ‘† isobtained,ğ¼âˆ’ğ‘† isanotherformofobjectcontourmap
0 0
with binary image, where ğ¼ is the identity matrix in the trainer, as
showninFig.7.
3.3. Scalepredictionofcontourpixels
The normalization of contour scales and loss functions in the Fig.8. Therelationshipbetweenthegroundtruthandthesizesofreceptivefields.In
regressiontask:Thescaleofacontourpixelisarealnumberthatcan H2layer,iftherangeofreceptivefieldsofâ€˜â€˜LRTask1â€™â€™is[0,14),thesizeofthefield
bepredictedinregressiontasks.Inordertoimprovethepredictionac- equalsto14;iftherangeofreceptivefieldsofâ€˜â€˜LRTask2â€™â€™is[0,40),thesizeofthe
fieldequalsto40.
curacy,anormalizationmethodofcontourscaleisproposedintraining
steps. The network proposed in this paper consists of five stages, and
eachstagehasindependentregressiontaskstopredictthescales.Noted
ğºğ‘‡ denotes the initial contour scale. ğ‘Ÿ is the magnification rate. In
thattherearedifferentnumbersofregressiontasksatdifferentlevels. ğ‘–ğ‘›ğ‘–ğ‘¡
Forexample,theH1levelhasfiveregressiontasksandtheH2levelhas thestandardnormalization,thevalueofğ‘Ÿtakesâ€˜â€˜1â€™â€™.Whileğ‘Ÿissetas
fourregressiontasks.Theregressiontaskisusedtopredictthecontour â€˜â€˜2â€™â€™ in the experiments of this paper. The regression loss is set as the
scale at different stages, depending on the size of receptive fields. standardEuclideanlossfunction.
Therefore,itisnecessarytoestablishthecorrespondencebetweenthe Scalepredictionofcontourpixels:Accordingtotheregressiontask,
ground truth of contour scales and the values of receptive fields at the real numbers of contour scales are obtained. The experimental
differentstagesduringthetrainingstage,asshowninFig.8. results show that the classification task is able to get a clear contour
Thecontourscalenormalizedbeforetrainingisinthefollowing: map.BecauseofthelimitationoftheVGG16-basednetwork,onlyfive
categoriescanbeobtainedandthenumericalvalueisinaccurate.The
ğºğ‘‡ =â§ âª ğºğ‘‡ ğ‘ğ‘–ğ‘›ğ‘–ğ‘¡ Ã—ğ‘Ÿ, ğºğ‘‡ ğ‘–ğ‘›ğ‘–ğ‘¡â‰¤ğ‘ ğ‘– (3) regression task helps to get a numerical value, but the contours are
â¨ ğ‘– obscure,asshowninFig.9.
âª 0, ğºğ‘‡ ğ‘–ğ‘›ğ‘–ğ‘¡>ğ‘ ğ‘–
â© Asmentionedabove,theclassificationtasksandtheregressiontasks
where ğ‘ denotes the receptive field at stage ğ‘–. ğºğ‘‡ denotes the are combined to further improve the prediction accuracy of contour
ğ‘– ğ‘›ğ‘œğ‘Ÿğ‘š
contour scale after normalization, which is abbreviated as ğºğ‘‡ in (3). scalesintheproposedmethod.Thefinalscalevaluesofobjectcontours
4X.Yuanetal. Array17(2023)100270
PseudoCode1ComputationofCoefficientğœ†
Input: (â„,ğ‘¤)
Output: ğœ†
1: % â„ means the width of original image, ğ‘¤ means the height of
originalimage,andğœ†istheweightcoefficient.
2: Letğ‘–representpixelpointandğ‘— bethecategoryofpixel.
3: ğ‘ ğ‘¢ğ‘šğ‘ƒğ‘–ğ‘¥ğ‘’ğ‘™ğ‘ =â„Ã—ğ‘¤
4: forğ‘–=1toğ‘ ğ‘¢ğ‘šğ‘ƒğ‘–ğ‘¥ğ‘’ğ‘™ğ‘ do
5: ifpixelğ‘–âˆˆ{ğ‘—}, ğ‘—=1,2,â‹¯ğ‘˜. then
6: Computeğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ¶ğ‘™ğ‘ğ‘ ğ‘  ğ‘—+1
Fig.9. Thecontourscalepredictionresults.(a)istheoriginalimage.(b)isthecontour 7: endif
mapgeneratedbyclassificationtasks.(c)isthecontourmapgeneratedbyregression 8: endfor
tasks. 9: Compute ğœ† ğ‘— = 1 âˆ’ ğ‘ğ‘–ğ‘¥ğ‘’ â„ğ‘™ Ã—ğ¶ ğ‘¤ğ‘™ğ‘ğ‘ ğ‘ ğ‘—, and the weight coefficient ğœ† ğ‘— for
categoryğ‘— canbeobtained.
canbecalculatedby(4):
{
ğ¿ğ‘…ğ‘–ğ‘›ğ‘–ğ‘¡ Ã—ğ‘, ğ¿ğ‘… â‰¤ğ‘
ğ¿ğ‘… = ğ‘Ÿ ğ‘– ğ‘–ğ‘›ğ‘–ğ‘¡ ğ‘– (4) 4.1. Datasetandimplementationdetails
ğ‘ ğ‘ğ‘ğ‘™ğ‘’ 0, ğ¿ğ‘… >ğ‘
ğ‘–ğ‘›ğ‘–ğ‘¡ ğ‘–
whereğ¿ğ‘… istheinitialpredictedscaleandğ¿ğ‘… isthefinalcontour
ğ‘–ğ‘›ğ‘–ğ‘¡ ğ‘ ğ‘ğ‘ğ‘™ğ‘’ In order to obtain suitable image datasets with object contour
scaleintheregressiontask.Inaddition,ğ‘Ÿisthemagnification,andit
annotations, three different datasets are used for experiments, which
issetasâ€˜â€˜2â€™â€™inthispaper.
areobtainedfromthreeopendatasets:SK-LARGE[19],SK-SMALL[18],
If ğ¿ğ‘… ğ‘ ğ‘ğ‘ğ‘™ğ‘’âˆˆ[ğ‘† ğ‘™ğ‘œğ‘¤,ğ‘† â„ğ‘–ğ‘”â„),then: andWH-SYMMAX[20].Onaccountoftheskeletonandscaleinforma-
ğ‘ ğ‘ğ‘ğ‘™ğ‘’=ğ¿ğ‘… , tioninthesedatasets,thecontourinformationofobjectscanbelocated
ğ‘ ğ‘ğ‘ğ‘™ğ‘’
If ğ¿ğ‘… âˆ‰[ğ‘† ,ğ‘† ),then: (5) according to the scales of skeletons. Then, the training datasets with
ğ‘ ğ‘ğ‘ğ‘™ğ‘’ ğ‘™ğ‘œğ‘¤ â„ğ‘–ğ‘”â„
contourlabelsaregenerated.ThenewSK-LARGEcontains746training
ğ‘† +ğ‘†
ğ‘™ğ‘œğ‘¤ â„ğ‘–ğ‘”â„
ğ‘ ğ‘ğ‘ğ‘™ğ‘’= . imagesand745testimageswithcorrespondingcontourground-truth.
2
The new SK-SMALL dataset contains 506 images, and the first 300
Accordingtothepredictedcontourpixelcategoryandtheintroduc-
imagesareusedfortraining,whichisalsoasubsetofSK-LARGE.The
tionofSection3.2,thecoarserangeofpixelscalescanberepresented
newWH-SYMMAXdatasetcontains328horseimages,andthefirst228
as[ğ‘† ,ğ‘† ).Forexample,ifapixelbelongstocategory2,thatmeans
ğ‘™ğ‘œğ‘¤ â„ğ‘–ğ‘”â„
thescaleofthispixelrangesat[10,26),wherethevalueofğ‘† is10 areusedfortraining.
ğ‘™ğ‘œğ‘¤
and the value of ğ‘† is 26. Combining the predicted values of the To ensure justice, the experimental results of all methods are pro-
â„ğ‘–ğ‘”â„
regressiontaskandtheclassificationtask,thefinalcontourpixelscale duced under the same conditions (i.e., the same type of server and
canbegotaccordingto(5). GPU). Then, the GPU configuration is Titan XP in the operation con-
dition.
3.4. Designedsoftmaxlossfunction
It is known that the standard Softmax loss function [28] is com- 4.2. Evaluationprotocol
monlyusedtoclassifymultipleclasseswithbalancednumberofsam-
ples.However,theclassificationaccuracyofthestandardSoftmaxloss TheF-measureindicatorandprecision/recall(PR)curvesareused
functionisbadwhentheclassesofdifferentsamplesareunbalanced.In
as the evaluation protocol in this paper. The calculation process of
salientobjectcontourrecognition,thecontourpixelsoftheobjectare
F-measureisdefinedasfollows:
farlessthanthenon-contourpixels,sothereexistsasampleimbalance
2ğ‘ƒğ‘…
problem.Inordertoimprovetheclassificationaccuracy,anewSoftmax Fâˆ’measure= (7)
ğ‘ƒ+ğ‘…
lossfunctionisdesigned,whichcanbespecificallyexpressedas:
ğ½(ğœƒ)=âˆ’1 [ âˆ‘ğ‘š âˆ‘ğ‘˜
ğœ† ğ¼{ğ‘¦ =ğ‘—}ğ‘™ğ‘œğ‘”
ğ‘’ğ‘ğ‘— ]
(6)
w thh ee yre arğ‘ƒ
e
ca an ld cuğ‘… latm edea fn ros mpr te hc eisi do en tea cn ted dr ce oca nl tl ouv ralu me as p, r ae nsp de tc hti eve gl ry o, ua nn dd
-
ğ‘š
ğ‘–=1ğ‘—=1
ğ‘–ğ‘— ğ‘– âˆ‘ğ‘˜ ğ‘–=1ğ‘’ğ‘ğ‘—
truth. Specifically, the calculation process of PR curve complies with
where ğ‘š marks the number of samples in the training set, ğ‘˜ is the thefollowingsteps:
number of classes, ğœ† is the defined balance factor namely the weight
ofthelossfunction.Forthelabelğ‘¦,itcantakeonğ‘˜differentvalues,so 1. The predicted contour map is transformed into a binary map
inthetrainingset{(ğ‘¥1,ğ‘¦1),â€¦,(ğ‘¥ğ‘š,ğ‘¦ğ‘š)},ğ‘¦(ğ‘–)âˆˆ{1,2,â€¦,ğ‘˜}isobtained. accordingtothethresholdvalue.
ğ¼{ğ‘¦ =ğ‘—}istheindicatorfunctionthatindicateswhetherthesampleğ‘– 2. Thepredictedbinarycontourmapismatchedwiththeground-
ğ‘–
belongstocategoryğ‘—.Ifitis,thevalueis1,otherwisethevalueis0. truth contour map. In this process, a relatively small position
Thecoefficientğœ† ğ‘–ğ‘— isdefinedasğœ† ğ‘–ğ‘— =1âˆ’ ğ‘Œ ğ‘Œğ‘– ğ‘–ğ‘—.ğ‘–meanstheğ‘–-thsample offset is allowed between the detected contour pixels and the
inonebatchofthetrainingdataset.Iftheparameterofbatch-sizeis1 ground-truth. If a detected contour pixel matches at least one
(i.e.,ğ‘–=1),thatistosayonlyoneimageforonebatchisselected.|ğ‘Œ ğ‘–| ground-truthcontourpixel,thispointwillbemarkedasamatch-
representsthetotalnumberofpixelsintheğ‘–-thimage.Iftheheightof ing point ğ‘‡ğ‘ƒ (true positive). Otherwise, the detected contour
theimageisâ„andthewidthoftheimageisğ‘¤,thenthevalueof|ğ‘Œ ğ‘–| pointdoesnotmatchanyground-truthcontourpoint,thenthis
isâ„Ã—ğ‘¤.|ğ‘Œ ğ‘–ğ‘—|representsthenumberofpixelsbelongingtoclassğ‘—.
pointwillbemarkedasğ¹ğ‘ƒ (falsepositive).Ifthispredictedpixel
doesnotbelongtoanycontourpixelpoint,anditisconsidered
4. Experimentalresults
asacontourpointinground-truth,thenthispointiscalledğ¹ğ‘
Inordertoverifytheperformanceofthenewmethod,somecom- (falsepositive).
parative experiments are performed based on different datasets, with 3. According to different thresholds, different
âˆ‘ğ‘‡ğ‘ƒ, âˆ‘ğ¹ğ‘ƒ,
and
precision/recallcurvesandF-measurescoreestimation.
âˆ‘ğ¹ğ‘
canbecalculated.
5X.Yuanetal. Array17(2023)100270
Fig.10. ThePRcurvesofdifferentcontourextractionmethodsonthreedatasetsinexperiments.
Table2 Table3
TheexperimentresultsofdifferentcontourextractionalgorithmswithF-measures. TheexperimentalcomparisonresultsoftheSoftmaxfunctionwithadaptivecoefficients.
SK-LARGE SK-SMALL WH-SYMMAX Thevaluesinthetablearerelativeerrors.
SK-LARGE SK-SMALL WH-SYMMAX
HED[10] 0.5630 0.5750 0.5590
RCF[12] 0.6730 0.6994 0.7430 Ours1A(VsOurs1) +3.23% âˆ’0.88% +0.53%
DFI[17] 0.7076 0.7000 0.8552 Ours2A(VsOurs2) âˆ’0.58% +0.14% +0.89%
Ours3A(VsOurs3) +0.75% +1.18% +1.58%
Ours1 0.6912 0.6900 0.8368
Ours1A 0.7135 0.6840 0.8412
Ours1AR 0.6525 0.7040 0.8385
Table4
Ours2 0.7354 0.7150 0.8652 Theexperimentalcomparisonresultsofregressiontask.Thesignâ€˜â€˜+â€™â€™indicatesthatthe
Ours2A 0.7311 0.7150 0.8652 F-measureisimprovedafteraddingtheregressiontask,andthesignâ€˜â€˜âˆ’â€™â€™indicatesa
Ours2AR 0.7416 0.7110 0.8676 decrease.
Ours3 0.7166 0.6997 0.8504 SK-LARGE SK-SMALL WH-SYMMAX
Ours3A 0.7220 0.7080 0.8637
Ours1AR(VsOurs1A) â€“ + â€“
Ours3AR 0.7130 0.7110 0.8669
Ours2AR(VsOurs2A) + â€“ +
Ours3AR(VsOurs3A) â€“ + +
ThePRvaluesarecalculatedaccordingtotheseformulas: Table5
âˆ‘ğ‘‡ğ‘ƒ âˆ‘ğ‘‡ğ‘ƒ ThecomparisonofpositionaccuracybetweenouralgorithmandtheHED.Thevalues
ğ‘ƒ = âˆ‘ğ‘‡ğ‘ƒ+âˆ‘ğ¹ğ‘ƒ, ğ‘…= âˆ‘ğ‘‡ğ‘ƒ+âˆ‘ğ¹ğ‘ (8) inthetablearerelativeerrors.
SK-LARGE SK-SMALL WH-SYMMAX
According to different thresholds, a series of precision and recall Ours1 +22.77% +20.00% +49.70%
valuescanbeobtained.Finally,thesevaluesarefittedtogetaPRcurve. Ours2 +26.73% +11.30% +50.48%
Ours1A +15.89% +22.43% +50.00%
Ours2A +30.62% +24.17% +53.42%
4.3. Resultsofcomparativeexperiments
Ours1AR +29.86% +24.35% +54.78%
Ours2AR +31.72% +23.65% +55.21%
Three representative algorithms of HED, RCF, and DFI [10,12,17]
are selected to compare with the method proposed in this paper.
The HED originally exploits â€˜â€˜side outputâ€™â€™ convolution, which greatly
most. For example, when two levels are used, they are called Ours2.
improves the edge extraction accuracy and computational efficiency.
Due to the limitation of the GPU memory, some higher levels of the
Then, the RCF further enriches edge features by fused convolutions.
proposed method cannot be trained. In addition, in order to verify
Next, the DFI adopts dynamic feature integration to optimize edge
the influence of the variable coefficient loss function and the regres-
features, which is a state-of-the-art edge extraction algorithm. In this
siontaskoncontourextraction,relevantindependentexperimentsare
paper,someimprovementsarepresentedonthebasisofHEDandRCF
also carried out. Assuming that the Ours1A indicates the method of
network structures, and we optimize the object contours using pixel
Ours1withthevariablecoefficientSoftmaxlossfunctionmethod.The
scalefeatures.
Ours1ARindicatesthemethodofOurs1Aaddedwithregressiontask.
In experiment procedures, firstly, the contour maps are generated
The definition of other symbols can be analogized. The Table 2 lists
basedonthetrainedmodels.Then,therefinedcontoursareobtained
the F-measure values of the contour accuracy produced by different
bythestandardnon-maximumsuppressionalgorithm[29].Finally,the
networkstructureexperiments.
refined contour maps are evaluated by PR curves and F-measures. In
addition,theperformanceoftheproposedalgorithmisverifiedthrough TheaccuracydifferencesbetweenthenewSoftmaxlossfunctionand
comparative experiments on three datasets, and the generalization the standard Softmax loss function have been tested in experiments.
abilityofthenewalgorithmisprovedbycrossvalidation. FromTable3,itcanbefoundthatthenewSoftmaxlossfunctionhas
Thecontourextractionaccuracyresultsofseveraldifferentnetwork abetterperformancethanthestandardSoftmaxlossfunctiononthree
structuresarecomparedindetail.Generally,theVGG16-basedhierar- datasets,whichindicatesthatthenewSoftmaxlossfunctionisvalid.
chicalnetworkmaximallyhasfourlevels.Intheexperimentsofcontour In Table 4, it is shown that after adding the regression task, the
extraction,themethodsofdifferentlevelsarenamedasOurs1,Ours2, contourpixelextractionaccuracyisimprovedinmostcases.Thebest
andOurs3.Itisverifiedthatfortheproposedmethod,whenthelevel result is obtained by using Ours2AR network structure on the SK-
exceeds 2, the accuracy will be reduced, so the Ours3 is adopted at LARGE and WH-SYMMAX datasets, and the best F-measure value is
6X.Yuanetal. Array17(2023)100270
Fig.11. IllustrationofObjectcontourextractionresultsonSK-LARGEdatasetforseveralselectedimages.
Table6 other contour extraction methods in terms of salient object contours.
ThecomparisonofpositionaccuracybetweenouralgorithmandtheRCF.Thevalues Moreover,thesalientobjectcontoursobtainedbytheproposedmethod
inthetablearerelativeerrors.
contain scale information, which can be applied to the subsequent
SK-LARGE SK-SMALL WH-SYMMAX
skeletongenerationandsalientobjectgeneration.Asapriorknowledge
Ours1 +2.70% âˆ’1.34% +12.62%
intraining,itisalsohelpfultoimprovethemodelperformance.
Ours2 +6.02% âˆ’8.49% +13.22%
Ours1A âˆ’3.05% +0.66% +12.85% Inthefuturework,thenewapproachwillbeextendedlyoptimized
Ours2A +9.27% +2.09% +15.42% intwopossiblemanners.Firstly,amoreeffectivenetworkstructurecan
Ours1AR +8.63% +2.23% +16.45% be designed for object recognition. Secondly, multi-regressive aiding
Ours2AR +10.19% +1.66% +16.77%
taskslikeclassificationtaskscanbemergedintothenetwork,whichare
notappearedinexistingmethods.Afterthefusionstep,theregression
data can be used to estimate more accurate inscribed circle radiuses
0.7416and0.8676(seeTable2),respectively.Whiletheresultsonthe betweenthecontourandtheskeleton.Finally,thecontoursarelimited
SK-SMALLdatasethaschangelittlebyusingOurs2AandOurs2AR. tofivecategories,whichreducesthecontouraccuracy,sobetterresults
Tables 5 and 6 list the F-measures of the proposed method com- maybegeneratedbyincreasingscalecategories.
pared with HED and RCF methods on SK-LARGE, SK-SMALL, and
WH-SYMMAX datasets. It is worth noting that if the weighted loss
CRediTauthorshipcontributionstatement
functionandtheregressiontaskarenotused(i.e.,Ours1andOurs2),
theperformanceontheSK-SMALLdatasetisslightlyworsethanthatof
the RCFmethod. If only onelevel and theweighted loss functionare XixiYuan:Investigation,Methodology,Writingâ€“review&editing.
used without regression tasks (i.e., Ours1A), the performance on the Youqing Xiao: Conceptualization, Software, Methodology, Writing â€“
SK-LARGEdatasetis3.05percentagepoints,whichislowerthanthat originaldraft.ZhanchuanCai:Supervision,Writingâ€“review,Funding.
oftheRCFmethod. LeimingWu:Investigation,Writingâ€“review&editing.
The PR curves in Fig. 10 show that the proposed method is much
better than the HED and RCF methods. Specifically, Ours2AR has the
Declarationofcompetinginterest
best results on SK-LARGE and WH-SYMMAX datasets. The Ours2A
is slightly better than the Ours2AR on SK-SMALL dataset, with the
The authors declare that they have no known competing finan-
F-measurevaluesof0.715and0.711,respectively.
cial interests or personal relationships that could have appeared to
SometypicalsalientobjectcontourextractionresultsonSK-LARGE
influencetheworkreportedinthispaper.
dataset are shown in Fig. 11. From Fig. 11, it can be found that the
extracted results of HED and DFI methods contain some non-contour
detailsandbackgroundinformation.Inaddition,theresultsextracted Dataavailability
bytheRCFmethodlosesomecontourinformation.Whiletheproposed
methodavoidstheseproblemsandachievesbetterresults.
Datawillbemadeavailableonrequest.
5. Conclusion
Acknowledgments
Inthispaper,asalientobjectcontourextractionmethodisproposed
by fusing scale information on hierarchically convolutional network. This work was supported in part by the Science and Technol-
In the method, a new network structure with regression task and ogy Development Fund of Macau under Grant 0059/2020/A2, Grant
hierarchical feature integration mechanism is established, which has 0052/2020/AFJ, and Grant 0038/2020/A, in part by the National
good performance when extracting salient object contours from nat- Science Foundation of China under Grant 62101346, in part by the
ural images; a new variable coefficient loss function is proposed to GuangdongBasicandAppliedBasicResearchFoundationunderGrant
handletheunevenlydistributedpixelclassesinmachinelearning.The 2021A1515011702, and in part by the Stable Support Plan for Shen-
experimental results show that the proposed method is superior to zhenHigherEducationInstitutionsunderGrant20200812104316001.
7X.Yuanetal. Array17(2023)100270
References [17] Liu JJ, Hou QB, Cheng MM. Dynamic feature integration for simultaneous
detection of salient object, edge, and skeleton. IEEE Trans Image Process
[1] GuptaA,AnpalaganA,GuanL,KhwajaAS.Deeplearningforobjectdetection 2020;29:8652â€“67.
and scene perception in self-driving cars: Survey, challenges, and open issues. [18] ShenW,ZhaoK,JiangY,WangY,ZhangZ,BaiX.Objectskeletonextraction
Array2021;10:100057.http://dx.doi.org/10.1016/j.array.2021.100057. innaturalimagesbyfusingscale-associateddeepsideoutputs.In:Proceedings
[2] LiJ,FanJ,ZhangZ.Towardsnoiselessobjectcontoursforweaklysupervised oftheIEEEconferenceoncomputervisionandpatternrecognition.LasVegas,
semantic segmentation. In: Proceedings of the IEEE Conference on Computer NV,USA;2016,p.222â€“30.
VisionandPatternRecognition.NewOrleans,Louisiana;2022,p.16856â€“65. [19] Shen W, Zhao K, Jiang Y, Wang Y, Bai X, Yuille A. DeepSkeleton: Learning
[3] Kazuma F, Kazuhiko K. Generative and self-supervised domain adaptation for multi-task scale-associated deep side outputs for object skeleton extraction in
one-stageobjectdetection.Array2021;11:100071. naturalimages.IEEETransImageProcess2017;26(11):5298â€“311.
[4] Gabriel L, Nasim H, Irene C. Semi-supervised learning approach for local- [20] ShenW,BaiX,HuZ,ZhangZ.Multipleinstancesubspacelearningviapartial
ization and pose estimation of texture-less objects in cluttered scenes. Array randomprojectiontreeforlocalreflectionsymmetryinnaturalimages.Pattern
2022;100247. Recognit2016;52:306â€“16.
[5] ChengM,MitraNJ,HuangX,TorrPH,HuS.Globalcontrastbasedsalientregion [21] RampunA,LÃ³pezLinaresK,MorrowPJ,ScotneyBW,WangH.Breastpectoral
detection.IEEETransPatternAnalMachIntell2015;37(3):569â€“82. musclesegmentationinmammogramsusingamodifiedholistically-nestededge
[6] Gong XY, Su H, Xu D, Zhang ZT, Shen F, Yang HB. An overview of contour detectionnetwork.MedImageAnal2019;57:1â€“17.
detectionapproaches.IntJAutomComput2018;15(6):656â€“72. [22] Liu Y, Yang X, Wang Z, Lu C, Li Z, Yang F. Aquaculture area extraction
[7] Martin DR, Fowlkes CC, Malik J. Learning to detect natural image boundaries andvulnerabilityassessmentinSanduaobasedonricherconvolutionalfeatures
using local brightness, color, and texture cues. IEEE Trans Pattern Anal Mach networkmodel.JOceanolLimnol2019;37(6):1941â€“54.
Intell2004;26(5):530â€“49. [23] ManinisKK,Pont-TusetJ,ArbelÃ¡ezP,GoolLV.Convolutionalorientedbound-
[8] Martin DR, Fowlkes CC, Malik J. Learning to detect natural image boundaries aries: From image segmentation to high-level tasks. IEEE Trans Pattern Anal
using local brightness, color, and texture cues. IEEE Trans Pattern Anal Mach MachIntell2018;40(4):819â€“33.
Intell2004;26(5):530â€“49.http://dx.doi.org/10.1109/TPAMI.2004.1273918. [24] YangJ,PriceB,CohenS,LeeH,YangMH.Objectcontourdetectionwithafully
[9] RishiJ,ACD.Preservingboundariesforimagetexturesegmentationusinggrey convolutionalencoder-decodernetwork.In:ProceedingsoftheIEEEconference
levelco-occurringprobabilities.PatternRecognit2006;39(2):234â€“45. oncomputervisionandpatternrecognition.LasVegas,Nevada,USA;2016,p.
[10] Xie S, Tu Z. Holistically-nested edge detection. Int J Comput Vis 2017;125(1â€“ 193â€“202.
3):3â€“18. [25] Li G, Xie Y, Lin L, Yu Y. Instance-level salient object segmentation. In:
[11] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
imagerecognition.2014,arXivpreprintarXiv:1409.1556. Honolulu,HI,USA;2017,p.2386â€“95.
[12] LiuY,ChengM,HuX,BianJ,ZhangL,BaiX,etal.Richerconvolutionalfeatures [26] QinX,ZhangZ,HuangC,GaoC,DehghanM,JagersandM.BASNet:Boundary-
foredgedetection.IEEETransPatternAnalMachIntell2018;41(8):1939â€“46. aware salient object detection. In: Proceedings of the IEEE conference on
[13] DollÃ¡r P, Zitnick CL. Fast edge detection using structured forests. IEEE Trans computervisionandpatternrecognition.LongBeach,CA;2019,p.7479â€“89.
PatternAnalMachIntell2015;37(8):1558â€“70. [27] ZhaoK,ShenW,GaoS,LiD,ChengM.Hi-Fi:hierarchicalfeatureintegration
[14] Zhao JX, Liu JJ, Fan DP, Cao Y, Yang JF, Cheng MM. EGNet: Edge guidance forskeletondetection.In:Proceedingsofthe27thinternationaljointconference
network for Salient object detection. In: Proceedings of the IEEE international onartificialintelligence.Stockholm,Sweden:AAAIPress;2018,p.1191â€“7.
conferenceoncomputervision.Seoul,Korea;2019,p.8778â€“87. [28] Liu W, Wen Y, Yu Z, Yang M. Large-margin softmax loss for convolutional
[15] YuZ,FengC,LiuMY,RamalingamS.CASENet:Deepcategory-awaresemantic neural networks. In: Proceedings of the International conference on machine
edgedetection.In:ProceedingsoftheIEEEconferenceoncomputervisionand learning.NewYork,NY,USA:ProceedingsofMachineLearningResearch;2016,
patternrecognition.Honolulu,Hawaii;2017,p.5964â€“73. p.507â€“16.
[16] Zhu C, Yan W, Liu S, Li T, Li G. Salient contour-aware based twice learning [29] NeubeckA,VanGoolL.Efficientnon-maximumsuppression.In:Proceedingsof
strategy for saliency detection. In: Proceedings of the IEEE/CVF international the 18th international conference on pattern recognition, Vol. 3. Hong Kong,
conferenceoncomputervisionworkshops.Seoul,Korea;2019,p.2541â€“8. China:IEEE;2006,p.850â€“5.
8