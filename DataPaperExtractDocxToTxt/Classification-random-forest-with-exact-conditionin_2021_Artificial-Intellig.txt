Artificial Intelligence in Geosciences 2 (2021) 82–95

		




Classification random forest with exact conditioning for spatial prediction of categorical variables
Francky Fouedjio
AngloGold Ashanti Australia Ltd., Growth and Exploration, 140 St. Georges Terrace, Perth, WA, 6000, Australia



A R T I C L E I N F O

Keywords: Categorical variable Classification
Exact conditioning
Principal component analysis Signed distance
Spatial prediction Quadratic programming
A B S T R A C T

Machine learning methods are increasingly used for spatially predicting a categorical target variable when spatially exhaustive predictor variables are available within the study region. Even though these methods exhibit competitive spatial prediction performance, they do not exactly honor the categorical target variable's observed values at sampling locations by construction. On the other side, competitor geostatistical methods perfectly match the categorical target variable's observed values at sampling locations by essence. In many geoscience applica- tions, it is often desirable to perfectly match the observed values of the categorical target variable at sampling locations, especially when the categorical target variable's measurements can be reasonably considered error-free. This paper addresses the problem of exact conditioning of machine learning methods for the spatial prediction of categorical variables. It introduces a classification random forest-based approach in which the categorical target variable is exactly conditioned to the data, thus having the exact conditioning property like competitor geo- statistical methods. The proposed method extends a previous work dedicated to continuous target variables by using an implicit representation of the categorical target variable. The basic idea consists of transforming the ensemble of classification tree predictors' (categorical) resulting from the traditional classification random forest into an ensemble of signed distances (continuous) associated with each category of the categorical target variable. Then, an orthogonal representation of the ensemble of signed distances is created through the principal component analysis, thus allowing to reformulate the exact conditioning problem as a system of linear inequalities on principal component scores. Then, the sampling of new principal component scores ensuring the data's exact conditioning is performed via randomized quadratic programming. The resulting conditional signed distances are turned out into an ensemble of categorical outputs, which perfectly honor the categorical target variable's observed values at sampling locations. Then, the majority vote is used to aggregate the ensemble of categorical outputs. The effectiveness of the proposed method is illustrated on a simulated dataset for which ground-truth is available and showcased on a real-world dataset, including geochemical data. A comparison with geostatistical and traditional machine learning methods show that the proposed technique can perfectly match the categorical target variable's observed values at sampling locations while maintaining competitive out-of-sample predictive performance.





Introduction

The spatial prediction of a categorical target variable when auxiliary spatial information is available everywhere within the region under study has become ubiquitous in geosciences. Typical examples include pre- dicting land use classes, land cover categories, drainage classes, vegetation species, landslide types, rock types, soil types, lithofacies, hydrofacies, and geological units. The mapping of categorical variables plays an essential role in a wide variety of geoscience applications. It is used to aid in risk- aware decision-making in many areas, such as environmental studies
and natural resource management. Various methods have been proposed for spatially predicting categorical target variables when spatially exhaustive predictor variables are available within the study region. These methods include geostatistical methods (Goovaerts, 2001; Hengl et al., 2004, Hengl et al., 2007), generalized linear mixed models-based ap- proaches (Cao et al., 2011, Cao et al., 2014), and classification machine learning techniques (Kanevski, 2008; Kanevski et al., 2009; Hengl et al., 2018; Maxwell et al., 2018; Du et al., 2020; Giaccone et al., 2021).
Geostatistical methods for spatially predicting a categorical variable in the presence of auxiliary spatial information available everywhere



E-mail addresses: ffouedjio@anglogoldashanti.com, francky.fouedjio@gmail.com. https://doi.org/10.1016/j.aiig.2021.11.003
Received 13 August 2021; Received in revised form 25 October 2021; Accepted 30 November 2021
Available online 11 December 2021
2666-5441/© 2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. Signed distance transform approach - (a) categorical spatial variable with two categories; (b) signed distance function associated with category 0; (c) signed distance function associated with category 1.



Table 1
Simulated data example - simulation parameters.


within the region under study include indicator kriging with external drift (IKED), regression-kriging of indicators (RKI), and regression- kriging of memberships (RKfM). Indicator kriging with external drift (IKED) assumes that auxiliary variables are linearly related to the class occurrence of the categorical target variable (Goovaerts, 2001). The auxiliary variables are incorporated into the indicator kriging system as deterministic linear functions. Its implementation is challenging since it is often problematic to simultaneously estimate the parameters of external drift and the covariance function of the stochastic component. Regression-kriging of indicators (RKI) combines multinomial logistic regression of the categorical target variable on predictor variables with kriging of the regression residuals (Hengl et al., 2004, Hengl et al., 2007). Thus, the regression modeling is supplemented with the modeling of variograms for regression residuals, which are then interpolated and added back to the regression estimate. RKI has been adapted to regression-kriging of memberships (RKfM) by substituting crisp indicator values with continuous membership values (Hengl et al., 2007). Indeed,
under the RKI method, the interpolation of residuals might lead to values outside the physical range (< 0 or > 1). Although easy to implement, these indicator kriging-based methods have some well-known short- comings. The predicted probabilities of the target variable's categories
are not guaranteed to belong to the [0, 1] interval and sum up to one. Therefore, post-processing methods of the predicted probabilities are required (Bogaert, 2004; Allard et al., 2011). Also, under these methods, the outcome values of the conditional cumulative distribution function may not be monotonic. A posterior correction of the resulting conditional probabilities is often necessary either through a Gaussian transformation or via a logistic regression model (Pardo-Igúzquiza and Dowd, 2005).
Another alternative for spatially predicting a categorical response variable in the context of spatially exhaustive auxiliary information available within the spatial domain of interest consists of using gener- alized linear mixed models-based approaches. Cao et al. (2011) propose a spatial multinomial logistic mixed model in which spatially correlated latent variables are assumed to account for the spatial dependency in the categorical target variable. The proposed model is represented as a
multinomial logistic function of spatial covariances between target and sampling locations. The sought-after class occurrence probability func- tion for a target location is written as a multinomial logistic linear combination of covariance values between the target and source data locations, which can be analogous to the dual form of kriging methods. This method was later extended to incorporate heterogeneous auxiliary information for spatial prediction of categorical variables (Cao et al., 2014). These generalized linear mixed models-based approaches are free of the aforementioned inherent problems of indicator kriging-based methods. However, they are computationally intensive compared to in- dicator kriging-based methods.
Machine learning techniques are increasingly used for spatially pre- dicting a categorical response variable when auxiliary information is available everywhere within the study region. Indeed, the number of predictor variables that help explain the spatial variation in the target variable has grown dramatically, making other methods cumbersome to apply. Also, some machine learning methods are well-known for handling complex non-linear relationships and interactions and require less data pre-processing. Classification machine learning methods have proven relevant for spatially predicting categorical variables in many research works, including Albrecht et al. (2021); Kumar et al. (2020); Hengl et al. (2018); Latifovic et al. (2018); Kuhn et al. (2018); Sahoo and Jha (2017); Othman and Gloaguen (2017); Cracknell and Reading (2015, 2014); Yu et al. (2012). Even though classification machine learning methods (e.g., random forest, support vector machines) exhibit competitive spatial prediction performance, they do not exactly honor the categorical target variable's observed values at sampling locations by construction. On the other side, competitor geostatistical techniques (such as regression-kriging of indicators) perfectly match the categorical target variable's observed values at sampling locations by essence. In many geoscience applications, it is desirable to perfectly match the observed values of the categorical target variable at sampling locations, especially when the categorical target variable's measurements can be reasonably considered error-free (hard data).
This work addresses the problem of exact conditioning of machine learning methods for the spatial prediction of categorical variables. It in- troduces a classification random forest-based method in which the cate- gorical target variable is exactly conditioned to the data, thus having the exact conditioning property like competitor geostatistical methods. The random forest popularity for spatial prediction relies on its ability to effi- ciently deal with many predictor variables, handle complex nonlinear re- lationships and interactions, and require less data pre-processing, and be a non-parametric method. The proposed approach extends a previous work dedicated to continuous target variables by using an implicit representation of the categorical target variable (Fouedjio, 2020). The exact conditioning to the data is achieved through a step-by-step approach. First, classification




Fig. 2. Simulated data example - (a), (b), (c), (d) predictor variables, (e) exhaustive categorical target variable, and (f) sampled categorical target variable.


tree predictors' ensemble (categorical) resulting from the traditional clas- sification random forest is transformed into an ensemble of signed distances (continuous) corresponding to each category of the categorical target var- iable. Second, an orthogonal representation of the ensemble of signed dis- tances is created through the principal component analysis. Third, the exact conditioning problem is reformulated as a system of linear inequalities on principal component scores, thus allowing the sampling of new principal component scores (via the randomized quadratic programming), ensuring the exact conditioning to the data. Fourth, the resulting conditional signed distances are turned out into an ensemble of categorical outputs, which perfectly honor the categorical target variable's observed values at sampling
locations. Finally, the majority vote is used to aggregate the ensemble of categorical outputs. The final output also matches the target variable's observed values at sampling locations by construction. On the one hand, the proposed method's effectiveness is illustrated on a simulated dataset for which the ground truth is available. On the other hand, the proposed technique is exhibited on a real-world dataset comprising geochemicaldata. A comparison is also made with geostatistical and classical machine learning methods (regression-kriging of indicators, random forest, and support vector machines).
The remainder of the paper is structured as follows. Section 2 de- scribes the different ingredients required to apply the proposed method.




Fig. 3. Simulated data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machines; (c) cat- egorical target variable’ observed values at training locations. The misclassification rate in the training data is 18.20% and 35.00%, respectively, for the traditional random forest and support vector machines.


Fig. 4. Simulated data example - B ¼ 10 000 unconditional first PC scores and T ¼ 1000 conditional first PC scores.




Fig. 5. Simulated data example - prediction map for (a) traditional classification random forest, (b) support vector machines, (c) regression-kriging of indicators, and
(d) classification random forest with exact conditioning.


Section 3 demonstrates the proposed approach's effectiveness on a syn- thetic dataset as well as a real-world dataset. A comparison with geo- statistical and classical machine learning methods is provided. Section 4 offers concluding remarks.

Methodology

Let {C(s): s ∈ D} be the categorical target variable defined on a fixed
continuous spatial domain of interest D⊂Rd, with a finite set of possible categorical outputs (categories) {c1, …, cK} which are mutually exclusive
and collectively exhaustive. There exist n categorical target variable's observed values {C(si)}i=1;…;n (hard data) corresponding to sampling locations {si ∈ D}i=1;…;n . In addition to the categorical target variable, there is a set of predictor variables {x1(s), …, xp(s): s ∈ D} exhaustively known in the spatial domain D. We address the problem of predicting the categorical target variable over the spatial domain D represented by N grid locations using the categorical target variable's observed values and predictor variables' data. In addition, the categorical target variable's predicted values at sampling locations must be the same as the categor- ical target variable's observed values at sampling locations, i.e., C^(si) = 
C(si); i = 1; …; n. The description of the different ingredients needed to
implement the proposed exact conditioning method is given in this sec- tion. The implementation is carried out in the R platform (R Core Team, 2020).
Random forest classifier

The first step of the proposed method consists of training the tradi- tional random forest (RF) classifier on the data. Random forest classifier is an ensemble method where several individual decision trees are trained on various subsets of the training dataset (bootstrap samples) using different subsets of available predictor variables, followed by an aggregation (Breiman, 2001). The bootstrapping of the training data and the random selection of subsets of predictor variables ensure that each decision tree in the random forest is unique, which reduces the overall variance of the random forest classifier. For the final decision, the RF classifier aggregates the decisions of individual trees through a voting scheme such as the majority voting, i.e., for each observation, each de- cision tree votes for one category, and RF chooses the category with the highest number of votes.
Classification random forest has some tuning parameters that can be optimized. There are, among others, the number of trees, number of predictor variables randomly selected at each node, proportion of ob- servations to sample in each decision tree, and minimum number of observations in a decision tree's terminal node. These hyperparameters are optimized via cross-validation. In practice, there is no need to tune the number of decision trees; it is usually recommended to set it to a large number, allowing the convergence of the prediction error to a stable minimum (Hengl et al., 2018). The implementation of the classification random forest is carried out using the R packages ranger (Wright and




Fig. 6. Simulated data example - prediction uncertainty (entropy) map for (a) traditional classification random forest, (b) support vector machines, (c) regression- kriging of indicators, and (d) classification random forest with exact conditioning.


Table 2
Simulated data example - predictive performance statistics in the testing dataset containing 39 500 observations.

an ensemble of unconditional signed distance functions (continuous) corresponding to each category of the categorical target variable. The categorical target variable {C(s): s ∈ D} with K categories {ck}k=1;…;K can







Ziegler, 2017) and tuneRanger (Probst et al., 2018).
The training of the classical random forest classifier results in an ensemble of classification tree predictors {C~b (s) : s ∈ D}b=1;…;B , where B is the number of decision trees. As the traditional random forest classifier is not explicitly designed to match the data perfectly, classification tree predictors and the aggregated classification tree predictors do not necessarily match the categorical target variable's observed values at sampling locations. Since {C~b(s) : s ∈ D}b=1;…;B do not match data perfectly, they will be called “unconditional classification tree pre- dictors”. The next steps aim to generate conditional classification tree predictors that perfectly fit the categorical target variable's observed values at sampling locations.

Signed distance transform

The second step of the proposed method includes transforming the unconditional classification tree predictors’ ensemble (categorical) into
be viewed as a variable that creates distinct boundaries in the study re- gion D. Each category ck(k = 1, …, K) can be codified by a binary variable indicating their presence or absence: Ik(s) = 1 if C(s) = ck, and Ik(s) = 0 if C(s) /= ck, 6s ∈ D. Each category ck can be represented by a signed dis-
tance function φk(⋅) such that ck = {s ∈ D, φk(s) ≤ 0}. The signed distance
transform approach (Grevera, 2007; Davies, 2012) can be used to
transform each category ck ({Ik(s): s ∈ D}) into a signed distance function
φk(⋅). Indeed, each category ck defines a p-dimensional binary image
{Ik(s): s ∈ D} where each point (pixel) has either a value of 1 indicating
the presence of the category ck or a value of 0 indicating the absence of the category ck. For every point (pixel) set to 1, a distance transform assigns a value indicating the negatively signed distance from that point (pixel) to the nearest point (pixel) set to 0. Similarly, for every point (pixel) set to 0, a distance transform assigns a value indicating the positively signed distance from that point (pixel) to the nearest point (pixel) set to 1. An illustration of the signed distance transform method is given in Fig. 1. Additionally, the signed distance transformation is one-to-one. The bijectivity is obtained using the following rule:
C(s)= argmin(φ1 (s); …; φK (s)); 6s ∈ D.	(1)
c1 ;…;cK

Thus, the ensemble of unconditional classification tree predictors
{C~b (s) : s ∈ D}b=1;…;B is converted to an ensemble of unconditional




Fig. 7. Real-world data example - some predictor variables: (a) elevation, (b) Landsat 8 band 6, (c) gravity survey high-pass filtered Bouguer anomaly, (d) potassium counts from gamma ray spectrometry.


Fig. 8. Real-world data example - (a) categorical target variable and (b) training and testing locations.


signed distance functions φ~b(s) : s ∈ D b=1;…;B for each category ck(k
= 1, …, K) via the signed distance transform approach previously described. The following idea uses principal component analysis to create an orthogonal representation of the ensemble of unconditional

φ~b(s) : s ∈ D b=1;…;B (k = 1; …; K). This results in the following decomposition in finite dimensions:
φ~b(s)= XL  αb ψ (s); 6s ∈ D; b = 1; …; B; k = 1; …; K;	(2)

reformulated as a linear inequality problem on the principal compo- nent scores. Then, new principal component scores that ensure the exact conditioning to the hard data are generated through the ran- domized quadratic programming. Since the principal component orthogonalization is bijective, conditional signed distance functions are obtained by reconstruction. The combination rule defined in Eq. (1) is applied to obtain conditional classification tree predictors (categorical) that exactly match the categorical target variable's observed values at sampling locations.

Principal component analysis

This step consists of performing principal component analysis (PCA) on  each  ensemble  of  unconditional  signed  distance  functions
where {αbk }l=1;…;L are principal component scores (coefficients) and
{ψ l;k (s) : s ∈ D}l=1;…;L are principal components factors (eigen-functions);
L = min(B, N).
For each category ck(k = 1, …, K), PCA is applied to a matrix Γk(B × N) arranged as a set of B row vectors, each representing a single un- conditional signed distance function φ~b(s) : s ∈ D . PCA is paralleliz-
able for each matrix Γk(k = 1, …, K). In Eq. (2), the ensemble
φ~b(s) : s ∈ D b=1;…;B can be viewed as a set of images and φ~b(s) : s ∈ D
as an image. Thus, the resulting principal components factors
{ψ l;k (s) : s ∈ D}l=1;…;L are images as well. Hence, Eq. (2) provides a decomposition of the images into a set of eigen-images and a set of co- efficients. It is important to note that in the PCA framework, the eigen- functions are considered fixed, while the coefficients are considered



Fig. 9. Real-world data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machines; (c) categorical target variable’ observed values at training locations.	


random. The PCA is used here more as an orthogonal decomposition method than a dimension reduction technique since all the principal component factors are kept, as shown in Eq. (2). The bijective nature of PCA allows the reconstruction of signed distance functions from co- efficients. In other words, an image can be reconstructed back once all the principal component factors and scores are used.
category c2 should be negative at location s1 (φ2(s1)≤ 0), and the signed distance functions associated with other categories should be positive at location s1 (φk (s1)≥ 0; 6k /= 2; k = 1;…;K). For each φk (k = 1;…;K), the
conditioning to all data locations is expressed by the following inequalities:
8<φ˘k(s1)= θ1;kψ1;k(s1)+ θ2;kψ2;k(s1)+ ⋯ + θL;kψ L;1(s1)≤ 0 or ≥ 0

Randomized quadratic programming
…
:φ˘k (sn)= θ1;kψ1;k (sn)+ θ2;kψ2;k (sn)+ ⋯ + θL;kψ L;k (sn)≤ 0 or ≥ 0
.	(4)

Given the PCA decomposition of the ensemble of unconditional signed distance functions as described in Sect. 2.3, this step consists of generating new principal component scores such that signed distance functions defined in Eq. (2) perfectly match the data. Let
XL
In Eq. (4), the n inequalities corresponding to n hard data can be summarized as:
Ψ~ kθk ≤ 0; k = 1; …; K.	(5)
New PC scores vector θk = (θ1;k; …; θL;k)T that matches data are



where {θl;k}
are random coefficients and {ψ l (s) : s ∈ D}
are
problem (Fouedjio et al., 2021a):

l=1;…;L	;k	l=1;…;L

principal component factors derived from the PCA decomposition of unconditional signed distance functions as given in Eq. (2).
The categorical target variable's observed values at sampling loca- tions (hard data) inform the sign of the signed distance function associ-
min (θk — βk )T Σ—1(θk — βk )	subject to
Ψ~ kθk ≤ 0; k = 1; …; K.

(6)

ated with a category at sampling locations. Thus, the set of hard data can be converted into a set of inequality constraints using Eq. (3). Let assume that at the sampling location s1, the category c2 is observed, i.e., C(s1) = c2. This means that the signed distance function associated with the
where βk ~ N (μk ; Σk), and the mean μk and the covariance matrix Σk are
computed using unconditional PC scores {αbk }l=1;…;L derived from the PCA of unconditional signed distance functions given in Eq. (2). Specifically,





μ = "1 XB
αb #

1
; Σ =

XB  (αb — μ )(αb — μ T
with αb = hαb i

.	(7)








Fig. 10. Real-world data example - unconditional and conditional first two PC scores associated with each category.


For each sample βt ~ N (μk ;Σk)(t = 1;…; T), quadratic programming (Goldfarb and Idnani, 1983) is performed to find a solution θt that sat- isfies the inequality constraints and minimizes the quadratic objective
(Fouedjio et al., 2021b). However, this approach can be time-consuming for very large datasets since Gibbs samples are highly correlated.
Given conditional PC scores {θt }t=1;…;T , conditional signed distance

function in Eq. (6). The covariance matrix Σk in Eq. (7) is a diagonal matrix because the PC scores are uncorrelated by construction. Co- efficients θt can be also generated via the Gibbs sampling method
functions are obtained by reconstruction:
φt (s)= XL  θt ψ (s); 6s ∈ D.	(8)




Fig. 11. Real-world data example - prediction map provided by (a) traditional classification random forest, (b) support vector machines, and (c) regression-kriging of indicators, and (d) classification random forest with exact conditioning.


Fig. 12. Real-world data example - prediction uncertainty (entropy) map provided by (a) traditional classification random forest, (b) support vector machines, (c) regression-kriging of indicators, and (d) classification random forest with exact conditioning.


Conditional classification tree predictors are given by applying the combination rule defined in Eq. (1):
Ct(s)= argmin φt (s); …; φt (s) ; 6s ∈ D; t = 1; …; T.	(9)

























Since all the individual reconstructed signed distance functions

predictors B under randomized quadratic programming. That is to say, T
can be smaller or greater than B.
To summarize, the proposed classification random forest with exact conditioning is performed using the following pseudo algorithm:































Empirical examples

{φt (s) : s ∈ D}t

=1;…;T
(k = 1; …; K) perfectly match the hard data, condi-

tional classification tree predictors {Ct (s) : s ∈ D}t

=1;…;T
do also. The ag-
The proposed classification random forest with exact conditioning is

gregation of the conditional classification tree predictors using the majority vote rule leads to the final outcome {C^(s) : s ∈ D}. This latter coincides with the categorical target variable observed values at sam- pling locations.
The number of unconditional classification tree predictors B should be large enough to allow good coverage of the solution space when performing the exact conditioning. Indeed, the number of categorical target variable’ observations defines the number of inequalities con- straints as shown in Eq. (4). The larger is the number of unconditional classification tree predictors, the wider is the solution space of Eq. (6). So, too many constraints (hard data) relative to too few unconditional clas- sification tree predictors will lead to too small uncertainty. It is worth mentioning that the number of conditional classification tree predictors T does not depend on the number of unconditional classification tree


Table 3
Real-world data example - predictive performance statistics in the testing dataset containing 140 observations.
illustrated using simulated and real-world datasets. A prediction perfor- mance comparison is carried out with a geostatistical method (regres- sion-kriging of indicators) and traditional machine learning techniques (random forest and support vector machines). Hyper-parameters associ- ated with each machine learning method have been optimized through cross-validation. The proposed classification random forest with exact conditioning uses the same ensemble of decision trees generated by the classical random forest.
The predictive performance of each method is assessed on a testing dataset using the first evaluation statistic, i.e., the accuracy. The accuracy corresponds to the percentage of observations that are correctly classi- fied. It has a value between 0 and 1. The higher is the accuracy, the better is the model. In addition to the accuracy, the Rand index is calculated. The Rand index measures the similarity between the predicted classifi- cation and true classification on the testing data by considering all pairs of points and counting pairs that are assigned in the same or different category in the predicted and true classifications. The Rand index has a value between 0 and 1, with 0 indicating that the two classifications do not agree on any pair of points and 1 indicating the same classifications.

Simulated data example

In this simulated case study, we consider a categorical target variable with four categories, and four continuous predictor variables defined


over the spatial domain [0,100]2. The categorical target variable is generated according to the following model:
6s ∈ [0; 100]2;  C(s)= ck  if  Y(s)∈ [qk; qk 1[;  k = 1; …; 4;  (10)

with Y(s) = 50sin(X1(s)) + 3X1(s)X2(s)+ X3(s)2 + 50sin(X4(s)) + ϵ(s); X1(⋅), X2(⋅), X3(⋅), and X4(⋅) are predictor variables; and ϵ(⋅) is a latent (non-observed) variable; the limits qj's are taken as the 0, 0.25, 0.50, 0.75, and 1 quantiles of the random function Y(⋅), so that the complete system of events ck (k = 1, …, 4) can be defined.
The four predictor variables and the latent variable are simulated on the spatial domain [0,100]2 based on five independent Gaussian isotropic stationary random functions (Chiles and Delfiner, 2012) with different specification of means and covariance functions as given in Table 1. The simulation is performing using the R package RGeostats package (Renard et al., 2020). Fig. 2 presents the simulated data over a 200 × 200 regular grid. The map of the categorical target variable dis- played in Fig. 2e is considered as the reference map.
To demonstrate the proposed method's ability to exactly match the categorical target variable's observed values at sampling locations, n = 500 stratified random samples are taken as the training data (Fig. 2). The set of n = 500 stratified random samples amounts to 1.25% of total lo- cations in the reference map and each category contains 125 samples. The rest of data (39 500 samples) is kept aside for the testing. The goal is to reconstruct the reference map of the categorical target variable (Fig. 2e) using the sampled categories (Fig. 2f) with an aid of the observed four spatial auxiliary variables (Fig. 2a - d) such that the cate- gorical target variable's predicted values coincide with the categorical target variable's observed values at sampling locations.
Fig. 3 shows the categorical target variable's observed values at training locations and those predicted by the traditional random forest and support vector machines. There is a significant disagreement be- tween the observed values and the predicted values of categorical target variable at training locations. The misclassification rate in the training data is 18.20% and 35.00%, respectively, for the traditional random forest and support vector machines. For the traditional random forest, the number of decisions trees has been set to 10 000, and the hyper- parameters have been optimized through cross-validation. Concerning the support vector machines, the kernel function and the hyper- parameters have been selected using cross-validation.
The traditional classification random forest is performed on the training data with a large number of decision trees set to B = 10 000. Thus, an ensemble of B = 10 000 unconditional classification tree pre- dictors {C~b (s) : s ∈ [0; 100]2}b 1 10 000 is constructed. This latter ensemble is transformed into an ensemble of unconditional signed dis-
tance functions  φ~b(s) : s ∈ [0; 100]2 b=1;…;10 000 for each category ck(k
= 1, …, 4), according to the methodology described in Sect. 2. Principal component analysis is performed on this ensemble, followed by the sampling of new PC scores ensuring the data's exact conditioning. T =
1000 new PC scores are generated, thus giving an ensemble of T = 1000

expressed as a system of linear inequalities.
Fig. 5 presents prediction maps provided by the traditional classifi- cation random forest, support vector machines, regression-kriging of indicators, and the proposed classification random forest with exact conditioning. One can notice the prediction maps provided by regression- kriging of indicators (Fig. 5c) and the proposed classification random forest (Fig. 5d) are exactly conditioned to the training data (Fig. 2f), which is not the case for the ones provided by traditional classification random forest (Fig. 5a) and support vector machines (Fig. 5b). The general appearance of prediction maps resulting from the traditional classification random forest and the proposed classification random forest with exact conditioning looks similar. However, there are some local differences due to the exact conditioning nature of the proposed method. It is important to highlight that the proposed classification random forest with exact conditioning uses the ensemble of decision trees generated by the classical random forest as starting point. Overall, the prediction map of the proposed classification random forest exhibits more similar spatial patterns present in the reference map (Fig. 2e) than the regression-kriging of indicators.
The traditional classification random forest, support vector machines, regression-kriging of indicators, and proposed classification random forest with exact conditioning provide the probabilities for each possible outcome of the categorical target variable at any spatial location. Thus, the prediction uncertainty can be quantified through information en- tropy (Wellmann and Regenauer-Lieb, 2012). Fig. 6 presents the pre- diction uncertainty maps associated with each method. The regression-kriging of indicators and the proposed classification random forest provide zero entropy at sampling location by construction while the traditional classification random forest and support vector machines do not. The traditional classification random forest, support vector ma- chines, and proposed classification random forest provide lower entropy in local neighborhoods dominated by a single category compared to the regression-kriging of indicators.
The predictive performance statistics in the testing dataset (contain- ing 39 500 observations) for the traditional classification random forest, support vector machines, regression-kriging of indicators, and proposed classification random forest with exact conditioning are reported in Table 2. In addition to exactly fitting the categorical target variable's observed values at sampling locations, the proposed classification random forest maintains a competitive out-of-sample predictive performance.

Real-world data example

In this real case study, the categorical target variable is Tl (Thallium) geochemical concentration transformed into five categories through quantiles and observed at 568 locations over the study region in south- west England (Kirkwood et al., 2016). Predictor variables include elevation, gravity, magnetic, Landsat, radiometric, and their derivatives, totaling 26 predictor variables. Some predictor variables are displayed in
Fig. 7. Fig. 8a shows the categorical target variables's observations. The

conditional signed distance functions n t (s) : s ∈ [0; 100]2o	(k	observations are partitioned into a training set (
75%) and testing set

= 1, …, 4). This latter are turned into conditional classification tree

predictors {Ct (s) : s ∈ [0; 100]2}t

=1;…;1000
that perfectly match the cate-
categories have roughly the same number of observations.
Fig. 9 shows the categorical target variable's observed values at

gorical target variable's observed values at sampling locations. The ma- jority vote scheme is then used to derive the predicted categorical target variable {C^(s) : s ∈ [0; 100]2}. This latter also perfectly matches the cat- egorical target variable's observed values at sampling locations by construction.
Fig. 4 shows PC scores before the conditioning (unconditional PC scores {αl }l=1;…;10 000) and after the conditioning (conditional PC scores
{θt }t=1;…;1000 ) for each category ck(k = 1, …, 4). One can notice that the
points cloud of conditional PC scores is less scattered than those from unconditional PC scores due effectively to the exact conditioning
training locations and those predicted by the classical random forest and support vector machines. There is a considerable disagreement between the observed values and the predicted values of categorical target vari- able at training locations. The misclassification rate in the training data is 33.80% and 52.58%, respectively, for the classical random forest and support vector machines. For the classical random forest, the number of decisions trees has been set to 10 000, and the hyper-parameters have been optimized through cross-validation. Regarding the support vector machines, the kernel function and the hyper-parameters have been selected using cross-validation.



The learned conventional random forest model consists of an ensemble of B = 10 000 classification tree predictors. This latter ensemble is transformed into an ensemble of unconditional signed dis- tance functions for each category, following by PCA and randomized quadratic programming as described in the methodology section (Sect. 2). This results to unconditional and conditional PC scores as shown in Fig. 10; T = 1000 conditional PC scores are generated. As mentioned in the simulated data example, the points cloud of conditional PC scores is less spread out than those from unconditional PC scores because of the exact conditioning.
Prediction maps provided by the traditional classification random forest, support vector machines, regression-kriging of indicators, and proposed classification random forest with exact conditioning are depicted in Fig. 11. The prediction map provided by each method differs notably. In particular, the prediction map of the proposed classification random forest is different from the one provided by the traditional classification random forest. This is explained by the exact conditioning nature of the proposed method. It is important to highlight that the proposed classification random forest with exact conditioning uses the same ensemble of decision trees generated by the traditional classifica- tion random forest. Although the regression-kriging of indicators pro- vides exact conditioning, its prediction map shows a noisier spatial distribution of categories (Fig. 11c). In contrast, the prediction map provided by the proposed classification random forest with exacting conditioning depicts more regular and continuous contours (Fig. 11d) and is consistent with the training data shown in Fig. 9c.
Fig. 12 presents the prediction uncertainty (entropy) map under the traditional classification random forest, support vector machines, regression-kriging of indicators, and proposed classification random forest with exact conditioning. The prediction uncertainty map resulting from the proposed classification random forest with exact conditioning differs significantly from the others. In particular, the prediction uncer- tainty map provided by the traditional and proposed classification random forest differ notably due to the exact conditioning in the pro- posed method and not in the conventional method. Under the proposed classification random forest, local neighborhoods dominated by only one category show lower entropy than local neighborhoods dominated by several categories of the target variable.
Table 3 provides the predictive performance of the traditional clas- sification random forest, support vector machines, regression-kriging of indicators, and proposed classification random forest with exact condi- tioning in the testing dataset (containing 140 observations). The pro- posed classification random forest shows better predictive performance than the three other methods according to the accuracy. The cost of non- using the proposed classification random forest in this case is not negli- gible. There is an accuracy improvement of 25% and 9% respectively, compared to the regression-kriging of indicators and traditional classi- fication random forest. Thus, the proposed approach can exactly match the categorical target variable's observed values at sampling locations while achieving good out of sample predictive performance.

Conclusion

This paper proposed a classification random forest-based method for the spatial prediction of categorical variables in which the categorical target variable is exactly conditioned to the data. The exact conditioning means that the predicted values of the categorical target variable at sampling locations are the same as those observed at sampling locations. This property is well-known in geostatistical methods. The proposed method combines classification random forest, signed distance functions, principal component analysis, and randomized quadratic programming to achieve the exact conditioning of the categorical target variable to the data. The effectiveness of the proposed method has been demonstrated on simulated and real datasets.
Typical characteristics of the proposed method are the following. It can perfectly match the categorical target variable's observed values at
sampling locations while achieving good out-of-sample predictive per- formance compared to competitor geostatistical methods such as regression-kriging of indicators. It is easy to implement since it combines well-known existing statistical and machine learning methods. It can easily handle a large number of categories consistently through the signed distance representation. The proposed method can provide real- istic prediction uncertainties of the categorical target variable. It has the advantage of not producing noisy spatial prediction maps, as one can observe for regression-kriging of indicators. The proposed method is free of the inherent problems of regression-kriging of indicators such as the predicted probabilities of the target variable's categories that are not guaranteed to belong to the [0, 1] interval and sum up to one. Updating the categorical target variable predictive map when few observations are added can be carried out quickly. Only the last part of the proposed method, i.e., the randomized quadratic programming, should be performed.
The proposed method is computationally intensive compared to regression-kriging of indicators and conventional classification random forest. However, it comprises components that can be performed in parallel according to the target variables' categories, including condi- tional principal component scores generation. The proposed method re- quires that the number of unconditional classification tree predictors should be large enough to allow good coverage of the solution space when performing the exact conditioning. Indeed, the number of cate- gorical target variable’ observations defines the number of inequalities constraints. The larger is the number of unconditional classification tree predictors, the wide is the solution space for the exact conditioning. So, too many constraints (hard data) relative to too few unconditional clas- sification tree predictors will lead to too small uncertainty. Nonetheless, it will always be possible to meet this constraint because the number of unconditional classification tree predictors is a free parameter. Although the proposed method uses the random forest as the base learner, it can be used with other ensemble machine learning methods (e.g., boosting).

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

References

Albrecht, T., Gonz´alez-A´lvarez, I., Klump, J., 2021. Using machine learning to map Western Australian landscapes for mineral exploration. ISPRS Int. J. Geo-Inf. 10.
Allard, D., D’Or, D., Froidevaux, R., 2011. An efficient maximum entropy approach for categorical variable prediction. Eur. J. Soil Sci. 62, 381–393.
Bogaert, P., 2004. Spatial prediction of categorical variables: the bme approach. In: Sanchez-Vila, X., Carrera, J., Go´mez-Hern´andez, J.J. (Eds.), geoENV IV — Geostatistics for Environmental Applications. Springer Netherlands, Dordrecht,
pp. 271–282.
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.
Cao, G., Kyriakidis, P., Goodchild, M., 2011. A multinomial logistic mixed model for prediction of categorical spatial data. Int. J. Geogr. Inf. Sci. 25, 2071–2086.
Cao, G., Yoo, E.H., Wang, S., 2014. A statistical framework of data fusion for spatial prediction of categorical variables. Stoch. Environ. Res. Risk Assess. 28, 1785–1799.
Chiles, J.P., Delfiner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley &
Sons.
Cracknell, M., Reading, A., 2015. Spatial-contextual supervised classifiers explored: a challenging example of lithostratigraphy classification. IEEE J. Select. Topics Appl. Earth Observ. Remote Sens. 8, 1–14.
Cracknell, M.J., Reading, A.M., 2014. Geological mapping using remote sensing data: a comparison of five machine learning algorithms, their response to variations in the spatial distribution of training data and the use of explicit spatial information.
Comput. Geosci. 63, 22–33.
Davies, E., 2012. Chapter 9 - binary shape analysis. In: Davies, E. (Ed.), Computer and Machine Vision, fourth ed. Academic Press, Boston, pp. 229–265.
Du, P., Bai, X., Tan, K., Xue, Z., Samat, A., Xia, J., Li, E., Su, H., Liu, W., 2020. Advances of four machine learning methods for spatial data handling: a review. J. Geovisual.
Spatial Anal. 4.
Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.
Artifi. Intel. Geosci. 1, 11–23.
Fouedjio, F., Scheidt, C., Yang, L., Achtziger-Zupanˇciˇc, P., Caers, J., 2021a.
A geostatistical implicit modeling framework for uncertainty quantification of 3D



geo-domain boundaries: application to lithological domains from a porphyry copper deposit. Comput. Geosci. 157, 104931.
Fouedjio, F., Scheidt, C., Yang, L., Wang, Y., Caers, J., 2021b. Conditional simulation of categorical spatial variables using Gibbs sampling of a truncated multivariate normal distribution subject to linear inequality constraints. Stoch. Environ. Res. Risk Assess. 35, 457–480.
Giaccone, E., Oriani, F., Tonini, M., Lambiel, C., Marie´thoz, G., 2021. Using data-driven
algorithms for semi-automated geomorphological mapping. Stoch. Environ. Res. Risk Assess. 1–17.
Goldfarb, D., Idnani, A., 1983. A numerically stable dual method for solving strictly convex quadratic programs. Math. Program. 27, 1–33.
Goovaerts, P., 2001. Geostatistical modelling of uncertainty in soil science. Geoderma 103, 3–26.
Grevera, G.J., 2007. Distance Transform Algorithms and Their Implementation and Evaluation. Springer New York, New York, NY, pp. 33–60.
Hengl, T., Heuvelink, G.B., Stein, A., 2004. A generic framework for spatial prediction of soil variables based on regression-kriging. Geoderma 120, 75–93.
Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr€aler, B., 2018. Random forest as a
generic framework for predictive modeling of spatial and spatio-temporal variables. PeerJ 6, e5518.
Hengl, T., Toomanian, N., Reuter, H.I., Malakouti, M.J., 2007. Methods to interpolate soil categorical variables from profile observations: lessons from Iran. Geoderma 140, 417–427. Pedometrics 2005.
Kanevski, M., 2008. Advanced Mapping of Environmental Data: Geostatistics, Machine Learning and Bayesian Maximum Entropy. John Wiley & Sons.
Kanevski, M., Pozdnoukhov, A., Timonin, V., 2009. Machine Learning for Spatial Environmental Data: Theory, Applications, and Software. EPFL press.
Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016. Stream sediment geochemistry as a tool for enhancing geological understanding: an overview of new data from south west England. J. Geochem. Explor. 163, 28–40.
Kuhn, S., Cracknell, M.J., Reading, A.M., 2018. Lithologic mapping using random forests applied to geophysical and remote-sensing data: a demonstration study from the Eastern Goldfields of Australia. Geophysics 83, B183–B193.
Kumar, C., Chatterjee, S., Oommen, T., Guha, A., 2020. Automated lithological mapping by integrating spectral enhancement techniques and machine learning algorithms using aviris-ng hyperspectral data in gold-bearing granite-greenstone rocks in Hutti, India. Int. J. Appl. Earth Obs. Geoinf. 86, 102006.
Latifovic, R., Pouliot, D., Campbell, J., 2018. Assessment of convolution neural networks for surficial geology mapping in the South Rae geological region, Northwest territories, Canada. Rem. Sens. 10.
Maxwell, A.E., Warner, T.A., Fang, F., 2018. Implementation of machine-learning classification in remote sensing: an applied review. Int. J. Rem. Sens. 39, 2784–2817.
Othman, A.A., Gloaguen, R., 2017. Integration of spectral, spatial and morphometric data into lithological mapping: a comparison of different machine learning algorithms in the Kurdistan region, NE Iraq. J. Asian Earth Sci. 146, 90–102.
Pardo-Igúzquiza, E., Dowd, P.A., 2005. Multiple indicator cokriging with application to optimal sampling for environmental monitoring. Comput. Geosci. 31, 1–13.
Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and tuning strategies for random forest. Wiley Interdiscipl. Rev.: Data Min. Knowl. Discov. https://doi.org/ 10.1002/widm.1301.
R Core Team, 2020. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. URL. https://www.R-project.o rg/.
Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. RGeostats: geostatistical package. URL: http://cg.ensmp.fr/rgeostats. r package version 12.0.1.
Sahoo, S., Jha, M.K., 2017. Pattern recognition in lithology classification: modeling using neural networks, self-organizing maps and genetic algorithms. Hydrogeol. J. 25, 311–330.
Wellmann, J.F., Regenauer-Lieb, K., 2012. Uncertainties Have a Meaning: Information Entropy as a Quality Measure for 3-D Geological Models. Tectonophysics, vols.
526–529. Modelling in Geosciences, pp. 207–216.
Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for high dimensional data in C++ and R. J. Stat. Software 77, 1–17.
Yu, L., Porwal, A., Holden, E.J., Dentith, M.C., 2012. Towards automatic lithological classification from remote sensing data using support vector machines. Comput. Geosci. 45, 229–239.
