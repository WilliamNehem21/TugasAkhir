Available online at www.sciencedirect.com



AASRI Procedia 3 (2012) 148 – 154




2012 AASRI Conference on Modelling, Identification and Control
Study and Realization of Large Rotation Angle Matching Images Algorithm Based on Charateristic Matching
YongSong Zhu*
School of Science, Hubei University of Technology, Wuhan, Hubei,China, 430068



Abstract

Combining the advantages of SIFT charateristic matching and pyramid image matching, this paper aims to solve the matching problem of automatic rotation with large rotation angle in aerial photographing. To start with, match the images at the highest pyramid level with SIFT characteristic matching method. Then use the RANSAC algorithm to combine the results from SIFT method. Before matching, predict the initial location of the matching points by using the estimated rotation angle and then make rotation compensation of the matching window images. Last, validate the method by carrying out an aerial digital imagery with large rotation of a certain area.

© 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and/or peer review under responsibility of American Applied Science Research Institute

Keywords:Image matching,Large rotation angle,SIFT feature matching,Automation;


Introduction

Matching images with large rotation angle has not gained much attention in the field of aerial photogrammetry, but it is vastly investigated in the computer vision and medial image matching. Ullah proposes a direction code based matching method, but it needs to know approximate value of rotation angle, and the speed is somewhat slow. Tasi uses the ring-projection transformation to overcome the rotation effect, but there are relatively large matching bias in results. Suzuki proposes a method that firstly extracts edge


* * Corresponding author. Tel.: +86-013707129143.
E-mail address:zoyoso@yahoo.com.cn..








2212-6716 © 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and/or peer review under responsibility of American Applied Science Research Institute doi:10.1016/j.aasri.2012.11.025


using Canny edge detector and then performs Hough transform on the resulting binary image to estimate rotation angle. Xu Jianbin takes the Zernike moments as the similarity measure, and introduces genetic algorithm to match remote sensing images. But on the whole, the above mentioned methods are not suitable for the aerial image matching due to lack of universality and robustness. In recent years, a new SIFT feature operator has been proposed, which can effectively adapt itself to a variety of image transformation, maintaining invariance to the image rotation and scaling the brightness changes. But it has the shortcoming of low match success rate and high computing time, so directly applied to the aerial image matching, it will be time-consuming and lack of uniformity of point distribution.
Therefore, SIFT feature matching and NCC based pyramid matching method are comprehensively utilized, a matching method based on SIFT feature matching and rotation compensation is proposed in this article. The following will firstly introduce the basic idea of the method, then focus on the key strategies, and finally analyze the experiment results to verify this method's effectiveness.
Matching process
Firstly, image pyramid is generated for each images. Secondly, the rotation angles between the reference image and the searching images are estimated using the SIFT feature matching results. Thirdly, feature points are uniformly extracted by Förstner operator in the overlapping area, and the NCC image matching algorithm is followed to search conjugate points. The difference between this process and the traditional one lies in that the rotation angle is introduced in the matching process to make correction to the conjugate point' approximate position and the grey information of matching window. Also, the relative orientation with connection condition is incorporated into each level of pyramid image matching to remove the remaining wrong matches
Matching algorithm design
The SIFT feature extraction and matching is only performed in the highest pyramid image level. It does not attempt to find plenty of accurate conjugate point pairs. Instead, the SIFT feature matching is to establish a coarse coordinate relationship between the images with large rotation angle. With the strategy in this paper, the shortcoming of large computation burden and low speed of SIFT feature matching will be reduced to a large degree, as the image in the highest pyramid level is usually small, needing comparatively little computation.
The SIFT feature for each image is extracted by the method proposed by the Lowe D.G[16]. There are four major steps for Lowe's method:
Scale space extreme detection: the local extrame in both the DoG (Difference of Gaussian) scale space and the 2D image plane are detected as the keypoints, as Fig. 1 depicted.


Scale




Fig.1 feature point detection in DoG scale space
DoG


Keypoints localization: the location of the keypoint is determined by 3D quadratic function, which provides a substantial improvement of keypoint stability and elimination of false edge response.
Orientation assignment: according to the local image gradient direction, a consistent orientation is assigned to each keypoint. By doing this, the keypoint has the ability of invariant to the image rotation.
Keypoint descriptors: the SIFT descriptor is computed by partitioning the scale space image region surrounding the keypoint into 4×4 grid of subregion, and in each subregion an orientation histogram of 8 bins within [0,2π) interval is computed. So a normalized 128 component vector is formed as the keypoint descriptor.
After these four steps, the keypoints are considered as the feature points, and the keypoints descriptors as the feature to be used. And the Euclidean distance between feature vectors is taken as the similarity measure. If the ratio of the nearest distance to the less nearest distance is less than the given threshold (0.7 in this paper), feature vector with the nearest distance is accepted as the conjugate point.

Image Rotation Angle Robust Estimation

To estimate the rotation angle between the images, it primarily contains the following two steps: firstly, fitting the SIFT feature matching results with the affine transform model, and secondly taking advantage of the coefficients to compute the rotation angle. So the key to the robust estimation is how to accurately calculate the transform coefficients in the condition that there may be large wrong matching results in the SIFT feature matching result.
In this paper, the RANSAC based affine transformation algorithm is adopted. The details to carry it out are as follow:
Set the sample number k to be infinite and the counting number t to be zero.
Three point pairs are randomly selected from the SIFT feature matching results, and using them to compute the six affine transformation coefficients.
The discrepancies between the remaining candidate points' coordinates and those derived by the affine transformation model are calculated for each SIFT feature pair. If the discrepancy is less than the given threshold, the corresponding SIFT feature pair is regarded as the correct matching, that is the interior point pair; otherwise it is regarded as the wrong matching, that is the outlier pair. And finally the ratio of outlier point pairs to the total point pairs  is computed.
Sample number k is computed by the formula k    log(1)   , where  is set 0.99 in this paper;
log(1 (1  3 )t )
Counting number is increased by one, t  t  1 .
When k  t , the iteration is stopped, otherwise back to the first step to loop.
After the iteration termination, the set with largest number of interior point pairs is selected as the best.
The affine transform coefficients are recalculate by using the interior point pairs in the best set, and they are taken as the final results.
After obtaining the affine transform coefficients a0 , a1 , a2 ,b0 , b1 ,b2 , the rotation angle	between images can be calculated using following formula:

  0.5  (arctan(a2 / a1 )  arctan(b1 / b2 ))
(1)


NCC Based Pyramid image matching with rotation compensation

The problems created by the large rotation angle to the traditional template image matching are mainly two aspects: one is wrong prediction of initial point position of the conjugate point, and the other is the distortion of the image in matching window.


For the prediction of the conjugate points' initial position, at the highest pyramid image level, affine transformation coefficients are directly used to calculate the coordinate of initial position. At other pyramid image levels, for the correct matching point at higher pyramid level, just projecting it to the current pyramid level will be all right. But for those points fail to obtain the correct matching points, the neighboring successfully matched points' parallax information is taken advantage to derive its initial position. It should be noted that the effect of rotation angle on the parallax should be considered during the initial position prediction.

Assuming
p1 (x1 , y1 ) is the point whose conjugate point's position needed to be calculated, and p2 (x2 , y2 )

is the nearest successful matching point, whose conjugate point is p2 (x2 , y2 ) , (dx, dy) is the corresponding

parallax between
p2 and
p2 . Then initial position (x1, y1) of point
x1  x2  dx cos  dy sin
y1  y2  dx sin  dy cos
p1 's conjugate point is:

(2)

Where  is the estimated rotation angel between these two images.
Then, plus the coordinates of the origin point A, the corresponding row and column number in the image of the pixel in searching window is determined. Finally, by traversing, the coordinates in the image for each pixel in the searching window are obtained, and by bilinear interpolation the rotation compensated searching window image is formed.
After that, the NCC is used as the similarity measure to search for conjugate point under the epipolar line constraint. This can not only speed up the matching, but also enhance the reliability of matching results.

Elimination of Wrong Matching Points

Due to the disadvantage factors, such as geometric distortion and repetitive texture, there are maybe several peaks when using NCC as the similarity measure to search conjugate points. At the same time, the point corresponding to the highest peak is not necessarily the truth, so certain constraints should be introduced in order to eliminate the wrong matching results. And in this paper, the relative orientation with model conjunction condition is utilized, through integrating the iteration weighting strategy, the matching point with wrong vertical and horizontal parallax can be automatically identified and deleted.
The basic principle of the algorithm lies in that the conjugate points' ray should be intersected in the three dimensional object spaces. For each point, which appears in the left, middle and right images, the following couple of error equation will be formed:

vQ  ad  bd  cd  rdu  sd  lQ
vP  ad  bd  cd  sd  tdm  lP
The details of each parameter can refer to.
(3)

If there are n points, the 2n equations can be listed, but there are just 6 unknown parameters, they can be solved by least squares adjustment method.
In the process, the iterative weighing strategy[18] is introduced to detect and eliminate the wrong matching results. The principle that the iterative weighing strategy based lies in the posterior variance estimation theory. Firstly, the statistics value Ti of the observed values is calculated:
v2

Ti    i   (i  1, 2, 3,⋯, n)
ˆ 2r
(4)

where vi is the residual of the ith observation, ri is its corresponding redundant observation component, ˆ0 is the unit weight mean error, and n is the total number of the observations. The threshold for the vertical and horizontal parallax is set to 1 and 1.5 times pixel size of the current pyramid image level.


Matching algorithm realization and analysis
Introduction of Experiment Data
The test data is a set of gray valued aerial digital images covering the Badaling area in the city of Beijing. Due to the large overlap up to 88%, the rotation angle between these large overlapping images is relatively small, and cause little affect on the traditional matching algorithm. But after analyzing the results of image bundle adjustment, it has been found that the orientation angle of some images are comparatively large, and the maximum value for phi, omega and kappa angle is 8° 8° 24° respectively. The main reason for these large orientation angles is that the photograph is taken by the small helicopter, which is influenced a lot by the air at the exposure. To evaluate the matching method proposed in this paper, two triples of aerial images are chosen, in each group large rotation angle exists.
Experimental Results and Aanalysis
For each group, the middle image is taken as the reference image, in the area overlapping by all three images about 1000 feature points are extracted for matching. The searching window size is set to 35×35 pixels, matching window size is set to 13×13 pixels, and the threshold for NCC is 0.6. The matching result is shown in fig. 2.

image 28	image 30	image 26
the matching results of first group



image63	image 61	image 65
the matching results of second group Fig. 2 Matching Results of Method in This Paper
In order to examine efficiency and match success rate of the method, the traditional pyramid image matching based on NCC similarity measure is utilized to match the same feature points according to the identical parameters. The matching results of two methods are listed in Table 1.
Table 1. The matching results of two methods

Note: NFP denotes the number of total feature points to match, NSMP denotes successfully matched points; MSR denotes match success rate, and CCT is the CPU cost time.
Two sets of test images covering the suburbs both have rich texture. However, due to air effect to the non- sealed small aircraft taking photograph at low altitude, the rotation angle is large for some images. For group of image 61-63-65, the  is 12.7°  19.2° and 1.0° respectively, and for group of image 30-28-26, the  is
2.1° -5.7° and 1°. It can be seen that, for the first group, the largest rotation angel is between the middle and right image, that is 18.2°, and for the second group, 7.8°.When large rotation angle between matching images exists, for the traditional matching method, using initial parallax to predict initial position of conjugate points at highest pyramid level will swing away from its true position. Also for matching at other pyramid levels, it will fail to make use of neighbor successful matched point's parallax to predict the initial position of conjugate points for failure matching points at higher levels, and image rotation will lead to calculated value of NCC below the threshold. These will finally cause that the traditional method does not have a high match success rate, even no match out points at all. The greater the rotation angle is, the more serious this phenomenon will be, which contributes to the facts that the second group image has better results than the first group image for matching with traditional method. By the method in this paper, through SIFT feature


matching at highest level, the rotation angel between images can be accurately estimated, which is then used to carry out the initial position prediction and image window compensation. These will guarantee the accurate initial position and the validity of correlation coefficient calculation, and as a result, the match success rate is enhanced. Figure 3b is feature points of some area in image 63, figure 3a is the conjugate points matched by the traditional method, and figure 5c is the matching results of our method. ○ denotes the successful matching points, × the wrong matching points.
		
a) traditional method	b) feature point in image 63	c) method in this paper Fig. 3..Detail View of Matching Results for 63-65 Image Pair in First Group Image
Conclusions

This paper aims to solve the matching problem of automatic rotation with large rotation angle in aerial photographing. To start with, match the images at the highest pyramid level with SIFT characteristic matching method. Then use the RANSAC algorithm to combine the results from SIFT method. Before matching, predict the initial location of the matching points by using the estimated rotation angle and then make rotation compensation of the matching window images. Last, validate the method by carrying out an experiment on an aerial digital imagery with large rotation of a certain area.

References

[1]Y.Zhang, “Research and Application of the Vertical Lines Supported Aerial Triangulation,” Ph.D. thesis,
Wuhan University, 2006
[2]F. Ullah, S. Kaneko, “Using Orientation Codes for Rotation Invariant Template Matching,” Pattern Recognition, vol. 37, pp. 201-209, 2004
[3]D.M. Tsai, C.H. Chiang, “”Rotation-invariant Pattern Matching Using Wavelet Decomposition,” Pattern Recognition Letters, vol. 23, pp. 191-201, 2002
[4]J.B. Xu, W. Hong, Y.R Wu, “A Remote Sensing Images Matching Method Based on Zernike Moments and Steady Genetic Algorithms,” Journal of Electonics and Information Technology, vol. 27, pp. 925-927
