Available online at www.sciencedirect.com



AASRI Procedia 3 (2012) 707 – 714




2012 AASRI Conference on Modelling, Identification and Control
Monocular VisionNavigation and Control ofMobile Robot
Runchen Yana,*, Hong Wanga,Yuzhi Yangb, Huanbing Weib and Yonggang Wangb
aState Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
bThe Hongfeng Co., Hubei, Xiaogan, 432000. China



Abstract

In this paper, we implement a monocular vision navigation system on a mobile robot which can travelin known outdoor environment. The systemuses5 different sensors on the robotwhich are monocular color camera, laser radar, sonar, gyroscopes and optical encoder.And the robot cancruise independently in known continuous structured and unstructured
road environment mainly through the vision navigation module. There are road recognition module,signs
recognitionmodule, stop line recognitionmodule, zebra crossing recognitionmodule, obstacle sensing module, control module and communication module.The system utilizes multi-threading technology to collaborate different modules in the same control cycle and automatic state machine to switch different control modes.Experimental results show that the robotcan accomplishautonomous cruise task on the predetermined routeand operate stablywhen it is in a good lighting conditions.


© 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and/or peer review under responsibility of American Applied Science Research Institute

Keywords: vision navigation, mobile robot, monocular






* Corresponding author. Tel.: +86-13141259359.
E-mail address:chen_0_1987@sina.com.






2212-6716 © 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and/or peer review under responsibility of American Applied Science Research Institute doi:10.1016/j.aasri.2012.11.112


Introduction

Compared to intelligent vehicles,small intelligent mobile robotics has many different characteristics,such as low image stability, low control precision,different waysof turning,narrow angle of shooting due tolow camera height.Therefore, itneeds higher accuracy ofenvironmental awareness, collaboration of multi-sensor and self- control technology. In application of environment exploring and cruise task, small intelligent mobile robot canplay a better role.
In this paper, the autonomous vision navigation system is developed onthe small mobile robot platform, Adept MobileRobots PT 3-AT.Our vision navigation robot is designed for known outdoors routes.It can travel along the edges of the unstructured roads and the lane lines of the structured roads. It will turn when recognizing a special signs or zebra crossing.Section 2 introduces the design framework of the whole system.Section 3 gives a brief introduction of visionrecognitionmodule.Section 4elaborates the control module and strategy.Experiment results are given in section 5.And the conclusion and future work are discussed in section 6.


Fig. 1. Adept MobileRobots PT 3-AT

Design andFramework of Whole System

The whole system is divided into two parts, the upper machine client and lower machine host. In the paper itmainly introduces the system in lower machine host.


Fig. 2. The state transition simplified diagramof the FSM system


In the system,there are many states which compose a finite state machine (FSM)[1]. In state of straight traveling, turning and road finding,the system uses different well-designed strategies and rules to control the robot behaviorbased according to the road number.Manual control mode is automatically entered when system go into the stop state. In that state,people can send commands to control the robot how to walkthrough the upper machine client.

VisionRecognition Module

We designed two different road-edge detection algorithms for unstructured road[2]. One is color-based threshold segmentation algorithm,to deal with the situation of high color contrastbetween road and non-road area.The other is a grayscale-based extension algorithm of Otsu[3], to deal with the situation that the road and non-road area boundaries are not obvious.Wedesigned another color-based threshold segmentation algorithm for structured road, to recognize the lane lines.Afterrecognizing correct traveling path and calculating by inverse affine transformation, the algorithm above will give final results, the distance deviation and the angle deviation at the current position.
The recognition algorithms for stop lines and zebra crossings are similar to lane detection algorithm. They are used to control the behavior of the robot stopped and specific sections turn.
In order to improve the accuracy of the turning andadapt to the complex environment, we specifically designed ahigh-contrastsign for our vision navigation system, whichis composed ofyellow and black. The signs are placed in fixed positions of the turning area and maintain the same height to mark the accurate turning point. By using the HSV color threshold segmentation, edge detection, RANSAC[4] ellipse fitting and misrecognition filtering, the recognition algorithm can identify the turning signs outdoors. And it will give a turning signal to system when the detected signs in images reach a predetermined size. Thenthe system will go into the turning state from a straight traveling state at the predetermined turning points.

Control Module and Strategies

Since onlyusing monocular camera, the vision algorithmsmaymakeslight deviations when estimating distance. It will lead continuous adjusted in corrective processof straight travelling.The robot outdoors walking and turning in differential waywill makegreat influence on image stabilitybecause of the friction between wheels and ground. Moreover,false detectionsof vision recognition algorithmsoften appearin complex outdoor environment. All these factors present challenges of robotcontrol strategy design to us.

The definition of distance deviation and angle deviation

The robot has its own global coordinate system.When the robot starts, itis on the origin of coordinates and face to the x-axis positive direction, 90degrees counterclockwise is the y-axis positive direction. The range of angleis 0 to 180degreesin the first and the second quadrant and-180 to 0degreesin the third and fourth quadrant. In short, in the positive x direction, it is 0 degree. Clockwise is negative and counterclockwise is positive. As the same way, the angle deviation, which is defined as the angle betweencorrect direction and current direction, will in range of 0 to 180 degrees clockwise, and range of -180 to 0degrees counterclockwise.
The distance deviation is defined as vertical distancebetween the center point of the robot and the correct path line. When facing the correct direction, the distance deviation of the robot in the correct path line is 0. The left side is positive and the right side is negative.


The strategies of straight travelling and correction

In the design, we introduce a fuzzy controller to suppress the excessive response of the system to complex noise and keepthe stability and accuracy of traveling straight.Fuzzy control is a control system to control the complex system which is difficult to bedescribed by accurate mathematical model[5].
We firstly designeda group of simple linear control equations.


𝜔= −𝛼×𝛩−𝛽×ð
{	𝛾
𝑣 = 𝑣𝑚𝑎𝑥 − |𝑎× 𝜃| − |𝑏× ð|

(1)

Where 𝜔 is angular velocity(degrees/s), 𝑣 is linear velocity(mm/s),  is distance deviation,  is angle deviation,𝑣𝑚𝑎𝑥 = 1000, α  β  𝗒  a  b are adjustable parameters.
Of course, the equations aboveare far to meet demand of stable controlling.So we improve them inthe experiments. Linear velocityremains linear. Angular velocityis controlled by a rule-based fuzzy controller in only part of the situations. Fuzzy controller structure is shown asfollows:


Fig. 3. The structure of fuzzy controller

We obtain the distance deviation and angle deviation advantage from the vision algorithms and pose estimation, which are referredin chapter 2.And the two parameters are used as the input fuzzy parameters.The fuzzy sets of them both are {NB, NM, NS, PS, PM, PB}.
The figure below presents the fuzzy membership of distance deviation.Horizontal axisrepresents the distance deviation of robot (mm). And the vertical axis represents the value of membership.


Fig. 4. The fuzzy membership of distance deviation

The fuzzy membership of angle deviation is shown below. Horizontal axis represents the angle deviation of robot (degrees). And the vertical axis represents the value of membership.




Fig. 5. The fuzzy membership of angle deviation

The fuzzy membership of angular velocity is shown as following. Horizontal axis represents the angular velocity of robot (degrees/s).The vertical axis represents the value of membership. Whenangular velocityis greater than 11degrees/s or less than -11degrees/s, the value of fuzzymembership is 0. These cases are out of the range of the fuzzy controller. Because ofrobot control accuracy itself, it canjustutilizethe discrete formulation for defuzzification calculation.


Fig. 6. The fuzzy membership of angular velocity

Considering the robot travel behaviors features when human controlling, we summarized the following fuzzy control rules. The fuzzy inference synthesize rule is maximum - minimum rule. And defuzzification directly utilizes the max criterion method (MC)[6]. The blank means that angular velocity is calculated by using the original linear equation in these situations.

Table 1. fuzzy control rules

		
NB	NM	NS	PS	PM	PB




After defuzzification, we add the following additional rule:(|θ| > θm AND ω×θ > 0) → ω = 0. This rule can limitangular velocity when correction of distance deviation,in order to eliminatethe excessive rotation phenomenon. In experiments, we set 𝜃𝑚 = 10.
Inertial navigation correction assistance

The visionnavigation module requires certain conditions so that it can work well.However, only using the inertial navigation sensor will cause accumulated errors. Sofor stable travelling, it is essential thatusingoptical encoder and gyroscope toassist the vision navigation.
As we said above, when optical encoder and gyroscope is on, our system has a global coordinate system. Through the vision information in the past and the location, orientation of robot at that time,we can calculate the equation of correct path line in global coordinate system.
According to the vision data, angle deviation θ which is accepted by system, and the orientation angle th in coordinate system at that time, we can obtain the slope of the correct path line: k = tan (th − θ). And we can also get the angle between correct orientation and the x-axis positive direction:

𝑡ℎ − 𝜃 − 360(𝑡ℎ − 𝜃 > 180) finalth = {𝑡ℎ − 𝜃(−180 ≤ 𝑡ℎ − 𝜃 ≤ 180)(2)
𝑡ℎ − 𝜃 + 360(𝑡ℎ − 𝜃 < −180)

According to the coordinates (𝑥1, 𝑦1) at that time and distance deviation  which is provided by visionnavigation, we can figure out the coordinates of a point on correct path line:

{𝑥0 = 𝑥1 +ð × 𝑠𝑖𝑛 (𝑓𝑖𝑛𝑎𝑙𝑡ℎ)(3)
𝑦0 = 𝑦1 −ð × 𝑐𝑜𝑠 (𝑓𝑖𝑛𝑎𝑙𝑡ℎ)


Then, we obtain the equation of the line as below:


𝑘𝑥 − 𝑦 − 𝑘𝑥0 + 𝑦0 = 0(4)


According to current coordinates(𝑥𝑛, 𝑦𝑛), we can use the equation above to get the new distance deviation
:



ð = { 
−(k𝑥𝑛−𝑦𝑛−k𝑥0+𝑦0) (−90 < 𝑓𝑖𝑛𝑎𝑙𝑡ℎ < 90)
√k2+1

(5)

𝑛	k𝑥𝑛−𝑦𝑛−k𝑥0+𝑦0 (𝑓𝑖𝑛𝑎𝑙𝑡ℎ ≥ 90, 𝑓𝑖𝑛𝑎𝑙𝑡ℎ ≤ −90)
√k2+1
There  are  two  special  cases:  When finalth = 90 ,  .  When finalth = −90 ,

The new angle deviation  can be calculatedaccording to finalth which represents the angle between correct path line and the x-axis. Here nowth represents the current angle of robot in global coordinate system.

𝑛𝑜w𝑡ℎ − 𝑓𝑖𝑛𝑎𝑙𝑡ℎ − 360(𝑛𝑜w𝑡ℎ − 𝑓𝑖𝑛𝑎𝑙𝑡ℎ > 180)
𝜃𝑛 = {𝑛𝑜w𝑡ℎ − 𝑓𝑖𝑛𝑎𝑙𝑡ℎ(−180 ≤ 𝑛𝑜w𝑡ℎ − 𝑓𝑖𝑛𝑎𝑙𝑡ℎ ≤ 180)(6)
𝑛𝑜w𝑡ℎ − 𝑓𝑖𝑛𝑎𝑙𝑡ℎ + 360(𝑛𝑜w𝑡ℎ − 𝑓𝑛𝑎𝑙𝑡ℎ < −180)


The calculated results,  and  will be put into the control strategyas inputparameters to control straight travelling of robot.
In every cycle,the system will compared the deviation data from inertial navigationwith the one fromvision navigation. If the difference between the two data is obvious, that is to say, distance deviationgreater than 800mm or angle deviation more than 17degrees with distance deviation less than 343mm, the system will accept the data frominertial navigation; otherwise use the vision navigation data.The above-mentioned values can be adjusted.

Turning, path-finding and collision avoidance strategies

Because of vibration caused by friction when the robot is turning, the image recognition algorithms tend to result more false detections or misses.Wedonot focus ondesigning a precise solution to turning recognition andcontrolling. So wejust usethe signs which referred in Chapter 3 to mark the fixed turning point. Thenwe set turning angle and distance before and after turning in every turn,which are estimatedfrom experiments.
The path-finding strategies strategy is applied after turning, at the beginning at each road. Every time after turning, the robot will search along the current direction in an  45degrees angle range. The correct direction should deviate from the current direction within a certain angular.
The laser radar and sonar sensors in the system are only for detecting an obstacle and avoiding collision by sending stop command when an obstacle is detected in front of the robot within  45degrees angular range as well as the distance between the obstacle and the robot is less than presetsafe distance.

Experiment results

We used the robot simulator to do the ideal test which compared the control effect before and after adding fuzzy controller.The distance deviation and angle deviation are given completely correctfor each control cycleand start correction at same distance deviation. From the ideal result, we can find a decrease in the oscillation of correction and overshooting. The whole correction process became relatively steady. In the practical experiments, these advantages play a very important role to enhance the stability and accuracy and prevent the robot from significantS-shape walking.


Fig. 7. (a) linear control in correction; (b) After add fuzzy controller in correction

We hold a two-month practical adjustment and experiment on 6 predeterminedcontinuousroads in a natural environment. The overall distance of the 6 roads is over 360m. Two of them are structural road where the robot walked along the center lane lines of the roads and turned at every zebra crossing. The other four roads are not structural roads where the robot went along the curb and turn at every sign. The monocular color camera is Sony EVI-D100P. The control cycle period is set to 1s and a = 1, β = 0.2, γ = 8, a = 15, b = 0.05 in the linear equations. The average success rate on every road is over 95%, with over 98% in fourof the six. The total success rate of continuous walking on the six roads is more than 80%, which reach the expectation for this experiment.




Fig. 8. (a) Lane recognition on structured road; (b) road edge recognition in unstructuredroad; (c) Some scene and results in experiments

Conclusion and Future Work

This research combines the technology of computer vision, pattern recognition, fuzzy control, etc. and realizes an intellectual visual navigation on a small robot platform. Based on the limited states, it manages the traveling condition of the robot, collaborating simultaneously others modules and underwent an experiment on six roads of known conditions. The experiments show that fuzzy controller and inertia navigation can well reduce the instability of the vision navigation. And mainly through well-designed vision navigation,small mobile robot can accomplish the task of automatic cruising in good lighting conditions on both structured and unstructured outdoor roads.
In the process of walking straight, the occasional corrections of the robot present continuous oscillations. Reasons are: 1) Although the fuzzy controller will stable the angular velocity at a certain value, the blurred image during turning will still cause an increase of errors in the visual navigation; 2) The ground fiction of the four wheels of the robot are different. We should enhance the tolerance for this situation in the controlling strategy or develop new algorithms in terms of image stability. Also we should do more research for precise turning strategy in the future. Finally,we will gradually try to run our vision navigation and control system in unknown environment.


Reference
A. Haasch, N. Hofemann, J. Fritsch, and G. Sagere. Cavallaro, A multi-modal object attention system for a mobile robot, Intelligent Robots and Systems, 2005, 2712-2717.
Yunpeng Zhao, Hong Wang, Runchen Yan, Unstructured Road Edge Detection and Initial Position Approach Based on Monocular Vision, AASRI Conference on Computational Intelligence and Bioinformatics, 2012.
Nobuyuki Otsu . A threshold selection method from gray-level histograms. IEEE Trans. Sys., Man., Cyber. 1979, 9 (1): 62–66.
Fischler, M.A. and Bolles, R.C. Random Sample Consensus. A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications of the ACM, 1981, 24(6): 381–395.
Mamdani, E. H., Application of fuzzy algorithms for the control of simple dynamic plant. Electrical Engineers, Proceedings of the Institution, 1974, 1585-1588.
[6]J. L. Castro,Fuzzy logic controllers are universal approximators, IEEE Transactions on Systems, Man and Cybernetics, 1995, 25(4):629-635.
