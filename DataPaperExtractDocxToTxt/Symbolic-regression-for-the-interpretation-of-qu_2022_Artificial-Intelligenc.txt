Artificial Intelligence in the Life Sciences 2 (2022) 100046

		


Research Article
Symbolic regression for the interpretation of quantitative structure-property relationships
Katsushi Takaki a, Tomoyuki Miyaoa,b,âˆ—
a Graduate School of Science and Technology, Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan
b Data Science Center, Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan


a r t i c l e	i n f o	a b s t r a c t

	

Keywords:
Model interpretability
Quantitative structure-activity relationships Quantitative structure-property relationships Symbolic regression
Genetic programming
The interpretation of quantitative structureâ€“activity or structureâ€“property relationships is important in the field of chemoinformatics. Although multivariate linear regression models are typically interpretable, they do not generally have high predictive abilities. Symbolic regression (SR) combined with genetic programming (GP) is a well-established technique for generating the mathematical expressions that describe the relationships within a dataset. However, SR sometimes produces complicated expressions that are hard for humans to interpret. This paper proposes a method for generating simpler expressions by incorporating three filters into GP-based SR. The filters are further combined with nonlinear least-squares optimization to give filter-introduced GP (FIGP), which improves the predictive ability of SR models while retaining simple expressions. As a proof-of-concept, the quantitative estimate of drug-likeness and the synthetic accessibility score are predicted based on the chemical structures of compounds. Overall, FIGP generates less-complicated expressions than previous SR methods. In terms of predictive ability, FIGP is better than GP, but is outperformed by a support vector machine with a radial basis function kernel. Furthermore, quantitative structureâ€“activity relationship models are constructed for three matching molecular series with biological targets. In the case of one target, the activity prediction models given by FIGP exhibit better predictive ability than multivariate linear regression and support vector regression with the radial basis function kernel, whereas for the remaining cases, FIGP is slightly less accurate than multivariate linear regression.





Introduction

The interpretation of quantitative structureâ€“property or structureâ€“ activity relationships (QSPR/QSAR) is an important topic in the field of chemoinformatics [1]. Classical QSAR models are interpretable when multivariate linear regression (MLR) is employed in combination with meaningful molecular descriptors [2,3]. MLR has been widely employed for a range of QSPR/QSAR applications, such as determining the rela- tion between enantio-selectivity and chemical reaction parameters [4â€“ 6]. However, as a modeling method, MLR has a poor predictive ability when the relationship between the molecular descriptors and the prop- erty (activity) is nonlinear. Thus, in practical applications, nonlinear machine learning (ML) algorithms such as random forests (RF) [7], sup- port vector machines with a nonlinear kernel function (SVM) [8], and neural networks (NNs) are frequently employed. These nonlinear ML models accurately predict the property values, even when the structureâ€“ property relationship is linear, by adjusting the model parameters.
In terms of interpreting nonlinear ML models, approaches are gener- ally based on individual compounds. That is, the prediction output of a ML model for a compound can be decomposed into additive molecular descriptor contributions. Because this method is quite effective for un- derstanding the relation between the model output and an input descrip- tor set, i.e., local interpretation, it is widely employed for various target types [9â€“12]. However, such a local interpretation does not always pro- vide an understanding of the predictive model itself (i.e., QSAR/QSPR). Thus, modeling approaches that are interpretable to humans and are more flexible than MLR are necessary.
Symbolic regression (SR) searches for the mathematical expressions that explain a training dataset. Roughly speaking, an SR expression con- sists of a combination of arithmetic operators or mathematical func- tions and terminals (variables and numerical constants). SR expressions do not rely on a fixed functional form, unlike the engineered features of MLR (multiplication/division). Thus, SR has the potential to repre- sent nonlinear QSARs/QSPRs as explicit expressions without any prior knowledge regarding the functional form of the expression. Because the search space of expressions is generally vast, and expressions can be nat-

âˆ— Corresponding author at: Data Science Center and Graduate School of Science and Technology, Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan.
E-mail address: miyao@dsc.naist.jp (T. Miyao).

https://doi.org/10.1016/j.ailsci.2022.100046
Received 6 September 2022; Received in revised form 1 November 2022; Accepted 2 November 2022
Available online 5 November 2022
2667-3185/Â© 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)



urally represented as tree structures, genetic programming (GP) is used to solve SR problems [13].
In materials science, GP-based SR and its variants have recently been employed to derive mathematical expressions of physical phenomena [14]. An SR system named AI Feynman successfully recovered 100 equa- tions of physical laws from datasets [15]. AI Feynman produces sets of expressions through a trade-off between the accuracy and simplicity of the expressions. GP-based SR has also been used to engineer highly cor- related features for a target variable [16,17]. In terms of QSAR analysis, only a limited number of SR studies have been reported, and these have mainly focused on the predictive ability of the models [18,19].
To ensure interpretable QSAR/QSPR models, the expressions gen- erated by SR should be as simple as possible, but flexible enough to represent nonlinear structureâ€“property (activity) relationships. For the purpose of improving the fit to a dataset, Kommenda et al. [20] proposed to optimize numerical constants during the evolution of the SR expres- sions, resulting in a great improvement in the predictive ability of the generated expressions, even for test datasets. This technique is called GP with nonlinear least-squares (NLS) optimization of constant terms. However, the expressions generated by this method sometimes contain complicated functional relations and many constants, most likely from overfitting to a training dataset.
In this paper, we consider the generation of interpretable QSAR/QSPR models. For this purpose, we introduce three filters into GP with NLS optimization to give filter-introduced GP (FIGP). The in- troduced filters are a function filter (F-filter), variable filter (V-filter), and domain filter (D-filter). These filters constrain the generated ex- pressions to be simple and valid for compounds outside the domain of the training dataset [21]. As a proof-of-concept, two well-defined prop- erties are employed: the quantitative estimate of drug-likeness (QED)
[22] and the synthetic accessibility score (SAScore) [23]. The predic- tive performance of the proposed FIGP model is compared with that of two other SR models, namely GP and AI Feynman. The effects of the filters are analyzed by monitoring the expressions generated during the
calculations in this study along with their average values and ranges are listed in Table 1.

Formulas

The QED [22] and SAScore [23] were employed for the formula es- timation in this study. These metrics were chosen because they can be analytically derived from a chemical structure and have been widely used in previous retrospective in-silico studies.

QED
The QED is the geometric mean of eight desirability functions. Each function represents a desirable property of a compound in terms of a sin- gle molecular descriptor, and the combination of these functions quanti- fies the drug-likeness of the compound. These descriptors are the molec- ular mass (Mr), octanolâ€“water partition coeï¬ƒcient (ALOGP), numbers of hydrogen bond donors (HBDs) and acceptors (HBAs), molecular po- lar surface area (PSA), number of rotatable bonds (ROTB), number of aromatic rings (AROM), and number of structural alerts (ALERTS). Each desirability function d is an asymmetric double-sigmoidal function with six parameters. These parameters were determined by fitting the func- tion to the density (histograms) of the descriptor values from a collection of 771 orally dosed approved drugs. For each descriptor, a high desir- ability score is assigned to molecules with descriptor values around the mode of the distribution. Because each desirability function is scaled by the maximum desirability function score, the QED scores range from 0 (undesirable) to 1 (desirable). The outputs of the eight desirability functions are weighted as follows to derive the weighted QED score.
QEDw = ğ·ğ‘€ğ‘Ÿ â‹… ğ·ALOGP â‹… ğ·HBD â‹… ğ·HBA â‹… ğ·PSA â‹… ğ·ROTB â‹… ğ·AROM â‹… ğ·ALERTS ,
(1)
where
ğ· = exp ( ğ‘¤ğ‘– ln ğ‘‘ğ‘– ),

evolution process. The FIGP model gives simpler expressions and more	ğ‘–	W
stable predictive performance than GP without the filters. Furthermore,

three QSAR models for substituents of the chemical structures of active compounds are built using FIGP as demonstrative case studies. Our im- plementation of FIGP is publicly available in the GitHub repository at https://github.com/takakikatsushi/FIGP.


Materials and methods

Compound dataset and molecular representations

From the ZINC15 database [24], a total of 11,670,964 substances from 718 tranches were downloaded as SMILES strings using the fol- lowing options: representation: 2D, reactivity: clean, purchasability: in- stock. After standardizing the chemical structures in the files, e.g., re- moving salts and converting to neutralized forms of (de)protonated sub- structures of chemical structures, 1,000,000 compounds were randomly sampled. This compound pool was used for our virtual experiments. All the molecular descriptors used in this study were implemented in RDKit [25]. These descriptors were manually chosen with the aim of directly connecting to the interpretation of chemical structures. Thus, topological descriptors and descriptors based on the sum of atom-wise surface areas with property contributions were excluded. Furthermore, descriptors counting the functional groups were omitted to prevent the interpretation from being too specific to the functional groups. From this descriptor set, those descriptors having the same value for more than 90% of the molecules were removed. Further variable selection was conducted so that any pair of descriptors had a correlation coeï¬ƒ- cient less than or equal to 0.9. In this variable selection, variables ex- hibiting correlation coeï¬ƒcients greater than 0.9 more than once were iteratively removed. The remaining 23 descriptors used for benchmark
W = ğ‘¤ğ‘€r + ğ‘¤ALOGP + ğ‘¤HBD + ğ‘¤HBA + ğ‘¤PSA + ğ‘¤ROTB + ğ‘¤AROM + ğ‘¤ALERTS
and wi is the weight for the i-th desirability function (one of the eight descriptors). A high weighted QED score can only be achieved when a molecule gives high scores for all desirability functions. Note that QEDw takes a value of 0 if any one of the di is equal to 0. Three weighting schemes have been proposed based on the information content of QEDw . In this study, QEDw,mo is used, which takes the average of the top 1000
weights are wMr = 0.66, wALOGP = 0.46, wHBD = 0.61, wHBA = 0.05, weight combinations that give the highest information content. These wPSA = 0.06, wROTB = 0.65, wAROM = 0.48, and wALERTS = 0.95. The
QED scores were calculated by the QED.default function implemented in the RDKit library [25].

SAScore
The SAScore represents the diï¬ƒculty of synthesis based on the chem- ical structure of a compound, from 1 (easy to synthesize) to 10 (diï¬ƒcult to synthesize) [23]. The SAScore consists of two factors: the appearance of rare substructures (FragmentScore) and the complexity of molecular structures (ComplexityPenalty).
ğ‘†ğ´ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ğ‘¤ = âˆ’ğ¹ ğ‘Ÿğ‘ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ + ğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦	(2)
In FragmentScore, frequently appearing molecular fragments con- tribute to positive values, while rare fragments produce negative scores. These fragment frequencies were determined from 1,000,000 molecules in the PubChem database [26]. The ComplexityPenalty term is further decomposed into four equally weighted penalty terms:
ğ‘…ğ‘–ğ‘›ğ‘”ğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = log10(ğ‘›ğ‘…ğ‘–ğ‘›ğ‘”ğµğ‘Ÿğ‘–ğ‘‘ğ‘”ğ‘’ğ´ğ‘¡ğ‘œğ‘šğ‘  + 1)
+log10(ğ‘›ğ‘†ğ‘ğ‘–ğ‘Ÿğ‘œğ´ğ‘¡ğ‘œğ‘šğ‘  + 1),	(3)


Table 1
Statistics of descriptor values for 1,000,000 ZINC compounds.



ğ‘†ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘œğ¶ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = log10(ğ‘›ğ‘†ğ‘¡ğ‘’ğ‘Ÿğ‘’ğ‘œğ¶ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘  + 1),	(4)
ğ‘€ğ‘ğ‘ğ‘Ÿğ‘œğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ = log10(ğ‘›ğ‘€ğ‘ğ‘ğ‘Ÿğ‘œğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğ‘  + 1),	(5)
ğ‘†ğ‘–ğ‘§ğ‘’ğ‘ƒ ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦ = ğ‘›ğ‘ğ‘¡ğ‘œğ‘šğ‘ 1.005 âˆ’ ğ‘›ğ‘ğ‘¡ğ‘œğ‘šğ‘ ,	(6) where a macrocycle is defined as having more than eight atoms in a ring. Therefore, large compounds consisting of many complicated frag- ments represented by the structural features above produce high values
of SAScoreraw in Eq. (2). This raw score is scaled from 1â€“10 to pro- duce the final SAScore. In the RDKit implementation for SAScore, sas-
corer.calculateScore, the original SAScoreraw definition was slightly mod- ified to treat macrocyclic structures and the symmetry of molecules.

Symbolic regression with genetic programming

The goal of SR is to learn the mathematical expressions underlying a regression model between an objective variable y and independent vari- ables x from a training dataset. Unlike black-box ML models, a mathe- matical expression describes the regression model and the structure of the model is not fixed before training. Because the solution space of ex- pressions is vast, eï¬ƒcient search algorithms are necessary. GP [13] mim- ics the natural evolution process to search for the optimal solution, and has been successfully applied to searches across the solution space of ex- pressions. In GP, an expression is represented as a tree, where leaves are numerical constants or variables and (non-leaf) nodes are mathematical operators applied to their child node(s) (Fig. 1A). This structure makes it possible to apply evolution-mimicking operations in GP, namely mu- tation and crossover. In the mutation operation, a selected subtree is replaced by another randomly generated subtree with a certain proba- bility. In the crossover operation, two individuals (expressions) are ran- domly selected, and one (randomly selected) subtree from each expres- sion is swapped with one from the other expression (Fig. 1B).
To guide the evolution of the expressions in a desirable direction, a fitness function is used. The fitness function determines whether newly created individuals (expressions) survive into the next generation. In general, individuals with higher fitness values survive. As a fitness met- ric, the root mean square error (RMSE), the coeï¬ƒcient of determination (R2), or the mean absolute error (MAE) between the observed and pre- dicted y values is usually employed [13, 20].
Constant optimization in GP
One of the issues in GP with SR is the treatment of numerical pa-
constants a and b appear in the expression y=ax+b. In a naÃ¯ve approach, rameters or constants in expression trees. For example, in Fig. 1A, the
only a limited number of choices, such as 0, 1, and ğœ‹. In more refined these parameters can be absorbed in the structure of GP by giving them
ways, these parameters can be either sampled from a probability dis- tribution, e.g., a uniform distribution (Fig. 1C left), or numerically op- timized as a parameter set during the GP operation (Fig. 1C right). A previous methodological comparison showed that the NLS estimation of the constant terms outperforms other GP variants in addition to sev- eral ML models [20]. Thus, our proposed method is based on GP with NLS optimization in addition to introducing three filters, as explained below. In this manuscript, GP with NLS optimization of the constant terms is referred to as GP for simplicity.


Filter-introduced GP

Three filters
One motivation for using SR as the modeling method is to under- stand natural phenomena or experimental results in the form of math- ematical expressions. Therefore, simple and understandable mathemat- ical expressions must be constructed. The expressions generated by GP without any constraints sometimes contain undesirable features, such as one variable appearing in several terms (1), nested operations (2), and invalid operations (3). These three situations are illustrated in Fig. 2. Fig. 2A depicts situation (1) with an expression tree containing three variables. Variable x2 appears twice in different terms of the expres- sion, resulting in a model that is diï¬ƒcult to interpret. Fig. 2B represents situation (2) with an expression tree containing two consecutive â€œexpâ€ operations. Situation (3) only occurs when applying the expression to another dataset (Fig. 2C). Expressions that are well-fitted to the training dataset sometimes produce infinite values for unseen test compounds be- cause of an invalid operation. This occurs when test data points reside outside the domain of applicability of the expression [27]. For exam- ple, when HBD is selected as the descriptor, and all molecules in the training dataset have at least one HBD, this descriptor may become the denominator of an expression. Subsequently, if molecules without HBDs are encountered, this expression will result in division by zero and be invalid.




Fig. 1. Concept of symbolic regression (SR) with genetic programming (GP). A: Linear regression model with a graph representation in GP, where a and b are constants to be optimized. B: Crossover operation of two expressions, where red dotted-squares are selected as target subtrees. C: Optimization of constants a and b. Initial values for these two constants are randomly sampled (left), followed by nonlinear least-squares optimization (right).


To solve situation (1), expressions containing the same variables in different leaves are simply discarded. This is the V-filter. For situ- ation (2), expressions with specific operators such as â€œexpâ€ or â€œlogâ€ that appear more than once in a subtree are removed by the F-filter. In this study, the F-filter detects the hierarchical usage of {exp, ln} and {sqrt, square, cube}. For situation (3), the detection of poten- tially harmful expressions during the training phase of GP is intro- duced through the D-filter. In the D-filter, several test data points out- side the domain of the training data are used to determine whether the output values exceed the range of the objective variable or not. In this study, all data points in the training and test datasets passed through this filter. By applying these three filters during expression
evolution, surviving expressions are expected to be easy for humans to understand.

Limitations of FIGP
One of the biggest limitations of the proposed FIGP is the possibility of not being able to find the optimal expression for a phenomenon, such as when the ground-truth formula for the phenomenon violates one or more of the to-be-avoided situations explained in Fig. 2. The precise expression of a mathematical formula usually requires the same variable to appear in different terms (violation of situation (1)). However, by ensuring that a variable appears at most once, the relation between the objective and independent variables becomes clearer. Note that simple




Fig. 2. Three cases where interpretation of the expression becomes difficult. A: The same variable appears more than once in a GP tree. B: Nested exponential operations (right), while unnested usage of the operation is allowed (left). C: Possibility of zero division.


expressions can also be generated by introducing penalty terms in the fitness function of GP, instead of using hard constraints. However, in our opinion, the three constraints must be satisfied to make the expression understandable, and are thus introduced as filters instead of numerical penalty terms.


FIGP procedures
The procedures involved in FIGP are summarized in Fig. 3. There are four components: initial expression generation, selection, evolutionary operation, and fitness calculation. In the initial expression generation, one half of the individuals are generated with the Full method and the other half are generated with the Grow method [13] to ensure a diverse set of individuals. Constant nodes in these individuals are optimized by the NLS method. These individuals (expressions) are filtered out by the V-, F-, and D-filters. This process is repeated until the number of indi- viduals reaches a predetermined population size. Expressions consisting of only numerical parameters are disqualified in this phase. In the se- lection phase, the expression with the best fitness value passes to the next generation, alongside the expressions selected through tournament selection from five randomly selected individuals. In the evolutionary process, crossover and mutation operations are applied to individuals or pairs of individuals with predefined probabilities. When the same individual is produced as a result of an evolutional operation, another operation is applied until a unique individual is created (up to a prede- fined number of iterations). In the fitness calculation module, numerical parameters (constants) are optimized by NLS, followed by score calcula- tion. In this study, the negative RMSE is applied to the training dataset as the fitness function to be maximized. This process is repeated for a predefined number of iterations.
Table 2
Experimental conditions for FIGP.



Experimental conditions for GP

The parameter names and values of the FIGP procedure are listed in Table 2. These values were determined based on trial runs using GP. For the conventional GP modeling, the same parameter values as for FIGP were used.

Comparison methods

As comparison methods, we considered MLR and support vector re- gression (SVR) [28] with radial basis function (RBF) and linear kernels. The objective loss function of the SVR is the sum of the norms of the co- eï¬ƒcient vectors and the soft margin loss, which leads to robust models even in the presence of outliers. Nonlinear SVR with the RBF kernel has been extensively used for QSAR models [29,30]. Linear SVR and MLR are directly interpretable based on the regression coeï¬ƒcients of the de-




Fig. 3. Procedure of FIGP. Four steps of finding the best SR model are described with methodological points in each step. Initial population generation creates the individuals in the initial population. Selection selects the individuals that survive, followed by evolutional operations and fitness calculations.



scriptors. In this study, the SVR hyperparameters C, ğœ€, and ğ›¾ (only for
RBF kernel) were optimized by five-fold cross-validation of the training
dataset with the help of Optuna using the TPESampler function [31].
AI Feynman [15], a physics-inspired SR modeling system, was also used for comparison. AI Feynman contains a cascade of filtering pro- cesses to generate feasible equations satisfying physics-oriented con- straints such as symmetry. AI Feynman can propose expressions for SR models on the Pareto frontier between fitness and complexity.

Evaluation metrics

The prediction performance was evaluated in terms of the coeï¬ƒcient of determination (R2), RMSE, and MAE on test datasets.

Software and implementation of FIGP

FIGP was implemented on top of the DEAP GP library [32]. The code for FIGP, containing the V-, F-, and D- filters, is publicly available in a GitHub repository at https://github.com/takakikatsushi/FIGP, along with example notebooks.

Results and discussion

Study design

The predictive ability of various ML models was compared in terms of the two objective variables of the QED and SAScore. The modeling methods employed in this study were FIGP with the F- and D-filters (FIGP_FD), FIGP with the F-, V-, and D-filters (FIGP_FVD), GP, MLR, and SVR with the RBF kernel (SVR (rbf)) and linear SVR (SVR (linear)). With- out the D-filter, the FIGP expressions sometimes output infinite values for test compounds. Thus, FIGP was constrained to include the D-filter. In the GP algorithm, the division, sqrt, and ln operators were pro- tected from undefined operations, such as zero division. In the DEAP implementation [32], a value of 1 is returned when zero division is at- tempted. For GP, R2 values of less than zero for the test datasets were
treated as zero for ease of comparison in terms of property prediction.
To understand the effect of the size of the training dataset on the predictive ability and complexity of expressions, the number of training compounds was varied from 50 to 800: {50, 100, 200, 400, 800}. The rest of the 1,000,000 ZINC compounds constituted the test data. For each number of training compounds, five training datasets were randomly compiled, and five prediction trials were conducted. For each training dataset, five GP and FIGP models were built by changing the seed val- ues of the random number generator in GP. The representative model was chosen as that which gave the highest R2 for the training dataset. Note that cross-validation was not conducted during the training phase because the FIGP and GP models have no hyperparameters to be opti- mized. Thus, the expression that best explains the training dataset was selected.
The AI Feynman system was only applied to the QED with default parameters. The training dataset size was fixed to 100. For this calcula- tion, a further limited descriptor set was employed as a means of reduc- ing the computational cost and to ensure errorless outputs. The top 13 of 26 descriptors were selected based on the mutual information against y (Table S1) for 1,000,000 ZINC compounds. Thus, only meaningful descriptors were employed in this method.

SR for QED and SAScore

Predictive performance
As a metric of the predictive ability of ML models, the R2 values pro- duced for the test datasets were measured against the number of training compounds (Fig. 4). Overall, SVR (rbf) shows the best predictive abil- ity. While GP models without any filters give a better fit to the training datasets than those with filters, the R2 values for the test datasets exhibit large variances, implying that the GP models tend to be overfitted to the training data, especially when the training datasets are small. In con- trast, FIGP_FD and FIGP_FVD exhibit stable predictive ability with the various training dataset sizes. For the QED, these two modeling methods consistently outperform SVR (linear) and MLR. In terms of the SAScore, the FIGP models perform as well as MLR, but are inferior to SVR (lin- ear) and SVR (rbf) in terms of R2 scores. This may be explained by the nature of SAScore: a simple summation of complexity scores, although




Fig. 4. Predictive capability of SR models. The average R2 values for the test datasets of QED and SAScore are plotted against the training dataset sizes. Five modeling methods were tested: FIGP_FD, FIGP_FVD, GP, SVR (rbf), and SVR (linear). Error bars represent the 95% confidence intervals based on the results of five trials.


each penalty term is in the logarithmic scale. This assertion is supported by the fact that FIGP_FD and FIGP_FVD exhibit similar performance for this target. By introducing the V- and D-filters into GP, the fitness scores with the training data become slightly worse, whereas the R2 values for the test datasets remain unchanged.
Convergence of GP-NLS
For both target properties, the fitness values achieved on the training data (RMSE) and the expression diversity were monitored as the compu- tations progressed. The expression diversity was measured in terms of operators and descriptors. For the operator diversity, the ratio of expres- sions containing specific operators to the total number of expressions, i.e., population size, was monitored. In the same way, for the descriptor diversity, the ratio of expressions containing specific descriptors to the population size was monitored. Fig. 5 reports the expression diversity for the QED when the training seed ID was 0 and the training dataset had a size of 100. Transition plots with other seeds (IDs of 1, 2, 3, and 4) are consistent with that obtained from a seed of 0, as shown in Figs. S1â€“S4 in the Supporting Information. Overall, the RMSE values decrease mono- tonically and converge within 200 generations. The minimum RMSE value is achieved by GP, followed by FIGP_FD and FIGP_FVD. FIGP_FVD exhibits slower convergence than FIGP_FD in terms of RMSE and the descriptor and operator ratios.
Furthermore, as an indirect metric of the degree of overfitting to the training data, the ratio of constant nodes per expression was monitored during the GP progress. Over the five trials for the QED with a training dataset of size 100, the average constant node ratio after convergence is 4.36 (sd: 1.59) for GP, 2.63 (0.74) for FIGP_FD, and 2.16 (0.36) for
FIGP_FVD. For the QED, the FIGP models used fewer constant nodes
than GP. For the SAScore with a training dataset of size 200, the average constant node ratios are 4.56 (sd: 2.06) for GP, 4.40 (1.54) for FIGP_FD, and 4.15 (1.35) for FIGP_FVD, showing no significant difference.
For the QED, where a nonlinear relation was expected between the chemical structure and the objective variable, GP tends to employ more constant nodes than the other methods.
Expressions returned by AI Feynman
AI Feynman was also used to evaluate the QED with 100 training compounds and a seed of 0. Recall that AI Feynman produces a set of expressions on the Pareto frontier, with a tradeoff between fitness and simplicity of expression (Table S2). For most of the Pareto solutions,
tional operations. The simplest expression showed an R2 value of âˆ’0.09 the R2 values for the test data were infinite because of undefined func-
for the training set and negative infinity for the test set. The generated expression was
QED = arccos âˆ’0.03 Ã— NAliCc2 + 0.03 Ã— NAliCc + 1.0
+ arctan âˆ’0.08 Ã— NSR3 + 0.3 Ã— NSR2 + 0.11 Ã— NSR + 0.06
âˆ’1.52 Ã— arccos 1.0 âˆ’ 0.01 Ã— NHOHCount2
Ã— arctan âˆ’0.55 Ã— NAliHc3 + 1.84 Ã— NAliHc2 âˆ’ 1.08 Ã— NAliHc âˆ’ 0.39 .
(7)
This expression contains two arccos functions and two arctan func- tions. The number of aliphatic heterocycles (NAliHc) appears in several terms. Even the simplest expression is hard to interpret through a vi- sual inspection, notwithstanding that it completely fails to explain the training data. For the most complex expression, the R2 value for the training data set was 0.98, suggesting a good fit to the training data.




Fig. 5. Convergence of FIGP training process. For the first of five GP trials (seed 0), the converge of the minimum RMSE values are plotted against the generation (top row), the probabilities of using operators in an expression (middle row), and the probabilities of using descriptors in an expression (bottom row). For GP, protected versions of the division, logarithm, and sqrt operation were used.


However, the R2 value for the test data was âˆ’18.22 and the generated
expression contained too many terms (274 plus signs and 286 minus
signs).
AI Feynman is intended to derive physics formulas from a dataset, with symmetries and separability considered inside the system. This might not be appropriate for QSPR/QSAR analysis, because models for QSPR/QSAR simply approximate the relations between chemical struc- tures and properties/activities. Furthermore, in AI Feynman, a neural network is constructed using a training dataset to detect symmetries and smoothness. To form an accurate response surface, many data points are needed. In our calculation setting, we selected 13 out of 26 variables based on the mutual information and 100 training compounds. This calculation setting might impede AI Feynman from generating â€œtrueâ€ expressions.

Expression analysis
The three expressions for QED generated by FIGP_FD, FIGP_FVD, and GP from a training seed ID of 0 and a training dataset of size 100 are reported in Table 3. All SR expressions from five trials gen- erated by FIGP_FD, FIGP_FVD, and GP with a training dataset of size 100 are reported in Tables S3â€“S5, respectively. Among the expressions in Table 3, GP gives the best predictive ability, although this is not al-
ways true according to Fig. 4. The expression generated by GP is more complicated than those given by FIGP_FD and FIGP_FVD. For example, the effect of molecular weight (EMWt) on the QED prediction values is hard to understand. The expressions from FIGP are similar to each other (Table S4). Neither expression contains a variable appearing in more than one term. This is not always true for FIGP_FD, as can be seen from Table S3 (seed ID1 and ID4). The logarithm of the FIGP_FVD ex-
pression in Table 3 becomes the product of (NAroR3 + 7.13 â‹… NRB) and
(âˆ’0.000694 â‹… NHetAtm âˆ’ 0.000694 â‹… logp). The effects of the contributing
features on the logarithm of QED values differ in scale and combination.
might have an equivalent effect on the QED values as 7.13âˆ—NRB (number For example, the third power of the number of aromatic rings (NAroR)
of rotatable bonds). Likewise, the number of heteroatoms (NHetAtm) and logP values have equivalent effects on the QED values based on this equation. Note that the correlation among descriptors is not considered, although it is likely that these descriptor values cannot be altered inde- pendently. Compared with the ground-truth QED definition in Eq. (1), several descriptors appear frequently in the FIGP expressions: NAroR, NRB, and logP. The exponential term in Eq. (1) was correctly identi- fied by FIGP_FVD in all five trials (Table S4) and by FIGP_FD in four of the five trials (Table S3). However, the generated expressions are not identical to Eq. (1).


Table 3
GP and FIGP expressions for QED. For the first of five GP trials (seed 0) with a training dataset size of 100 for QED, SR expressions along with R2 for the training and test datasets are listed. The descriptors in the expressions are defined in Table 1.

Method	Expression	Train R2	Test R2
FIGP_FD	exp(âˆ’1.40 â‹… 10âˆ’6 â‹… EMWt â‹… (NRB + 8.02) â‹… (NAroR3 + NHetAtm + 32.9))	0.72	0.64
FIGP_FVD	exp((NAroR3 + 7.13 â‹… NRB) â‹… (âˆ’0.000694 â‹… NHetAtm âˆ’ 0.000694 â‹… logp))	0.70	0.59
1.40â‹…107 â‹…NAliHcâˆ’2.60â‹…107 â‹…NAroR+ EMWt+9.30â‹…109

GP		NHA+46.3 
EMWt3 âˆ’EMWt+6.03â‹…107 + MinESI+1.96â‹…108
0.84	0.72




Table 4
GP and FIGP expressions for SAScore. For the first of five GP trials (seed 0) with a training dataset size of 200 for SAScore, SR expressions along with R2 for the training and test datasets are listed. The descriptors in the expressions are defined in Table 1.

Method	Expression	Train R2	Test R2

FIGP_FD	FCSP3 + 0.0179 â‹… NRB +  EMWtâˆ’4.99â‹…103  + 21.2 + (NAliR + 131) â‹… NAliR+âˆšNAroHc+2.02
0.66	0.47

NHOHCount+227	EMWt+MaxAbsESI
FIGP_FVD	FCSP3 âˆ’ MaxPC + 0.262 â‹… NAroHc + 0.262 â‹… RCount + (0.000490 â‹… EMWt âˆ’ 0.554) â‹… (NAroR + 2.44) + 3.25	0.62	0.44
GP	0.777 â‹… FCSP3 + 0.0815 â‹… NHOHCount + 0.102 â‹… NRB + (0.608 âˆ’ 0.0395 â‹… NRB) â‹… (NAliR + 0.596 â‹… NAroHc) + 1.19	0.64	0.46
Table 5
MMS profiles.









Similar analysis was conducted for SAScore with a dataset size of
200. The expressions generated by the three algorithms are reported in Table 4 for a training seed ID of 0. All SR expressions for the five trials with a training dataset size of 200 are reported in Tables S6â€“S8 for FIGP_FD, FIGP_FVD, and GP, respectively. The three expressions in Table 4 indicate comparable predictive performance for the test dataset. All expressions use the fraction of SP3 carbon atoms (FCSP3) as a de- scriptor with a positive effect on the SAScore values (diï¬ƒcult to synthe- size). This descriptor is related to the number of stereo centers, and is thus an important descriptor for SAScore prediction. The FIGP_FVD ex- pression is the simplest, as expected. For FIGP_FD and GP, the effect of NRB on the SAScore is not clear because this descriptor appears in multi- ple terms in the expressions. Although FIGP_FD produces the most com- plicated expression in Table 4, GP without any filters generates more complicated expressions based on the number of terms in Tables S6â€“ S8.

Demonstration of QSAR modeling

Datasets
For a demonstrative application of FIGP, three sets of substituents with specific cores (analogous compounds) against specific targets were compiled from the ChEMBL database version 29 [33]. Only bioactive compounds annotated with Ki values against specific human target macromolecules were considered. These compound and Ki data were extracted from assays with a confidence score of 9 (highest) and direct binding. Targets with more than 300 bioactive compounds after dis- carding the upper and lower 10th percentiles in the number of heavy atoms and showing a minimum potency range of 5 were extracted. From these compound datasets, target-wise matching molecular series (MMS)
[34] were created with the help of the RDKit community contribution module â€œmmpaâ€ [25], with a substituent ratio against the core of 0.35. MMS with a single-cut core and containing at least 40 substituents with a minimum potency range of 3.0 were selected. Twelve MMS against eight targets were identified. From the eight targets, three MMS were selected based on their target diversity and potency range. The profiles of the selected MMS are provided in Table 5.

Substituent descriptors
The following seven descriptors were used: number of aromatic rings (NaroR), number of hydrogen bond acceptor/donor atoms (NHA/NHD), logarithm of the octanol/water partition coeï¬ƒcient (logp), rotatable bond counts (NRB), topological polar surface area (TPSA), and molecu- lar weight (MWt). These descriptor values were only calculated for the substituents after replacing the attachment points with carbon atoms. The descriptor calculations were conducted using the Molecular Oper-









Table 6
Predictive ability of ML models for training and test datasets.

For each dataset, the best predictive performance for the test data is highlighted in bold.


ating Environment Software ver. 2022.02 [35]. The MMS datasets with these descriptors and potency values, as well as substituent SMILES strings, are provided as tab-separated text in the Supporting Informa- tion.

FIGP models
Each MMS dataset was randomly split into training (80%) and test (20%) sets. MLR, SVR (rbf), and FIGP_FVD were employed with the same settings as for property prediction. For FIGP, all the filters were included, and all the data points for each target were used in the D-filter. The goodness-of-fit to the training and test datasets is reported in Table 6. The best modeling methods are different for each dataset. For datasets ID1 and ID3, MLR performs almost as well as FIGP_FVD. For dataset ID3, SVR (rbf) is the best method, whereas for dataset ID2, FIGP_FVD is the best. Table 7 reports the expressions generated by FIGP_FVD. The ex- pression for dataset ID2 is nonlinear. In the numerator, NRB multiplied by MWt has a negative effect on the pKi prediction. The effect of NHA is less important because it is much smaller than the other constant in the logarithm function. This is consistent with the regression coeï¬ƒcient value of 0.001 for NHA in the MLR model. NAroR is divided by logp in the denominator. The domain of logp contains zero, so this function is
not defined for compounds for which logp = 0. Thus, data points for the
D-filter should be carefully selected for avoiding invalid operations. The
FIGP_FVD expression for dataset ID3 can be expressed by linear com- binations of descriptors and polynomial terms. That is why FIGP_FVD and MLR exhibit a similar predictive ability for the test dataset. Over- all, FIGP_FVD provides a better fit to the training data than MLR, but the predictive ability of FIGP_FVD is not always better than that of MLR.


Table 7
Mathematical expressions generated by FIGP_FVD.

Data ID	Expression
1	logp2 â‹… (NRB âˆ’ 0.476) â‹… (âˆ’0.0201 â‹… TPSA âˆ’ 0.105) + (1.01 âˆ’  23.7 ) â‹… (NAroR + 0.286 â‹… NHD + 9.74)

  âˆ’0.0541 â‹…NRBâ‹…(MWt âˆ’ 137)+ 52.3 
NAroR + 0.576 â‹…NHD +log(NHA + 1.17 â‹…103)
MWt

3	0.0164 â‹… MWt + 0.0984 â‹… NRB âˆ’ 0.152 â‹… logp âˆ’ (0.0551 âˆ’ 0.0510 â‹… NAroR) â‹… (âˆ’4.88 â‹… NHD + TPSA âˆ’ 7.87) + 7.27


Conclusions

This paper has described an interpretable QSAR/QSPR method based on the use of three filters in GP for SR. The V-filter forces every variable to appear no more than once, the F-filter suppresses the recursive usage of functionals, and the D-filter ensures the expression does not output infinite or undefined values when compounds outside the domain of the training dataset are given.
In our proof-of-concept study, the proposed FIGP generated simpler QSPR models than two existing SR methods (AI Feynman and GP). The QSPR expressions given by FIGP provide insights into the original func- tional forms of the objective variable for the QED and SAScore, while maintaining a distance from the ground-truth expressions. For the QED, FIGP showed better predictive ability than linear regression modeling methods, while for the SAScore, the predictive ability was slightly infe- rior to that of SVR (linear). The black-box machine learning method of SVM (rbf) exhibited the highest predictive ability.
The mathematical expressions generated by GP can be used to derive gradients. This makes it possible to constrain the generation of expres- sions to those with smooth response surfaces during evolution. Design- ing molecules based on the gradient of a compound may be a useful op- timization approach. Furthermore, GP contains stochastic operations in nature, so we must determine which expression should be used in prac- tical applications. This selection process may be heuristic, but FIGP has been designed to help humans interpret QSPR/QSAR. It is also possible to derive common features by analyzing multiple generated expressions, which might lead to interpretation of QSPRs/QSARs.
Inside the FIGP architecture, the only criterion tested in this study was the goodness-of-fit of expressions produced using a training dataset (RMSE). Other criteria could be used, such as the Akaike informa- tion criterion and Bayesian information criterion. Thus, further re- search is needed to identify methods for generating simple predictive expressions.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Data availability

Data will be made available on request.


Acknowledgements

We thank Swarit Jasial for carefully proofreading a draft of this manuscript. We also thank Ryosuke Asahara for helping us set up com- putational analyses. This work was supported by a Grant-in-Aid for Transformative Research Areas (A) 21A204 Digitalization-driven Trans- formative Organic Synthesis (Digi-TOS) from the Ministry of Education, Culture, Sports, Science & Technology, Japan, and was supported by JSPS KAKENHI Grant Number JP20K19922. We thank Stuart Jenkin- son, PhD, from Edanz (https://jp.edanz.com/ac) for editing a draft of this manuscript.
Supplementary materials

Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.ailsci.2022.100046.
References

Polishchuk P. Interpretation of quantitative structure-activity relationship models: past, present, and future. J Chem Inf Model 2017;57(11):2618â€“39.
Hansch C, Maloney PP, Fujita T, Muir RM. Correlation of biological activity of phe- noxyacetic acids with Hammett substituent constants and partition coeï¬ƒcients. Na- ture 1962;194(4824):178â€“80.
Hansch C. The advent and evolution of QSAR at Pomona College. J Comput Mol Des 2011;25(6):495â€“507.
Zahrt AF, Athavale SV, Denmark SE. Quantitative structure-selectivity rela- tionships in enantioselective catalysis: past, present, and future. Chem Rev 2020;120(3):1620â€“89.
Santiago CB, Guo JY, Sigman MS. Predictive and mechanistic multivariate linear regression models for reaction development. Chem Sci 2018;9(9):2398â€“412.
Reid JP, Proctor RSJ, Sigman MS, Phipps RJ. Predictive multivariate linear regres- sion analysis guides successful catalytic enantioselective minisci reactions of di- azines. J Am Chem Soc 2019;141(48):19178â€“85.
Ho TK. The random subspace method for constructing decision forests. IEEE Trans Pattern Anal Mach Intell 1998;20(8):832â€“44.
Cortes C, Vapnik V, Saitta L. Support-vector networks. Mach Learn 1995;20(3):273â€“97.
RodrÃ­guez-PÃ©rez R, Bajorath J. Interpretation of machine learning models using shapley values: application to compound potency and multi-target activity predic- tions. J Comput Aided Mol Des 2020;34(10):1013â€“26.
Balfer J, Bajorath J. Visualization and interpretation of support vector machine ac- tivity predictions. J Chem Inf Model 2015;55(6):1136â€“47.
Tamura S, Jasial S, Miyao T, Funatsu K. Interpretation of ligand-based activ- ity cliff prediction models using the matched molecular pair kernel. Molecules 2021;26(16):4916.
Asahara R, Miyao T. Extended connectivity fingerprints as a chemical reaction rep- resentation for enantioselective organophosphorus-catalyzed asymmetric reaction prediction. ACS Omega 2022;7(30):26952â€“64.
Koza JR. Genetic programming as a means for programming computers by natural selection. Stat Comput 1994;4(2):87â€“112.
Schmidt M, Lipson H. Distilling free-form natural laws from experimental data. Sci- ence 2009;324(5923):81â€“5.
Udrescu SM, Tegmark M, Feynman AI. A physics-inspired method for symbolic re- gression. Sci Adv 2020;6(16):eaay2631.
Xie J, Zhang L. Machine learning and symbolic regression for adsorption of atmo- spheric molecules on low-dimensional TiO2,. Appl Surf Sci 2022;597:153728.
Weng B, Song Z, Zhu R, Yan Q, Sun Q, Grice CG, Yan Y, Yin WJ. Simple descrip- tor derived from symbolic regression accelerating the discovery of new perovskite catalysts. Nat Commun 2020;11(1):1â€“8.
Archetti F, Lanzeni S, Messina E, Vanneschi L. Genetic programming for computa- tional pharmacokinetics in drug discovery and development. Genet Program Evolv- able Mach 2007;8(4):413â€“32.
Archetti F, Giordani I, Vanneschi L. Genetic programming for QSAR investigation of docking energy. Appl Soft Comput 2010;10(1):170â€“82.
Kommenda M, Burlacu B, Kronberger G, Affenzeller M. Parameter identification for symbolic regression using nonlinear least squares. Genet Program Evolvable Mach 2020;21(3):471â€“501.
Miyao T, Funatsu K. Finding chemical structures corresponding to a set of coordi- nates in chemical descriptor space. Mol Inform 2017;36(8):1700030.
Bickerton GR, Paolini GV, Besnard J, Muresan S, Hopkins AL. Quantifying the chem- ical beauty of drugs. Nat Chem 2012;4(2):90â€“8.
Ertl P, Schuffenhauer A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. J Chemin- form 2009;1(1):1â€“11.
Sterling T, Irwin JJ. ZINC 15 - ligand discovery for everyone. J Chem Inf Model 2015;55(11):2324â€“37.
RDKit Open-source cheminformatics. https://www.rdkit.org
Kim S, Chen J, Cheng T, Gindulyte A, He J, He S, Li Q, Shoemaker BA, Thiessen PA, Yu B, et al. PubChem in 2021: new data content and improved web interfaces. Nu- cleic Acids Res 2021;49(D1):D1388â€“95.
Dragos H, Gilles M, Alexandre V. Predicting the predictability: a unified ap- proach to the applicability domain problem of Qsar Models. J Chem Inf Model 2009;49(7):1762â€“76.



Smola AJ, SchÃ¶lkopf B. A tutorial on support vector regression. Stat Comput 2004;14(3):199â€“222.
Li L, Wang B, Meroueh SO. Support vector regression scoring of receptor-ligand complexes for rank-ordering and virtual screening of chemical libraries. J Chem Inf Model 2011;51(9):2132â€“8.
RodrÃ­guez-PÃ©rez R, Bajorath J. Evolution of support vector machine and regres- sion modeling in chemoinformatics and drug discovery. J Comput Aided Mol Des 2022;2022:1â€“8.
Akiba T, Sano S, Yanase T, Ohta T, Koyama M. Optuna: a next-generation hyperpa- rameter optimization framework. In: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.; 2019. p. 2623â€“31.
Fortin F-A, Marc-AndrÃ© Gardner U, Parizeau M, GagnÃ© C. DEAP: evolutionary algo- rithms made easy. J Mach Learn Res 2012;13:2171â€“5.
Gaulton A, Bellis LJ, Bento AP, Chambers J, Davies M, Hersey A, Light Y, McGlinchey S, Michalovich D, Al-Lazikani B, Overington JP. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic Acids Res 2012;40:D1100â€“7.
substructureâˆ’activity relationship trailing. J Med Chem 2011;54:2944â€“51. [34] Wawer M, Bajorath J. Local structural changes, global data views: graphical
[35] MOE (Molecular Operating Environment). Montreal, Canada: Chemical Computing Group Inc; 2022.
