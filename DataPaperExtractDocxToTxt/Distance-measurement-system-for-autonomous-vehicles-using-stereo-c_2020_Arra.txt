Array 5 (2020) 100016

		




Distance measurement system for autonomous vehicles using stereo camera
Abdelmoghit Zaarane *, Ibtissam Slimani , Wahban Al Okaishi , Issam Atouf , Abdellatif Hamdoun
LTI Lab, Department of Physics, Faculty of Sciences Ben M’Sik, University Hassan II Of Casablanca, Morocco



A R T I C L E I N F O

Keywords:
Distance measurement Vehicle detection Stereo vision
Image processing Stereo camera
A B S T R A C T

The focus of this paper is inter-vehicles distance measurement which is a very important and challenging task in image processing domain. Where it is used in several systems such as Driving Safety Support Systems (DSSS), autonomous driving and traffic mobility. In the current paper, we propose an inter-vehicle distance measurement system for self-driving based on image processing. The proposed system uses two cameras mounted as one stereo camera, in the hosting vehicle behind the rear-view mirror. The detection of vehicles is performed first in a single camera using a recent powerful work from the literature. Then, the same vehicle is detected in the image captured by the second camera using template matching technique. Thus, the inter-vehicle distance is calculated using a simple method based on the position of the vehicle in both cameras, geometric derivations and additional technical data such as distance between the cameras and some other specific angles (e.g. the cameras view field angle). The results of the extensive experiments showed the high accuracy of the proposed method compared to the previous works from literature and it allows to measure efficiently the distances between the vehicles and the hosting vehicle. In addition, this method could be used in several systems of various domains in real time regardless of the object types. The experiments results were done on a Hardware Processor System (HPS) located in a VEEK-MT2S provided by TERASIC.





Introduction

In the last twenty years, the self-driving cars have obtained a huge importance in the research domain, they are expected to take places of humans in different fields by performing several missions. The autono- mous vehicles development has been one of the most important subjects in the field of automotive research due to the growth of the traffic problems in most of the world. Therefore, the expectance for increasing safety of the road and comfort of driving are high by relieving the drivers of driving tasks in partial or complete way. Because the automation of the driver responsibilities may significantly reduce the collisions and in- crease the road safety.
The researchers face a lot of difficulties in self-driving field due to the dynamic and complex environment and the complex movement in fast way. The automated vehicles need to detect the other vehicles whatever their shape and type [1–3,16]. Thus, several algorithms should be per- formed such as vehicle detection, license plate detection [15] and speed and distance estimation. The extracted information using these algo- rithms are used by the automated vehicles for making some decisions for example bypassing other vehicles or changing their path or their speed. The distance measurement between vehicles is very important subject
in autonomous vehicles. Therefore, detecting surroundings vehicles in- formation accurately (e.g. distance between vehicles) in real time is very important and challenging task. In the literature, two main methods exist for the distance measurement, active methods and passive methods.
The active methods measure the distance by sending signals to the target. These systems are based generally on computing the time of flight of laser beams, ultrasound, or radio signals, to measure and search for the objects. The time of flight systems are used to estimate the object distance using specific sensors by measuring the needed time of a signal pulse to transmit to the object and reflect by it. Their main inconvenient are the potential confusion of echoes from previous or subsequent pulses and also the accuracy range of distance for these systems is usually bounded between one to 4 m. Carullo and Parvis [4], presented an ultrasonic system that can measure the distance of selected points, where the ul- trasonic sensor is based on measuring the time of flight of an ultrasonic pulse, which is reflected by the object. Nakahira et al. [5], presented an ultrasonic system using a pulse time-of-flight estimation, by combining frequency-modulated emissions and correlation detection for time-of flight estimation in real-time from the noisy echoes. Their purpose is to tackle with confusion of echoes from previous or subsequent pulses, those of other systems, or from other objects.



* Corresponding author.
E-mail addresses: z.abdelmoghit@gmail.com (A. Zaarane), ibtissamslimani7@gmail.com (I. Slimani).

https://doi.org/10.1016/j.array.2020.100016
Received 30 May 2019; Received in revised form 26 December 2019; Accepted 7 January 2020
Available online 8 January 2020
2590-0056/© 2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).



However, the passive methods, measure the distance by receiving information about the position of the object. These systems are based generally on cameras and computer vision techniques. In principal, two type of systems exist for the passive method, Mono Vision systems and Stereo Vision systems.
The mono vision systems use one camera to estimate the distance based on reference points in the camera view field and there are usually used for visual servoing purposes. Zhang et al. [6], presented an absolute localization of an object in the camera coordinate frame using the dis- tance estimation between principal point and feature point based on the calculated area in the frame. Their process follows three parts. The first part is about the calibration of the camera, in other words the intrinsic parameters calibration. The second part is to constitute a model for dis- tance measurement over the optical axis direction according to the mapping relationship between the objects in the camera coordinates frame and their projection in the pixel coordinate frame and the final part is about the absolute distance estimation. Aswini et al. [7], proposed an obstacle avoidance and distance measurement system using mono vision method. They measure the distance between the vehicle and the obstacle based on camera calibration techniques and the pixel variation in consecutive video frames using the key points extracted by SIFT and SURF algorithms. Huang et al. [8], proposed a mono vision system with instance segmentation and camera focal length to detect the cars distance in front of the current car. Their proposed system composed of three stages. In the first stage, the locations of the cars are extracted. In the second stage, the located cars are classified to get their types and their mask values using a model that is trained by the CompCars dataset to classify car types. Then, a new instance segmentation model by the Cityscapes dataset is used to get the car mask. In the third stage, the cars distances are calculated based on the relationship between the size in- formation of the different car types and their mask values. The incon- venient of the mono vision methods in estimating distances is that we should first not only detect the objects but also extract their types. Therefore, to recognize the detected objects types we need a huge dataset contains all types of objects (models of brands) and their dimensions, and even using a huge dataset it will be always an issue in estimating dis- tances of unknown objects. another inconvenient reside in the high complexity of the used algorithm to classify the objects types and also to match the real dimensions of the objects and the dimensions of the same objects in the images in different positions, especially when there is overlapping.
The stereo vision system is a computer vision system that is based on
stereoscopic ranging techniques to calculate the distance. This system use two cameras as one camera, trying to give the impression of depth and use the disparity of the objects between the cameras to compute the distance with high accuracy. Salman et al. [9], presented a distance estimation method based on stereoscopic measurement using trigono-
metric equations, their method is divided into three parts. The first part is
based on the object’s disparity values, the cameras focal length and other technical parameters. Nurnajmin et al. [11], are based on stereo vision method to estimate the distance and use a novel image template matching approach to increase the accuracy of the system. They used the Simulated Kalman Filter (SKF) algorithm for template matching which shows more efficient to solve the distance measurement problem. Mrovlje et al. [12], estimate the distance from the differences between the pictures taken by the two cameras and additional technical data like focal length and distance between the cameras.
Even if there are many existing works related to the object distance measurement, their distance measurement methods have used profes- sional cameras and their calculation formulas have contained complex computational terms that make the process time consuming. However, in this paper, we proposed an algorithm based on stereo vision for distance measurement using only web cameras, and the calculation formula contains simple terms based on web cameras criteria obtained by manual measurement (or noted in cameras package). With the proposed algo- rithm, the distance measurement accuracy is better than those of previ- ous works. The results of the experiment showed that by using the proposed method, we can accurately obtain inter-vehicles distances. Our method starts with capturing the images from the scene using both cameras. Then, a vehicle detection algorithm is applied to only one image. Next, a stereo matching algorithm is applied to detect and match the same vehicles in the other image. Finally, the horizontal centroids of vehicles in both images are used to measure the inter-vehicles distances. Fig. 1 shows the overall flow diagram of the method.

Stereo vision method

Stereo vision is a well-known technique that aimed at extracting depth information of a scene from two cameras horizontally aligned and vertically displaced from one to another, to obtain two different views of the same scene at the same time, similarly to our own eyes. The principal idea is to record a scene from two different viewpoints and to utilize the disparity to indicate the position, relation and structure of objects in scene. The difference between pixel positions in two images produces the impression of depth. The object’s distance is measured when it is in the overlapping field of views of the two cameras.
As shown in Fig. 2, the two cameras are horizontally mounted and separated by distance A. where h is the desired distance between the object and the cameras. To measure the distance h, we need these parameters:
B: the distance separates the object and the left camera. C: the distance separates the object and the right camera.
α; ϕ; θ : the angles of the triangle formed by the object and the two
cameras, as shown in Fig. 2.
Depending on the trigonometric functions, we have:

about applying some image processing methods to improve the compu- tational speed such as reducing the input image resolution and con- verting the input image from RGB domain to grayscale domain. The
sin φ ¼ h
B

h
(1)

second part is about extracting the object position from the two cameras. The third part is about finding out the state where the object is, depending on the object position. Then estimating the distance using state equation based on the trigonometric method. Hsu and Wang [10], proposed a stereo vision system for estimating an object distance based
on the cameras focal length and the disparity between the images. Their
sin θ ¼ C	(2)
So: h ¼ B ⋅ sin φ ¼ C ⋅ sin θ	(3)
According to the law of sines, we have:

proposed method is composed of four stages, the first stage is about applying some pre-processing methods on the images to reduce the
A


sin α
B
¼ sin θ
So:  B ¼ Asin θ
sin α
Eq.4

computational speed like down scaling the images size to certain level.
The second stage is the region segmentation where they divided the images into small blocks and they applied the local threshold selection algorithm to isolate the objects from the background. The third stage is about looking for the disparity information of each object between the
In the end, from Eq.3 and Eq. (4), we obtain:
h ¼ Asin θ  sin ϕ
sin α
A. Calculation of θ; ϕ and α:


Eq.5

two images by extracting their features then matching them, using spe- cific descriptors. The final stage is about computing the object distance
In Euclidean geometry, the sum of the angles of a triangle is invari-



Fig. 1. The Overall flow diagram of the proposed method.
ϕ = 01 + β1	Eq.7
and
θ = 02 + β2	Eq.8
According to Fig. 4: β = 180—ω
So, β1 = 180—ω1 and. β2 = 180—ω2
2	2
Now β1 and β2 are known, so we still need O1 and O2. These two angles can be obtained by multiplying the position of the object in both cameras (P1 and P2) by angles that correspond to each pixel in the two
cameras (Ap1 and Ap2), as shown below:
O1 = P1.Ap1	Eq.9
O2 = P2.Ap2	Eq.10
Therefore, we must calculate the angles Ap1 and Ap2. We have the
angle ω1 correspond to H1 pixels for the first camera and the angle ω2 correspond to H2 pixels for the second camera. So, AP1 and AP2 are defined by:


Fig. 2. Example of two cameras mounted as a stereo camera.

ably equal to the straight angle, so we have:
θ + α + ϕ = 180∘	Eq.6
Ap1 = ω1
H1
Ap2 = ω2
H2

Eq.11


Eq.12

According to Eq. (6), when we get θ and ϕ we can conclude α. Based on Fig. 3, we calculate the angles in question.
According to Fig. 3:

ω1, ω2: the view angles of the two cameras respectively.
H1, H2: the number of horizontal pixels of the two cameras respectively.
P1, P2: the position of the object in both cameras, where P1 is the distance in pixel between the centroid of the object and the end of the overlap area for the camera on the left.
P2 is the distance in pixel between the centroid of the object and the beginning of the overlap area for the right camera.

According to Fig. 3, we have:
So, according to Eq. (7), Eq. (8), Eq. (9), Eq. (10), Eq. (11) and Eq.

Fig. 4. The angles of the camera.



Fig. 3. Illustration of the angles used for computing the distance.



(12):
ϕ = P1. ω1 + β1	Eq.13
1
θ = P2. ω2 + β2	Eq.14
H2
Now, we have ϕ and θ. However, we still need α . According to Eq. (6) we get:
α = 180 — ϕ + θ = 180 —  P1. ω1 + β  + P2. ω2 + β	Eq.15
selected object in the left camera is the same one in the right camera, when there are multiple objects. Therefore, before measuring the objects distances, we need to define the location of the same object in the two images. In such systems, objects detection methods are applied to images captured by both cameras. Then, they apply some stereo matching al- gorithms to match the detected objects in both cameras, which consume time. However, the main idea in this paper is to detect vehicles by applying the vehicle detection method [1] to the images captured by single camera. Then, match them with the same vehicles in the images captured by the other camera. This matching is done by performing the
cross-correlation technique between the detected vehicle in the images

H1	1
H2	2
taken by the first camera and the same horizontal position in the images

Finally, according to Eq. (5), Eq. (13), Eq. (14) and Eq. (15) the dis- tance h is defined as below:
cross-correlation function varies between +1 and —1, where the best taken by the second camera, as shown in Fig. 6. In principle, the
correlation state is identified when the cross-correlation function takes

Asin  P2 . ω2
h
+ β2  sin  P1 . ω1
+ β1 
values close to +1. Therefore, the best match is detected when the result
of performing the cross-correlation technique takes the maximum value

=
sin
 180 —
 P2 . ω2
+ β2 + P1 . ω1
+ β1
	Eq.16
greater than a predefined threshold. However, no match is detected when the result takes a value less than the predefined threshold. In other

The distance to the object can be calculated easily as given in Eq. (16) by considering view angles of both cameras, distance between cameras and the object positions in both cameras, which are the only terms in the distance calculation formula (Eq. (16)) that has to be calculated while all the other terms are already known.

Object recognition

Object detection

Detecting objects is an important task in distance measurement sys- tems where the performance of vehicle detection algorithm acts in pro- portion to the distance measurement performance. Therefore, before measuring the vehicle distance, an efficient vehicle detection algorithm is applied [1]. This algorithm is composed of two steps: hypothesis generation step and hypothesis verification step. In the hypothesis gen- eration step, potential locations of vehicles (hypotheses) are generated, this generation is based on matching vehicles templates with the images using cross-correlation [13] after performing a pre-processing using edge detection. In the hypothesis verification, the generated hypotheses in the first step are verified by performing two operations: features extraction and classification. The third level of two-dimensional discrete wavelet transform [14] is performed to extract features from the generated hy- potheses then use them to classify the hypotheses as vehicles or non-vehicles using AdaBoost classifier. In stereo vision system for dis- tance measurement, the object detection methods are applied to images captured by both cameras, which consume time. However, in our pro- posed method, object detection method is applied only to images captured by one camera then stereo matching method is performed which obviously reduce the treatment time. The Fig. 5 shows the overall flow diagram of this process.


Stereo matching

The problem we may face in such systems is how to know that the

Fig. 5. The Overall flow diagram of the vehicle detection process.
words, the vehicle is detected outside of the overlapping field of views of both cameras.

Experiment results

Equipment setup

Stereoscopy is an important technique used to obtain the illusion of depth by using two images from two slightly offset positions (stereo- scopic images), which permit us to measure the distance between the stereo camera and the chosen object using the proposed method.
The stereoscopic images may be captured using two cameras (stereo camera) mounted similarly as human eyes. The most important thing to capture stereoscopic images is how are the two cameras mounted? Here are the criteria should be respected while mounting the two cameras:
The cameras should be mounted at the same level.
The cameras should be mounted at the same horizontal position.
The cameras should be vertically displaced by a predefined distance.
The pictures should be captured from both cameras at the same time. This paper uses C++ and OpenCV as programing language, to test the proposed method. The device used in the implementation is 1.2 GHz
Dual-core ARM Cortex-A9 (HPS) that runs under LXDE desktop with 1.0 GB memory DDR3. The HPS is located in a VEEK-MT2S that is composed of DE10 standard FPGA and the MTLC2 module provided by TERASIC.

Performance metrics

The used cameras are two web cameras contain color CMOS image
per second  their  horizontal  view angle is  60◦  degree. sensor that emit color images with resolution of 640x480 up to 30 frames
The experiments led us to test the proposed method accuracy for
measuring the distances of objects with the impact of changing the base (distance between the cameras). The proposed system has been tested in several scenes in cars parking. Therefore, we took several shots of each scene by changing base. The following figure (Fig. 7) shows some scenes used as test.
The Table 1 shows some measured distances compared to the real distance.
The results presented in the Table 1 shows that the distance mea- surement error depends on the chosen base and also depends implicitly on object detection quality. The use of several values in the base gave good results. However, we observe the distance was computed accurately with low error using 0,6 m in the base.
The experiments led us also to measure the rapidity of the proposed method by computing the number of frames treated per second.



Fig. 6. Stereo matching process.


Fig. 7. Some examples of test scenes.

Table 1
The measured distance in various base length.
Base	Measured distance (m)	Real distance (m)


To ensure this part of experiment, we have used three different videos sequences taken from road. The Fig. 8 shows some used scenes.
The following figure shows some statistics of the number of frames treated per second.
The Fig. 9 shows that the proposed method could treat up to 23 frames per second. the average of frames per second through all the ex- periments is 20.57 frames per second which is enough for real time treatments.

Evaluation results

To evaluate our work, we compared it with three works that we have implemented and adapted to our dataset. The method proposed by Hsu
and Wang [10] is based on the focal length of the camera, the disparity and the base which is a fixed parameter. Mrovlje and Vranˇci´c [12] pre- sented a method measures the distance using formula based on the base
and the tangent of angle formed by the view angle bisector and the ob- ject. Salman et al. [9] presented a method of measuring distance based on trigonometric calculations depending on which state is the detected object.
Table 2 shows the results of these three works compared to the results
of our work in different scenes. This comparison shows that even our method is simple, it has the least error and that our results are more accurate than the other results.

Conclusion

A Real-time distance measurement method for self-driven system is introduced in this paper. The utilized method is based on using stereo camera, which is two cameras mounted in the same horizontal position and displaced vertically by a predefined distance (the base). To measure the distance to vehicles, a vehicle detection method is performed first following two steps: hypothesis generation and hypothesis verification. In the first step, the hypotheses are generated using cross-correlation after performing an edge detection method. In the second step, the generated hypotheses are verified by extracting the desired features using the third level of 2D-DWT and then classify them using AdaBoost classifier. Several methods apply the detection task on both images, which is time consuming. However, in this paper the vehicles are detected first in only one camera then the similar vehicles are detected on the other camera using a stereo matching method. After detecting and matching the same vehicles in both cameras, the distance measurement




Fig. 8. Some examples of test scenes from the road.

method based on the distance between the two cameras, the position of vehicles in both cameras and certain geometric angles, is performed. Although the method is based on relatively simple algorithm, the dis- tance is measured accurately. Furthermore, a comparison between the proposed method and some other methods from literature was performed to evaluate the proposed method, where it showed that despite the simplicity of the proposed method, it measures the distance with high accuracy. The proposed method may be used to perform several tasks in several systems such as computing safety distance between vehicles and vehicles speed and it may also be used to measure objects distances regardless of their types by simply changing the detection algorithm.

Data availability




Fig. 9. Statistics of the frames treated per second according to the detec- ted vehicles.




Table 2
The evaluation results of four distance measurement methods.
The data used to support the findings of this study are available from the corresponding author upon request.

Declaration of competing interest

The authors declare that there are no conflicts of interests exist.

CRediT authorship contribution statement

Abdelmoghit Zaarane: Conceptualization, Methodology, Resources, Software, Formal analysis, Writing - original draft, Writing - review & editing. Ibtissam Slimani: Methodology, Software, Formal analysis, Writing - original draft, Writing - review & editing. Wahban Al Okaishi: Investigation, Resources, Writing - review & editing. Issam Atouf: Validation, Visualization, Supervision. Abdellatif Hamdoun: Valida- tion, Visualization, Supervision.



References

Zaarane Abdelmoghit, Slimani Ibtissam, Hamdoun abdellatif, et al. Real-time vehicle detection using cross-correlation and 2D-DWT for feature extraction.
J. Electrical and Computer Eng 2019;2019. https://doi.org/10.1155/2019/ 6375176.
Slimani Ibtissam, Zaarane Abdelmoghit, Hamdoun Abdellatif, et al. Traffic surveillance system for vehicle detection using discrete wavelet transform. J Theor Appl Inf Technol 2018;96(17).
et Prakoso Puguh Budi, Sari Yuslena. Vehicle detection using background subtraction and clustering algorithms. Telkomnika 2019;17(3).
et Carullo Alessio, Parvis Marco. An ultrasonic sensor for distance measurement in automotive applications. IEEE Sens J 2001;1(2):143.
Nakahira Kenji, Kodama Tetsuji, Morita Shin, et al. Distance measurement by an ultrasonic system based on a digital polarity correlator. IEEE Trans on Instrumentation and Measurement 2001;50(6):1748–52.
Zhang Zhisheng, Han Yanxiang, Zhou Yifan, et al. A novel absolute localization estimation of a target with monocular vision. Optik-Int J Light and Electron Optics 2013;124(12):1218–23.
Aswini N, Uma SV. Obstacle avoidance and distance measurement for unmanned aerial vehicles using monocular vision. Int J Electr Comput Eng 2019;9(5):3504. https://doi.org/10.11591/ijece.v9i5.pp%25p.
Huang Liqin, Chen Yanan, Fan Zhengjia, et al. Measuring the absolute distance of a front vehicle from an in-car camera based on monocular vision and instance segmentation. J Electron Imaging 2018;27(4). 043019.
et Salman, Dawood Yasir, Ku-Mahamud, Ku Ruhana, Kamioka Eiji. Distance measurement for self-driving cars using stereo camera. In: Proceedings of the 6th international conference on computing and informatics, ICOCI; 2017.
et Hsu Tsung-Shiang, Wang Ta-Chung. An improvement stereo vision images processing for object distance measurement. Int J Automation and Smart Technol 2015;5(2):85–90.
Ann Nurnajmin Qasrina, Pebrianti Dwi, Bayuaji Luhur, et al. SKF-based image template matching for distance measurement by using stereo vision. In: Intelligent manufacturing & mechatronics. Singapore: Springer; 2018. p. 439–47.
et Mrovlje Jernej, Vrancic Damir. Distance measuring based on stereoscopic pictures. In: 9th international PhD workshop on systems and control: young generation viewpoint; 2008. p. 1–6.
Wei S-D, Lai S-H. Fast template matching based on normalized cross correlation with adaptive multilevel winner update. IEEE Trans Image Process 2008;17(11): 2227–35.
Slimani I, Zaarane A, et Hamdoun A. Convolution algorithm for implementing 2D discrete wavelet transform on the FPGA. In: Computer systems and applications (AICCSA), 2016 IEEE/ACS 13th international conference of. IEEE; 2016. p. 1–3.
Slimani I, Zaarane A, Hamdoun A, April Atouf I. Vehicle License Plate Localization and Recognition System for Intelligent Transportation Applications. IEEE; 2019.
p. 1592–7.
Al OW, Zaarane A, Slimani I, Atouf I, Benrabh M. Vehicular queue length measurement based on edge detection and vehicle feature extraction. J Theor Appl Inf Technol 2019;97(5).
