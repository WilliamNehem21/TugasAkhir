Array 11 (2021) 100072

		




Emotion recognition from EEG-based relative power spectral topography using convolutional neural network
Md. Asadur Rahman a, Anika Anjum b, Md. Mahmudul Haque Milu c, Farzana Khanam c,
Mohammad Shorif Uddin d,*, Md. Nurunnabi Mollah e
a Department of Biomedical Engineering, Military Institute of Science and Technology (MIST), Dhaka, 1216, Bangladesh
b Department of Biomedical Engineering, Khulna University of Engineering & Technology (KUET), Khulna, 9203, Bangladesh
c Department of Biomedical Engineering, Jashore University of Science and Technology (JUST), Jashore, 7408, Bangladesh
d Department of Computer Science and Engineering, Jahangirnagar University (JU), Dhaka, 1342, Bangladesh
e Department of Electrical and Electronic Engineering, Khulna University of Engineering & Technology (KUET), Khulna, 9203, Bangladesh



A R T I C L E I N F O

Keywords: Electroencephalography (EEG) Emotion recognition
Relative power spectral density Convolutional neural network (CNN) SEED dataset
A B S T R A C T

Emotion recognition, a challenging computational issue, finds interesting applications in diverse fields. Usually, feature-based machine-learning methods have been used for emotion recognition. However, these conventional shallow machine learning methods often find unsatisfactory results as there is a tradeoff between feature di- mensions and classification accuracy. Besides, extraction and selection of features from the spatial and frequency domains could be an additional issue. This work proposes a method that transforms EEG (electroencephalog- raphy) signals to topographic images that contain the frequency and spatial information and utilizes a convolu- tional neural network (CNN) to classify the emotion, as CNN has improved feature extraction capability. According to the proposed method, the topographic images are prepared from the relative power spectral density rather than power spectral density that shows remarkable improvement in classification accuracy. The proposed method is applied to the well-known SEED database and has given outperforming results than the current state-of- the-art.





Introduction

Emotion is a complex state of mind that is associated with someone's surroundings, thoughts, feelings, and circumstances and it results in physical and psychological changes. Some researchers have found that emotion is a cognitive task [1–3]. A human can understand emotion through the behavior, mood, and temperament of another person, but it is hard for a machine to understand human emotion unless the human makes the machine intelligent enough to decode emotions. This method for the machines can be named as human emotion recognition. In the case of the treatment of patients with expression problems, recognition of a real emotional state by a machine is helpful for providing better medical treatment. There are some psychophysiological studies [4–6] where they found a strong correlation between emotion recognition and brain activities.
In recent years, EEG modality has gained much attention for measuring brain activity in a precise manner along with setting a
communicational pathway between humans and machines. Among various modalities, this method is very much promising because of its high accuracy and objective evaluation comparing with other external appearances like facial expression and gesture [7]. But, the raw EEG signal is contaminated with artifacts and this signal is complex due to its variation with time and space. The feature extraction and classification of EEG signal have two important aspects which must be ensured in emotional state recognition. Specified domain knowledge is required for conventional manual feature extraction and feature selection. However, the cost of conventional feature selection increases at a quadratic rate with the increase of the number of features [8]. In most of the studies for feature extraction, researchers have only focused on time [9–14], fre- quency [15–17], or time-frequency [18,19] domains of the EEG signal, and rarely focused on the spatial dimension. A multichannel EEG system acquires data from the different spatial locations of the human brain. So, the motivation of this work is to combine time, frequency, and spatial domains to classify the emotional state from the multichannel EEG



* Corresponding author.
E-mail addresses: bmeasadur@gmail.com (Md.A. Rahman), anikaanjum123@gmail.com (A. Anjum), mahmudhmilu@gmail.com (Md.M.H. Milu), farzanabme@ just.edu.bd (F. Khanam), shorifuddin@gmail.com, shorifuddin@juniv.edu (M.S. Uddin), nurunnabim12@gmail.com (Md.N. Mollah).
https://doi.org/10.1016/j.array.2021.100072
Received 24 October 2020; Received in revised form 7 May 2021; Accepted 31 May 2021
Available online 10 June 2021
2590-0056/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).


signals with high accuracy.
Several studies have considered body movement, voice tone or facial expressions, and physiological activities to recognize emotions [20–23]. These physical changes had some common complications as they did not show reliable emotion. Our state of mind could only be a reliable way to understand the emotional state that could have been revealed by the brain signal like EEG. Some research works [24–26] are accomplished so far to construct machine learning-based predictive models to classify the emotional states from the EEG signals. A study led by Wang et al. [24] reported the alpha, beta, and gamma bands as features to classify four emotions (joy, relax, sad, and fear) and found 66.5% (in average) accuracy by support vector machine (SVM) using different kinds of emotional video as stimuli. Another study done by Lin et al. [25] achieved 82.29% accuracy using the same classifier. Their experiment included 26 subjects and used 30 s long music samples as stimuli. To achieve a highly accurate predictive model, recent studies prefer the deep neural network as a feature extractor and classifier. Zheng et al. [26] extracted density entropy of the multi- channel EEG signals using a short-time Fourier transform and used deep belief network (DBN) as a classifier to classify positive, negative, and neutral emotions and obtained 86.65% accuracy. A recent study [27] has proposed to construct topographic images from the multichannel EEG signals to feed them into the deep neural network which proves to achieve a high classification accuracy than the current state of the arts. The method describes in Ref. [27] used power spectral density (PSD) to form the topographic image which is slightly questionable to attain high-level ac- curacy, as the value of the PSD can vary with the individuals, time, and system. Therefore, using relative PSD (RPSD) is more technically sound to ensure the power level variation measurement of the concerned band with respect to the full band power in case of EEG signals [27,28]. Therefore, feature extraction, feature selection, and setting of the proper classifier are the most important issues to construct a machine learning-based predictive model for emotional state classification.
To overcome these issues, the present work proposes a mechanism to
combine the time, frequency, and spatial domain of the multichannel EEG emotional signals into topographic images. The topographic image represents the RPSD of each trial of an emotional state that needs time, frequency, and spatial information. There may arise an argument for using the RPSD instead of PSD. Although a beautiful explanation is given in Refs. [27,28], a graphical realization is presented in the Results and Discussions Section to explain the effect of PSD and RPSD through power level variation. This graphical presentation clarifies how RPSD improves the feature quality of the EEG signal power distribution.
Many recent research works [29–31] recommend that the convolu- tional neural network (CNN) method is very effective in feature extrac- tion and classification tasks. For this, utilizing the automated feature extraction and classification facilities of the CNN, the topographical images are classified. The main contributions of this work are as follows:
A method is proposed that utilizes the RPSD of each spatial position of EEG data on the brain (channel) to prepare the topographic images.
A CNN has been applied for the feature extraction and emotion classification from the topographic images.
We compared our work with the other recent works on emotion recognition using the SEED dataset.

The article is organized as follows: Section 2 describes the data collection, representation, and the proposed methodology, Section 3 presents the results of this research work and compares its findings with the findings of the recent works, finally, Section 4 concludes the paper with future research directions.

Materials and methods

Dataset description

The proposed methodology of this work has been conducted on the

among them eight are females and seven are males (Age ¼ 23.27 2.37 SEED dataset [32]. In the data acquisition, there are fifteen subjects, years). All participants are right-handed native Chinese students of
Shanghai Jiao Tong University. They are reported as normal or corrected to normal vision and normal hearing.
Various stimuli can be utilized in emotion-related research such as movie clips, music, and verbal command, etc. Among them, the movie clip has greater efficiency and reliability [33] as it contains both audio and video. In this experiment, Chinese movie clips were selected since all the participants were native Chinese. The videos are of 4 min long and can be categorized as: positive, negative, and neutral. The following se- lection criteria were maintained-
The total time of the experiment should be small otherwise subjects might become fatigued.
The movie clips should be well understood without clarification.
The clips should stimuli a single target emotion. It should not contain mixed emotions.

Each film is edited to create coherent emotion-eliciting and maximize emotional meanings. The details of the film clips used in the experiments are given in Table 1.
Each subject experienced three experiments: positive, negative, and neutral. So, 15 subjects experienced 45 experiments in total. There is a total of 15 trials for each experiment. Each trial is started with 5 s hint of starting. Then, film clips are played for 4 min and 45 sec is allotted after each clip for feedback. Within this period, participants are asked to report their emotional reactions toward the shown video clips. The order of presentation is arranged in such a way that two clips targeting the same emotion are not shown consecutively. The detailed protocol is given in Fig. 1.

Data acquisition and processing

The experiments were performed in three sessions for each subject. The time interval between each session is one week or longer. This process ensured stable patterns of neural activities across sessions and individuals. Facial videos and EEG data are recorded simultaneously. Subjects are seated in front of a big screen where movie clips are shown. EEG data is recorded using an ESI NeuroScan System at a sampling rate of 1000 Hz from a 62-channel active AgCl electrode cap following the In- ternational 10–20 System [34]. The SEED dataset contains a down- sampled, preprocessed, and segmented version of EEG data. It is downsampled to 200 Hz. A bandpass filter of 0–75 Hz is applied to remove any artifacts. There are 45 mat files in total, one per experiment. Each subject file contains 15 arrays. In this work, we have used a band-pass filter (3–30 Hz) to remove unnecessary frequencies. In the EEG signal, up to 3 Hz represents the Delta Band, which is present in deep sleep or coma. As subjects were awake and alert during trials, Delta Band is rejected in our processing. Frequencies higher than 30 Hz are also removed as it does not represent any brain activity regarding emotional effects.

Base construction of the EEG topography

is given in Fig. 2. It can be generalized through a matrix of m × n. Here, m An illustration of 62-electrode placement regarding the SEED dataset vertical test points, respectively. In this work, m × n equals to 9 × 8 and n represent the maximum-point number in horizontal test points and matrix. The red circled points in Fig. 3 are the EEG electrodes of the SEED
dataset. These points indicate the value of relative PSD of the EEG signals
plete 9 × 8 matrix. These points are the interpolation of the surrounding of corresponding electrodes. The gray points are added to form a com- red points. The method of EEG topography construction is shown step-
wise in Fig. 4. A pseudo-code for the topographic image formation from the multichannel EEG signals is added in Appendix A.


Table 1
Exemplary movie clips for the positive, negative, and neutral emotional stimulation used in the experiment.
Examples of the movie clips of Positive, Negative, and Neutral emotions
Positive Clips	Negative Clips	Neutral Clips
Flirting Scholar	Back to 1942	World Heritage in China
		
Lost in Thailand	Tangshan Earthquake	World Heritage in China



The construction of some new points using specific neighborhood discrete data points is found by interpolation. To make 72 points of the 9
× 8 matrix, 10 additional points are calculated through interpolating the
neighborhood points. The interpolation process of gray points can be
calculated as:
The frequency content of the signal x(t) is _x(ω) which is calculated by Fourier transformation. Then, the PSD can be calculated as follows [35,
36]:
Sxx(ω)= lim E |x(ω)|2	(3)
T→∞

'	'	'	'

Γ(m; n)= Γ (m + 1; n)+ Γ (m — 1; n)+ Γ (m; n + 1)+ Γ (m; n — 1)
K
(0 ≤ m; n ≤ 8; m; m ∈ N)
(1)
The ratio of the PSD of the band of interest (PSDBOI) and the PSD of the total frequency band (PSDtotal) is called RPSD. The value of the PSD varies from person to person and in the case of the same individual, it varies from time to time. Thus PSD is not a reliable source of information

where, Γ (m, n) presents the required value of the gray points, Γ’ (m, n)
are the values of the point surrounding Γ (m, n). Here, K is the number of non-zero elements in the numerator.

2.4 RPSD calculation and normalization
irrespective of person and time [27,28]. RPSD solves this problem by
comparing the PSD of the concerned band with respect to the PSD value of the total frequency range of the signal [27,28]. Therefore, RPSD can be presented as (4).
PSDBOI


PSD of a signal means the distribution of power over its frequency
RPSD =
total
(4)

components that represents the impact of the frequency components included in the signal. Let, P is the average power of a signal x(t), then the power for the total time period T is,
To reduce inter-participant variability, the RPSD of each subject can be normalized by scaling between 0 and 1 [34] as given in (5).
'  π — πmin 


T
P  lim  1
T→∞T
0
|x(t)|2dt	(2)
π = πmax — πmin	(5)
Whereπ' is the normalized value of the feature; πmax , πmin are the
maximum and minimum value of the subject features, respectively. Using




Fig. 1. Each subject faced 45 trials in total. No consecutive trial was targeted at the same emotion.




Fig. 2. The positions of the 62 electrodes of the data acquiring device according to the international 10–20 method.





Fig. 3. Feature matrix of EEG is made considering RPSD as a feature. The shown gray points are interpolated from the surrounding red points. Ten new points are interpolated during this topographic image construction.



normalized RPSD values of 72 positions (as shown in Fig. 3) the EEG topographic images were constructed.

2.5. Construction of CNN-based classifier

For automatic feature extraction and classification CNN is used in this
work. CNN is a type of machine learning system in which a model learns automatically to classify objects from images, numerical values, or videos. It is highly capable of learning from the input data by optimizing the weight parameters of each filter by minimizing the classification error. CNN consists of an input layer and an output layer, along with multiple hidden layers. It takes an image as an input, then processes it




Fig. 4. Flow diagram of the procedure to construct the topographic image from multichannel EEG.




Fig. 5. Different layers of a convolutional neural network consisting of an image as an input layer, different filters in convolution, ReLU and pooling layer, and finally output layer with the final score.


through multiple hidden layers. It gives the output as a probable class name. The hidden layers of CNN typically consist of convolutional layers, ReLU layers, pooling layers, a fully connected layer, a batch normaliza- tion layer [37]. A generalized pattern of the layers in a CNN is presented in Fig. 5. The design details of the different layers regarding this work are discussed briefly in the following.

CNN. A color image can be represented as m × n × c where m is the width In this work, topographic images are considered as the input of the image has c = 3. Here, the input image size is 192 × 192 × 3. The and n is the height of the image and c is the number of channels e.g. RGB convolutional layer works for feature extraction. It is considered the core
building block of the CNN architecture [38]. Convolutional layers




Fig. 6. The values of the different parameters of the layers in the proposed CNN along with the clarification of the regarding layers.


Fig. 7. Comparison between the raw EEG data and the filtered EEG signal. EEG signal is filtered with a band-pass filter of band 8–32 Hz.




Fig. 8. An example of an EEG topographic image. AF7, AF5, AF6, AF8, PO9, PO10, CB5, CB3, CB4, and CB6 are the interpolated points from surrounding points.



Fig. 9. Comparison among topographic images of (a) negative, (b) neutral, and (c) positive emotions. The center part of the brain which contains a limbic system is always active during these emotions as this part is responsible for creating the emotions.




Fig. 10. RPSD- and PSD-based topographs presenting the variation of EEG signals' spatial power level distribution for positive, negative, and neutral emotions. These are the corresponding results of Subject 3.



from the previous layer. This convolutional layer convolves the 192 × 192 × 3 image by moving a filter along with the vertical and horizontal transform the input data by using a patch of locally connecting neurons
filter size of the input argument is specified as 8 × 8. Stride is known as the step size with which the filter moves. For 8 × 8 filter scanning input image. While creating a convolutional two-dimensional layer, the
through the input image, the stride of 1 is used in this research.
In dilated convolution, the filters are expanded by inserting spaces between the elements without increasing the number of parameters or computation. We apply ([3,3,4,4]) zero padding to input image borders
Here, g represents the input image matrix to be convolved with the kernel matrix h to result in a new matrix G.
In our proposed CNN structure, the input channel is normalized by a batch normalization layer. It is generally used between the convolutional layer and the ReLU layer to speed up the training process of CNN and reduce the sensitivity to network initialization. A batch normalization layer normalizes its inputs by calculating the mean and variance over a mini-batch and each input channel. Then it calculates the normalized activations as [39],
 pi — μ 

to add zero values vertically and horizontally. The output size of this	B
2
designed convolutional layer can be equated by:	B
(8)

Output Size =
1
S + 1 (IS — ((FS — 1) * DF + 1)+ 2P)	(6)
Here, pi= inputs; μB = mean; σ2= variance; ε improves numerical stability when the mini-batch variance is very small. The rectified linear
unit or ReLU was used as an element-wise activation function over the

Here, IS = Input size of image; S = Stride; FS = Filter size; DF =
Dilation factor; P = Padding. So, the mathematical formula of the two- dimensional convolutional layer is:
G[m, n]=	h[i, j].g[m — i, n — j]	(7)
i=—∞ j=—∞
input data thresholding. Running ReLU over the input volume changes the pixel values but does not change the spatial dimension of the input data in the output. ReLU is more preferred than other functions because it makes the neural network faster [39].
The details of these layers in our proposed CNN model are shown in Fig. 6. The fully connected layer is used to compute class scores that can




Fig. 11. (a) Accuracy and (b) loss of the training and validation of the proposed CNN structure with respect to iterations.



Table 2
Classification accuracy for 15 subjects using CNN.
Subject	Accuracy with 25% data for training and 75% data for testing (%)




Accuracy with 50% data for training and 50% data for testing (%)
among topographic images of negative, neutral, and positive emotions is shown in Fig. 9. One thing to notice in those images is that the central part of the brain is always active in all three emotions. The central part of the brain or medial temporal lobe is consists of the limbic system [40]. It has been said that the limbic system is responsible for emotion formation

1	82.79	93.48
2	90.91	100
3	84.85	92.48
4	90.85	95.24
5	84.70	100
6	90.91	90.71
7	83.82	90.71
8	84.85	95.24
9	88.82	100
10	93.94	90.48
11	92.91	95.24
12	85.73	88.67
13	90.88	94.48
14	96.97	94.48
15	92.91	98.24
[41]. Thus, despite the emotion type, the limbic system is always active. Another important issue is to use the RPSD instead of PSD for making the topographical images. It is a hypothesis that power level distribution- based images are better in the case of RPSD than that of the PSD. Based on this argument, we have presented PSD- and RPSD-based topograph- ical images in Fig. 10. It is clearly observable that power level distribu-
tion is more precise in the case of RPSD than that of the PSD.
The topographic images are given as input to the CNN for feature extraction and emotion classification. To feed the CNN, there were three types of EEG data corresponding to three emotional states of the brain. Since each participant took part in 3 different emotional states (positive, negative, and neutral) and there are 15 trials for an individual state. So,
there are 15 × 3 = 45 multichannel EEG data from each participant.

Average Standard Deviation
89.056  4.32	94.63  3.68
The accuracy and loss of the proposed CNN structure with respect to iterations are presented in Fig. 11(a) and Fig. 11(b), respectively. Here, the maximum epoch is considered 15. We have selected the filter size of


ume is 1 × 1 × N, where N is the number of output classes (here the number     of     classes     N     =    3). be used as the output of the network. The dimension of the output vol-
Results and discussions

All steps of the processing, image formation, feature extraction, and data classification were performed in Matlab 2018a. Data utilized in this research work has been collected from the SEED dataset. Some pre- processing was already applied to the dataset e.g. data were down- sampled to 200 Hz, bandpass filter was applied from 0 to 75 Hz to remove artifacts. Despite this preprocessing, some other processing were also done. A bandpass filter was applied from 8 to 32 Hz to remove noise. A graphical representation of raw and bandpass filtered EEG signals is shown in Fig. 7.
Fig. 8 shows one sample of EEG topographic image, where 10 new data points are interpolated from the neighbor points: AF7, AF5, AF6, AF8, PO9, PO10, CB5, CB3, CB4, and CB6. The RPSD of each electrode is calculated and normalized within the range of 0–1. Each RPSD value is mapped in a two-dimensional plot according to the locations of each electrode. These locations are given specific names. A color bar is also shown in Fig. 8 to represent the activity level of the different positions of our brain during different emotions.
Different emotions create different effects on the human brain. A sad movie can make someone emotionally unstable while a natural scene can create a soothing effect on our brain. The state of an emotional condition changes the EEG result taken during different emotions. A comparison
the convolutional layer based on the trial and error method. Each result is checked using the Matlab simulator several times to select the best pattern. The convolutional layer, batch normalization layer, ReLU layer, pooling layer are used twice in this work. The final results are the clas- sification accuracies against the three classes: positive, negative, and neutral emotional states. Therefore, the fully connected layer produces 3 output classes.
We have done experimentation with the proposed CNN in two sce- narios. In the first scenario, 25% of data are used for training, and the rest 75% are used for testing. Similarly, in the second scenario, 50% of data are used for training, and the rest 50% are used for testing. The training and testing data were chosen randomly five times for each participant and the resultant classification accuracy is tabulated from the average value of the corresponding five classification accuracy of each partici- pant. The classification accuracy of the proposed method is presented in Table 2. We found that the average classification accuracy achieved by our proposed work is 89% for the first scenario and 94% for the second scenario. This outcome is the highest classification accuracy with respect to the other recent methods applied to the SEED dataset so far. The comparison of the methods and their classification accuracies are shown in Table 3.

Conclusion

In this research work, human emotion has been detected from multichannel EEG signals. The multichannel emotional EEG signals were mapped into two-dimensional tomographic images using RPSD. These images combined frequency and spatial domain information of the EEG




Table 3
Comparison with other recent methods with SEED dataset.
Author & Study	Method	Classifier	Average Accuracy (%)
W. Zheng et al., 2017 [13]	Group Sparse Canonical Correlation Analysis	86.65

W. L. Zheng et al., 2017 [14]	Differential Entropy as Features	Discriminative Graph regularized Extreme Learning
Machine
79.28

W. L. Zheng et al., 2015 [17]	Critical Frequency Band Investigation	Deep Belief Network	86.08  8.34
Y. M. Jin et al., 2020 [30]	Differential Entropy	Domain Adaptation Network	79.19
Y. Yang, 2018 [42]	Differential Entropy as Features	Hierarchical Network with Subnetwork Nodes	86.42

M. A. Rahman, 2019 et al. [43]
PCA and t-statistics-Based Feature Selection Method
SVM and ANN	85.85  5.72 and 86.57  4.08

Proposed Method	RPSD-Based Topographic Image for Emotional EEG
Data



SVM: Support Vector Machine [44,45], ANN: Artificial Neural Network [44,45].
CNN	89.056  4.32 (25% data for
training)
94.63 3.68 (50% data for training)



signals. For feature extraction and classification CNN was used in this work and found a significant enhancement in the classification accuracy. Compared with the other recent methods that were applied on the SEED dataset, the proposed method achieved the highest classification accu- racies 89.056% 4.32 (using 25% data in training and the rest 75% in testing) and 94.63% 3.68 (using 50% data in training, and rest 50% in testing). From this convincing output, it is expected that the proposed expert system would work efficiently in other types of EEG signal clas- sification, which will be our next focus.

Declaration of competing interest

The authors have no conflict of interest regarding this publication.

Funding Acknowledgement

No funding organization/institute supported this research work.

Appendix A

Pseudo Code:
EEG topographic image (used in MATLAB R2018a):

data= load the RPSD values of total estimated channel; % Load your 2D EEG data
xc= [horizontal axis coordinates]; % get the x-axis yc= [vertical axis coordinates]; % get y-axis points
trlen= normalized value of each electrode; % find the normalized
xi=linspace(min(xc),max(xc),30); yi=linspace(min(yc),max(yc),30); value
[XI, YI]=meshgrid(xi,yi); % Create a mesh with xi and yi
zc = griddata(xc,yc,trlen,XI,YI,'natural’); % relative power of each electrode
[cs,hh]=contourf(XI,YI,ZI,20,'LineStyle','none’); % contour plot of the data
set(hh,'EdgeColor','none') shading interp
set(gca,'Visible','off’); colormap(jet); set(gcf,'PaperUnits','inches','PaperPosition',[0 0 2 2]);
print(1,'-dpng’, '.png','-r0');
Author statement

Md. Asadur Rahman: Conceptualization, Methodology, Formal anal- ysis, Software, Investigation, Resources, Draft preparation. Anika Anjum: Methodology, Formal analysis, Software, Investigation, Draft prepara- tion. Md. Mahmudul Haque Milu: Formal analysis, Data curation, Investigation, Draft preparation. Farzana Khanam: Formal analysis, Data curation, Investigation, Draft preparation. Mohammad Shorif Uddin: Validation, Visualization, Review & Editing. Md. Nurunnabi Mollah: Supervision, Review & Editing

Funding

No funding was received for this research.

Informed consent

The studies reported in this work did not require any informed consent.

Ethical approval

This article does not contain any studies with human participants or animals performed by any of the authors. It used a publicly available
dataset. Proper acknowledgments with citation guidelines are main- tained for the use of this dataset.

References

Lazarus R. Emotion and adaptation. USA: Oxford University Press; 1991.
Mueller SC. The influence of emotion on cognitive control: relevance for development and adolescent psychopathology. Front Psychol 2011;2:327. https:// doi.org/10.3389/fpsyg.2011.00327.
Tyng CM, Amin HU, Saad MNM, Malik AS. The influences of emotion on learning and memory. Front Psychol August 2017;8(1454):24. https://doi.org/10.3389/ fpsyg.2017.01454.
Sammler D, Grigutsch M, Fritz T, Koelsch S. Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music. Psychophysiology 2007;44(2):293–304. https://doi.org/10.1111/j.1469- 8986.2007.00497.x.
Knyazev GG, Slobodskoj-Plusnin JY, Bocharov AV. Gender differences in implicit and explicit processing of emotional facial expressions as revealed by event-related theta synchronization. Emotion 2010;10(5):678–87. https://doi.org/10.1037/ a0019175.
Mathersul D, Williams LM, Hopkinson PJ, Kemp AH. Investigating models of affect: relationships among EEG alpha asymmetry, depression, and anxiety. Emotion 2008; 8(4):560–72. https://doi.org/10.1037/a0012811.
Ahern GL, Schwartz GE. Differential lateralization for positive and negative emotion in the human brain: EEG spectral analysis. Neuropsychologia 1985;23(6):745–55. https://doi.org/10.1016/0028-3932(85)90081-8.
Dash M, Liu H. Feature selection for classification. Intell Data Anal 1997;1(3): 131–56. https://doi.org/10.1016/S1088-467X(97)00008-5.
Murugappan M, Ramachandran N, Sazali Y. Classification of human emotion from EEG using discrete wavelet transform. J Biomed Sci Eng 2010;3:390–6. https:// doi.org/10.4236/jbise.2010.34054.
Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from EEG using higher- order crossings. IEEE Trans Inf Technol Biomed 2010;14:186–97. 0.1109/ TITB.2009.2034649.
Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from brain signals using hybrid adaptive filtering and higher-order crossings analysis. IEEE Transaction on Affective Computing 2010;1:81–97. https://doi.org/10.1109/T-AFFC.2010.7.
Murugappan M, Rizon M, Nagarajan R, Yaacob S. Inferring of human emotional states using multichannel EEG. Eur J Sci Res 2010;48(2):281–99.
Zheng W. Multichannel EEG-based emotion recognition via group sparse canonical correlation analysis. IEEE Transactions on Cognitive and Developmental Systems Sept. 2017;9(3):281–90. https://doi.org/10.1109/TCDS.2016.2587290.
Zheng WL, Zhu JY, Lu BL. “Identifying stable patterns over time for emotion recognition from EEG. IEEE Transaction on Affective Computing 2017;1. https:// doi.org/10.1109/TAFFC.2017.2712143.
Thammasan N, Moriyama K, Fukui K, Numao M. Continuous music-emotion recognition based on electroencephalogram99. ” IEICE Transaction on Information System; 2016. p. 1234–41. https://doi.org/10.1587/transinf.2015EDP7251.
Jirayucharoensak S, Pan-Ngum S, Israsena P. ““EEG-based emotion recognition using deep learning network with principal component based covariate shift adaptation. Sci World J 2014:1–10. https://doi.org/10.1155/2014/627892. 2014, 627892.
Zheng WL, Lu BL. Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. IEEE Transactions on Autonomous Mental Development 2015;7(3):162–75. https://doi.org/10.1109/TAMD.2015.
Yin Z, Wang Y, Liu L, Zhang W, Zhang J. Cross-subject EEG feature selection for emotion recognition using transfer recursive feature elimination. Front Neurorob 2017;11:19. https://doi.org/10.3389/fnbot.2017.00019.
Li X, Qi XY, Sun XQ, Xie JL, Fan MD, Kang JN. An improved multi-scale entropy algorithm in emotion EEG features extraction. Journal of Medical Imaging & Health Informatics 2017;7:436–9. https://doi.org/10.3772/j.issn.1002-0470.2015.10-
11.001.
Schirmer A, Adolphs R. Emotion perception from face, voice, and touch: comparisons and convergence. Trends Cognit Sci 2017;21(3):216–28. https:// doi.org/10.1016/j.tics.2017.01.001.
Rigoulot S, Pell MD. Seeing emotion with your ears: emotional prosody implicitly guides visual attention to faces. PloS One 2012;7(1):1–11. https://doi.org/ 10.1371/journal.pone.0030740.
Banziger T, Grandjean D, Scherer KR. Emotion recognition from expressions in face, voice, and body: the multimodal emotion recognition test (MERT). Emotion 2009; 9(5):691–704. https://doi.org/10.1037/a0017088.
Stathopoulou IO, Tsihrintzis GA. “Emotion recognition from body movements and gestures,” Intelligent Interactive Multimedia Systems and Services. July 2011.
p. 295–303. https://doi.org/10.1007/978-3-642-22158-3_29.
Wang XW, Nie D, Lu BL. EEG based emotion recognition using frequency domain features and support vector machines. Neural Information Processing 2011;7062: 734–43. https://doi.org/10.1007/978-3-642-24955-6_87.
Lin YP, Wang CH, Jung TP. EEG-based emotion recognition in music listening. IEEE (Inst Electr Electron Eng) Trans Biomed Eng 2010;57(7):1798–806. https:// doi.org/10.1109/TBME.2010.2048568.
Li Y, Huang J, Zhou H, Zhong N. Human emotion recognition with electroencephalographic multidimensional features by hybrid deep neural networks. Appl Sci 2017;7(10). https://doi.org/10.3390/app7101060.
Rahman MA, Rashid MMO, Khanam F, Alam MK, Ahmad M. EEG based brain alertness monitoring by statistical and artificial neural network approach. Int J Adv



Comput Sci Appl January 2019;10(1). https://doi.org/10.14569/ IJACSA.2019.0100157.
Khanam F, Rahman MA, Ahmad M. Evaluating alpha relative power of EEG signal during psychophysiological activities in Salat. In: International conference on innovations in science, engineering and Technology 2018 (ICISET). Bangladesh: International Islamic University Chittagong (IIUC); 2018. p. 1–6. https://doi.org/ 10.1109/ICISET.2018.8745614. 27-28 October.
Mahmud M, Kaiser MS, Hussain A, Vassanelli S. Applications of deep learning and reinforcement learning to biological data. in IEEE Transactions on Neural Networks and Learning Systems June 2018;29(6):2063–79. https://doi.org/10.1109/ TNNLS.2018.2790388.
Noor MBT, Zenia NZ, Kaiser MS, Mamun SA, Mahmud M. “Application of deep learning in detecting neurological disorders from magnetic resonance images: a survey on the detection of Alzheimer's disease, Parkinson's disease and schizophrenia. Brain Informatics 2020;7(11). https://doi.org/10.1186/s40708- 020-00112-2.
Rahman MA, Uddin MS, Ahmad M. Modeling and classification of voluntary and imagery movements for brain-computer interface from fNIR and EEG signals through convolutional neural network. Health Inf Sci Syst 2019;7(22). https:// doi.org/10.1007/s13755-019-0081-5.
Emotional EEG Dataset. Available in: http://bcmi.sjtu.edu.cn/home/seed/index.ht ml.
Jin Y, Luo Y, Zheng W, Lu B. EEG-based emotion recognition using domain adaptation network. International Conference on Orange Technologies. Singapore: ICOT); 2017. p. 222–5. https://doi.org/10.1109/ICOT.2017.8336126.
Homan RW, Herman J, Purdy P. “Cerebral location of international 10–20 system electrode placement. Electroencephalogr Clin Neurophysiol 1987;66(4):376–82. https://doi.org/10.1016/0013-4694(87)90206-9.
Rieke F, Bialek W, Warland D. Spikes: Exploring the Neural Code (Computational Neuroscience). MIT Press; 1999, ISBN 978-0262681087.
Scott Millers, Childers Donald. Probability and random processes. Academic Press; 2012. p. 370–5.
Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Adv Neural Inf Process Syst 2012:1097–105. https://doi.org/10.1145/3065386.
Cun YL, Bengio Y. “Convolutional networks for images, speech, and time series,” The handbook of brain theory and neural networks. 1995.
Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adeli H. Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals. Comput Biol Med 2018;100:270–8. https://doi.org/10.1016/ j.compbiomed.2017.09.017.
Morgane PJ, Galler JR, Mokler DJ. A review of systems and networks of the limbic forebrain/limbic midbrain. Prog Neurobiol 2005;75(2):143–60. https://doi.org/ 10.1016/j.pneurobio.2005.01.001.
Pessoa L. “Emotion and cognition and the amygdala: from “what is it?” to “what's to be done”. Neuropsychologia 2010;48(12):3416–29. https://doi.org/10.1016/ j.neuropsychologia.2010.06.038.
Yang Y, Wu QMJ, Zheng W, Lu B. EEG-based emotion recognition using hierarchical network with subnetwork nodes. IEEE Transactions on Cognitive and Developmental Systems June 2018;10(2):408–19. https://doi.org/10.1109/ TCDS.2017.2685338.
Rahman MA, Hossain MF, Hossain M, Ahmmed R. Employing PCA and t-statistical approach for feature extraction and classification of emotion from multichannel EEG signal. Egyptian Informatics Journal 2019. https://doi.org/10.1016/ j.eij.2019.10.002.
Rahman MA, Khanam F, Ahmad M, Uddin MS. “Multiclass EEG signal classification utilizing R´enyi min-entropy-based feature selection from wavelet packet transformation. Brain Informatics 2020;7(7). https://doi.org/10.1186/s40708-020- 00108-y.
Rahman MA, Rashid MA, Ahmad M, Kuwana A, Kobayashi H. Modeling and classification of voluntary and imagery movements from the prefrontal fNIRS signals. IEEE Access 2020;8:218215–33. https://doi.org/10.1109/ ACCESS.2020.3042249.
