Array 19 (2023) 100300










ResneSt-Transformer: Joint attention segmentation-free for end-to-end handwriting paragraph recognition model
Mohammed Hamdan âˆ—, Mohamed Cheriet
Synchromedia Lab, System Engineering, University of Quebec (ETS), 1100 Notre-Dame St W, Montreal, QC H3C 1K3, Canada


A R T I C L E  I N F O	A B S T R A C T

	

Keywords:
Handwritten text recognition ResneSt
Transformer Self attention
Segmentation-free Lexicon-free
Paragraph transcription OCR
Encoderâ€“decoder Image-seq
Offline handwritten text recognition (HTR) typically relies on segmented text-line images for training and transcription. However, acquiring line-level position and transcript information can be challenging and time-consuming, while automatic line segmentation algorithms are prone to errors that impede the recognition phase. To address these issues, we introduce a state-of-the-art solution that integrates vision and language models using efficient split and multi-head attention neural networks, referred to as joint attention (ResneSt-Transformer), for end-to-end recognition of handwritten paragraphs. Our proposed novel one-stage, segmentation-free pipeline employs joint attention mechanisms to process paragraph images in an end-to-end trainable manner. This pipeline comprises three modules, with the output of one serving as the input for the next. Initially, a feature extraction module employing a CNN with a split attention mechanism (ResneSt50) is utilized. Subsequently, we develop an encoder module containing four transformer layers to generate robust representations of the entire paragraph image. Lastly, we designed a decoder module with six transformer layers to construct weighted masks. The encoder and decoder modules incorporate a multi-head self-attention mechanism and positional encoding, enabling the model to concentrate on specific feature maps at the current time step. By leveraging joint attention and a segmentation-free approach, our neural network calculates split attention weights on the visual representation, facilitating implicit line segmentation. This strategy signifies a substantial advancement toward achieving end-to-end transcription of entire paragraphs. Experiments conducted on paragraph-level benchmark datasets, including RIMES, IAM, and READ 2016 test datasets, demonstrate competitive results compared to recent paragraph-level models while maintaining reduced complexity. The code and pre-trained models are available on our GitHub repository here: HTTPSlink.





Introduction

Historical documents (HD) often require optical character recogni- tion (OCR) systems to extract text data. One popular approach is the handwritten text recognition (HTR) model. HTR can identify individual characters within an image of handwritten text [1,2]. Conventional methods for achieving this involve the segmentation and transcription phases. However, unlike printed texts, handwriting images are chal- lenging to split into characters due to various reasons such as: cursive text, inter-intra class variations and quality of background images. Early algorithms attempted to use heuristic over-segmentation to com- pute segmentation hypotheses for characters, which were then scored as a whole. Over the decades, significant research has been dedicated to studying word-level and line-level segmentation-based techniques for handwriting.
Despite significant advancements, the HTR problem remains chal-
lenging due to the inherent variability in handwriting between and
within classes. Segmentation-free approaches have been developed to overcome this, which take a whole word image as input and generate a series of scores. These scores can then be decoded using a lexicon to re- trieve the original sequence of characters. However, when dealing with paragraph image recognition, researchers face additional challenges. For example, the modeling framework must include mechanisms for detecting and recognizing text regions within paragraph images. Ad- ditionally, the model must establish a reading order to retrieve the final paragraph transcription from all the seen text parts. Handwritten paragraphs exhibit various patterns, such as horizontal and vertical alignment, skew, and slanted text, further complicating recognition.
The accuracy of the preceding text detection and segmentation steps often limits a brief background about HWR models. Motivated by this, Wigington et al. [3] presented a deep learning model that jointly learns text detection, segmentation, and recognition mainly using images without detection or segmentation annotations. The study


âˆ— Corresponding author.
E-mail addresses: mohammed.hamdan.1@ens.etsmtl.ca (M. Hamdan), mohamed.cheriet@etsmtl.ca (M. Cheriet).

https://doi.org/10.1016/j.array.2023.100300
Received 7 May 2023; Received in revised form 12 June 2023; Accepted 14 June 2023
Available online 20 June 2023
2590-0056/Â© 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).



by [4] aims to accomplish two main objectives. First, it endeavors to create two distinct datasets: the Mayek27, which consists of 4900 isolated characters from the Meitei Mayek alphabet, and the MM (Meitei Mayek) dataset, which contains 189 pages of comprehensive handwritten text. For the lack of training data [5] proposed a ran- dom text line erasure approach that randomly erases text lines and distorts documents. Yousef et al. [6] present a newly developed neu- ral network module, OrigamiNet. This module can enhance any fully convolutional, CTC-trained single-line text recognizer into a multi-line version. This is accomplished by providing the model with the adequate spatial capacity to effectively collapse a 2D input signal into a 1D representation while maintaining information integrity. The proposed method is deemed novel and straightforward in design. To this end, Dolfing [7] investigated an end-to-end inference approach without text localization which takes a handwritten page and transcribes its full text. Sharma et al. [8] presented a fully convolution-based deep network architecture for cursive handwriting recognition from line-level images. Singh et al. [9] presented a Neural Network-based Handwritten Text Recognition (HTR) model architecture that can be trained to recognize the entire handwritten or printed text page without image segmenta- tion. Authors in this study [10] presented an efficient procedure using two state-of-the-art approaches from the literature of handwritten text recognition as Vertical Attention Network and Word Beam Search. It is stated that the approach being discussed requires additional physical segmentation annotations to train the segmentation stage. To address this issue, Coquenet et al. [11] proposed an end-to-end approach that performs handwritten text recognition for the entire document, thereby eliminating the need for additional physical segmentation annotations. Text extraction can be challenging because various factors can pose difficulties in accurately extracting text from images or other media. However, Nag et al. [12] present a novel unified method for tackling these challenges. According to the authors, [13], their model contains an innovative process of mapping an image to a sequence of characters corresponding to the imageâ€™s text by combining deep convolutional networks with recurrent encoderâ€“decoders. In their work, Carbonell and colleagues [14] introduced an integrated model designed to con- currently carry out the tasks of detecting, transcribing, and recognizing named entities in the handwritten text at the page level. This com- prehensive approach allowed the model to take advantage of common features across these tasks, enhancing its overall performance. In this study [15], the authors presented an approach for determining whether a document scan contains handwriting. Authors [16] presented a line segmentation algorithm for Urdu handwritten and printed text and subsequently to ligatures of binding two characters or words together. Kaur et al. [17] proposed a holistic approach and eXtreme Gradient Boosting (XGBoost) technique to recognize offline handwritten Guru- mukhi words. This raises the question: are segmentation-free strategies the best solution to HTR? Peng et al. [18] proposed a method that uti- lizes a simple yet efficient fully convolutional network for recognizing
handwritten Chinese text.
Advancements in text extraction from images are evident across various approaches [19â€“22]. These include deep learning models for license plate recognition, semantic graph embedding techniques, al- gorithms for sorting characters in multi-line license plates, and tools combining computer vision and machine learning for efficient table textual extraction. All these strategies signify substantial progress in text extraction and recognition technology.
Most existing methods for handwriting recognition use two stages: text segmentation and text recognition. The first stage involves seg- menting the text using a hidden Markov model (HMM) [23,24], while the second stage was focused on text recognition. Although each stage could produce good results independently, they have some fundamental flaws. For instance, manually constructing ground truth segmentation and transcription labels at the line level is costly and time-consuming. Moreover, any segmentation mistakes can lead to recognition errors,
which can impact the systemâ€™s overall accuracy. Additionally, modify- ing one stage may require retraining the other stage to ensure optimal performance. Furthermore, explicitly segmenting the text presents the issue of how to define a line, which can be a challenging task.
Our proposed approach addresses traditional segmentation-based modelsâ€™ limitations by introducing an end-to-end HTR model that does not require prior segmentation or a constrained lexicon. We synthesized and augmented existing paragraph-level datasets to achieve this and de- veloped an optimized pipeline for our HTR model. The attention mech- anism in each block of our proposed architecture plays a crucial role in helping the model learn robust line representations within a paragraph and decode the corresponding language dependencies of character sequences. We use a Convolutional Neural Network (CNN) split atten- tion (ResNeSt50) for feature extraction, generating a two-dimensional feature map for each image by combining convolution with depth-wise separable convolutional modules. An encoderâ€“decoder transformer- based multi-headed self-attention is the transcriber method, making our model capable of retrieving textual information from an input image, regardless of document restrictions such as text size, style, or layout. Our experiments have shown that our proposed model performs better than traditional segmentation-based models, slightly improving HTR performance. As a result, laborious, error-prone ground truth segmentation and transcription labels are no longer needed at the line level. In summary, the following contributions are made:
Our research presents an innovative approach for HTR that in- volves a one-stage segmentation-free pipeline with joint atten- tion mechanisms, enabling end-to-end transcription of the entire paragraph.
In this study, the focus is on exploring the combination of SOTA deep learning methods for HTR to alleviate the challenges posed by complex segmentation hypotheses. The approach is to shift from line segmentation to processing the entire paragraph, which has consistently improved HTR performance by simplifying the implementation process.
To achieve this, we leverage the latest state-of-the-art vision and language models, such as split and multi-head attention neural networks (ResneSt-Transformer). These models allow us to bypass the need for line segmentation or a pre-defined lexicon, making the transcription process more efficient and accurate.
Data augmentation was performed on synthetic paragraph im- ages using the IAM line dataset, READ2016 line dataset, and the RIMES line dataset, which contributed to the competitive performance of the model using only a few publicly available paragraph annotations.
The proposed architecture is tested on three benchmark datasets, including IAM, READ2016, and RIMES, and produces results that are competitive with those achieved by traditional methods. However, our less complex approach represents a significant step toward a more streamlined transcription process.
The proposed model based on segmentation-free and lexicon- free techniques demonstrates excellent generalization power, as evidenced by both quantitative and qualitative results.
The study conducted experiments on three public datasets of hand- written paragraphs, Rimes, READ 2016, and IAM. It achieved compet- itive results with state-of-the-art models that use ground-truth para- graph segmentation. As for the rest of the paper, Section 2 discusses re- lated methods and modeling choices. Section 3 describes the proposed modification and provides system details. Section 4 presents the exper- iment results. Section 6 presents a brief discussion that compares the proposed system with other methods, suggests potential improvements, and discusses the challenge of applying it to full documents.



Related works

The literature review highlights that previous research on text recognition has mainly focused on recognizing single lines of text, with relatively little attention given to identifying entire paragraphs. The study categorizes the methods used in this research according to two key characteristics: (a) segmentation-free methods, which do not require explicit text line segmentation before recognition; and (b) segmentation-based approaches, which typically involve explicit text line segmentation before recognition.

Segmentation-free text recognition methods

Multiline recognition systems, as described in several studies [25â€“ 29], recognize entire paragraphs or pages without initial segmentation. These systems use CNN-attention coupled with decoder attention net- works, such as LSTM attention with the CTC function. Some studies, like [25,26], use attention mechanisms for paragraph recognition tasks with implicit line and character segmentation. The authors [30] pro- posed attention blocks with encoderâ€“decoder architectures at the line and character levels. The subnetwork decoder (LSTM) generates output based on the charactersâ€™ likelihood computed from the representation. These topologies require line-level image pre-training but do not re- quire line breaks at the transcription label. Another study by [31] proposed an implicit segmentation Urdu character recognition system in which an MD-LSTM-based recognition engine is trained on statistical features.
Bluche et al. [26] proposed a model that can identify a single paragraph without making assumptions about the documentâ€™s layout or size. However, this technique required enormous memory requirements and lacked GPU acceleration for MDLSTM training. Additionally, the in- ference time was unacceptable, eventually abandoning this technique. Later, an extended work by Bluche et al. [25,32] proposed a new model that could recognize paragraphs by applying the CTC encoder approach and the Multidimensional (MDLSTM) attention model, which outputs a single text-line via an automatic line segmentation method. This ap- proach is much better than the single-line recognition model. However, it has a hard-coded assumption that the text line extends horizontally from left to right (LR - Latin scripts) and fills the entire input image. As a result, this approach only works on paragraph segmentation and fails to handle text layouts. Furthermore, this approach cannot output a variable-length sequence, and a hard separator joins the predicted lines. Therefore, there is a lower likelihood that the model will accurately recreate blank lines and indentations.
Kaltenmeier et al. [33] proposed a segmentation-free Hidden Markov Model (HMM) based method that can output the sequence scores of the image text (word level) with the help of the lexicon dic- tionary used for the decoding step. Unlike Bluche et al.â€™s approach, this method does not rely on segmenting text lines or paragraphs. Instead, it recognizes individual words based on their probabilities and the lexicon dictionary. This approach is advantageous when dealing with images that contain irregular or distorted text. However, the modelâ€™s accuracy depends heavily on the quality of the lexicon dictionary. Therefore, building a high-quality lexicon dictionary is crucial to achieving the best results.
Coquenet et al. [34] introduced the vertical attention network (VAN) - a cutting-edge solution for recognizing paragraph-level docu- ments using a hybrid attention mechanism to process paragraph images line by line iteratively, relying on line annotations. This unique ap- proach enables VAN to implicitly segment the text while accurately recognizing character sequences associated with each line. With this innovative technique, VAN further elevates the accuracy and efficiency of its text recognition capabilities.
Segmentation-based recognition methods

HTR models aim to recognize text strings in scanned documents, including Historical Documents (HD). Sueiras et al. [35] proposed a sequence-to-sequence deep neural network model that uses a hori- zontally sliding window over word image patches. Sueirasâ€™s model performed best on the word level of the IAM and RIMES benchmark datasets using the MDLSTM/CTC architecture. Similarly, the work of Ma et al. [36] introduced a novel multi-scale attention network to extract a robust representation of text images and improve performance by replacing the fully connected layer with a global maximum pooling layer. In contrast to printed text, handwritten scanned documents are challenging to segment into different characters, and the tradi- tional approach [37â€“39] involves an initial segmentation step followed by transcription. Traditional machine learning methods with hidden Markov models (HMMs) were employed in the early stages of devel- oping HTR systems. When character segmentation was required for further text processing, the work of Mohamed et al. [40] and Mou-Yen et al. [41] attempted to calculate character segmentation hypotheses by performing heuristic over-segmentation and evaluating groups of segments.
Unlike [25,26], [3] do not require a pre-segmentation as earlier approaches, such as those developed by [3,42]; they focus on paragraph and full-page handwritten text recognition. The latter techniques [3] on training data require a prior selection for text lines (ground-truth) to learn text line localization by individual networks or subnetworks in a multitasking network. Line breaks are marked on any textual ground-truth transcriptions. The idea of adapting [3] as a combinatorial optimization problem presented by [42] in a weakly guided fashion in which the authors suggested aligning the anticipated transcription and the corresponding ground truth. These methods established the align- ments to be greedily resolved without the requirement for transcription line breaks. However, [42] takes the same amount of pre-training as [3] and performs at a lower level than either of the previous examples.
Bluche and Messina [26], and Puigcerver [29] followed the trend toward a more parallel architecture by replacing the MDLSTM encoder with CNN while still relying on CTC. The segmentation and recognition text line tasks are presented as two distinct networks by [43]inspired by object recognition methods. By focusing on modular techniques, the authors proposed a pipeline and described it in detail in [43]. The passages distinguish handwritten text areas, apply object recognition- based word-level segmentation is made, and the words are arranged logically. In contrast to [43], which focuses on line-level recognition, the authors of [44], taking an approach similar to that of [43], offers an end-to-end paradigm based on word-level recognition rather than line-level recognition.
MDLSTM [45], hybrid CNN coupled with Bidirectional (BLSTM)
[46] attention encoderâ€“decoder [47] and Gated fully convolutional net- work (GFCN) [48] are examples of improved techniques. CNN coupled with MDLSTM [49] can identify the beginning of each line reference, and MDLSTM can recognize text lines by using a special line ending token. Particularly suitable in multiline and full paragraph texts where CNNs operated as segmented multiline predictors, as in [3,42]. The iterative procedure then builds a normalized line by predicting the next location depending on the present position until the end of the line.
Finally, CNN + BLSTM served as OCR for line-level recognition. The
network will only operate with transcription labels in this situation. Un-
like the work of [3,42] that is based on a segmentation-free technique to handle transcriptions without line breaks at the paragraph level.

Deficiencies in current models

Though [25] is comparably faster to train than [26], in both works, their encoder subnetworks require pre-trained on isolated-line images before training on paragraph images. Hence, [25,26] are slow and hard to train, unlike our experiments. Our proposed model is straightforward



to train directly on paragraph images. Whereas most existing HTR models need prior image segmentation [50] to extract text components such as characters, words, and lines, these approaches have several flaws. These methods of image-text segmentation rely on heuristics or feature engineering that break under severe data changes. Many segmentation-based methods help correct slanted text, curvature, and bold, using wrong properties and heuristics. More critically, it is chal- lenging to separate text units cleanly in real-world handwriting. Lines, for example, may be warped or merged with non-textual symbols and other visual elements. The literature has further information on these limitations [25,26]. Synthesizing a general transcription from external transcribed text segments considers a different technique that might introduce errors and decrease performance.
Several methodologies have been investigated to address the chal-
lenge of extracting text from various image sources. Onim et al. [19] introduced BLPnet, an end-to-end DNN model designed explicitly for Automatic License Plate Recognition (ALPR) systems focusing on Ben- gali characters. This model surpasses existing YOLO-based ALPR mod- els regarding number-plate detection accuracy and outperforms the Tesseract model regarding time requirements. Etaiwi [20] proposed a novel approach named SemanticGraph2Vec for semantic graph em- bedding. This method considers the semantic relationships between vertices during the learning process, enhancing the overall performance of graph embedding techniques. Minhuz et al. [21] presented an algo- rithm for sequential sorting of discrete and connected characters found in multi-line license plates. By employing image processing techniques, their algorithm contributes to developing an automatic license plate recognition (ALPR) system capable of accurately distinguishing indi- vidual characters even from license plates of inferior quality. Colter et al. [22] emphasized the significance of data extraction from tables and highlighted the necessity of automated table extraction for practi- cal data mining. The proposed tool, Tablext, combines computer vision and machine learning methods to identify and extract textual data from tables efficiently, streamlining the data-mining process.
Additionally, the Right-to-Left (RL) direction scripts like Arabic
need stitching because text portions are typically concatenated with a space or new line. Thus, the formatting and indentation may be lost. This limitation indicates that the smaller text size may overlap and intersperse with the more significant parts of information that have not been transcribed. Luckily, the proposed model avoids the issues by implicitly processing and learning from data end-to-end. So, our proposed model is easy to implement with the best choice of hyper- parameters. They can jointly perform segmentation and recognition tasks using cutting-edge methods of training and searching. With- out human intervention, the training is carried out automatically on paragraph images.

Methodology

In this section, we define the problem of paragraph handwriting recognition. Then, we provide an overview of the proposed model and the relevant details about its components. We continue with a detailed description of the Seq2Seq architecture used by our model. This description explains the details of the three module components: split attention convolutional feature extraction, encoder transformer layers, and decoder transformer layers, respectively. Fig. 1 depicts the main components of the complete model architecture.

Defining the problem

Since we address the paragraph level of the HTR framework, there is no need to predefine image regions as text or non-text. The paragraph
image corresponds to its textual ground truth represented as ğ‘‹, and
ğ‘Œ denotes the paragraph handwritten images ğ‘‹, ğ‘Œ . The vocabulary
contains different characters, including capital (Aâ€“Z) and small (aâ€“z)
characters, digits (0â€“9), and the most common spatial characters and
symbols, including the white space. The proposed model can interpret the visual information and the text of the corresponding transcription
from the paragraph image and the paragraph text, where (ğ‘¥ğ‘– âˆˆ ğ‘‹) and
their associated strings (ğ‘¦ğ‘– âˆˆ ğ‘Œ ). We use the benchmark datasets in
academia, the IAM benchmark dataset [51] and RIMES [52], which
have more than 600 writers in English and French scripts. There- fore, it persists in challenging handwriting, inter-class, and intra-class variability.
Preferably, a visual feature representation encoder targets extract- ing valuable information using ResNeSt50 and Encoder Transformer from the paragraph images. We use the attention mechanism and positional encoding techniques to learn the paragraph-handwritten image characteristics and direct its attention to various charactersâ€™ positions. Following that, the text transcriber is dedicated to producing the decoded characters and simultaneously attending to visual and corresponding text transcription. The proposed model is an end-to-end trainable model that learns handwritten visual image representations and their textual ground truth.
Fig. 1 illustrates the architecture of the proposed approach where ResneSt50 [53] and encoderâ€“decoder transformer [54] make up the bulk of the proposed model.

Model overview

HTR models often struggle with obtaining accurate line segmenta- tion, complicating the training and transcription process. To address this, a state-of-the-art solution using ResneSt-Transformer models com- bines vision and language models through joint attention, providing an end-to-end recognition of handwritten paragraphs. This one-stage, segmentation-free pipeline, consists of three modules: feature extrac- tion, encoding, and decoding, each utilizing attention mechanisms and positional encoding. By employing joint attention, the neural network implicitly segments lines, enabling end-to-end transcription of para- graphs. Tested on RIMES, IAM, and READ 2016 datasets, this approach achieves competitive results with reduced complexity, paving the way for fully automated handwriting transcription. In Fig. 1, the ResneSt- Transformer model employs a one-stage segmentation-free pipeline that uses joint attention mechanisms to handle a paragraph image in an end-to-end trainable fashion. The pipeline consists of three modules, each building upon the output of the previous module. The first fea- ture extraction module utilizes CNN with a split attention mechanism (ResneSt50). Then, an encoder module with four transformer layers generates robust representations of the entire paragraph image. Finally, a decoder module with six transformer layers creates weighted masks. A Split Attention Network is used for handwriting paragraph recognition by splitting the input image into smaller parts and processing each part with its attention mechanism.
The encoder and decoder modules are coupled with a multi-head self-attention mechanism and positional encoding. This enables the model to focus on the current time step on specific feature maps. By leveraging joint attention and a segmentation-free model, the neural network computes split attention weights on the visual representation, allowing for implicit line segmentation. Images are processed using a CNN for features extractor, then the Transformer encodes and de- codes these features using multi-headed self-attention layers. A joint attention module decodes the paragraph image at the character level. The proposed method uses the backbone ResNeSt50 as a convolutional network to extract the 2D features representation. Before feeding data through a transformer encoder, we add the 2D positional encodings to the feature vector. We applied four encoders and six decoder layers; each has self-attention and Feed Forward Neural Network (FFNN).
Fig. 2 illustrates the flowchart of the proposed model. The process begins with the input image and target text as the initial inputs. The im- age undergoes feature extraction using a ResNeSt-50 backbone model, which extracts meaningful features. These features are passed through a 2D convolutional layer, converting them into 256 feature planes. 2D




/ig. 1. Network architecture overview: (left) A paragraph input image is fed to a CNN split attention that serves as the backbone for the feature extraction method. (right) A 2D positional encoded technique encodes the CNN output feature vector before feeding it to the encoder self-attention transformer layers. The CNN output feature representations serve as input to the decoder self-attention layers. The embedded ground truth is encoded with the same 1D position encoded technique before feeding it to the decoder self-attention layers. Finally, a Softmax activation function outputs the probability for each class.


/ig. 2. This flowchart illustrates the sequential steps involved in the HPR process.


positional encodings are generated and concatenated with the feature maps to capture spatial information. The concatenated feature maps and positional encodings are input to the transformer, comprising encoder and decoder components. The transformer utilizes multi-head self-attention mechanisms in the encoder and decoder blocks to capture dependencies between features and positional encodings. The target text is embedded and processed through a 1D positional encoding layer. The decoder applies multi-head self-attention to the feature maps and positional encodings. The final output is the predicted text.
This flowchart demonstrates the sequential steps in feature extraction, encoding, decoding, and prediction, highlighting the modelâ€™s ability to recognize handwritten text.

Feature visual representation

Encoding visual feature representation using ResNeSt [53] similar to ResNeXt [55], which applies a multi-path feature extraction of the input image. Then in each path, divided attention is employed. Similar



to SKNet [56], the split is accomplished by performing convolution with various kernel sizes to select the optimal kernel size channel-wise for each. ResNeSt, conversely, does convolution with kernels of the same size as a branch selection. Several models are generated and then assembled using divided attention by ResNeSt.
ten paragraph image (ğ‘¥ğ‘– âˆˆ ğ‘‹) in this feature visual encoder stage, High-level feature representations are extracted from the handwrit- where ğ‘¥ğ‘– represents a sample of input images as in Fig. 1. To process the handwritten paragraph images, the input image ğ‘¥ğ‘– is first processed
by the CNN, which can handle images of any size. An intermediate
ğ¹ğ‘£ representation of visual features is constructed of size (ğ‘“ = 2048)
as the feature vector. The ResNeSt50 convolutional architecture serves
as the backbone of the HTR architecture. Compactable visual feature representations that give a contextualized global view of the whole input image are rare. Therefore, the attention mechanism layer in ResNeSt50 helps extract this meaningful information.

Positional encoding

Text images in Latin scripts are typically processed sequentially, moving from left to right. In this process, the positional encoding stages are strategically positioned before the transformer encoder phase. These stages are designed to capture and encode this essential se- quence information without unnecessary repetition. Such encoding assists the model in determining the precise location of the next character in the text paragraph. To utilize order sequences effectively, the model leverages positional encoding (PE) as formulated in Eqs. (1), (2). This approach aids in encoding the tokensâ€™ positions within the sequence [54]. Consequently, we integrate the input embeddings with the PE at the decoder networks, visually represented in Fig. 1. Furthermore, we employ a 2D learnable PE [57] on the feature vector that the ResNeSt encodes after adjusting the vector to the same hidden dimension (256) matching the expected input of the transformer model. The inclusion of PE in our proposed architecture is highly beneficial. It instructs our model to focus actively on the specific position within the sequence, thereby enhancing its performance.
3.6. Textual representation and transcription

The text encoder and decoder are two model modules depicted in Fig. 1. Its goal is to output decoded characters using visuals and the knowledge of language-specific to create textual representations. Encoderâ€“decoder transformer-based systems to earn n-grams from the textual transcriptions, predicting the next most likely character. Using Eq. (4) Text encoding, Eq. (3) for language self-attention, and Eq. (5) mutual attention in order to make up the text transcriber as the final output in Eq. (6). To properly process the paragraph-level string, we need a small number of symbols with no textual content in addition
to the large vocabulary size of the dataset alphabet. âŸ¨ğ‘†ğ‘‚ğ‘ƒ âŸ© is used
to indicate the beginning of a new paragraph, âŸ¨ğ¸ğ‘‚ğ‘ƒ âŸ© the end of the paragraph, and âŸ¨ğ‘ƒ âŸ© to indicate padding for the short length of a given paragraph. In the prediction, the transcriptions ğ‘¦ğ‘¡ âˆˆ ğ‘Œğ‘¡ are lengthened to a maximum number of characters ğ¶ for a given paragraph.
Utilizing the same positional encoding Eqs. (1), (2) in Eq. (4), the model embeds the characters with the help of the final condense connected layer that transfers every symbol or character in the input string to its corresponding vector representation with a dimension of 256. Since each decoded character is sent back into the decoder to help anticipate the next character, the sequence-to-sequence tech- niques [58,59] prevent parallelization from occurring in the decoding phase. Thankfully, We leverage the transformer strategy [54], where all decoding stages are provided simultaneously with a masked procedure.
ğ¹ğ‘¡ = ğ‘€ ğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”(ğ‘¦ğ‘¡) + ğ‘ƒ ğ¸	(4)
where the output shape of the textual feature representation (ğ¹ğ‘¡ is
(f,C)). In Fig. 3, we randomly picked different positions of heat maps;
due to the limited space, we could not visualize the whole paragraph heat map at the decoder layer Multi-head self-attention.
features, forming the textual feature representation ğ¹Ì‚ğ‘¡,â€™ which seeks When the self-attention module implicitly produces n-gram-like
to condense the text information further and acquire language-specific
refer to the textual feature representation as ğ¹Ì‚ . This representation attributes. Concerning visual self-attention, as defined in Eq. (3), we is equivalent to ğ‘„ğ‘£ğ‘¡, derived from the textual features denoted as ğ¹Ì‚ğ‘¡. For our model, we define the query, key, and value as ğ‘„ğ‘£ğ‘¡, ğ¾ğ‘£, and ğ‘‰ğ‘£,
respectively. We denote the correlation information derived from the
attention textual feature, calculated by Eq. (5), by ğ´ğ‘–,

ğ‘ƒ ğ¸
(ğ‘ğ‘œğ‘ ,2ğ‘–)
= ğ‘ ğ‘–ğ‘›(	ğ‘ğ‘œğ‘ 
10 000 ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
ğ‘ğ‘œğ‘ 
)	(1)
ğ‘ğ‘–.ğ¾ğ‘£
ğ´ğ‘– ğ‘¡ = ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥( âˆšğ‘¡
ğ‘“
ğ‘¡
)ğ‘‰ğ‘£	(5)

ğ‘ƒ ğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = ğ‘ğ‘œğ‘ (
 2ğ‘–+1  )	(2)
where the ğ‘ğ‘– âˆˆ ğ‘„
as associated ground truth input query and ğ‘– range

10 000 ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
ğ‘¡	ğ‘¡

where the equations denote that the character at a particular position
ğ‘– is represented by ğ‘ğ‘œğ‘ , while the hidden dimension size in the trans-
text, ğ¾ğ‘£ and ğ‘‰ğ‘£ are the input key and value, respectively. Finally, from (0 to L) where L represents the input paragraph length of input
from Eq. (5) where ğ¹Ì‚ = {ğ´0 , ğ´1 , ğ´2 , . . . , ğ´ğ¿âˆ’1}, we obtain the high

former block is denoted by ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ . The Eqs. (1) and (2) include absolute
ğ‘£ğ‘¡
level textual representation.
ğ‘£ğ‘¡
ğ‘£ğ‘¡
ğ‘£ğ‘¡
ğ‘£ğ‘¡

positional information for the encoder at the decoder input.

Self-attention module
Aligning and merging the learned feature representations from the paragraph images and their ground truth is the final stage, accom-
plished through mutual self-attention. The ğ‘Œğ‘¡ transcription should align

with the visual ğ¹Ì‚  output. The final prediction is achieved by first

putting the visual representation ğ¹Ì‚
into a linear module and then into

The self-attention layer is used for the image feature vector to im-
a softmax activation function in Eq. (6). It is anticipated that the output

prove the visual representation further. Inspired by Vaswani et al. [54],
Ì‚
ğ‘£ğ‘¡
will transcribe in according to the ground truth, ğ‘Œğ‘¡. As a result, the

we used 4 heads of a multi-head self-attention module. The module is
final prediction is achieved by feeding the ğ¹Ì‚
to a softmax activation

described as taking in three inputs that correspond to query, key, and	function in a linear module. Finally, the output prediction is generated

value, which are represented by ğ¹Ì‚ as ğ‘„ , ğ¾ , and ğ‘‰ , respectively. The
using ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ in Eq. (6).

ğ‘£	ğ‘£	ğ‘£	ğ‘£

correlation information of the attention visual feature is represented as
ğ´ğ‘– and is obtained through the use of Eq. (3),
ğ‘ğ‘– .ğ¾ğ‘£
ğ‘‚ğ‘¡ = ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ¹Ì‚ )	(6)
For instance, the anticipated transcription for the given input image is shown in Fig. 5. Precisely, to decode the letter â€˜â€˜Tâ€™â€™ in the word

ğ´ğ‘– = ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥( âˆšğ‘£
ğ‘“
)ğ‘‰ğ‘£	(3)
â€˜â€˜Templateâ€™â€™ from the first line of a given input paragraph image ğ‘¦ğ‘¡, all
characters of positions higher than the position of â€˜â€˜Tâ€™â€™ are masked. This

where the ğ‘ğ‘– âˆˆ ğ‘„ğ‘£ as input query and ğ‘– range from (0 and ğ‘¤ âˆ’ 1), ğ¾ğ‘£

and ğ‘‰ğ‘£ are the input key and value, respectively. Finally, from Eq. (3) where ğ¹Ì‚ = {ğ´0, ğ´1, ğ´2, . . . , ğ´ğ‘Š âˆ’1}, we obtain the high level visual
representation.
ensures that the decoding characters rely exclusively on predictions of the ordered sequence of â€˜â€˜Tâ€™â€™ beforehand. When used in recurrent algorithms, the ability to analyze several time steps in parallel can significantly minimize training costs.




/ig. 3. Paragraph heat-map at decoder layer multi-head self-attention: randomly picked at 110 and 330 position character sequences.


Experiments

Experiments conducted on the paragraph level using the READ2016, RIMES, and IAM benchmark test datasets produced competitive results compared to recent models trained at the paragraph level, with less complexity. The ResneSt-Transformer modelâ€™s joint attention mech- anism has effectively handled the challenges of recognizing offline handwritten paragraphs without explicit line segmentation.

Data sets

This research evaluated the proposed approaches on three well- known handwriting datasets: RIMES, IAM, and READ 2016. We used the paragraph levels for our purposes, with the commonly used split by researchers as detailed in Table 1.

IAM
The IAM [60] handwriting dataset is a collection of handwritten text documents widely used for research in handwriting recognition. It consists of more than 1500 pages of handwritten text, including various handwriting styles and languages. The documents in the dataset have been manually transcribed and annotated, making it a valuable re- source for training and evaluating handwriting recognition algorithms. In our study, we utilized this dataset which consists of handwritten copies of text passages taken from the LOB corpus. The dataset includes grayscale images of English handwriting at a resolution of 300 dpi. It provides segmentation at the page, paragraph, line, and word levels and their corresponding transcriptions.

RIMES
The RIMES [61] handwriting dataset is a widely used collection of
Table 1
IAM READ2016 and RIMES datasets associated with the standard three splits: train, valid, and test sets.

Dataset	Line-level	Paragraph-level




READ2016
The READ2016 [62] handwriting dataset was proposed for the ICFHR 2016 competition on handwritten text recognition. It comprises a subset of the Ratsprotokolle collection used in the READ project and includes color images of Early Modern German handwriting. The dataset provides segmentation at the page, paragraph, and line levels. We follow the preprocessing steps of [34], so we eliminated the char-
acter â€˜Â¬â€™ from the ground truth as it is not genuine. The READ2016
dataset is frequently utilized with other datasets, such as the IAM and
RIMES datasets, to enhance handwriting recognition systemsâ€™ accuracy further.

Performance metrics

Character Error Rate (CER) is computed utilizing the Levenshtein distance [63]. This metric quantifies the number of character-level manipulations needed to transform the ground truth text into the Hand- writing Text Recognition (HTR) output. The Levenshtein edit distance underpins the total count of insertions, replacements, and deletions required to transition from one sequence to another. Essentially, CER is delineated as expressed in Eq. (7).

handwritten text documents for research in handwriting recognition.
ğ¶ğ¸ğ‘… =  1   âˆ‘  ğ¿ğ·(ğ‘¦Ì‚ , ğ‘¦ )	(7)

It consists of grayscale images of French handwritten text produced in the context of writing mail scenarios, with a resolution of 300
|ğ‘ | (ğ‘ğ‘– ,ğ‘™ğ‘– )âˆˆğ‘
ğ‘–  ğ‘–

dpi. The official split has 1500 pages for training and 100 pages for
evaluation. However, for comparison with other works, we utilized the last 100 training images for validation, as is commonly done. The dataset provides segmentation and transcription at the paragraph, line, and word levels, and we used paragraph segmentation levels in this study.
where |ğ‘ | is the number of ground truth characters at ğ‘ partition,
while the ğ¿ğ·(ğ‘¦Ì‚, ğ‘¦) is Levenshtein distance between prediction ğ‘¦Ì‚ and target label ğ‘¦ of ğ‘–th character at each predicted character ğ‘ğ‘– with respect to the corresponding label ğ‘™ğ‘–. Regarding ğ‘Š ğ¸ğ‘… in Eq. (8), it is defined similarly to CER. Whereas ğ¿ğ·(ğ‘¦Ì‚, ğ‘¦) is computed on word level, which requires transforming one string into another by dividing deletions ğ·



words sum, insertions ğ¼ , and substitutions ğ‘† by a total number of the ground-truth ğ‘ .
ğ‘†ğ‘¤ğ‘œğ‘Ÿğ‘‘ + ğ¼ğ‘¤ğ‘œğ‘Ÿğ‘‘ + ğ·ğ‘¤ğ‘œğ‘Ÿğ‘‘
transformer_decoder(ğ» â€², ğ‘‡ â€², ğ‘€ğ‘‡ , ğ‘€ğ‘ƒ ) to predict the output character probabilities: ğ‘Œ = vocab(ğ‘‚). where: ğ¼ represents the input image, ğ¹
represents the feature map extracted from the input image using the

ğ‘Š ğ¸ğ‘… =
ğ‘ğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ 
(8)
backbone model, ğ» represents the reduced feature dimensions after applying the convolution layer, ğ‘ƒ represents the positional encodings

Experimental setup

Fig. 1 describes the network architecture overview of the proposed solution for offline HTR on a paragraph level. The figure shows two network parts: the left part is the feature extraction module, and the right part is the encoderâ€“decoder module. The left part of the network uses a convolutional neural network (CNN) with a split at- tention mechanism as the backbone for feature extraction. The split attention mechanism allows the network to focus on different parts of the input image simultaneously, improving the feature extraction process. The input image is a paragraph image, and the output of the CNN is a 2D feature representation. The right part of the network consists of an encoder and a decoder module, both using self-attention transformer layers. Before feeding the 2D feature representation to the encoder module, a 2D positional encoded technique is applied to the feature vector. The positional encoding helps the network to understand the spatial relationship between different parts of the input image. The decoder module also uses the same positional encoded technique for the embedded ground truth, which is the expected output of the network. The decoder module produces weighted masks that are utilized to calculate the probability distribution of each class with the help of a Softmax activation function. During training, the weights are updated using a Kullbackâ€“Leibler Divergence (KLD) loss function to reduce the loss on the subsequent epoch. The final feedforward layer generates predictions that form a probability distribution. The KLD metric compares this distribution with the sample distribution in the corresponding training dataset. The proposed solution uses a joint attention mechanism that combines vision and language mod- els to achieve end-to-end recognition of handwritten paragraphs. The network architecture is straightforward to train and does not require line segmentation or a predefined lexicon model. The solution yields competitive results on benchmark datasets but requires much training data to be effective.

 Training setup	
Algorithm 1 Handwriting Paragraph Recognition (HTR) Model
Require: Input image ğ¼ , target sequence ğ‘‡
1: ğ¹ â† backbone(ğ¼ ) {Extract features using the ResneSt-50 backbone
network}
2: ğ» â† conv(ğ¹ ) {Reduce feature dimensions}
3: Calculate positional encodings ğ‘ƒ ğ‘’ using both sine (1) and cosine (2)
frequency equations.
4: ğ» â† ğ» + ğ‘ƒ ğ‘’ {Add positional encodings to the input features}
5: Generate source mask ğ‘€ğ‘† for ğ»
6: ğ»  â†  transformer_encoder(ğ» , ğ‘€ğ‘† ) {Pass features through the
transformer encoder}
7: Generate target mask ğ‘€ğ‘‡ and padding mask ğ‘€ğ‘ƒ for ğ‘‡
8: Calculate decoder embedding ğ· and positional encoding ğ‘„ for ğ‘‡
9: ğ‘‚ â† transformer_decoder(ğ» , ğ· + ğ‘„, ğ‘€ğ‘‡ , ğ‘€ğ‘ƒ ) {Pass features and
target through the transformer decoder}
10: ğ‘Œ â† vocab(ğ‘‚) {Predict output character probabilities}
11: return ğ‘Œ

Algorithm 1 shows the training procedures; we first extract the
for the feature map, ğ‘‡ represents the target sequence, ğ·(ğ‘‡ ) represents the decoder embedding of the target sequence, ğ‘„(ğ‘‡ ) represents the positional encoding of the target sequence, ğ‘€ğ‘‡ and ğ‘€ğ‘ƒ represent the target mask and padding mask, respectively, ğ‘‚ represents the output of the transformer decoder and ğ‘Œ represents the output character
probabilities.
The proposed method utilizes a ResneSt50 convolutional neural network and a Transformer network to learn the underlying structure of input paragraph images and their corresponding transcriptions through supervised learning. During the training phase, the model is trained on a training dataset, while in the testing phase, the trained model weights are used to predict transcriptions for a testing dataset. The model is designed to take paragraph-level images as input and is trained end- to-end on a given dataset. The model weights are then saved based on the optimal training period and used by the paragraph recognition model to extract features. This method offers a powerful approach to handwriting paragraph recognition, leveraging convolutional and transformer-based architectures to capture complex features in the input images and corresponding transcriptions.


Cost function
During the optimization phase of our model training, we consis- tently evaluated errors by applying a corrective approach for mis- predictions. Selecting a suitable cost function for measuring a model performance is instrumental in adjusting weights to diminish loss in the subsequent training rounds. The problem dictates the type of loss function that should be used in neural network models. Our task uses the softmax activation function to establish a probability distribution for the multiclass classification problem.
To evaluate the performance of our handwriting paragraph recogni- tion model, we employed the Kullbackâ€“Leibler Divergence (KLD) [64], a widely-used metric in information theory that measures the difference between two probability distributions. Specifically, the output of the
distribution for each sample ğ‘¥. This distribution was then compared final feedforward layer of our model was used to establish a probability to the corresponding ground truth distribution for ğ‘¥ from the training
dataset. To quantify the discrepancy between the predicted distribution
ğ‘ƒ (ğ‘¡) and the ground truth distribution ğº(ğ‘¡), we used KLD. The back-
distribution ğ‘ƒ (ğ‘¡) yielded a textual transcription that closely matched or propagation operation was performed iteratively until the predicted was identical to the ground truth distribution ğº(ğ‘¡). We used the ADAM
optimizer to modify the weights and biases of the model to achieve the most favorable distribution of prediction probabilities. The optimiza-
tion process minimized the KLD between ğ‘ƒ (ğ‘¡) and ğº(ğ‘¡) as expressed
in Eq. (9). Overall, the use of KLD allowed us to measure the accuracy of our handwriting paragraph recognition model by comparing the predicted distribution to the ground truth distribution for each sample in the training dataset. The iterative optimization process using KLD as a loss function enabled us to improve the modelâ€™s accuracy and achieve better performance on the recognition task.

features from the input image: ğ¹ = backbone(ğ¼ ), followed by reducing the feature dimensions using the convolution layer: ğ» = conv(ğ¹ ), and calculating and adding positional encodings to the features: ğ» â€² =
ğ¾ğ¿ğ· (ğ‘ƒ âˆ¥ğº) = âˆ’
âˆ‘
ğ‘–âˆˆğ‘‹
ğ‘ƒ (ğ‘–) âˆ— ğ‘™ğ‘œğ‘”( ğº(ğ‘–) )	(9)
ğ‘ƒ (ğ‘–)

ğ» + ğ‘ƒ , then calculating decoder embedding and positional encoding for the target sequence: ğ‘‡ â€² = ğ·(ğ‘‡ ) + ğ‘„(ğ‘‡ ). Finally, we pass the features and target sequences through the transformer decoder: ğ‘‚ =
In this context, ğ‘– denotes a specific instance from the set of decoded
ground truth elements of ğ‘‹ and its corresponding encoded representa-
tion in the paragraph image features.


Regularization technique
Challenges such as overfitting and excessive confidence frequently arise when training deep learning models. Various regularization meth- ods, including early stopping, weight decay, and dropout, have been de- veloped to tackle the overfitting problem. However, the label smooth- ing technique [65] can effectively address both issues. Label smoothing,
distribution with the updated one-hot-encoded label vector ğ‘¦, replacing as depicted in Eqs. (10) and (11), assists in merging the uniform
the traditionally used label vector with this updated version.

ğ‘¦Ì‚ = ğ‘¦
(1 âˆ’ ğ›¼) + ğ›¼
(10)

ğ‘–	â„ğ‘œğ‘¡	ğ¾
In this equation, ğ¾ is the total number of multi-class categories (97,
100, 89 for IAM, RIMES, and READ2016, respectively), and ğ‘¦â„ğ‘œğ‘¡ repre-
sents the embedded ground truth labels.
{1 âˆ’ ğ›¼,  ğ‘– = ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡

ğ‘¦Ì‚ğ‘– =
ğ›¼âˆ•ğ¾,  ğ‘– â‰  ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡
(11)

The label smoothing technique introduces a certain amount of noise to the distribution. This process effectively curtails the model tendency from extreme confidence in identifying the correct label. As a result, the disparity between the output values of predicted positive and negative samples is reduced, offering an effective strategy against overfitting and
of experiments and determined that an ğ›¼ value of 0.1 yielded the most boosting the model capacity for generalization. We conducted a series
optimal results.

Synthetic dataset and augmentation
We initially attempted to train our model using actual training data involving the best-selected hyperparameters. However, the model could not learn and converge due to data insufficiency. To overcome this problem, we decided to generate synthetic data since deep learning methods require a massive amount of labeled data to develop a gen- eralized model. However, in the case of handwritten text recognition (HTR), there is a lack of adequate labeled data at the paragraph level. Therefore, we synthesized additional images by concatenating a random number of lines from the IAM, READ 2016, and RIMES datasets to expand these datasets. We generated 150,000 additional images to the initial 747, 1584, and 1400 paragraph forms on the IAM, READ 2016, and RIMES training sets, respectively. Fig. 4 shows a sample from the synthetic IAM data. This proves our model can transcribe handwritten text with strikethrough and circled words.
Unlike the original IAM dataset, the synthetic data are more chal- lenging as they consist of random lines with variable lengths from the IAM dataset. To further enhance the training images, we utilized spontaneous augmentation approaches such as scaling, images, rota- tion, brightness, contrast, blurring, normal distribution, translation, and sharing.

Results and discussion

The methodology Section 3 is written rather clearly and provides many technical details. The experiment Section 4 adds further informa- tion about the system and training configuration and demonstrates the effectiveness of the proposed approach. In Section 4.2, we discussed an accurate assessment of the current study, using the evaluation metrics such as CER and WER. We directly compare our proposed model and the corresponding baselines (attention model without aug- mentation and non-attention model with augmentation) on the IAM, READ2016, and RIMES datasets. We use identical design decisions and training methodologies to ensure that our proposed joint attention- augmentation-based free performs comparably to the baselines. The following Sections 5.1, 5.2 briefly analyzed the reported results.






/ig. 4. A sample image of generated synthetic datasets from the IAM line dataset.


Table 2
The impact of the attention mechanism on IAM, READ 2016, and RIMES benchmark databases.



Quantitative analysis

The efficacy of an attention mechanism within the feature extraction framework is quantitatively demonstrated in Table 2. Initially, we utilized the pre-trained ResNet50 architecture for feature extraction, which lacked an attention mechanism. Conversely, ResneSt50 incorpo- rates an attention scheme, yielding more robust feature representations. Comparatively, the latter is dropping the CER and WER since the robust feature extractions are due to the split attention mechanism in its backbone block. Table 2 underscores the influence of the attention mechanism [66] across three benchmark datasets: IAM, READ 2016, and RIMES. The comparison includes two models: one without an atten- tion mechanism and the other equipped with the attention mechanism. As mentioned, the standard key evaluation metrics used are CER and WER in our evaluation. The attention mechanism, a special layer of neural network technique, leads the model to pay more attention to spe- cific aspects of input data resulting in enhanced recognition of offline handwritten paragraphs. The resneSt-transformer model utilizes a joint attention mechanism, enabling implicit line segmentation and thus obviating the need for explicit line segmentation in text recognition. The table reveals the superior performance of the model incorporating the attention mechanism across all three benchmark datasets, with significantly lower CER and WER, which attests to the enhancement in model accuracy through the attention mechanism. The RIMES database shows the best performance, with the lowest CER and WER scores. In contrast, the IAM database presents the most significant challenge for the model due to its diversity of handwriting structures from more than 600 writers. In conclusion, the attention mechanism substantially elevates the accuracy of the model in recognizing offline handwritten paragraphs across all benchmark databases, with RIMES offering the best performance.




/ig. 5. Input image with the corresponding ground truth and its transcription from IAM dataset.


Table 3 provides a comprehensive performance comparison with the latest relevant research. Current models, as shown in the table, are di- vided into two approaches before the recognition phase: segmentation- free or segmentation-based paragraph recognition methods, as dis- cussed in Section 2. We conducted three experiments. The first ex- periment involved a model without attention and with augmentation. The subsequent experiments using ResNeSt50 were performed either with augmentation (joint attention model with-aug) or without aug- mentation (joint attention model no-aug). The third experiment, devoid of split attention but with augmentation on ResNet50, is referred to as (our no-attention model with-aug). We also report the results of other methodologies in the literature that utilize both recurrent and non-recurrent models, with or without attention mechanisms.
Table 3 contrasts recent models that do not employ prior segmen- tation or language model correction for the IAM and RIMES datasets. This facilitates a more sound analysis of the effects of the split attention convolution network and transformer-based models relative to stan- dard CNN and recurrent models. To the best of our knowledge, our proposed model secures the second-best performance on READ 2016, IAM, and RIMES benchmark datasets at paragraph levels with no line break pretraining as opposed to [34]. The recognition task was exe- cuted without needing prior segmentation or a lexicon model on both datasets. We achieved 4.2%, 5.62%, and 2.18% CER on READ2016, IAM, and RIMES, respectively, resulting in a slight 1% decrease in CER and WER compared to the most recent work on paragraphs by Denis et al. [34]. Although the results of [3] marginally surpass our CER by approximately 0.08% on the RIMES test set, their model was based on segmentation and used a lexicon model (LM) constraint to assist the BLSTM CTC decoderâ€™s predictions.

Qualitative analysis

We provide the complete ground truth and predictions for the image in Fig. 5. We present a selection of paragraph image predictions, illustrating the accuracy and reliability of our model, even when con- fronted with obstacles like overlapping characters and non-text noise. For instance, line 5 of the input image contains non-text noise, yet the predictions remain robust. Moreover, the model successfully handled an ambiguous non-textual element (highlighted with a manually drawn square) in the paragraph situated middle-left of Fig. 5. The model correctly predicted the text preceding and following this non-textual element, demonstrating its resilience to non-textual components in documents.
Fig. 3 showcases heatmaps of various positions in a paragraph at the decoder layer multi-head self-attention. We highlight character positions 110 and 330, showing that the model accurately locates these positions. The heatmaps support our hypothesis that the proposed model can competently predict full-page documents with varied context
layouts, encompassing text, non-text, diagrams, graphs, and other com- ponents. Despite the overall success, we acknowledge some challenges. Certain characters, such as â€˜â€˜f,m,u, and bâ€™â€™ were mispredicted due to significant overlap and intra-class variability amongst associated words. For example, the words â€˜â€˜andâ€™â€™ and â€˜â€˜butâ€™â€™ were erroneously predicted as â€˜â€˜audâ€™â€™ and â€˜â€˜lutâ€™â€™, respectively, as evidenced in the predictions under paragraph 5. However, these challenges can be addressed in future work by applying a lexicon and auto-correction models.

Comparison to the SOTA paragraph models

Table 3 compares the performance of the recent related models based on CER and WER across three benchmark datasets: RIMES, IAM, and READ 2016. The table presents a juxtaposition between the pro- posed joint attention model (with and without augmentation) and other contemporary models, including the vertical attention model, joint attention model with MDLSTM, a no-attention model with augmen- tation, HTR detection and transcription, scan-attend-read MDLSTM, joint attention MDLSTM, start-follow-read CNN-LSTM, and LexiconNet. CER and WER, critical metrics for evaluating handwriting recognition models, quantify the percentage of inaccurately recognized characters and words, respectively. Lower scores in both these metrics indicate superior model performance.
Table 3 also delineates two additional columns labeled â€˜Lexâ€™ and â€˜Segâ€™, indicating the model is lexicon-based and segmentation-based, respectively. Our proposed joint attention model with augmentation surpasses most other models in CER and WER across all three datasets. The model does not necessitate explicit line segmentation, employing a split attention mechanism for implicit paragraph image segmenta- tion instead. Our joint attention model with augmentation exhibits the lowest CER and WER scores for the IAM, Rimes, and READ2016 datasets compared to all other models in the table. The vertical atten- tion model [34], while outperforming many other models, including our proposed joint attention-based approach, relies on a pre-trained line dataset and utilizes line break as a post-processing technique to strengthen model performance. LexiconNet [10], exhibiting compet- itive results, is reliant on [34] as a pre-trained model and employs lexicons, which may only sometimes be readily available or pragmatic for use.

Conclusion and future work

This study focuses on handwritten text recognition (HTR) at the paragraph level, introducing an end-to-end solution that leverages attention-based feature extraction and multi-head attention transformer-based encoderâ€“decoder architecture. Our proposed archi- tecture, which eliminates the need for line segmentation and a prede- fined lexicon model, streamlines the training process for the automatic


Table 3
Paragraph handwriting text recognition performance (CER, WER) using the ResneSt-Transformer model compared to recent Handwriting Paragraph recognition models on the IAM, RIMES, and READ datasets.


transcription of entire text paragraphs. Incorporating an attention mechanism within the feature extraction convolutional neural network (CNN) backbone (ResNeSt50) and the encoderâ€“decoder Transformer- based, our model brings up joint attention across the backbone net- work and the encoderâ€“decoder portion. Accordingly, the model per- formance is enhanced with the assistance of these attentive feature representations. Despite demonstrating competitive results on the IAM, READ2016, and RIMES datasets, our model requires considerable train- ing data for optimal performance, which may constrain its practical application in specific contexts. However, our model can accurately transcribe image paragraphs of varying sizes, as substantiated by the qualitative result predictions.
Our future work will explore the potential of pre-processing tech- niques, such as recursive slanted text adjustment and illumination com- pensation, to enhance performance. Additionally, we plan to conduct experiments with and without label smoothing and various transformer variations to bolster model efficiency. Another promising avenue for further research that has emerged from our reviewersâ€™ feedback is the integration of a syntax checker within the output of our architecture. While the scope of the present work was primarily focused on the novel combination of ResneSt50 with a transformer encoderâ€“decoder model, adding a syntax checker could provide another layer of refinement specifically aimed at ensuring the grammatical correctness of the gen- erated text. This enhancement would benefit commercial applications where high-quality, error-free output is crucial. We look forward to ex- ploring this feature in our subsequent research. While not implemented in this study, this represents an exciting direction for future work, enabling us to continue improving our modelâ€™s practical applicability and effectiveness. Future efforts will also extend our model capabilities to full-page recognition, accompanied by an in-depth analysis of the training process and optimal model selection. Moreover, we plan to broaden the generalizability of the model to accept input text in any script, facilitating efficient parallelization and reducing training time on large datasets.

CRediT authorship contribution statement

Mohammed Hamdan: Conceptualization, Methodology, Data anal- ysis, Data processing, Literature review, Training analysis, Interpreta- tion of results, Writing of the manuscript. Mohamed Cheriet: Overall critical revision of the manuscript and final approval of the manuscript.

Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

Data availability

Data will be made available on request
Acknowledgments

The authors would like to thank NSERC, Canada for their financial support under grant # 2019-05230.

References

Graves A, Liwicki M, FernÃ¡ndez S, Bertolami R, Bunke H, Schmidhuber J. A novel connectionist system for unconstrained handwriting recognition. IEEE Trans Pattern Anal Mach Intell 2008;31(5):855â€“68.
Bianne-Bernard A-L, Menasri F, Mohamad RA-H, Mokbel C, Kermorvant C, Likforman-Sulem L. Dynamic and contextual information in HMM model- ing for handwritten word recognition. IEEE Trans Pattern Anal Mach Intell 2011;33(10):2066â€“80.
Wigington C, Tensmeyer C, Davis B, Barrett W, Price B, Cohen S. Start, follow, read: End-to-end full-page handwriting recognition. In: Proceedings of the European conference on computer vision. ECCV, 2018, p. 367â€“83.
Inunganbi S. CP, K. M. Meitei Mayek handwritten dataset: compilation, segmentation, and character recognition. Vis Comput 2021;37:291â€“305.
Le AD. Automated transcription for pre-modern Japanese Kuzushiji documents by random lines erasure and curriculum learning. 2020, arXiv e-prints.
Yousef M, Hussain KF, Mohammed US. Accurate, data-efficient, uncon- strained text recognition with convolutional neural networks. Pattern Recognit 2020;108:107482.
Dolfing HJGA. Whole page recognition of historical handwriting. 2020, arXiv e-prints.
Sharma A, Jayagopi DB. Towards efficient unconstrained handwriting recognition using dilated temporal convolution network. Expert Syst Appl 2021;164:114004.
Singh SS, Karayev S. Full page handwriting recognition via image to sequence extraction. 2021, p. 55â€“69.
Kumari L, Singh S, Rathore VVS, et al. LexiconNet: An end-to-end handwritten paragraph text recognition system. 2022, arXiv e-prints.
Coquenet D, Chatelain C, Paquet T. End-to-end handwritten paragraph text recognition using a vertical attention network. IEEE Trans Pattern Anal Mach Intell 2022;45:508â€“24.
Nag S, Ganguly PK, Roy S, et al. Offline extraction of indic regional language from natural scene image using text segmentation and deep convolutional sequence. 2018, p. 49â€“68.
Chowdhury A, Vig L. An efficient end-to-end neural model for handwritten text recognition. 2018, arXiv.
Carbonell M, FornÃ©s A, Villegas M, et al. A neural model for text localization, transcription and named entity recognition in full pages. Pattern Recognit Lett 2020;136:219â€“27.
Bartz C, Seidel L, Nguyen D-H, et al. Synthetic data for the analysis of archival documents: Handwriting determination. 2020, p. 1â€“8.
Malik S, Sajid A, Ahmad A, et al. An efficient skewed line segmentation technique for cursive script OCR. Sci Program 2020;2020.
Kaur H, Kumar M. Offline handwritten Gurumukhi word recognition using extreme gradient boosting methodology. Soft Comput 2021;25:4451â€“64.
Peng D, Jin L, Ma W, et al. Recognition of handwritten Chinese text by segmentation: A segment-annotation-free approach. IEEE Trans Multimedia 2022;1.
Onim MdSH, Nyeem H, Roy K, Hasan M, Ishmam A, Akif MdAH, Ovi TB. BLPnet: A new DNN model and Bengali OCR engine for automatic li- cence plate recognition. Array 2022;15:100244. http://dx.doi.org/10.1016/j. array.2022.100244.
Etaiwi W, Awajan A. SemanticGraph2Vec: Semantic graph embedding for text representation. Array 2023;17:100276. http://dx.doi.org/10.1016/j.array.2023. 100276.



Minhuz Uddin Ahmed AJMd, Uddin MdA, Rahman MdA. Developing an algo- rithm for sequential sorting of discrete and connected characters using image processing of multi-line license plates. Array 2021;10:100063. http://dx.doi.org/ 10.1016/j.array.2021.100063.
Colter Z, Fayazi M, Youbi ZB-E, Kamp S, Yu S, Dreslinski R. Tablext: A combined neural network and heuristic based table extractor. Array 2022;15:100220. http:
//dx.doi.org/10.1016/j.array.2022.100220.
Jayech K, Mahjoub MA, Amara NEB. Synchronous multi-stream hidden markov model for offline Arabic handwriting recognition without explicit segmentation. Neurocomputing 2016;214:958â€“71.
Roy PP, Bhunia AK, Pal U. Date-field retrieval in scene image and video frames using text enhancement and shape coding. Neurocomputing 2018;274:37â€“49.
Bluche T. Joint line segmentation and transcription for end-to-end handwritten paragraph recognition. Adv Neural Inf Process Syst 2016;29.
Bluche T, Louradour J, Messina R. Scan, attend and read: End-to-end handwritten paragraph recognition with mdlstm attention. In: 2017 14th IAPR international conference on document analysis and recognition, Vol. 1. ICDAR, IEEE; 2017, p. 1050â€“5.
Schall M, Schambach M-P, Franz MO. Multi-dimensional connectionist classifi- cation: Reading text in one step. In: 2018 13th IAPR international workshop on document analysis systems. DAS, IEEE; 2018, p. 405â€“10.
Yousef M, Bishop TE. OrigamiNet: weakly-supervised, segmentation-free, one- step, full page text recognition by learning to unfold. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020, p. 14710â€“9.
Puigcerver J. Are multidimensional recurrent layers really necessary for hand- written text recognition? In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR), Vol. 1. IEEE; 2017, p. 67â€“72.
Hamdan M, Chaudhary H, Bali A, Cheriet M. Refocus attention span networks for handwriting line recognition. Int J Docum Anal Recogn (IJDAR) 2022;1â€“17.
Naz S, Umar AI, Ahmad R, Ahmed SB, Shirazi SH, Siddiqi I, Razzak MI. Offline cursive Urduâ€“Nastaliq script recognition using multidimensional recurrent neural networks. Neurocomputing 2016;177:228â€“41.
Bluche T, Messina R. Gated convolutional recurrent neural networks for multi- lingual handwriting recognition. In: 2017 14th IAPR international conference on document analysis and recognition, Vol. 1. ICDAR, IEEE; 2017, p. 646â€“51.
Kaltenmeier A, Caesar T, Gloger JM, Mandler E. Sophisticated topology of hidden Markov models for cursive script recognition. In: Proceedings of 2nd international conference on document analysis and recognition (ICDARâ€™93). IEEE; 1993, p. 139â€“42.
Coquenet D, Chatelain C, Paquet T. End-to-end handwritten paragraph text recognition using a vertical attention network. IEEE Trans Pattern Anal Mach Intell 2022.
Sueiras J, Ruiz V, Sanchez A, Velez JF. Offline continuous handwriting recognition using sequence to sequence neural networks. Neurocomputing 2018;289:119â€“28.
Ma M, Wang Q-F, Huang S, Huang S, Goulermas Y, Huang K. Residual attention- based multi-scale script identification in scene text images. Neurocomputing 2021;421:222â€“33.
Graves A, Schmidhuber J. Offline handwriting recognition with multidimensional recurrent neural networks. Adv Neural Inf Process Syst 2008;21.
Papavassiliou V, Stafylakis T, Katsouros V, Carayannis G. Handwritten doc- ument image segmentation into text lines and words. Pattern Recognit 2010;43(1):369â€“77.
Graves A, FernÃ¡ndez S, Gomez F, Schmidhuber J. Connectionist temporal clas- sification: labelling unsegmented sequence data with recurrent neural networks. In: Proceedings of the 23rd international conference on machine learning. 2006,
p. 369â€“76.
Mohamed M, Gader P. Handwritten word recognition using segmentation- free hidden Markov modeling and segmentation-based dynamic programming techniques. IEEE Trans Pattern Anal Mach Intell 1996;18(5):548â€“54.
Chen M-Y, Kundu A, Zhou J. Off-line handwritten word recognition using a hidden Markov model type stochastic network. IEEE Trans Pattern Anal Mach Intell 1994;16(5):481â€“96.
Tensmeyer C, Wigington C. Training full-page handwritten text recognition models without annotated line breaks. In: 2019 international conference on document analysis and recognition. ICDAR, IEEE; 2019, p. 1â€“8.
Chung J, Delteil T. A computationally efficient pipeline approach to full page offline handwritten text recognition. In: 2019 international conference on document analysis and recognition workshops, Vol. 5. ICDARW, IEEE; 2019, p. 35â€“40.
Carbonell M, Mas J, Villegas M, FornÃ©s A, LladÃ³s J. End-to-end handwritten text detection and transcription in full pages. In: 2019 international conference on document analysis and recognition workshops, Vol. 5. ICDARW, IEEE; 2019, p. 29â€“34.
Voigtlaender P, Doetsch P, Ney H. Handwriting recognition with large multi- dimensional long short-term memory recurrent neural networks. In: 2016 15th international conference on frontiers in handwriting recognition. ICFHR, IEEE; 2016, p. 228â€“33.
Wigington C, Stewart S, Davis B, Barrett B, Price B, Cohen S. Data augmentation for recognition of handwritten words and lines using a CNN-LSTM network. In: 2017 14th IAPR international conference on document analysis and recognition, Vol. 1. ICDAR, IEEE; 2017, p. 639â€“45.
Michael J, Labahn R, GrÃ¼ning T, ZÃ¶llner J. Evaluating sequence-to-sequence models for handwritten text recognition. In: 2019 international conference on document analysis and recognition. ICDAR, IEEE; 2019, p. 1286â€“93.
Coquenet D, Chatelain C, Paquet T. Recurrence-free unconstrained handwritten text recognition using gated fully convolutional network. In: 2020 17th interna- tional conference on frontiers in handwriting recognition. ICFHR, IEEE; 2020, p. 19â€“24.
Moysset B, Kermorvant C, Wolf C. Full-page text recognition: Learning where to start and when to stop. In: 2017 14th IAPR international conference on document analysis and recognition, Vol. 1. ICDAR, IEEE; 2017, p. 871â€“6.
Khare V, Shivakumara P, Navya B, Swetha G, Guru D, Pal U, Lu T. Weighted- gradient features for handwritten line segmentation. In: 2018 24th international conference on pattern recognition. ICPR, IEEE; 2018, p. 3651â€“6.
Marti U-V, Bunke H. The IAM-database: an english sentence database for offline handwriting recognition. Int J Docum Anal Recognit 2002;5(1):39â€“46.
Grosicki E, Carre M, Brodin J-M, Geoffrois E. RIMES evaluation campaign for handwritten mail processing. 2008.
Zhang H, Wu C, Zhang Z, Zhu Y, Lin H, Zhang Z, Sun Y, He T, Mueller J, Manmatha R, et al. Resnest: Split-attention networks. 2020, arXiv preprint arXiv:2004.08955.
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Å, Polosukhin I. Attention is all you need. Adv Neural Inf Process Syst 2017;30.
Xie S, Girshick R, DollÃ¡r P, Tu Z, He K. Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, p. 1492â€“500.
Jeny AA, Sakib ANM, Junayed MS, Lima KA, Ahmed I, Islam MB. Sknet: A convolutional neural networks based classification approach for skin cancer classes. In: 2020 23rd international conference on computer and information technology. ICCIT, IEEE; 2020, p. 1â€“6.
Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidi- rectional transformers for language understanding. 2018, arXiv preprint arXiv: 1810.04805.
Michael J, Labahn R, GrÃ¼ning T, ZÃ¶llner J. Evaluating sequence-to-sequence models for handwritten text recognition. In: 2019 international conference on document analysis and recognition. ICDAR, IEEE; 2019, p. 1286â€“93.
Kang L, Toledo JI, Riba P, Villegas M, FornÃ©s A, Rusinol M. Convolve, attend and spell: An attention-based sequence-to-sequence model for handwritten word recognition. In: German conference on pattern recognition. Springer; 2018, p. 459â€“72.
Marti U-V, Bunke H. The IAM-database: an english sentence database for offline handwriting recognition. Int J Docum Anal Recognit 2002;5:39â€“46.
Grosicki E, El-Abed H. Icdar 2011-french handwriting recognition competition. In: 2011 international conference on document analysis and recognition. IEEE; 2011, p. 1459â€“63.
Sanchez JA, Romero V, Toselli AH, Vidal E. ICFHR2016 competition on handwrit- ten text recognition on the read dataset. In: 2016 15th international conference on frontiers in handwriting recognition. ICFHR, 2016, p. 630â€“5.
Konstantinidis S. Computing the levenshtein distance of a regular language. In: IEEE information theory workshop, Vol. 2005. IEEE; 2005, p. 4.
Moreno P, Ho P, Vasconcelos N. A Kullback-Leibler divergence based kernel for SVM classification in multimedia applications. Adv Neural Inf Process Syst 2003;16.
MÃ¼ller R, Kornblith S, Hinton GE. When does label smoothing help? Adv Neural Inf Process Syst 2019;32.
Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. 2014, arXiv preprint arXiv:1409.0473.
