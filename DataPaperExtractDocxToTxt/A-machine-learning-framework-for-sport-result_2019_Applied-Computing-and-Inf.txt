Applied Computing and Informatics 15 (2019) 27–33







Original Article
A machine learning framework for sport result prediction
Rory P. Bunker a, Fadi Thabtah b,⇑
a Auckland University of Technology, Auckland, New Zealand
b Applied Business, Nelson Marlborough Institute of Technology, Auckland, New Zealand



a r t i c l e  i n f o 

Article history:
Received 4 May 2017
Revised 8 September 2017
Accepted 16 September 2017
Available online 19 September 2017

Keywords: Machine learning Event forecasting Data mining
Sport result prediction
a b s t r a c t 

Machine learning (ML) is one of the intelligent methodologies that have shown promising results in the domains of classification and prediction. One of the expanding areas necessitating good predictive accu- racy is sport prediction, due to the large monetary amounts involved in betting. In addition, club man- agers and owners are striving for classification models so that they can understand and formulate strategies needed to win matches. These models are based on numerous factors involved in the games, such as the results of historical matches, player performance indicators, and opposition information. This paper provides a critical analysis of the literature in ML, focusing on the application of Artificial Neural Network (ANN) to sport results prediction. In doing so, we identify the learning methodologies utilised, data sources, appropriate means of model evaluation, and specific challenges of predicting sport results. This then leads us to propose a novel sport prediction framework through which ML can be used as a learning strategy. Our research will hopefully be informative and of use to those performing future research in this application area.
© 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Introduction

One of the common machine learning (ML) tasks, which involves predicting a target variable in previously unseen data, is classification [28,1]. The aim of classification is to predict a target variable (class) by building a classification model based on a train- ing dataset, and then utilizing that model to predict the value of the class of test data [45]. This type of data processing is called supervised learning since the data processing phase is guided toward the class variable while building the model (see Fig. 1) [41]. Some common applications for classification include loan approval, medical diagnoses, email filtering, among others [2,42].
Sport prediction is usually treated as a classification problem, with one class (win, lose, or draw) to be predicted [33]. Although some researchers e.g. [7], have also looked at the numeric predic- tion problem, where they predict the winning margin – a numeric value. In sport prediction, large numbers of features can be col-

* Corresponding author.
E-mail address: fadi.fayez@nmit.ac.nz (F. Thabtah).
Peer review under responsibility of King Saud University.
lected including the historical performance of the teams, results of matches, and data on players, to help different stakeholders understand the odds of winning or losing forthcoming matches. The decision of which team is likely to win is important because of the financial assets involved in the betting process; thus book- makers, fans, and potential bidders are all interested in approxi- mating the odds of a game in advance [9]. Once a predicted result for the match is obtained, an additional problem is to then decide whether to bet on the match, given the bookmaker’s odds. In addition, sport managers are striving to model appropriate strategies that can work well for assessing the potential opponent in a match [31]. Therefore, the challenge of predicting sport results is something that has long been of interest to different stakehold- ers, including the media. The increasing amount of data related to sports that is now electronically (and often publically) available, has meant that there has been an increasing interest in developing intelligent models and prediction systems to forecast the results of matches.
In this paper, we provide a critical survey of the literature on ML for sport result prediction, focusing on the use of neural network (NN) for this problem. Several studies in the statistical and opera- tions research literature have previously considered sport results prediction, but the use of the NN paradigm for this purpose is a more recent area of study. The powerful NN technique has proven to be effective in deriving highly accurate classification models in other domains [29]. Discussions on the challenges that arise when



https://doi.org/10.1016/j.aci.2017.09.005
2210-8327/© 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. Supervised learning versus unsupervised learning [24].


using these intelligent models for sport results prediction is also provided. Our main contribution is that a CRISP-DM type frame- work for sport result prediction is proposed (SRP-CRISP-DM), based on the six steps of the standard CRISP-DM framework [38]. This paper serves researchers, sport fans, club managers, bookmakers, academics, and students who are interested in intelligent solutions based on NN for the challenging problem of sport results predic- tion. This paper will be of use to those who are interested in pur- suing future research within this application domain.
This remainder of this paper is organized as follows. In Section 2, studies that have used ANN exclusively, which was the key approach used in earlier research papers in the sport prediction application, are reviewed. Section 3 then provides critical discus- sion and observations on prior work in this application domain, in the context of the proposed SRP-CRISP-DM framework, conven- tional measures of model performance, and how we propose that model performance should be measured for the problem of sport results prediction. Finally, Section 4 concludes the paper.

Literature review and critical analysis

Artificial Neural Networks (ANNs) [10] are perhaps the most commonly applied approach among ML mechanisms to the sport result prediction problem. Thus, for this review, we focus on stud- ies that have applied ANNs. An ANN usually contains intercon- nected components (neurons) that transform a set of inputs into a desired output [45]. See Fig. 2 for an example of an ANN struc- ture. The power of ANN comes from the non-linearity of the hidden neurons in adjusting weights that contribute to the final decision. ANN output often relies on input features and other components


Fig. 2. Example structure of an ANN with 4 input nodes in the input layer, 5 hidden nodes in the hidden layer and one output node in the output layer [30].
associated with the network, such as these weights. The ANN model is constructed after processing the training dataset that con- tains the features used to build the ANN classification model. In other words, weights associated with interconnected components are continuously changing to accomplish high levels of predictive accuracy. These changes are performed by the ANN algorithm to fulfill the desired model’s accuracy given earlier by the user. This may lead in some cases to the problem of overfitting, as well as wasting computing resources such as training time and memory [27]. An appealing feature of ANNs is that they are quite flexible in terms of how the class variable is defined e.g. whether it is prob- ability of victory e.g. [25], or whether two classes are used e.g. with home goals and away goals represented in the two different classes
e.g. [3].
Purucker [32] conducted one of the initial studies on predicting results in the National Football League (NFL) using an ANN model. Data from the first eight rounds of the competition and five fea- tures were used, consisting of yards gained, rushing yards gained, turnover margin, time of possession, and betting line odds. Unsu- pervised methods based on clustering were used to distinguish between good and poor teams. An ANN with backward- propagation (BP) was then used [37]. Purucker achieved 61% accu- racy compared with 72% accuracy of the domain experts. The BP algorithm was found to be the most effective approach. A limita- tion of this study is that only a relatively small number of features were used.
Kahn [18] extended the work of Purucker [32] and achieved greater accuracy, performing slightly better than experts in the NFL who were making predictions on the same games. Data on 208 matches in the 2003 season were collected. The features that were used were: total yardage differential, rushing yardage differ- ential, turnover differential, away team indicator and home team indicator. There were two classes: away team outcome and home team outcome – a value of —1 indicating that the team lost the match, and a value of +1 indicating that the team won the match. The problem was treated as a classification problem. The first 192 matches were used as the training data set, and the remaining rounds (week 14 and 15) were used as the test set. Through testing, a network structure of 10-3-2 was found to be optimal. Accuracy of 75% was achieved across the week 14 and 15 matches. The results were compared to the predictions of eight sportscasters from ESPN.com. Across the same matches, the domain experts predicted an average of 63% of matches correctly.
McCabe and Trevathan [25] attempted to predict results in four different sports: NFL (Rugby League), AFL (Australian Rules foot- ball), Super Rugby (Rugby Union), and English Premier League Football (EPL) using data back to the year 2002. A multi-layer perceptron, trained with BP and conjugative-gradient algorithms was used. The ANN had 20 nodes in the input layer, 10 nodes in



the hidden layer, and 1 node in the output layer (20-10-1). Fea- tures that were used were the same across all the sports and attri- butes related to specific events within a rugby or soccer match were not considered. The average performance of the ANN algo- rithm in predicting results was around 67.5%, compared with expert tipster predictions that achieved around 60–65% accuracy. ANN has also been applied by Davoodi and Khanteymoori [6] to predict the results of horse races. The authors used data from 100 races at the Aqueduct Race Track held in New York during January of 2010. One ANN was used for each horse in the race, with the out- put being the finishing time of that horse. Eight features were used for the input nodes in each NN. These were horse weight, type of race, horse trainer, horse jockey, number of horses in race, race dis- tance, track condition, and weather. This optimal network architec- ture (8-2-1), in terms of mean-squared error, consisted of four layers: an input layer (with eight input nodes), two hidden layers, and an output layer (with horse finishing time). Five different training algorithms were applied to the data: gradient-descent BP (BP), gradient-descent with a momentum parameter (BPM), Levenberg-Marquadt (LM), and conjugate gradient descent (CGD) [37,39,20,46]. It was found that with 400 epochs, the BPM (with momentum parameter of 0.7) and the BP algorithms were most effective at predicting the winner of the race, with BP obtaining an accuracy of 77%. However, the disadvantage of BP was that the training time was lengthy (LM had the shortest training time). Tax and Joustra [40] used Dutch football competition data from the past 13 years to predict the results of football matches. The authors were interested in how a model with betting odds alone compared with a hybrid model of both betting odds and other match features. Importantly, and something that has most often been missed in previous studies, they mentioned that cross valida- tion is not appropriate for sport prediction because of the time- ordered nature of the data. A structured literature review from sta- tistical and sport science papers was conducted to identify relevant features to include. Principal component analysis (PCA), sequential forward selection, ReliefF attribute evaluation, and correlation based feature subset selection were used [16,21,19,11,19]. Nine classification algorithms were used in the experimentation, utiliz- ing the machine learning software WEKA, namely naive Bayes, LogitBoost (with decision stumps), NN with BP, Random Forest, CHIRP, FURIA, DTNB, C4.5, and hyper pipes [13,43,14,12,35]. The
highest performing classifiers on the full feature set were naive Bayes (used with a 3-component PCA), and the ANN (used with a 3 or 7-component PCA). Both achieved a classification accuracy of 54.7%. In a model including only betting odds features, the high- est accuracy of 55.3% was achieved with the FURIA classifier, and was slightly higher than the model with the full feature set (although not statistically significant). In a hybrid model of the public data features with the betting odds features, LogitBoost with ReliefF attribute selection provided the highest classification accu- racy of 56.1%. The difference between the public data model and the betting odds model was, however, not statistically significant according to McNemar’s test. However, this did highlight that bet- ting odds alone can be a reasonable predictor of match outcome.
In non-team sports, researchers have used machine learning models to predict the performance of the individual player. For instance, Maszczyk et al. [23] compared neural networks and non-linear regression to predict the distance of Javelin throws. The aim of the investigation was to identify the usefulness of neu- ral networks as an athlete recruitment tool, and how this com- pared to the commonly used regression models. The data set consisted of 70 javelin throws – a training set consisting of 40 cases, a validation set consisting of 15 cases, and a test set consist- ing of 15 cases. Their initial statistical analysis using a correlation matrix and regression analysis found four significant predictors of Javelin throw length: cross step, specific power of the arms
and the trunk, specific power of the abdominal muscles, and grip power. The numeric class variable used was the average distance of three throws from a full run-up after a 30 min warm up. Through experimentation, the best architecture in terms of nor- malized root mean squared error, of the neural network was found to be 4-3-1 (four input neurons/variables, one hidden layer with three neurons, and one outcome). The javelin throws of 20 javelin throwers from the Polish national team were predicted using the models, and were compared with the actual length of the throws. Their results showed that the neural network models offered much higher quality of prediction than the nonlinear regression model. The absolute network error was found to be 16.77 m, versus the absolute regression error of 29.45 m.
Edelmann-Nusser et al. [8] investigated modelling the perfor- mance of an elite female swimmer in the finals of the 200 m back- stroke at the Olympic Games in 2000 in Sydney. Data consisted of the performance output of 19 competitions in 200 m backstroke prior to the Olympics and data from the swimmer’s training period – the last 4 weeks prior to the competition. An MLP with 10 input neurons, 2 hidden neurons and 1 output neuron was used. The results show that the MLP was accurate; the error of the prediction was only 0.05 s. The MLP was also compared with linear regres- sion, which did not provide as accurate results. This paper as well as Maszczyk et al. [23] highlight the potential usefulness for machine learning techniques to be used by high performance staff and analysts in professional sport for identifying the factors to focus on when developing training programs, not just purely for result prediction.
Wiseman [44] predicted winning PGA golf score based on scores after round 1 of a competition. Note that they were predicting win- ning score, not tournament winner itself. The authors compared the performance of: linear regression, neural network regression, Bayesian linear regression, decision forest regression and boosted decision tree regression, in the Microsoft Azure service. The authors performed correlation matrix analysis of different features and selected Round 1 leading score, round 1 average score, course par, major event, course yardage and total prizemoney as the pre- dictors. R-squared value and MSE were used to evaluate algorithm accuracy. Data from 2004 to 2015 was used to construct the mod- els, and tournaments from 2016 were used to validate them. Linear regression and Bayesian linear regression were the best performing models on the 2016 data set, predicting the winning score to within 3 shots 67% of the time.

The proposed sport result prediction intelligent framework

We would argue that the use of a structured experimental approach to the problem of sport results prediction is useful to obtain the best possible results with a given data set. In this sec- tion, an intelligent architecture for sport results prediction is pre- sented, proposing steps of a possible ML framework, and describing the characteristics of the data used for sport results pre- diction, and how this fits within the framework. Our framework (Fig. 4) focuses on result prediction for team sports rather than individual sport. Our Sport Result Prediction CRISP-DM framework or SRP-CRISP-DM framework consists of six main steps, based on the steps of the standard CRISP-DM (Fig. 3) framework [38].

Domain understanding

Domain understanding includes comprehending the problem, the goal of the modelling, and the specific characteristics of the sport itself. This involves having some understanding of how the sport is played and what factors are potentially involved in deter- mining the outcome of matches. This could be obtained through






Fig. 3. The six phases of the traditional CRISP-DM Model [38].



personal knowledge of the sport or obtained by surveying existing literature or by consulting experts in the sport.
There also needs to be clarity regarding the objective of the model. It could be to predict results to compete with expert predic- tions, online competitions, or it could be to ultimately use the results of the model to bet on matches. If the predictive model is used for betting, there needs to also be consideration of which
matches will be bet on. For example, there will likely be some betting odds threshold where, although the model predicts a victory for that team, the betting odds are so low that the return does not warrant betting on that match at all (e.g. if a team is paying $1.01 to win, meaning that a $100 bet placed would only return $1).

Data understanding

Data for sport prediction is often able to be obtained online from publically available sources. Some prior studies have auto- mated the data collection process, writing scripts that automati- cally extract the online data and then load it into some form of database. Some studies have also built an end-user interface, where users can input data for an upcoming match and the predic- tion is then generated.
The granularity/level of the data is something that needs to be considered. Previous studies have generally had training data that is at the match/team level. It is also possible to include player-level data, which contains statistics on the players that have played in each of the matches. Player level data will generally be contained in a separate data set that would then have to be transposed and joined with the match level data so that each match has certain player statistics as attributes in the data set. Including player level data would have the advantage that we can investigate whether specific players’ actions or presence are important for the perfor- mance of the team in terms of whether they win or lose.
The definition of the class variable needs to also be considered. Most prior work has treated the sport prediction problem as a 2 or
3 class values classification problem (home win, away win) or (home win, draw, away win). Delen et al. [7], also considered the problem as a numeric prediction problem, using regression tech- niques to predict the points margin (home points minus away




Domain Understanding
Understand the problem and the objective of the model
Understand characteristics of the sport itself







Data Understanding
Source data (automate if possible)
Consider the level/granularity of the data (whether to include player level data)
Decide on the class variable





Data Preparation & Feature Extraction
Split original feature set into different subsets (in-play, external, expert-selected, betting odds)
Apply feature selection algorithms to select most important variables from original features and feature subsets
Preprocess data by averaging in-play variables for a certain match history for each team, and re-merge with the external features
6. Deploy Model
Automate source data extract and data pre- processing if possible
Re-train model based on fresh data
Generate predictions for upcoming matches








Select best performing model







5. Model Evaluation
Select measure of model performance – accuracy is fine if data is not imbalanced
	Preserve order of instances/matches – Cross- validation is not appropriate to use
Decide on training test split – recommend round- by-round split within each season as described.






Preprocessed data sets
Modelling
Select candidate models based on literature survey
Experiment with these candidate models on a range of different machine-selected and human- selected feature sets




Fig. 4. Steps of our proposed SRP-CRISP-DM framework.



points), and then making a win-loss prediction based on the pre- dicted points margin. The authors ultimately found that treating the problem as a classification resulted in superior results, but that is not to say that this would be the case on all sports or all data sets.

Data preparation & feature extraction

Creating feature subsets
Features in sport result data can be divided into several differ- ent subsets. Miljkovic et al. [26], for example, split the features into match-related and standings features. Tax and Joustra [40], consid- ered how a hybrid model of betting odds and public data features compared with a feature set of betting odds alone. Hucaljuk and Rakipovic [15] used a separate expert-selected feature set against their own feature set, to investigate the value of expert opinion for feature selection. And of course, there are feature sets that are selected based on feature selection algorithms – either on the full feature set or a subset of the original feature set. Ideally, researchers should test several different feature selection approaches in conjunction with testing their candidate classifica- tion models. Then, as Tax and Joustra [40] did, one can investigate which classifier and feature selection algorithm together produces the best classification accuracy.
Hucaljuk and Rakipovic [15] included an expert-selected fea- ture set in addition to their initial feature set. Although this was not found to result in improved accuracy, this could depend on the characteristics of the sport, and perhaps the experts them- selves. Another way that expert opinion can be used is in compar- ing the predictive accuracy of the predictive models, with the predictions of the experts. To incorporate expert opinion, one could either generate an expert-selected feature set to compare with machine-learned feature selection approaches, or alternatively, compare their model with expert predictions.

Data preprocessing: match features versus external features
There can be a distinction made between ‘match-related’ and ‘external’ features (see Fig. 5). Match-related features relate to actual events within the sport’s match. For example, in football these could be meters gained, passes made, and so on [17]. Exter- nal features do not relate to events within the match, that is are external to the match itself (e.g. recent form, travel, players avail- able for the match, etc.). This distinction is important for data pre- processing purposes. External features are known prior to the upcoming match to be played. For example, we know the distance that both teams have travelled and we know both teams’ recent form leading into the upcoming match. Match-related features however, are not known until the match has been played. Thus, we only know an average of these features for a certain number of past matches for these teams. For example, we would know the average passes made per match by both teams prior to the match, but do not know the actual passes made in the upcoming match until after it has been played. This means that only past average statistics for these features can be used to predict an upcoming match. Therefore, match-related features should undergo a separate averaging process before being re-merged with the external features. Buursma [4] followed this process, and found, through experimentation, that using an average across the past 20 matches resulted in the best classification accuracy.

Modelling

The first step in the modelling process is to select which candi- date models will be used in the experimentation [34]. This would involve a review of past literature, and identification of commonly applied predictive models that have previously been successful.
Each model can then be trialed on each feature subset, and subsets that have been selected by feature selection algorithms. Experi- mentation with these different feature selection methods and clas- sification models will identify the best combination of classifier and feature selection technique.

Sport prediction model evaluation

Measuring model performance
To evaluate model performance, one would classify match results into home wins, away wins and draws (if the sport has draws) and then look at the number of matches that the model has correctly identified, using a standard classification matrix. There is unlikely to be a great degree of imbalance in the class val- ues for the dataset, although given the commonly observed home advantage phenomenon, one is likely to see a slight skew in favor of home wins. In this case, classification accuracy is a reasonable measure of evaluation. In cases where the data is highly imbal- anced, ROC curve evaluation may be more appropriate.

Training and testing
As has been mentioned, it is important to preserve the order of the training data for the sport prediction problem, so that upcom- ing matches are predicted based on past matches only. Cross- validation (Fig. 6) generally involves shuffling the order of the instances and therefore is not an appropriate means of splitting the data into training and testing, for the sport result prediction problem. A held-out training test split is more appropriate, with the order of the instances being preserved. Machine learning soft- ware such as WEKA provide the option to preserve the order of instances.
An appropriate training-test split needs to be decided on. This may depend on the amount of data that the researcher has on hand – whether they have only one season of data, or multiple seasons. Usually professional sport competitions are organized in rounds, with teams playing matches over the weekend. Teams usually play one match in each round unless they have a ‘bye’. In the case where one season of data is on hand, the number of rounds that will be used for training the model, and the number of rounds that will be used for testing the model needs to be determined. For example, in a data set with 10 rounds of data, the first 7 rounds of the com- petition could be used for training the model and the last 3 rounds of the competition could be used for testing the model. However, to obtain a more realistic measure of model performance, round 1 could be used as training to test on round 2, round 1 & 2 could be used as training to test on round 3, round 1–3 could then be used as training to test on round 4, and so on. So, within a season which contains a certain number of competition rounds, we use rounds 1 to n — 1 to train our model, and use round n as the test data set, for each round n in N, where N is the total number of rounds in the competition. We thus obtain a classification accuracy for each of these training/test splits, and take an average of the accuracies to give an overall measure of model performance.
Rather than round-by-round prediction, another possibility is to update the training data set after every match has been played. In this case, all past matches up to the current match as training data, and the upcoming match as the training data (i.e. only having that one record as the training data). This is essentially like order- preserved leave-one-out cross-validation. This match-by-match approach is probably not necessary unless teams play more than one match over the same competition round.
Some papers have used multiple seasons of data. A common approach has been to use earlier seasons as training data, to predict the later seasons as the test data set. For example, Cao [5] used sea- sons up from 2005/2006 to 2009/2010 were used as the training data, and the 2010/2011 season was used as the test data, to eval-





Fig. 5. Match-Related statistics should go through an averaging process across a certain number of historical matches for each team, and then be re-merged with the external match features.


Fig. 6. Diagrammatic representation of 10-fold Cross-Validation [36].


uate models that predict basketball match results. Prior seasons may not be relevant to predict matches in future seasons, particu- larly in sports where team rosters and strengths can change signif- icantly from year to year. This approach may not give a reliable picture of model performance (although this could be mitigated to some extent if player level data is included, and so player changes would be captured from season to season). We would argue that, although more computationally intensive and labori- ous, our round-by-round training test split approach mentioned above should be used within each season. An average model classi- fication accuracy could then be produced for each season, and a plot could be shown of model accuracy by season.



Model deployment

Ideally, one can automate the process so that new round data is obtained from the web, and added to the match database (or other- wise added to the database manually by the end-user). The train- ing data and test data are then adjusted, the model is retrained with the new training data, and new matches are predicted. Predic- tions are then returned to the end user. The learning model in the proposed architecture could also be online and dynamically receiv- ing input data prior to the match beginning (external features) and while the match is played (match features). It also should be incre- mental in the way that the training data set is continuously
updated, and thus the classifier would keep changing to reflect those of the learning environment.


4. Conclusions

One of the vital applications in sport that requires good predic- tive accuracy is match result prediction. Traditionally, the results of the matches are predicted using mathematical and statistical models that are often verified by a domain expert. Due to the speci- fic nature of match-related features to different sports, results across different studies in this application can generally not be compared directly. Despite the increasing use of ML models for sport prediction, more accurate models are needed. This is due to the high volumes of betting on sport, and for sport managers seek- ing useful knowledge for modelling future matching strategies. Therefore, ML seems an appropriate methodology for sport predic- tion since it generates predictive models that can predict match results using predefined features in a historical dataset.
This article critically analyses some recent research on sport prediction that have used ANN, and following this, we proposed a sport result prediction ‘SRP-CRISP-DM’ framework for the com- plex problem of sport result prediction. Moreover, challenges fac- ing the sport prediction application were shown to pinpoint future work for scholars in this important application. Future stud- ies concerning ML in sport result prediction research will hopefully be benefitted by this study.



References

N. Abdelhamid, A. Ayesh, F. Thabtah, S. Ahmadi, W. Hadi, MAC: A multiclass associative classification algorithm, J. Info. Know. Mgmt. (JIKM) 11 (2) (2012) 125001-1–1250011-10, WorldScinet.
N. Abdelhamid, F. Thabtah, Associative classification approaches: review and comparison, J. Inform. Knowl. Manage. (JIKM) 13 (3) (2014).
A.C. Arabzad, M.E.T. Araghi, S.N. Soheil, Football match results prediction using artificial neural networks: the case of Iran pro league, Int. J. Appl. Res. Ind. Eng. 1 (3) (2014) 159–179.
D. Buursma, Predicting sports events from past results ‘‘Towards effective betting on football matches”, in: Conference Paper, presented at 14th Twente Student Conference on IT, Twente, Holland, 21 January 2011, 2001.
C. Cao, Sports data mining technology used in basketball outcome prediction. Master’s Thesis, Dublin Institute of Technology, Ireland, 2012.
E. Davoodi, A. Khanteymoori, Horse racing prediction using artificial neural networks, Recent Adv. Neural Networks, Fuzzy Syst. Evol. Comput. 2010 (2010) 155–160.
D. Delen, D. Cogdell, N. Kasap, A comparative analysis of data mining methods in predicting NCAA bowl outcomes, Int. J. Forecast. 28 (2) (2012) 543–552.
J. Edelmann-Nusser, A. Hohmann, B. Henneberg, Modeling and prediction of competitive performance in swimming upon neural networks, Eur. J. Sport Sci. 2 (2) (2002) 1–10.
M. Fernandez, B. Ulmer, Predicting Soccer Match Results in the English Premier League, 2014.
S. Grossberg, Nonlinear neural networks: principles, mechanisms, and architectures, Neural Networks (1988) 17–61.
M.A. Hall, Correlation-based Feature Subset Selection for Machine Learning, PhD Dissertation, Department of Computer Science, University of Waikato, 1999.
M. Hall, E. Frank, Combining naive Bayes and decision tables, in: Proc 21st Florida Artificial Intelligence Research Society Conference, Miami, Florida, AAAI Press, 2008.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, I. Witten, The WEKA data mining software: an update, SIGKDD Explor. 11 (1) (2009) 10–18.
J. Hühn, E. Hüllermeier, FURIA: an algorithm for unordered fuzzy rule induction, Data Min. Knowl. Disc. 19 (3) (2009) 293–319.
J. Hucaljuk, A. Rakipovic´, Predicting football scores using machine learning techniques, in: MIPRO, 2011 Proceedings of the 34th International Convention, IEEE, 2011, pp. 1623–1627.
I.T. Jolliffe, Principal Component Analysis, second ed., Springer-Verlag, Berlin, 2002.
A. Joseph, N.E. Fenton, M. Neil, Predicting football results using Bayesian nets and other machine learning techniques, Knowl.-Based Syst. 19 (2006) 544– 553.
J. Kahn, Neural network prediction of NFL football games, World Wide Web Electronic Publication 2003 (2003) 9–15.
F. Kamalov, F. Thabtah, A feature selection method based on ranked vector scores of features for classification, Ann. Data Sci. (2017) 1–20, Springer.
C. Kanzow, N. Yamashita, M. Fukushima, Levenberg-Marquardt methods with strong local convergence properties for solving nonlinear equations with convex constraints, JCAM 172 (2) (2004) 375–397.
Igor Kononenko et al., Overcoming the myopia of inductive learning algorithms with RELIEFF, Appl. Intell. 7 (1) (1997) 39–55.
A. Marcano-Cedeno, J. Quintanilla-Dominguez, M. Cortina-Januchs, D. Andina, Feature selection using sequential forward selection and classification applying artificial metaplasticity neural network, in: 36th Annual Conference on IEEE Industrial Electronics Society, 2010, pp. 2845–2850.
A. Maszczyk, A. Gołas´, P. Pietraszewski, R. Roczniok, A. Zaja˛ c, A. Stanula, Application of neural and regression models in sports results prediction, Proc. – Soc. Behav. Sci. 117 (2014) 482–487.
Mathworks (n.d.). machinelearning_supervisedunsupervised.png. Retrieved from
<https://de.mathworks.com/help/stats/machinelearning_supervisedunsupervised. png>.
A. McCabe, J. Trevathan, Artificial intelligence in sports prediction, in: Information Technology: New Generations, ITNG 2008. Fifth International Conference on, IEEE, 2008, pp. 1194–1197.
D. Miljkovic´, L. Gajic´, A. Kovacˇevic´, Z. Konjovic´, The use of data mining for basketball matches outcomes prediction, in: Intelligent Systems and Informatics (SISY), 2010 8th International Symposium on, IEEE, 2010, pp. 309–312.
R. Mohammad, F. Thabtah, L. McCluskey, Predicting phishing websites based on self-structuring neural network, Neural Comput. Appl. 25 (2) (2014) 443–
458.
R. Mohammad, F. Thabtah, L. McCluskey, Tutorial and critical analysis of phishing websites methods, Comput. Sci. Rev. J. 17 (2015) 1–24.
R. Mohammad, F. Thabtah, L. McCluskey, An improved self-structuring neural network, in: Pacific-Asia Conference on Knowledge Discovery and Data Mining, Auckland, New Zealand, 2016, pp. 35–47.
H. Mohamed, A. Negm, M. Zahran, O.C. Saavedra, Assessment of artificial neural network for bathymetry estimation using High Resolution Satellite imagery in Shallow Lakes: case study El Burullus Lake, in: International Water Technology Conference, 2015.
S. Nunes, N. Sousa, Applying data mining techniques to football data from European championships, Proceedings of Conferência de Metodologias de Investigação Científica (2006) 4–16.
M.C. Purucker, Neural network quarterbacking, IEEE Potentials 15 (1996) 9–
15.
D. Prasitio, D. Harlili, Predicting football match results with logistic regression, in: Proceedings of the 2016 International Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA), 16–19 Aug. 2016, Penang, Malaysia, 2016.
I. Qabajeh, F. Thabtah, F. Chiclana, A dynamic rule-induction method for classification in data mining, J. Manage. Anal. 2 (3) (2015) 233–253.
J. Quinlan, C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA, 1993.
Raschka (n.d.). k-fold.png. Retrieved from <https://sebastianraschka.com/ images/faq/evaluate-a-model/k-fold.png>.
D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning representations by back- propagating errors, Nature 323 (6088) (1986) 533–536.
C. Shearer, The CRISP-DM model: the new blueprint for data mining, J. Data Warehousing 5 (4) (2000) 13–22.
R.S. Sutton, Two problems with backpropagation and other steepest-descent learning procedures for networks, in: Proc. 8th Annual Conf. Cognitive Science Society, 1986.
N. Tax, Y.P. Joustra, Predicting the Dutch football competition using public data: A machine learning approach, Trans. Knowl. Data Eng. 10 (10) (2015) 1–
13.
F. Thabtah, S. Hammoud, H. Abdeljaber, Parallel associative classification data mining frameworks based mapreduce, To Appear in Journal of Parallel Processing Letter, March 2015, World Scientific, 2015.
F. Thabtah, R.M. Mohammad, L. McCluskey, A dynamic self-structuring neural network model to combat phishing, Neural Networks (IJCNN), 2016 International Joint Conference, Vancouver, Canada, 2016, pp. 4221–4226.
L. Wilkinson, A. Anand, D. Tuan, CHIRP: a new classifier based on composite hypercubes on iterated random projections, in: Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2011, vol. 11, pp. 6–14.
O. Wiseman, Using Machine Learning to Predict the Winning Score of Professional Golf Events on the PGA Tour (Doctoral dissertation, Dublin, National College of Ireland), 2016.
I.H. Witten, E. Frank, M.A. Hall, Data Mining: Practical Machine Learning Tools and Techniques, Morgan Kaufmann, Burlington, MA, 2011.
N. Yvan, Flexible conjugate gradients, SIAM J. Sci. Comput. 22 (4) (2000) 1444.
