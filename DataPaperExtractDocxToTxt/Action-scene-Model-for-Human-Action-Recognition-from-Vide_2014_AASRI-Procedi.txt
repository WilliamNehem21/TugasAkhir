Available online at www.sciencedirect.com
ScienceDirect

AASRI Procedia 6 (2014) 111 – 117


2013 2nd AASRI Conference on Computational Intelligence and Bioinformatics
Action-Scene Model for Human Action Recognition from Videos*
Yifei Zhang+, Wen Qu, Daling Wang
Northeastern University, Shenyang 110819, China




Abstract

Human action recognition from realistic videos attracts more attention in many practical applications such as on-line video surveillance and content-based video management. Single action recognition always fails to distinguish similar action categories due to the complex background settings in realistic videos. In this paper, a novel action-scene model is explored to learn contextual relationship between actions and scenes in realistic videos. With little prior knowledge on scene categories, a generative probabilistic framework is used for action inference from background directly based on visual words. Experimental results on a realistic video dataset validate the effectiveness of the action-scene model for action recognition from background settings. Extensive experiments were conducted on different feature extracted methods, and the results show the learned model has good robustness when the features are noisy.
© 2014 The Authors. Published by Elsevier B. V. Open access under CC BY-NC-ND license.
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute

Keywords: action recognition, contextual cue, video processing, action-scene model






* Supported by the Key Program of National Natural Science Foundation of China under Grant No.61033007 and the Fundamental Research Funds for the Central Universities of China under Grant No.N100304004.


+ Corresponding author. Tel.: +86-(0)24-83687776
E-mail address: zhangyifei@mail.neu.edu.cn









2212-6716 © 2014 The Authors. Published by Elsevier B. V. Open access under CC BY-NC-ND license.
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute doi:10.1016/j.aasri.2014.05.016


Introduction

As more and more video surveillance systems appear in every walk of life, automatic human action recognition from realistic videos has become a major concern for both academic researchers and commercial companies. In action recognition, one fundamental problem is to recognize whether an action is belong to a given action category. Although a great amount of impressive results have been achieved on datasets collected from controlled environments, such as KTH [1], WEIZMEN [2], much less progresses have been made on realistic videos due to complex background settings and action alternations from realistic videos. However, the intricate background and varied actions equally give us a clue to explore the contextual relationship between actions and scenes for action recognition on realistic videos.
As human actions always occur under particular scenes where a rich source of contextual cues will present, the contextual relationships of background settings could be learned as a complement for action recognition. Although recognition based on scene detectors have become commonplace in the related literatures [3,4], a main problem of detector based methods is how to select general scene and action categories for all videos. Otherwise, training detectors is time-consuming and performances of recognition will be affected by the learned detectors.
In this paper, we intend to learn contextual cues using a generative framework with little prior knowledge on scene categories. The contextual relationship between actions and scenes could be used to infer actions from background settings. An action-scene model is proposed to automatically model the relationship between actions, scenes and background features, where the number of scenes only need to be given instead of the categories of scenes. Firstly, a video will be segmented into person regions and background regions. Then the relationship of a given action category and scenes will be modeled using an action-scene model. Finally a factor is computed based on the proposed model to measure the importance of learned context cue and the responding probability of action category is given from this model.
The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes the process of feature extraction and video representation. Action-scene model is proposed in Section 4 and experimental results are shown in section 5. Finally we present concluding remarks and future work in Section 6.

Related Work

Human action recognition has been an important area of research in the field of computer vision. From single scenario to complicated scenario, various approaches are presented for action recognition to meet different application demands.
Early works were focus on action recognition with single scene [5]. However, this scenario is too ideal to appear in realistic circumstances. As more attention paid to complex scenarios, some recent approaches [3,6,7,8] tried to deal with “wild” videos. Laptev et al automatically annotated movie videos using multi- channel non-linear SVMs [6]. Liu et al proposed a framework to recognize actions which combined both motion and static features [7]. These appear-based learning approaches work hard to select general scene types and object categories suitable for all videos. To handle the complex realistic videos, contextual semantics are used in lots of works. Ikizler-Cinbis et al combined the features of object, scene and actions in a multiple instance learning framework [9]. However, they didn’t learn the relationship between objects, actions and scenes for action identification. Marszalek et al exploited the context of movie scripts and developed a joint scene-action SVM-based classifier by training several scene detectors [4]. However, due to a lack of training data for all the potential objects and scenes, it is time-consuming to collect annotated data for each category. In addition, a semantic gap would be generated between the learned contextual cues from texts and


visual features.
In this paper, we propose a novel approach different from the detector-based methods in learning contextual cues that has no use for action and scene categories. Inspired by the recent work of action recognition in static images [10] and unsupervised learning method in [2], we use a generative model to represent action and scene in videos. The underlying action-scene dependency is captured on the action-scene model directly from visual features.

Visual Feature Extraction and Video Representation

Body detection

The person body detection in videos is based on Felzenszwalb's object detector [11] and mean shift tracking [12]. We first use the object detector to find candidate person regions from each frame image. Since there are many false alarms and miss-detections, a sliding window based method is used to discard false alarms and track person area for missing detections. Two detections are viewed as similar if the bounding boxes overlap exceeds 40%. The window size is empirically set as 15 frames and the threshold is half of the windows size. Then, the miss detections are filled with tracking from previous frames using mean shift algorithm. The person detection provides the bonding boxes of person location at each frame.

Features extraction

Person-centric feature extraction. We use the spatio-temporal interest point detector proposed by Dollar
to capture the spatially and temporally interesting movements. After extracting the person-centric features from a video clip, we describe each interest point with HOG (Histograms of oriented Gradients)
descriptor.
Background feature extraction. The non-person region is seen as background region. In order to capture the color, shape and local features of the background, we extract color histograms for color features, Gist descriptors [15] for shape features and SIFT descriptors [16] from key frames for static features.

Bag-of-features video representation

Bag-of-feature based video representation has approved to be effective for action recognition in unstrained videos [7]. The bag-of-feature model represents a video clip as a vector over a feature vocabulary or “visual codebook”. The “visual codebook” is constructed by clustering visual features detected from all videos. The center of each cluster is defined to be a codeword or visual word in the codebook. Thus, each detected feature in a video clip can be assigned to a unique cluster having the nearest distance. The ith bin of bag-of-feature is the number of features that be assigned to the ith cluster in the video.

Learning of Action-scene Relationship

The detector based context learning needs to know the prior knowledge about scene categories which is usually dependent on the dataset. Therefore, the detector-based method is usually inflexible to the general datasets, as well as the unstable performance of detectors is prone to affect the accuracy. In this paper, we aim to identify the action class in a video from scene features with less prior knowledge. Compared with scene categories occur in the dataset, the approximate number of them is easier to get.
According to the relation between videos, actions, scenes and visual words, we naturally deduce a


generative probabilistic model to model the contextual cues between them. In the action-scene model, each video can be seen as a mixture of action categories, where each action is a probability distribution over scenes and each scene is associated with a distribution over visual words. Each video clip is looked as a distribution of actions, and each action is a distribution of visual word. The action-scene model discovers not only which action happens in a video, but also which scene is associated with the action.
Suppose we have a collection of V video clips, each video v is represented as a set of visual words. Each word belongs to the visual codebook including W unique visual words. Assume we have K action categories, S scene categories. Now we can describe the process generating each video v as:
For each clip v, there is a multinomial distribution over K action categories șv~ Dirichlet(Į).
For each visual word vi of the clip v:
Choose an action category zvi according to Multinomial distribution (șv).
Choose a scene svi from p(svi|zvi, ȥz), a multinomial probability conditioned on the action zvi and ȥz. Here,
ȥz is a multinomial distribution of action over scene svi with ȥz~ Dirichlet(ȕ).
Choose a visual word wvi from p(wvi|svi, ĳs), a multinomial probability conditioned on the scene svi and ĳs. And ĳs is the distribution of scene over visual words with ĳs~ Dirichlet(ȟ).
Here, ș, ĳ and ȥ are multinomial distribution associated with video clip v, scene s and action z. Under this generative processing, the probability of video corpus on visual word set w, conditioned on Į, ȕ and ȟ is:


N
P(w, z, s,θ ,ψ | α , β ,ξ ) = ∏ P(wvi | Svi ,φsvi ) p(φsvi | β )
i =1
N
∏ P(wvi | zvi ,ψ zvi ) p(ψ zvi | ξ )
i =1



(1)

V  Nv
∏∏ P(zvi | θv ) p(θv | α )
v =1 i =1

This action-scene model includes the following parameters: the action distribution of video clip ș, action- scene distribution ȥ, the scene distribution ĳ and the assignments of individual words to action z and scene s. In this paper, we use Gibbs sampling [17] to evaluate the posterior distribution on z and scene s. Then ș, ĳ and ȥ can be inferred from the results of z and s.
Given z, s, w, Į, ȕ and ȟ, computing the posterior distributions on ș, ĳ and ȥ is straightforward. Using the hypothesis that the Dirichlet is conjugate to the multinomial, we can estimate ș, ĳ and ȥ with:



θzv
C ZV + α
= 	zv	 , φ

zv
CWS + β
∑CWS + Wβ
SZ
	sz	
sz
sz

(2)

where C ZV is number of the words from video v been assigned to an action z, CWS is the time that a word w
been assigned to a scene s, and Csz  is the time that a scene s been assigned to an action z, and ∑Czv  ,
∑CWS and ∑C SZ respectively denote the total number of words in video v, the total number of words
assigned to scene s, and the total number of scenes assigned to action z. Then for each word, the basic equation for the Gibbs sampler is:



P(zvi = x, svi =
j | wvi = m, s−vi , z−vi , wvi ,α, β ,ξ ) = θzvφwsψ sz

(3)


where zvi=z, svi=s represent the assignments of the ith word in a video clip to action z and scene s respectively. wvi=w represents the observation that the ith word is the wth word in the codebook, and z-vi, s-vi represent all action and scene assignment not including the ith word. Since the assignment of visual words to actions and scenes will be given quickly from equation 3, the recognition processing will be carried out by tracking three
count matrixes C ZV , CWS and C SZ .
In the training stage, the Gibbs sampling is applied over all the visual words in the training set while the count matrixes will be computed and saved. When classifying a new video clip, the Gibbs sampling is run on the visual word in this clip. The assignments of visual words to actions and scenes will be evaluated from equation 3. The count matrices are updated from each assignment. The video clip is classified with a category index with the largest probability in șv. Since șv only draw the index of action category without knowing the actual action class label for the index, an action class label is assigned to the index by using ground truth labels in the training dataset. For each action index, it is named with the most popular action class label within videos belongs to this index.

Experimental Results

We evaluate our approach on the challenging YouTube dataset provided by Liu et al [7] which consists of 1168 videos collected by Liu et al. The dataset covers 11 action classes: basketball shooting, biking, diving, golf swing, horse riding, soccer juggling, swing, tennis swing, trampoline jumping, volleyball spiking, and walking with a dog. Each category of action is performed under several different scenes and divided into 25 subsets.
The hidden parameter ș, ĳ and ȥ were estimated based on training set. For all models we fixed the number of actions at K=11 and S is set to be the actual number of scene categories in dataset. The other parameters are set as Į=50/K, ȕ = 0.3 and ȟ = 0.01 from experimental analysis.
We compare our action-scene model with LDA[18]. The average performances with two models are shown in Table 1. We see that the proposed method achieve an improvement over all the action categories. The improvement is especially remarkable for the actions that have strong relation with scenes: basketball (15.13%) and trampoline jump (10.72%). It demonstrates that the scene cues are informative and complementary for identifying the actions in realistic videos.



100.00%
90.00%
80.00%
70.00%
60.00%
50.00%
40.00%
30.00%
20.00%
10.00%
0.00%



category


Fig. 1. The Recognition performance of action recognize for Youtube Dataset

To evaluate the effect of person detector on the recognition performance, we compare the results using manually annotated person areas and the areas from person detectors in Section 3.1, and the results in Table 1 show that the performance of LDA can receive a better performance than the action-scene model when the person areas are annotated precisely, and a worse performance when the person areas are automatically detected. The action-scene model has better robustness than LDA when the features are noisy.
Table 1. The Recognition performance with different feature extraction methods


In this paper, we explore to recognize human actions from background settings of realistic videos. An action-scene model is proposed to model and learn the relationship between actions and scenes with little prior knowledge on scene categories. A generative learning method is used to infer actions from scenes according to a certain distribution. Experimental results validate that the addition of the context cues indeed improve the recognition precision. Besides, our proposed action-scene model has good robustness when applied in realistic video datasets.
There are a wide variety of potential applications on the research of action recognition. For further work, we intend to combine our method with known methods to learn application specified contextual cues from the massive realistic videos.


References
C.Schuldt, I.Laptev, and B.Caputo, “Recognizing human actions: a local SVM approach,” in ICPR, 2004.
J.Niebles, H.Wang, and L.Fei-Fei, “Unsupervised learning of human action categories using spatial-


temporal words,” International Journal of Computer Vision, vol. 79, issue 3, pp. 299-318, 2008.
T.Motwani, R.Mooney, “Improving video activity recognition using object recognition and text mining,” in ECAI, 2012.
M.Marszalek, I.Laptev, C.Schmid, “Actions in context,” in CVPR, 2009.
M. Blank, L. Gorelick, E. Shechtman, “Actions as space-time shapes,” Phil. Trans. Roy. Soc. London, vol. A247, pp. 529-551, April 1955.
Laptev, M.Marszalek, C.Schmid, B.Rozenfeld, “Learning realistic human actions from movies,” in CVPR, 2008.
J.Liu, J.Luo, M.Shah, “Recognizing realistic actions from videos “in the wild” in CVPR, 2009.
D.Han, L.Bo, C.Sminchisescu, “Selection and context for action recognition,” in ICCV, 2009.
N.Ikizler-Cinbis, S.Sclaroff, “Object, scene and actions: combining multiple features for human action recognition,” in ECCV, 2010.
B.Yao, L.Fei-Fei, “Modeling mutual context of object and human pose in human-object interaction activities,” in CVPR, 2010.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained partbased models, ” IEEE Trans. Pattern Anal. Mach. Intell., 2010.
D. Comaniciu, V. Ramesh, P. Meer, “Real-time tracking of non-rigid objects using mean shift, ” in CVPR, 2000.
P.Dollar, V.Rabaud, G.Cottrell, S. Belongie. “Behavior recognition via sparse spatio-temporal features,” in 2nd joint IEEE International workshop on visual surveillance and performance evaluation of tracking and surveillance, 2005.
N. Dalal, B. Triggs, “Histograms of oriented gradients for human detection,” in CVPR, 2005.
A.Oliv, A.Torralba, “Modeling the shape of the scene: a holistic representation of the spatial envelope,” International Journal of Computer Vision, vol. 42, issue 3, pp. 142-175, 2001.
D.Lowe. “Distinctive image features form scale-invariant keypoints,” International Journal of Computer Vision, vol. 60, issue 2, pp. 91-110, 2004.
B.Schölkopf, A.Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond, MIT Press, 2002.
D.Blei, A.Ng, M.Jordan, “Latent dirichlet allocation,” Journal of Machine Learning Research, issue 3, pp. 993-1022, 2003.
