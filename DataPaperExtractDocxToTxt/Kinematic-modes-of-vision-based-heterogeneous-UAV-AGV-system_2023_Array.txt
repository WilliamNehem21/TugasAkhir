Array 17 (2023) 100269










Kinematic modes of vision-based heterogeneous UAV-AGV system
Ashok Kumar Sivarathri *, Amit Shukla, Ayush Gupta
Indian Institute of Technology Mandi, Mandi, Himachal Pradesh, India



A R T I C L E I N F O 

Keywords:
UAV AGV
Apparent displacement Kinematic mode Camera frame
Image frame
Sliding mode control
A B S T R A C T 

UAV-AGV (Unmanned Aerial Vehicle-Autonomous Ground Vehicle) heterogeneous multi-agent system is gaining attention of researchers and became the topic of interest. UAV-AGV system is more advantageous than individual robots as both mobile platforms benefit from independent capabilities. Vision-based technique is the most researched method to build collaborative connection between both agents as it can function in GPS inaccessible areas. Take-off, tracking, and landing are basic tasks of the system. Vision-based solutions for these tasks are given most attention by various researchers so far. However, complex collaborative kinematics between agents are necessary to accomplish the complicated tasks which requires further investigation of the system. Moreover, a detailed kinematic model is necessary to better describe a robotic system which still needs to be developed to the vision-based UAV-AGV system. Kinematic analysis of the system is performed in the present work to address these issues. A differential form of mathematical representation is obtained for the vision-based collaboration between UAV and AGV. Forward and inverse kinematic notions are introduced for the system. Three kinematic modes of the system are identified and explained based on the kinematic model. Third mode reveals an inter- esting relative motion and novel behaviour of the system. Further step is taken to implement sliding mode-based control theory to develop novel kinematic controllers for the motion of UAV using each kinematic mode. Ki- nematic controllers are validated with thorough simulations. The performance of the controllers is acceptable and may be applied to activate the different modes of the system depending on the mission requirement.





Introduction

Multi-agent robotic system is the topic of interest in the present ro- botics research community. More than one robotic agent involves in the system and combination can be either homogeneous or heterogeneous. Homogeneous combination is similar group of robots whereas hetero- geneous is dissimilar group of robotic agents [1]. All agents cooperate with each other to collaboratively finish the task. Main advantage of multi-agent system lies in adding up the individual capabilities of each agent to overcome the drawbacks of each depending upon the task. Multi robot systems can overcome the spatial limitations and can perform the task at minimal time compared to single robot system [1,2]. One of the popular multi-agent systems is heterogeneous UAV-AGV which involves single UAV and AGV. UAV has better maneuverability, but poor payload capacity compared to AGV. AGV offers better payload and computation power over UAV, but poor maneuverability and perception of environment compared to the UAV [3–5]. Both agents will
benefit from individual capabilities to overcome their drawbacks. AGV
can have large perception of the environment with the presence of UAV
which helps in better navigation to reach the goals which are not in the perception of AGV. The UAV-AGV system is suitable for applications such as inspection, surveillance, agriculture, environment mapping and target tracking [6–14].
When multiple agents are performing an autonomous task, collabo-
ration and cooperation among all agents becomes important for better utilization of the individual capabilities. Thus, collaborative motion between UAV and AGV is the basic requirement of UAV-AGV system. A recent review on UAV-AGV systems discovers various functional roles for the agents and presents a comprehensive investigation and analysis [4]. Vision-based method is one of the techniques to build collaborative connection between both agents [15–18]. UAV can use its down facing
camera for localizing the AGV [18] or AGV may localize UAV through its
sky facing camera [19]. Vision-based method is a promising technique in the GPS (Global Positioning System) denied environments [20]. Detec- tion and localization of AGV in the image plane is one of the challenges in UAV-AGV system. Many researchers have reported vision-based control algorithms for the collaborative motion over the past few years [16–31]. Moreover, application of vision-based motion control is



* Corresponding author.
E-mail address: d19042@students.iitmandi.ac.in (A.K. Sivarathri).

https://doi.org/10.1016/j.array.2022.100269
Received 22 September 2022; Received in revised form 23 November 2022; Accepted 5 December 2022
Available online 7 December 2022
2590-0056/© 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).



constantly growing due to the advancements of AI (Artificial Intelli- gence) and machine vision [20]. Vision-based control strategies for the autonomous take-off, landing of UAV over AGV, and tracking of AGV by
UAV are reported in the literature [21,22,24–26,31]. One of the major advantages of UAV is guiding the navigation of the AGV. UAV can make
a global map of the environment and can provide it to the AGV for path planning to a location which may not be perceived by the AGV with its sensors [35–40]. So, UAV can plan its navigation as well as guide the
navigation of AGV by providing the global map [35]. The list of appli-
cations of UAV-AGV system grabbing the attention of the researchers around the world to study and explore its capabilities furthermore [2–13].
Related work and present contribution

A review [4] presents the several engineering applications of UAV-AGV system therefore, the practical significance of the same can be understood. The potential applications of this system in several in- dustries motivate the present work. In the present study, UAV-AGV system is considered where both agents are connected by a vision sensor carried by the UAV. A better collaboration between both agents is required for efficient utilization of the individual capabilities. Generally, visual features (markers) of the AGV in the image plane are exploited to develop collaborative motion control which may be known as visual
servoing. Many researchers have developed control techniques for the collaborative motion using vision data of UAV camera [16–34]. Take-off, tracking, and landing are basic collaborative tasks performed
by the system to execute any intended task or application. In Ref. [20], authors have applied CNN (Convolutional Neural Networks) for pose estimation of AGV to implement landing of aerial vehicle over moving AGV. In Ref. [27], vision-based landing over moving vehicle is devel- oped for UAV-AGV system. A visual marker is placed over ground vehicle and detected using classical machine vision techniques. The proposed technique is validated with the field experiments. Similarly, in Ref. [31], autonomous landing of UAV over a static target is experi- mentally verified [22,24,26]. have developed vision-based techniques for the autonomous take-off, tracking and landing for the UAV-AGV system. Thus, vision-based motion control techniques are well estab-
lished in the literature for these basic tasks [2–4]. However, to execute
any mission, complex collaborative kinematics are necessary. For example, in the case of inspection and surveillance tasks, UAV may have to execute complex trajectories while keeping the AGV in the FOV [3,4, 6,20]. Therefore, further investigation is required to better understand and explore the capabilities of the system [3,4].
Moreover, with the increasing demand for multi-robot systems for various industrial applications [4,6], attention is to be given towards developing general frameworks for better description of systems. A detailed kinematic model is required to describe a robotic system [41, 42]. This is still an open problem of vision-based UAV-AGV system and a novel opportunity to the present work. Thus, a dedicated mathemati- cal/kinematic model is necessary for the vision-based system to explore its capabilities for complex missions. It also serves to understand deeper
Kinematic analysis

Collaborative motion development between UAV and AGV is pri- mary need for the system to perform autonomous navigation during the operation. Vision-based collaborative system is widely explored tech- nique in the literature as it is advantageous in GPS denied environments. Autonomous inspection of industrial infrastructures is one of the ap- plications which requires such robotic system [4,6,20,32,33]. Thus, this is going to be a prominent technique for the perception and control of the system to operate in the GPS denied areas. UAV carries a down facing camera to keep connected with AGV during execution of tasks. This vision connection develops constraints to the motion of each vehicle. The constrained motion of each vehicle must be efficiently controlled to keep them connected as well as to better utilize the ca- pabilities of each to accomplish the mission. For example, in an indoor surveillance task as in Ref. [9], both vehicles cooperatively collect the required data while maintaining visual contact. So, it is important to model the constrained motion to understand and implement the com- plex relative kinematics for different applications. Hence, there is a need for the dedicated analytical model to explain the complete vision-based connection between the UAV and AGV.
Fig. 1(a) shows the UAV keeping the AGV in its camera FOV (Field of View). Both UAV and AGV are considered as point objects thus, UAV can have 3 degrees of freedom (DOF) and AGV (holonomic drive) will have two DOF. Fig. 1(b) shows the position of the AGV in the image plane of
the UAV camera which is represented by a tuple (rX, rY). Xb, Yb is body
frame of the UAV, XG, YG is body frame of the AGV, and rX, rY is the image plane coordinate system. Z axis of the camera frame coincides with the optical axis, X and Y axes direct along the image plane axes as shown in Fig. 1(a). The major considerations of this analysis are, AGV is always in the FOV of UAV and the optical axis of the camera is always normal to the ground plane irrespective of the orientation of the UAV. The second assumption is practically feasible by using a gimbal camera. The following equations transform the image plane from pixel space to the metric space and shift the origin to the center of the image plane. This transformation simplifies further mathematical analysis.
rX = DrX — u
2
rY = DrY — v
2
Where, u and v are pixel coordinates, DrX and DrY are resolution of the image plane along rX and rY directions respectively and σ is the pixel density of the camera sensor.
UAV is flying at an altitude Z from the AGV and keeping it in its FOV. Relative distance (R) between UAV and AGV is measured in camera coordinate system and r is the apparent relative distance between them in the image plane of the UAV camera. Position of the AGV in the image plane and camera frame can be written in the vector form. rX, rY and XC, YC are unit vectors of image plane and camera frame respectively as shown in Fig. 1.

insights and presents a different perspective of the vision-based

→r = r
r + r r
(1)

UAV-AGV system. And this is our first attempt to develop a kinematic model to the multi-robot system. The contributions of this paper are as follows. Development of a kinematic model for the UAV-AGV system. A
differential form of kinematic model has been established which de-
X  X	Y  Y

→R = X X̂C + Y ŶC	(2)

scribes the vision-based collaborative connection between both agents. Three kinematic modes that exist in the system are discussed through mathematical analysis. Novel kinematic controllers have been devel-
→
= | |, =
Consider the system at an instant, UAV is at height Z1
(3)

and AGV is at

oped for the motion of UAV using each kinematic mode of the system and validated with thorough simulations.
The organization of the paper is as follows. In the next section, mathematical analysis is presented and kinematic modes with controller design are discussed along with simulation results in section 4. And then finally conclusions are drawn in section 5.
R1 in the camera frame of UAV. System reaches to the next instant/state
in time dt, where height of UAV is Z2 and relative displacement of AGV is R2. For both instants of the system configuration, apparent displace- ments of the AGV in the image frame are r1 and r2 respectively. Following relations can be obtained for both instants using pin hole camera model [43,44].




Fig. 1. (a) UAV-AGV system with vision connection and (b) image plane of the UAV camera.


R1  r1 R2	r2

Z = ρ , Z
= ρ	(4)
dr	dR
r2 dZ

1	2	R dt — r dt + ρ dt = 0	(16)

Z = R1 ρ,  Z = R2 ρ
1	r2
Taking the difference of both instants of the system
(5)
Eqn. (16) is a scalar equation and kinematic model of the system which describes the vision-based collaborative connection between UAV and AGV. r˙ is apparent relative velocity, R˙ is true relative velocity and Z˙

Z2 — Z1 =
R2ρ r2
R1ρ
—
1
(6)
is relative depth velocity of the UAV. For all possible kinematics of the system, the above equation holds until the AGV is in the FOV. An intuitive understanding of the kinematic model may be obtained as

Z2 —
Z1 =
ρ(R2r1 — R1r2)	(7)
follows. Consider a situation where, AGV is executing the circular mo- tion about the optical axis of the UAV (hovering) camera. In this situa- tion, the second term in left hand side of Eqn. (16) is zero and for all rest

By considering the infinitesimal increments of all variables during time
dt,
Z2 = Z1 + dZ	(8)
R = R + dR	(9)
of the possible kinematics, summation of rest of the two terms is zero. At this point, apparent displacement of the AGV changes with the depth of the UAV. Supposing that, apparent position of the AGV is also constant
during the motion (r˙ = 0). Then, depth of the UAV must be constant
which can be obtained from the resulting equation (Z˙ = 0) as well.

2	1	When altitude of the UAV is constant (Z˙ = 0), the equation describes

r2 = r1 + dr	(10)
By substituting Eqns. (8)–(10) in Eqn. (7),
dZ = ρ ((R + dR)r — (r + dr)R)	(11)
relative in-plane motion between them which can be used to develop the tracking control between both the agents. When relative distance is
constant (R˙ = 0), it explains the variation of apparent distance of the
AGV with relative altitude between both agents. As the motion in alti-
tude direction is restricted to the AGV, it is the change in the altitude of
the UAV from the AGV. When apparent relative distance is constant (r˙ =

dZ =
ρ rdR — Rdr
r(r + dr)
(12)
0), equation explains the formation motion between them. In this case
both agents should have a cooperative motion to maintain constant apparent relative displacement between them. All kinematic modes of

Eqn.12 can be expanded to get the following
the system are detailed in the next section. Z is independent of the angular position of the AGV. So, altitude of UAV can also be written as

 ρdR 
(r + dr)
ρR	dr 
r (r + dr)
— dZ = 0	(13)
follows.
X ρ	Y ρ

From following simplification,
1	1	(14)
Z = rX = rY
R can be expressed as function of X or Y using the polar forms.


 
(r + dr) = r 1 + dr)
 X 	 Y 

r

And approximating,
R = cos θ or R = sin θ
Substituting R =  X  and R =  Y  independently in Eqn. (16) and

cos θ	sin θ

dr
1 + r ≈ 1  (∴dr ≪ r)	(15)
simplifying both, we get the following.

drX	dX	r2 dZ

Position of AGV and depth of UAV in camera frame changes during the time interval dt. By taking the differentiation with respect to time, we
dt — rX dt + X dt = 0	(17)

drY	dY	r2 dZ

get the rate of change of states of the system and by simplifying the resultant equation, we arrive at the following,
dt — rY dt + Y dt = 0	(18)



X˙ = VAGV — VUAV and Y˙ = VAGV — VUAV
Thus, Eqn. (16) can be obtained independently along X and Y di-
rections of the camera frame. Z cannot be computed when the AGV is at the origin as Eqn. (4) is undefined for the same position. When the AGV is moving on and along the X axis, both Y and rY are zero. Eqn. (17) describes the kinematics of the system. Similarly, Eqn. (18) describes the motion of the system when AGV moving on and along the Y axis. R and r in Eqn. (16) are positive scalars however, X, Y, rX and rY are signed quantities. The kinematic model of the system is satisfied when both
vehicles are connected (AGV is in the FOV of the UAV). As the system is forced to have a certain relation among the kinematic parameters (r,Z,R, r˙,Z˙,R˙), it may be possible to introduce the forward and inverse kinematic notions for the system. Forward relation for the UAV can be obtained by re-arranging Eqns. (17) and (18).
lution for the same where UAV can have motion along X as well as Z.
Eqn. (16) describes the collaborative connection in the form of the velocity of UAV and AGV. It can be extended to the get the relation in the form accelerations. By differentiating the equation again with time and simplifying, we get,
d2 r	d2 R	d2 Z	dr dZ
dt — r dt + f1 dt + f2 dt dt = 0	(19)
r2	r
1 = ρ and f2 = ρ	(20)
Kinematic modes
Collaborative kinematics between both agents (UAV and AGV) can be divided into three kinematic modes. Each mode captures the modu-





UAV X
—Z
ρ
= ⎢⎣
—rX
0	ρ
1  0 ⎤ ⎡
⎥ ⎢
r˙X r˙Y
Z˙ ⎥
larity in the kinematics of the system and explains a certain relative motion between both agents. These modes are obtained by restricting one of the variables (R, r, and Z) of the system (Eqn. (16)) to be constant
at a time. All possible relative kinematics between UAV and AGV may be

UAV
Y
—Z —rY
0	ρ	ρ
0  1 ⎦ ⎢⎢⎣
AGV X AGV Y
⎥⎥⎦
described using the combination of these three kinematic modes which are detailed in the next sub sections. Using the simplified kinematic model of each mode, a control technique has been developed for

In compact form,
{VUAV } = [S]{VI}
Where, VUAV is velocity vector of UAV, S is system matrix and VI is the
input velocity vector. Input parameters space contains apparent velocity of AGV, depth velocity of UAV and velocity of the AGV. Velocity of UAV and AGV are expressed in the camera frame. Velocity of the UAV for various input vectors, satisfies the kinematic model of the system. Thus, forward relation calculates velocity vector for the UAV for different input vectors such that both agents are connected, or the kinematic model is satisfied. Similarly, for the AGV, forward relation can be ob- tained as,
collaborative motion. The objective of this study is to establish and use the kinematic model to develop control techniques. Sliding mode con- trol theory is chosen to support the objective of the present work. However, it is possible to implement both linear and non-linear control techniques using the kinematic model.
Sliding mode-based control methods are being explored for the robot motion control [45–48]. The control structure has two stages of action, reaching and sliding phases. Reaching phase forces the system on to the
sliding surface, and then sliding phase keeps the system on the surface and brings down the error to zero [45]. The main advantage of sliding mode controller is its robustness and simple implementation. Thus, in the present work, sliding mode control theory is applied to develop motion control for the UAV using each kinematic mode of the system.
Controller generates the high-level motion commands to control the


[	]	⎡ Z	0
rX  1  0 ⎤ ⎡
r˙X r˙Y
position of UAV with respect to AGV.

AGV
X
AGV Y
= ⎢⎣
ρ
rY
ρ	ρ
0 1 ⎥⎦ ⎢⎢
Z˙
UAV
UAV Y
Kinematic mode I

This mode can be observed when relative height (Z) of UAV from AGV is constant. This condition arises when both vehicles have the in- plane motion parallel to the ground plane. Eqn. (16) reduces to the

Input parameters space contains velocity of UAV instead of AGV. Inverse kinematic analysis of the UAV calculates the required input vector for the desired velocity vector of the UAV. Generally, inverse kinematics leads to the development of control to the system. The calculated input parameters of the system must be maintained to have the desired velocity vector for the UAV. Thus, for any given velocity of the UAV, the agents are connected or satisfies the kinematic model when the required input parameters are maintained. It is possible to evaluate
following form in this mode.
R dr — r dR = 0	(21)
dt	dt
dR  R dr
dt = r dt	(22)
We know from Eqns.4,

an input vector to keep AGV in the FOV for any given random velocity of	R  Z

UAV. Similarly, for the AGV, inverse kinematic analysis results in an input vector for the given velocity of the AGV. By commanding the system to maintain the input parameters, it is possible to contain the
AGV in the FOV. For example, if the desired velocity of AGV is [ 1  0 ]T,
one of the possibilities of input vectors is [ 0  0  0  1  0 ]T. VUAV = 1
r = ρ	(23)
Relative height (Z) and focal length (ρ) are constants in the above
expression. Substituting Eqn. (23) in Eqn. (22), we get the following equation.

from the calculated input vector. So, by commanding the system to
dR  Z dr
=
(24)

possess the calculated input vector ( VUAV = 1), AGV is always contained
[ 0 0 Z˙ 0 0 ]T . To satisfy the desired velocity of AGV, Z˙ =  ρ . Thus, in the FOV of the UAV. Another possible input vector for the same is it is also possible to keep the AGV in the FOV by appropriately com-
manding the depth velocity of the UAV using inverse kinematic relation of AGV. Appropriate combination of both ( VUAV, ˙Z) can also be a so-
dt	ρ dt
Eqn. (24) is a standard equation that appears in the pin hole model of the camera [43]. R is measured in the camera frame and r is measured in the image plane of the camera. Z is depth from the camera and ρ is focal length. A tracking control technique can be developed for UAV to track the AGV or vice versa using this mode of the system. Fig. 2 describes the mode I kinematics of the system. Rotational motion of AGV or UAV

eX = rXa — rXd	(29)
Where, rXa is actual and rXd is desired apparent displacement. P–I
(Proportional-Integral) sliding surface function can be a choice for designing the control law. During the initial reaching stage, integral action results in overshoot. So, to avoid this, proportional surface is
chosen. However, it is straight forward to implement P–I sliding surface to this problem for getting much better results during tracking.
sX = c1eX	(30)
r˙X = r˙Xd + 1 (s˙X)	(31)
1
Considering hyperbolic tangent reaching law [45], driving control law can be obtained as,

r˙X = r˙Xd + 1 ( — η
tanh(s) )	(32)

Fig. 2. Relative in-plane motion of mode I system.
c1	1

keeping constant R is also a possible relative motion between them in this mode where both sides of Eqn. (24) is zero. If the UAV is hovering, relative velocity is equal to the velocity of the AGV and if the AGV is

Where, η1 is a positive constant. Control law for velocity of the UAV can be obtained as,
VUAV = VAGV — Z (r˙Xd +  1 ( — η tanh(s) ) )	(33)

system can be obtained as,
R¨ = Zr¨
ρ

(25)
Velocity of the AGV can be obtained from Eqn. (26) (equation of the mode) and can be fed to the above control law. Velocity of the AGV can be obtained from Eqn. (27) as,

It can be observed from Eqn. (25) that, relative acceleration between
VAGV = VUAV + Z drX
(34)

both agents is function apparent relative acceleration. Tracking control method can be developed by calculating the appropriate acceleration commands to the UAV or AGV to track the other. Low-level control commands (Torque) can be designed for the motion control of UAV using acceleration form.
X	X	ρ dt

Then the control law for the UAV velocity along X direction can be obtained as,
VUAV = VUAV + Z drX — Z (r˙Xd +  1 ( — η tanh(sX) ) )	(35)


Controller design
X	X	ρ dt	ρ
c1	1

A tracking controller for the UAV to reach and track the AGV can be developed using this mode of the system as discussed earlier. Several authors have proposed vision-based control algorithms for the tracking
Eqn. (35) can be split into reaching and tracking parts. Former one is responsible for reaching the AGV and keeping it at the origin of the image plane. Latter one is responsible for tracking of AGV by the UAV.

motion between UAV and AGV [11,22,29,32,33]. However, attention is not given towards developing a general kinematic model, which can be useful to develop kinematic control techniques for different relative
motions between the agents. This work develops kinematic model to the
UAV	UAV
X	Reching

Where,
UAV
Tracking

system and presents its applications to develop kinematic control tech-

VUAV
Z (r˙Xd +  1 ( — η
tanh(sX) ) )

niques. Relative tracking is one of the relative kinematics of the system which is described by the mode I as discussed above.
Reching = —ρ
c1	1
Z drX

Let us consider the velocity form of this mode along the X direction, which describes the relative motion between UAV and AGV in the X
UAV
Tracking
= VUAV + ρ dt

direction of the camera frame. The objective of the controller is to reduce the offset of AGV from the center of the image plane and to track it.
dX  Z drX
Similarly, control law for the UAV velocity along Y direction can be obtained as,
VUAV = VUAV + Z drY — Z (r˙Yd +  1 ( — η tanh(sY ) ) )	(36)

dt = ρ
dt	(26)	Y
Y	ρ dt	ρ
c2	2

Above equation can be written as,
Where, sY is sliding surface function and rY is apparent position

VAGV — VUAV =
Z drX
(27)
measured along the rY
direction in the image plane.

X	X	ρ dt
Stability analysis
Consider a following Lyapunov function which is positive seme-

VUAV = VAGV — Z drX
dt
(28)
definite [45,46],
V = 1s2
(37)

Velocity of both vehicles is with respect to the inertial coordinate system, however, expressed in the camera frame of the UAV. r˙X in Eqn.
(28) is a virtual input (driving control law) which can be calculated as follows. Error function can be defined as,
2 X
Where sX is sliding surface function. Taking the differentiation with respect to time,

V˙ = sXs˙X	(38)
V˙ = sX(c e˙X)	(39)
Substituting the driving control law (Eqn. (32)) in the above Eqn. (39),
V˙ = sX(c ( 1 ( — η tanh(sX) ) + r˙Xd — r˙Xd ) )

V˙ = —sX(η1 tanh(sX) )
V˙ ≤ — η1 |sX| , η1 > 0
V˙ is negative seme-definite for entire range of sX, thus the system is
tangent reaching law forces the system on to the sliding surface (S = 0) stable according to the Lyapunov stability theory [45,46]. Hyperbolic and thereafter error decays exponentially.

Simulation
The controller is validated by performing the simulation with different initial conditions. For convenience, proportional constant in Eqn. (26) is taken as 30 in the simulation. Fig. 3(a) shows the velocity of UAV and AGV. Initially, AGV is at 5 cm away from the origin of the
image plane along the X direction and stationery ( VAGV = 0). The
controller output which is the velocity of the UAV in X direction reduces
the offset of the AGV in the image plane as shown in Fig. 4. Fig. 3(b) shows the variation of apparent velocity (rate of change of apparent position) of the AGV. This relative motion of UAV may be referred as reaching motion as it is reaching the AGV and keeps it at its center of the image plane and hovers above it.
Fig. 5 shows the controller output for VAGV = 20 cm/s with same
initial position (5 cm) in the image plane. Control action begins to
reduce the offset of the AGV as shown in Fig. 4 and gradually matches with the velocity of the AGV as shown in Fig. 5(a). This relative motion may be regarded as reaching and tracking of AGV by the UAV [24]. Fig. 5(b) shows the apparent velocity of AGV in the image plane. It starts with a negative velocity and reaches to zero as shown in Fig. 5(b), which indicates that apparent position is decreasing with time. This can also be observed from Fig. 4. The total control action can be split into two parts, one is reaching control and other is tracking control as discussed in section 3.1.1. First two terms of the right-hand side of the control laws (Eqns. (35) and (36)) correspond to the tracking and the rest correspond to the reaching action. Thus, for continuous tracking of AGV by UAV, motion of the AGV must be estimated in real-time [20,27].
Controllers’ performance is evaluated with time varying velocity of
the AGV. Initial apparent position of the AGV is at 7 cm along X axis and oscillates with a mean velocity of 40 cm/s and amplitude of 20 cm/s.











Fig. 4. Variation of apparent position in the image plane.

the AGV due to the initial offset of AGV in the image plane. UAV then gradually, matches its velocity with AGV and continues to track it. Fig. 6
(b) shows the apparent velocity of AGV. Another set of conditions are
mean velocity of —40 cm/s and amplitude of —20 cm/s. This time UAV initial apparent position of AGV is 7 cm and oscillates with a negative starts with a velocity (approximately 50 cm/s) less than the previous
case (130 cm/s). It is because the AGV is moving towards the UAV with negative velocity. UAV reaches the AGV with reaching control action as shown in Fig. 7(b) and gradually matches with velocity of AGV as shown in Fig. 8(a). Fig. 8(b) shows the apparent velocity of AGV. Though, ve- locity of AGV is different in both cases, performance of reaching controller is similar as can be seen from Fig. 7
These simulations are performed considering only the motion of UAV; however, the aim is to control the in-plane relative velocity be- tween both vehicles (Eqn. (27)). Thus, it is possible to control the motion of either UAV or AGV or together depending upon on the task. This is planned as the extension of present work. For example, both UAV and AGV can be commanded to keep the AGV at the origin of the camera frame depending on the flexibility of the task being performed by the system.

Kinematic mode II

In this kinematic mode, relative displacement (R) between UAV and AGV which is measured in the camera frame is held constant. Then Eqn.
(16) reduces to the following.

Apparent position of AGV vanishes to zero due to the reaching action of
R dr  r2 dZ

the controller as shown in Fig. 7(a). Reaching part of the control action can be observed in Fig. 6(a) as the UAV starts from a higher velocity than
dt — ρ
dt = 0	(40)




Fig. 3. (a) Velocity of UAV and AGV and (b) apparent velocity of AGV.




Fig. 5. (a) Controller performance for AGV velocity of 20 cm/s and (b) apparent velocity of AGV.


Fig. 6. (a) Velocity of UAV and AGV and (b) apparent velocity of AGV.


Fig. 7. (a) Variation of apparent position of AGV for positive velocity and (b) for negative velocity of the AGV.


Fig. 8. Velocity of UAV and AGV and (b) apparent velocity of AGV.

dZ
dt = —
Rρ dr r2 dt

(41)
r˙ = r˙d + 1 (s˙)	(47)

R, ρ are constants in Eqn. (41). r varies with the relative height of the UAV from AGV. Eqn. (41) explains the variation of apparent relative
displacement with change in the depth (Z) of the UAV [49]. This relation
is reported in the literature which validates the present kinematic model
Considering the hyperbolic tangent function as reaching law, control law for the depth velocity of the UAV can be obtained as,
VUAV = — Rρ (r˙d + 1 (—η tanh(s)))	(48)

[49]. Fig. 9 shows the schematic of the mode III kinematics of the sys-	Z
tem. Relative displacement is constant, and UAV displaces normal to the

r2	c

ground plane as shown in Fig. 9(a). This mode is also possible when relative velocity between both agents is zero as shown in Fig. 9(b) which
brings in R˙ = 0. This mode can be used for the depth control of the UAV.
Acceleration form of mode II can be obtained as,
Stability of the control law can be verified by similar procedure discussed in section 3.1.2.

4.2.2. Simulation
In this simulation, relative displacement (R) of AGV and focal length

Z¨ = — Rρr¨— 2r˙Z˙
(42)
(ρ) of the camera are considered as 200 cm and 3 mm respectively.

r2	r
Substituting Eqn. (41) in above equation we get,
Z¨ = — Rρr¨+ 2Rρr˙2	(43)
r2	r3
It can be observed that, the relative depth acceleration of UAV is function of both relative apparent acceleration as well as relative depth velocity and apparent velocity (Eqn. (43)). Depth acceleration of UAV is showing a non-linear relationship with the apparent position. Relative velocity between both agents is zero when both are static or having the same velocity. This kinematic mode helps in developing the depth control of UAV from the AGV for both cases. As per the authors knowledge, a differential form of relation among apparent acceleration, apparent velocity of AGV in the image plane and depth acceleration of UAV is presented for the first time. This result may be one of the deeper insights of the vision-based UAV-AGV system.

Controller design
The objective of this controller is to change the depth of the UAV to achieve the desired apparent position of the AGV in the image plane. Controlled variable is apparent position of the AGV, and controller output is velocity of the UAV along the Z direction. Consider the velocity form of this kinematic mode, which is function of both apparent displacement and its rate of change. r˙ is input or driving velocity to the controller.
Fig. 10(a) shows the variation of the apparent position of the AGV. Initially it is at 5 cm in along X. Desired apparent position is lesser than the initial position so, UAV has to direct away from the AGV along the Z. Fig. 10(b) shows the controller output which is the depth velocity of the UAV. Initially UAV starts with less velocity as it is inversely proportional to the square of the apparent position of the AGV and then reaches the maximum velocity as shown in Fig. 10(b). When the apparent position reaches close to the desired value, it smoothly slows down and hovers at certain height. This controller can be implemented even when both vehicles are moving with zero relative motion which implies that both agents are moving at the same velocity. This relative motion may be regarded as take-off action of UAV from the AGV.
Fig. 11(a) shows the variation of apparent position of the AGV where initial position is larger than the desired position. To achieve this, UAV has to move towards AGV along negative Z direction to increase the offset of the AGV along the X direction which can be observed in Fig. 11
(b). This relative motion of the mode brings down the depth of the UAV and may be useful in the landing operation of the UAV on stationery or dynamic AGV. In Ref. [34], a vision-based range/depth controller is developed to reach the target. Pixel size of the target in the image plane is used to develop the controller. An empirical relation between distance of UAV and pixel size of the target is obtained. However, the kinematic model (Eqn. (44)) which relates UAV kinematics with size of the target in the image plane simplifies the situation.


UAV
Rρ dr
Kinematic mode III

VZ	= — r2 dt	(44)
Error function is,
er = ra — rd	(45)
Where, ra is actual apparent and rd is desired apparent displacement.
Sliding surface function is,

This mode can be observed when apparent displacement (r) of the AGV in the image plane is constant throughout the motion. This is an interesting mode of the system which was not reported in the literature as per the authors survey. Eqn. (16) reduces to the following in this mode.
dZ  ρ dR

s = cer
Driving control law r˙ can be obtained as,
(46)
dt = r dt	(49)
ρ and r are constants in Eqn. (49). This equation explains the relation
between out of plane (Z) velocity of UAV and in-plane relative velocity




Fig. 9. Mode II kinematics of UAV-AGV system.




Fig. 10. (a) Variation of apparent position of AGV and (b) depth velocity of UAV.


Fig. 11. (a) Variation of apparent position of AGV and (b) controller output.


between both of them to maintain the constant apparent relative displacement. Fig. 12 shows the mode III system where UAV will have the both in-plane and out of plane velocities but the latter one is con- nected to the relative velocity between both agents as per Eqn. (49).
dR  r
dt = ρvz	(50)
R˙ = VAGV — VUAV	(51)
VUAV, VAGV are with respect to the inertial coordinate system and VUAV
includes only the in-plane velocities parallel to the ground plane. If the UAV velocity vector is specified, the relative velocity between both agents can be found to maintain the constant apparent relative
displacement in the image plane. From the relative velocity, velocity of AGV or UAV can be found given the other one. This kinematic mode explains a formation phenomenon between UAV and AGV. For example, if the UAV is moving towards the ground plane, AGV has to displace towards UAV on the ground plane to maintain the constant r in the image plane. It can be inferred from Fig. 12, when UAV moves towards AGV normal to the ground, AGV has to displace towards UAV. Converging and diverging natures can be observed in this mode. When the displacement between centres of mass of both vehicles is increasing it is diverging behaviour and the converse is converging behaviour.
A formation control can be developed for the system by using this mode. In a situation where, UAV and AGV are performing a collabora- tive task and finished, formation mode can be activated where both move together towards each to facilitate the landing of UAV over AGV. An artificial formation mechanism is described for UAV-AGV system in Refs. [50,51] where UAV is controlled to keep constant separation from AGV and to follow it. However, present analysis, brings out a novel natural formation (in-built behaviour) of the system which is useful to facilitate the landing task and also to contain the AGV in the FOV. This kinematic mode also explains the shrinking or broadening of the posi- tion of the AGV in camera frame. It may be useful to navigate the AGV in the complex environments [52].
Acceleration form of mode III system can be obtained as,

Z¨ = ρR¨
r
(52)









Fig. 12. Mode III kinematics of UAV-AGV system.
Since r and ρ are constants, relative acceleration normal to the ground is proportional to the relative in-plane acceleration to maintain the constant relative apparent displacement. Depth of UAV can be controlled depending on the in-plane relative velocity or relative displacement can be controlled depending on the depth velocity of the UAV to achieve the constant apparent displacement.



Controller design
The objective is to control apparent displacement of AGV by com- manding the velocity of UAV along the Z direction to ensure the property
of this mode (r˙ = 0). Keeping the AGV in the FOV of the UAV is an
inherent feature of this kinematic mode. So, activation of this mode
during mission/task will ensure the AGV is always in FOV of the UAV. In this mode, R˙ is the driving velocity to the controller which can be either velocity of UAV or AGV or a combination of them.
Consider the velocity form of this mode,
ρ dR
ground where lines of motion intersect at an angle of 90◦ which can be observed in Fig. 12. When the initial position of the AGV is larger than
the desired position then both vehicles to move close to each other to reduce the relative distance. This demands the downward motion of the UAV to maintain the constant apparent position. Fig. 14 shows the simulation result of this situation where initial relative position AGV is larger than the desired. Converging nature can be visualized with this relative motion.
Controller performance is also evaluated for timely varying desired relative distance as shown in Fig. 15. Initial relative displacement is 6 m

VUAV =
r dt
(53)
and desired relative displacement oscillates about 4 m with an ampli- tude of 1.5 m as shown in Fig. 15(a). UAV begins to move with negative

Error function is,
e = Ra — Rd
Sliding surface
s = ce
Driving velocity can be computed as,
R˙ = R˙d + 1 (s˙)
Considering hyperbolic tangent function as reaching law, Control law
for the UAV velocity in Z direction can be obtained as,
VUAV = ρ (R˙d + 1 ( — η tanh(s) ) )	(54)
velocity (towards the ground) as to have the constant apparent position of the AGV with reducing relative displacement in the camera frame. And then gradually, UAV oscillates about zero velocity as shown in Fig. 15(b). Diverging and converging phenomena of the system can be observed from Fig. 14. UAV velocity is positive (normal and away from ground) when the slop of Fig. 14(a) is positive (increasing R) and negative (normal and towards the ground) when the slop is negative (decreasing R). R˙ can be either in-plane velocity of UAV or AGV. For example, if UAV is initially hovering, driving velocity corresponds to the velocity of AGV which can be obtained by dividing the result (UAV velocity) by 2 in Fig. 15(b).
Thus, the depth velocity of the UAV can be controlled by this mode of motion for the timely varying relative displacement to keep the constant relative apparent displacement. If it is desired to change the depth of the



R˙ achieves the desired relative position of AGV and Z˙ efforts to keep the r unchanged. Stability of the system subjected to the above control law can be verified by a similar procedure discussed in section 3.1.2. Desired relative displacement can be obtained by either controlling the motion of UAV, AGV or both simultaneously. For example, it is desired to control the position of the AGV from an initial position (R1) to the final position (R2) keeping the UAV stationary. Then the driving velocity to the controller is the velocity of the AGV itself. The above control law for UAV velocity ensures the constant apparent position of the AGV in the image frame throughout the task/mission. Given an initial apparent position of AGV, UAV can be commanded to change its depth to main- tain constant offset of the AGV and thereby keeping it in the FOV.

Simulation
The proportionality constant in Eqn. (50) is taken as 2 in this simu- lation. Fig. 13(a) shows the variation of the relative position of AGV in camera frame with time. Initially, it is at 2 m along the X direction and reaches to desired position (5 m). The controller output (depth velocity of UAV) commands the UAV to remain the initial position of the AGV in the image plane. Fig. 13(b) shows the variation of the UAV velocity along Z direction. In this situation, the relative motion has the diverging nature. Both vehicles diverge from each other from a point on the
apparent relative displacement to be constant. Desired relative
displacement can be achieved by either controlling in-plane motion of UAV parallel to ground plane or motion of AGV. A formation mechanism can be designed to facilitate for landing of the UAV over the AGV after the mission. And, shrinking and broadening of position of the AGV with respect to UAV can be controlled to guide the AGV in the complex en- vironments such as narrow passages [52]. Similarly, when UAV is controlled to a desired depth position, AGV can be commanded to maintain the constant r. During a mission, if AGV is tending to move out of the image plane, this mode can be activated so that UAV directs up- ward to keep the AGV in the FOV. This behaviour of the system is useful during the collaborative tasks to contain the AGV in the FOV.
Each kinematic mode is obtained by assuming the one of the vari- ables to be constant at a time. And then the resulting equations for each mode are used to develop the collaborative control. However, complex situations arise when combination of all modes is considered. For instance, UAV is required to track the AGV while having certain depth velocity. In this case apparent position of AGV is function of both R and
Z. However, Eqn. (16) shows the systematic way to develop control and simplifies this situation. Thus, the model shows its significance to develop motion control for the complex relative motion between the agents. Vision-based techniques for landing of UAV over stationary




Fig. 13. (a) Position of AGV in the camera frame and (b) controller output.




Fig. 14. (a) Relative position of AGV in the camera frame and (b) controller output.


Fig. 15. (a) Actual and desired relative position and (b) controller output.


target [31] and moving target [17] have been developed in the litera- ture. The 3-D (3- Dimensional) relative position is calculated by trans- forming the 2-D position of the target in the image plane. Velocity of UAV is controlled along 3 directions based on the 3-D position of the target. However, the 2-D position of target in the image plane is coupled
with both depth of UAV (ZC) and relative displacement in the plane XC —
YC. This can be visualized from Fig. 1. The expressions for the velocity of
UAV from forward kinematics relation considering static AGV can be written as,
Conclusion

Vision-based UAV-AGV system is analyzed to develop kinematic model for collaborative control applications. A unified dedicated kine- matic model has been obtained which explains the collaborative kine- matics between the agents and strengthens the theoretical base of the system. The kinematic model is useful to develop kinematic controllers for the position control of both vehicles. The model presents a different perspective of the system and gives the deeper insights of the relative


UAV
—Zr
rX ˙
kinematics. The model captures the modularity in the kinematics of the

VX  = ρ ˙X — ρ Z	(55)
system. Forward and inverse kinematic notions are presented for the system using the kinematic model. Different kinematic modes such as


VUAV = —Zr˙
— rY Z˙
(56)
take-off, landing, tracking, and formation are mathematically described

Y	ρ Y	ρ
It is clear from Eqns. (55) and (56) that, in-plane kinematics ( VUAV and VUAV) of UAV are coupled with apparent velocity of target/AGV and depth velocity of UAV. Thus, in-plane and depth velocity of UAV have to be coupled to solve the landing problem in Ref. [31]. Similarly, in Ref. [34], object tracking by UAV with front facing camera is proposed. Yaw, depth, and forward motions of UAV are controlled using position
and pixel size of object in the image plane. However, the coupling effect is not considered, and it is clearly due to the lack of kinematic model of the system. The kinematic model (Eqn. (16)) captures this coupling and hence can be used to solve such complicated cases. Development of collaborative motion control between UAV and AGV for such complex situations and experimental validation will be taken up as the extension of present work. Kinematic controllers are designed and validated in the present work; however, dynamic controllers can be developed using the acceleration forms of the kinematic modes.
using the kinematic model. Mode I describes the tracking motion be- tween both agents which is useful for developing the tracking control technique. Mode II explains the take-off or landing of UAV from or over the AGV and can be applied for the depth control of the UAV from AGV. Third mode is an interesting observation of the current study where both vehicles converge to each or diverge from each other. In this mode, the in-plane relative velocity is related to the depth velocity of the UAV in order maintain the constant apparent relative displacement of the AGV in the image plane. Mode III is useful to contain the AGV in the FOV during collaborative navigation. The Kinematic model brings in the modularity to the system in the form of kinematic modes. It may be possible to extend the model to a single UAV and multiple AGVs system. Sliding mode-based novel kinematic controllers for the motion of UAV have been developed using each kinematic mode and validated with thorough simulations. Thus, the model is useful to develop kinematic control to the system as per the task requirement. However, it is possible to apply different advanced control techniques using kinematic model. A qualitative comparison of the present approach with the literature is presented. Experimental validation of control algorithms will be taken up as the extension of the present work. Another possible extension of



present work is to consider the acceleration forms of each mode to develop dynamic/low-level control for the collaborative motion of the system.

Credit author statement

Ashok Kumar Sivarathri: Conceptualization, Investigation, Writing-Original draft preparation, Amit Shukla: Supervision, Meth- odology, Reviewing, Ayush Gupta: Supervision, Editing.

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Data availability

No data was used for the research described in the article.

Acknowledgements

Authors would like to acknowledge the research supervisor and colleagues at Indian Institute of Technology Mandi (IIT Mandi).

References

Ças¸ ka S, Gayretli A. A survey of UAV/UGV collaborative systems. CIE44&IMSS 2014;14:453–63.
Yan Zhi, Jouandeau Nicolas, Ali Cherif Arab. A survey and analysis of multi-robot
coordination. Int J Adv Rob Syst 2013;10.12:399.
Waslander Steven L. Unmanned aerial and ground vehicle teams: recent work and open problems. Autonom Control Syst Veh 2013:21–36.
Ding Yulong, Xin Bin, Chen Jie. A review of recent advances in coordination
between unmanned aerial and ground vehicles. Unmanned Syst 2021;9(2):97–117.
Yara Rizk, Awad Mariette, Tunstel Edward W. Cooperative heterogeneous multi- robot systems: a survey. ACM Comput Surv 2019;52(2):1–31.
Cantieri Alvaro, et al. Cooperative UAV–UGV autonomous power pylon inspection:
an investigation of cooperative outdoor vehicle positioning architecture. Sensors 2020;20(21):6384.
Gu´erin François, et al. UAV-UGV cooperation for objects transportation in an
industrial area. In: 2015 IEEE international conference on industrial technology (ICIT). IEEE; 2015.
Hament Blake, Oh Paul. Unmanned aerial and ground vehicle (UAV-UGV) system prototype for civil infrastructure missions. In: 2018 IEEE international conference on consumer electronics (ICCE). IEEE; 2018.
Asadi Khashayar, et al. An integrated UGV-UAV system for construction site data collection. Autom ConStruct 2020:103068.
Cantelli Luciano, et al. UAV/UGV cooperation for surveying operations in humanitarian demining. In: 2013 IEEE international symposium on safety, security, and rescue robotics (SSRR). IEEE; 2013.
Liang Xiao, et al. Moving target tracking method for unmanned aerial vehicle/ unmanned ground vehicle heterogeneous system based on AprilTags. Meas Control
2020;53(3–4):427–40.
Sutera Giuseppe, et al. A multi-robot system for thermal vision inspection. In: 2020 23rd international symposium on measurement and control in robotics (ISMCR). IEEE; 2020.
Dang Tung, et al. Graph-based subterranean exploration path planning using aerial and legged robots. J Field Robot 2020;37(8):1363–88.
Kulkarni Mihir, et al. Autonomous teamed exploration of subterranean
environments using legged and aerial robots. arXiv 2021. preprint arXiv: 2111.06482.
Bonin-Font Francisco, Ortiz Alberto, Oliver Gabriel. Visual navigation for mobile robots: a survey. J Intell Rob Syst 2008;53(3):263–96.
Li Wei, Zhang Tianguang, Kühnlenz Kolja. A vision-guided autonomous quadrotor
in an air-ground multi-robot system. In: 2011 IEEE international conference on robotics and automation. IEEE; 2011.
Keipour Azarakhsh, et al. Visual servoing approach for autonomous UAV landing on a moving vehicle. arXiv 2021. preprint arXiv:2104.01272.
Lange Sven, Sünderhauf Niko, Protzel Peter. Autonomous landing for a multirotor UAV using vision. In: International conference on simulation, modeling, and programming for autonomous robots (SIMPAR 2008); 2008.
Cantelli L, et al. Autonomous cooperation between UAV and UGV to improve navigation and environmental monitoring in rough environments. In: 10th international symposium on humanitarian demining coupled with the 11th IARP WS HUDEM2013, Vol. 23; 2013.
Yang Tao, et al. Hybrid camera array-based uav auto-landing on moving ugv in gps-denied environment. Rem Sens 2018;11:1829.
Harik El, Chouaib Houssein, et al. Fuzzy logic controller for predictive vision-based target tracking with an unmanned aerial vehicle. Adv Robot 2017;31(7):368–81.
Lee Jae-Keun, et al. Collaborative control of uav/ugv. In: 2014 11th international
conference on ubiquitous robots and ambient intelligence (URAI). IEEE; 2014.
Falanga Davide, et al. Vision-based autonomous quadrotor landing on a moving platform. In: 2017 IEEE international symposium on safety, security and rescue robotics (SSRR). IEEE; 2017.
Respall Victor Massague, Sellami Sami, Afanasyev Ilya. Implementation of autonomous visual detection, tracking and landing for AR. Drone 2.0 quadcopter. In: 2019 12th international conference on Developments in eSystems Engineering (DeSE). IEEE; 2019.
Lange Sven, Sunderhauf Niko, Protzel Peter. A vision based onboard approach for landing and position control of an autonomous multirotor UAV in GPS-denied environments. In: 2009 international conference on advanced robotics. IEEE; 2009.
Hui Cheng, et al. Autonomous takeoff, tracking and landing of a UAV on a moving UGV using onboard monocular vision. In: Proceedings of the 32nd Chinese control conference. IEEE; 2013.
Baca Tomas, et al. Autonomous landing on a moving vehicle with an unmanned aerial vehicle. J Field Robot 2019;5:874–91.
Fu Mengyin, et al. Autonomous landing of a quadrotor on an UGV. In: 2016 IEEE
international conference on mechatronics and automation. IEEE; 2016.
Demirhan Malik, Premachandra Chinthaka. Development of an automated camera- based drone landing system. IEEE Access 2020;8:202111–21.
Kalinov Ivan, et al. High-precision UAV localization system for landing on a mobile
collaborative robot based on an IR marker pattern recognition. In: 2019 IEEE 89th vehicular technology conference (VTC2019-Spring). IEEE; 2019.
Sudevan Vidya, Amit Shukla, Karki Hamad. Vision based autonomous landing of an unmanned aerial vehicle on a stationary target. In: 2017 17th international conference on control, automation and systems (ICCAS). IEEE; 2017.
Xiaoqian Huang, Amit Shukla, Karki Hamad. Autonomous ground pipelines tracking via an UAV. In: 2016 13th international computer conference on wavelet active media Technology and information processing (ICCWAMTIP). IEEE; 2016.
Xiaoqian Huang, et al. Variant PID controller design for autonomous visual tracking of oil and gas pipelines via an unmanned aerial vehicle. In: 2017 17th international conference on control, automation and systems (ICCAS). IEEE; 2017.
Kendall Alex G, Salvapantula Nishaad N, Stol Karl A. On-board object tracking control of a quadcopter with monocular vision. In: 2014 international conference on unmanned aircraft systems (ICUAS). IEEE; 2014.
Peterson John, et al. Online aerial terrain mapping for ground robot navigation. Sensors 2018;18(2):630.
Lakas Abderrahmane, et al. A framework for a cooperative UAV-UGV system for path discovery and planning. In: 2018 international conference on innovations in Information Technology (IIT). IEEE; 2018.
Xu Xuecheng, et al. Collaborative localization of aerial and ground mobile robots through orthomosaic map. In: 2020 IEEE international conference on Real-time Computing and Robotics (RCAR). IEEE; 2020.
Li Jianqiang, et al. A hybrid path planning method in unmanned air/ground
vehicle (UAV/UGV) cooperative systems. IEEE Trans Veh Technol 2016;65(12): 9585–96.
Huo Jianwen, et al. Path planning based on map matching in UAV/UGV
collaboration system. Int J Intell Unman Syst 2019. https://www.emerald.com/ insight/content/doi/10.1108/IJIUS-03-2019-0020/full/html.
Miki Takahiro, Khrapchenkov Petr, Hori Koichi. UAV/UGV autonomous cooperation: UAV assists UGV to climb a cliff by attaching a tether. In: 2019 international conference on robotics and automation (ICRA). IEEE; 2019.
Lynch Kevin M, Park Frank C. Modern robotics. Cambridge University Press; 2017.
Saha Subir Kumar. Introduction to robotics. Tata McGraw-Hill Education; 2014.
Megalingam Rajesh Kannan, et al. Monocular distance estimation using pinhole camera approximation to avoid vehicle crash and back-over accidents. In: 2016 10th international conference on intelligent systems and control (ISCO). IEEE; 2016.
Garzo´n Mario, et al. An aerial–ground robotic system for navigation and obstacle mapping in large outdoor areas. Sensors 2013;13(1):1247–67.
Liu Jinkun, Wang Xinhua. Advanced sliding mode control for mechanical systems. Berlin: Springer; 2012.
Kamath Archit Krishna, Kumar Tripathi Vibhu, Behera Laxmidhar. Vision-based autonomous control schemes for quadrotor unmanned aerial vehicle. In: Unmanned robotic systems and applications; 2019.
Modali Sashank, Ghosh Satadal, Sujit PB. Sliding mode-based guidance for UAV landing on a stationary or moving ground vehicle. IFAC-PapersOnLine 2020;53(1):
453–8.
Singh Padmini, et al. Vision-based guidance and switching-based sliding mode controller for a mobile robot in the cyber physical framework. IEEE Trans Ind Inf
2018;15(4):1985–97.
Sivarathri Ashok Kumar, Mohammad Amir, Shitole Pankaj Popatrao. Detection of velocity based on change in the apparent size. Eng Proc 2020;2(1):27.
Bacheti Vinícius Pacheco, Santos Branda˜o Alexandre, Sarcinelli-Filho Ma´rio. Path-
following with a ugv-uav formation considering that the uav lands on the ugv. In: 2020 international conference on unmanned aircraft systems (ICUAS). IEEE; 2020.
Marcos Rabelo, Santos Felipe, Santos Branda˜o Alexandre, Sarcinelli-Filho Ma´rio.
Landing a uav on static or moving platforms using a formation controller. IEEE Syst J 2020;1:37–45.
Jafari Mohammad, Sengupta Shamik, Hung Manh La. Adaptive flocking control of
multiple unmanned ground vehicles by using a uav. In: International symposium on visual computing. Cham: Springer; 2015.
