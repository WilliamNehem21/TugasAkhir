Artificial Intelligence in the Life Sciences 1 (2021) 100004

		




Quantifying sources of uncertainty in drug discovery predictions with probabilistic models
Stanley E. Lazica,âˆ—, Dominic P. Williamsb
a Prioris.ai Inc., 459â€“207 Bank Street, Ottawa, K2P 2N2, Canada
b Functional and Mechanistic Safety, Clinical Pharmacology and Safety Sciences, AstraZeneca, R&D, Cambridge CB4 0WG, UK


a r t i c l e	i n f o	a b s t r a c t

	

Keywords:
Bayesian Prediction
Probabilistic machine learning Toxicity
Uncertainty quantification
Knowing the uncertainty in a prediction is critical when making expensive investment decisions and when patient safety is paramount, but machine learning (ML) models in drug discovery typically only provide a single best estimate and ignore all sources of uncertainty. Predictions from these models may therefore be over-confident, which can put patients at risk and waste resources when compounds that are destined to fail are further developed. Probabilistic predictive models (PPMs) can incorporate all sources of uncertainty and they return a distribution of predicted values that represents the uncertainty in the prediction. We describe seven sources of uncertainty in PPMs: data, distribution function, mean function, variance function, link function(s), parameters, and hyper- parameters. We use toxicity prediction as a running example, but the same principles apply for all prediction models. The consequences of ignoring uncertainty and how PPMs account for uncertainty are also described. We aim to make the discussion accessible to a broad non-mathematical audience. Equations are provided to make ideas concrete for mathematical readers (but can be skipped without loss of understanding) and code is available for computational researchers (https://github.com/stanlazic/ML_uncertainty_quantification).





Introduction

At each stage of the drug discovery pipeline, researchers decide to progress or halt compounds using both qualitative judgements and quantitative methods. This is formally a prediction problem, where, given some information, a prediction is made about a future observ- able outcome. Standard predictive or machine learning models such as random forests, support vector machines, or neural networks only report point-estimates, or a single â€œbestâ€ value for a prediction; they provide no information on the uncertainty of the prediction. Prediction uncer- tainty is important when (1) the range of plausible values is as impor- tant as the best estimate, (2) users need to know that the model cannot confidently make a prediction, (3) users need to reliability distinguish between ranked items, or (4) the cost of an incorrect decision is large; for example, when making expensive investment decisions or when as- sessing patient safety.
To illustrate the importance of prediction uncertainty, Fig. 1 shows predicted clinical blood alanine aminotransferase (ALT) levels â€“ an in- dicator of liver toxicity â€“ for two hypothetical compounds. Assume that levels below 8 (arbitrary units) are considered safe, and that only one compound can be taken forward for clinical trials. Based only on the best estimate, compound A (blue) is preferable (Fig. 1A). Knowing the prediction uncertainty changes the picture (Fig. 1B). 14% of compound
âˆ— Corresponding author.
E-mail address: stan.lazic@cantab.net (S.E. Lazic).
Aâ€™s distribution is in the unsafe shaded region, while only 2% of com- pound Bâ€™s distribution is in the unsafe region. Based on these distribu- tions, compound B maybe the better candidate to progress to clinical trials; or a project team may decide to run more experiments to reduce compound Aâ€™s uncertainty.
We define a probabilistic predictive model (PPM) as any machine learning model that returns a distribution for the prediction instead of a single value. PPMs differ in how they represent uncertainty. At one end, fully probabilistic Bayesian models specify a distribution for the out- come and for all unknown parameters in the model. These models are the gold-standard for quantifying uncertainty but they can be compu- tationally expensive. At the other end are methods that return multiple predicted values without specifying a probability distribution. Exam- ples include fitting the same model on multiple bootstrapped datasets, or for models with a stochastic component, fitting the same model using different random number generator seeds [31]. Although these models are often computationally tractable, the connection between the dis- tribution of predicted values and uncertainty is unclear. For example, does varying something trivial such as the random number generator seed adequately capture our uncertainty in a prediction? Between these extremes are approaches that try to obtain the benefits of fully proba- bilistic models using approximations, reformulations, or computational shortcuts. For example, instead of using a fully Bayesian deep neural


https://doi.org/10.1016/j.ailsci.2021.100004
Received 17 May 2021; Received in revised form 24 May 2021; Accepted 25 May 2021
Available online 26 May 2021
2667-3185/Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)


Fig. 1. Prediction uncertainty. Predicted blood ALT levels for two compounds, with Compound A appearing better (A). Com- pounds with values above the safety threshold of 8 (grey shaded region) will not be progressed. Reporting prediction uncertainty shows that 18% of Compound A is above the threshold but only 2% of Compound B, indicating that Com-
pound B is better (B). ALT = alanine aminotransferase; a.u. =
arbitrary units.










network, Kristiadi et al. were able to obtain many of the benefits by making only the final layer of the network Bayesian [29]. Also in the neural network literature, Gal and Ghahramani approximated param- eter uncertainty by randomly inactivating nodes at prediction time â€“ a procedure known as Monte Carlo dropout [15], and Teye and colleagues used the mean and variances calculated from batch normalisation steps for the same purpose [63]. These approximate methods are an active area of research [3,27,46,70] (see Mervin et al., for a recent review [40]), and will be critical for making PPMs more widely adopted, but here we focus on fully probabilistic models as they better highlight the key areas of uncertainty.
All prediction models can be written as
ğ‘¦ ğ‘¥	(1)
and read as â€œğ‘¦ given ğ‘¥â€ â€“ where ğ‘¦ is the outcome to be predicted and ğ‘¥ are one or more variables used to predict ğ‘¦. Both ğ‘¥ and ğ‘¦ are available when training a model, and when a model is deployed, new ğ‘¥ values are observed and used to predict the unknown ğ‘¦â€™s. PPMs additionally provide a probability distribution for ğ‘¦, denoted as
ğ‘ƒ (ğ‘¦ ğ‘¥)	(2)
and read as â€œthe probability of ğ‘¦ given ğ‘¥â€. A distribution for ğ‘¦ enables us to calculate any metric of interest, such as the best guess for ğ‘¦ (e.g.
mean, median, or mode), thereby providing the same information as standard methods. But in addition, prediction intervals (PI) can be cal-
culated around the best estimate, or the probability that ğ‘¦ is greater
or less than a predefined threshold can be calculated, as was done in
Fig. 1B.
Below we describe the sources of uncertainty and the advantages of PPMs. We aim to make the discussion accessible to a broad non- mathematical audience. Equations are provided to make ideas concrete for mathematical readers, but can be skipped without loss of under- standing of the remaining text. In addition, code is provided for com- putational researchers (https://github.com/stanlazic/ML_uncertainty_ quantification) and implemented in Julia using Turing [1,16].

Sources of uncertainty

The definition of a probabilistic model in Eq. (2) lacked a crucial component, which is the model itself:
ğ‘ƒ (ğ‘¦ ğ‘¥, Model).	(3)
(ğ‘¥, ğ‘¦) or in the model. A prediction for ğ‘¦ not only depends on ğ‘¥, but on The sources of uncertainty are now clear: they can reside in the data the model used to connect ğ‘¥ and ğ‘¦. Hence, predictions are conditional on
a model, and uncertainty in the model should lead to greater uncertainty in the prediction. Models are composed of the following components, which we discuss in greater detail below:
1. Data. The outcome ğ‘¦ and the predictor or input variables ğ‘¥.
tainty in ğ‘¦, also called the likelihood or data generating distribution: 2. Distribution function. The distribution that represents our uncer-
ğº(â‹…).
scribing   how   ğ‘¦   changes   as   ğ‘¥   changes:   ğ‘“ğœ‡ (â‹…). 3. Mean function. The functional or structural form of the model de-
Variance function. Describes how the uncertainty in ğ‘¦ varies with
ğ‘¥: ğ‘“ğœ (â‹…).
Parameters. The unknown coeï¬ƒcients or weights for the mean (ğœƒğœ‡ ) and variance (ğœƒğœ ) functions that are estimated from the data.
Hyperparameters. Parameters or other options used to define a model that are not estimated from the data but fixed or selected
by the analyst: ğœ™.
Link functions. Nonlinear transformations of the mean (ğ‘™ğœ‡ (â‹…)) and/or variance function (ğ‘™ğœ (â‹…)) used to keep values within an al-
lowable range.
Combining these seven components gives the generic formulation of a PPM (Eq. (4)). Although this equation is abstract, it captures where uncertainty can reside. In this section we carefully describe the terms in the equation and then provide a concrete example.
ğ‘¦ âˆ¼ ğº(ğœ‡, ğœ)
ğœ‡ = ğ‘™ğœ‡ (ğ‘“ğœ‡ (ğ‘¥; ğœƒğœ‡ ))
ğœ = ğ‘™ğœ (ğ‘“ğœ (ğ‘¥; ğœƒğœ ))	(4)
Starting with the first line of Eq. (4), ğ‘¦ is a future value that we want to predict and it could represent a clinical outcome, an IC50 value,
require that we specify a distribution for our uncertainty in ğ‘¦, which we or a physicochemical property of a compound such as solubility. PPMs can informally think of as the distribution from which ğ‘¦ was generated. We denote this distribution as ğº(â‹…) and the â€œ(â‹…)â€ notation indicates that
ğº is a function with inputs (ğœ‡ and ğœ in this case), but places the focus on
ğº and not the inputs, thereby reducing clutter. Common distributions
include the normal/Gaussian (for continuous symmetric data), Student-
t (continuous symmetric data with outliers), Bernoulli (0/1 data), and Poisson (count data). The mean of the chosen distribution is given by
ğœ‡ and many distributions have a second parameter, ğœ, that controls the
because it describes the uncertainty in ğ‘¦. The âˆ¼ symbol can be read spread or width of the distribution. This parameter is critical for PPMs
we might represent our uncertainty in ğ‘¦ for a given compound with a as â€œis distributed asâ€ or â€œis generated fromâ€. To make this concrete, Gaussian distribution that has a mean ğœ‡ = 2.45 â€“ which represents our best estimate of ğ‘¦ â€“ and a standard deviation of ğœ = 2.1. This would be written as ğ‘¦ âˆ¼ Normal(2.45, 2.1).
But where did our best estimate ğœ‡ come from? This is defined on the second line of Eq. (4). ğ‘¥ is the input data used to predict ğ‘¦ and it could
represent compound structures (encoded as a binary fingerprint for ex- ample), assay results, or physicochemical properties. The prediction task



reduces to using ğ‘¥ to predict ğ‘¦, but they need to be connected through a
of this model is denoted by ğ‘“ğœ‡ (â‹…) and it could represent the structure of a statistical or machine learning model. The structural or functional form
simple linear regression model, the architecture of a neural network, an
netic model. We refer to ğ‘“ğœ‡ (â‹…) as the mean function because it tells us the ensemble of trees, or a differential equation representing a pharmacoki- predicted mean value of ğ‘¦ for a given value of ğ‘¥. The mean function con- tains parameters (ğœƒğœ‡ ) that are estimated from the data, and the parame-
ters could represent the coeï¬ƒcients of a linear model or the weights and biases of a neural network. The â€œlearningâ€ in machine learning refers to finding parameter values that maximise predictive performance, given
the data and functional form of the model. ğœƒğœ‡ usually represents mul-
tiple parameters in the mean function; for example, it would represent
both the intercept and slope in a simple regression model. The subscript
ğœ‡ on ğ‘“ and ğœƒ indicates that the function and parameters refer to the mean, since we also have a function and parameters for the variance, ğœ,
which we described further below.
A potential problem is that ğœ‡ is unconstrained and can be any value calculated from ğ‘“ğœ‡ (â‹…), which may lead to impossible predictions. For
example, if weâ€™re predicting the probability that a compound is toxic,
ğ‘“ğœ‡ (â‹…) needs to be between zero and one â€“ values outside this range do not make sense. Hence, we need to transform ğ‘“ğœ‡ (â‹…) with a link function
ğ‘™ğœ‡ (â‹…) to put it within a permissible range. One option is to use the equa-
tion 1âˆ•(1 + exp(âˆ’ğ‘“ (â‹…))) as a link function, which compresses ğ‘“ (â‹…) into


Fig. 2. Simulated data with the true relationship between ğ‘¥ and ğ‘¦ given by ğœ‡ =
ğ‘“ğœ‡ (ğ‘¥; ğœƒğœ‡ ) (orange curve) and based on Eq. (5). ğº is the Gaussian data generating distribution with a mean ğœ‡ and a constant variance ğœ, which models the spread of points around the line. Link functions for ğœ‡ and ğœ are not used and hence are
not shown.

ğœ‡	ğœ‡

unnecessary when no restriction on ğ‘“ğœ‡ (â‹…) is required, and ğ‘™ğœ‡ (â‹…) can be the 0â€“1 range, but other functions are also possible. A link function is dropped from the formula. (Useful mnemonics: ğ‘“ = Function; ğº = data Generating distribution; ğ‘™ = Link function; with subscripts ğœ‡ and ğœ re-
ferring to the mean and variance, respectively).
standard deviation of ğœâ€. But how does ğ‘¦ depend on ğ‘¥? The second line shows how ğ‘¥ enters and how it depends on two parameters: ğœƒ1 and ğœƒ2 (ğ‘’ is a constant, not a parameter). We set this equation equal to ğœ‡ and can substitute it for ğœ‡ in the first line of Eq. (5) giving


a predicted value of ğ‘¦ can be specified in the same way as we specify The third line of Eq. (4) shows that the variance or uncertainty in
ğ‘¦ âˆ¼ Normal(ğœƒ2
+ 1 âˆ’ ğ‘’âˆ’ğœƒ1 ğ‘¥ ğœ .

1 + ğ‘’âˆ’ğœƒ1 ğ‘¥

the mean of ğ‘¦. Many models assume a constant value for ğœ, which is the
However, a modelâ€™s uncertainty in ğ‘¦ may vary for different values of ğ‘¥, â€œhomogeneity of varianceâ€ assumption in traditional statistical models. which can be captured with a variance function ğ‘“ğœ (â‹…). Since variances are positive, a link function for the variance (ğ‘™ğœ (â‹…)) is needed to constrain ğœ
to be greater than zero.
parameters ğœƒğœ‡ and ğœƒğœ before analysing the data, and the hyperparam- Fully Bayesian PPMs also require us to specify the uncertainty in the eters (ğœ™) refer to this specification. Many of the approximate methods avoid this step and ğœ™ may not correspond to any part of the model, but
defines other options for the learning algorithm and therefore it is not included in Eq. (4).
Usually the ğ‘¥ and ğ‘¦ data are all we have, and we need to choose ğº,
ğ‘™ğœ‡ , ğ‘“ğœ‡ , ğ‘™ğœ , ğ‘“ğœ , and ğœ™ based on background knowledge, preliminary plots
best. For example, an initial model might assume that ğº is Gaussian and of the data, or trying several options and empirically assessing which is that ğ‘“ğœ‡ follows a hypothesised mechanistic relationship based on a phar- macokinetic model. Then, we estimate or learn the values of ğœƒğœ‡ based
on training data, and assess the prediction on a separate test data set.
pounds, where ğ‘¦ is a clinical outcome and ğ‘¥ is an assay result. Assume To make these ideas concrete, Fig. 2 shows simulated data for 100 com- that higher values of ğ‘¦ indicate greater toxicity.
A nice feature of PPMs is that they are generative, meaning that they can generate or simulate data. Indeed, simulation and learning are oppo- site sides of the same coin: learning takes the fixed data and infers likely values of the parameters that could have generated the data, whereas simulation fixes the parameters and generates the data. The model in Eq. (5) generated the data in Fig. 2 and we will use it as a running example throughout
ğ‘¦ âˆ¼ Normal(ğœ‡, ğœ)	(5)
â€“
Writing the equation in one line makes the relationship between ğ‘¥
and ğ‘¦ clearer, but multi-lined equations are easier to read with more
teresting, but to help interpret the model, ğœƒ2 is the ğ‘¦-intercept (value of complex models. For most prediction models the parameters are unin-
ğ‘¦ when ğ‘¥ = 0), and ğœƒ1 controls how quickly the line in Fig. 2 reaches the upper asymptote as ğ‘¥ gets large.
To simulate a value for ğ‘¦, we need to (1) select parameter values, and we use the following: ğœƒ1 = 3.25, ğœƒ2 = 0.2, and ğœ = 0.1; (2) select a value of ğ‘¥, which enables us to calculate ğœ‡; then (3) draw a random number from a Gaussian distribution with a mean of ğœ‡ and standard deviation of
ğœ. This can be repeated any number of times to obtain the ğ‘ƒ (ğ‘¦ ğ‘¥) distri- bution, and for different values of ğ‘¥. The data in Fig. 2 were generated for 100 ğ‘¥ values uniformly distributed between 0 and 1. Note that ğœƒğœ‡
in Eq. (4) is a place-holder for several variables, which correspond to
ğœƒ1 and ğœƒ2 in Eq. (5). Given this data, we now illustrate were where the
seven sources of uncertainty enter.

Mean function uncertainty

Uncertainty in the mean function ğ‘“ğœ‡ (â‹…) arises because we rarely know the true form of the relationship between ğ‘¥ (assay) and ğ‘¦ (outcome) â€“
that is, we donâ€™t know the form of Eq. (5). Uncertainty in the mean function is also called model uncertainty, but this term is ambiguous because models have multiple components. Choices for the mean func- tion include which predictors, interaction terms, transformations, basis expansions, hierarchies, and time-varying components to include in the model. Assume we only observe the data in Fig. 2, several models we
might consider for the relationship between ğ‘¥ and ğ‘¦ are:
Linear âˆ¶  ğ‘¦ = ğœƒ0 + ğœƒ1ğ‘¥
Quadratic âˆ¶  ğ‘¦ = ğœƒ0 + ğœƒ1 ğ‘¥ + ğœƒ2 ğ‘¥2

ğœ‡ = ğœƒ2
+ 1 âˆ’ ğ‘’ ğœƒ1 ğ‘¥ .
1 + ğ‘’âˆ’ğœƒ1 ğ‘¥
Parameter Exponential âˆ¶  ğ‘¦ = ğœƒ2 (1 âˆ’ ğ‘’
âˆ’ğœƒ1 ğ‘¥ )

The first line of the equation is read as: â€œthe outcome ğ‘¦ is generated (âˆ¼) from a Gaussian or Normal distribution with a mean of ğœ‡ and a
Parameter Exponential âˆ¶  ğ‘¦ = ğœƒ3 + ğœƒ2 (1 âˆ’ ğ‘’âˆ’ğœƒ1 ğ‘¥ )
Michaelis âˆ’ Menten âˆ¶ ğ‘¦ = ğœƒ1 ğ‘¥ âˆ• (ğœƒ2 + ğ‘¥).










	  			  			  	
Fig. 3. Model averaging. Three models fit the data well (A-C), even though none are the true model. They make different predictions at low assay values and when extrapolating to higher values. The mean predictions are superimposed for easier comparison (D). Model averaged prediction (E). Comparison of prediction interval widths (F). Shaded regions are the 95% prediction intervals.


model, they all saturate at high values of ğ‘¥ or are concave and there- None of these are the true model (Eq. (5)), but except for the linear
fore capture the main trend in the data. Which model should we use? Typically, only a single model is selected and predictions are made from that. If one model is clearly better than the others, there may be little lost by using one model for predictions. However, if two or more models fit the data equally well, making predictions from only one will under- estimate the prediction uncertainty. Fortunately, we are not forced to choose one model but can fit several and combine their predictions. To illustrate, we will use the quadratic, 2-parameter exponential, and 3-parameter exponential models. The three models are fit to the data (Fig. 3Aâ€“C) and predictions are extrapolated to show both how similar the fits are where there is data, and how different the fits are when ex- trapolating. The shaded regions show the 95% prediction intervals (PI), and if a model is suitable, we expect 95% of the data to fall in the shaded region.
To better compare the predictions, the mean functions ğœ‡ for three
models are plotted together in Fig. 3D. The models make similar pre-
dictions within the range of the data, except at very low values of
ğ‘¥, where the 2-parameter exponential model predicts smaller values.
is greater at high values of ğ‘¥, where the models make different predic- Fig. 3E shows the model-averaged prediction, and note how uncertainty
tions. To better appreciate how model averaging incorporates uncer- tainty, Fig. 3F shows the width of the 95% PI for the three models and
low values of ğ‘¥ than any of the original models. This occurs because the the averaged model. Note how the averaged model has a wider PI at
region is more uncertain. For intermediate values of ğ‘¥, all three mod- three models make different predictions at low values and hence this
els make similar predictions and the averaged PI is wider than some models and lower than others. When extrapolating to larger values of
ğ‘¥, the model averaged PI width quickly becomes the widest, reflecting
both the diverging predictions of the individual models and the greater
uncertainty in the quadratic model.
Predictions are always conditional on a model, and if we donâ€™t know the true model, predictions from the wrong model are likely to be over- confident. We extrapolated well beyond the data to illustrate how model averaging accounts for model uncertainty. Such extrapolation may seem
dictions for the same values of ğ‘¥, the uncertainty in the model-specific unrealistic, but the message is that whenever models make different pre-
predictions will be overconfident, as we see to a lesser degree for low values of the assay.

Parameter uncertainty

from the data and control how the predictor variables (ğ‘¥) are related Most predictive models have parameters or weights that are learned to the outcome variable (ğ‘¦) â€“ these parameters are the ğœƒ symbols in the
five models considered above. Most machine learning methods only use the single best value of each parameter when making a prediction. But since the parameters are learned from the data, they are uncertain, and this uncertainty should be propagated into the prediction. Parameter uncertainty decreases as the sample sizes increases, so to better illustrate the effect of parameter uncertainty on predictions, a smaller dataset was made by taking every eighth data point from the previous example.
(ğœƒ0 , ğœƒ1 , ğœƒ2 , ğœ). The grey shaded region shows the 95% prediction inter- Fig. 4 uses the quadratic model, which has four parameters
val from a Bayesian model that accounts for parameter uncertainty. The dashed black lines show the 95% PI from a classic quadratic regres- sion model which ignores parameter uncertainty, and note how they are slightly narrower. The mean function is identical for both the Bayesian and classic model.
The difference in PI width may seem negligible when focusing on the mean prediction, but ignoring parameter uncertainty gives approx- imately 7% narrower PIs. This may be important with â€œpoint of depar- tureâ€ calculations when the tails of the distributions are more important than the means [50]. Assume clinical outcomes above 1.2 are considered






Fig. 4. Parameter uncertainty. The grey shaded region is the 95% prediction interval for a Bayesian model and the narrower black dashed lines for a classical model. The classic interval is narrower because it doesnâ€™t account for parameter uncertainty.
problematic and the assay result for one compound is ğ‘¥ = 0.5. When fully
accounting for parameter uncertainty, there is a 5.9% chance that the
true value of the clinical outcome is above 1.2, versus a 4.3% chance when ignoring uncertainty. The ratio of these numbers is 1.37, indi- cating that the tail area is nearly 1.4 times larger when accounting for uncertainty.
Models typically have many more parameters than this example (the state-of-the-art Generative Pre-trained Transformer 3 (GPT-3) deep learning language model has 175 billion parameters [6]) and simply collecting more data is often not an option to reduce parameter uncer- tainty because more data enables more complex models to be fit (e.g. including nonlinear terms and interactions), which then increases the number of parameters.
Hyperparameter uncertainty

Hyperparameters are a diverse set of tunable options that affect the training and predictions. Unlike parameters, they are not estimated from the data but selected by the analyst; examples include the amount of regularisation in a lasso model, the number of trees in a random forest model, or the cost function in a support vector machine. Suitable val- ues are typically found by trying several options and selecting the best using crossvalidation. In the hyperparameter category we can also in- clude options that are rarely part of a formal selection process such as the choice of optimisation algorithm or random number seed for mod- els with a stochastic component. For fully Bayesian models we can also include parameters for prior distributions, which are not updated by the data. These hyperparameters are selected pragmatically to provide good predictions, but other sets of hyperparameter values might give equally good predictions, on average, but slightly different predictions for each test compound. Hence, uncertainty in hyperparameter values is rarely taken into account. A further complication is that most hyperparame- ters are not related to any biological or chemical quantity of interest and hence it is unclear what the uncertainty is actually about. Nevertheless, Lakshminarayanan and colleagues showed that by running many mod- els with a different random seed, the ensemble of predictions performed better than a single model, and the distribution of predicted values pro- vided a measure of uncertainty [31].
Data uncertainty

sally ignored â€“ is the uncertainty in the data, both in the predictors (ğ‘¥) One of the main sources of uncertainty â€“ and which is almost univer-
and in the outcome to be predicted (ğ‘¦). The uncertainty can be in the
training data used to build the model, in the new data to be predicted,
or both. The main sources of data uncertainty are measurement error, misclassification error, binning, censoring and truncation, and missing values. Each of these are discussed below.
Predictors are often experimental measurements and are therefore subject to measurement error, or they are samples from a larger pop- ulation and are therefore subject to sampling error (e.g. only cells in the field of view are measured, not all cells in a well, and if a differ- ent subset of cells were selected, a differnt measured value would be
obtained). Predictors may also be calculated quantities such as IC50 val-
ues estimated from dose-response or concentration-response curves, and
hence are uncertain. Furthermore, some predictors such as cLogP are the output of other (imperfect) prediction models and therefore are also uncertain. Finally, some predictors are not measured directly but are es- timated from a standard curve, which introduces additional uncertainty because the curves may not be not perfectly calibrated.
All these are examples of classic measurement error, defined as
ğ‘¥measured = ğ‘¥true + error, where the measured ğ‘¥ value is the true value
where ğ‘¥measured = ğ‘¥true Ã— error. Another type of error is Berkson error, corrupted by some error or noise. Errors can also be multiplicative,
where samples or experimental units are assumed to have the same ex- posure but actually differ. For example, several wells in a microtitre plate are given the same concentration of a compound, but the true concentration may differ due to variations in the amount of compound dispensed, or, wells on the edge of a plate may have greater evapora- tion of the solution and thus have a higher effective concentration of the compound. Compound toxicity classifications can also introduce Berk- son error. A compound may be classified as â€œseverelyâ€ hepatotoxic, even though most people tolerate the compound well and only a few expe- rience severe reactions. The class label is therefore defined by a few members of the class instead of the majority response.
Berkson error can be introduced when converting continuous values into bins or groups. For example, compounds are categorised as active versus inactive, despite having a range of activity values. Or, compounds are classified as having no, mild, or severe toxicity, even though com- pounds will have a range of toxicity levels within each category. Binning can also lead to misclassification error, where a compound is placed into the incorrect category. This can occur if the measured assay value dif- fered from the true value and fell on the wrong side of a threshold. Hence, binning is strongly discouraged [33,35]. Misclassification can also occur due to incorrect diagnoses, labelling errors, or data-entry er- rors. The standard response to these known and often large sources of
data uncertainty is to ignore them and assume that ğ‘¥measured = ğ‘¥true.
Ignoring error in ğ‘¥ can bias parameter estimates, but for prediction
models the parameters are usually not of interest. Even though noisy
data can lead to biased parameter estimates, the model is still consis- tent for the prediction, meaning that as the sample size increases, the prediction will be correct, on average [9,20]. This likely explains why
error in ğ‘¥ has received little attention in the predictive modelling and
machine learning literature. However, if weâ€™re interested in the uncer-
tainty in the prediction, then making good predictions on average is not good enough, we need to ensure that the prediction uncertainty is calibrated.
Data are censored when they are known only up to a boundary value, but not beyond, and therefore only partial information is available. For example, assays typically have upper and lower limits of detection (LoD) and uncertainty arises because the exact value is unknown, but the LoD is typically treated as the â€œtrueâ€ measured value.
Data are truncated when values outside of a range are omitted, and the number of omitted values is unknown. For example, for objects to be segmented as a cell in a standard image analysis, they must have a minimum user-defined cell size. Smaller cells will therefore not be in- cluded in the analysis, and hence both the estimated cell size and prop- erties that are correlated with cell size can differ from their true values, thereby introducing both uncertainty and bias.


Fig. 5. Data uncertainty. Error bars indicate 1
clinical outcome (A). The red point at ğ‘¥ = 0.15 standard error of the estimated assay value and
is the new value to be predicted. Predictions are more uncertain when measurement error in the test data is included (assuming the training data is measured without error; B). Predictions for the test data when accounting for measure- ment error in the training data using multiple generated data (assuming no error in the test data; C). Averaged predictions from the gener- ated data shows greater prediction uncertainty compared with ignoring measurement error in the training data (D). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this ar- ticle.)



Missing data is the final source of data uncertainty and can arise for many reasons. Imputation is a common approach to deal with missing data, where a plausible value is generated and substituted for the miss- ing value. The imputed value is then taken as the true value, ignoring that it was generated and not measured. A simple way to account for un- certainty in an imputed value is to impute many values â€“ known as mul- tiple imputation â€“ and the variation in the imputed values captures the uncertainty [37,65]. Predictions are the made for each imputed value and the predictions combined.
To illustrate data uncertainty, Fig. 5A shows the same simulated data but with uncertainty in both the predictor and outcome variable. Here
we assume that the error in ğ‘¥ differs for each sample because it depends
on the precision of the measurement, whereas the uncertainty in ğ‘¦ is
constant; for example, we know that the measurements are accurate to
Â± some fixed amount. Variable and fixed uncertainty are accounted for
in the same way, and we include both for illustration purposes.
The uncertainty in ğ‘¥ and ğ‘¦ can be handled in two ways. The first is
example, if ğ‘¥ is measured with error, the true value can be inferred to directly model the errors in a Bayesian analysis [20,39,41,51]. For with ğ‘¥measured âˆ¼ Normal(ğ‘¥true , ğœerror ) where ğœerror is the uncertainty in the measured value of ğ‘¥ â€“ usually the standard error. However, fitting such
models may be diï¬ƒcult as each sample (row) adds as many parameters as there are variables with measurement error (columns).
A second simpler option is to generate multiple data sets, where the ğ‘¥
and ğ‘¦ values are drawn from a distribution [2]. For example, suppose an
ğ‘¥ variable for one compound has a measured value of 0.5 with a standard error of Â±0.06. We can sample, say, 10 values from a normal distribution
ing 10 new datasets where the observed value of ğ‘¥ is replaced with the with a mean of 0.5 and a standard deviation of 0.06, thereby generat-
generated value. This is a form of multiple imputation and Blackwell, Honaker, and King describe a more sophisticated method of generat- ing new data by taking the correlations between variables into account [2]. Each dataset is then analysed separately, and the predictions from each analysis are combined. Variations between the different datasets will lead to different parameter estimates, which in turn will lead to
certainty in ğ‘¥. The models can be fit to the separate datasets in parallel, different predictions, and the ensemble of predictions captures the un-
and so the computation time is the same as fitting the model to one dataset.
We use the second method to illustrate the effect of ignoring mea- surement error in two ways. First, we assume the training data is mea- sured without error, but the test data is measured with error. Then we assume that the training data is measured with error but the test data is not. In both cases we compare the result to the standard approach of ignoring measurement error in both the training and test data. The test
data is a single new compound with an assay value of ğ‘¥ = 0.15 Â± 0.06,
shown as the red point in Fig. 5A (plotted at ğ‘¦ = 0, but the objective is to predict ğ‘¦). The 3-parameter exponential model is used in this exam-
ple. The narrow blue distribution in Fig. 5B corresponds to the standard approach of ignoring uncertainty in both the training and test data, and the red distribution shows the greater uncertainty in the prediction for
ğ‘¦ when the measurement error in ğ‘¥ is included. To obtain this distribu-
tion, 1000 samples were drawn from a normal distribution with a mean
of these 1000 samples which reflects the uncertainty in ğ‘¥. of 0.15 and standard deviation of 0.06. A prediction was made for each
The blue distribution in Fig. 5C is again the standard analysis, and the five red lines show the slightly different predictions from each of the five datasets that account for measurement error in the training data (the test data was assumed to be error-free). Predictions from the five datasets are averaged and shown as the red distribution in Fig. 5D, which is slightly wider than the standard analysis from the blue distribu- tion. The additional uncertainty appears negligible in this example, es- pecially compared with uncertainty in the test data (Fig. 5B). However, the variation between datasets is expected to increase with (1) greater uncertainty in the variables, (2) more variables with measurement error included in the model, and (3) more parameters in the model with the total sample size remaining fixed. The effect of measurement error in the training data can be assessed during model development and vali- dation, and if the additional prediction uncertainty is negligible, then the final production model might ignore it.


Fig. 6. Truncated predictive distributions. Fits and 95% PI for models without (A and C) and with (B and D) a constraint that the predicted values must be positive. Prediction for a new
compound given ğ‘¥ = 0.5 without the constraint
shows that 9.1% of the predicted distribution is negative (C), while the truncated model redis- tributes the prediction to positive values (D). Red dashed lines are the data boundary.
















Distribution function uncertainty

Another source of uncertainty is the distribution function ğº(â‹…), which represents our uncertainty in a predicted value of ğ‘¦. Dozens of distribu-
tions are available but the list can be narrowed down based on back- ground knowledge of the outcome. For example, if the outcome is bi- nary such as absent/present, safe/toxic, or alive/dead, then a Bernoulli distribution is appropriate; if the outcome is a count such as the num- ber of seizures, then a Poisson or negative binomial distribution are two common options; if the data are positive values and skewed such as liver enzyme levels, then a log-normal or gamma distribution may be suitable; if the outcome is an ordered category such as none/mild/severe, then an ordered categorical distribution would be appropriate; if the outcome is continuous and unbounded with no outliers, then a Gaussian distribu- tion may be suitable; and if there are outliers, a Student-t distribution might be appropriate. Many Bayesian textbooks have appendices that list the common distributions and their properties [18,36,38].
Choosing between distributions is made easier because many distri- butions are special cases of other distributions. For example, both the Gaussian and Cauchy distributions are special cases of the Student-t dis- tribution, the Poisson distribution is a special case of the negative bino- mial distribution, and the exponential distribution is a special case of a gamma distribution. Hence, we often donâ€™t need to choose between a set mutually exclusive options, but can select the more general distri- bution and allow the model to determine if one of the special cases is more appropriate. The more general distributions usually have only one additional parameter and therefore do not make the model much more complex. However, not all potentially suitable distributions are related (e.g. gamma and lognormal) and hence two or more models may need to be compared. The data for our running example was generated from a Gaussian distribution and which we have been using for all the mod- els throughout. Hence using the more general Student-t distribution will inform us that the Gaussian is suitable, and so the results are not shown. A key consideration when selecting a distribution function is the bounds of the data. In our running example, the clinical outcome has
a minimum value of zero, but is being modelled with a Gaussian dis- tribution. Since a Gaussian distribution is defined for both positive and negative numbers, there is nothing to prevent negative predictions. A model is clearly inappropriate if it predicts impossible values. Fortu- nately, we can easily define truncated versions of standard distributions, and so we could specify a Gaussian distribution with a lower bound of zero. Fig. 6 shows an example using the 2-parameter exponential model
to predict the clinical outcome for an assay value of ğ‘¥ = 0.05 both with-
out (A) and with (B) truncation at ğ‘¦ = 0. Without truncation the model gives 9% chance that ğ‘¦ will be less than zero (Fig. 6C). With truncation,
this probability gets redistributed to positive values (Fig. 6D, the small proportion of the distribution below zero is a plotting artefact). Hence, if training or test data are near boundaries, using truncated versions of standard distributions is sensible. However, if the data are bounded but the values are far from the boundaries, then accounting for such bound- aries may be unnecessary.

Link function uncertainty
Link functions are required when the predicted mean ğœ‡ = ğ‘“ğœ‡ (â‹…) is bounded by an upper and/or lower limit. For example, when predicting
the probability of an event, the predicted value must lie between 0 and
1. Values returned from the mean function are unconstrained and can lie well outside this range. Hence, a link function is used to transform the values to respect the bounds. The logit, probit, cauchit, and comple- mentary log-log functions all take unconstrained numbers and compress them into the 0â€“1 range (Fig. 7). There is no â€œcorrectâ€ link function and each provides a different mapping from the unconstrained input to the constrained output and hence gives a different prediction, especially for large values of the input. Link functions are also required for the vari- ances, since variances cannot be negative values, and exponential or power links are often used.
Link functions are analogous to activation functions in neural net- works, although they are used as nonlinear transformations between neurons and not necessarily to constrain values to allowable ranges. But




 



Fig. 7. Link function uncertainty. Four functions that map the mean function
ğœ‡ = ğ‘“ (â‹…) from âˆ’âˆ to âˆ to values between 0 and 1. These link functions are
required when the outcome is a probability and must be between 0 and 1. The predicted probabilities therefore differ depending on the link function.


the same issue arises in that many activation functions exist and differ- ent functions will lead to different predictions.

Variance function uncertainty

The variance function models the uncertainty in ğ‘¦ for given values of ğ‘¥. Another way to think of a variance function is that it models the
proach assumes that uncertainty in ğ‘¦ is constant (Fig. 8A). However, spread of points around the mean prediction (Fig. 8). The standard ap- when the variance is not constant, a model for ğœ is required (Fig. 8B). Just like modelling ğœ‡ as a function of ğ‘¥, we now need to model ğœ as a function of ğ‘¥. This function could be a simple function of one ğ‘¥ vari- able or a full neural network for all ğ‘¥ variables [44]. The latter option
involves creating a second neural network for the variance, but this dou- bles the complexity of the model and the training time.
In Fig. 8B we do not use ğ‘¥ directly, but model ğœ as a function of ğœ‡
â€“ in other words, the uncertainty in ğ‘¦ is proportional to the predicted value of ğ‘¦. This allows for a simple mean function such as ğ‘“ğœ = ğœ0 + ğœ1 ğœ‡, where ğœ0 and ğœ1 are parameters that control the relationship between ğœ and ğœ‡. Since variances must be positive values, a link function is needed to constrain ğ‘“ğœ , and the softplus function ğ‘™ğœ = log(1 + exp(ğ‘“ğœ (â‹…)) is used
here. The result is shown in Fig. 8B, where the 95% shaded prediction region better matches the spread of the data compared with assuming a constant variance (Fig. 8A).
Instead of using a variance function, another option is to transform the outcome variable (e.g. log, square-root, or inverse), so that the un-
certainty in ğ‘¦ is constant. Alternatively, some distributions such as the
Poisson and Bernoulli have a defined relationship between the variance
the mean, which allows for non-constant variances, but they are only appropriate for certain type of data.

Sources of uncertainty combined

Breaking down the sources of uncertainty into seven items enables us to think about them separately and assess their importance when de- veloping a prediction model. A final model may include several sources and they can be easily combined. For example, suppose variance and link functions were not required but two mean functions and two dis- tribution functions performed similarly and therefore four models with each combination of distribution and mean function are fit to the train- ing data and the predictions averaged. If fully Bayesian models are used,
parameter uncertainty is already account for. And if the test data are measured with error, we can use the approach in 5 B to draw multiple samples for each test sample and feed them all through the prediction models. The more sources of uncertainty accounted for the more com- plex the prediction model. Hence, sources of uncertainty that make little contribution to the overall prediction uncertainty can be ignored.
Model fitting

A generic Bayesian 2-parameter exponential model is shown below, and each subsection above modified this basic model in different ways to highlight the sources of uncertainty.
ğ‘¦ âˆ¼ Normal(ğœ‡, ğœ)   Distribution function
ğœ‡ = ğœƒ2 (1 âˆ’ ğ‘’âˆ’ğœƒ1 ğ‘¥ )	Mean function
ğœƒ1 âˆ¼ Truncated Normal(1, 5)	Priors
ğœƒ2 âˆ¼ Truncated Normal(0, 5)
ğœ âˆ¼ Truncated Normal(0, 5)
ğ‘¦ is modelled a Normal distribution with a mean ğœ‡ and standard de- viation ğœ. The second line defines the mean function, and there is no
variance or link function defined. The next three lines specify the prior distributions for the three parameters in the model. Given the mean
function, we know that ğœƒ1 and ğœƒ2 are positive, and so is ğœ, by defini-
tion. Hence, we use truncated normal distributions with a lower limit of
truncation at zero to represent our prior uncertainty in the parameters. All the models were fit to the data with the Turing package in Julia. The No-U-Turn Sampler (NUTS) was used to update the uncertainty in the parameters after conditioning on the data. Three chains with 10,000 samples each were used and convergence was assessed with graphical
and numeric summaries (trace plots and ğ‘…Ì‚ statistics). Predictions were
then generated for new values of ğ‘¥ by (1) drawing samples from the updated parameter distributions, (2) using these to calculate ğœ‡, and (3) using ğœ‡ and ğœ to generate values for ğ‘¦. Finally, any summary statistic can be calculated, such as the proportion of samples from ğ‘¦ that are greater
than a threshold value.

Uncertainty for classification tasks

The previous examples had a continuous outcome variable, but often outcomes are categorical such as toxic versus safe. Much of the previous
in a parameter (ğœ‡) and uncertainty in a prediction for a new observable discussion applies, but an important distinction is between uncertainty (ğ‘¦) [5,17]. Greater parameter uncertainty leads to greater prediction
predict which of ğ¾ classes a sample belongs to. This point is illustrated uncertainty, but not for classification tasks, where the objective is to
in Fig. 9.
tors (ğ‘¥1, ğ‘¥2), and assume the grey triangles are the â€œtoxicâ€ class and the Fig. 9 A plots data for a 2-group classification task with two predic-
blue circles are the â€œsafeâ€ class. The black line is the optimal separat- ing boundary. A logistic regression model is used to separate the classes and the prediction from the model will be a number between 0 and 1, where 1 corresponds toxic and zero corresponds to safe. This prediction is derived from the mean function and is passed through a link func- tion to constrain the predictions to lie between 0 and 1. Weâ€™ll call these
predicted values ğœ‡ and the uncertainty in the prediction ğœ. Fig. 9B plots
ğœ‡ versus ğœ for each point in Fig. 9A, and the inverted-U relationship is
certain and are highlighted in red (ğœ â‰¥ 0.8). Samples with intermediate a known feature of such models. But some samples are especially un- uncertainty (ğœ between 0.6 and 0.8) are highlighted in orange. Fig. 9C
shows that the uncertain samples are all close to the decision bound- ary, and that the most uncertain red points lie near the edge of the data where the location of decision boundary itself is uncertain. The shaded grey region in Fig. 9C represents the uncertainty in the decision bound- ary, and note how the uncertainty is wider at the ends compared with the middle.


Fig. 8. Variance function uncertainty. A con- stant variance implies that the uncertainty in a
prediction is the same for all values ğ‘¥ (A). How-
as function of ğ‘¥, so can the uncertainty in the ever, just like the mean prediction can change
prediction (B).























Fig. 9. Uncertainty in classification. Simulated data with two features (A). A plot of the mean (ğœ‡) and uncertainty (ğœ) shows the expected inverted-U relationship
the boundary (C). Two compounds with the same mean but different uncertainties (D), have the same uncertainty in the final predicted value for ğ‘¦ (E,F). Outcome (B). The highest variance predictions (red points) are near the decision boundary and at the edge of the data, and high variance predictions (orange points) follow uncertainty is largely explained by ğœ‡ (G) and is similar for the two compounds (H). Parameter uncertainty is nearly 3.5 times greater for the compound with the larger ğœ (I). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)



same ğœ‡, but one has twice the uncertainty (these are highlighted with Fig. 9 D plots the full distribution for two samples that have the
for the parameter ğœ‡ and not for the observable outcome ğ‘¦, which is ei- green circles in Fig. 9 B, C, and G). Recall that these distributions are
ther safe or toxic. To obtain a prediction for the observable we need a data generating distribution such as the Bernoulli distribution, giving us
ğ‘¦ âˆ¼ Bernoulli(ğœ‡). Fig. 9E and F shows the predicted values of ğ‘¦ for these
of ğœ. At first this may seem strange, and it occurs because the Bernoulli two compounds and note that they are identical, despite different values distribution doesnâ€™t have a ğœ parameter, and so this information is lost.
for the probability of heads is 3âˆ•4 = 0.75 (and 1âˆ•4 = 0.25 for tails). Sim- Consider tossing a coin four times and getting 3 heads, our prediction
ilarly, if we toss a coin 1000 times and get 750 heads, our prediction
of heads is 0.75 (i.e. ğœ is much smaller). The width of the distributions is still 0.75, even though we are much more certain that the proportion in Fig. 9D (captured by ğœ) provides the weight of evidence [26] which
quantifies how much the prediction will change as more data are gath-
four coin tosses alters the probability to either 3âˆ•5 = 0.6 or 4âˆ•5 = 0.8, de- ered or as the inputs change. For example, a single new observation with
pending on whether a heads or tails was observed, whereas with 1000 tosses the probability is still 0.75, rounding to two decimal places.
Uncertainty in the predicted classes is often divided into aleatoric and epistemic uncertainty [25,28]. Aleatoric uncertainty is supposedly due to â€œinherent randomnessâ€ whereas epistemic uncertainty is due to a lack of knowledge. Without starting a philosophical debate, we take the position that all uncertainty is due to a lack of knowledge [5,26]. Nevertheless, we can decompose our uncertainty into two components, which we call the outcome uncertainty and parameter uncertainty, and which correspond to aleatoric and epistemic uncertainty, respectively.
The first component is how close ğœ‡ is to zero or one â€“ a more confident
diction would be ğœ‡ = 0.5. This corresponds to outcome uncertainty. The prediction would be close to these bounds, and the most uncertain pre- second component is how confident we are in ğœ‡. When a weatherperson
70.00000% but 70% Â± some amount. The uncertainty in the stated value states there is a 70% chance of rain tomorrow, they do not mean exactly
of the distributions in Fig. 9D and the parameter ğœ (If the model in- corresponds to parameter uncertainty and is represented by the width
cluded other sources of uncertainty, these would also be captured by
ğœ and hence ğœ would represent more than just parameter uncertainty).
Using the approach of Kwon et al. we decompose the two sources of
mean prediction (ğœ‡) in Fig. 9G [30]. Note how ğœ‡ largely explains the uncertainty for the compounds and plot outcome uncertainty versus the
tainty since they have a similar value of ğœ‡ (Fig. 9H). However, the com- outcome uncertainty. The two compounds have similar outcome uncer- pound with the larger value of ğœ has nearly 3.5 times greater parameter
(ğœ), provides important information about the uncertainty of a predic- uncertainty (Fig. 9I). Parameter uncertainty or the weight of evidence
tion, which is critical for high-stakes decisions.

Discussion

Generalisations and extensions

The above examples used simple models but this framework can be generalised to more complex cases. For example, we had functions for the mean and variance, but any parameter in the distribution function can be modelled. For example, a Student-t distribution has a parameter
tails. The df could be modelled as a function of ğ‘¥, just like ğœ‡ or ğœ [52]. called the degrees of freedom (df) which controls the heaviness of the
The above examples used a single distribution function, but flexi- bility can be increased by using mixtures of distributions. For example, outliers can be modelled with a mixture of Gaussian distributions: one to account for the regular observations and the second to account for the outliers. Metabolite, gene, and protein levels are non-negative and often positively skewed, and hence gamma or lognormal distributions may be appropriate. But these distributions are only defined for values
greater than zero, and there may be zeros in the data, which are often dealt with by adding a small value to all data points. A better option can be to model the data with a two-part model, one which accounts for the zeros and the other (e.g. gamma or lognormal) which accounts for the non-zero values. Such â€œhurdle modelsâ€ provide this flexibility and also return a parameter that estimates the proportion of zeros, which may be scientifically interesting [13]. Taking this idea a step further, Dirichlet Process models allow us to specify as many distributions as needed to model the data. Instead of specifying a single distribution, we specify a prior over distributions, and learn them from the data (yes, we can specify a distribution over distributions! [42]).
The above examples also used a single mean function for each model, but itâ€™s possible to have a distribution of mean functions, which are called Gaussian Process models [19,49,54]. These flexible models can
fit complex relationships between ğ‘¥ and ğ‘¦. Surprisingly, they are not im-
plemented via the mean function, but by generalising the variance func-
tion to make it a covariance function. Covariance functions are not dis- cussed here but they are also useful for modelling hierarchical or nested data [24,48], and for modelling dependencies in time or space. Neu- ral networks [21,56,68] and Bayesian additive regression trees (BART) [12,60] are other options for flexible mean functions.

Further advantages of PPMs

In addition to providing prediction uncertainty, PPMs have several other benefits. Hyperparameter values are typically selected by trying many options and choosing the combination that performs best. To avoid overfitting, crossvalidation or a similar approach divides the train- ing data into smaller subsets, some of which are used for training and others to assess performance. But with small datasets, crossvalidation can give unstable models and a poor assessment of performance. Many Bayesian approaches can learn values of some hyperparameters using all the training data and have a built-in prevention of overfitting [64,71]. They also incorporate the uncertainty in the hyperparameters in the pre- dictions. Models can still be compared using only the training data by estimating leave-one-out (LOO) crossvalidation performance, without the computational cost of actually retraining the model for each sample [66,67]. Vehtari and colleagues have also developed methods to assess when a LOO estimate is unreliable, and the model can be retrained only for these samples [69].
Another advantage is that background information such as adverse outcome pathways [7], constraints on parameters [34], or monotonic relationships [14] can often be incorporated into the model, which can guide the model to better solutions.
Often several structurally similar compounds are available that have different binding aï¬ƒnities or potencies, but also with different results in toxicity assays, and a decision must be taken to designate one compound in the series as the lead. PPMs can not only rank compounds but also obtain an uncertainty in the ranking, thus enabling decision makers to conclude that one compound is reliably better than another [34,55].
Finally, many popular machine learning methods have a PPM or Bayesian analogue, including regularised linear and generalised lin- ear models (lasso, ridge regression) [10,47,53,45], tree models (ran- dom forests, xgboost) [11,12,60], support vector machines [59,64], and neural networks [43,56,68]. Hence, it is often possible to convert your favourite model into one that provides prediction uncertainty.

Drawbacks and challenges

The main drawback of Bayesian or other PMMs is that they require more work, possibly twice as much, since getting appropriately cali- brated uncertainty is just as hard as getting accurate predictions. For example, 95% prediction intervals should contain 95% of the out-of- sample or test data values[72].
For fully Bayesian methods, the computational overhead may be high, making it diï¬ƒcult to iteratively fit, check, and update models dur-



ing development (although computations are often much quicker when making predictions). Storage for parameter values may be a problem for large models since this equals the number of parameters times number of Markov chain Monte Carlo draws. These approaches may therefore be harder to scale to large datasets, but faster and scalable algorithms is an active area of research. Another solution to large data is to cleverly select a weighted subset of samples that is much smaller than the origi- nal but captures the essential features. This â€œcoresetâ€ approach enables standard PPM methods to be used on the smaller dataset with little loss of information [8,22].
Finally, not all sources of uncertainty can be captured. Many sources of uncertainty discussed above arise because many modelling options are available, and different choices lead to different predictions. All of the choices relate to the prediction model, but many decisions need to be made outside of the model. We refer to these extra-model choices as the project workflow and they include experimental decisions such as the technology, cell-line, assay, antibodies, protocol, and so on. Also included are data processing pipelines where raw data are cleaned, transformed, categorised, coded, and normalised before they are entered into a prediction model. A single workflow is commonly used, with the untested assumption that variations in the workflow will lead to the same predictions and results. However, variations in workflows and an- alytic decisions do lead to variations results [4,23,32,57,58,61,62].

Reporting uncertainty to help risk communication and decision making

The ultimate aim of prediction models in drug discovery is to en- able better decision making. Thus, not only should predictions be accu- rate with prediction uncertainty adequately represented, but the results should be easy to understand by decision makers. Fortunately, PPMs provide intuitive results for continuous (Fig. 6D), binary (Fig. 9D), cat- egorical, and ordered categorical outcomes [56,71], as well as for com- pound rankings [34,55]. We have found that safety pharmacologists and other project members can easily interpret the predictive distributions provided by PPMs and value the confidence in the predictions that these distributions provide [34,71].
With recent advances in algorithms, hardware, and software, Bayesian or other PPMs are now feasible for most â€“ if not all â€“ ma- chine learning problems encountered in drug discovery. Making PPMs the standard approach for critical ML problems will enable more in- formed and better decisions.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
References

Bezanson J, Edelman A, Karpinski S, Shah VB. Julia: a fresh approach to numerical computing. SIAM Rev 2017;59(1):65â€“98.
Blackwell M, Honaker J, King G. A unified approach to measurement error and miss- ing data: overview and applications. Sociol Methods Res 2015;46(3):303â€“41.
Blundell C, Cornebise J, Kavukcuoglu K, Wierstra D. Weight uncertainty in neural networks. In: Proceedings of the 32nd international conference on international con- ference on machine learning. In: ICMLâ€™15, 37; 2015. p. 1613â€“22.
Botvinik-Nezer R, Holzmeister F, Camerer CF, Dreber A, Huber J, Johannesson M, Kirchler M, Iwanir R, Mumford JA, Adcock RA, Avesani P, Baczkowski BM, Ba- jracharya A, Bakst L, Ball S, Barilari M, Bault N, Beaton D, Beitner J, Benoit RG, Berkers RMWJ, Bhanji JP, Biswal BB, Bobadilla-Suarez S, Bortolini T, Bottenhorn KL, Bowring A, Braem S, Brooks HR, Brudner EG, Calderon CB, Camilleri JA, Castrel- lon JJ, Cecchetti L, Cieslik EC, Cole ZJ, Collignon O, Cox RW, Cunningham WA, Czoschke S, Dadi K, Davis CP, Luca AD, Delgado MR, Demetriou L, Dennison JB, Di X, Dickie EW, Dobryakova E, Donnat CL, Dukart J, Duncan NW, Durnez J, Eed A, Eickhoff SB, Erhart A, Fontanesi L, Fricke GM, Fu S, GalvÃ¡n A, Gau R, Genon S, Glatard T, Glerean E, Goeman JJ, Golowin SAE, GonzÃ¡lez-GarcÃ­a C, Gorgolewski KJ, Grady CL, Green MA, Guassi Moreira JF, Guest O, Hakimi S, Hamilton JP, Han- cock R, Handjaras G, Harry BB, Hawco C, Herholz P, Herman G, Heunis S, Hoff- staedter F, Hogeveen J, Holmes S, Hu CP, Huettel SA, Hughes ME, Iacovella V, Iordan AD, Isager PM, Isik AI, Jahn A, Johnson MR, Johnstone T, Joseph MJE,
Juliano AC, Kable JW, Kassinopoulos M, Koba C, Kong XZ, Koscik TR, Kucukboy- aci NE, Kuhl BA, Kupek S, Laird AR, Lamm C, Langner R, Lauharatanahirun N, Lee H, Lee S, Leemans A, Leo A, Lesage E, Li F, Li MYC, Lim PC, Lintz EN, Liphardt SW, Vermeer AB, Losecaat Love BC, Mack ML, Malpica N, Marins T, Maumet C, Mc- Donald K, McGuire JT, Melero H, MÃ©ndez Leal AS, Meyer B, Meyer KN, Mihai G, Mitsis GD, Moll J, Nielson DM, Nilsonne G, Notter MP, Olivetti E, Onicas AI, Pa- pale P, Patil KR, Peelle JE, PÃ©rez A, Pischedda D, Poline JB, Prystauka Y, Ray S, Reuter-Lorenz PA, Reynolds RC, Ricciardi E, Rieck JR, Rodriguez-Thompson AM, Romyn A, Salo T, Samanez-Larkin GR, Morales E, Schlichting ML, Schultz DH, Shen Q, Sheridan MA, Silvers JA, Skagerlund K, Smith A, Smith DV, Sokol-Hess- ner P, Steinkamp SR, Tashjian SM, Thirion B, Thorp JN, TinghÃ¶g G, Tisdall L, Tomp- son SH, Toro-Serey C, Torre Tresols JJ, Tozzi L, Truong V, Turella L, van â€™t Veer AE, Verguts T, Vettel JM, Vijayarajah S, Vo K, Wall MB, Weeda WD, Weis S, White DJ, Wisniewski D, Xifra-Porxas A, Yearling EA, Yoon S, Yuan R, Yuen KSL, Zhang L, Zhang X, Zosky JE, Nichols TE, Poldrack RA, Schonberg T. Variability in the analy- sis of a single neuroimaging dataset by many teams. Nature 2020;582:84â€“8.
Briggs W. Uncertainty: the soul of modeling, probability and statistics. New York,
NY: Springer; 2016.
Brown T.B., Mann B., Ryder N., Subbiah M., Kaplan J., Dhariwal P., Neelakantan A., Shyam P., Sastry G., Askell A., Agarwal S., Herbert-Voss A., Krueger G., Henighan T., Child R., Ramesh A., Ziegler D.M., Wu J., Winter C., Hesse C., Chen M., Sigler E., Litwin M., Gray S., Chess B., Clark J., Berner C., McCandlish S., Radford A., Sutskever I., Amodei D.. Language models are few-shot learners. 2020. ArXiv.
Burgoon LD, Angrish M, Garcia-Reyero N, Pollesch N, Zupanic A, Perkins E. Predict- ing the probability that a chemical causes steatosis using adverse outcome pathway bayesian networks (AOPBNs). Risk Anal 2019;40(3):512â€“23.
Campbell T, Broderick T. Bayesian coreset construction via greedy iterative geodesic ascent. In: Dy J, Krause A, editors. Proceedings of the 35th international conference on machine learning. Proceedings of Machine Learning Research, 80. PMLR; 2018.
p. 698â€“706.
Carroll RJ, Ruppert D, Stefanski LA, Crainiceanu CM. Measurement error in nonlin- ear models: a modern perspective. 2nd ed. Boca Raton, FL: Chapman & Hall/CRC; 2006.
Carvalho CM, Polson NG, Scott JG. Handling sparsity via the horseshoe. Proc Mach Learn Res 2009;5:73â€“80.
Chipman HA, George EI, McCulloch RE. Bayesian CART model search. J Am Stat Assoc 1998;93(443):935â€“48.
Chipman HA, George EI, McCulloch RE. BART: Bayesian additive regression trees. Ann Appl Stat 2010;4(1):266â€“98.
Cragg JG. Some statistical models for limited dependent variables with application to the demand for durable goods. Econometrica 1971;39(5):829.
DePalma G, Craig BA. Bayesian monotonic errors-in-variables models with applica- tions to pathogen susceptibility testing. Stat Med 2017;37(3):487â€“502.
Gal Y, Ghahramani Z. Dropout as a bayesian approximation: representing model uncertainty in deep learning. In: Balcan MF, Weinberger KQ, editors. Proceedings of The 33rd International Conference on Machine Learning. Proceedings of Machine Learning Research, 48. New York, New York, USA: PMLR; 2016. p. 1050â€“9.
Ge H, Xu K, Ghahramani Z. Turing: a language for flexible probabilistic inference. In: International conference on artificial intelligence and statistics, AISTATS 2018, 9â€“11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain; 2018. p. 1682â€“90.
Geisser S. Predictive inference: an introduction. New York, NY: Chapman & Hall; 1993.
Gelman A, Carlin JB, Stern HS, Rubin DB. Bayesian data analysis. 2nd ed. Boca Raton: Chapman & Hall/CRC; 2004.
Gramacy RB. Surrogates: gaussian process modeling, design, and optimization for the applied sciences. CRC Press; 2020.
Gustafson P. Measurement error and misclassification in statistics and epidemiology: impacts and bayesian adjustments. Boca Raton: Chapman & Hall/CRC; 2004.
Hirschfeld L, Swanson K, Yang K, Barzilay R, Coley CW. Uncertainty quantifica- tion using neural networks for molecular property prediction. J Chem Inf Model 2020;60(8):3770â€“80.
Huggins J.H., Campbell T., Broderick T.. Coresets for scalable Bayesian logistic re- gression. 2016. ArXiv.
Huntington-Klein N, Arenas A, Beam E, Bertoni M, Bloem JR, Burli P, et al. The influence of hidden researcher decisions in applied microeconomics. Econ Inq 2021.
Johnstone RH, Bardenet R, Gavaghan DJ, Mirams GR. Hierarchical Bayesian infer- ence for ion channel screening dose-response data. Wellcome Open Res 2016;1:6.
Kendall A, Gal Y. What uncertainties do we need in Bayesian deep learning for com- puter vision?. Advances in Neural Information Processing Systems, 30. Curran As- sociates, Inc.; 2017.
Keynes JM. A treatise on probability. London: Macmillan & Co; 1921.
Khosravi A, Nahavandi S, Creighton D, Atiya AF. Comprehensive review of neu- ral network-based prediction intervals and new advances. IEEE Trans Neural Netw 2011;22(9):1341â€“56.
Kiureghian AD, Ditlevsen O. Aleatory or epistemic? Does it matter? Struct Saf 2009;31(2):105â€“12.
Kristiadi A, Hein M, Hennig P. Being Bayesian, even just a bit, fixes overconfidence in ReLU networks. In: I HDII, Singh A, editors. Proceedings of the 37th international conference on machine learning. Proceedings of Machine Learning Research, 119. PMLR; 2020. p. 5436â€“46.
Kwon Y, Won JH, Kim BJ, Paik MC. Uncertainty quantification using Bayesian neural networks in classification: application to biomedical image segmentation. Comput Stat Data Anal 2020;142:106816.
Lakshminarayanan B, Pritzel A, Blundell C. Simple and scalable predictive uncer- tainty estimation using deep ensembles. In: Advances in neural information process- ing systems, 30; 2017. p. 6402â€“13.



Landy JF, Jia ML, Ding IL, Viganola D, Tierney W, Dreber A, et al. Crowdsourc- ing hypothesis tests: making transparent how design choices shape research results. Psychol Bull 2020;146(5):451â€“79.
Lazic SE. Four simple ways to increase power without increasing the sample size. Lab Anim 2018;52(6):621â€“9.
Lazic SE, Edmunds N, Pollard CE. Predicting drug safety and communicating risk: benefits of a bayesian approach. Toxicol Sci 2018;162(1):89â€“98.
Lazic SE, Williams DP. Improving drug safety predictions by reducing poor analytical practices. Toxicol Res Appl 2020;4 239784732097863.
Lesaffre E, Lawson AB. Bayesian biostatistics. Chichester, UK: Wiley; 2012.
Little RJA, Rubin DB. Statistical analysis with missing data. 3rd ed. Hoboken, NJ: WIley; 2020.
Lunn D, Jackson C, Best N, Thomas A, Spiegelhalter D. The BUGS book: a practical introduction to bayesian analysis. Boca Raton, FL: CRC Press; 2013.
McElreath R. Statistical rethinking: Bayesian course with examples in R and Stan. Boca Raton, FL: CRC Press; 2016.
Mervin LH, Johansson S, Semenova E, Giblin KA, Engkvist O. Uncertainty quantifi- cation in drug design. Drug Discov Today 2021;26(2):474â€“89.
Muff S, Riebler A, Held L, Rue H, Saner P. Bayesian analysis of measure- ment error models using integrated nested Laplace approximations. J R Stat Soc 2014;64(2):231â€“52.
Muller P, Quintana FA, Jara A, Hanson T. Bayesian nonparametric data analysis. Springer; 2015.
Neal RM. Bayesian learning for neural networks. New York, NY: Springer; 1996.
Nix D, Weigend A. Estimating the mean and variance of the target probability dis- tribution. In: Proceedings of 1994 IEEE international conference on neural networks (ICNNâ€™94). IEEE; 1994.
Park T, Casella G. The Bayesian lasso. J Am Stat Assoc 2008;103(482):681â€“6.
Pearce T, Leibfried F, Brintrup A. Uncertainty in neural networks: approximately Bayesian ensembling. In: Chiappa S, Calandra R, editors. Proceedings of the twenty third international conference on artificial intelligence and statistics. Proceedings of Machine Learning Research, 108. PMLR; 2020. p. 234â€“44.
Piironen J, Vehtari A. Sparsity information and regularization in the horseshoe and other shrinkage priors. Electron J Stat 2017;11(2):5018â€“51.
Pinheiro JC, Bates DM. Mixed-effects models in S and S-Plus. London: Springer; 2000.
Rasmussen CE, Williams CKI. Gaussian processes for machine learning. Cambridge, MA: MIT Press; 2006.
Reynolds J, Malcomber S, White A. A Bayesian approach for inferring global points of departure from transcriptomics data. Computat Toxicol 2020 100138.
Richardson S, Gilks WR. A Bayesian approach to measurement error prob- lems in epidemiology using conditional independence models. Am J Epidemiol 1993;138(6):430â€“42.
Rigby R. Distributions for modelling location, scale, and shape : using GAMLSS in
R. Bocat Raton, FL: CRC Press; 2020.
RoÄkovÃ¡ V, George EI. The spike-and-slab LASSO. J Am Stat Assoc 2018;113(521):431â€“44.
Schulz E, Speekenbrink M, Krause A. A tutorial on gaussian process regression: mod- elling, exploring, and exploiting functions. J Math Psychol 2018;85:1â€“16.
Semenova E., Guerriero M.L., Zhang B., Hock A., Hopcroft P., Kadamur G., Afzal A.M., Lazic S.E.. Flexible fitting of PROTAC concentration-response curves with Gaussian processes. 2020a. BioRxiv.
Semenova E, Williams DP, Afzal AM, Lazic SE. A Bayesian neural network for toxicity prediction. Computat Toxicol 2020;16:100133.
Shi L, Campbell G, Jones WD, Campagne F, Wen Z, Walker SJ, et al. The microar- ray quality control (MAQC)-II study of common practices for the development and validation of microarray-based predictive models. Nat Biotechnol 2010;28:827â€“38.
Silberzahn R, Uhlmann EL, Martin DP, Anselmi P, Aust F, Awtrey E, et al. Many analysts, one data set: making transparent how variations in analytic choices affect results. Adv Methods Pract Psychol Sci 2018;1(3):337â€“56.
Sollich P. Bayesian methods for support vector machines: evidence and predictive class probabilities. Mach Learn 2002;46(1/3):21â€“52.
Sparapani RA, Logan BR, McCulloch RE, Laud PW. Nonparametric survival analysis using Bayesian Additive Regression Trees (BART). Stat Med 2016;35(16):2741â€“53.
Stanton-Geddes J, de Freitas CG, Dambros CdS. In defense of P values: comment on the statistical methods actually used by ecologists. Ecology 2014;95:637â€“42.
Steegen S, Tuerlinckx F, Gelman A, Vanpaemel W. Increasing transparency through a multiverse analysis. Perspect Psychol Sci 2016;11(5):702â€“12.
Teye M, Azizpour H, Smith K. Bayesian uncertainty estimation for batch normalized deep networks. In: Dy J, Krause A, editors. Proceedings of the 35th international conference on machine learning. Proceedings of Machine Learning Research, 80. PMLR; 2018. p. 4907â€“16.
Tipping ME. Sparse bayesian learning and the relevance vector machine. JMLR 2001;1(1):211â€“44.
van Buuren S. Flexible imputation of missing data. Boca Raton, FL: CRC Press; 2012.
Vehtari A, Gelman A, Gabry J. Erratum to: practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 2016;27(5) 1433â€“1433.
Vehtari A, Gelman A, Gabry J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 2016;27(5):1413â€“32.
Vehtari A, Lampinen J. Bayesian neural networks: case studies in industrial ap- plications. In: Soft Computing in Industrial Applications. Springer London; 2000.
p. 415â€“24.
Vehtari A., Simpson D., Gelman A., Yao Y., Gabry J.. Pareto smoothed importance sampling. 2015. ArXiv 1507.02646.
Welling M, Teh YW. Bayesian learning via stochastic gradient Langevin dynamics. In: Proceedings of the 28th international conference on international conference on machine learning. In: ICMLâ€™11, 33; 2011. p. 681â€“8.
Williams DP, Lazic SE, Foster AJ, Semenova E, Morgan P. Predicting drug-induced liver injury with Bayesian machine learning. Chem Res Toxicol 2020;33(1):239â€“48.
Zhang Y, Lee AA. Bayesian semi-supervised learning for uncertainty-calibrated pre- diction of molecular properties and active learning. Chem Sci 2019;10(35):8154â€“63.
