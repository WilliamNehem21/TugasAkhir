Array 17 (2023) 100268










Evaluation of next-generation high-order compressible fluid dynamic solver on cloud computing for complex industrial flows
R. Al Jahdali a,âˆ—, S. Kortas c, M. Shaikh c, L. Dalcin a, M. Parsani a,b
a King Abdullah University of Science and Technology (KAUST), Computer Electrical and Mathematical Science and Engineering Division (CEMSE), Extreme Computing Research Center (ECRC), 23955-6900 Thuwal, Saudi Arabia
b King Abdullah University of Science and Technology (KAUST), Physical Sciences and Engineering Division (PSE), 23955-6900 Thuwal, Saudi Arabia
c KAUST Supercomputing Core Laboratory, 23955-6900 Thuwal, Saudi Arabia


A R T I C L E  I N F O	A B S T R A C T

	

Keywords:
Cloud computing
Amazon Web Services Elastic Compute Cloud Fluid mechanics
Compressible Navierâ€“Stokes equations Fully-discrete entropy stable algorithms
Industrially relevant computational fluid dynamics simulations frequently require vast computational resources that are only available to governments, wealthy corporations, and wealthy institutions. Thus, in many contexts and realities, high-performance computing grids and cloud resources on demand should be evaluated as viable alternatives to conventional computing clusters. In this work, we present the analysis of the time-to-solution and cost of an entropy stable collocated discontinuous Galerkin (SSDC) compressible computational fluid dynamics framework on Ibex, the on-premises cluster at KAUST, and the Amazon Web Services Elastic Compute Cloud for complex compressible flows. SSDC is a prototype of the next generation computational fluid dynamics frameworks developed following the road map established by the NASA CFD vision 2030. We simulate complex flow problems using high-order accurate fully-discrete entropy stable algorithms. In terms of time-to-solution, the Amazon Elastic Compute Cloud delivers the best performance, with the Graviton2 processors based on the Arm architecture being the fastest. However, the results also indicate that the Ibex nodes based on the AMD Rome architecture deliver good performance, close to those observed for the Amazon Elastic Compute Cloud. Furthermore, we observed that computations performed on the Ibex on-premises cluster are currently less expensive than those performed in the cloud. Our findings could be used to develop guidelines for selecting high-performance computing cloud resources to simulate realistic fluid flow problems.





Introduction

The process of predicting fluid flow (such as gases and liquids), mass transfer, chemical reactions, and other associated phenomena with a computer during the design or production process is known as computational fluid dynamics (CFD). State-of-the-art CFD is critical in modeling many physical phenomena, including biomedicine [1], plasma science [2], climate [3,4], weather [5,6], and aerodynamics [7,8], and for elucidating and analyzing the underlying mechanisms of these complex phenomena. In the aeronautics and aerospace fields, aggressive use of CFD has resulted in significant reductions in wind tunnel time as well as a reduction in the number of experimental rig tests. Thus, through the use of CFD, industries, governments and national laboratories have been able to produce products faster and at reduced costs [8], saving hundreds of millions of dollars. In addition to reducing testing requirements, CFD has the additional potential of providing superior understanding and insight into the critical physical
phenomena limiting component performance, thereby opening up new frontiers in a variety of fields.
Among the critical goals, NASAâ€™s vision roadmap [9] indicates the computational procedure and algorithms to perform realistic aerody- namic simulations by 2030 with the next generation of computing architectures. The path to the final aerodynamic simulation goal in- cludes a scheduled demonstration of various critical technologies and production scalable entropy-stable solvers are one of them [9]. A prototype of these new solvers, named SSDC, where â€˜â€˜Entropyâ€™â€™ is represented in the acronym by its thermodynamic symbol, â€˜â€˜Sâ€™â€™, has been developed and deployed for industrial collaborations [10]. SSDC combines a state-of-the-art, provably stable, adaptive order solver, with software engineering that exploits hybrid shared-distributed memory capabilities.
High-performance computing (HPC) has had a significant influence on CFD. The demand for HPC systems increases with the increasing


âˆ— Corresponding author.
E-mail addresses: Rasha.Aljahdali@kaust.edu.sa (R. Al Jahdali), samuel.kortas@kaust.edu.sa (S. Kortas), mohsin.shaikh@kaust.edu.sa (M. Shaikh), dalcinl@gmail.com (L. Dalcin), matteo.parsani@kaust.edu.sa (M. Parsani).
https://doi.org/10.1016/j.array.2022.100268
Received 25 October 2022; Received in revised form 1 December 2022; Accepted 5 December 2022
Available online 9 December 2022
2590-0056/Â© 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).



complexity of the simulation, which is a primary element in fluid flow simulations. Owning high-performance computer facilities helps to process simulations faster and allows one to perform large-scale sim- ulations. Supercomputer centers and other HPC computing capabilities are being used to perform massive simulations in academia, national laboratories, and industry. Nevertheless, the cost of the computing cluster may be exorbitant to purchase and maintain, and therefore, it may not be able to keep up with todayâ€™s computational demands. Hence, on-demand grids and cloud resources should be viewed as alternatives to traditional computing clusters in various situations and circumstances. A major advantage of cloud resources is the opportunity to use cutting-edge hardware without the requirement for financial investment or IT maintenance costs. In addition, instead of an annual license, the software can be purchased on a pay-per-use basis. Be- cause the industry is interested in using on-demand computing services for engineering simulations, benchmarking the performance of next- generation of compressible CFD solver prototypes, such as SSDC, on cloud resources is essential.
Currently, the industry is experimenting with a variety of ways for incorporating cloud computing into CFD workloads. Thus, it is pivotal to evaluate the performance of parallel systems to determine whether they are capable of running complex industrial CFD problems with up- coming solvers. On one side, important studies in this direction for well established classic solvers based on finite volume, finite differences, and finite elements (up to second order accuracy) have been reported, for instance, by PeÃ±a-Monferrer et al. [11], Ashton et al. [12] and Turner et al. [13] for complex industry problems. In particular, in early 2020, the results of the performance of the open-source OpenFOAM software on the Amazon Web Services (AWS) Elastic Compute Cloud (EC2) service for racing vehicles employing a hybrid RANS-LES model were presented by Ashton and co-authors [12]. The authors demonstrated that AWS might provide an HPC environment that would enable wider usage of high-fidelity CFD methods by permitting higher core counts and reducing turnaround time. In early 2021, in PeÃ±a-Monferrer et al.
[11] presented a hybrid cloud solution for efficient simulation and analysis of drop dispersions by breaking down and study the CFD data analysis pipeline into small microservice-like processes. Turner and co-authors [13] gave an exhaustive analysis of how well GPU and CPU architectures work and how much they cost for a complete aircraft RANS simulation using the CFD code zCFD. On the other side,
studies on the performance of adaptive non-linearly stable (high-order)
Numerous commercial solvers are currently deployed and utilized on the cloud to simulate fluid flow problems based on the Reynolds- averaged Navierâ€“Stokes (RANS) approach. However, there is no doubt that large eddy simulation (LES) has the potential to deliver results that are more accurate and reliable and will be one of the main approaches to solving challenging fluid flows problems. This study reports the first step towards high-performance CFD simulations on the AWS cloud
cluster for industrial workloads based on LES approaches with an â„ğ‘-
adaptive compressible solver. The main contributions of this work are
as follows. First, we deploy and test a novel prototype of the next generation of compressible CFD solver â€˜â€˜SSDCâ€™â€™ on the HPC cluster on the cloud. SSDC is a prototype of the framework defined in the NASA CFD 2030 vision, and it has novel capabilities in terms of adaptivity in space and time. Second, we manage and operate the AWS cloud cluster by optimizing the PETSc library and SSDC framework for each of the architectures used in the study. The installation details of the op- timized PETSc library can be found in https://github.com/ecrc/petsc_ installation_aws_cloud_2022. The former step is done in conjunction with the Slurm scheduler to simplify the management of jobs. Since we are simulating the LES model, a large number of AWS EC2 instances is needed. Third, we compare the on-premises cluster and the most recent architectures on the AWS cloud in terms of performance and cost.
The paper is organized as follows. In Section 2, we present the key ideas and elements of the spatial and temporal algorithms implemented in SSDC, whereas in Section 3, we give a brief overview of the soft- ware implementation and infrastructure. Section 4 briefly describes the computing environments of the on-premise Ibex cluster and Amazon EC2 cloud computing environment. Section 5 presents the performance results of the test case studies to analyze the performance of the SSDC solver on Ibex and Amazon EC2 clusters. Finally, the conclusion and future work are drawn in Section 6.

The compressible Navierâ€“Stokes equations

In this section, we give an overview of the discretization of the compressible Navierâ€“Stokes equations. A detailed presentation of the key elements of the spatial discretization is presented for the advectionâ€“ diffusion equation in multiple dimensions in Appendix A.
In the framework of the method of line approach, we first present the spatial discretization and subsequently describe the temporal inte- gration approach. The compressible Navierâ€“Stokes equations in Carte- sian coordinates read

compressible algorithms on the HPC cloud computing is lacking. Those
ğœ•ğ’’
+ âˆ‘3
ğœ•T ğ¼	3
ğ‘¥ğ‘š =
ğœ•T ğ‘‰
ğ‘¥ğ‘š ,  âˆ€
(ğ‘¥ , ğ‘¥ , ğ‘¥
) âˆˆ ğ›º,  ğ‘¡ â‰¥ 0,

spatial and temporal discretizations, in the wake of the final NASA 2030 CFD vision report [9], received an impetus from leading U.S.
ğœ•ğ‘¡
(
ğ‘š=1
ğœ•ğ‘¥ğ‘š
)
ğ‘š=1(
ğœ•ğ‘¥ğ‘š
1  2  3
)	(	)
(1)

ğ’’ ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¡
= g(ğµ)
ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¡ ,  âˆ€
ğ‘¥1, ğ‘¥2, ğ‘¥3
âˆˆ ğ›¤ ,  ğ‘¡ â‰¥ 0,

national laboratories, (e.g., NASA and Sandia National Laboratories),
ğ’’ (ğ‘¥ , ğ‘¥ , ğ‘¥ , 0) = g(0) (ğ‘¥ , ğ‘¥ , ğ‘¥ ) ,  âˆ€ (ğ‘¥ , ğ‘¥ , ğ‘¥ ) âˆˆ ğ›º,

top-tier academic institutions in Europe, U.S., and the Middle East. In
1  2  3
1  2  3
1  2  3

particular, at the end of 2021, the first fully discrete entropy stable
solver of any order on unstructured for compressible CFD, named
where the vectors ğ’’, T ğ¼
ğ‘š
and T ğ‘‰
ğ‘š
denote the conserved variables, the

SSDC, was presented in [10]. The SSDCâ€™s scalable algorithms have been proven to run fast and effectively on hardware ranging from laptops to supercomputers with more than 180,000 processors, such as the Shaheen XC40 supercomputer. However, no studies on the performance of these new solvers on the HPC cloud computing is available in literature.
Here, we provide a comprehensive cost analysis of the SSDC solver for simulating flow problems on AWS EC2. The purpose is to assess the viability of the HPC cluster on the Amazon cloud for solving complex
CFD industry flow problems and identify a set of AWS EC2 instances
g(ğµ), and the initial condition, g(0), are assumed to be in ğ¿2(ğ›º), with inviscid fluxes, and the viscous fluxes, respectively. The boundary data, the further assumption that g(ğµ) will be set to coincide with linear,
well-posed boundary conditions, prescribed in such a way that either entropy conservation or entropy stability is achieved. System (1) is closed with the assumption of the ideal gas model. The conserved variable vector can be written as
ğ’’ = [ğœŒ, ğœŒT1, ğœŒT2, ğœŒT3, ğœŒÂ£] T,
where ğœŒ denotes the density, U = [T1, T2, T3] T is the velocity vector,
and  is the specific total energy. The inviscid fluxes are given as

that deliver the shortest time and the lowest possible price. We use four test cases [10,14,15]: the turbulent flow over two spheres in tandem at
T ğ¼
ğ‘¥ğ‘š
= [ğœŒT , ğœŒT T
+ ğ›¿ğ‘š,1
h, ğœŒT T
+ ğ›¿  h,

a Reynolds number and a Mach number of ğ‘…ğ‘’ = 3.9Ã—103
and ğ‘€ğ‘ = 0.1,
ğœŒT T3
+ ğ›¿
ğ‘š,3
h, ğœŒT îˆ´] T,

respectively; the flow past a delta wing with the experimental sting
where h is the pressure, îˆ´ is the specific total enthalpy and ğ›¿
is the

at ğ‘…ğ‘’ = 106 and ğ‘€ğ‘ = 0.07; the NASA juncture flow experiment at
Kronecker delta. The required constituent relations are

ğ‘…ğ‘’ = 2.4 Ã— 106 and ğ‘€ğ‘ = 0.189; and the flow past the Imperial front wing at ğ‘…ğ‘’ = 2.2. Ã— 105 and ğ‘€ğ‘ = 0.036.
îˆ´ = ğ‘h f +
1 U TU ,  h = ğœŒğ‘…f ,  ğ‘… =  ğ‘…ğ‘¢ ,
2	ğ‘€ğ‘¤

where f is the temperature, ğ‘… is the universal gas constant, ğ‘€  is the molecular weight of the gas, and ğ‘h is the specific heat capacity at
constant pressure. Finally, the specific thermodynamic entropy is given

The continuous entropy stability analysis is mimicked by approx- imating the derivatives of the inviscid fluxes as follows [27], i.e.,

as
 ğ‘… 
(  f  )
( ğœŒ )
ğ‘ğ‘
ğœ• fğ¼
ğ‘¥ğ‘š â‰ˆ 2ğ–£ğ¼,ğœ… â—¦ğ–¥
(ğ’’ , ğ’’
) ğŸğœ… ,	(7)

ğ‘  =	log
ğ›¾ âˆ’ 1
â€“ ğ‘… log
ğœŒ
,  ğ›¾ =
ğ‘
â€“ ğ‘… ,
ğœ•ğ‘¥ğ‘š
ğ‘¥ğ‘š	ğ‘š

âˆ	âˆ	ğ‘
where ğ–£ğ¼,ğœ… is a differentiation matrix for the ğ‘¥
direction, â—¦ denotes the

where fâˆ and ğœŒâˆ are the reference temperature and density. The
ğ‘¥ğ‘š	(	)	ğ‘š

viscous fluxes T ğ‘‰
ğ‘š
are given by
Hadamard product, ğ–¥ğ‘¥ğ‘š
ğŸğœ… is a vector of ones.
ğ’’ğœ… , ğ’’ğœ…
is a two-point flux function matrix, and

[	3	 f 
The differentiation matrix ğ–£ğ¼,ğœ… is constructed as

T ğ‘‰
ğ‘š
0, ğœ1,ğ‘š, ğœ2,ğ‘š, ğœ3,ğ‘š,
âˆ‘
ğ‘–=1
ğ‘–,ğ‘š  ğ‘–
ğœ•
ğœ•ğ‘¥ğ‘š
T,	(2)
ğ–£ğ¼,ğœ… â‰¡ 1 âˆ’1 âˆ‘3 (
[J  ğœ•ğœ‰ğ‘™ ]
ğ‘¥ğ‘š
[
J  ğœ•ğœ‰ğ‘™ ]	)


while the viscous stresses are defined as
ğ‘¥ğ‘š
2 ğœ…
ğ‘™=1
ğœ‰ğ‘™
+
ğœ•ğ‘¥ğ‘š ğœ…
ğœ•ğ‘¥ğ‘š
ğ–£ğœ‰ğ‘™
ğœ…
,	(8)

ğœ  = ğœ‡
( ğœ•T
  ğ‘– +
ğœ•T
ğ‘— âˆ’ ğ›¿  2
âˆ‘3 ğœ•T )
,	(3)
where ğ–©ğœ… denotes the determinant of the discrete Jacobian, ğ–£ğœ‰ğ‘™
ğœ•	ğœ•ğœ‰
is an

ğ‘–,ğ‘—
ğœ•ğ‘¥ğ‘—
ğœ•ğ‘¥ğ‘–
ğ‘–,ğ‘— 3
ğ‘›=1
ğœ•ğ‘¥ğ‘›
SBP operator approximating
ğœ•ğœ‰
, while
J   ğ‘™
ğœ•ğ‘¥ğ‘š
indicates the discrete
ğœ…

where ğœ‡(f ) is the dynamic viscosity and ğœ…(f ) is the thermal conduc-
metric terms which must satisfy a discrete version of the geometric
ğ‘‰ ,ğœ…

conservation law (GCL) constraints [19,25,28]. In addition, ğ–£ğ‘¥1  is the

tivity.
ğ‘š
differentiation matrix, constructed as

vex extension, that when integrated over the physical domain, ğ›º, The compressible Navierâ€“Stokes equations given in (1) have a con-
ğ–£ğ‘‰1 ,ğœ… â‰¡ ğ–©âˆ’1 âˆ‘ ğ–£
[J ğœ•ğœ‰ğ‘™ ]
.	(9)

depends only on the boundary data and negative semi-definite dissi-
ğ‘¥ğ‘š
ğœ…
ğ‘™=1
ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š ğœ…

pation terms. This convex extension depends on an entropy function,
, that is constructed from the thermodynamic entropy as
S
Furthermore, the derivative of the entropy variables, w, is approxi- mated as

= âˆ’ğœŒğ‘ ,
and provides a mechanism for proving stability in the ğ¿2
norm. The
ğœ½ğ‘
= ğ–£ğ‘‰2 ,ğœ… ğ’˜
+ ğ’ğ€ğ“ğ‘‰2 â‰ˆ ğœ•w .	(10)
ğœ•ğ‘¥ğ‘

entropy variables M are an alternative variable set related to the conservative variables via a one-to-one mapping. They are defined
T
The differentiation matrix ğ–£ğ‘‰2,ğœ… in (10) is defined as
ğ‘—
âˆ‘3  [	]

in terms of the entropy function S by the relation M	= ğœ•Sâˆ•ğœ•î‰—
ğ–£ğ‘‰2 ,ğœ…
â‰¡ ğ–©âˆ’1
J ğœ•ğœ‰ğ‘
ğ–£ .	(11)

and they are extensively used in the entropy stability proofs of the algorithms used herein; see, for instance, [16â€“19] and the references
ğ‘¥ğ‘—
ğœ…
ğ‘=1
ğœ•ğ‘¥ğ‘—
[
ğœ‰ğ‘
ğœ…
]	[	]

therein. In addition, they simultaneously symmetrize the inviscid and the viscous flux Jacobians in all three spatial directions. Following
The metric terms
J  ğœ•ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š
and
ğœ…
J ğœ•ğœ‰ğ‘
ğœ•ğ‘¥ğ‘—
can be computed in different
ğœ…

the analysis described in [16,20], we multiply Eqs. (1) by the (local) entropy variables M and arrive at the integral form of the (scalar) entropy equation
ways. In the SSDC solver, these metrics terms are computed based upon
conforming interfaces are given in [25]; for ğ‘- and â„ğ‘-nonconforming the optimization procedure of Crean et al. [28]. Algorithmic details for
interfaces, the interested reader is referred to [19,24].

d dğ‘¡
ğ‘†dğ›º =  d ğœ‚ â‰¤
ğ›º	dğ‘¡
âˆ‘3
ğ‘š=1 ğ›¤
(M TT ğ‘‰
ğ‘¥âˆ’
)
ğ‘¥ ğ‘›ğ‘¥ dğ›¤ âˆ’ ğ·ğ‘‡ ,	(4)
The entropy function of the semidiscretization (6) mimics closely Eq. (4). In fact, following closely the entropy stability analysis, the SBP operators and their equivalent telescoping forms yield [16,19,22,24â€“

where ğ‘›ğ‘¥ is the ğ‘šth component of the outward facing unit normal to ğ›¤
and
26]
 ğ‘‘ ğŸâŠ¤Ì‚ğ–¯ ğ’ =  ğ‘‘ ğœ‚ = ğğ“ âˆ’ ğƒğ“ + Î¥,	(12)

âˆ‘3	( ğœ•M )âŠ¤
ğœ•M
ğ‘‘ğ‘¡
ğ‘‘ğ‘¡

ğ·ğ‘‡ =
ğ‘š,ğ‘—=1 ğ›º
ğœ•ğ‘¥ğ‘š
ğ–¢ğ‘š,ğ‘— ğœ•ğ‘¥ dğ›º.	(5)
which is the semi-discrete analog of (4). Here ğğ“ is the discrete
boundary term (i.e., the discrete version of the first integral term on

More details about the continuous entropy analysis can be found [21â€“ 23].

Spatial discretization of the compressible Navierâ€“Stokes equations

The details of the algorithm for conforming and nonconforming interfaces can be found in [10,16,19,24â€“26]. Herein, we summarized the main steps of the algorithm for conforming interfaces.
main ğ›º is divided into ğ¾ nonoverlapping elements. Then, following the To approximate the compressible Navierâ€“Stokes equations, the do-
Eqs. (A.1), on the ğœ…th element, the generic entropy stable discretization procedure outlined for the spatial discretization of convectionâ€“diffusion
of (1) reads
the right-hand side of (4)), ğƒğ“ is the discrete dissipation term (i.e., the
discrete version of the second term on the right-hand side of (4)) and
completeness, we note that the matrix Ì‚ğ–¯ may be thought of as the Î¥ enforces interface coupling and boundary conditions [16,20]. For mass matrix in the context of the discontinuous Galerkin finite element
method.
In our framework, the boundary conditions necessary to close sys- tem (1) preserve the entropy stability of the interior operators described above. Precisely, solid inviscid and viscous wall boundary conditions are imposed as described in [20,29] whereas, for the far-field, we use the approach described in [30]. The implementation of boundary conditions for a general framework (not necessarily entropy stable) can be found in [31].

dğ’’ğœ… dğ‘¡
+ âˆ‘3
ğ‘š=1
2ğ–£ğ¼,ğœ… â—¦ğ–¥ğ‘¥
(ğ’’ , ğ’’
) ğŸğœ… =
âˆ‘3
ğ‘š,ğ‘—=1
ğ‘‰1 ,ğœ… [
ğ‘¥ğ‘š
ğ–¢ğ‘š,ğ‘— ]
ğœ½ğ‘—
(6)

Temporal discretization of the compressible Navierâ€“Stokes equations

+ ğ’ğ€ğ“ğ¼ + ğ’ğ€ğ“ğ‘‰ + ğğ¢ğ¬ğ¬ğ¼ + ğğ¢ğ¬ğ¬ğ‘‰1 ,
where ğ’’ğœ… is the numerical solution vector at the mesh nodes, while ğğ¢ğ¬ğ¬ğ¼ and ğğ¢ğ¬ğ¬ğ‘‰ denote the interface dissipation contributions for the inviscid
and viscous parts of the equations, respectively.
A general (explicit or implicit) ğ‘ -stage Rungeâ€“Kutta (RK) scheme
is composed of a matrix ğ´ of dimensions ğ‘  Ã— ğ‘  and two vectors ğ‘ and can be concisely encapsulated using its Butcher tableau [32], which
ğ‘ of length ğ‘ . The basic idea of the relaxation procedure is to enforce
conservation, dissipation, or other solution properties with respect to



a convex functional by scaling the weights ğ‘ğ‘– of the RK method by a real-value parameter ğ›¾Ìƒ. Hence, the time step from ğ‘¢ğ‘› â‰ˆ ğ‘¢(ğ‘¡ğ‘›) is given by
âˆ‘ğ‘ 
Table 1
Details of the Ibexâ€™s architectures.

ğ‘¦ğ‘– = ğ‘¢ğ‘› + ğ›¥ğ‘¡	ğ‘ğ‘–ğ‘— ğ‘“ (ğ‘¡ğ‘› + ğ‘ğ‘— ğ›¥ğ‘¡, ğ‘¦ğ‘— ),
ğ‘—=1
âˆ‘ğ‘ 

ğ‘¢ğ‘›+1 = ğ‘¢ğ‘› + ğ›¾Ìƒğ‘›ğ›¥ğ‘¡	ğ‘ğ‘–ğ‘“ (ğ‘¡ğ‘› + ğ‘ğ‘–ğ›¥ğ‘¡, ğ‘¦ğ‘–),
ğ‘–=1
Table 2
Amazon EC2 instances feature details.

where the stage values of the RK method are denoted by ğ‘¦ğ‘–. The
parameter ğ›¾Ìƒğ‘› is computed using the global relaxation procedure, i.e., ğ›¾Ìƒğ‘› is a root of a global nonlinear algebraic equation for ğœ‚ [33]. We recover the step of the classic RK method if ğ›¾Ìƒğ‘› = 1.
In the SSDC solver, global relaxation RK schemes are used to in- tegrate in time systems of ODEs (6) such that Eq. (12) is fulfilled. For performance and robustness reasons, we select the secant method as the root finding algorithm for computing the relaxation parameter
ğ›¾Ìƒğ‘› [14,34].
Software implementation

The SSDC solver used in this work is being developed in the Ad- vanced Algorithms and Numerical Simulations Laboratory (AANSLab), which is part of the Extreme Computing Research Center (ECRC) at King Abdullah University of Science and Technology (KAUST). The SSDC framework is built on top of the highly-scalable Portable and Ex- tensible Toolkit for Scientific computing (PETSc) [35], its mesh topol- ogy abstraction (DMPlex) [36], and its scalable differentialâ€“algebraic
equation solver components [37]. The spatial discretization features â„ğ‘-
adaptive capabilities on unstructured quadrilateral/hexahedral meshes.
Support for nonconforming meshes relies on the p4est software library [38,39] and its bridge to PETScâ€™s DMPlex [40]. Leveraging the capa- bilities of the PETSc library allows support for different mesh formats including fluent, Exodus II, CGNS and GMSH. Triangle/tetrahedral meshes are converted on the fly into quadrilateral/hexahedral ele- ments; uniform and non-uniform mesh refinements algorithms are also available.

HPC clusterâ€™s architectures

Amazon EC2 offers a variety of instance types that are suitable for a wide range of applications. It is possible to mix and match CPU, memory, storage, and networking resources by using different instance types. Depending on the workload, we can choose from a variety of instance sizes available for each type of instance. AWS Cloud provides highly flexible computing platforms that are suited for performing HPC applications in terms of resource availability and configurability. CFD usually necessitates a large amount of computer resources, and its software architecture makes it suited for parallel processing; this requires the use of an HPC or cloud computing infrastructure. In order to deal with parallel computation needs, an adequate resource management system is also necessary. AWS ParallelCluster is one of the possible solutions to the aforementioned requirements. Here, we will briefly describe the compute environments of Amazon EC2 cloud and on-premise resource Ibex cluster.

Ibex cluster

Ibex is a heterogeneous cluster composed of nodes with various CPU architectures and an assortment of GPUs hosted at KAUST. Ibex is made up of more than 400 nodes that are constantly monitored by the systems team. This heterogeneous cluster has a mix of CPU architectures, including Intel Xeon Platinum 8000 series (Skylake-SP) 1st generation, Intel Xeon Platinum 8276M (Cascade Lake-SP) 1st gen- eration, AMD EPYC 7642 (AMD Rome), and various GPUs nodes with P100s, V100s, GTX1080Ti, and RTX2080Ti, and with variable amounts



of RAM ranging from 360 GB to 700 GB. The network connectivity on Ibex depends on InfiniBand HDR Director Switch. It has the following features and benefits: Mellanox HDR Infiniband is capable of 200 Gbps, it uses in compute and storage nodes with HDR-100 at 100 Gbps speed. Table 1 summarizes the main features of the Ibexâ€™s architectures used in this work. We highlight that Ibex has a daily occupancy of about 75%, with Intel architectures in high demand.

Amazon ParallelCluster

AWS ParallelCluster is an open source cluster management tool that is supported by AWS to deploy and manage HPC clusters in the AWS cloud. It enables the operation of HPC clusters in the AWS Cloud Environment and provides a wide range of configuration options. AWS ParallelCluster offers a variety of batch schedulers for manag- ing nodes, resources, and parallel workloads. Users can use them to run large-scale scientific and engineering tasks. AWS cloud has many data centers throughout the world. In this work, all the experiments were conducted on Amazonâ€™s cloud data center in US East region in Northern Virginia. Our assessments cover the possible instance types which are similar to those available on KAUSTâ€™s Ibex cluster described in Section 4.1 with in addition one Arm architecture as described at the end of this section. Compute Optimized instances are suited for compute intensive applications that benefit from powerful CPUs. In this work, we have built AWS ParallelCluster and we included the following compute-optimized instances: C5d, C5a, and C6 g instances. This familyâ€™s instances are ideal for HPC. C5d will run on the Intel Xeon Scalable Processor 2nd generation or the Intel Xeon Platinum 8000 series (Skylake-SP) 1st generation with a sustained turbo frequency of up to 3.4 GHz and a single core turbo frequency of up to 3.5 GHz using Intel Turbo Boost Technology at launch. C5a instances provide the best x86 price/performance for a wide range of compute-intensive tasks, using 2nd generation 3.3 GHz AMD EPYC 7002 series processors built on a 7 nm technology node for greater efficiency. C5a instances can deliver up to 20 Gbps of network capacity as well as 9.5 Gbps of dedicated bandwidth to Amazon. EC2 C6 g instances are powered by AWS Graviton2 processors based on the Arm architecture. Table 2 shows the instances feature details.

Performance of the SSDC solver on Ibex and AWS EC2 clusters

The purpose of this study is to analyze the performance of the SSDC solver on Ibex and Amazon EC2 clusters for complex flow problems and provide the current cost per core hour. The default number of CPU cores for an EC2 instance depends on its type. We only allowed the instance to use the available physical CPU cores. This means we disabled hyperthreading. In the following sections, we report the arithmetic average of the wall-clock time (WCT) in seconds of three independent runs and the associated estimated cost in United States Dollars (USD).



/ig. 1. Upper panel: Illustration of the mesh. Lower panel: Contour plots of the velocity magnitude for the flow past two identical spheres in tandem at time ğ‘¡ = 100.



/ig. 2. Flow past two spheres in tandem: Wall-clock time in seconds for each simulation against the number of nodes for Intel CPU architectures. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)





/ig. 3. Flow past two spheres in tandem: Cost performance in USD of Intel CPU architectures.



Flow past two spheres in tandem

D, held fixed in a rectangular domain located at a separation distance of We simulate the flow past two equally sized spheres with a diameter 10D [41]. The Reynolds and Mach numbers are set to ğ‘…ğ‘’ğ· = âˆDâˆ•ğœˆ = 104 and ğ‘€ğ‘âˆ = 0.1, respectively. The Prandtl number is set to ğ‘ƒ ğ‘Ÿ = 0.7.
The quite upstream flow conditions of the first sphere are used to define the similarity parameters, e.g., âˆ is the free-stream velocity.
Regardless of the geometryâ€™s simplicity, capturing the flow in this regime is relatively difficult. The relevance of such flows around sev- eral bodies, specifically around two spheres, is considered significant in many practical applications, as it allows a better understanding of the effect of the wake behind a leading bluff body on the flow around a trailing one, for instance. A non-exhaustive list of important applications ranging from industrial fluidized beds to bio-reactors, to the combustion of aerosols, could be liquidâ€“gas two-phase flows [42], suppression of icing on the solid surface [43], and oil droplets [44]. The complexity of the flow fields is shown in the lower panel of Fig. 1
by plotting the velocity magnitude at time ğ‘¡ = 100.
We perform the numerical simulations using one of the grids TandemSpheresHexMesh2Pm provided by Steve Karman of Point- wise for the HiOCFD5 [41]. An illustration of the grid structure is shown in the upper panel of Fig. 1. In this study, we use this mesh
in combination with a solution polynomial order of ğ‘ = 10 (leading to
degrees  of   freedom   (DOFs)   of   â‰ˆ  2.616  Ã—  107. a formally eleventh-order accurate scheme), yielding a total number of
Figs. 2 and 4 show the WCT in seconds for the Ibex cluster and the
cluster, we simulated the flow problems using 40 (dark blue â€” Ibex I) EC2 instances using various CPU architectures. On the on-premises Ibex and 20 (gray â€” Ibex II) physical cores. Moreover, each processor is
responsible for one MPI thread. The wall-clock time for an on-premises cluster based on Intelâ€™s Cascade Lake architecture grows proportionally
with the node count, as shown in Fig. 2. This pattern holds in the 40
and 20 physical core setups. On-premise cluster performance degrades
for several reasons, including the problemâ€™s size and the fact that the
nodes are non-exclusive, with a daily occupancy of the Ibex cluster of approximately 75%. Thus, the more computing nodes, (1) the greater the likelihood that computing resources will be shared with other users, and (2) the more partitioned the job, the more likely it is that network bandwidth will be shared. We highlight that this behavior can also be observed in the case of exclusive access to the nodes. In fact, for a â€˜â€˜sufficientlyâ€™â€™ large number of CPU cores count and hence, sufficiently smaller local problems, communication between partitions cannot be hidden behind computations. Thus, the solverâ€™s performance degrades and departs from the ideal behavior.
For the EC2 c5d instances and all orders of accuracy, the simulations are speeded up as the number of CPUs increases. Moving from one to





/ig. 4. Flow past two spheres in tandem: Wall-clock time in seconds for each simulation against the number of nodes for AMD & Arm CPU architectures. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)



eight computing nodes, in particular, results in a speed-up factor of about 5.6 for both cases (8 being the perfect scaling factor). Overall, the c5d.9xlarge EC2 instance delivers the shortest time-to-solution. Notably, although the on-premises clusterâ€™s wall-clock time increases as the number of compute nodes increases, up to two Ibex nodes running
40 and 20 MPI threads are still faster than EC2 instances. The executions
on the on-premises cluster with 20 MPI threads are marginally faster
or comparable to the executions on the c5d.4xlarge instance with four
computing nodes. Therefore, it seems that utilizing half of a nodeâ€™s physically available cores has a favorable effect on wall-clock time. In fact, only half of the cores utilize the shared intra-node network, and more crucially, the workload per core permits better communications to be hidden behind computations.
As observed from Fig. 2, the results are favorable for simulations conducted on 8 EC2 nodes. In particular, the c5d.9xlarge EC2 instance delivers the results in the least amount of time. However, in terms of cost, Fig. 3 leads to a different conclusion: For any number of nodes and order of accuracy tested, it is cheaper to run on the Ibex cluster.
Specifically, the simulations run with 20 physical cores cost at least
two times less than those performed on AWS EC2 instances. Except for
the third-order accurate solver on 8 nodes, running in an on-premises
of    available    physical    cores    (i.e.,    40). cluster is always advantageous, even when using the maximum number
For the on-premises cluster with the AMD Rome architecture, the performance results are shown in Fig. 4. We observe that for the second- order accurate solver, the wall-clock time oscillates and does increase drastically when 8 nodes, and 40 cores per node are used. This is again the combined effect of the problem size and the network connectivity shared with other users â€” a common scenario for a parallel cluster. On the contrary, for the third-, fourth-, and fifth-order accurate solvers, we observe that by increasing the number of nodes, the wall-clock time decreases substantially. Increasing the number of nodes of EC2 AMD EPYC (i.e., c5a instances) also leads to a reduction in time- to-solution. While the WCT decreases by around 35%-to-40% when moving from the c5a.4xlarge to the c5a.8xlarge. Thus, more on-chip cores are beneficial for this problem size. Fig. 4 also exhibits the performance of the Arm-based AWS Graviton2 processors (i.e., c6 g instances). As we can observe, the c6 g instances perform well and deliver the least wall-clock time. Additionally, the time to solution is roughly halved when the nodes are doubledâ€”indicating that a nearly perfect scaling is achieved. Furthermore, using four Arm nodes, the time-to-solution is nearly identical to that required by eight AMD EPYC nodes. Nevertheless, as shown in Fig. 4, the WCT on the Arm- based architecture c6 g.8xlarge and the Ibex cluster with eight nodes using 40 MPI processes per node are almost identical. Thus, the AMD





/ig. 5. Flow past two spheres in tandem: Cost performance in USD of AMD & Arm CPU architectures.



Rome nodes of the on-premises cluster can deliver similar performance compared to the c6 g.8xlarge instance.
In Figs. 5, we show the cost of simulating the test case with AMD and Arm architectures. Compared to the AWS ParallelCluster, the Ibex cluster offers significantly more affordable computations. While between all AWS EC2 instances, the Arm nodes are the least expensive for c6 g.4xlarge and c6 g.8xlarge. Finally, it is worth noting that the cost and performance of the EC2 c6 g instances are similar to those of the AWS EC2 c5d instances for all orders of accuracy.

Flow around delta wing
We study the flow around a 65Â° swept delta wing. We use the geometry reported by Hummel and Redeker [45] for the Second In-
radius leading edge configuration, ğ‘Ÿğ¿ğ¸ âˆ•ğ‘Ì„ = 0.0015, where ğ‘Ì„ = 0.653 m. ternational Vortex Flow Experiment. In this work, we use the medium The delta wing has a mean aerodynamic chord of ğ“ = 0.667 m, a root chord length of ğ‘ğ‘Ÿ = 1.47ğ“, and a wing span of ğ‘ = 1.37ğ“. Furthermore,
coordinate system is positioned at the delta wingâ€™s apex with the ğ‘¥1 the central region is flat, and it has no twist or camber. A Cartesian coordinate pointing downstream, the ğ‘¥2 coordinate pointing in a span- wise direction, and the ğ‘¥3 coordinate perpendicular to the flat plate. We consider the sting as part of the setup up to the position ğ‘¥1âˆ•ğ‘ğ‘Ÿ = 1.758.
The grids consist of â‰ˆ 9.209 Ã— 104 hexahedral cells. As shown in
a different solution polynomial degree, ğ‘. Given the degree of the 6(a), the grid is divided into three blocks, where each block is assigned
is â‰ˆ 1.435 Ã— 107. The simulations are carried out for an angle of attack solution and the number of cells in each block, the number of DOFs of AoA = 13Â°, a Mach number ğ‘€ğ‘ = 0.07 and a Reynolds number of
ğ‘…ğ‘’ = 106, based on the mean aerodynamic chord.
The performance of the simulation for both on-premises cluster Ibex using Intel Cascade Lake architecture and AWS ParallelCluster using EC2 c5d instances are present in Fig. 7. The performance of the on-premise cluster is degraded as the number of cores increases. This is because of the interplays of two factors: the high occupancy and non-exclusivity of the Intel Cascade nodes on the Ibex cluster, and the considerable increase of the fraction of the communication time on the overall computational time for smaller and smaller local problems (communication cannot be hidden behind computations). When the size problem becomes â€˜â€˜sufficientlyâ€™â€™ large, we observe that an increment of the number of nodes corresponds to a decrease in time- to-solution, as shown in Fig. 7(d). However, the performance degrades again for eight nodes, where we observe a growth of the time-to- solution. In contrast, for the AWS EC2 c5d instances, the simulation is speed-up as the number of nodes increases for all orders of accuracy.





/ig. 6. (a) Geometry (top) and solution polynomial degree distribution (bottom) for the 65Â° swept delta wing test case; ğ‘ = 2 in the far-field region (yellow), ğ‘ = 5 in the region surrounding the delta wing and its support (blue), and ğ‘ = 3 elsewhere (green). (b) Average flow field past the 65Â° swept delta wing: the ğ‘„-criterion colored by the normalized velocity magnitude (left) and mean axial velocity (right) at ğ‘¥1âˆ•ğ‘ğ‘Ÿ = 0.2, 0.4, 0.6, 0.8, and 0.95; the wing surface is colored using the time-averaged pressure coefficient.  (For
interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)




/ig. 7. Flow around a delta wing: Wall-clock time in seconds for each simulation against the number of nodes for Intel CPU architectures.





/ig. 8. Flow around a delta wing: Cost performance in USD of Intel CPU architectures.



In particular, a speed-up factor of about 1.8 is delivered for both AWS EC2 c5d instances.
Fig. 8 shows the cost analysis of running on Ibex and the c5d instances. The runs performed with the on-premises cluster using 40 and 20 MPI threads show better cost-efficiency than the AWS EC2 c5d instances. In fact, Ibex provides up to 70% better pricing performance than c5d instances, regardless of solver accuracy and the number of nodes.
As done for the flow past the two spheres in tandem, we also study the performance of the solver and the cost of each simulation for the AMD and Arm architectures. Fig. 9 shows the wall-clock time in seconds for the on-premise cluster with the AMD Rome architecture, the EC2 AMD c5a instances, and the EC2 Arm c6 g instances. The performance results of the Ibex and AWS clusters is mostly matching the results obtained for the previous test case, i.e., the wall-clock time decreases substantially by increasing the number of nodes. In particular, by doubling the number of nodes we observe a speed-up factor of about
1.7 for all the runs. As for the previous application, moving from the c5a.4xlarge to the c5a.8xlarge instances lead to a reduction in the wall-clock time by around 35%-to-40%. Furthermore, the smallest wall- clock time is obtained using the EC2 Arm c6 g instances, for all orders of accuracy. In addition, almost perfect scaling is observed. Doubling
the nodes by switching from c6 g.4xlarge to c6 g.8xlarge leads to halves the solution time.
A detailed cost analysis is shown in Fig. 10. Among the EC2 in- stances, the compute-optimized c6 g instances perform better than the c5a instances for all the number of nodes and order of accuracy. However, on the on-premises cluster, using the largest number of physical cores possible is always the most convenient solution.
In the next section, we further assess the performance of the solver on-premises cluster Ibex and the AWS ParallelCluster by considering two more complex industrially-relevant flow problems. Precisely, we will simulate the NASA junction flow experiment and the flow past a Formula (1) front wing. The accurate simulation of these indus- trial problems via large eddy simulation (LES) is representative of the type of simulations that exascale will allow performing in a 24- hour turnaround, a typical requirement for industry standards (see, for instance, [46]). In our context, we use these test cases to explore the influence of the problem size on the performance of both clusters.

NASA juncture flow experiment

In this section, we simulate the NASA juncture configuration. This test case has a wing based on the DLR-F6 geometry and is equipped





/ig. 9. Flow around a delta wing: Wall-clock time in seconds for each simulation against the number of nodes for AMD & Arm CPU architectures.



with a leading edge horn to reduce the effect of the horseshoe vor- tex around the wing-fuselage juncture [47]. The Reynolds and Mach
numbers based on the freestream conditions are 2.4 Ã— 106 and 0.189,
respectively. The angle of attack is âˆ’2.5â—¦. We perform simulations by
neglecting the sting and the mast. An overview of the mesh is shown
represents a different degree of approximation (ğ‘) for the solution field. in Fig. 11(a). The grid is broken up into three blocks, each of which Specifically, we utilize ğ‘ = 1 in the far-field region (dark green), ğ‘ = 3 in the region surrounding the model (dark orange), and ğ‘ = 2 in the
is â‰ˆ 6.762 Ã— 105, and the number of DOFs is â‰ˆ 4.091 Ã— 107. In Fig. 11(b), remaining part (dark yellow). The total number of hexahedral elements
instantaneous   velocity,   i.e.,        1âˆ•|     âˆ|. we plot the Q-criterion using isocontours colored by normalizing the
In Fig. 12, we show the WTC for different numbers of nodes for the Ibex cluster and the AWS EC2 instances. For all the type of nodes except the Intel Cascade Lake, doubling the number of nodes yields an efficiency of about 95%. For the Intel Cascade Lake architecture with 40 MPI threads, moving from one to four nodes leads to a rapid decrease in efficiency. Eventually, for eight nodes, the time to solution increases. The fastest time-to-solution is delivered by the Cascade Lake with 20 MPI threads per node and the c6g8xlarge AWS EC2 instance. In terms of cost, the on-premise cluster simulations done on Intel architectures are much cheaper than the AWS EC2 cd5 instances, as illustrated in Fig. 13. For the AMD-Ibex nodes and the c5a and c6 g instances, the on-premises cluster still delivers the smallest cost.
Flow past a Formula (1) front wing

Here, we consider the flow past a Formula (1) front wing [48]. We refer to this test case as the Imperial Front Wing, based on the front
by â„ the distance between the ground and the lowest part of the front wing and endplate design of the McLaren 17D race car [49]. We denote wing endplate and by ğ‘ the chord length of the main element. The
angle of 1.094â—¦. Here we use â„âˆ•ğ‘ = 0.36 which can be considered as position of the wing in the tunnel is further characterized by a pitch
higher loads on the wing. The corresponding Reynolds number is ğ‘…ğ‘’ = a relatively low front ride height, with high ground effect and hence
2.2 Ã— 105, based on the main element chord ğ‘ of 250 mm and a free stream velocity ğ‘ˆ of 25 mâˆ•s. The Mach number is set to ğ‘€ğ‘ = 0.036.
This corresponds to a practically incompressible flow.
The computational domain is divided into 3.4 Ã— 106 hexahedral ele- ments with a maximum aspect ratio of approximately 250. The solution polynomial degree is set to ğ‘ = 2 â€” a formally third-order accurate scheme. Thus, the total number of DOFs is approximately 9.18 Ã— 107.
The grid is constructed using the commercial software Pointwise V18.3 released in September 2019; solid boundaries are described using a quadratic mesh. The panel of Fig. 14(a) illustrates an overview of the front wing geometry and the mesh, where the contour plot of the time- averaged pressure coefficient on the surface of the front wing is shown in Fig. 14(b).





/ig. 10. Flow around a delta wing: Cost performance in USD of AMD & Arm CPU architectures.



The variation of the wall-clock time against the number of nodes for different types of CPU architecture is shown in Fig. 15. Overall, we observe that the simulations speed up as the number of CPUs increases for all architectures on the on-premises cluster and the AWS EC2 instances. These results confirm that the problem size influenced the performance of the on-premises cluster Ibex presented in the previous sections. Fig. 15(a) shows the wall-clock time used by the Intel Cascade Lake architecture on the Ibex cluster and the AWS EC2 c5d instances. The simulation on 8 nodes of the on-premises cluster with 40 MPI threads produced favorable results. For all four computations, doubling the number of nodes reduces the time-to-solution by a factor of ap- proximately 1.8. However, for 8 nodes Intel Cascade Lake architecture and 40 MPI threads, this factor reduces to 1.7. In Fig. 15(b), we report the performance of the Ibex cluster using AMD processes and AWS ParallelCluster using c5a and c6 g instances. As we can observe, the c6 g.8xlarge AWS EC2 instance delivers the results in the least amount of time with a parallel efficiency of approximately 70%. By comparing Figs. 15(a) and 15(b), we observe that the c6 g.8xlarge AWS EC2 instance achieve the best performance among all the CPU architectures. Although parallel performance is important, the cost of the simu- lations is a significant concern. As illustrated in Fig. 16, the cost of computations on the on-premises cluster is much lower than that of the AWS ParallelCluster. Across all the AWS EC2 instances, the c6 g.8xlarge
Arm architecture is the cheapest one.
Conclusion


In this work, we evaluate the performance of a prototype of next generation high-order entropy stable solvers for compressible flows on unstructured grids on the Amazon Web Services Elastic Compute Cloud and the on-premise resource Ibex cluster hosted at KAUST. The study aims to establish the possibility of using Amazonâ€™s cloud-based high-performance computing service to address complex computational fluid dynamics industry flow problems and propose a set of Elastic Cloud Computing instances that provide the fastest time to the solution and offer more affordable computations. In terms of time-to-solution, the Amazon Web Services Elastic Compute Cloud delivers the best performance, with the Graviton2 processors based on the Arm archi- tecture being the fastest. However, the results also indicate that the nodes based on the AMD Rome architecture of Ibex deliver very good performance, close to those observed for the Amazon Cloud service. In addition, we found that the simulations on the Ibex cluster are currently less expensive than those performed on the cloud for all orders of accuracy. Future work will demonstrate the performance at higher core counts and include post-processing elements of the computational fluid dynamics process.




/ig. 11. NASA juncture flow experiment. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)




/ig. 12. NASA juncture flow experiment: Wall-clock time in seconds for each simulation against the number of nodes.



CRediT authorship contribution statement

R. Al Jahdali: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Soft- ware, Validation, Visualization, Writing â€“ original draft, Writing â€“ review & editing. S. Kortas: Resources, Software. M. Shaikh: Re- sources, Software. L. Dalcin: Resources, Software, Review & editing.
M. Parsani: Conceptualization, Project administration, Supervision, Writing â€“ review & editing.

Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.
Data availability

Data will be made available on request.

Acknowledgments

The work described in this paper was supported by King Abdullah University of Science and Technology, Saudi Arabia through the award OSR-2019-CCF-3666. The authors are also thankful for the computing resources of the Supercomputing Laboratory and the Extreme Com- puting Research Center at King Abdullah University of Science and Technology.


	


/ig. 13. NASA juncture flow experiment: Cost performance in USD of Ibex, and AWS EC2 instances.


/ig. 14. Formula (1) front wing (Imperial Front Wing).



/ig. 15. Formula (1) front wing: Wall-clock time in seconds for each simulation against the number of nodes.


	


/ig. 16. Formula (1) front wing: Cost performance in USD of Ibex, and AWS EC2 instances.




Appendix A. An overview of the semidiscrete entropy stable spa- tial discretization

This Appendix gives some of the details of the spatial discretization implemented in the SSDC framework. As a model problem, we use the advection-diffusion equation in multiple dimensions.

Mapping

partition the physical domain ğ›º âŠ‚ R3 into ğ¾ non-overlapping elements, In general, to solve partial differential equations numerically, we where ğ¾ is a whole number greater than zero. Then, each element
in physical space is transformed using a local and invertible curvilin- ear coordinate transformation that is compatible at shared interfaces, meaning that the push-forward element-wise mappings are continuous
across physical element in(terfaces. )To achieve that, one maps from
the reference coordinates ğœ‰1, ğœ‰2, ğœ‰3  âˆˆ [âˆ’1, 1]3 to the p(hysical e)le-
me(nt (see F)ig. A.17) by the push-forward transformation ğ‘¥1, ğ‘¥2, ğ‘¥3 =

ğ‘‹ ğœ‰1, ğœ‰2, ğœ‰3 , which, in the presence of curved elements, is usually
a high-order degree polynomial. This procedure requires no explicit
/ig. A.17. Example of mapping procedure of the reference element (left) to a physical element (right).

knowledge nor construction of the pull-back mappings in unstructured mesh schemes.
(	)
ğœ‰	ğ–²ğœ‰	2 ğœ‰
, ğ–²ğœ‰ = âˆ’
( ) T
ğœ‰
, ğ–¤ğœ‰ =
(ğ–¤ ) T,

ğ–¤ = diag (âˆ’1, 0, â€¦ , 0, 1) = ğ’†
ğ’† T âˆ’ ğ’† ğ’† T, ğ’†
â‰¡ [1, 0, â€¦ , 0] T, and


Summation-by-parts operators
ğœ‰
ğ’†ğ‘
â‰¡ [0, 0, â€¦ , 1] T.
ğ‘ ğ‘
1 1	1

The physical (spatial) domain ğ›º âŠ‚ R3 with boundary ğœ•ğ›º is dis-
cretized using tensor-product elements. In this work, the derivatives in each element are discretized using one-dimensional SBP operators [50, 51] which are given for completeness in Definition 1.
Definition 1.  A matrix operator, ğ–£ğœ‰ âˆˆ Rğ‘Ã—ğ‘ , is an SBP o[pera]tor of
An SBP operator of degree ğ‘ is hence an operator that differentiates
exactly monomials up to degree ğ‘.
In this work, we use a collocated discontinuous Galerkin approach with diagonal norm SBP operators are constructed on the Legendreâ€“ Gaussâ€“Lobatto (LGL) nodes [10]. The SBP operators used in this work are explicitly constructed in [22]. These operators are extended to
multiple dimensions by using tensor products (âŠ—).

degree ğ‘ approximating the derivative  ğœ•
ğœ•ğœ‰
nodal distribution ğƒ having ğ‘ğ‘™ nodes, if
1. ğ–£ğœ‰ ğƒğ‘— = ğ‘—ğƒğ‘—âˆ’1, ğ‘— = 0, 1, â€¦ , ğ‘;
on the domain ğœ‰ âˆˆ
ğ›¼, ğ›½
with


Semidiscretization of the linear advection-diffusion equation

2. 
â‰¡ (ğ–¯ )âˆ’1 ğ–° , where the norm matrix, ğ–¯ , is symmetric positive
The linear convectionâ€“diffusion equation in Cartesian physical co-

ğœ‰	ğœ‰	ğœ‰	ğœ‰
definite;	ordinates reads



âˆ€ ğ‘¥1, ğ‘¥2, ğ‘¥3
âˆˆ ğ›º
ğ‘š=1	ğ‘š
1  3	ğœ•
ğœ…
ğœ•ğœ‰
ğœ…	ğ‘š	ğœ…	ğ‘š

(	)	(ğµ) (	)
(A.1)
ğœ•ğ‘¡
2 ğ‘™,ğ‘š=1
ğœ•ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š
ğœ•ğ‘¥ğ‘š ğœ•ğœ‰ğ‘™

ğ’’ (ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¡ = g
ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¡ ,
1 âˆ‘3  {
(	)}	âˆ‘3	(	)

âˆ€ ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğœ•ğ›º,  ğ‘¡ â‰¥ 0,
â€“	ğ‘ ğ’’ ğœ•
ğ½ ğœ•ğœ‰ğ‘™	=
ğœ•	ğ½
ğœ•ğœ‰ğ‘™
ğœ•ğœ‰ğ‘ ğœ•(ğ‘ğ‘šğ’’)
,

(	)	(	)
2	ğ‘š
ğœ•ğœ‰ğ‘™
ğœ… ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘™
ğœ… ğœ•ğ‘¥ğ‘š ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘

ğ’’ ğ‘¥1, ğ‘¥2, ğ‘¥3, 0
= g(0)
ğ‘¥1, ğ‘¥2, ğ‘¥3, 0 ,
ğ‘™,ğ‘š=1
ğ‘™,ğ‘,ğ‘š=1

âˆ€ (ğ‘¥1, ğ‘¥2, ğ‘¥3) âˆˆ ğ›º,
where (ğ‘ğ‘šğ’’) are the inviscid fluxes, ğ‘ğ‘š are the (constant) components of
(A.6)
where the last set of terms on the left-hand side are zero by the GCL

the convection speed, ğœ•(ğ‘ğ‘šğ’’) are the viscous fluxes, and ğ‘
ğœ•ğ‘¥ğ‘š
are the (con-
conditions (A.4). Then, a stable semi-discrete form can be constructed
similarly to the split form (A.6) by d[iscreti]zing the inviscid portion

stant and positive) diffusion coefficients. If the coefficients ğ‘ğ‘š are set
to zero, we will have a purely hyperbolic system. The boundary data,
of (A.2) and (A.5) using ğ–£ğœ‰ğ‘™
, ğ–©ğœ… , and
J  ğœ•ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š
, and by averaging the
ğœ…

g(ğµ), and the initial condition, g(0), are assumed to be in ğ¿2(ğ›º), with the further assumption that g(ğµ) is prescribed so that linear stability (energy
results. The viscous terms are obtained from the discretization of the viscous portion of (A.5). This process leads to

stability) is achieved. Here, derivatives are approximated with SBP
differentiation operators defined in computational space. Therefore, we use the Jacobian of the push-forward mapping and the chain rule to
ğ–©ğœ…
dğ’’ğœ… + dğ‘¡
1 âˆ‘3
2 ğ‘™,ğ‘š=1
ğ‘ğ‘š
{
ğ–£ğœ‰ğ‘™
[J  ğœ•ğœ‰ğ‘™ ]
ğœ•ğ‘¥ğ‘š ğœ…
+ [J  ğœ•ğœ‰ğ‘™ ]
ğœ•ğ‘¥ğ‘š ğœ…
}
ğ–£ğœ‰ğ‘™
ğ’’ğœ…

transform Eq. (A.1) from physical to computational space as
1 âˆ‘3  {
(  )	[
 ğœ•ğœ‰ ]	}

ğœ•ğ’’	âˆ‘3
ğœ•ğœ‰ğ‘™ ğœ•
(ğ‘ ğ’’)	âˆ‘3
ğœ•ğœ‰ğ‘™
ğœ• (
ğœ•ğœ‰ğ‘ ğœ•(ğ‘ğ‘šğ’’) )
â€“
ğ‘™,ğ‘š=1
ğ‘ğ‘š diag
ğ’’ğœ…
ğ–£ğœ‰ğ‘™
J
ğœ•ğ‘¥ğ‘š
ğŸğœ…	=
ğœ…
(A.7)

ğ½ğœ… ğœ•ğ‘¡ +	ğ½ğœ… ğœ•ğ‘¥
ğ‘™,ğ‘š
ğœ•ğœ‰	=	ğ½ğœ… ğœ•ğ‘¥  ğœ•ğœ‰
ğ‘™,ğ‘,ğ‘š
ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘
,	(A.2)
âˆ‘3
ğ‘ ğ–£
ğ–©âˆ’1 [J  ğœ•ğœ‰ğ‘™ ]
[J ğœ•ğœ‰ğ‘ ]

where ğ½ğœ… is the determinant of the metric Jacobian. Moving the metric
ğ‘š
ğ‘™,ğ‘š,ğ‘=1
ğœ‰ğ‘™ ğœ…
ğœ•ğ‘¥ğ‘š ğœ…
ğœ•ğ‘¥ğ‘š
ğ–£ğœ‰ ğ’’ğœ… + ğ’ğ€ğ“ğœ… ,
ğœ…

terms ğ½
 ğœ•ğœ‰ğ‘™
ğœ… ğœ•ğ‘¥ğ‘š
inside the derivative, and using the product rule, leads to
where ğŸğœ…
is a vector of ones of size ğ‘ğœ… . The semi-discrete form (A.7)

ğœ•ğ’’
âˆ‘3	ğœ• (
ğœ•ğœ‰ğ‘™
)	âˆ‘3
ğœ• (
ğœ•ğœ‰ )
highlights a set of discrete GCL conditions (the analog of the continuous GCL conditions (A.4))

ğ½ğœ… ğœ•ğ‘¡ +	ğœ•ğœ‰
ğ‘™,ğ‘š
ğ½ğœ… ğœ•ğ‘¥ ğ‘ğ‘šğ’’
â€“	ğ‘ğ‘šğ’’
ğ‘™,ğ‘š=1	ğ‘™
ğ½ğœ… ğœ•ğ‘¥	=
âˆ‘3	[J  ğœ•ğœ‰ğ‘™ ]

âˆ‘3	ğœ• (
ğœ•ğœ‰ğ‘™
ğœ•ğœ‰ğ‘ ğœ•(ğ‘ğ‘šğ’’) )
âˆ‘3	ğœ•ğœ‰ğ‘ ğœ•(ğ‘ğ‘šğ’’) ğœ• (
ğœ•ğœ‰ğ‘™ )
ğ‘™=1
ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š
ğŸğœ… = ğŸ,  ğ‘š = 1, 2, 3.	(A.8)
ğœ…

ğ‘™,ğ‘,ğ‘š=1
ğœ•ğœ‰ğ‘™
ğ½ğœ… ğœ•ğ‘¥
ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘
â€“
ğ‘™,ğ‘,ğ‘š=1
ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘
ğœ•ğœ‰ğ‘™
ğ½ğœ… ğœ•ğ‘¥	.
The satisfaction of conditions (A.8) will result in the telescoping, prov- ably stable, semi-discrete form

(A.3)

ğ–© dğ’’ğœ… + 1 âˆ‘
{
ğ‘	ğ–£
[J ğœ•ğœ‰ğ‘™ ]
+ [J ğœ•ğœ‰ğ‘™ ]
}
ğ–£	ğ’’ =

The last terms on the left- and right-hand sides of (A.3) are zero via the
geometric conservation law (GCL) relations
ğœ… dğ‘¡
ğ‘š
ğ‘™,ğ‘š=1
ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š ğœ…
ğœ•ğ‘¥ğ‘š ğœ…
ğœ‰ğ‘™	ğœ…
(A.9)

âˆ‘3   ğœ•
(   ğœ•ğœ‰ğ‘™
ğœ…
)
= 0,  ğ‘š = 1, 2, 3,	(A.4)
âˆ‘3
ğ‘™,ğ‘š,ğ‘=1
ğ‘ğ‘šğ–£ğœ‰ğ‘™ ğ–©ğœ…
âˆ’1 [J
 ğœ•ğœ‰ ] [J
ğœ•ğ‘¥ğ‘š ğœ…
ğœ•ğœ‰ ]
ğœ•ğ‘¥ğ‘š ğœ…
ğ–£ğœ‰ ğ’’ğœ… + ğ’ğ€ğ“ğœ… .

ğ‘™=1 ğœ•ğœ‰ğ‘™
ğœ•ğ‘¥ğ‘š
At one of the ğ‘–th interfaces of the ğœ…th element, the ğ’ğ€ğ“
ğœ…,ğ‘–
in the normal

yielding the strong conservation form of the convectionâ€“diffusion equa-	direction is defined as

tion in computational space
ğ’ğ€ğ“


= âˆ’ 1 ğ–¯âˆ’1Â£Ì‚(ğ‘ ğ’’
â€“ ğ›½ğ·ğ’’
â€“ ğŸ ğ‘”) ,	(A.10)

ğœ•ğ’’
âˆ‘3	ğœ• (
ğœ•ğœ‰ğ‘™
)	âˆ‘3
ğœ• (
ğœ•ğœ‰ğ‘™
ğœ•ğœ‰ğ‘ ğœ•(ğ‘ğ‘šğ’’) )
ğœ…,ğ‘–	2
ğ‘› ğœ…
ğœ…	ğ‘–

ğ½ğœ…
ğœ•ğ‘¡
+
ğ‘™,ğ‘š=1 ğœ•ğœ‰ğ‘™
ğ½ğœ…
ğœ•ğ‘¥ğ‘š
ğ‘ğ‘šğ’’
=
ğ‘™,ğ‘,ğ‘š=1 ğœ•ğœ‰ğ‘™
ğ½ğœ…
ğœ•ğ‘¥ğ‘š ğœ•ğ‘¥ğ‘š
ğœ•ğœ‰ğ‘
. (A.5)
where ğ–¯ is the norm-matrix of the SBP operator, and Â£Ì‚ is a matrix that extracts from the solution vector only the solution values associated to

Now, consider the following differentiation matrices for the three- dimensional case:
ğ–£ğœ‰ â‰¡ ğ–£ âŠ— I	âŠ— Iğ‘ , ğ–£ğœ‰	ğ‘ âŠ— ğ–£ğœ‰ âŠ— Iğ‘ , ğ–£ğœ‰	ğ‘ âŠ— Iğ‘ âŠ— ğ–£ğœ‰ ,
the LGL points that lie on the ğ‘–th interface. The scalar ğ‘ğ‘› is the advection velocity projected in the normal direction, while the scalar ğ›½ is defined as ğ›½ = 2|ğ‘|âˆ•Pe. The acronym Pe stands for the PÃ¨clet number. The
symbol ğŸ represents a vector of ones of size (ğ‘ + 1)ğ‘‘ğ‘–ğ‘šâˆ’1. For an interior

1	2	3	2	1
3	3	1	2
ğ‘–
interface, ğ‘” contains the information of the adjoining solution element,

where Iğ‘ is an ğ‘ğ‘™ Ã— ğ‘ğ‘™ identity matrix and ğ‘ğ‘™ represents the number
of LGL points per direction in a given element. The diagonal matrix
containing the metric Jacobian is defined as
whereas ğ‘” is constructed using boundary data for a boundary interface.

References

ğ–© â‰¡ diag (ğ½ (ğƒ(1)), â€¦ , ğ½ (ğƒ(ğ‘ğœ… ))) ,

ğœ…	ğœ…	ğœ…
[  ğœ•ğœ‰ ]
Chung B, Cebral JR. CFD for evaluation and treatment planning of aneurysms:
review of proposed clinical uses and their challenges. Ann Biomed Eng

while the diagonal matrix of the metric terms, chosen to be a discretization of
J   ğ‘™
ğœ•ğ‘¥ğ‘š
, has to be
ğœ…
2015;43(1):122â€“38.
Tang WM, Chan VS. Advances and challenges in computational plasma science.

(	ğœ•ğœ‰
ğœ•ğœ‰	)
Plasma Phys. Controlled Fusion 2005;47(2):R1â€“34.

diag
ğ½ğœ…   ğ‘™ (ğƒ(1)), â€¦ , ğ½ğœ…   ğ‘™ (ğƒ(ğ‘ğœ… ))  ,
Schneider T, Teixeira J, Bretherton CS, Brient F, Pressel KG, SchÃ¤r C,

ğœ•ğ‘¥ğ‘š
where ğ‘  â‰¡ ğ‘ ğ‘ ğ‘
ğœ•ğ‘¥ğ‘š
is the total number of LGL nodes in the ğœ…th
Siebesma AP. Climate goals and computing the future of clouds. Nature Clim Change 2017;7(1):3â€“5.

ğœ…	1  2  3
element. To construct a stable scheme, we canonically split the inviscid
terms into one half of the inviscid terms in (A.2) and one half of the inviscid terms in (A.3) (see [22]), while the viscous terms are handled in strict conservation form. At the continuous level, this process leads
Neumann P, DÃ¼ben P, Adamidis P, Bauer P, BrÃ¼ck M, Kornblueh L, Klocke D, Stevens B, Wedi N, Biercamp J. Assessing the scales in numerical weather and climate predictions: will exascale be the rescue? Phil Trans R Soc A 2019;377(2142):20180148.
Bauer P, Thorpe A, Brunet G. The quiet revolution of numerical weather prediction. Nature 2015;525(7567):47â€“55.



Richardson LF. Weather prediction by numerical process. Cambridge University Press; 2007.
Anderson JD. Basic philosophy of CFD. In: Computational fluid dynamics. Springer; 2009, p. 3â€“14.
Spalart PR, Venkatakrishnan V. On the role and challenges of CFD in the aerospace industry. Aeronaut J 2016;120(1223):209â€“32.
Slotnick J, Khodadoust A, Alonso J, Darmofal D, Gropp W, Lurie E, Mavriplis D. CFD vision 2030 study: A path to revolutionary computational aerosciences. NASA-CR-2014-218178, 2014.
Parsani M, Boukharfane R, Nolasco IR, Del Rey FernÃ¡ndez DC, Zampini S, Hadri B, Dalcin L. High-order accurate entropy-stable discontinuous collo- cated Galerkin methods with the summation-by-parts property for compressible CFD frameworks: Scalable SSDC algorithms and flow solver. J Comput Phys 2020;424:109844.
PeÃ±a-Monferrer C, Manson-Sawko R, Elisseev V. HPC-cloud native framework for concurrent simulation, analysis and visualization of CFD workflows. Future Gener Comput Syst 2021;123:14â€“23.
Ashton N, Sachs S, Foti L, Eberhardt S. Towards high-fidelity CFD on the cloud for the automotive and motorsport sectors. In: WCX SAE world congress experience. SAE International; 2020.
Turner M, Appa J, Ashton N. Performance of CPU and GPU HPC architectures for off-design aircraft simulations. In: AIAA scitech 2021 forum. 2021, p. 1â€“7.
Al Jahdali R, Dalcin L, Parsani M. On the performance of relaxation and adaptive explicit rungeâ€“kutta schemes for adaptive high-order compressible flow simulations. J Comput Phys 2022;In press.
Ranocha H, Dalcin L, Parsani M, Ketcheson DI. Optimized Runge-Kutta methods with automatic step size control for compressible computational fluid dynamics. Commun Appl Math Comput 2021;1â€“38.
Carpenter MH, Fisher TC, Nielsen EJ, Frankel S. Entropy stable spectral colloca- tion schemes for the Navierâ€“Stokes equations: Discontinuous interfaces. SIAM J Sci Comput 2014;36(5):835â€“67.
Parsani M, Carpenter MH, Fisher T, Nielsen E. Entropy stable staggered grid discontinuous spectral collocation methods of any order for the compressible Navierâ€“Stokes equations. SIAM J Sci Comput 2016;38(5):3129â€“62.
ter MH. An entropy stable â„âˆ•ğ‘ non-conforming discontinuous Galerkin method [18] Friedrich L, Winters AR, Del Rey FernÃ¡ndez DC, Gassner GJ, Parsani M, Carpen-
with the summation-by-parts property. J Sci Comput 2018;77(2):689â€“725.
stable â„âˆ•ğ‘-nonconforming discretization with the summation-by-parts property [19] FernÃ¡ndez DCDel Rey, Carpenter MH, Dalcin L, Zampini S, Parsani M. Entropy
for the compressible Euler and Navierâ€“Stokes equations. SN Partial Differ Equ Appl 2020;1(2):1â€“54.
Parsani M, Carpenter MH, Nielsen EJ. Entropy stable wall boundary conditions for the three-dimensional compressible Navierâ€“Stokes equations. J Comput Phys 2015;292:88â€“113.
Dafermos CM. Hyperbolic conservation laws in continuum physics. Berlin: Springer-Verlag; 2010.
Carpenter MH, Parsani M, Fisher TC, Nielsen EJ. Entropy stable staggered grid spectral collocation for the Burgersâ€™ and compressible Navier-Stokes equations. NASA TM-2015-218990, 2015.
SvÃ¤rd M. A convergent numerical scheme for the compressible Navierâ€“Stokes equations. SIAM J Numer Anal 2016;54(3):1484â€“506.
Gassner GJ, Parsani M. Entropy-stable ğ‘-nonconforming discretizations with [24] FernÃ¡ndez DCDel Rey, Carpenter MH, Dalcin L, Fredrich L, Winters AR,
the summation-by-parts property for the compressible Navierâ€“Stokes equations. Comput & Fluids 2020;210:104631.
Nolasco IR, Dalcin L, FernÃ¡ndez DCDel Rey, Zampini S, Parsani M. Opti- mized geometrical metrics satisfying free-stream preservation. Comput & Fluids 2020;207:104555.
Parsani M, Carpenter MH, Nielsen EJ. Entropy stable discontinuous interfaces coupling for the three-dimensional compressible Navierâ€“Stokes equations. J Comput Phys 2015;290:132â€“8.
stable â„âˆ•ğ‘ non-conforming discretization with the summation-by-parts property [27] Del Rey FernÃ¡ndez DC, Carpenter MH, Dalcin L, Zampini S, Parsani M. Entropy
for the compressible Euler and Navierâ€“Stokes equations. SN Partial Differ Equ Appl 2020;1(2):1â€“54.
Crean J, Hicken JE, Del Rey FernÃ¡ndez DC, Zingg DZ, Carpenter MH. Entropy- stable summation-by-parts discretization of the Euler equations on general curved elements. J Comput Phys 2018;356:410â€“38.
Dalcin L, Rojas D, Zampini S, Del Rey FernÃ¡ndez DC, Carpenter MH, Parsani M. Conservative and entropy stable solid wall boundary conditions for the com- pressible Navierâ€“Stokes equations: Adiabatic wall and heat entropy transfer. J Comput Phys 2019;397:108775.
SvÃ¤rd Magnus, Ã–zcan Hatice. Entropy-stable schemes for the Euler equations with far-field and wall boundary conditions. J Sci Comput 2014;58(1):61â€“89.
Mengaldo Gianmarco, De Grazia Daniele, Witherden Freddie, Farrington Antony, Vincent Peter, Sherwin Spencer, Peiro Joaquim. A guide to the implementa- tion of boundary conditions in compact high-order methods for compressible aerodynamics. In: 7th AIAA theoretical fluid mechanics conference. 2014, p. 2923.
Butcher JC. Numerical methods for ordinary differential equations. Chichester: John Wiley & Sons Ltd; 2016.
Ranocha H, Sayyari M, Dalcin L, Parsani M, Ketcheson DI. Relaxation Runge-Kutta methods: Fully-discrete explicit entropy-stable schemes for the compressible Euler and Navierâ€“Stokes equations. SIAM J Sci Comput 2020;42(2):A612â€“38.
Rogowski M, Dalcin L, Parsani M, Keyes DE. Performance analysis of relaxation rungeâ€“kutta methods. Int J High Perform Comput Appl 2022;10943420221085947.
Balay S, Abhyankar S, Adams MF, Benson S, Brown J, Brune P, Buschelman K, Constantinescu E, Dalcin L, Dener A, Eijkhout V, Gropp WD, Hapla V, Isaac T, Jolivet P, Karpeev D, Kaushik D, Knepley MG, Kong F, Kruger S, May DA, McInnes L Curfman, Mills R Tran, Mitchell L, Munson T, Roman JE, Rupp K, Sanan P, Sarich J, Smith BF, Zampini S, Zhang H, Zhang H, Zhang J. PETSc/TAO users manual. Technical report ANL-21/39 - revision 3.16, Argonne National Laboratory; 2021.
Knepley MG, Karpeev DA. Mesh algorithms for PDE with Sieve I: Mesh distribution. Sci Program 2009;17(3):215â€“30.
Abhyankar S, Brown J, Constantinescu EM, Ghosh D, Smith BF, Zhang H. PETSc/TS: A modern scalable ODE/DAE solver library. Technical report, 2018.
Burstedde C, Wilcox LC, Ghattas O. p4est: Scalable algorithms for par- allel adaptive mesh refinement on forests of octrees. SIAM J Sci Comput 2011;33(3):1103â€“33.
Isaac T, Burstedde C, Wilcox LC, Ghattas O. Recursive algorithms for distributed forests of octrees. SIAM J Sci Comput 2015;37(5):C497â€“531.
Isaac T, Knepley MG. Support for non-conformal meshes in PETScâ€™s DMPlex interface. 2015, arXiv preprint arXiv:1508.02470.
Cenaero. HiOCFD5, 5th international workshop on high-order CFD methods. 2018, URL https://how5.cenaero.be.
Kimball E, Whitaker T, Kevrekidis YG, Benziger JB. Drops, slugs, and flooding in polymer electrolyte membrane fuel cells. AIChE J 2008;54(5):1313â€“32.
Cebeci T, Kafyeke F. Aircraft icing. Annu Rev Fluid Mech 2003;35(1):11â€“21.
Madani S, Amirfazli A. Oil drop shedding from solid substrates by a shearing liquid. Colloids Surf A 2014;441:796â€“806.
Hummel D, Redeker G. A new vortex flow experiment for computer code validation. In: RTO/AVT symposium on vortex flow and high angle of attack aerodynamics, meeting proc. RTO-MP-069. 2003, p. 8â€“31.
Mengaldo Gianmarco, Moxey David, Turner Michael, Moura Rodrigo Costa, Jassim Ayad, Taylor Mark, Peiro Joaquim, Sherwin Spencer. Industry-relevant implicit large-eddy simulation of a high-performance road car via spectral/hp element methods. SIAM Rev 2021;63(4):723â€“55.
Rumsey CL, Morrison JH. Goals and status of the NASA juncture flow experiment. NATO, STO-MP-AVT-246, 2016.
Pegrum JM. Experimental study of the vortex system generated by a formula 1 front wing (Ph.D. thesis), Imperial College London; 2007.
Buscariolo FF, Hoessler J, Moxey D, Jassim A, Gouder K, Basley J, Murai Y, Assi GRS, Sherwin SJ. Spectral/hp element simulation of flow past a formula one front wing: validation against experiments. 2019.
SvÃ¤rd M, NordstrÃ¶m J. Review of summation-by-parts schemes for initial boundaryâ€“value problems. J Comput Phys 2014;268:17â€“38.
Del Rey FernÃ¡ndez DC, Hicken JE, Zingg DW. Review of summation-by-parts operators with simultaneous approximation terms for the numerical solution of partial differential equations. Comput & Fluids 2014;95:171â€“96.
