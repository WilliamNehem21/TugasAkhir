Artificial Intelligence in the Life Sciences 1 (2021) 100021

		


Research Article
Dealing with a data-limited regime: Combining transfer learning and transformer attention mechanism to increase aqueous solubility prediction performance
Magdalena Wiercioch a,b,∗, Johannes Kirchmair b
a Department of Applied Computer Science, Faculty of Physics, Astronomy and Applied Computer Science, Jagiellonian University, Łojasiewicza 11, Kraków 30-348,
Poland
b Department of Pharmaceutical Chemistry, Faculty of Life Sciences, University of Vienna, Vienna 1090, Austria


a r t i c l e	i n f o	a b s t r a c t

	

Keywords: Aqueous solubility Deep Learning Cheminformatics
Transformer model Drug discovery Regression
Aqueous solubility is a key chemical property that drives various processes in chemistry and biology. Its com- putational prediction is challenging, as evidenced by the fact that it has been a subject of considerable interest for several decades. Recent work has explored fingerprint-based, feature-based and graph-based representations with different machine learning and deep learning methodologies. In general, many traditional methods have been proposed, but they rely heavily on the quality of the rule-based, hand-crafted features. On the other hand, limitations in the quality of aqueous solubility data become a handicap when training deep models. In this study, we have developed a novel structure-aware method for the prediction of aqueous solubility by introducing a new deep network architecture and then employing a transfer learning approach. The model was proven to be competitive, obtaining an RMSE of 0.587 during both cross-validation and a test on an independent dataset. To be more precise, the method is evaluated on molecules downloaded from the Online Chemical Database and Modeling Environment (OCHEM). Beyond aqueous solubility prediction, the strategy presented in this work may be useful for modeling any kind of (chemical or biological) properties for which there is a limited amount of data available for model training.





Introduction

Significant progress has been made in the prediction of quantitative structure-property relationships (QSPRs) for small organic molecules, in particular in machine learning [1]. In the context of small-molecule drug discovery and the development of agrochemicals, cosmetics, func- tional foods, etc., aqueous solubility has been a field of active research for several decades now as it determines, among many other things, a compound’s ability to cross the biological membranes and, hence, its ca- pacity to induce desired biological effects (e.g. in the context of pharma- cology) and/or undesired biological effects (in the context of toxicology) in organisms. The development of highly eﬃcacious small-molecules that are suﬃciently soluble in water remains a challenging task because aqueous solubility and biological activity are often observed to be in an indirect proportional relationship [2].
Aqueous solubility is usually reported as log 𝑆, the logarithm of base
10 of the aqueous solubility in mol/L. However, aqueous solubility is
not an easy molecular property to measure accurately, for which rea-
son various approaches for measuring it are in existence and use. Data quality varies substantially between different data sources, and results obtained with different technologies are not always comparable. This is the reason why we can in part observe only a moderate reproducibil- ity of aqueous solubility measurements. On the whole, there are varied sources of data, and the exact information connected with aqueous sol- ubility methodology is not always given [3]. In consequence, the lack of high-quality datasets for computational aqueous solubility prediction
square errors (RMSE) ranges 0.6–1.4 log, with the average at 0.9 log [6]. is problematic [4,5]. Please note that the reported prediction root mean
Classical machine learning techniques have demonstrated remark- able performance in QSPR modeling, including aqueous solubility pre- diction. Generally, classical machine learning approaches can be split into two stages. The first stage aims to encode the input data and extract the most important features or properties connected with the molecule. In consequence, one gets a molecular representation. The second stage usually applies an algorithm that takes a calculated molecular represen- tation as an input and returns some response. In addition, while building


∗ Corresponding author at: Department of Applied Computer Science, Faculty of Physics, Astronomy and Applied Computer Science, Jagiellonian University, Łojasiewicza 11, 30-348 Kraków, Poland.
E-mail addresses: magdalena.wiercioch@uj.edu.pl, mgkwiercioch@gmail.com (M. Wiercioch), johannes.kirchmair@univie.ac.at (J. Kirchmair).

https://doi.org/10.1016/j.ailsci.2021.100021
Received 4 October 2021; Received in revised form 20 November 2021; Accepted 23 November 2021
Available online 26 November 2021
2667-3185/© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)



machine learning models, one uses the train-test split as a method for evaluating the performance of a machine learning algorithm.
To address the problem of aqueous solubility prediction, several methods have been proposed [7,8]. The first technique appeared in 1924 in work by Fühner [9]. Fühner observed that the addition of successive methylene groups causes a decrease of aqueous solubility. Subsequently, scientists discovered that various molecular features affect aqueous solu- bility [10–12]. Over the years, different models were applied. Erickson used linear regression to investigate the aqueous solubility of homol-
ogous series of organic molecules [13]. He observed that log 𝑆 was a
Table 1
Important notations used in this study.

Notation	Description
 = (F, £)	an undirected graph
F	set of nodes in a graph 
£	set of edges in a graph 
𝑁̃𝑖	neighborhood set of vertex 𝑣𝑖 ∈ F
𝑎𝑖	attributes for vertex 𝑣𝑖 ∈ F
𝑒𝑖,𝑗	attributes for edge (𝑣𝑖 , 𝑣𝑗 ) ∈ £
𝑑𝑔	input dimensionality of vertices

negative rectilinear function of alkyl chain length of homologous series.
̃
𝑔
𝑒̃
output dimensionality of vertices dimensionality of edges

Later, Hewitt et al. analyzed multilinear regressions performance to pre-
dict aqueous solubility [14]. Their conclusions were surprising since a simple linear regression approach proved to be superior over more com-
tical fit of the training and validation data was reasonably good (𝑅2 val- plex modeling methods at that time. The authors noticed that the statis-
ues of 0.74 and 0.67, respectively). Finally, the root mean square error (RMSE) value for the test set of 0.95 was obtained. With Random Forest, Palmer et al. studied the prediction of aqueous solubility for a data set of compounds that are solid at room temperature [15]. Their method has been shown to perform comparably to other methods, including some that require 3D structure calculation. To specify, the authors predicted the log molar solubility values for the molecules in the test set with RMSE of 0.69. Also, Lind and Maltseva have demonstrated that Sup- port Vector Machines with a Tanimoto similarity kernel detects aque- ous solubility with accuracy, expressed in terms of root mean square error, produce comparable results than other reported approaches [16].
in a RMSE of 0.62 and 𝑅2 of 0.88. Besides, artificial neural networks They reported that the cross-validation on a diverse data set resulted
have been utilized to predict aqueous solubility [17]. The goal was to take advantage of their ability to approximate non-linear functions. For instance, Erić et al. proposed a methodology for the automatic adjust- ment of descriptor’s relative importance in counter-propagation shallow neural networks in order to predict aqueous solubility [18]. The perfor- mance of their final model based on seven descriptors for prediction of aqueous solubility was satisfactory since the authors reported an RMSE of 0.679 on test dataset.
Very recently, deep neural networks (DNNs) [19–21] have yielded impressive performance in molecular and other materials property pre- diction [22–25]. While the classical machine learning approaches re- quire hand-crafted molecular descriptors as inputs, DNNs can employ more lossless formats and train models in an end-to-end fashion in order to predict the target endpoints. Besides, there are many different formats of input representation [26]. The standard type is a topological graph that describes the connectivity of the atoms. Another example is SMILES strings (Simplified Molecular Input Line Entry System). Broadly speak- ing, SMILES is a text-based format where chemical species are mapped to single ASCII strings [27].
Deep learning systems have improved the state-of-the-art in aque- ous solubility prediction. For instance, Lusci et al. proposed an archi- tecture that uses a recurrent neural network and molecular structures converted into directed, acyclic graphs. Their methodology, called UG- RNN, sometimes outperformed current then state-of-the-art techniques [28]. For instance, UG-RNN achieves RMSE of 0.96 on the intrinsic sol- ubility data set. Another example is the work of Wu et al., where the authors constructed a topology-based multi-task deep learning strategy (MT-DNN) and achieved some of the most accurate predictions of aque- ous solubility (RMSE of 0.649) and the partition coeﬃcient [29]. Also, Liu et al. presented a competitive approach (Chemi-Net) based on convo- lutional neural networks to predict aqueous solubility and other ADME
itive models and achieves an 𝑅2 of 0.585. Finally, in 2020 Tang et al. properties [30]. They demonstrated that Chemi-Net beats the compet-
[31] proposed a self-attention-based message passing neural network to identify the relationship between molecular solubility and structure. Their method obtained an RMSE of 0.661 (for aqueous solubility pre- diction) on a small collection of compounds.
𝐹	number of layers
𝑡	state vector of node 𝑣𝑖 ∈ F at layer 𝑡; ℎ𝑡 ∈ ℝ𝑑𝑔
𝛼𝑖,𝑗	attention coeﬃcient for edge (𝑣𝑖 , 𝑣𝑗 ) ∈ £





Although the performance of computational models for the predic- tion of aqueous solubility has greatly improved by employing deep- learning architectures, there remains room for improvement. First, among the deep learning approaches, graph neural network (GNN)- based methods attract significant attention because of their ability to model interactions between atoms. The idea is to treat a molecule as a molecular graph where atoms are associated with nodes. However, sim- ple operations such as summation and average may not capture various characteristics. Thus, more studies are required to analyze the contri- butions of different parts of the compound to make decisions. Second, a great majority of deep learning aqueous solubility prediction models that can be found in the literature are rather shallow networks hav- ing around seven layers [32]. Obviously, the performance could be en- hanced by going wider or deeper [33,34]. Unfortunately, such modifica- tions are constrained by the limited number of molecules with reliable aqueous solubility information.
To address the above-mentioned problems, in this study, we propose a deep learning architecture that employs both a text representation of molecules and a graph-based strategy with an attention mechanism to learn and predict aqueous solubility. The core contributions of this work are as follows. (1) We treat aqueous solubility prediction as a transla- tion problem. Our architecture represents an encoder-decoder design. However, in order to learn a latent representation, our main encoder consists of two subencoders, i.e., a graph encoder and an encoder that employs a Transformer. We call this architecture M2M. (2) To address the problem of the availability of limited amounts of high-quality data and to increase the aqueous solubility prediction performance, transfer learning is incorporated. Therefore, we first pretrain the model on pKa dataset that consists of more than 6000 chemical compounds. Then, the learned knowledge is transferred to be used on a smaller water solubility dataset. The final architecture is called TunedM2M. (3) We demonstrate that the proposed method outperforms the state-of-the-art approaches in aqueous solubility prediction. Since we found the most recent and rele- vant work connected with aqueous solubility prediction was conducted by Tang et al., our model performance is evaluated against the dataset used in their paper.


Materials and methods

In this section, we describe the methodology presented in the work, the datasets, and the algorithms employed. Table 1 summarizes all no- tations used throughout this paper.


Model design

Fig. 1 provide schematic diagram of the workflow of the proposed M2M and TunedM2M that utilizes the pre-trained M2M.




Fig. 1. Architecture of M2M and TunedM2M.


Datasets characteristics

Firstly, a pKa dataset was applied to pretrain our M2M model. Then,
log 𝑆 values related to aqueous solubility [35] were used to M2M trans-
fer learning that we call TunedM2M.
Dataset for developing the pretrained M2M
We use dataset that contains pKa data for 7911 chemical compounds, and apply it to pretrain M2M. The data come as SMILES. One may obtain the chemicals from the DataWarrior [36] application folder. According to the analysis performed using Toxprint chemotypes [37], the chem- ical structures have a high diversity of functional groups. Therefore, they are suﬃcient to support our research, i.e. pretrain M2M. For the purpose of this study, preprocessing of the chemical structures was per- formed. More specifically, in the first step, the minor components of any multi-component compounds were stripped. Second, duplicated struc- tures were removed from the set. Also, we excluded inorganic chemicals and mixtures. The final, processed pKa dataset consists of 6245 chemi- cals with measured pKa values. Fig. 3 illustrates the similarity map for pKa dataset. The measured values are highly diverse (Fig. 3).
Dataset for TunedM2M
The dataset we employ in this paper for M2M transfer learning was obtained from OCHEM [35]. It consists of 1311 molecules, which were
randomly assigned to a training set (80% of all molecules), a valida- tion set (10%), and test set (10%), following the procedure described in Tang’s paper. In addition, in order to ensure that there was no overlap between the data used for pretraining and transfer learning, we checked for matches in both datasets. The corresponding chemical compounds were removed. In addition, Fig. 4 depicts the similarity map for solubil- ity dataset. As one may note, the solubility properties are diverse among the molecules. Also, in order to explore the chemical space, we apply PCA to project the final representation obtained from TunedM2M into a three dimensional space. In addition, the Euclidean distances to the cen- troid were calculated. The result is shown in Fig. 5. The red star is the centroid of the space. Chemical compounds that are in the 75 percentiles closest to the centroid are colored in blue, whereas those further apart are colored in brown. The analysis indicate that the number of outliers is relatively small.

Graph neural networks

Graph neural networks (GNNs) were introduced by Scarselli et al.
[38] in 2009. According to the concept, a graph  = (F, £) is defined by its vertices F and edges £ ⊆ F × F. Moreover, each vertex 𝑣𝑖 ∈ F is nat- urally associated with a vector of attributes 𝑎𝑖 , and each edge (𝑣𝑖 , 𝑣𝑗 ) ∈ £



Table 2
The atom (graph node) attributes used for molecular representation.

Attributes	Explanation

atom type	C, O, H, N, O, P, … : one-hot encoding hybridization	sp, sp2, sp3, sp3d : one-hot encoding
chirality	R (Rectus), S (Sinister), N (None) : one-hot encoding radical electrons	0 (no), 1 (yes)
aromatic	0 (no), 1 (yes)
implicit valence	0 (no), 1 (yes)
acceptor	0 (no), 1 (yes)
donor	0 (no), 1 (yes)



Table 3
The bond (graph edge) attributes used for molecular representation.

Attributes	Explanation

type	single, double, triple, aromatic : one-hot encoding
Molecular transformer

Transformer [41] architecture was originally proposed for various sequence-to-sequence tasks, including machine translation, and lan- guage understanding [42,43]. In fact, the huge success of transformer- based models is attributed to the multihead self-attention component that enables the network to capture contextual information from the entire sequence. A multihead attention layer includes several scaled dot attention layers that run in parallel and are concatenated at the end.
In case of the 𝑖th attention head and for a matrix 𝐻 ∈ ℝ𝑙×𝑑 , the self-
attention maps 𝐻 into three matrices: the query matrix 𝑄 = 𝐻𝑊𝑄, the key matrix 𝐾 = 𝐻𝑊𝐾 and the value matrix 𝑉 = 𝐻𝑊𝑉 , where 𝑙 is the length of the input sequence, 𝑑 is the dimension of the input sequence,
𝑊𝐾 , 𝑊𝐾 , 𝑊𝑉 are learnable parameters. Then, the attention scores can
be expressed as:
𝑄𝐾𝑇

part of ring	0 (no), 1 (yes) part of conjugation	0 (no), 1 (yes)
chirality	E, Z : one-hot encoding
𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄, 𝐾, 𝑉 ) = 𝑠𝑜𝑓 𝑡𝑚𝑎𝑥(
𝑑
Molecular embedding
)𝑉 .	(2)





is naturally associated with a vector of attributes 𝑒𝑖,𝑗 . There is also de- fined a neighborhood function 𝑁̃𝑖 = {𝑣𝑗 ∶ (𝑣𝑗 , 𝑣𝑖) ∈ £} that assigns a set of neighbors 𝑁̃𝑖 to each vertex 𝑣𝑖 ∈ F. The primarily goal of GNNs is to learn a state vector ℎ𝑖 that is associated with each vertex 𝑣𝑖 ∈ F. In the beginning the vector is initialized as ℎ0 = 𝑎𝑖. The state of all the vertices
is then updated until the stopping criterion is reached. The update pro-
its neighbors by sending and receiving messages. Therefore, the state ℎ𝑡 cedure is based on the assumption that each vertex communicates with of vertex 𝑣𝑖 at layer 𝑡 depends on its state at the previous layer, ℎ𝑡−1, and all messages which came from neighbors 𝑣𝑗 ∈ 𝑁̃𝑖. In general, it can be
defined as follows:
ℎ𝑡 = 𝑈𝑝(ℎ𝑡−1, 𝐴𝑔𝑔({𝑀 (𝑣𝑖, 𝑣𝑗 , 𝑡) ∶ 𝑣𝑗 ∈ 𝑁̃𝑖})),	(1)
where 𝑈𝑝 is a function that updates the state, 𝐴𝑔𝑔 denotes a neighbor- hood aggregation function, and 𝑀 is a message function.


Graph attention mechanism

Attention mechanism in deep learning was first proposed in 2015 by Bahdanau et al. [39] to address the common problem in natural lan- guage processing, i.e., translation of long sequences. In fact, attention allows to assigning a learnable weight to pairs of elements such as nodes in a graph to focus on the most relevant parts of the graph. More for- mally, attention is defined as a function
Mikolov et al. [44] introduced Word2Vec to learn word embeddings in the natural language processing field. The idea behind this method follows the assumption that words that appear frequently in similar con- texts share a semantic relationship. The method achieves an embedding for each word in the training corpus by training a fully-connected shal- low neural network to predict either the target word given the context (the CBOW model) or the context given a target word (the Skip-Gram model). It shows that Word2Vec has inspired much research in various domains such as bioinformatics [45]. It was also employed for the rep- resentation of chemical compounds [46].
M2M and TunedM2M architecture

According to the notion of transfer learning, we divide our approach into two parts: a source task and a target task. In our work, the source task is how to represent the molecules and then predict pKa. However,
the main target task is to predict aqueous solubility expressed as log 𝑆.
In the following, we describe the proposed methodology in detail.

M2M
In order to accomplish the source and target tasks, we design an architecture that we call M2M. The model aims to learn feature repre- sentation using molecules as described in Subsection Dataset for devel- oping the pretrained M2M, and then predict pKa. Fig. 2 illustrates the proposed model. As visualized in this figure, our architecture consists of three main components: the encoder, the embedding layer, and the pre- diction layer. With respect to representation learning, the core part of

𝑔 ∶ 𝑣𝑖 × 𝑁̃𝑖
→ [0, 1]
the model is the M2M encoder, which includes the embedding block and two (sub)encoders, namely graph (sub)encoder and transformer

that for a given vertex 𝑣𝑖 ∈ F projects each vertex in 𝑁̃𝑖 to a relevance
score that informs how much attention is given to a particular neigh-
it means that the function 𝐴𝑔𝑔 in Eq. (1) employs an attention mecha- bor vertex. If one considers an attention-based graph neural network, nism. In this case, we add an extra attention coeﬃcient 𝛼𝑖,𝑗 to modify the weight of ℎ𝑡 , where 𝑣𝑗 ∈ 𝑁̃𝑖.

Initial featurization

Tables 2 and 3, respectively. The M2M’s initial attributes 𝑎𝑖 are the atom The initial atom and bond features used in our study are shown in features for the vertex 𝑣𝑖 ∈ F, while the M2M’s initial edge features 𝑒𝑖,𝑗 are the bond features for an edge (𝑣𝑖, 𝑣𝑗 ) ∈ £. In order to extract these
features, RDKit [40] was used. Finally, the initial representation of the
atom is a 130-dimensional vector, and the input representation of the edge is a 8-dimensional vector.
(sub)encoder. Each of these elements has an inevitable impact on the final form of the M2M encoder and involves several operations that can be summarized as follows.
Graph (sub)encoder aims to generate a low-dimensional, hidden representation for atoms in a given chemical compound. Firstly, in order to construct a graph (sub)encoder, we assume that a molecule is repre- sented as a graph, as discussed in Subsection Graph neural networks. In this setting, vertices are associated with atoms, and edges correspond to bonds. Additionally, we propose a set of features to provide initial molecular attributes, as explained in Subsection Initial featurization. To build the graph, we employ the deep graph library (DGL) [47]. Our graph (sub)encoder includes a stack of the single (sub)encoder layers. In addition, each layer aggregates the attribute information from the neighboring vertices of a target vertex. Fig. 2a illustrates a single layer of our graph (sub)encoder. Formally, it is defined as follows:
ℎ𝑡+1 = 𝜎( ∑ 𝛼𝑖𝑘𝑊 ℎ𝑡 ),	(3)
𝑘∈𝑁̃ 𝑡

Fig. 2. M2M network architecture.














































where 𝑊 ∈ ℝ𝑑̃ ×𝑑𝑔 is a weight matrix. Furthermore, the vital component
of each single layer is an attention coeﬃcient that is associated with an
attention mechanism introduced in Subsection Graph attention mecha- nism. More specifically, during the process, features are concatenated
2𝑑̃ +𝑒̃
different representations learned by different heads in the hidden layer. Then, they are averaged on the final layer of our graph (sub)encoder as follows:
𝑀

and parameterized by a weight vector 𝑎⃖⃗ ∈ ℝ 𝑔
. Also, nonlinearity is
ℎ𝑡+1
= 𝜎
(  1
∑ ∑ 𝛼𝑚 𝑊 𝑚ℎ𝑡 ),	(5)

provided by LeakyReLU function. It is interesting to note that we per-	𝑖
form normalization using a softmax function denoted by 𝜎. All these
operations can be expressed as:
𝑚=1 𝑘∈𝑁̃ 𝑡
𝑖𝑘	𝑘

exp(𝜎(𝑎⃖⃗𝑇 [𝑊 ℎ⃖⃖⃗||𝑊 ℎ⃖⃖⃖⃗||𝑒
]))
where 𝑀 is the number of heads involved in the multi-head attention
mechanism.

𝛼𝑖𝑗 =
𝑘∈𝑁̃ 𝑖
exp(𝜎(𝑎⃖⃗𝑇 [𝑊 ℎ⃖⃖⃗||
𝑊 ℎ⃖⃖⃖⃗||𝑒
.	(4)
]))
Embedding block is designed to obtain a vector representation of a molecule based on SMILES. Therefore, in order to transform the

What is more, to capture multiple types of relationships between vertices, multihead attention is used. As a result, we concatenate the
molecule SMILES to vector representation, we train a Word2vec model. Specifically, we follow a Skip-gram configuration. Our goal is to maxi-




Fig. 3. Similarity map for pKa dataset.


Fig. 4. Similarity map for solubility dataset.


mize the average log probability
Transformer (sub)encoder takes as input representations 𝑟̃′′ (see

1 ∑	∑	log 𝑃 (𝑠
𝑁 𝑖=1 −𝑐≤𝑚≤𝑐,𝑚≠0
𝑖+𝑚|𝑠𝑖 ),	(6)
Fig. 2b)
𝑟̃′′ = 𝐾 𝑟̃′

where 𝑁 is the size of training set, 𝑠1 , 𝑠2 , … , 𝑠𝑁 are the training SMILES
𝑟̃′ = 𝐾1 𝑟̃

and 𝑐 is the size of the training context. Moreover, 𝑃 (𝑠
lated using the softmax function
𝑖+𝑚
𝑖
|𝑠𝑖 is calcu-

𝑟̃ = [𝑠′ ||ℎ𝐹 ]

𝑃 (𝑠
|𝑠 ) =
exp((𝑢̃
𝑖+𝑚
)𝑇 𝑢𝑖 )
,
𝑖	𝑖	𝑖

𝑖+𝑚  𝑖
∑𝑆
exp((𝑢̃ )𝑇 𝑢 )
where 𝑠′ is a vector representation of SMILES returned by the embedding

𝑘=1
𝑘	𝑖
𝑖
block, 𝐾1 and 𝐾2 are weight matrices to perform linear transformations

where 𝑆 is the number of SMILES in the vocabulary, 𝑈 , 𝑈̃ are the input
on 𝑟̃ and 𝑟̃′ respectively. Then, we concatenate the outputs of 𝑘 attention

𝑠	𝑠
𝑖	𝑖

and output vector representations of 𝑠.	heads and get the output that is considered as a linear projection of the






Fig. 5. PCA scores plot for the solubility data. The representation returned by TunedM2M is projected onto three axes.


concatenated representations as follows:
𝑀𝑢𝑙𝑡𝑖ℎ𝑒𝑎𝑑(𝑄, 𝐾, 𝑉 ) = 𝐶𝑜𝑛𝑐𝑎𝑡(ℎ𝑒𝑎𝑑1, … , ℎ𝑒𝑎𝑑𝑘)𝑊𝑜,	(7)
where  𝑊𝑜  is  an  output  projection  matrix,  and  ℎ𝑒𝑎𝑑𝑖 =
𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑖, 𝐾𝑖, 𝑉𝑖), as Eq. (2) indicates (see Fig. 2c). Finally, a
position-wise feed-forward neural network is applied to the output of
the network that can be expressed as follows:
𝐹 𝐹 𝑁 (𝑜𝑢𝑡) = max(0, 𝑜𝑢𝑡𝑊1 + 𝑏1)𝑊2 + 𝑏2,	(8)
where 𝑊1, 𝑊2, 𝑏1, 𝑏2 are kernel and bias parameters.

TunedM2M
pressed as log 𝑆, we opted to utilize the learned knowledge from a pre- In terms of the main target task of predicting aqueous solubility ex-
trained network (M2M) and apply the trained parameters to a new re- gression task in a process called transfer learning. Therefore, the idea is
to transfer the weights calculated for 𝑝𝐾𝑎 prediction. Here, we make use
of the encoder and the embedding layer from M2M. Then, fine-tuning
of the parameters is employed to accommodate aqueous solubility pre- diction. The resulting architecture is called TunedM2M.


Evaluation details

Root mean square error (RMSE), mean absolute error (MAE), and mean square error (MSE) are used to measure the prediction perfor- mance. Additionally, in some cases, to provide more reliable results, Pearson correlation coeﬃcient (PCC) and coeﬃcient of determination
(𝑅2) are given as well. PCC is used to measure the linear association
between two variables. In turn, (𝑅2) is seen as the proportion of the
variance in the dependent variable that is predictable from the inde-
pendent variables. Moreover, throughout this paper, unless otherwise stated, each dataset was randomly split into a training, validation, and test set (80%, 10%, and 10%, respectively).
Table 4
Model hyperparameters.

Hyperparameter	Values considered
no. of layers for the transformer	4–8
no. of multi-heads	6–12
dropout rate	0.00.6
initial learning rate	0.00015, 0.0015, 0.015, 0.15
𝛼	0.25
dimension of the final representation	128, 256, 512





Results and discussion

To evaluate the performance of our method, we compared our ap- proaches with the state-of-the-art models that have demonstrated su- perior performance in Tang’s work on aqueous solubility prediction task. Specifically, random forests (RF), message passing network (MPN), self-attention-based message-passing neural network (SAMPN), multi message passing network (multiMPN), and multi self-attention-based message-passing neural network (multiSAMPN) are used. RF classifier
[48] is one of the most popular and successful classification/regression methods in Euclidean spaces. It employs multiple decision trees to train and predict samples. MPN [49] is a deep neural network-based approach that operates on a graph and separates the prediction process into two phases: message passing phase and readout. SAMPN [31] can be seen as an extended version of MPN with an extra added attention mecha- nism. Tang et al. also introduce multiMPN and multiSAMPN that are variants of MPN and SAMPN, respectively, where the models learn both the relationship between chemical structures and properties and the re- lationship between intrinsic attributes of molecules. In addition, since our goal is to obtain a model that performs robustly, the same set of hy- perparameters for fine-tuning was determined (see Table 4). We design tasks to answer a few questions.
Q1: Do M2M and TunedM2M improve the aqueous solubility predic- tion results?
One may notice that either M2M or TunedM2M outperformed all competitive methods (see Figs. 6, 7 and Table S1 in the Supplemen- tary Materials). TunedM2M achieves an overall RMSE of 0.587, MAE of 0.449, and MSE of 0.403, which indicates a high degree of accuracy. Crucially, although M2M is trained only on aqueous solubility dataset that comprises a small number of samples, it still achieves a reasonable performance. As shown in Fig. 6a, multiSAMPN achieves an RMSE of 0.661, MAE of 0.482, and MSE of 0.424. Compared to those scores, M2M obtains better prediction performance since it obtains an RMSE of 0.646, MAE of 0.456, and MSE of 0.411. Furthermore, the experiments indicate that selection of evaluation measure doesn’t matter here since we arrive at the same conclusion. As summarized in Figs. 6b and 7, M2M leads to
an overall 𝑃 𝐶𝐶 = 0.94 and 𝑅2 = 0.88, while TunedM2M achieves 𝑃 𝐶𝐶
= 0.97 and 𝑅2 = 0.92. All in all, one may observe that the M2M archi-
tecture has a positive impact on prediction performance since M2M is
trained only on the aqueous solubility dataset and achieves the second- best performance among all models. However, these results also clearly indicate that the pretraining phase of M2M as performed in TunedM2M improves performance even more. Undoubtedly, TunedM2M produces fairly bad results for only a few molecules (see Fig. 7b). These molecules are diverse in structure, i.e., that they cannot be assigned to one or a few chemical classes. In contrast, the multiSAMPN produces many more bad
too low log 𝑆 values for a range of different compounds. Also, it produces predictions than our model (see Fig. 7a). More specifically, it produces too high log 𝑆 values for some polyhalogenated compounds and some
polycyclic aromatic hydrocarbon. Additionally, note that we evaluate statistical significance using a one-sided Wilcoxon signed-rank test and
define the statistical significance as 𝑝-value less than 0.05. All in all,
both M2M trained only on aqueous solubility dataset and TunedM2M
significantly outperform the other models.




Fig. 6. The scores of various methods on regression task and test set. We achieved the best results.


Fig. 7. Scatter plot for measured and computational model output values of aqueous solubility in two different approaches: (a) multiSAMPN, (b) TunedM2M.


Q2: Is there any noticeable difference between M2M when learned only on the aqueous solubility dataset, and TunedM2M?
The computational tasks run to answer Question no. 1 suggest that TunedM2M works better than M2M trained on the aqueous solubility dataset. Nevertheless, to answer Q2, we conducted additional analyses:
TunedM2M was again compared to M2M that is trained from scratch only on the aqueous solubility dataset. However, this time, both M2M and TunedM2M were trained on different numbers of training data and tested on the test set with the ratio of train:valid:test at 8:1:1 and random split. As Fig. 8 shows, with different numbers of train-






Fig. 8. Performance of M2M and TunedM2M on the different size of the training set.


ing data, our TunedM2M outperformed in all cases our M2M trained from scratch on aqueous solubility dataset. This suggests that the trans- fer learning strategy that is applied in the TunedM2M architecture pro- vides a robust improvement of the model. In other words, the outcomes
Table 5
Evaluations of prediction for 32 molecules from the 2019 Solubility Challenge [6].

Method	RMSE
RF	1.62
MPN	3.14
SAMPN	3.35
multiMPN	3.24
multiSAMPN	3.48
TunedM2M	3.19
mean that the training of pKa data benefits the prediction of aqueous solubility.
Q3: What is the chemical explainability of TunedM2M?
We propose a set of tasks to check how each atom contributes to the predictive performance gain. Fig. 9 shows the impact of atoms at the example of three selected chemical compounds. In general, TunedM2M




Fig. 9. Visualization of atoms and their calculated impact on aqueous solubility. Please note that TunedM2M accurately assigns negative scores to the hydrophobic atoms and positive importance scores to the atoms that help water dissolve the substance.




Fig. 10. Heat maps of the atom similarity matrix for the arbitrary selected molecule.

Fig. 11. 2D plot of the 𝐿2-norm distances between chemical compounds in the latent space associated with log 𝑆. The scale bar on the right side is associated with the distance between two molecules labeled by column and row.


accurately assigned positive importance scores to atoms that contribute to aqueous solubility and negative scores to atoms that are hydrophobic. Besides, it can be observed that the atoms that have positive importance scores form a groups that can be associated with some functional groups that are hydrophilic, such as amines, ketones, aldehydes or ethers.
Q4: What is the interpretability of TunedM2M?
In order to get more insight into TunedM2M and to gain a better un- derstanding of its performance, we selected an exemplary molecule pre- sented in Fig. 9c and analyzed how the atom state vectors change during the learning process. Fig. 10 shows heat maps of the atom similarity ma- trix for the selected chemical compound. Indeed, one may notice that the pattern after training is different from the pattern observed before training. Fig. 10a demonstrates that the heat map of the similarity matrix reveals similar levels of randomness across different layers. In contrast, after training, we obtain three clusters of atoms that are associated with functional groups: an oxime group (atoms no. 0, 1, 2), an arene group
(atoms no. 3, 4, 5, 6, 7) and a nitro group (atoms no. 8, 9, 10). In fact, this observation is strongly correlated with the chemical interpretation of the given structure. Therefore, one may infer that TunedM2M is able to learn a representation related to molecular aqueous solubility.
Q5: Do the models capture meaningful patterns in the distance map- ping?
1000 molecules from the dataset and calculated the 𝐿2-norm distance We also perform our analysis in latent space. We randomly chose
between them. The normalized distance mapping is shown in Fig. 11. The scale bar on the right side refers to the distance between two molecules labeled by row and column. Furthermore, the diagonal el- ements colored in blue show the distance of a molecule from itself. In turn, the red parts mean the farthest distance. It seems that both
tance mapping. It clearly indicates that for similar log 𝑆 values the mod- multiSAMPN and TunedM2M reveal meaningful patterns in the dis-
els are able to locate similar molecules in the latent space. Neverthe-




Fig. 12. Selection of chemical compounds and predicted values.





Fig. 13. Prediction error (measured as RMSE) versus the average similarity of the molecule to the rest of the training set.


less, TunedM2M puts chemical compounds that have similar proper- ties closer to each other in space, and that explains the performance of TunedM2M (Figs. 12 and 13).
Q6: Does the usage of (sub)encoders affect the final performance of M2M?
To investigate the contribution of different factors that influence the performance of M2M, we conducted ablation studies. As shown
in Fig. 14, decoupling knowledge-aware components such as a graph (sub)encoder and an embedding block in M2M encoder from our learn- ing framework yields a significant drop of performance. Moreover, it seems that a graph (sub)encoder is crucial and effective for better re- sults. In addition, the lack of transformer causes the worst performance.
Q7: Does scaffold-based splitting of the data affect the performance of M2M and TunedM2M?
We also evaluate the models on scaffold-based split. Scaffold split- ting is performed using Bemis-Murcko [50] methodology that divides the dataset into a training set, a validation set, and a test set according to their two-dimensional molecular structures. As the Fig. 15 shows, both M2M and TunedM2M beat all the competitive approaches. To be more precise, TunedM2M reaches an RMSE of 0.672, MAE of 0.501, and MSE of 0.438. The second-best approach is M2M trained only on aqueous sol-
ubility dataset (RMSE = 0.721, MAE of 0.564 MSE of 0.471), while the
third-best is multiSAMPN that obtains an RMSE of 0.735, MAE of 0.571,
and MSE of 0.48. In fact, the results indicate that scaffold splitting is a more challenging setting since it yields less impressive scores than the random split approach. Nevertheless, the outcomes indicate that even when the training and the test set have different characteristics related to the molecular structures, the TunedM2M is able to learn the general representation of the molecules and returns a competitive prediction.


Fig. 14. Ablation studies. After removing a graph (sub)encoder, an embedding block and a transformer in M2M encoder from our learning framework, a significant drop of performance is observed.






Fig. 15. Scaffold split. The scores of various methods on regression task and test set. We achieved the best results.


Q8: How does the model work for a dataset of intrinsic water solu- bility?
As part of an additional task, we apply the proposed model to the molecules provided by the organisers of the 2019 Solubility Challenge [6]. It is somewhat not surprising that RF obtains the best results, i.e., RMSE of 1.62 (see Table 5). It shows that for an extremely small dataset, including 100 molecules in the training set, deep learning-based ap- proaches reveal very high error on a test set (32 molecules). Although TunedM2M’s performance is not satisfactory, it achieves the third best result (RMSE of 3.19). We hope this observation will help to explore fur- ther the challenges connected with prediction of the intrinsic solubilities of molecules for small datasets.

Conclusions

To increase the accuracy of aqueous solubility prediction and over- come the diﬃculties of training deep neural networks caused by limited reliable aqueous solubility-related datasets, in this work, we introduced a transformer-based architecture. Moreover, we employed a transfer learning strategy and called the final architecture as TunedM2M. This paper presents the workflow and performance of our method. In fact, thanks to the well-designed encoder, TunedM2M provides insights on which fragments of the chemical structure have the greatest influence on the property of interest. Furthermore, several proposed tasks have shown our approach is competitive with the state-of-the-art models. The results also demonstrated that the accuracy of the aqueous solubility prediction was significantly improved. Therefore, the outcomes reveals that transfer learning is an effective way to solve the data-hungry prob- lem and may be beneficial for improving the regression performance of other models.

Data and software availability

Our model was implemented using Python 3.7 and Pytorch 1.2.0. The files used in this study are available on Github (https://github.com/ magdalenawi/TunedM2M).

Author contributions
M.W. contributed the concept, implementation and wrote the manuscript. M.W and J.K. contributed to the interpretation of results. All authors reviewed and approved the final manuscript.

Funding

This work is a part of broader research funded under the Iwanowska Program of the Polish National Agency for Academic Exchange (NAWA)
- a decision no. PPN/IWA/2018/1/00094/DEC/1. This research was also supported by the National Science Centre (Poland) Grants No. 2016/21/N/ST6/01019. This publication has been partially funded from the SciMat Priority Research Area budget under the program “Ex- cellence Initative - Research University” at the Jagiellonian University.
Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Supplementary material

Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.ailsci.2021.100021.
References

Zhang L, Tan J, Han D, Zhu H. From machine learning to deep learning: progress in machine intelligence for rational drug discovery. Drug Discov Today 2017;22(11):1680–5.
Hann MM. Molecular obesity, potency and other addictions in drug discovery. Med- chemcomm 2011;2(5):349–55.
Boobier S, Osbourn A, Mitchell JB. Can human experts predict solubility better than computers? J Cheminform 2017;9(1):1–14.
Wang J, Hou T. Recent advances on aqueous solubility prediction. Comb Chem High Throughput Screening 2011;14(5):328–38.
Palmer DS, Mitchell JB. Is experimental data quality the limiting factor in predicting the aqueous solubility of druglike molecules? Mol Pharm 2014;11(8):2962–72.
Llinas A, Avdeef A. Solubility challenge revisited after ten years, with multilab shake-flask data, using tight (SD 0.17 log) and loose (SD 0.62 log) test sets. J Chem Inf Model 2019;59(6):3036–40.
Dearden JC. In silico prediction of aqueous solubility. Expert Opin Drug Discov 2006;1(1):31–52.
Jiménez-Luna J, Grisoni F, Schneider G. Drug discovery with explainable artificial intelligence. Nat Mach Intell 2020;2(10):573–84.
Fühner H. Die wasserloslichkeit in homologen reihen. Berichte der deutschen chemischen Gesellschaft (A and B Series) 1924;57(3):510–15.
Hansch C, Quinlan J, Lawrence G. The Linear Free-Energy Relationship between Partition Coeﬃcients and the Aqueous Solubility of Organic Liquids. J Org Chem 1924.
Kamlet MJ, Doherty RM, Abboud J-LM, Abraham MH, Taft RW. Linear solvation energy relationships: 36. Molecular properties governing solubilities of organic non- electrolytes in water. J Pharm Sci 1986;75(4):338–49.
Yalkowsky SH, Valvani SC. Solubility and partitioning i: solubility of nonelectrolytes in water. J Pharm Sci 1980;69(8):912–22.
Erickson L.. The solubility of homologous series of organic compounds. 1952.
Hewitt M, Cronin MT, Enoch SJ, Madden JC, Roberts DW, Dearden JC. In sil- ico prediction of aqueous solubility: the solubility challenge. J Chem Inf Model 2009;49(11):2572–87.
Palmer DS, O’Boyle NM, Glen RC, Mitchell JB. Random forest models to predict aqueous solubility. J Chem Inf Model 2007;47(1):150–8.
Lind P, Maltseva T. Support vector machines for the estimation of aqueous solubility. J Chem Inf Comput Sci 2003;43(6):1855–9.
Könczöl Á, Dargó G. Brief overview of solubility methods: recent trends in equilibrium solubility measurement and predictive models. Drug Discov Today 2018;27:3–10.
Erić S, Kalinić M, Popović A, Zloh M, Kuzmanovski I. Prediction of aqueous solubility of drug-like molecules using a novel algorithm for automatic adjustment of relative importance of descriptors implemented in counter-propagation artificial neural net- works. Int J Pharm 2012;437(1–2):232–41.
Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolu- tional neural networks. Commun ACM 2017;60(6):84–90.
Chen L-C, Zhu Y, Papandreou G, Schroff F, Adam H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the Eu- ropean conference on computer vision (ECCV); 2018. p. 801–18.
Yang W, Zhang X, Tian Y, Wang W, Xue J-H, Liao Q. Deep learning for single image super-resolution: abrief review. IEEE Trans Multimedia 2019;21(12):3106–21.
Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A, et al. Convolutional networks on graphs for learning molecular fingerprints. Adv Neural Inf Process Syst 2015;28:2224–32.
Kearnes S, McCloskey K, Berndl M, Pande V, Riley P. Molecular graph convolutions: moving beyond fingerprints. J Comput Aided Mol Des 2016;30(8):595–608.
Wójcikowski M, Kukiełka M, Stepniewska-Dziubinska MM, Siedlecki P. Development of a protein–ligand extended connectivity (PLEC) fingerprint and its application for binding aﬃnity predictions. Bioinformatics 2019;35(8):1334–41.
Louis S-Y, Zhao Y, Nasiri A, Wang X, Song Y, Liu F, et al. Graph convolutional neural networks with global attention for improved materials property prediction. PCCP 2020;22(32):18141–8.
LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521(7553):436–44.
Weininger D. Smiles, a chemical language and information system. 1. Introduction to methodology and encoding rules. J Chem Inf Comput Sci 1988;28(1):31–6.
Lusci A, Pollastri G, Baldi P. Deep architectures and deep learning in chemoinformat- ics: the prediction of aqueous solubility for drug-like molecules. J Chem Inf Model 2013;53(7):1563–75.
Wu K, Zhao Z, Wang R, Wei G-W. TopP–S: persistent homology-based multi-task deep neural networks for simultaneous predictions of partition coeﬃcient and aque- ous solubility. J Comput Chem 2018;39(20):1444–54.



Liu K, Sun X, Jia L, Ma J, Xing H, Wu J, et al. Chemi-Net: a molecular graph convolutional network for accurate drug property prediction. Int J Mol Sci 2019;20(14):3389.
Tang B, Kramer ST, Fang M, Qiu Y, Wu Z, Xu D. A self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility. J Cheminform 2020;12(1):1–9.
Korotcov A, Tkachenko V, Russo DP, Ekins S. Comparison of deep learning with multiple machine learning methods and metrics using diverse drug discovery data sets. Mol Pharm 2017;14(12):4462–75.
Ba J, Caruana R. Do deep nets really need to be deep? Adv Neural Inf Process Syst 2014;27:2654–62.
Rajasegaran J, Jayasundara V, Jayasekara S, Jayasekara H, Seneviratne S, Rodrigo R. DeepCaps: going deeper with capsule networks. In: Proceedings of the IEEE confer- ence on computer vision and pattern recognition; 2019. p. 10725–33.
Sushko I, Novotarskyi S, Körner R, Pandey AK, Rupp M, Teetz W, et al. Online chemical modeling environment (OCHEM): web platform for data storage, model development and publishing of chemical information. J Comput Aided Mol Des 2011;25(6):533–54.
Sander T, Freyss J, von Korff M, Rufener C. DataWarrior: an open-source pro- gram for chemistry aware data visualization and analysis. J Chem Inf Model 2015;55(2):460–73.
Yang C, Tarkhov A, Marusczyk J, Bienfait B, Gasteiger J, Kleinoeder T, et al. New publicly available chemical query language, CSRML, to support chemotype rep- resentations for application to data mining and modeling. J Chem Inf Model 2015;55(3):510–28.
Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G. The graph neural network model. IEEE Trans Neural Netw 2008;20(1):61–80.
Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. 2014; arXiv preprint arXiv:14090473.
Landrum G, et al. Rdkit: open-source cheminformatics; 2006.
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. 2017; arXiv preprint arXiv:170603762.
Devlin J, Chang M-W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. 2018; arXiv preprint arXiv:181004805.
Dai Z, Yang Z, Yang Y, Cohen W, Carbonell J, Le Q, et al. Attentive language models beyond a fixed-length context. 2019; arXiv preprint arXiv:190102860.
Mikolov T, Chen K, Corrado G, Dean J. Eﬃcient estimation of word representations in vector space. 2013; arXiv preprint arXiv:13013781.
Asgari E, Mofrad MR. Continuous distributed representation of biological sequences for deep proteomics and genomics. PLoS ONE 2015;10(11):e0141287.
Zhang Y-F, Wang X, Kaushik AC, Chu Y, Shan X, Zhao M-Z, et al. SPVec: a Word2vec-inspired feature representation method for drug-target interaction pre- diction. Front Chem 2020;7:895.
Wang M, Yu L, Zheng D, Gan Q, Gai Y, Ye Z, et al. Deep graph library: towards eﬃcient and scalable deep learning on graphs; 2019.
Ho TK. Random decision forests. In: Proceedings of 3rd international conference on document analysis and recognition, vol. 1. IEEE; 1995. p. 278–82.
Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE. Neural message passing for quantum chemistry. In: International conference on machine learning. PMLR; 2017.
p. 1263–72.
Bemis GW, Murcko MA. The properties of known drugs. 1. Molecular frameworks. J Med Chem 1996;39(15):2887–93.
