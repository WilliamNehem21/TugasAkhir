Array 17 (2023) 100272










Harmonizing motion and contrast vision for robust looming detection
Qinbing Fu âˆ—,1, Zhiqiang Li1, Jigen Peng âˆ—
Machine Life and Intelligence Research Centre, School of Mathematics and Information Science, Guangzhou University, Guangzhou, 510006, China


A R T I C L E  I N F O	A B S T R A C T

	

Keywords:
Neural modelling Neuromorphic computing
Low-contrast looming detection Parallel ON/OFF channels Contrast neural computation
This paper presents a novel neural model of insectâ€™s visual perception paradigm to address a challenging problem on detection of looming motion, particularly in extremely low-contrast, and highly variable natural scenes. Current looming detection models are greatly affected by visual contrast between moving target and cluttered background lacking robust and low-cost solutions. Considering the anatomical and physiological homology between preliminary visual systems of different insect species, this gap can be significantly reduced by coordinating motion and contrast neural processing mechanisms. The proposed model draws lessons from research progress in insect neuroscience, articulates a neural network hierarchy based upon ON/OFF channels encoding motion and contrast signals in four parallel pathways. Specifically, the two ON/OFF motion pathways react to successively expanding ONâ€“ON and OFFâ€“OFF edges through spatialâ€“temporal interactions between polarity excitations and inhibitions. To formulate contrast neural computation, the instantaneous feedback normalization of preliminary motion received at starting cells of ON/OFF channels works effectively to suppress time-varying signals delivered into the ON/OFF motion pathways. Besides, another two ON/OFF contrast pathways are dedicated to neutralize high-contrast polarity optic flows when converging with motion signals. To corroborate the proposed method, we carried out systematic experiments with thousands of looming-square motions at varied grey scales, embedded in different natural moving backgrounds. The model response achieves remarkably lower variance and peaks more smoothly to looming motions in different natural scenarios, a significant enhancement upon previous works. Such robustness can be maintained against extremely low- contrast looming motion against cluttered backgrounds. The results demonstrate a parsimonious solution to stabilize looming detection against high input variability, analogous to insectâ€™s capability.





Introduction

Insects possess parsimonious visual systems capable of dealing with complex navigation tasks in a both robust and low-energy man- ner [1]. Among visually guided abilities in navigation, looming de- tection, i.e., the perception of objects that approach, is essential to determine a variety of behaviours including predation and defence against natural enemy [2], landing [3], clustering [4], and so forth. In the human world, looming detection is also a frontier of scientific research to build collision-free artificial vision systems in service of mobile robots [5â€“8], UAVs [9,10], and ground vehicles [11â€“14].
Typical looming detection methods mainly depend on different sensor strategies, such as radar [15], infrared [16], ultrasonic [17], and visual modalities [18]. In respect of retrieving more abundant of motion features shortly, vision-based methodologies are prevailing over other physical sensing techniques, however suffering from impact by chaotic and dynamic environments. In another word, current artificial vision systems for looming detection are vulnerable to (1) low-contrast
motion, (2) noisy background motion, (3) high solution cost on dealing with high-dimensional features. Although the new technology based on deep learning has good performance in reality, it demands large- scale data sets and consumes large volume of computing resources. In order to leverage system robustness and energy consumption in motion detection, peopleâ€™s attention is gradually attracted by natural ability. Learning from the homology between looming detection neural systems of different insect species could provide effective, low cost, tractable solutions [19,20].
Locusts and flies are two prominent modelling paradigms to study looming detection strategies. A considerable amount of computational works thus has been proposed to simulate biological visual percep- tion mechanisms for looming detection either at local, optical flows level [19], or in neural networks hierarchy [20]. The advantage of such biologically plausible solutions stems from their resource effi- ciency (or parsimony) especially in terms of power and mass, thus creating many successful applications in micro-machines with restricted

âˆ— Corresponding authors.
E-mail addresses: qifu@gzhu.edu.cn (Q. Fu), jgpeng@gzhu.edu.cn (J. Peng).
1 Qinbing Fu and Zhiqiang Li are joint first authors.

https://doi.org/10.1016/j.array.2022.100272
Received 14 October 2022; Received in revised form 10 December 2022; Accepted 13 December 2022
Available online 16 December 2022
2590-0056/Â© 2022 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).



computational capacity like micro-mobile robots [21â€“23], micro aerial vehicles [24â€“26], and small bio-mimic sensors [27]. Moreover, a few methods have been proposed to enhance the robustness and adaptabil- ity of insect-inspired looming detection models in complex, dynamic environments in recent years [20]. Accordingly, such insect-inspired approaches have, to some extent, overcome the aforementioned prob- lems via filtering out irrelevant background motions and minimizing resource cost with successful applications onto micro-robot [22].
However, current methodologies are still significantly influenced by spatial contrast, the difference of illumination between adjacent local areas of receptive field. The model response temporally fluctuates with high variance against input variability of natural signals. In addition, recognition of low-contrast looming motion has always been a chal- lenging problem for artificial vision systems. The causality would be (1) internally non-linear attribute of motion detection, and (2) externally large variation in local contrast of natural signals [28].
To solve this problem, the research into insect neuroscience recently has revealed their compact neural circuits harmonize motion and con- trast visual processing for robust motion vision against natural signals, though with organizational and functional disparities between different species [29,30]. The neural circuits of fruit fly Drosophila have been studied and investigated most intensively [31â€“34]. Actually, contrast neural computation has been found to play crucial roles which is prob- ably generic to preliminary visual systems of other animals including mammals, concerning their commonalities after millions of years of natural evolution [35]. Biologically dynamic vision systems decently resolve the local contrast issues whereas artificial vision systems are still greatly challenged.
To this end, this paper addresses the local contrast problem in looming detection whereby the proposed method coordinates motion and contrast neural computation in harmony. Specifically, the received signals are split into four parallel ON/OFF channels specializing in en- coding motion and contrast information of visual streams, respectively. The two ON/OFF motion pathways correlate brightness change in order to extract the successive expansion of ONâ€“ON/OFFâ€“OFF edges through spatialâ€“temporal competition between excitatory and suppressive lo- cal response. The contrast neural computation includes instantaneous feedback suppression of preliminary motion arrived at the entrance of ON/OFF motion channels for dynamic normalization of time-varying signals, cascaded with motion correlation. The two ON/OFF parallel contrast pathways are dedicated to attenuate high-contrast optic flows for inhibiting ON/OFF local motion, respectively. In this manner, our proposed model demonstrates significant enhancement in looming de- tection against high input variability of natural signals, especially in extremely low-contrast scenes. To corroborate this model, we created a new data set consisting of thousands of looming-square motions at varied grey scales embedded in different shifting natural backgrounds, as input stimuli. The systematic experiments demonstrated threefold achievements of this research upon previous works:
The proposed model coordinates motion and contrast vision within four parallel ON/OFF channels, which works effectively to recog- nize looming motion in extremely low-contrast scenes. The robust- ness against natural signals has been enhanced whereby the model response peaks more smoothly.
Compared to the typical model that only handles with motion signals for looming detection [36], the proposed contrast computation can significantly reduce response variance tested by the large visual data set of high input variability.
The neuromorphic computing of insectâ€™s four-layer, preliminary vi- sual neural circuits demonstrates a parsimonious and effective solu- tion to stabilize looming detection against natural signals, analogous to insectâ€™s capability.
The rest of this paper is structured as follows: Section 2 reviews re- lated works. Section 3 formulates the proposed neural model. Section 4 evaluates the proposed method. Section 5 concludes this paper.
Related work

Within this section, we concisely review related works in the areas of (1) insect-inspired visual systems for looming detection, (2) contrast neural computation for signal processing. We also highlight the new achievements upon related works.

Insect-inspired methods for looming detection

Insect-inspired intelligence has emerged as influential model sys- tems for artificial intelligence and robotics benefiting from its resource efficiency; it also has shown great potential to connect biological and artificial lives [37]. For building artificial visual systems, insect vision offers tractable and low-cost solutions to achieve effective performance in complex, dynamic, and sometimes hostile environments. In particu- lar, the visual systems of flies and locusts are prevalent model systems leading many successful looming-detecting applications in computing hardware with extraordinarily restricted resources, like flying robots and ground micro-robots [1,19,22,25]. By reflecting system complex- ity at different scales, they each apply different strategies of visual processing in distinct structures.
Fly visual systems have been modelled most extensively, illustrated mainly as optical flow(OF)-based methods [19]. In terms of physi- ology, a group of visual neurons called lobula plate tangential cells (LPTCs) in flyâ€™s visual brain has been identified to indicate direction and magnitude of local motion flows [38â€“41]. Such looming detection strategy has been widely used in flying robots including micro aerial vehicles [26,42]. However, to the best of our knowledge, such OF- based approach is primarily used for lateral looming motion sensing and collision avoidance, and is insufficient for detection of frontal object approaching. Moreover, such visual method is intolerant of low-contrast motion always resulting in failure of looming detection.
To address these OF-based problems, one category of modelling solutions stems from the lobula giant movement detectors (LGMD) in locustâ€™s visual brain [43â€“46]. These neurons respond most strongly to looming objects that directly approach the eyes, a situation that the OF-based methods cannot deal with very well. Specifically, the loom- selectivity of LGMD models or neural networks has been shaped well to moving objects that signal proximity rather than other kinds of movements [20]. There have been different approaches to formulate LGMD neuronal circuits with which the commonality is extracting looming features through multi-layered neural networks. Although such models have been satisfactorily applied in small-sized hardware for collision detection [6,7,9,21,23], the model performance is still greatly affected by spatial contrast, especially in natural scenarios with high input variability.
Another new category of methods specializing in looming detection originates from the lobula Plate/lobula columnar type II (LPLC2) neu- ronal ensemble, deeper than LPTCs in flyâ€™s visual brain, that features ultra-selectivity to only radial looming motion, i.e., edges diverging from the centroid of view [47]. Two computational works have been lately proposed to simulate such specific looming detection neural systems [48,49].
Compared with above methods, we emphasize the coordination of motion and contrast vision in parallel ON/OFF channels mimicking the generally four neuropil-layers of insect physiology shown in Fig. 1. We propose the contrast neural computation as shown in Fig. 3. The advantages over previous studies are abstracted as the following:
The robustness of looming detection against various natural signals has been significantly enhanced through introducing motion and contrast neural computation in parallel ON/OFF pathways encod- ing ONâ€“ON and OFFâ€“OFF dynamic features separately. The model can recognize extremely low-contrast looming motion, a significant enhancement upon previous works.
The proposed contrast mechanism and pathway harmonize in ex- traction of looming motion features to stabilize the neural model against natural signals of high input variability. The fidelity of looming detection has been remarkably improved.




/ig. 1. Schematic illustrations of multi-layered preliminary visual systems of insects for looming detection, and the proposed neural model mimicking insect physiology: abbreviations are P: photoreceptor, M: normalized motion unit, E: excitation unit, I: inhibition unit, TD: time delay unit, C: contrast unit, NL: non-linear unit, S: summation unit, G: grouping unit. The visual neural systems consist of four layers of Retina, Lamina, Medulla, Lobula, simulated by the proposed neural model. The red/green pipelines indicate ON/OFF motion pathways respectively, and the blue ones denote contrast pathways. Within ON/OFF channels, inhibition is obtained by convolving surrounding delayed excitations, local contrast is obtained by convolving surrounding unit responses. In the Lobula layer, grouping cell convolves surrounding summation unit responses. The looming perception unit at Lobula integrates all local grouped responses. The flowchart of only one processing unit is shown. Formulation of the proposed neural model is elucidated in Section 3. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


Contrast neural computation

Biological visual systems can perform key survival tasks in complex, dynamic environments, whereas artificial vision systems are far from such capability. Visual contrast issue is critical to be addressed [50,51]. Some mechanisms can adapt to natural signals such as the processing of environmental statistics [52,53]. However, the issue of spatial contrast has not been well resolved which always results in response fluctuation against highly variable input. Accordingly, new methods are requested to address such a problem.
Contrast computation is a general mechanism for neurons to process signals [54,55], not only in visual [29], but also in olfactory [56], and auditory [57] nervous systems. Such circuit mechanism works effec- tively to remove higher-order correlations from natural signals [30, 58]. In fact, there are a few prominent works in fly visual systems to demonstrate the efficacy of neural circuits implementing contrast mechanisms. The authors reported evidence for a non-linear, divisive normalization mechanism to deal instantly with local spatial contrast that emerges at an intermediate neuropil layer of preliminary visual systems, i.e., the Medulla [30]. More precisely, the foreground signal intensity of each Medulla inter-neuron is divided into a neighbouring background field by spatially integrating surrounding feedback signals, the whole process of which happens prior to motion correlation. This bio-plausible mechanism represents dynamic, non-linear properties in normalization as a basis of sensory circuit mechanism. To consolidate it, the authors also compared with feed-forward contrast normalization, and linear, static normalization methods through training a batch of elementary motion detectors (EMD). The instantaneous, feedback, dy- namic contrast normalization can finally fit best with the physiological data. Another research from Bahl et al. pointed out parallel contrast pathways beginning from the Medulla affect motion signals negatively at the Lobula area [29]. Such contrast pathway behaves to attenuate high-contrast local motion signal.
For computationally implementing contrast neural computation in motion sensitive visual systems, Fu et al. introduced fly contrast mech- anisms and pathways for estimation of natural background motion; the model demonstrated improved response compared to neural model processing only motion signals [59]. Li et al. incorporated in a single- channel LGMD model the contrast neural computation, initially in- dicated its effectiveness in looming detection [60]. Wang et al. also showed effectiveness of contrast computation in a small target motion detector preventing fake detection [61]. Differently to those works, and combining the latest physiological research, this paper presents an ap- proach to harmonizing motion and contrast vision within four parallel ON/OFF channels for looming detection. The method is validated by a large data set of natural signals.

/ormulation of the proposed neural model

Based on anatomical knowledge and modelling experience of in- sectsâ€™ preliminary visual systems, the proposed neural model mimics multi-layered insect physiology shown in Fig. 1. We herein highlight the coordination of motion and contrast neural computation in four par- allel ON/OFF channels. The proposed model is systematically described by diagrams and mathematical formulas, and it can respond robustly to looming objects in highly variable natural scenes. In general, it is an organizational neural network composed of different types of nerve cells placed in four computational neuropil layers.

Computational retina

Retina layer comes from the compound eyes of insects, in which each ommatidium consists of photoreceptors and independently collect
light signals in the field of vision. In this paper, a matrix of ğ‘Ÿ-rows
and ğ‘-columns is used to simulate the arrangement of photoreceptors,
and each value in the matrix represents the intensity of optical signal



determines the baseline sensitivity. ğ‘ƒÌ‚
(ğ‘¥, ğ‘¦, ğ‘¡) can be obtained by

Gaussian convolution of surrounding cell responses as

Ì‚
ğ‘œğ‘›
ğ‘ƒÌ‚
(ğ‘¥, ğ‘¦, ğ‘¡) = âˆ¬ ğ‘ƒğ‘œğ‘›
(ğ‘¥, ğ‘¦, ğ‘¡) = âˆ¬ ğ‘ƒ
(ğ‘¢, ğ‘£, ğ‘¡)ğºğœ (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£	(5)
(ğ‘¢, ğ‘£, ğ‘¡)ğº (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£	(6)

ğº (ğ‘¥, ğ‘¦) =  1  exp(âˆ’ ğ‘¥2 + ğ‘¦2 )	(7)
ğœ	2ğœ‹ğœ2	2ğœ2

/ig. 2. Inputâ€“output relationships of linear, static and dynamic contrast normalization models. In the dynamic model, the output is affected by the Gaussian normalized field, while in the static and linear models, the relationship between input and output is fixed.

collected by the eyelet. The input to the proposed model is ğ¿(ğ‘¥, ğ‘¦, ğ‘¡) âˆˆ
R3 where ğ‘¥ and ğ‘¦ represent abscissa and ordinate of the matrix, and ğ‘¡ is
the temporal position of the visual streams. In the propose model, every
photoreceptor retrieves preliminary motion information by calculating luminance change over time:
Parallel ON/OFF contrast channels The second is the calculation of local spatial contrast delivered by ON/OFF contrast pathways parallel to ON/OFF motion pathways. According to the fundamental research in [29], in the nerve centre of Drosophila, the calculation of contrast information and motion information are carried out in different paths. That is to say, it will not be affected by the motion information when calculating the contrast. Therefore, the normalized signal flow will enter a parallel channel specially used to calculate the contrast through implementing neural competition between centre and surrounding unit response. The calculation can be expressed as the following:
ğ¶ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) = |ğ‘€ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡)

ğ‘ƒ (ğ‘¥, ğ‘¦, ğ‘¡) = âˆ« (ğ›¿(ğ‘¡ âˆ’ ğœ) âˆ’ ğ›¿(ğ‘¡ âˆ’ ğ›¥ğ‘¡ âˆ’ ğœ))ğ¿(ğ‘¥, ğ‘¦, ğœ)ğ‘‘ğœ	(1)
where ğ›¿ is the unit impulse function. Note that for digital signals we
â€“ âˆ¬ ğ‘€ğ‘œğ‘›(ğ‘¢, ğ‘£, ğ‘¡)ğ¾ğ‘ (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£|
ğ¶ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = |ğ‘€ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡)
(8)

process, the time is discrete.
â€“ âˆ¬ ğ‘€
ğ‘œğ‘“ ğ‘“
(9)
(ğ‘¢, ğ‘£, ğ‘¡)ğ¾ğ‘ (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£|

Computational lamina

Lamina layer locates in the early stage of insectâ€™s visual information processing. Notably, the photoreceptors of Retina synapse onto the Lamina form the starting nerve cells of ON/OFF channels through oper- ations by polarity inter-neurons. In the process of transmission, visual signals are divided into two parts to enter ON/OFF channels connecting different nerve cells in next Medulla layer. This shunt mechanism can be calculated by half-wave rectification as follows:
ğ‘ƒğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) = [ğ‘ƒ (ğ‘¥, ğ‘¦, ğ‘¡)]+, ğ‘ƒğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = âˆ’[ğ‘ƒ (ğ‘¥, ğ‘¦, ğ‘¡)]âˆ’	(2) where [ğ‘¥]+, [ğ‘¥]âˆ’ are described mathematically as
[ğ‘¥]+ = max(ğ‘¥, 0), [ğ‘¥]âˆ’ = min(ğ‘¥, 0)	(3)

Computational medulla

Medulla layer, located in the middle region of optic lobe, plays a central role to coordinate motion and contrast vision in the proposed neural model. The stained micro-graph of the Medulla layer shows that there is an obvious hierarchical structure in this layer, and various synapses extend into different fibre layers for signal transmission and interaction (see Fig. 1). For imitating this, the computational Medulla layer consists of three parts of neural computation.
The contrast kernel ğ¾ğ‘ obeys to spatially uniform distribution. The
aforementioned two operations of contrast neural computation are
illustrated together in Fig. 3.
Parallel ON/OFF motion pathways The third part is the calculation of motion information in ON/OFF channels. To extract looming mo- tion features, the model responds to continuously expansion of ONâ€“ ON/OFFâ€“OFF edges via spatialâ€“temporal interaction between excita- tion and inhibition cells. The normalized information (Eq. (4)) directly forms visual excitement without delay.
ğ¸ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) = ğ‘€ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡), ğ¸ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = ğ‘€ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡)	(10)
The acquisition of visual inhibition requires the information flow to be delayed with a centreâ€“surround antagonism. Therefore, the polarity inhibition can be calculated by the following formulas
ğ¼ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) = âˆ¬ ğ‘€ğ‘œğ‘›(ğ‘¢, ğ‘£, ğœ)T (ğ‘¡ âˆ’ ğœ)ğ¾ğ¼ (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£ğ‘‘ğœ	(11)
ğ¼ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = âˆ¬ ğ‘€ğ‘œğ‘“ ğ‘“ (ğ‘¢, ğ‘£, ğœ)T (ğ‘¡ âˆ’ ğœ)ğ¾ğ¼ (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£ğ‘‘ğœ	(12)
ğ¾ğ¼ is the convolution kernel, which represents the local suppression
weight. The closer the inhibitory cells are to the excited cells, the
stronger the inhibitory effect is. The farther the inhibitory cells are from the excited cells, the smaller the inhibition weight is. After discretiza- tion, the kernel can be described in a simple form as

Contrast normalization mechanism Firstly, the signals delivered by start- ing nerve cells of ON/OFF channels will undergo an instantaneous nor- malized feedback operation, which is the contrast suppression mecha- nism. There are three forms of this mechanism, namely linear, static,
[ğ¾ğ¼ ]
0.125  0.25  0.125
=	0.25	0	0.25
â¢â£ 0.125  0.25  0.125 â¥â¦

(13)

and dynamic normalization as shown in Fig. 2. These can be expressed by the following formulas:
Linear: ğ‘€ğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = ğ‘ƒğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡)
T (ğ‘¡) is the time delay function. As we process digital signals, it is rep-
resented by a first-order low-pass filtering to correlate two successive
moments.
Local summation of motion and contrast signals Next, the polarity sum-

Static: ğ‘€
Dynamic: ğ‘€
ğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡) = tanh( ğ‘ƒğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) )
ğ›¼1
(ğ‘¥, ğ‘¦, ğ‘¡) = tanh(   ğ‘ƒğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡)   )
(4)
mation cells integrate the signal flow of the above three parts. The ex- citatory and inhibitory quantities of nerve cells are linearly summed to reflect the remaining motion induced local-excitation as the following

ğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“
ğ‘œğ‘›âˆ•ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡) + ğ›¼1
formulas:

has been verified to fit best the physiological results [30,60]. Here ğ‘¡ğ‘ğ‘›â„ Note that we apply the dynamic suppression in this neural model which operation indicates the hyperbolic tangent function. The coefficient ğ›¼1
ğ‘†ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) = ğ¸ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡) âˆ’ ğ›½ âˆ— ğ¼ğ‘œğ‘›(ğ‘¥, ğ‘¦, ğ‘¡)	(14)
ğ‘†ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) = ğ¸ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡) âˆ’ ğ›½ âˆ— ğ¼ğ‘œğ‘“ ğ‘“ (ğ‘¥, ğ‘¦, ğ‘¡)	(15)




	

/ig. 3. (a) Diagram of instantaneous feedback normalization in ON/OFF channels. Contrast normalization is the feedback of the combination of partial medulla neurons. (b) Illustration of parallel motion and contrast calculation and convergence. Abbreviations â€˜Câ€™, â€˜Eâ€™, â€˜Iâ€™, â€˜TDâ€™ and â€˜HPâ€™ are shorthand for contrast, excitation, inhibition, time delay and high-pass filtering.


ğ›½ is the inhibition weight coefficient, which represents the influence of
inhibitory cells on excited cells. Then, because the motion information
is mixed with a large amount of contrast information of natural scenes, the motion channel needs to converge with the contrast channel (see Fig. 3(b)). At the time of convergence, the motion information is sub- tracted from the contrast information to weaken the influence of high- contrast optical flow on looming detection. At the same time, through half-wave rectification, the negative-sign signals will be filtered out.
Table 1
The parameters of proposed neural model. Parameter	Value	Description
ğ‘Ÿ, ğ‘	adaptable	length and width of the input image
ğ›¼1	3	baseline sensitivity value of contrast normalization
ğ›½	0.4	weight coefficient of inhibition unit
ğœƒ1 , ğœƒ2 , ğœƒ3	1, 1, 1	influence coefficient of ON/OFF channels
ğ¶ğ‘¤	4	a constant used to calculate ğ‘¤
ğ›¥ğ¶	0.01	a small real number used to prevent calculation errors.

ğ‘†Ì‚ (ğ‘¥, ğ‘¦, ğ‘¡) = [ğ‘†
ğ‘œğ‘›
(ğ‘¥, ğ‘¦, ğ‘¡) âˆ’ ğ¶
ğ‘œğ‘›
(ğ‘¥, ğ‘¦, ğ‘¡)]+	(16)
ğ‘‡ğ‘”	1.4	threshold of grouping mechanism
ğ›¼2	0.1	scale coefficient in sigmoid transformation

Ì‚
ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡) = [ğ‘†
ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡) âˆ’ ğ¶
ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡)]+	(17)

Computational Lobula

Lobula layer is the last nerve layer of insect optic lobe (Fig. 1). It has a large number of motion sensitive neurons, which can quickly capture the edge information of moving objects in the field of vision and respond to the target accordingly. After harmonizing motion and contrast features in the Medulla layer, the Lobula layer integrates polarity signals from ON/OFF channels at two levels. First at the local level, excitations are integrated obeying the supra-linear rule.
ğ¾(ğ‘¡) = (1 + exp(  âˆ’ğ‘˜(ğ‘¡)  ))âˆ’1	(24)
ğ›¼2 âˆ— ğ‘Ÿ âˆ— ğ‘
where ğ›¼2 is a scale parameter in sigmoid transformation, tuned to avoid
saturation problem of different sized signals. Therefore, the sigmoid
membrane potential with regard to time is the output of the proposed neural model. Then it projects to different areas of insectâ€™s forebrain through axons, so as to control different behaviours of insects.

ğ‘†(ğ‘¥, ğ‘¦, ğ‘¡) = ğœƒ1 âˆ— ğ‘†Ì‚ (ğ‘¥, ğ‘¦, ğ‘¡) + ğœƒ2
Ì‚
ğ‘œğ‘“ ğ‘“
(ğ‘¥, ğ‘¦, ğ‘¡)
(18)
Setting the parameters

+  ğœƒ3 âˆ— ğ‘†Ì‚  (ğ‘¥, ğ‘¦, ğ‘¡) âˆ— ğ‘† Ì‚
(ğ‘¥, ğ‘¦, ğ‘¡)
The parameters of the proposed neural model are given in Table 1.

Here, the combination of ğœƒ1, ğœƒ2, ğœƒ3 represents the influence coefficient
of each channel. After that, a grouping mechanism acts to enhance
the extraction of looming cues, alleviate effect of dynamic background clutter and reduce isolated excitation by convolution and thresholding processes, described as
ğ¶ğ‘’(ğ‘¥, ğ‘¦, ğ‘¡) = âˆ¬ ğ‘†(ğ‘¢, ğ‘£, ğ‘¡)ğ¾ğ‘” (ğ‘¥ âˆ’ ğ‘¢, ğ‘¦ âˆ’ ğ‘£)ğ‘‘ğ‘¢ğ‘‘ğ‘£	(19)
As most parameters of motion pathways are adapted from previous modelling works [21,62], the only adaptable parameters are depending on the resolution of input visual streams. Learning method is avoided in this research as it is out of the scope of this study. The comparison between linear, static, and dynamic contrast normalization has been taken in our preliminary research recently [60], whereby the result coincides with the physiological study well. That is to say, the dynamic

ğœ”(ğ‘¡) = ğ›¥
+ max(|ğ¶ (ğ‘¥, ğ‘¦, ğ‘¡)|) âˆ— ğ¶âˆ’1
(20)
contrast normalization addresses the contrast problem well in complex,




ğº(ğ‘¥, ğ‘¦, ğ‘¡) = ğ‘†(ğ‘¥, ğ‘¦, ğ‘¡) âˆ— ğ¶ğ‘’(ğ‘¥, ğ‘¦, ğ‘¡) âˆ— ğœ”(ğ‘¡)âˆ’1	(21)
value in Eq. (4) will be investigated in our experiments. It is also worth
emphasizing the proposed model processes visual signals within four

ğºÌ‚(ğ‘¥, ğ‘¦, ğ‘¡) = { ğº(ğ‘¥, ğ‘¦, ğ‘¡)	if ğº(ğ‘¥, ğ‘¦, ğ‘¡) â‰¥ ğ‘‡ğ‘”
(22)
ON/OFF channels, in a feed-forward neural network structure, which is a novel form of coordinating motion and contrast vision compared

Concretely, a passing coefficient matrix ğ¶ğ‘’ is obtained by a convolution process with an equally weighted kernel ğ¾ğ‘” . ğœ” is a scale parameter updated at every time. ğ¶ğ‘¤ is a constant, and ğ›¥ğ¶ stands for a small real number. ğ‘‡ğ‘” indicates the threshold in grouping mechanism.
Finally, the looming sensitive neuron in the Lobula collects all
movement information and converts it into membrane potential. The computations are as follows:
to relevant works specializing in looming detection.

Experimental evaluation

In this section, we carry out systematic experiments to evaluate the proposed method for looming detection against high input variabil- ity of natural signals. The experiments are taken gradually by three

ğ‘Ÿ
ğ‘˜(ğ‘¡) =
1
ğ‘
ğºÌ‚(ğ‘¥, ğ‘¦, ğ‘¡)ğ‘‘ğ‘¥ğ‘‘ğ‘¦	(23)
1
categories of tests. To highlight the improvement of this method, we also compare two related works. Concretely, we first demonstrate basic






/ig. 4. Samples of looming-square in clean-and-consistent background with different contrast between them. â€˜Bâ€™ and â€˜Fâ€™ respectively represent the grey value of background and foreground looming squares. The stimuli of extremely low-contrast looming is depicted at bottom.
are synthesized by computer, which represents simple scenes free of noise in order to demonstrate the basic characteristics of our proposed neural model. The backgrounds and the foreground looming-squares of such kind of video sequences are solid colour pictures with different grey values, whereby the foreground objects expand with respect to time. The background grey value varies between minimum of 0 and maximum of 255, and the sampling interval is taken every 25 between 0 and 255. The same is true for the foreground looming-square. Some samples of the video sequence used in the experiment are shown in Fig. 4. Specifically, in the first video, the background grey value is 0, and the foreground grey value is 255; at this time, the background and foreground form the maximum contrast. The background grey value of the second video is 100, and the foreground grey value is 150; at this time, the contrast becomes smaller. Notably, we also set the scenario with extremely low contrast, tiny difference between foreground and background, that is, the grey value of the background is 125, and the foreground is 130. In this kind of visual stimuli testing, the squares loom between frame-17 and frame-38, otherwise remain stationary. All
input videos have the resolution of 600 Ã— 600 at the sampling frequency
of 30 Hz.
The second part is the test of complex scenarios. In this test, our data set includes 1100 original videos. These videos are made based on different realistic, wide-field scenes adapted with respect from a previous research [63]. More specifically, the indoor scenes include libraries, shopping malls, etc. The outdoor scenes include forests, grass- lands, villages, etc. Based on these panoramic scenes, we generate a looming square with different grey values, similarly to the test in the aforementioned simple scenarios. In addition, in order to simulate the visual stimulation of insects during flight, we also set the background to move to the right at a constant angular speed of 20 degrees per
second. The resolution of these videos is 926 Ã— 250 with the frame
rate 30 Hz. Every foreground looming motion starts from frame-20 and
ends at frame-63. On the other hand, the background shifts rightward from beginning to end. Some samples of video sequence used in the experiment are shown in Fig. 5.

Metric

Due to the use of a large number of data samples, we compare the experimental results of various models by calculating the dispersion of
the coefficient of variation (ğ‘‰ğ‘ ) with respect to time. The calculation is
as the following:


/ig. 5. Samples from thousands looming processes within a variety of cluttered moving
ğ‘‰ (ğ‘¡) = ğœ(ğ‘¡)
(25)

scenes: the background shifts rightward at a constant speed ğ‘‰ğ‘ ; the foreground looming
squares vary at grey scales.



characteristics of the proposed neural model with emphasis laid on performance in extremely low-contrast scenarios, and against various natural signals. After that, we show improvement based upon the incorporation of ON/OFF channels in looming detection. Lastly, as the contrast neural computation is the main novelty of this modelling research upon previous works, we look into the effects of dynamic normalization mechanism, and ON/OFF parallel contrast pathways on looming detection against high input variability.

Setting the experiments

We have the implementation code and data sets used in this paper uploaded to the link below.2 The visual stimuli used in our experiments can be divided into two categories. Firstly, a set of visual streams

2 https://github.com/fuqinbing/harmonizing-motion-and-contrast-vision- for-looming-detection
ğ‘   ğœ‡(ğ‘¡)
Here, ğœ represents the standard deviation of the model output, and ğœ‡
represents the mean value; t indicates the time position. The dispersion
degree of the coefficient of variation is measured by the inter-quartile range (IQR) of the violin diagram. More precisely, the denser the dis- tribution of the coefficient of variation, the better the statistical result is. The smaller the coefficient of variation, the better the statistical result is. Therefore, we would prefer the distribution of the coefficient of variation concentrated at the bottom of the violin plot. In addition, in the violin plot, the coloured dots represent the calculated coefficients of variation. The border of violin (grey line) shows the corresponding distribution of the coefficient of variation. The green line in each violin represents a box-plot, used to characterize the dispersion degree of the coefficient of variation.

Demonstration of basic characteristics

This subsection introduces the first type of experiments with com- parison to two related works on the same testing data of both simple and complex scenarios.
As can be seen from Fig. 6, the model of Hu et al. [36] cannot detect looming motion in very low contrast (e.g. F 125-B 130) and the output




/ig. 6. The experimental results of the proposed model, the model of Li et al. [60], and the model of Hu et al. [36] in pure scenes. â€˜Fâ€™ and â€˜Bâ€™ in the legend represent the grey values of foreground and background respectively. The red and blue dashed lines indicate the ground-truth time window of looming process. Compared to the two previous methods, the proposed model (left panel) performs more consistently, and can recognize looming motion with extremely low contrast. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


/ig. 7. The statistical responses of the proposed model and the comparative models against a variety of natural signals. The red and blue dashed lines indicate the ground-truth time window of looming process. The green solid line represents the average output of the model with respect to time, and the purple shadow represents the corresponding standard deviation. The proposed model (left panel) performs more robustly against high input variability. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)






/ig. 8. The violin diagram of the statistical results: in each violin, the coloured dots indicate the coefficient of variation. The contour of violin (grey solid line) indicates the distribution of coefficient of variation. The denser the distribution is, the more prominent the contour is. The green in the middle of the violin represents the box plot of the coefficient of variation. The proposed model outperforms the two comparative models against looming motion in thousands of dynamic scenes. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)



of the model will be greatly affected by the contrast representing the largest variance to the tested data. Both of our proposed model and the model of Li et al. [60] are able to detect looming motion in low-contrast environments. In contrast to the model of Li et al. [60], the output of our proposed model is more consistent across different contrasts.
Secondly, Fig. 7 compares the statistical responses between the proposed method and the two comparative models against natural signals. From another perspective, Fig. 8 represents the dispersion of the coefficient of variation for each tested model. It is clear that the variation of our proposed model relative to the model of Hu et al. [36] and the model of Li et al. [60] is much lower and the coefficient distri- bution is denser and smaller. In addition, the corresponding box-plots have smaller inner spacing which indicates that our proposed model is more robust against natural scenes with drastic contrast changes. Importantly, we also solve the problem of high-frequency oscillations of response between frame-55 and frame-62 observed in the model of Li et al. [60] through the effective coordination of motion and contrast neural computation in four separate ON/OFF channels. Our proposed model peaks more smoothly to looming motion, and is more robust to irrelevant background movements.

Improvement based upon ON/OFF channels

This subsection continues the systematic experiments to highlight the improvements based on ON/OFF channels coordinating motion and contrast vision for looming detection against natural signals. Differ- ently to the previous type of tests, we add two extra sets of complex scenarios to the original 1100 ones by increasing and decreasing the
global illumination of background images with +20% and âˆ’20% on
grey levels respectively, thus generating a total of 2200 additional
video sequences. These video sequences are used to simulate brighter and darker environments for insect navigation, as new inputs to the investigated models.
The results in Fig. 9 demonstrate that despite increase and decrease of global background illumination, our proposed model is enhanced












































/ig. 9. The statistical results of the proposed model and the comparative model [60] in different background illuminations: (a) the mean and standard deviation of the model output after the background grey value increased by 20%. (b) the violin plots formed by the coefficient of variation corresponding to (a). (c) the results of background grey value reduced by 20%. (d) the corresponding violin figure to (c). The proposed model performs more robustly against increase and decrease of background illumination which peaks more smoothly to looming motion.


significantly by incorporating the ON/OFF channels, in comparison with the model of Li et al. [60] that processes motion and contrast vi- sion regardless of polarity changes, i.e., ON-contrast and OFF-contrast. Precisely, the proposed model peaks more smoothly to looming motions in a large number of different natural scenes. The statistical results show the proposed model has more densely distributed coefficients of variation and smaller inner distances. This suggests that our proposed model is invulnerable to environmental illumination.

Investigation on contrast neural computation

In the last kind of experiments, as the contrast neural computation is the main novelty of this modelling research, and plays crucial roles in enhancing the robustness of looming detection against natural signals of high input variability, we investigate two factors that lead the performance improvement.
normalization that open the gates for ON/OFF motion signals, i.e., ğ›¼ The first is the baseline sensitivity parameter of dynamic contrast
in Eq. (4). We adopt three different values in the proposed model tested by all the original 1100 natural data. The statistical responses are shown in Fig. 10. Specifically, when the baseline sensitivity is very
small (ğ›¼ = 0.5, the middle panel of Fig. 10), the looming-detecting
neural model is also activated by movements induced by the shifting
of natural background. Increasing the baseline sensitivity improves the model performance indicating the dynamic contrast normalization also works effectively to suppress the cluttered background movement. The model represents smaller fluctuations and variance of response
robust performance even increasing to a relatively larger value (ğ›¼ = 10, to background movements. In addition, the model can maintain such
selected value of the baseline sensitivity is ğ›¼ = 3 (the left panel of the right panel of Fig. 10). Note that in the previous experiments, the
Fig. 10) which can achieve the best performance in our investigation. The second is the parallel ON/OFF contrast pathways that neutralize strong excitations induced by high-contrast, local ON/OFF motion. To
grey value of all natural backgrounds in our data set, which is 122. The indicate the efficacy of ON/OFF-channels, We calculate the average
OFF contrast pathway is firstly closed. In this case, the foreground grey scale values between 0â€“100 are lower than the average grey scale value of the background. These looming-square motions are mainly han- dled by the OFF-rather than the ON-channels of the proposed model. Therefore, the effect of OFF contrast pathway can be demonstrated via the comparative experiments of switching OFF contrast channels. The results in Fig. 11 reveal that when the OFF contrast pathway is turned off, the distribution of the coefficient of variation of the model response for videos with a foreground between 0â€“100 has greater dispersion (larger inner spacing of box-plots) than the cases with OFF contrast pathway turned on. In addition, the larger the grey value of the foreground looming-square is (closer to the average background grey value of 122), the greater the dispersion of the coefficient of variation is (showing an upward trend) (see the left panel in Fig. 11). That is to say, when the contrast between the foreground and background is smaller, the effect of OFF contrast pathway is more obvious, consistent to our previous results on dealing with low-contrast looming motion.
On the other hand, the ON contrast pathway is closed. The fore- ground grey value between 150â€“255 is greater than the background average grey value of 122, so the ON contrast pathway matters in this case. When the ON contrast pathway is turned on, the dispersion of the coefficient of variation of each foreground grey value does not change much (hardly affected by contrast changes). Nevertheless, when the ON contrast pathways is turned off, the dispersion of the coefficient of variation begins to change more greatly. The larger the grey value of the foreground looming-square is, i.e., the greater the contrast between the background and the foreground is, the smaller the dispersion of the coefficient of variation is (there is an overall downward trend) (see the right panel in Fig. 11). Accordingly, similarly to the results of closing merely the OFF contrast pathway, the smaller the contrast is, the effect of ON contrast pathway is more obvious.




/ig. 10. The statistical responses of the proposed model under the investigation on baseline sensitivity of dynamic contrast normalization: three baseline values are applied for investigation, respectively.


/ig. 11. The violin plots of examination on closing either ON or OFF contrast pathway in looming detection against natural signals: the ğ‘‹-axis indicates different grey values of foreground looming-squares and the ğ‘Œ -axis indicates coefficient of variation. The ON/O// contrast pathways work effectively to maintain the robustness of the proposed
model for detection of low-contrast looming motion.


Conclusion and discussion

This paper has presented a novel way of coordinating contrast and motion vision to improve effectively the performance of looming de- tection in extremely low-contrast scenarios, and against a large variety of natural signals. The proposed neural model features feed-forward visual processing in a stratified neural network with four bio-plausible parallel ON/OFF channels encoding polarity motion and contrast in- formation separately. The contrast neural computation is the main novelty of this modelling research as it first works as an instant, dy- namic normalization mechanism to open the gates for ON/OFF motion signals. There are also ON/OFF contrast pathways to neutralize high- contrast local ON/OFF motion induced excitations, in order to stabilize model response against high input variability of natural scenes. Accord- ingly, the proposed model performs consistently in highly variable, and various-contrasted scenes.
To corroborate the proposed method, we have crafted a new data set consisting of thousands of looming-square motions in cluttered and dynamic backgrounds. To highlight our achievements, the comparative experiments have been carried out. The results verify our proposed method is more robust for looming detection in natural scenes, espe- cially at extremely low contrast. Separating motion and contrast into ON/OFF channels works effectively to alleviate the response fluctuation against natural signals, and make the visual system peak more smoothly to looming motion.
For resolving real-world, complicated detection problems, recent years have witnessed much progress based upon image processing and deep learning methods [64,65], as well as advanced sensor strate- gies [66]. We insist another promising way is drawing lessons from neuroscience on how animals deal with similar situations. Insect in- telligence is featured by efficiency and parsimony that can offer a number of excellent paradigms to build artificial vision systems and neuromorphic sensors. In this regard, the proposed approach is also of great potential to be utilized in hardware applications like bio-inspired robotic systems [67,68], and micro/aerial robotic systems [19,20].
CRediT authorship contribution statement

Qinbing /u: Conceptualization, Methodology, Writing, Validation. Zhiqiang Li: Software, Visualization, Investigation, Data curation. Ji- gen Peng: Supervision, Funding acquisition, Project administration.

Declaration of competing interest

No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.

Data availability

Data will be made available on request.

Acknowledgements

This research has received funding from the National Natural Sci- ence Foundation of China under the Grant No. 12031003, and the Social Science Fund of the Ministry of Education of China under the Grant No. 22YJCZH032.

References

Franceschini N. Small brains, smart machines: From fly vision to robot vision and back again. Proc IEEE 2014;102:751â€“81.
Yamawaki Y. Defence behaviours of the praying mantis tenodera aridifolia in response to looming objects. J Insect Physiol 2011;57(11):1510â€“7.
Tammero LF, Dickinson MH. Collision-avoidance and landing responses are mediated by separate pathways in the fruit fly, drosophila melanogaster. J Exp Biol 2002;205:2785â€“98.
Baird E, Kornfeldt T, Dacke M. Minimum viewing angle for visually guided ground speed control in bumblebees. J Exp Biol 2010;213(10):1625.



Yue S, Rind FC. A collision detection system for a mobile robot inspired by the locust visual system. In: Proceedings of the 2005 IEEE international conference on robotics and automation. ICRA, IEEE; 2005, p. 3832â€“7.
Fu Q, Hu C, Liu T, Yue S. Collision selective LGMDs neuron models research benefits from a vision-based autonomous micro robot. In: Proceedings of the 2017 IEEE/RSJ international conference on intelligent robots and systems. IROS, IEEE; 2017, p. 3996â€“4002.
Cizek P, Faigl J. Self-supervised learning of the biologically-inspired obstacle avoidance of hexapod walking robot. Bioinspir Biomim 2019;14(4):046002.
Milde MB, Blum H, DietmÃ¼ller A, Sumislawska D, Conradt J, Indiveri G, et al. Obstacle avoidance and target acquisition for robot navigation using a mixed signal analog/digital neuromorphic processing system. Front Neurorobotics 2017;11:1â€“17.
Salt L, Howard D, Indiveri G, Sandamirskaya Y. Parameter optimization and learning in a spiking neural network for UAV obstacle avoidance targeting neuromorphic processors. IEEE Trans Neural Netw Learn Syst 2020;31(9):3305â€“18.
Zhao J, Ma X, Fu Q, Hu C, Yue S. An LGMD based competitive collision avoid- ance strategy for UAV. In: Artificial intelligence applications and innovations. Springer International Publishing; 2019, p. 80â€“91.
Yue S, Rind FC, Keil MS, Cuadri J, Stafford R. A bio-inspired visual collision detection mechanism for cars: Optimisation of a model of a locust neuron to a novel environment. Neurocomputing 2006;69(13â€“15):1591â€“8.
Stafford R, Santer RD, Rind FC. A bio-inspired visual collision detection mech- anism for cars: Combining insect inspired neurons to create a robust system. Biosystems 2007;87(2â€“3):164â€“71.
Krejan A, Trost A. LGMD-based bio-inspired algorithm for detecting risk of collision of a road vehicle. In: Proceedings of the 2011 IEEE 7th international symposium on image and signal processing and analysis. IEEE; 2011, p. 319â€“24.
Hartbauer M. Simplified bionic solutions: A simple bio-inspired vehicle collision detection system. Bioinspir Biomim 2017;12(2):026007.
Reich GM, Antoniou M, Baker C. Memory-enhanced cognitive radar for autonomous navigation. IET Radar Sonar Navig 2020;14(9):1287â€“96.
Arvin F, Samsudin K, Ramli AR. Development of IR-based short-range com- munication techniques for swarm robot applications. Adv Electr Comput Eng 2010;10(4):61â€“8.
Everett H. Sensors for mobile robots: Theory and application. Taylor & Francis; 1995.
Mukhtar A, Xia L, Tang TB. Vehicle detection techniques for collision avoidance systems: A review. IEEE Trans Intell Transp Syst 2015;16(5):2318â€“38. http:
//dx.doi.org/10.1109/TITS.2015.2409109.
Serres JR, Ruffier F. Optic flow-based collision-free strategies: From insects to robots. Arthropod Struct Dev 2017;46(5):703â€“17.
Fu Q, Wang H, Hu C, Yue S. Towards computational models and appli- cations of insect visual systems for motion perception: A review. Artif Life 2019;25(3):263â€“311.
Fu Q, Hu C, Peng J, Yue S. Shaping the collision selectivity in a looming sensitive neuron model with parallel ON and OFF pathways and spike frequency adaptation. Neural Netw 2018;106:127â€“43. http://dx.doi.org/10.1016/j.neunet. 2018.04.001.
Fu Q, Hu C, Liu P, Yue S. Towards computational models of insect motion detectors for robot vision. In: Towards autonomous robotic systems conference. 2018, p. 465â€“7.
Fu Q, Hu C, Peng J, Rind FC, Yue S. A robust collision perception visual neural network with specific selectivity to darker objects. IEEE Trans Cybern 2019;5(12):5074â€“88. http://dx.doi.org/10.1109/TCYB.2019.2946090.
Ruffier F, Viollet S, Franceschini N. OSCAR and OCTAVE: Two bio-inspired visually guided aerial micro-robots. In: Proceedings of the 11th international conference on advanced robotics. IEEE; 2003, p. 726â€“32.
Franceschini N, Ruffier F, Serres J. Insect inspired autopilots. J Aero Aqua Bio-Mech 2010;1(1):2â€“10.
Ruffier F, Franceschini N. Optic flow regulation in unsteady environments: A tethered MAV achieves terrain following and targeted landing over a moving platform. J Intell Robot Syst 2015;79(2):275â€“93.
Floreano D, Pericet-Camara R, Viollet S, Ruffier F, Bruckner A, Leitel R, et al. Miniature curved artificial compound eyes. Proc Natl Acad Sci 2013;110(23).
Salazar-Gatzimas E, Chen J, Creamer M, Mano O, Mandel H, Matulis C, et al. Direct measurement of correlation responses in drosophila elementary motion detectors reveals fast timescale tuning. Neuron 2016;92(1):227â€“39.
Bahl A, Serbe E, Meier M, Ammer G, Borst A. Neural mechanisms for drosophila contrast vision. Neuron 2015;88:1240â€“52.
Drews MS, Leonhardt A, Pirogova N, Richter FG, Schuetzenberger A, Braun L, et al. Dynamic signal compression for robust motion vision in flies. Curr Biol 2020;30:209â€“21.
Mauss AS, Vlasits A, Borst A, Feller M. Visual circuits for direction selectivity. Annu Rev Neurosci 2017;40(1):211.
Borst A, Haag J, Reiff DF. Fly motion vision. Annu Rev Neurosci 2010;33:49â€“70.
Borst A, Euler T. Seeing things in motion: Models, circuits, and mechanisms. Neuron 2011;71(6):974â€“94.
Borst A, Haag J, Mauss AS. How fly neurons compute the direction of visual motion. J. Comp Physiol A 2020;206:109â€“24.
Borst A, Helmstaedter M. Common circuit design in fly and mammalian motion vision. Nature Neurosci 2015;18(8):1067â€“76.
Hu C, Arvin F, Xiong C, Yue S. Bio-inspired embedded vision system for autonomous micro-robots: The LGMD case. IEEE Trans Cogn Dev Syst 2017;9(3):241â€“54.
de Croon GCHE, Dupeyroux JJG, Fuller SB, Marshall JAR. Insect-inspired AI for autonomous robots. Science Robotics 2022;7(eabl6334):1â€“11.
Schnell B, Raghu SV, Nern A, Borst A. Columnar cells necessary for motion responses of wide-field visual interneurons in Drosophila. J Comp Physiol 2012;198:389â€“95.
Maisak MS, Haag J, Ammer G, Serbe E, Meier M, Leonhardt A, et al. A directional tuning map of drosophila elementary motion detectors. Nature 2013;500(7461):212â€“6.
Takemura S-y, Bharioke A, Lu Z, Nern A, Vitaladevuni S, Rivlin PK, et al. A visual motion detection circuit suggested by drosophila connectomics. Nature 2013;500(7461):175â€“81.
Wei H, Kyung HY, Kim PJ, Deswplan C. The diversity of lobula plate tangential cells (LPTCs) in the drosophila motion vision system. J Comp Physiol A 2019;1â€“10. http://dx.doi.org/10.1007/s00359-019-01380-y.
Ruffier F, Franceschini N. Optic flow regulation: The key to aircraft automatic guidance. Robot Auton Syst 2005;50(4):177â€“94.
Rind FC, Simmons PJ. Local circuit for the computation of object approach by an identified visual neuron in the locust. J Comp Neurol 1998;395(3):405â€“15.
Rind FC, Simmons PJ. Seeing what is coming: Building collision-sensitive neurones. Trends Neurosci 1999;22(5):215â€“20.
Simmons PJ, Rind FC. Responses to object approach by a wide field visual neurone, the LGMD2 of the locust: Characterization and image cues. J Com Physiol- [A] 1997;180(3):203â€“14.
Rind FC, Wernitznig S, Polt P, Zankel A, Gutl D, Sztarker J, et al. Two identified looming detectors in the locust: Ubiquitous lateral connections among their inputs contribute to selective responses to looming objects. Sci Rep 2016;6:35525.
Klapoetke NC, Nern A, Peek MY, Rogers EM, Breads P, Rubin GM, et al. Ultra-selective looming detection from radial motion opponency. Nature 2017;551:237â€“41.
Zhou B, Li Z, Kim SSY, Lafferty J, Clark DA. Shallow neural networks trained to detect collisions recover features of visual loom-selective neurons. ELife 2022.
Hua M, Fu Q, Peng J, Yue S, Luan H. Shaping the ultra-selectivity of a looming detection neural network from non-linear correlation of radial motion. In: IEEE the international joint conference on neural networks. 2022.
Geisler WS. Visual perception and the statistical properties of natural scenes. Annu Rev Psychol 2008;59:167â€“92.
Rieke F, Rudd ME. The challenges natural images pose for visual adaptation. Neuron 2009;64(5):605â€“16.
Fitzgerald JE, Clark DA. Nonlinear circuits for naturalistic visual motion estimation. Elife 2015;4:e09123.
Clark DA, Fitzgerald JE, Ales JM, Gohl DM, Silies MA, Norcia AM, et al. Flies and humans share a motion estimation strategy that exploits natural scene statistics. Nature Neurosci 2014;17(2):296â€“303.
Carandini M, Heeger DJ. Normalization as a canonical neural computation. Nat Rev Neurosci 2011;13:51â€“62.
Heeger DJ. Normalization of cell responses in cat striate cortex. Visual Neurosci 1992;9:181â€“97.
Olsen SR, Bhandawat V, Wilson RI. Divisive normalization in olfactory population codes. Neuron 2010;66:287â€“99.
Rabinowitz NC, Willmore BDB, Schnupp JWH, King AJ. Contrast gain control in auditory cortex. Neuron 2011;70(6):1178â€“91.
Barnett PD, Nordstrom K, Oâ€™Carroll DC. Motion adaptation and the velocity coding of natural scenes. Curr Biol 2010;20:994â€“9.
Fu Q, Yue S. Bioinspired contrast vision computation for robust motion estima- tion against natural signals. In: IEEE the international joint conference on neural networks. 2021.
Li Z, Fu Q, Li H, Yue S, Peng J. Dynamic signal suppression increases the fidelity of looming perception against input variability. In: IEEE the international joint conference on neural networks. 2022.
Wang H, Peng J, Zheng X, Yue S. A robust visual system for small target motion detection against cluttered moving backgrounds. IEEE Trans Neural Netw Learn Syst 2020;31(3):839â€“53.



Yue S, Rind FC. Collision detection in complex dynamic scenes using an LGMD- based visual neural network with feature enhancement. IEEE Trans Neural Netw 2006;17(3):705â€“16.
Brinkworth RSA, Oâ€™Carroll DC. Robust models for optic flow coding in natural scenes inspired by insect biology. PLoS Comput Biol 2009;5(11).
Cha Y-J, Choi W, Suh G, Mahmoudkhani S. Autonomous structural visual Inspection Using Region-based deep learning for detecting multiple damage types. Comput-Aided Civ Infrastruct Eng 2018;33:731â€“47.
Cha Y-J, Choi W. Deep learning-based crack damage detection using convolu- tional neural networks. Comput-Aided Civ Infrastruct Eng 2017;32:361â€“78.
Gallego G, Delbruck T, Orchard G, Bartolozzi C, Taba B, Censi A, et al. Event-based vision: A survey. IEEE Trans Pattern Anal Mach Intell 2020.
Liu P, Huda MN, Sun L, Yu H. A survey on underactuated robotic systems: Bio-inspiration, trajectory planning and control. Mechatronics 2020;72:102443.
Liu P, Neumann G, Fu Q, Pearson S, Yu H. Energy-efficient design and control of a vibro-driven robot. In: Proceedings of the 2018 IEEE/RSJ international conference on intelligent robots and systems. IROS, IEEE; 2018, p. 1464â€“9.
