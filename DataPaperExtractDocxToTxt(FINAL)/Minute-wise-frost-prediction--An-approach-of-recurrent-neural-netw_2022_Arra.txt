Array 14 (2022) 100158










Minute-wise frost prediction: An approach of recurrent neural networks
Ian Zhou a,b,‚àó, Justin Lipman a,b, Mehran Abolhasan a, Negin Shariati a,b
a University of Technology Sydney, Australia
b Food Agility CRC Ltd, 81 Broadway, Ultimo, NSW, 2007, Australia


A R T I C L E  I N F O	A B S T R A C T

	

Keywords:
Frost prediction Internet of Things Machine learning
Recurrent neural network Temporal prediction
Frost events incur substantial economic losses to farmers. These events could induce damage to plants and crops by damaging the cells. In this article, a recurrent neural network-based method, automating the frost prediction process, is proposed. The recurrent neural network-based models leveraged in this article include the standard recurrent neural network, long short-term memory, and gated recurrent unit. The proposed method aims to increase the prediction frequency from once per 12‚Äì24 h for the next day or night events to minute- wise predictions for the next hour events. To achieve this goal, datasets from NSW and ACT of Australia are obtained. The experiments are designed considering the scene of deploying the model to the Internet of Things systems. Factors such as model processing speed, long-term error and data availability are reviewed. After model construction, there are three experiments. The first experiment tests the errors between different model types. The second and third experiments test the effect of sequence length on error and performance for recurrent neural network-based models. All tests introduce artificial neural network models as the baseline. Also, all tests for model error are conducted in two rounds with testing datasets from the current year (2016) and next year (2017). As a result, recurrent neural network-based models are more suitable for short-term deployment with a smaller sequence length. In contrast, artificial neural network models demonstrate a lower error over the long term with faster processing time. With the results presented, the limitations of the proposed method are discussed.





Introduction

In the field of agriculture, frosts occur when ice crystals are formed within the plants and damage the cells [1]. As a result, frost could cause significant losses on the economy and ecosystem [2]. Currently, there are many active and real-time protection methods against frost, including heaters, sprinklers, artificial fog, and air disturbance tech- nologies [3]. However, the frost prediction methods automating the activation of these protection methods can still be improved [3].
This article focuses on predicting the condition of future frost damage to plants. The potential of Recurrent Neural Networks (RNNs) in frost prediction is explored in this article. RNNs are a special form of artificial neural network (ANN) with recurrent connections, which provide the capability to recognize sequential patterns [4]. RNNs are different from the basic ANN that only accepts one input at a time. RNNs can accept several inputs in a sequence. In terms of time-series data, individual data points are processed at once in the sequence of time [4]. The output of the current time state is generated from the input of the current time state and the output of the previous time state, recursively [5]. The standard RNN has issues such as gradient explosion
and gradient vanishing [4]. To address these issues, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are proposed as variants of the RNN [5]. This article leverages RNN, LSTM, and GRU models for frost prediction.
In recent years, the Internet of Things (IoT) technologies have been widely applied in the field of agriculture to provide real-time moni- toring and actuation services [6]. There are also a few IoT-based frost protection systems. However, most of these frost protection systems rely on thresholds of real-time sensor readings to trigger the frost pro- tection equipment [3]. The effect of these simple mechanisms is limited compared to the accuracy of the prediction algorithms [3]. Therefore, this article considers a few factors related to the future deployment of frost prediction algorithms. These factors include model processing speed, long-term accuracy and data availability. Since system resources are limited for IoT systems, the model should require a faster pro- cessing speed [7]. Also, IoT systems should eliminate extra human interventions [6]. Therefore, the deterioration of model accuracy over time should be minimum to ensure manual updates to IoT nodes are infrequent. Finally, as most frost prediction models depend on on-site

‚àó Corresponding author at: University of Technology Sydney, Australia.
E-mail addresses: ian.zhou@student.uts.edu.au (I. Zhou), justin.lipman@uts.edu.au (J. Lipman), mehran.abolhasan@uts.edu.au (M. Abolhasan), negin.shariati@uts.edu.au (N. Shariati).
https://doi.org/10.1016/j.array.2022.100158
Received 7 October 2021; Received in revised form 4 March 2022; Accepted 1 April 2022
Available online 9 April 2022
2590-0056/¬© 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).



historical data [3], data availability is important when creating these models. Since a large amount of historical data cannot be assumed to be available at all sites, our set scene assumes that only a small amount (three months) of data is available to minimize the data collection time for new models.

Related work

In [3], frost prediction methods are categorized as ‚Äò‚Äòclassification methods‚Äô‚Äô and ‚Äò‚Äòregression methods‚Äô‚Äô. Classification methods predict the occurrence of frost as a percentage in a future time, whereas regression methods predict the minimum temperature in a future period [3]. Both methods rely on climate data as the model input. Since the frost resis- tances for different species of plants are different [8], frost regression prediction methods are proposed in this article to provide the farmers future environmental insights and provide a more generalized solution avoiding the differences between individual plant species.
There are a few existing frost regression prediction methods. In [9‚Äì 11], traditional machine learning methods are leveraged to predict temperature or minimum temperature in the next day or night. Random forest models are used in [11] to predict next-day minimum tempera- ture with temperature and humidity inputs. Linear regression is used in both [9] and [10]. Environmental parameters such as temperature, dew point, and humidity are inserted as model inputs in [10]. On the other hand, to consider the effect of wind machines, the authors of [9] introduced the distance to wind machines along with elevation, time of local sunset, and radiation received during the previous day as input parameters.
Apart from the traditional machine learning models. ANN with fully connected layers, as a deep learning model, can also predict future minimum temperatures [12‚Äì14]. Models in [12‚Äì14], predict minimum temperature in the next 12‚Äì24 h as a numerical value. These three works all implement prediction models with air temperature, relative humidity, precipitation, wind direction and speed. However, [12] also includes daytime length, daytime maximum and minimum temperature to support night temperature predictions with a daytime baseline. In [13], precipitation, cloud cover, moisture, and pressure are included as model inputs. The authors also considered humidity and wind ve- locity at 19:00. The authors of [14] predicted next day minimum temperature with fewer input parameters, but introduced radiation to build their prediction ANN.
The above machine learning and deep learning models all predict frost conditions in the next 12‚Äì24 h [9‚Äì14]. Therefore, in extreme conditions, protection equipment might need to be switched on for 12‚Äì24 h to ensure zero frost damage when solely considering model predictions. However, by constant manual observations, the opera- tional time of protection equipment could be reduced [3]. Hence, to reduce the operational time automatically, the major aim of this article is to implement minute-wise next hour minimum temperature predic- tion for frost prediction. Also, as mentioned in the above paragraphs,
the potential of RNN-based models (RNN, LSTM, GRU) are explored
Methodology

This section explains the methodology and settings of the experi- ments. The study area is firstly explained, followed by the data pre- processing procedures. Then, model construction and testing leveraging the preprocessed data are described. Finally, the experiment processes are summarized.

Study area

The study area is located in New South Wales and Australian Capital Territory of Australia. In the study area, datasets from 30 different weather stations are obtained. Fig. 1 is a map of the study area with the weather station locations and IDs. Also, a list of location coordinates of the weather stations are presented by Tables A.7 and A.8 in the Appendix [15]. All these raw datasets used in this work can be acquired from the public weather station directory service hosted by the Bureau of Meteorology (BOM) of Australia [15].
This study is focused on the months June, July, and August of winter [16,17]. Minute-wise climate data of these winter months in years 2016 and 2017 are extracted from the 30 weather stations in the study area. After extracting these data, they are further processed to be prepared for model construction and testing. These procedures are discussed in the next subsection.

Data preprocessing

The raw data needs to be preprocessed before model construction. After extracting the required columns (Timestamp, Air Temperature, Dew Point, Relative Humidity, Wind Speed, Wind Direction) from the raw datasets, seven more steps of data preprocessing are performed. These seven steps belong to two phases. In phase 1, the raw data is processed to transform useful features and handle empty values. The outputs of phase 1 are saved for reuse in the next phase. In phase 2, the outputs from phase 1 are inputted and modified to fit a specific target model and model setting. As an example, Fig. 2 outlines the changes of data structure during data preprocessing, using an RNN with a sequence length of 20 as an example. The red figures indicate a change to that data in that step.
Phase 1 consists of five steps and starts with transforming wind speed and direction into features of ‚Äò‚ÄòN-wind‚Äô‚Äô and ‚Äò‚ÄòE-wind‚Äô‚Äô. These two new features represent wind speeds towards the north (N-wind) and east (E-wind) directions. This transformation is done to prevent the possible errors generated by the wind direction values (0‚Äì359) [18]. The original wind direction data from BOM is the bearing of the direc-
original wind direction (ùëöùëíùë°) is changed to the wind blowing direction tion the wind is originated [19]. Therefore, to begin the conversion, the (ùëëùëíùëî) by reversing the direction (Eq. (1)).
{
ùëöùëíùë° + 180 ,  if ùëöùëíùë° < 180

to solve this prediction problem. The performance of different RNN- based models is also compared in this article. In conclusion, the major
ùëëùëíùëî =
(1)
ùëöùëíùë° ‚àí 180‚ó¶, if ùëöùëíùë° ‚â• 180‚ó¶

contributions of this article are presented as follows.
Application of an RNN-based frost prediction method.
Increasing the prediction frequency from once per 12‚Äì24 h for the next day or night events to minute-wise predictions for the next hour events.
Evaluated the limitations of RNN-based frost prediction.
The rest of this paper is arranged as follows. Section 2 describes the methodology and experiment settings with the study area, data pro- cessing procedures, and experiments. Experiments include comparing temperature prediction models (ANN, RNN, LSTM, GRU), analyzing dif- ferent RNN model settings, and the performance of predicting minimum temperature with other frost-related parameters. Then, the experimen- tal results are discussed in Section 3 and lead towards limitations with open challenges. In the end, Section 4 concludes the whole article.
Then, with the wind speed (ùë£) and the wind blowing direction
(ùëëùëíùëî), magnitudes of wind vector towards north (ùë£ùëÅ ) and east (ùë£ùê∏ ) are
obtained through Eq. (2) [18].
ùë£ùê∏ , ùë£ùëÅ = ùë£ √ó sin(ùëëùëíùëî), ùë£ √ó cos(ùëëùëíùëî)	(2)
In the second step of data preprocessing, the minimum temperature is constructed as the prediction target. The minimum temperature is simply constructed by getting the minimum temperature value in the next 60 min. As the recording of data is done once per minute, the minimum temperature of the current data entry is the minimum value of the next 60 temperature values.
To fit the data structure for RNN, LSTM, and GRU models, sequences of data are created in step 3. A sequence contains all the features (Air Temperature, Dew Point, Relative Humidity, Wind Speed, Wind Direction). For every data entry, the current sequence is defined as




/ig. 1. Weather stations with ID.


sequence ùë°, the sequence from one minute before is added to the current entry and defined as sequence ùë° ‚àí 1. Similarly, the sequence from two minutes before is added and defined as sequence ùë° ‚àí 2. Since the
maximum sequence length of the experiments is 120, sequences from previous entries are added to the current entry from one minute before until 119 min before. Therefore, each entry includes 120 sequences
from sequence ùë° ‚àí 119 to sequence ùë°.
The timestamp column is a tool to help extract the data from the desired time period. Now, as all of the features and sequences are generated, the timestamp column can be removed in step 4. Then, in the final step of phase 1, the listwise deletion data imputation technique is applied to eliminate data entries with empty features [20]. This also includes the removal of data sequences with missing features to preserve the time differences between observations. The final product of phase 1 is output to hard disk to be used in phase 2. For every weather station dataset, the steps in phase 1 are conducted for data in years 2016 and 2017.
In phase 2 of data preprocessing, the results from phase 1 are trans- formed to the required form for different models with different settings. After reading an output from phase 1, step 6 of data preprocessing removes excess data sequences. For ANN models, only one sequence
is required. Therefore, sequences ùë° ‚àí 119 to ùë° ‚àí 2 should be removed.
For RNN, LSTM, and GRU models, sequences are removed according to the sequence length of the target model. For example, with a desired
sequence length of 20, sequences ùë° ‚àí 119 to ùë° ‚àí 20 are removed, leaving
only 20 data sequences (Fig. 2).
Step 7 of data preprocessing is only executed to prepare the data for RNNs and their variants. Every entry in the dataset is converted to a 2D structure. Each row of this 2D structure represents a sequence of a specific time. The rows are structured top to bottom from an earlier time to a more recent time. At this stage, the data can proceed to model construction.
Table 1 describes the features at the end of data preprocessing. Most features demonstrate a normal distribution. The relative humidity is the only skewed-left feature. Table 2 shows the Pearson correlation matrix of the processed features. Dew point and relative humidity show strong correlations to temperature. Other features are relatively independent of each other.
Model construction and testing

In this subsection, information of model construction and testing is provided. The computation environment is firstly described, followed by the usage of datasets. Next, the model structures and hyperparam- eters are clarified. Finally, the two stages of model construction are revealed.
All models in this article are constructed on a desktop computer using an Intel i7-8700K 3.70 GHz processor, equipped with 32 GB RAM. The graphics processing unit is a Nvidia RTX 2080 graphics card. The deep learning framework for model construction and testing is TensorFlow 2.3.0.
The data preprocessing processes generate two datasets from 2016 and 2017 for every weather station. In the experiment scene, models should be built in the current year and deployed in the next year. The year 2016 has been set as the ‚Äò‚Äòcurrent‚Äô‚Äô year and 2017 is the ‚Äò‚Äònext‚Äô‚Äô year. Therefore, datasets from 2016 are used for model construction and datasets from 2017 are used for final testing.
It is assumed that during the process of model construction only the ‚Äò‚Äòcurrent‚Äô‚Äô year data is available. The datasets from year 2016 are split for model training, validation, and testing. For every dataset from different weather stations, 80% of the data are randomly allocated to the training dataset and the other 20% to the testing dataset. From the training dataset, a further split of 20% of the data forms the validation dataset. The training dataset, which contains most of the data, is used to fit the parameters of the models. During the training process, the validation dataset helps to tune the hyperparameters. After all training, hyperparameter tuning, and validation are completed, test predictions are conducted by the model with the testing dataset. This provides an error metric of the model. However, as the model should be deployed in the ‚Äò‚Äònext‚Äô‚Äô year, datasets from 2017 are used as extra and final testing datasets. In the experiments, the errors generated from the 2017 testing datasets are also compared.
The model structure for all models is defined as a three-layer model. The first layer has five neurons. The second has seven and the output layer has one neuron. The first two layers change according to the target model. For example, if it is an ANN model, the cells in these two layers are ANN cells with Rectified Linear Unit (ReLU) activation functions. For RNN and its variants, the cells use the tanh activation




/ig. 2. Data preprocessing steps. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)


functions with relevant cells to the models. Also, for the first layer of RNN, LSTM, and GRU models, the hidden state of the cells is output as sequential inputs of the second layer. The third layer only consists of a single linear cell to output the result.
Adam is the optimizer used during training. Learning rate, ùõΩ1,
ùõΩ2, and ùúñ, are the hyperparameters required for Adam [21]. In the
experiments, learning rate is set to the ‚Äò‚Äògood default settings‚Äô‚Äô as 0.001,
ùõΩ1 as 0.9, ùõΩ2 as 0.999, and ùúñ as 10‚àí7. Another hyperparameter, batch
size is configured to 64. With these settings, all models are trained for
100 epochs with a Mean-Square Error (MSE) loss function.
Models are constructed in two groups. The groups are separated by different model structures and settings. In the first group, one ANN model is constructed for each of the 30 weather stations. In the second group, RNN, LSTM, and GRU models with different sequence lengths


Table 1
Description of the processed training features.


Table 2
Pearson correlation matrix of the processed training features.


are trained for all 30 weather stations. There are 6 different sequence length settings (20, 40, 60, 80, 100, 120). Altogether, the second group outputs 540 models. Overall, there are 570 models constructed for this article. The usage of these models in experiments is explained in the next subsection.

Experiments

There are three experiments conducted to test the model error and performance of models. In the first experiment, the errors of different model types (ANN, RNN, LSTM, GRU) are compared. ANN models from the first model group are the baseline of this experiment. For each weather station, there are eight conducted tests to measure the losses from the current year and next year data for the four model types. RNN models and their variants are tested with a sequence length of 120. Then, in the second experiment, the effect of the sequence length of RNN, LSTM, and GRU models are compared. Additional to the results obtained in the first experiment, 30 more tests are conducted for each weather station to obtain the results of the three RNN-based model types with five sequence number settings, and tested with the current year and next year testing datasets. Model errors of all RNN- based model types are compared with different sequence length settings against the ANN baseline. Similarly, in the final experiment, the train- ing time and inference time of different sequence length settings are compared against the ANN baseline.

Results and discussions

To validate and compare different model settings for next hour frost prediction models, three experiments are conducted. The first experiment compares the MSE between ANN and RNN-based model types. Then, in the second experiment, MSEs of RNN-based models with different sequence lengths are assessed. Finally, processing time related factors are also analyzed with different sequence lengths. This provides an overview of different models‚Äô real-time computation abilities.

Model error

In this experiment, the model errors of RNN-based models with a sequence length of 120 are evaluated with ANN models. Fig. 3 shows that when testing with testing datasets derived from the same year when the training datasets is collected, LSTM seems to perform with the best accuracy with the lowest MSE loss. LSTM is also the only RNN- based model type to exceed the accuracy of ANN models. This result is
also confirmed with one-sided paired T-tests. From the ùëù-values (RNN:
0.1544; LSTM: 6.1225e‚àí12; GRU: 0.2644), LSTM is the only model
type that the ùëù-value is smaller than the 0.05 ùõº value. This means the
null hypothesis is rejected and ANN is likely to produce outputs with a greater loss than LSTM. LSTM models have the highest accuracy among other RNN-based models due to the extra gates to memorize sequence patterns [5].
An assumption made for the models of this paper is that the models are constructed using data from ‚Äò‚Äòthis‚Äô‚Äô year and deployed in the ‚Äò‚Äònext‚Äô‚Äô year. Therefore, models are also tested with testing datasets obtained one year after the training datasets. The results are significantly dif- ferent compare to the results from the ‚Äò‚Äòcurrent‚Äô‚Äô year testing datasets (Fig. 3). ANN shows the lowest MSE loss, which indicates the highest accuracy. On the other hand, LSTM models have the lowest accuracy. The results of the one-sided paired T-tests demonstrate that LSTM and
GRU models have ùëù-values (RNN: 0.1350; LSTM: 0432; GRU: 0.0027)
less than the ùõº threshold. Therefore, it is likely that LSTM and GRU
models all have a significantly higher loss (less accurate) than ANN
models.
As all models are trained with the current year data, LSTM models with more parameters and gates [22] fit closer to the current year testing datasets. On the other hand, RNN-based models are constructed through learning the sequence patterns [4,5]. Thus, these models are sensitive to the change of sequence patterns. In [23,24], the global climate change induces an increase of instability in weather patterns over time. As a result, the accuracy of RNN-based models deteriorates when tested with the next year testing datasets. Compared to the baseline, LSTM and GRU models with more parameters [22] tend to ‚Äò‚Äòoverfit‚Äô‚Äô more to the current year pattern and are vulnerable to the changed next year pattern. However, the exact extent of accuracy reduction is unknown as the change of climate patterns in the future is also unknown.

Effect of sequence length on model error

In this experiment, the effect of sequence length on model error for RNN-based models is inspected. Fig. 4 shows the average MSEs of RNN- based models tested by current year datasets. Overall, the increase of sequence length does not reduce the average loss. Only LSTM shows a decreasing trend of losses with the increase of sequence length. However, this change is not very significant. When compared to ANN, LSTM and GRU with some settings (sequence length = 20, 40, 80, 100)
the one-sided paired T-tests results on Table 3. Only ùëù-values of LSTM, seem to have a smaller loss than ANN (0.4550). This is confirmed by
smaller than the ùõº. This reveals that when testing with the current year and GRU models with the sequence lengths of 20, 40, 80, and 100 are
datasets. This demonstrates the higher likelihood that LSTM and GRU (sequence length = 20, 40, 80, 100) models are more accurate than ANN models when tested with current year testing datasets.


	


/ig. 3. Average MSE tested with current and next year datasets.



/ig. 4. Average MSE tested with current year datasets for different sequence lengths.





Table 3
ùëÉ -values of comparing average MSEs between ANN and RNN-based models with
different model sequence lengths (current year).




Fig. 5 is the average MSEs for RNN-based models with different sequence lengths tested by the next year testing datasets. Similar to the reverse of results in Experiment 1, all RNN-based models have a
higher loss than the ANN MSE (0.7813). Table 4 is the ùëù-values obtained
from one-sided paired T-tests with an alternative hypothesis that each
hypothesis is in favor of LSTM and GRU models as their ùëù-values are less tested model has a greater MSE than the ANN baseline. The alternative than ùõº. This means LSTM and GRU models are likely to perform with
higher errors than ANN models in the next year. Also, as explained in Experiment 1, the change of climate patterns in the future is unknown. This could be the reason of the additional noise in Fig. 5, compared to Fig. 4.

/ig. 5. Average MSE tested with next year datasets for different sequence lengths.


Table 4
ùëÉ -values of comparing average MSEs between ANN and RNN-based models with
different model sequence lengths (next year).



/ig. 6. Average training time per epoch for different sequence lengths.



Effect of sequence length on processing time

In the final experiment, the training time per epoch and inference time per input are tested and compared between ANN models and RNN- based models with different sequence lengths. Fig. 6 demonstrates that the training time per epoch for RNN, LSTM, and GRU models increases as the sequence length increases. This is due to the additional param- eters for training as the sequence length increases. LSTM and GRU models have overlapping curves of training times. RNN training times are greater than the prior two models. This is because, in TensorFlow 2.3.0, only LSTM and GRU model training processes are optimized by cuDNN for faster training speeds [25,26]. However, whether optimized
or not, compared to the ANN training time of 1.070 s, as the ùëù-values
are smaller than the 0.05 ùõº, Table 5 indicates that the training time of
ANN models per epoch is more likely to be smaller than all settings of
RNN-based models.



Table 5
ùëÉ -values of comparing average training time per epoch between ANN and RNN-based
models with different model sequence lengths.



/ig. 7. Average inference time per input for different sequence lengths.


Table 6
ùëÉ -values of comparing average inference time per input between ANN and RNN-based
models with different model sequence lengths.
model deployed in the short term. In Experiment 2, RNN-based models do not present a significant reduction in model error as the sequence length increases. Therefore, considering the results from Experiment 3, RNN-based models with a short sequence can be deployed for short times to provide predictions with higher accuracy and performance. On the other hand, ANN models can be deployed over longer time spans without much accuracy deterioration. Both training and inference time is significantly lower than RNN-based models. Overall, ANNs might still be more suitable than RNN-based models for frost prediction in a long- term scenario with minimal system maintenance and update because of their higher accuracy and performance over the long term. After constructing the above different frost prediction models and analyzing their performance, the limitations presented in this subsection are discovered. Limitations create new challenges and lead towards future directions to frost prediction research.

Model accuracy requirements are not specified
Different plants have different frost tolerance and sensitivity [8]. Therefore, model accuracy requirements may vary between different plants. There are many studies on plant frost tolerance. However, the sensitivity of plants to individual frost factors is not fully revealed [28]. As a future direction, the sensitivity to different frost factors such as temperature, humidity, dew point, cloud coverage, solar radiation and wind speed should be tested with high precision for individual species of plants in controlled environments. With enough plant species studied, a model accuracy threshold can be set for frost prediction models.

Lack of standard datasets
The models and experiment results are obtained from a public dataset that represents local climate conditions of our study sites in Aus-

	  tralia. Therefore, the accuracy of the models at other sites with different
climate patterns is questionable. To the best of our knowledge, prior research has been based upon locally obtained private datasets [9‚Äì 14]. As a result, model results could be biased. There is a need of a standard dataset for frost prediction models that includes data entries from different locations.




Fig. 7 demonstrates the average inference time per input for RNN- based models with different sequence lengths. There is a trend of increase in inference time, along with the increase of sequence length. Similar to training time, a higher sequence length of RNN-based models implies a larger input sequence and a model structure with more pa- rameters. Thus, the inference time increases with the sequence length. Also, the inference time for all RNN-based settings is significantly larger
than ANN inference time (4.5214e‚àí4 s). This statement is supported
with the one-sided paired T-test results as all ùëù-values are less than the
ùõº (Table 6).
Limitations and open challenges

Experiment 1 shows that RNN-based models have lower model errors than ANN models when tested with current year datasets. LSTM models have the lowest errors and highest accuracy. However, RNN- based models‚Äô accuracy declines and is exceeded by ANN when tested with next year datasets. This decline is likely to be caused by climate pattern change over time [4,5,23,24]. Also, the models in this article are constructed with one year of data. From [27], RNN models often require data from more years to fully learn the seasonality patterns. However, this would increase the dependency on historical data, which is a limitation mentioned in the next subsection. With both sources of accuracy deterioration, RNN-based models are only suitable for a
The RNN input data format affects the system energy efficiency
The RNN-based models in this article require a sequence of climate data as model inputs. Each sequence of data contains sensor readings collected every minute over a long time span. This places a restriction on node duty cycles. Sensor readings must be collected per minute to satisfy the model input. For common agricultural IoT systems, sensor readings are transmitted through the radio from a node to a central processing node for further processing and data analytics [29]. In [30], radio power consumption increases as the time interval of radio trans- mission reports decreases. Even radio transmissions between 10-minute periods consume a significant amount of the system energy [30]. Therefore, if sensor readings are reported every minute, higher energy consumption will be placed on the whole system. This issue could be mitigated by aggregating a few minute-wise sensor readings into one radio transmission. Also, inference on edge with edge computing can also reduce the number of radio transmissions. A model can be deployed on edge devices and only transmit the predicted outcome when it triggers a preset condition. However, even there are mitigation plans, RNN-based models still limit the design of IoT systems with potential high energy impact. On the other hand, each ANN model inference only requires a single set of climate data as input. ANN- based frost prediction models can be applied to systems with different time intervals to obtain sensor data. Radio transmission intervals can be adjusted according to system requirements. Therefore, ANN models may still be more suitable for frost prediction systems.



Models depend on previous year data
All models constructed for this article depend on previous years of data. Therefore, model accuracy is dependent on the quality of historical data. This is a limitation on all machine learning models [31]. As more recent researches are site-specific, sites without any record of historical climate data require an IoT data collection system to be deployed for data collection. To ensure similar performance to the results of this article, the system must be deployed at least a year prior to produce a prediction model and provide frost prediction services. This increases the deployment time of the system. A possible solution is to explore the performance of models based on previous months of data. This solution could substantially reduce development time. However, it cannot eliminate this excess system development time for data collection. Methods eliminating the requirement of on-site historical data need to be developed. Another possible direction is to explore the generalization or transfer of models to similar locations.

Lack of stopping conditions
The frost prediction models constructed in this article only predict the start of a frost event with real-time sensor readings. This predic- tion could be the trigger of a frost protection mechanism. However, there are limited mentions of predicting the end of a frost event to switch off any protection mechanisms. As most frost protection systems rely only on a single sensor node [3], activation of a nearby protection mechanism might affect the sensor readings and contam- inate the prediction outcomes of frost prediction models. Therefore, future prediction models could be developed to eliminate the effect of frost protection mechanisms. As a possible benefit, frost protection mechanisms could be switched off earlier to reduce the operational cost.

Conclusion

The primary aim is to increase the prediction frequency from once per 12‚Äì24 h for the next day or night events to minute-wise predictions for the next hour events. RNN-based models are selected to learn the sequence pattern of historical data. ANN models are used as a baseline. Datasets from weather stations in the NSW and ACT areas of Australia are obtained. These datasets are recorded during the years 2016 and 2017. With these datasets, it is assumed that our models are built during the year 2016 (current year) and deployed in year 2017 (next year). Therefore, datasets from 2016 are used for model construction and preliminary testing. Datasets from 2017 are used for final testing. After constructing the models, there are three experiments testing the model errors, also the effect of sequence lengths on errors and processing time for RNN-based models. The errors of models is tested with both the current and next year datasets. LSTM seems to have the highest accuracy when tested with the current year testing datasets. However, the accuracy for all RNN-based models reduces when tested with the next year testing datasets. ANN models have the highest accuracy with the next year testing datasets. When testing RNN-based models with different sequence lengths, it seems that sequence lengths cannot affect the accuracy of models significantly. However, training and inference time increases with the sequence length. Therefore, RNN- based models should be used for short-term deployments with a shorter sequence length to ensure accuracy and performance. On the other hand, ANN models demonstrate the lowest error when tested with
different models. Thirdly, the RNN input data format affects the system energy efficiency. The limitations on energy efficiency could be avoided by the application of edge computing or message aggregation. On the other hand, ANN models do not have such limitations. The next limita- tion is related to the construction of models. All of these models depend on previous year data. For sites without data records, this limitation requires extra development and deployment time. Therefore, models that decouple from historical data could be developed to eliminate this restriction completely. The final limitation is that prediction models lack stopping conditions. As most systems read climate data from one sensor node, the activation of protection gear could contaminate the input data. Thus, it contaminates the prediction outcomes. A future model could be developed to remove this effect of data contamination and provide a model-based automatic stopping mechanism to reduce operational costs of frost protection systems.

CRediT authorship contribution statement

Ian Zhou: Conceptualization, Methodology, Software, Formal anal- ysis, Investigation, Data curation, Writing ‚Äì original draft, Visualiza- tion. Justin Lipman: Conceptualization, Supervision, Writing ‚Äì re- view & editing. Mehran Abolhasan: Methodology, Validation, Writing ‚Äì review & editing. Negin Shariati: Resources, Writing ‚Äì review & editing.

Declaration of competing interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgment

We would like to acknowledge the support of Food Agility CRC Ltd, 81 Broadway, Ultimo, NSW, 2007, Australia, funded under the Commonwealth Government CRC Program. The CRC Program sup- ports industry-led collaborations between industry, researchers and the community.
The authors are with Radio Frequency and Communication Tech- nologies (RFCT) research laboratory, Faculty of Engineering and IT, University of Technology Sydney, Ultimo, NSW 2007, Australia.

Appendix. Weather station location coordinates

See Tables A.7 and A.8.


Table A.7
Weather station location coordinates.


Station ID	Latitude (degrees)	Longitude (degrees) 69138	‚àí35.3635	150.4828


accuracy and performance.
There are limitations determined. Firstly, the model accuracy re-



Table A.8
Weather station location coordinates continue.




References

Webb L, Snyder RL. Frost hazard. In: Bobrowsky PT, editor. Encyclopedia of natural hazards. Dordrecht: Springer Netherlands; 2013, p. 363‚Äì6. http://dx.doi. org/10.1007/978-1-4020-4399-4_148.
Ma Q, Huang J-G, H√§nninen H, Berninger F. Divergent trends in the risk of spring frost damage to trees in Europe with recent warming. Global Change Biol 2019;25(1):351‚Äì60. http://dx.doi.org/10.1111/gcb.14479.
Zhou I, Lipman J, Abolhasan M, Shariati N, Lamb DW. Frost monitoring cyber‚Äìphysical system: A survey on prediction and active protection meth- ods. IEEE Internet Things J 2020;7(7):6514‚Äì27. http://dx.doi.org/10.1109/JIOT. 2020.2972936.
Goodfellow I, Bengio Y, Courville A. Sequence modeling: Recurrent and recursive nets. In: Deep learning. MIT Press; 2016, p. 363‚Äì408.
Salehinejad H, Sankar S, Barfett J, Colak E, Valaee S. Recent advances in recurrent neural networks. 2017, arXiv preprint arXiv:1801.01078.
Farooq MS, Riaz S, Abid A, Abid K, Naeem MA. A survey on the role of IoT in agriculture for the implementation of smart farming. IEEE Access 2019;7:156237‚Äì71. http://dx.doi.org/10.1109/ACCESS.2019.2949703.
Sarker VK, Gia TN, Tenhunen H, Westerlund T. Lightweight security algorithms for resource-constrained IoT-based sensor nodes. In: ICC 2020 - 2020 IEEE international conference on communications. 2020. p. 1‚Äì7. Dublin, Ireland. http://dx.doi.org/10.1109/ICC40277.2020.9149359.
di Francescantonio D, Villagra M, Goldstein G, Campanello PI. Drought and frost resistance vary between evergreen and deciduous Atlantic forest canopy trees. Funct Plant Biol 2020. http://dx.doi.org/10.1071/FP19282.
Halley V, Eriksson M, Nunez M. Frost prevention and prediction of temperatures and cooling rates using GIS. Aust Geogr Stud 2003;41(3):287‚Äì302. http://dx.doi. org/10.1046/j.1467-8470.2003.00235.x.
Iacono LE, V√°zquez Poletti JL, Garc√≠a Garino C, Llorente IM. Performance models for frost prediction in public cloud infrastructures. Comput Inform 2018;37(4):815‚Äì37.
Diedrichs AL, Bromberg F, Dujovne D, Brun-Laguna K, Watteyne T. Prediction of frost events using machine learning and IoT sensing devices. IEEE Internet Things J 2018;5(6):4589‚Äì97. http://dx.doi.org/10.1109/JIOT.2018.2867333.
Ghielmi L, Eccel E. Descriptive models and artificial neural networks for spring frost prediction in an agricultural mountain area. Comput Electron Agric 2006;54(2):101‚Äì14. http://dx.doi.org/10.1016/j.compag.2006.09.001.
Zeng W, Zhang Z, Gao C. A Levenberg-Marquardt neural network model with rough set for protecting citrus from frost damage. In: Proc. 2012 eighth international conference on semantics, knowledge and grids. 2012. p. 193‚Äì196. Beijing, China. http://dx.doi.org/10.1109/SKG.2012.4.
Fuentes M, Campos C, Garc√≠a-Loyola S. Application of artificial neural networks to frost detection in central Chile using the next day minimum air temperature forecast. Chil J Agric Res 2018;78(3):327‚Äì38. http://dx.doi.org/10.4067/S0718- 58392018000300327.
of Meteorology B. Weather Station Directory. Bureau of Meteorology; 2020, URL http://www.bom.gov.au/climate/data/stations/. [Accessed 04 April 202].
of Meteorology B. Australia in winter 2016. 2016, URL http://www.bom.gov.au/ climate/current/season/aus/archive/201608.summary.shtml. [Accessed 04 April 2020].
of Meteorology B. Australia in winter 2017. Bureau of Meteorology; 2017, URL http://www.bom.gov.au/climate/current/season/aus/archive/201708.summary.   shtml. [Accessed 04 April 2020].
Lee M, Moon S, Kim Y, Moon B. Correcting abnormalities in meteorological data by machine learning. In: 2014 IEEE international conference on systems, man, and cybernetics. San Diego, CA, USA; 2014, p. 888‚Äì93. http://dx.doi.org/10. 1109/SMC.2014.6974024.
of Meteorology B. Wind. Bureau of Meteorology; 2020, URL http://www. bom.gov.au/marine/knowledge-centre/reference/wind.shtml. [Accessed 04 April 2020].
Osborne JW. Six: Dealing with missing or incomplete data: Debunking the myth of emptiness. In: Best practices in data cleaning: A complete guide to everything you need to do before and after collecting your data. SAGE Publications, Inc; 2013, p. 105‚Äì38. http://dx.doi.org/10.4135/9781452269948.n6.
Kingma DP, Ba J. Adam: A method for stochastic optimization. 2017, arXiv preprint arXiv:1412.6980.
Chung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. 2014, arXiv preprint arXiv:1412.3555.
Augspurger CK. Reconstructing patterns of temperature, phenology, and frost damage over 124 years: Spring damage risk is increasing. Ecology 2013;94(1):41‚Äì50. http://dx.doi.org/10.1890/12-0200.1.
H√§nninen H. Does climatic warming increase the risk of frost damage in northern trees? Plant Cell Environ 1991;14(5):449‚Äì54. http://dx.doi.org/10.1111/j.1365- 3040.1991.tb01514.x.
Google. tf.keras.layers.LSTM. Google; 2020, URL https://www.tensorflow.org/ api_docs/python/tf/keras/layers/LSTM. [Accessed 12 October 2020].
Google. tf.keras.layers.GRU. Google; 2020, URL https://www.tensorflow.org/api_ docs/python/tf/keras/layers/GRU. [Accessed 12 October 2020].
Muzaffar S, Afshari A. Short-term load forecasts using LSTM networks. En- ergy Procedia 2019;158:2922‚Äì7. http://dx.doi.org/10.1016/j.egypro.2019.01. 952, Innovative Solutions for Energy Transitions.
Barlow KM, Christy BP, O‚ÄôLeary GJ, Riffkin PA, Nuttall JG. Simulating the impact of extreme heat and frost events on wheat crop production: A review. Field Crops Res 2015;171:109‚Äì19. http://dx.doi.org/10.1016/j.fcr.2014.11.010.
Deepika G, Rajapirian P. Wireless sensor network in precision agriculture: A survey. In: 2016 international conference on emerging trends in engineering, technology and science. 2016. p. 1‚Äì4. Pudukkottai, India. http://dx.doi.org/10. 1109/ICETETS.2016.7603070.
Martinez B, Mont√≥n M, Vilajosana I, Prades JD. The power of models: Modeling power consumption for IoT devices. IEEE Sens J 2015;15(10):5777‚Äì89. http:
//dx.doi.org/10.1109/JSEN.2015.2445094.
Mohammadi M, Al-Fuqaha A, Sorour S, Guizani M. Deep learning for IoT big data and streaming analytics: A survey. IEEE Commun Surv Tutor 2018;20(4):2923‚Äì60. http://dx.doi.org/10.1109/COMST.2018.2844341.
