Available online at www.sciencedirect.com



AASRI Procedia 3 (2012) 375 – 380




2012 AASRI Conference on Modelling, Identification and Control
Hidden Node Optimization for Extreme Learning Machine
Yan-wei Huang*, Da-hu Lai
College of Electrical Engineering & Automation, Fuzhou University, Fuzhou, Fujian, China




Abstract

The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.

© 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and/or peer review under responsibility of American Applied Science Research Institute

Keywords: ELM, VC confidence, structural risk, hidden nodes, PSO;


Introduction

Extreme Learning Machine(ELM)[1], a novel learning algorithm with fast learning speed and good generalization, is a single-hidden-layer feed forward neural networks(SLFNs). As traditional SLFNs, ELM tends to achieve a good generalization with the suitable hidden nodes. EM-ELM[2] pointed that how to choose the optimal number of hidden nodes is still unknown and important. RCGA-ELM[3] employs genetic algorithm (GA) to optimize the number of hidden nodes, input weights(w) and bias(b) in five-fold cross- validation procedures. It is so complex and time-consuming by involving multiple operators. PSO-ELM[4] optimizes only the weight w and bias b, illustrating their value ranges, Root mean square error(RMSE) as the


* *HUANG Yan-Wei.Tel: 059122866596; fax: ;
E-mail Address: sjtu_huanghao@sina.com








2212-6716 © 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and/or peer review under responsibility of American Applied Science Research Institute doi:10.1016/j.aasri.2012.11.059


optimization goals and variance(δRMSE) as the iterative stop criteria. ICGA-SRM-ELM[5] only considers RMSE and hidden nodes, doesn't give a specific form of the solution. Miche[6] presented OP-ELM to choose hidden nodes automatically. However, it needs Multi-response Sparse Regression and Leave-One-Out algorithms to get rid of the useless neurons of the hidden layer, which has to employ multi-certeria mechanism to increase or decrease the hidden node, tending to be more complex. µG-ELM[7] as another solution to optimize the hidden nodes by GA is easy to overfit with RMSE as the only training goal. Those previous works have made some attempts to improve generalization of ELM, but existed some shortcomings like overfitting, heavy time consumption or without specific form of the objective function.
SRM-ELM (Structural Risk Minimization ELM, SRM-ELM) algorithm is proposed in this work to obtain an optimal number of hidden nodes for ELM by PSO, with Structural Risk Minimization (SRM) principle that consist of empirical risk and VC confidence. The most superiority of SRM-ELM is to avoid the overfitting by introduced the VC theory. Moreover, PSO chosen as the optimal tool will reduce the operation time compared with GA or DE (Differential Evolution) algorithm.

Basic of ELM

Given N learning samples [xi,yi], where xi=[xi1,xi2,…,xin]T  Rn, yi=[ti1,ti2,…,tim]T  Rm and i=1,2,…,N, the SLFN is constructed with L hidden notes and the activation function g(w,x,b). ELM modeling that is looking for a function f(x) to obtain the right output y with respect to x (outside of the samples) after training network with [xi,yi]. f(x) is defined as[8] ,



y  f (x)  i g(wi , x,bi )
i 1
(1)

where βi=[βi1,βi2,…,βim]T is the output weights vector connecting the ith hidden node and the output nodes, wi=[wi1,wi2,…,win]T is the input weights vector connecting the ith hidden node and the input nodes, bi is the threshold of the ith hidden node, i=1,2,…,L.
According to N samples [xi,yi], Eq.(1) can be further written as
Hβ=Y	(2)
where

 g (w , x , b )  ⋯
g(w , x , b ) 
  T 
 y T 

			
 1 
(3)

H(w1 ,…, wL , b1 ,…, bL , x1 ,…, xN )  	⋯	…
⋯		   ⋯ 	Y   ⋯ 

  T 	 y T 

g (w1 , xN , b1 ) ⋯
g (wL , xN , bL ) N L
  L  L  m
 N  N  m

As in [8], H is the hidden layer output matrix of the neural network. For L«N, H is a non-square matrix, given any w and b, H- can be obtained according to Moore-Penrose generalized inverse theorem, β is
β=H-Y	(4)
With Eq.(4), β can be gained after setting parameters of ELM (hidden nodes L, activation function g(x), any data w and b) to accomplish the training of ELM.

Optimization of hidden nodes for ELM

Given the w, b and activation function, the generalization performance mainly relies on hidden nodes. Here, we present SRM-ELM to achieve the optimal number of hidden nodes by PSO based on SRM.

Basic of SRM

Some defects happen to Empirical Risk Minimization (ERM) principle has some disadvantages in machine


learning, so Vapnik presented SRM to improve the generalization performance. Consider a set of finite
function 0  Q(z, )  B     , while the VC-dimension h is limited[9], set Eq.(5) with probability at least 1-η,

R( )  Remp ( ) 

1 
1  4Remp ( ) 
(5)

2 	B	

where Remp(a) is an empirical risk, and
	

 =a  h(ln(bn / h)  1)  ln( / 4)
n


(6)

where n is the sample number, 0  a≤4 0  b≤2, η  (0,1],and B=1 in binary-class issues. The second summand on the right hand side of Eq.(5) is called VC confidence, whose value depends on VC-dimension h in the case of given samples. According to SRM, minimal R(a) will be obtained by minimizing right hand side of Eq.(5).

Modification for SRM

VC-dimension h can be used as a measure of the computational complexity for machine learning. A good VC-dimension can improve the generalization of neural network. But, there is not a universal formula to calculate h for neural network. VC-dimension h was deduced with some formulae for the feedforward network with sigmoid activation function[10-12],

2
lower bound 
2  2
low er bound	upper bound 
(7)

where λ is the number of weights in network, l is the number of layers and n is the sum of hidden nodes and output nodes. Some attention could be took on Eq.(7) that h has some relation to λ, and λ also has some relation to the total number of nodes. So, Eq.(7) suppose h can be obtained from the total number of nodes in network. Here, we consider the total number of nodes as the VC-dimension h for ELM. h can be written as,

h  input _ nodes  hidden _ nodes  output _ nodes
According to statistic learning theory, a=4	b=1 in Eq.(6), then Eq.(6) can be rewritten as,
 =4  h (ln(n / h )  1)  ln( / 4)
(8)

(9)

1

B=1 in Eq.(5), VC confidence in Eq.(5) can be revised as,


f (h)  1 
n

4Remp () 


(10)


Then differentiation to h of Eq.(10) is ,
1  1 	
	1	

	
	

f ' (h)  2ln(n/ h)
  2Remp()   

1
n	


(11)

With Eq.(11), f (h) tends to be maximum value when h=n. Generally, VC confidence should be concave

function. However, we find that Eq.(10) is a convex function with respect to h [0
confidence should be transformed into another form. It is obvious that Eq.(12) is true,
n] in Fig.1. So VC


then Eq.(5) is rewritten as, Eq.(5) can be transformed,
 (h)  e f (h)  f (h)

R( )  Remp ( )  f (h)  Remp ( )  (h)
(12)

(13)





With Eq.(13) and Eq.(14),

so set Eq.(16) with probability at least 1-η,
PR()  Remp ()  ƒ (h) 1-

PR( )  Remp ( )   (h)  0  1-

R( )  Remp ( )  (h)
(14)

(15)

(16)

where η0  (0,1]. We do simulations on binary-class issue Haberman from UCI database to evaluate  (h) in Fig.2. Obviously,  (h) is a concave function, but ƒ (h) is a convex function. So Eq.(17) can be used as the objective function, which consist of Remp and  (h) .

Objectƒun : F ( , h)  Remp ( )   (h)
1	1
(17)




0.5	0.5


0
0	100	200	300
VC-dimension h
0
0	100	200	300
VC-dimension h


Fig.1 Relationship of ƒ(h) and Remp	Fig.2 Relationship of ƒ(h), ф(h) and Remp

PSO algorithm
PSO is used to optimize the objective function Eq.(17). We define the particles position p as the hidden node number L. The update rules for the position p and velocity v are,

vk1    vk  c  rand ( pbestk  pk )  c  rand
(gbestk  pk )
(18)

id	id	1	1
id	id	2	2
d	id

p k +1 = p k
+ v k  1
(19)

where pk
id	id	id 
is the dth position of the ith particle in the kth iteration, and vk is the dth velocity of the ith particle

in the kth iteration. pbestk is the dth best position of the ith particle in the kth iteration, and gbestk is the dth
id	d
best position of the population in the kth iteration. d=1,2,…N. c1 and c2 are the learning factors, usually equal to constant 2. rand1 and rand2 are random number in the range of (0,1).  Formula of Inertia weight τ is,

  
max
  max   min  N itmax
where τmax and τmin represent the maximum and minimum value of inertia weight

respectively. itmax represents the maximum value of iteration, and NC is the current iteration. Particles have a minimum and a maximum velocity, also a minimum and a maximum position, the rules are in PSO-ELM[4].

SRM-ELM Algorithm ƒlow
Fig.3 is the flow of SRM-ELM algorithm. After initialize swarm population, the position value p is the hidden node number L, which is the key link between PSO and ELM. p and v will update in every iteration. If NC  itmax , output the optimal number of hidden nodes, else repeat the flow after initialization.


Experimental Results

Parameters

Parameters setting for PSO: population size: n=50, maximum iteration: itmax=30, learning factors: c1= c2=2, dimension: d=1, the maximum and minimum value of inertia weight: τmax=0.9, τmin=0.4, range of the position(number of hidden nodes): [Xmin,Xmax]=[1,300], range of the velocity: [Vmin,Vmax]= [-(Xmax-Xmin),(Xmax- Xmin)]=[-299,299]. Parameters for ELM: choose the sigmoid for the activation function. Parameters for

objective function F(a,h):a=4,b=1 for Eq.(6), =
, where n is the training sample number.


Result comparisons

All the simulations for SRM-ELM are carried out in Matlab7.0 in environment running in a Pentium(R) Dual-core 3.19GHZ CPU. We apply 6 datasets from UCI database to test the performance of SRM-ELM. Table.1 is the description for 6 datasets.
Fig.3 Flow for SRM-ELM algorithm Table.1 the description for 6 datasets






The classification accuracy and the number of hidden nodes are used to evaluate the performance for SRM- ELM. We also compare SRM-ELM with the other two algorithms, one is the original ELM with 10-fold cross
validation, the other one is using the cut-and-try work N 	 c to select the node number for ELM,
where N is the number of hidden nodes, a is the input nodes, b is the output nodes, and c is the random number in 1~10.
Table.2 is the result comparisons for accuracy and the node number. The optimal number by SRM-ELM drops in the range by ELM, and the accuracy of SRM-ELM are close to ELM's, which indicates that SRM-

ELM is feasible and effective. Moreover, the performance of SRM-ELM is better than the N 
most of the datasets.
 c in


Conclusions

This work proposed a novel algorithm to optimize the number of hidden nodes for ELM by SRM and PSO. We modified the formula for the VC confidence to reconstruct a concave function for SRM as the objective function. Then we employed PSO to optimize the SRM function for the optimal number for hidden nodes for ELM. The experiment results demonstrate that our algorithm can be used to obtain the effective number of hidden nodes and an excellent generalization.
Table.2 Performance Comparison in SRM-ELM, ELM and N 	 c

Note: L is the number of hidden nodes

Acknowledgements

This work is sponsored by Doctoral Fund of Chinese Ministry of Education(20113514120007) and Nature Science Fund of Fujian Province in China(2010J05132).

References

G.-B.Huang, Q.-Y.Zhu, C.-K.Siew. Extreme learning machine: a new learning scheme of feedforward neural networks. Proc of the IEEE International Joint Conference on Neural Networks. 2004; 2: 985–990.
Guorui Feng, GuangBin Huang, Qingping Lin. Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning. IEEE Trans on Neural Networks, 2009; 20(8):1352-1357.
S.Suresh, S.Saraswathi, N.Sundararajan. Performance Enhancement of Extreme Learning Machine for Multi-Category Sparse Cancer Classification. Engineering Applications of Artificial Intelligence, 2010; 23(7): 1149-1157.
You Xu, Yang Shu. Evolutionary Extreme Learning Machine Based on Particle Swarm Optimization. Lecture Notes in Computer Science Advances in Neural Networks, 2006; 3971:64-652.
Saraswathi, S.Sundaram, S.Sundararajan.N. ICGA-PSO-ELM Approach for Multiclass Cancer Classification Resulting Reduced Gene Sets in Which Genes Encoding Secreted Proteins Are Highly Represented inaccurate. IEEE/ACM Trans on Computational Biology and Bioinformatics, 2010; 8(2):452-463.
Yoan Miche, Sorjamaa.A, Bas.P. OP-ELM:Optimally Pruned Extreme Learning Machine. IEEE Trans on Neural Networks, 2010; 21(1):158-162.
Lahoz.D, Lacruz.B, Mateo.P.M. A Bi-objective Micro Genetic Extreme Learning machine. IEEE workshop on Hybrid Intelligent Models And Applications; 2011, 68-75.
GuangBin Huang, QinYu Zhu, CheeKheong Siew. Extreme learning machine: Theory and applications.
Neurocomputing, 2006; 70:489-501
Vladimir N. Vapnik. Statistical Learning Theory. 2nd ed. New York: Wiley; 1998.
Koiran P, Sontag E D. Neural Networks with Quadratic VC Dimension. Journal of Computer and System Sciences. 1997; 54:190-198.
Bartlett P L, Maiorov V, Meir R. Almost Linear VC-Dimension Bounds for Piecewise Polynomial Networks. Neural Computation.1998; 10(8):2159-2173.
Karpinski M, Macintyre A. Polynomial Bounds for VC Dimension of Sigmoidal and General Pfaffian Neural Networks. Journal of Computer and System Sciences.1997; 54:169-176.
