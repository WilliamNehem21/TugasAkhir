Array 6 (2020) 100018

		




Combining clusterings in the belief function framework☆
Feng Li a, Shoumei Li a, Thierry Denœux b,c,*,1
a Beijing University of Technology, College of Applied Sciences, Beijing, China
b Universit´e de Technologie de Compi`egne, CNRS, Heudiasyc (UMR 7253), France
c Institut Universitaire de France, Paris, France



A R T I C L E I N F O

Keywords: Evidence theory Belief functions
Clustering ensemble Intuitionistic fuzzy relation
A B S T R A C T

In this paper, we propose a clustering ensemble method based on Dempster-Shafer Theory. In the first step, base partitions are generated by evidential clustering algorithms such as the evidential c-means or EVCLUS. Base credal partitions are then converted to their relational representations, which are combined by averaging. The combined relational representation is then made transitive using the theory of intuitionistic fuzzy relations. Finally, the consensus solution is obtained by minimizing an error function. Experiments with simulated and real datasets show the good performances of this method.





Introduction

Clustering is an important task in Machine Learning and Pattern Recognition. It is a statistical method to divide objects into groups, in such a way that objects are similar within each group, and dissimilar across different groups. Clustering methods have proved useful in many real-world application domains, such as data mining, image segmenta- tion, etc.
According to the form of clustering output, we can distinguish be- tween hard and soft partitional clustering; the latter includes fuzzy and evidential clustering. In particular, evidential clustering, based on Dempster-Shafer (DS) theory (also called the theory of belief functions) [8,31], has recently attracted the attention of many researchers. Evidential clustering computes a credal partition, which describes cluster membership uncertainty using DS mass functions.
In recent years, several evidential clustering algorithms have been developed. Denœux and Masson (2004) [12] first introduced an evidential relational clustering method called EVCLUS. This method finds a credal partition such that the degree of conflict between the mass functions associated with any two objects match their dissimilarity. Antoine et al. [3] proposed a constrained version of EVCLUS, called CEVCLUS, which utilizes prior information provided as pairwise con- straints. Denœux et al. [14] introduced a faster version of EVCLUS, called k-EVCLUS, where a new cost function is defined and optimized by an
iterative row-wise quadratic programming (IRQP) algorithm. Li et al.
[23] further expanded the k-EVCLUS method by taking prior knowledge into account.
In Ref. [26], Masson and Denœux introduced the evidential c-means algorithm (ECM), which is an extension of the classic and fuzzy c-means in the framework of DS theory. The ECM alternatively searches for the best credal partition and the best prototypes. Masson et al. [27] proposed a variant of ECM for dissimilarity data, called RECM. Antoine et al. [2] introduced a constrained version of ECM (called CECM) by considering prior knowledge. Liu et al. [25] proposed another variant of the ECM algorithm, called CCM, by introducing the notion of meta-cluster. Zhou et al. [43] extended the median c-means and median fuzzy c-means to the Median Evidential c-means (MECM). Denœux et al. (2015) [10] intro- duced a new evidential clustering algorithm (Ek-NNclus) based on the evidential k nearest neighbor rule.
Different clustering algorithms may obtain different clustering results for one dataset, and even a single algorithm with different initializations may yield different solutions. It is generally agreed that there is no best single clustering algorithm [1]. To solve this problem and further improve the robustness, consistency and stability of the solution, clus- tering ensemble methods have emerged as an approach for combining multiple clustering results into an improved solution. Among those, the Evidence Accumulation Clustering (EAC) method [16,17] has attracted a lot of attention. It constructs a co-association matrix from base partitions,



☆ This research was supported by grant No. 11571024 from NSFC, and by the Overseas Talent program from the Beijing Government.
* Corresponding author. Universite´ de Technologie de Compi`egne, CNRS, Heudiasyc (UMR 7253), France.
E-mail address: tdenoeux@utc.fr (T. Denœux).
1 Professor Thierry Denoeux, one of the authors of this paper, is the Editor-in-Chief of Array. The editorial process for this manuscript was handled independently and the manuscript was subject to the Journal’s usual peer review process.
https://doi.org/10.1016/j.array.2020.100018
Received 11 October 2019; Received in revised form 20 December 2019; Accepted 21 January 2020
Available online 31 January 2020
2590-0056/© 2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).



which can be processed by a hierarchical clustering algorithm to obtain the final solution.
In this paper, we introduce an evidential clustering ensemble method that can be seen as an extension of the EAC method in the DS framework. In our method, base evidential partitions are generated by evidential clustering algorithms. As noted in Ref. [14], evidential partitions allow for ambiguity, uncertainty or doubt in the assignment of objects to clusters, and constitute a rich and informative description of the clus- tering structure of a dataset. The base credal partitions are transformed into their relational representations [11], which are combined using different rules of DS theory. The fused relational representation corre- sponds to the co-association matrix in the EAC method. It can be seen as defining an intuitionistic fuzzy relation [22], which is made transitive to obtain an intuitionistic fuzzy equivalence relation. The final evidential partition is obtained by minimizing an objective function based on the credal Rand index [11]. Experiments with real and simulated data show that our approach can reveal the underlying clustering structure of complex-shape datasets and achieve better results than EAC and single clustering algorithms.
The rest of this paper is organized as follows. Basic notions are first recalled in Section 2. Our method is then described in Section 3, and experimental results are reported in Section 4. Finally, Section 5 con- cludes the paper.

Background

In this section, we briefly introduce some basic notions used in this paper. The main definitions of DS theory are first recalled in Section 2.1. The notions of credal partition and relational representation are then reviewed in Section 2.2. Section 2.3 introduces intuitionistic fuzzy re- lations and the transitive closure theorem. We then describe the EAC

	
pl(A) = 1 — bel(A), where Adenotes the complement of A. The quantity bel(A)represents the degree of total support in A, while pl(A)can be interpreted as the degree to which the evidence is consistent with A.
Different combination rules have been proposed in the literature. For example, the conjunctive rule [32] and the dual disjunctive rule [33] are defined, respectively, as

(3a)


(3b)

for any two mass functions m1and m2 on the same frame Ωand all A ⊆ Ω. As shown by Smets [33], the conjunctive rule assumes that all mass functions to be combined are derived from reliable sources of informa- tion, whereas the disjunctive rule only assumes that at least one source of information is reliable, but we do not know which one. Dempster's rule, denoted by ⊕, is defined by the conjunctive rule (3a) followed by normalization (2), i.e., (m1 ⊕m2)( ∅)= 0 and

(4)

for all A /= ∅.
The Dubois-Prade (DP) rule [15] assumes that when two sources are not in conflict, they are both reliable, and at least one is correct when they are conflicting. Specifically, for two mass functions m1and m2, their combination by the DP rule, denoted by m1 ⊞ m2, is
(m1 ⊞ m2)(A)= X m1(B)m2(C)+ X m1(B)m2(C), 6A /= ∅.   (5)

method in Section 2.4, and we review some related work in Section 2.5.
B∩C=A
B∩C/=∅
B∪C=A
B∩C=∅

DS theory

Let Ω = {ω1, …, ωc}be a finite set. A mass function on Ωis a mapping from the power set 2Ωto [0, 1], satisfying the condition
m(A)= 1.	(1)
A⊆Ω
Each subset A of Ωsuch that m(A) > 0is called a focal set. In DS theory, a mass function is viewed as a piece of evidence about some question of interest, for which the true answer, denoted by ω, is supposed to be an element of Ω. For any nonempty focal set A, m(A)is a measure of the belief that is committed exactly to A [31]. The mass m( ∅)assigned to the empty set is a measure of the belief that the true answer might not belong to Ω. The mass m(Ω)is a measure of ignorance. A mass function is said to be logical if it has only one focal set. It is said to be normalized if the empty set is not a focal set, and unnormalized otherwise. An unnormalized mass function m can be converted into a normalized one m* by Dempster's normalization operation defined by m*( ∅) = 0and
m(A)
It can be seen as a reasonable trade-off between the conjunctive and disjunctive rules.
Let us now assume that mass function m represents our current state of knowledge about ω, and we need to choose one or several elements of Ωas our estimate about the true answer. Decision rules in the DS framework are reviewed in Ref. [7]. Here, we only mention two rules that will be used in the sequel. The maximum plausibility rule selects the element ω*with the highest plausibility,
ω* = argmaxpl({ω}).	(6)
ω∈Ω

This rule yields one single result. In contrast, the interval dominance rule [7] is based on the following dominance relation: ω dominates ω' , iff bel({ω}) > pl({ω' }). Then the set of its maximal (non-dominated) ele- ments can be obtained as
Ω* = {ω ∈ Ω|pl({ω}) ≥ bel({ω' }), 6ω' ∈ Ω}.	(7)
Instead of reaching a single decision, this rule selects a set of potential results.

m*(A)= 1
.	(2)
— m(∅)

Credal partitions

Given a mass function m, the corresponding belief and plausibility
functions are defined, respectively, as

bel(A) =	m(B)
∅/=B⊆A

and
pl(A)=	m(B),
B∩A/=∅

for all A ⊆ Ω. Clearly, functions beland plare linked by the relation

Let O be a set of n objects, and Ω = {ω1,…,ωc}the set of clusters. Each object is assumed to belong to at most one cluster. Uncertain knowledge about the cluster-membership of object oiis represented by a mass func-
tion mion Ω. A credal partition [12] is defined as an n-tuple M = (m1, ..,
mn). The notion of credal partition is more general than those of hard or fuzzy partitions, and a credal partition can be summarized into a parti- tion of any other type [9]. Credal partitions also encompass rough par- titions as a special case [14,30]: a rough partition corresponds to a credal partition in which all mass functions miare logical, namely mi(Ai) = 1for some Ai ⊆ Ω. The lower and upper approximations of cluster ωkcan then be


defined as follows,

ωL = {oi ∈ O |Ai = {ωk}}	(8a)
and
ωU = {oi ∈ O |ωk ∈ Ai}.	(8b)

Reflexivity:For all x ∈ X R(x, x)= 1 Irreflexivity: For all x ∈ X R(x, x)= 0 Symmetry: For all (x, y)∈ X2 R(x, y)= R(y, x)

For a non-logical mass function m , the set A can be selected by the
Transitivity: For all (x, y, z) ∈ X3 min(R(x, y), R(y, z)) ≤ R(x, z)

i	i

interval dominance rule (7), after which a rough partition can be obtained.
Suppose miand mjare two mass functions related to objects i and j. We consider the frame Θij = {sij, чsij}, where sijmeans that “Objects i and j belong to the same cluster”, and чsijmeans that “Objects i and j belong to
different clusters”. A mass function mijon Θijrepresenting our beliefs about the joint cluster-membership of objects i and j can be computed from miand mjas follows [11]:
mij( ∅ )= mi( ∅ )+ mj( ∅ )— mi( ∅ )mj( ∅ ),	(9a)
mij  sij} = Xmi({ωk})mj({ωk}),	(9b)


mij  чsij  =   mi(A)mj(B)— mij( ∅ ),	(9c)
A∩B=∅
mij Θij = X mi(A)mj(B)— mij  sij} ,	(9d)


The tuple ℛ = (mij)1≤i≤j≤n is called the relational representation of M. We note that mij( ∅) = 0whenever mi( ∅) = 0and mj( ∅) = 0. Given two credal partitions M and M' and their relational representations ℛand ℛ',the credal Rand index [11] is defined as
P δ m , m' 
Furthermore, R is dual transitive iff 1 — Ris transitive. If a fuzzy rela- tion is reflexive, symmetric and transitive, it is called a fuzzy equivalence relation.
Sometimes, another triangular norm (t-norm) than the minimum is used in the definition of transitivity. For a t-norm T, R is said to be T- transitive if for all (x, y, z)∈ X3,
T(R(x, y), R(y, z)) ≤ R(x, z).
The max-T composition for two fuzzy relations R and Q on X is the fuzzy relation R ∘ Qdefined by
(R ∘ Q)(x, y)= max T(R(x, z), Q(z, y)).	(12)
z∈X

Denoting R ∘ Ras R2 , the T-transitivity property can be expressed as
R2 ⊆ R.


The max-T transitive closure Rof a fuzzy relation R is the smallest max-T transitive fuzzy relation containing R. It can be computed as
∞

= ∪ R ,	(13)
i=1

where ∪ is the fuzzy set union based on the maximum t-conorm. In particular, if R is a reflexive and symmetric fuzzy relation on a finite set X of cardinality n, then R = Rn—1[4].
Intuitionistic Fuzzy relations. The notion of Intuitionistic Fuzzy Relation
(IFR) is a further generalization of relations based on the theory of

S	n(n — 1)/2
a pair of mappings μ : X → 0 1 and ν	X → 0 1 such that μ x

A	[ , ]
A :	[ , ]
A ( )+ 

where δ is the Jousselme's distance [20]. In the special case where mass functions are normal, i.e., mij( ∅) = m' ( ∅) = 0, we can write mass function mijas a vector
m = m  s } , m  чs } , m Θ  T .
νA(x)≤ 1for each x ∈ X. The values μA (x)and νA(x)represent, respec- tively, the membership degree and non-membership degree of element x in the set A. The pair (μA (x), νA(x))is called an intuitionistic fuzzy value (IFV).
Let L be the set of all IFVs, i.e., L = {α = (μα, να)|μα ∈ [0, 1], να ∈ [0, 1],



Jousselme's distance between mijand m' is then defined as
α≤ α' ⇔ μ ≤ μ	and  ν ≥ ν ,

 1 
  1/2


for all	'	L2. Any pair	'	L2has a unique least upper bound

ij  ij
2	ij	ij	ij
α V α' and a unique greatest lower bound α Λ α' given, respectively, by

where Jis the Jaccard matrix
α V α' = (max(μ , μ ), min(ν , ν ))

J = 0
@
1	0	1/2 1
0	1	1/2 A.	(11)

and
α  α'
α  α'


α  α'	α  α'

The range of ρS is [0, 1], and it boils down to the Rand index when both
Thus (L, ≤L)is a complete lattice, with top (1, 0)and bottom (0, 1).

M and M' are hard partitions. The credal Rand index measures the simi-
larity between any two soft partitions.


Intuitionistic fuzzy equivalence relation

Fuzzy relations. In classical set theory, an equivalence relation pro-
An IFR on a non-empty set X is an IFS of X2, i.e., a mapping R: X2 For an IFR R we define the following properties:
Reflexivity: For all x ∈ X R(x, x)= (1, 0),

Symmetry: For all (x, y)∈ X2 R(x, y)= R(y, x),
→ L.

vides a partition of the underlying set into disjoint equivalence classes. In fuzzy set theory, a fuzzy relation R on a finite set X is defined as a fuzzy
Transitivity: For all (x, y, z)∈ X3 V


y∈X
(R(x, y)Λ R(y, z))≤LR(x, z),

subset of the Cartesian product X2, i.e., a mapping from X2to Ref. [0, 1][29]. Each membership value R(x, y)represents the degree to which x stands in relation R with y. For a fuzzy relation R, we define the following properties:
A reflexive, symmetric and transitive IFR is called an Intuitionistic Fuzzy Equivalence Relation (IFER). An IFR R = (μR , νR)is an IFER if and only if μR is reflexive, symmetric and transitive, and νRis irreflexive,


symmetric and dual transitive [22]. Consequently, the transitivity of an IFR can be obtained by making μR and 1 — νRtransitive. In the experiments reported in Section 4, we consider three t-norms: the minimum, the product, and the Lukasiewicz t-norm defined as T(a,b) = max(0,a + b — 1).

Evidence Accumulation Clustering

In this section, we briefly summarize the EAC method [16,17]. More details can be found in Refs. [18,38,42]. The EAC method uses the co-association matrix to avoid the label correspondence problem. More
precisely, assume that a dataset has n objects O  = {o1,o2,…,on}. Suppose
that N base partitions P1,…,PN have been obtained in the first step. In the
second step, each base partition Pb is mapped to a co-association matrix Sb of size N × Nwith general term sb = I(cb = cb ), where cb is the cluster index of xiin Pb and I is the indicator function. The co-association matrix, denoted as S* = (s* ), is the average of all Sb; its general term is
s* =  1 X sb.	(14)



suitable combination rule, and use hierarchical clustering to get the final partition. Unlike other direct methods relying on a voting process, Li et al. [24] introduce another direct approach based on Dempster's rule of combination; their method consists of two steps: finding the correspon- dence labels and using the combination rule to produce the final result. Although these ensemble clustering methods are rooted in Dempster's theory, they still consider hard partitions as input and also output of the procedure. Wang et al. [39] propose an ensemble clustering method for evidential partitions. After solving the label correspondence problem, the final results are obtained by combining the selected base partitions. Due to high computational complexity, they only consider the fixed (true) number of clusters in the base partitions.
In this paper, we combine credal partitions, a very general form of partitions that can be generated by hard, fuzzy, rough or evidential clustering algorithms. After mapping the base partitions to their rela- tional representations, we combine these partitions in a coarser frame, where we only need to consider two focal sets. The combined relational representation is made transitive using the theory of IFRs. Finally, we generate an evidential partition that matches the combined relational representation, providing a much more informative output than can be


Each element s* represents the proportion of base partitions in which objects oiand ojare assigned to the same cluster. The co-association ma- trix can be treated as a new similarity matrix and used as input to single- linkage hierarchical clustering.
In EAC, the co-association matrix is computed by only taking into account whether two objects belong to the same cluster or not. Some researchers [19,37,40] have proposed to use additional information to construct a similarity measure that is more expressive about the rela- tionship between objects. For instance, Yang [41] proposed a fuzzy co-association matrix to summarize the ensemble of fuzzy partitions, where the membership of an object to clusters is expressed by a fuzzy membership function.
We can remark that, in the EAC method, the numbers of clusters in the base partitions do not need to be close to the “true” number of clusters. Indeed, in Ref. [17], the authors construct the base partitions using the

Evidential clustering ensemble method

In this section, we introduce the proposed method. The generation and combination of base credal partitions are described in Section 3.1. The computation of transitive closures to make the combined relational representation transitive is then addressed in Section 3.2, and the method for computing the final credal partition is presented in Section 3.3.

Generation of base partitions

In the first step of our method, base partitions can be obtained by hard, fuzzy and rough clustering methods, which all produce special forms of credal partitions. In this paper, we focus on base partitions generated by evidential clustering methods. We assume that we start
with N base partitions Mb = (mb , …, mb ), b = 1, …, Ngenerated by

k-means algorithms with a large (and sometimes random) number of	1	n

clusters. For instance, in one dataset with two clusters, they construct base partitions with up to 80 clusters. The underlying assumption is that objects that truly belong to true same cluster are likely to be assigned to the same cluster in different partitions, which is summarized in the co-association matrix. The number of clusters in the final aggregated partition is determined by analyzing the dendrogram after applying

In this paper, we propose to extend the EAC method in the framework of DS theory. In our method, the membership of an object to clusters is represented by a mass function in the credal partition, which contains more information than fuzzy and hard partitions. To better exploit this
evidential clustering algorithms such as ECM or EVCLUS. The number of clusters in base partition b is denoted by kb. Before converting credal partitions to their relational representations, we compute the average mass assigned to the empty set for each object i as
mb ( ∅ )=  1 X mb( ∅ ), i = 1, …, n,	(15)


and we normalize each base credal partition by (2). We denote the b-th normalized credal partition as Mb* = (mb*, …, mb*), where mb*is the normalized mass function defined by mb*( ∅) = 0and

type of information, we use the relational interpretation recalled in
Section 2.2 to measure the “similarity” between objects, which can be
mb* A	 mb (A)	(16)

i
seen as a generalization of the co-association matrix. To capture the
1 — mb(∅)

neighborhood relationship, we make the combined relational represen- tation transitive based on IFR theory recalled in Section 2.3.

2.5. Other related work

In the ensemble clustering literature, most contributions focus on hard partitions, and some are based on fuzzy partitions. Only a few methods are based on DS theory. DS theory is a sound approach for
ensemble clustering methods, because it provides ways to combine
for all nonempty subset A of Ω. The reason for this normalization is that the analogy between relation representations and IFRs developed in Section 3.2 requires the mass functions to be normalized. However, we cannot just discard the mass on the empty set, because it is useful to detect outliers. This information will be utilized in the last step of our method (see Section 3.3).
After the relational representations ℛb*have been computed, they can be combined using different rules. The combined relational representa-

different pieces of evidence. Preliminary results have already demon- strated the feasibility of this approach. For example, in Refs. [13,28] the authors propose to define mass functions on the lattice of interval par- titions of a set of objects; they obtain a consensus belief function by a
tion is denoted by ℛ* = (m*)
we get
. For example, using the average rule




*   1	b*
ij	ij
b=1

We can remark that, when the base partitions are hard, the n × nmatrix with general term [m*({s})]boils down to the co-association matrix (14). As a consequence, EAC is a special case of our method. Other combination rules introduced in Section 2.1 can also be used.

Transitivity

The combined relational representation ℛ*sometimes cannot be successfully exploited because it lacks a notion of transitivity, namely: if we believe that objects i and j belong to the same cluster, and objects j and k also belong to the same cluster, then we should believe that this is also the case for objects i and k. When applying hierarchical clustering in the second step, this property is used implicitly in the EAC method.
A relational representation is not a classical fuzzy relation, because it specifies two numbers for each pair (i, j); the degree of belief that i and j belong to the same cluster, and the degree of belief that they do not belong to the same cluster. We can observe the similarity between this
kind of information and IFRs recalled in Section 2.3. Using this formal analogy, we consider the combined relational representation ℛ*as an IFR, and we make it transitive using the techniques reviewed in Section 2.3.
For a normalized mass function m* on Θij, the degrees of belief in
sijand чsijare, respectively,



The new relational representation ℛ = (mij)1≤i≤j≤n will hereafter be referred to as the transitivized combined relational representation.

Recovering the combined credal partition

In the EAC method, the final result is obtained by applying hierar- chical clustering to the association matrix. We could apply the same procedure to matrices (belij({sij}))or (plij({sij}), similarly to what was proposed in Ref. [28]. However, our objective is to recover a credal partition, which cannot be obtained by a hierarchical clustering algorithm.
Our approach is to find a normalized credal partition M*whose
relation representation is as close as possible to the transitivized com- bined relational representation ℛ, closeness being measured by the credal Rand index (10). We need to fix the number kr of clusters as well as the focal sets in the credal partition M*. For example, we can take the singletons and Ωas focal sets, or we can also consider some pairs of clusters.
We thus seek a normal credal partition M*, solution of the maximi-
zation problem

maxρ (ℛ(M*), ℛ),	(18)
M*


where ℛ(M*)is the relational representation of M*. From (10), maxi- mizing (18) is equivalent to minimizing the following stress (error) function:


ij	ij
S M	X m

T
m	J m	m

(19)


and
( *)= 
*
ij	ij
i<j
ij	ij ,

bel*  чsij} = m*  чsij} ,
with bel*({s }) + bel*({чs }) ≤ 1. These numbers define an IFR R = (μ ,
νR)on the set O of objects, with
μR oi, oj = bel*  sij} 
where
m*	 m*  s } , m*  s } , m* Θ  T ,

mij = mij({s}), mij({чs}), mij Θij T ,
and Jis the Jaccard matrix (11). To solve (19), let us write m*as a function of mass functions m*and m*in matrix form. Assuming that each mass

and	i	j
function m*has f focal sets F1,…, Ff , it can be written as an f-vector m* =
i	i

νR oi, oj = bel*  чsij} .
This IFR is reflexive (as bel*({s }) = 1and bel*({чs }) = 0for all i) and
(m*(F ), …, m*(F ))T , and the credal partition can be written as an n ×
f matrix M* = (m* , …, m*)T . Let S = (Skℓ)and C = (Ckl)be the f ×

ii	ii
ii	ii	1	n

symmetric, but it is usually not transitive. As recalled in Section 2.3, R is transitive iff μ is transitive and ν is dual transitive, i.e., νd = 1 — ν is
f matrices defined as follows:

R	R	R	R
 1  if k = l and |F | = 1

transitive closures. We observe that
νd oi, oj = 1 — bel*  чsij} = pl*  sij} .
R	,
and

Denoting by μ and νd the transitive closures of μ and νd , let
Ckl =  1  if Fk ∩ Fl = ∅,
(21)

R	R
belij  sij} = μR oi, oj 

and
R	R	0	otherwise.
We have:
m*  sij} = m* T S m*,	(22a)

plij  sij} = νd oi, oj .
ij	i	j

}	 T

	



	
mijsuch that mij( ∅) = 0and
m* Θij = 1 — m*  sij} — m*  чsij} .	(22c)

mij  sij} = belij  sij} ,	(17a)
mij  чsij} = 1 — plij  sij} ,	(17b)
	}	  } 

We can observe that m*is linear in m*and, consequently, S(M*)is quadratic in m*, the other mass functions being fixed. Consequently, we can minimize S(M*)using a cyclic coordinate descent algorithm, mini- mizing S(M*)with respect to each m*in turn, while keeping the other m*constant; this is the iterative row-wise quadratic programming (IRQP)



at each step the following cost function:
Ω  { }
final consensus credal partition is M = (m ,

for all A ∈ 2 \ ∅ . The
b	b 1

X	 T 
…, mb n). The whole procedure is summarized in Algorithm 1.

gi m*
=
j=1 j/=i


m* — mij


J m* — mij
,	(23)

Algorithm 1
Summary of the method.



which is quadratic in m*. To simplify the expression of this function, let	Require N credal partitions Mb = (mb , …, mb T , b = 1, …, N, combination rule,
i	1	n )

us define the matrix Ajof size 3 × 3f as
number kr of clusters and focal sets F1, …, Ff of the combined credal partition


Aj = I3 

T
*  .	(24)
Compute mi ( ∅), i = 1, …, nusing (15) Normalize the base credal partitions by (16)
Compute the base relational representations ℛb*


= (mb*), b = 1, …, N



Compute the combined relational representation ℛ* = (m*)R = (mij )

where I3is the identity matrix of size 3 × 3and  is the Kronecker product, and the matrix Bof size 3f × f as
t ← 0, e0 ← 1
Initialize M*randomly, compute its relational representation Compute S0 using (19)

B = 0
@
S C
1 — S — C
1
(25)
while et ≥ εdo
t ← t + 1
St ← 0
for i = 1to n do
Compute Aj for all j /= iusing (24)

with these notations, (23) can be written as
g (m )= X A Bm* — m T J A Bm* — m	(26)
Compute Σ, uand c using (28)
Find m*(t)by minimizing (27) subject to (m*)T 1 = 1and m* ≥ 0
Replace row i of M*by (m*(t))T

i		j	i	ij j=1
i	ij
i
St ← St + gi (m*(t))

j/=i

Developing the right-hand side of (26) and rearranging the terms, we
end for
et ← 0.5et—1 + 0.5|St — St—1 |/St—1
end while

*	*	*	*	*(t)

obtain

gi m* = m* T Σm* + uT m* + c0,	(27) with
0	1
Let M = (m1 , …, mn )with mi = mi , i = 1, …, n Denormalize the combined credal partition using (30)
	return Credal partition Mb	



Concerning the time complexity of the method, converting each base credal	partition	to	its	relational	representations	requires
2

T BX T	C
O(n )operations, where n is the number of objects, and combining the

Σ = B


j=1 j/=i
0
Aj JAj
B	(28a)
A


1
average partition takes O(n2 N). To compute the transitive closure, Lee
[21] describes an optimal algorithm with O(n2)time complexity. Conse- quently, the calculation of the transitivized combined relational repre- sentation ℛcan be performed in O(n2 N)operations. The most computationally demanding step of the method is to recover the credal


T
ij
j=1 j/=i


T
ij
j=1 j/=i
Minimizing g(m )under the constraints (m*)T 1 = 1(where 1 =
partition from the obtained relational representation. The complexity of this step depends on the Quadratic Programming (QP) problem (27) solved at each iteration. As the Jaccard matrix (11) is positive definite [5], so is matrix Σin (27) (of size f × f , where f is the number of focal sets). Consequently, the quadratic function (27) being minimized in convex. It is known [36] that convex QP problems can be solved in polynomial time. The computing time of the optimization can be controlled by limited the number of focal sets in the recovered credal partition.

i	i

(1, …, 1)T ) and m* ≥ 0is a quadratic programming (QP) problem, which can be solved efficiently with any QP solver. As we iteratively update each row of M*, the overall stress S(M*)decreases and eventually reaches a local minimum. As in Ref. [14], we compute the following running mean after each cycle of the algorithm as e0 = 1and
|St — St 1|
Experiments

In this section, we evaluate our approach using various datasets. In Section 4.1, we study the influence of different parameters using simu- lated datasets. Detailed results with simulated and real datasets are then reported in Section 4.2.

et = 0.5et—1 + 0.5
—
St—1
,	(29)

Qualitative experiments

where t is the iteration counter and St is the error at iteration t. The al- gorithm stops when et becomes less than some given threshold ε.
Let M * = (m* ,…, m*)denote the normalized credal partition obtained after convergence of the algorithm. The last step is to “denormalize” it using the masses m( ∅)on the empty set computed in (15). This is done by

multiplying each mass mb (A)with A /= ∅by 1 — mb ( ∅):
b	b	i

In this section, we present the results on simulated datasets to investigate different stages of the ensemble procedure. We study the ef- fect of different combination rules and transitive closure with different t- norms.

Effect of different combination rules
We first investigate the effect of different combination rules on the Fourclass data, consisting of four classes in two-dimensional space, each generated from a Gaussian distribution. From Fig. 1a, we can see that the


	
































Fig. 1. Results for the Fourclass data with different combination rules: original data (a), one of the base partitions (b), average rule (c), Dempster's rule (d), disjunctive rule (e), DP rule (f).



clusters overlap, and there is an outlier.
In the first step, we ran the ECM algorithm [6,26] with the Fourclass dataset for generating base partitions. To obtain useful information, we use the two-step approach introduced in Refs. [14,34] to extract infor- mative focal sets. More precisely, the ECM algorithm was first run with ∅, the singletons and Ωas focal sets. Based on the obtained credal partition, the similarity between each pairs of clusters {ωj, ωl}was measured by s(j,
l) = n plijplil, where plijand plilare the normalized plausibility that object i belongs, respectively, to cluster j and l. The pairs {ωj,ωl}of mutual K = 2nearest neighbors were then selected as informative focal sets. In the second step, the evidential clustering initialized with the credal
partition computed in the previous step was run again with those infor- mative focal sets. In Ref. [14], this method has been shown to be tractable and to yield good results even when the number of clusters is large.
In this experiment, we generated N = 20base partitions, each of which had kb = 15clusters, and we set δ to the 0.2-quantile of the dis- similarities between objects. The diversity of the partitions is obtained by setting the number of clusters to a large number and using different random initializations. We considered the four combination rules reviewed in Section 2.1: average, Dempster, disjunctive and DP. For simplicity, after combining the relational representations we did not make the corresponding IFR transitive. We recovered a credal partition with four clusters, kr = 4. In the second step, we firstly recovered a credal partition with the mass on the singletons and Ω, and secondly with the mass on the informative focal sets as we did in the generation step. One of the base partitions is shown in Fig. 1b. In Fig. 1b–f, each point is repre- sented by a symbol corresponding to its true class, and a color corre- sponding to the maximum plausibility cluster (6). The convex hulls of the lower and upper approximations computed using the interval dominance rule (7) are displayed using solid and broken lines, respectively.
From Fig. 1d, we can see that Dempster's rule fails to capture the structure of this dataset. As recalled in Section 2.1, important assump- tions for Dempster's rule are that all the mass functions come from reli- able and independent sources; it is clear that none of these assumptions is satisfied in our case. When generating the base partitions, the number of clusters is set to a larger value than the number of “natural” clusters, and the base partitions, being based on the same data, are not independent. The DP rule behaves better than Dempster's rule, as it relaxed the assumption of perfect sources. However, the DP rule also cannot recog- nize the structure of the dataset (Fig. 1f). The average and disjunctive rules both yield acceptable results (Fig. 1c and e), but the credal partition obtained by the disjunctive rule is too imprecise: the inner approxima- tions of the clusters are very small and the upper approximations are identical, which means that all objects possibly belong to any cluster (Fig. 1e). This result is due to the very cautious assumption underlying this combination rule (at least one source is reliable). In contrast, the average rule successfully identifies the objects that can be clustered with high confidence (those in the inner approximations), as well as the ob- jects in the overlapping area between clusters (Fig. 1c). As the average rule appears to be the most effective, it was used in subsequent experiments.

Effect of transitive closure with different t-norms
In this section, we investigate the effect of transitive closure with respect to different t-norms on Half-rings data. This dataset is composed of
two clusters in two-dimensional space, separated by a nonlinear boundary (Fig. 2a). Such non-spherical clusters are typically difficult to identify without prior information. The parameter settings were the same as before, except that we set kb = 15, N = 20, kr = 2and δ equal to the 0.1-quantile of the dissimilarities between objects in this experiment. We combined the relational representations by the average rule and made the belief and plausibility matrix transitive with respect to the minimum, Lukasiewicz and product t-norms. We also considered the results without
operations are shown in Fig. 2c–e, and the recovered partition without transitivity is displayed in Fig. 2f. In these figures, each point is repre- sented by a symbol corresponding to its true class, and a color corre- sponding to the obtained cluster using the maximal plausibility rule. We can see that, without transitivity, our method fails to identify nonspherical clusters (Fig. 2f). The results with the Lukasiewicz t-norm are similar to those without transitivity (Fig. 2d). The best result is ob- tained with the minimum t-norm (Fig. 2c).
Discovering clusters with complex shape is one of the most chal- lenging issues in clustering. In this experiment, we have show that our method has the ability to discover such clusters after making the com- bined relational representation transitive in the sense of IFRs.

Quantitative experiments

In this section, we apply our method to the simulated and real data summarized in Table 1. Five simulated datasets2 are shown in Fig. 3; the
first four datasets contain complex shape clusters, while R15 contains a
comparatively larger number of well-separated clusters. All the real datasets can be found in the UCI Repository of machine learning data- bases.3 For all the datasets considered in this study, we assume that the “ground-truth” partition exists. To compare an evidential partition with the true partition, we first converted it to a hard partition using the maximum plausibility rule, and we computed the adjusted Rand index (ARI) between the derived hard partition and the true partition.
We used the ECM algorithm with informative pair of clusters to generate base partitions for each dataset. We considered three cases for the number of clusters in the base partitions (Table 2): fixed small number (case 1); randomly selected from an interval (case 2) and fixed large number (case 3). We averaged the relational representations, and we computed the transitive closure operations with the minimum, product and Lukasiewicz t-norms. We also included the solution without transitivity for comparison. The true number of clusters was assumed to be known; in practice, this number can often be guessed using, e.g., visualization techniques. When recovering the combined credal parti- tion, we first used only singletons and Ωas focal sets (denoted by “simple” in the table), and then we included informative pairs informative pairs (denoted by “pairs” in the table) as explained in Section 4.1. The ensemble size was N = 20. The procedure was run 10 times for each experiment. The average ARI values are shown in Table 3, and the standard deviations are shown in parentheses.
For comparison, we show the results of the ECM algorithm with the true number of clusters. For this method, only ∅and singletons were treated as focal sets, and we fixed δ = 100. We also compared our method with the EAC method: in the first step we used the hard c-means algorithm to obtain N = 100base partitions; in the second step, we used single-linkage hierarchical clustering to obtain the true number of clus- ters. For the EAC method, we also considered three cases as we did in our method. The results for the ECM algorithm and the EAC method are shown in the first and second columns of Table 3.
From Table 3, we can see that the results obtained by our method are better than those obtained by the ECM algorithm, except for the Wine
data. When compared to the EAC method, our method achieves higher accuracy and better stability, especially for real datasets. The EAC method performs very well with simulated datasets, but better or similar
results were obtained with our method, except for the Spiral dataset.
Comparing the results obtained with different t-norms for the tran-
sitive closure operation, we can see that the minimum t-norm often performs well, except with the R15 and Seeds dataset, for which better results are obtained with the other t-norms, or even without transitivity.
The results with the Lukasiewicz and product t-norms are often similar to those obtained without transitivity. Generally, it seems that making the

transitivity comparison. One of the base partitions is shown in Fig. 2b.		

The recovered partitions with the different transitive closure
2 Available at http://cs.joensuu.fi/sipu/datasets/.
3 Available at http://archive.ics.uci.edu/ml.


	























Fig. 2. Results for Halfrings data: original data (a), one of base partitions (b), and recovered partition from transitive closures the minimum (c), Lukasiewicz (d) and product (e) t-norms, as well as without transitivity (f).



Table 1
Datasets used in the experiments.


combined relational minimum-transitive is beneficial for datasets with complex-shaped clusters, but it can sometimes degrade the performance for datasets with overlapping clusters. The reason is that, where there is an overlapping area between clusters and we make the fuzzy relation transitive, objects from different clusters become similar to each other, which hinders the performance of the method. As far as the number of clusters in the base partitions is concerned, better results are generally obtained in Cases 2 and 3, i.e., with a larger number of clusters.

Conclusion

We have presented a method for combining clusterings in the DS framework. Each base clustering is assumed to take the form of a credal partition, in which the clustering membership of each object is allowed to
be uncertain and represented by a mass function. This very general formalism encompasses hard, fuzzy and rough partitions as special cases [9]. Credal partitions of special forms can be generated by hard, fuzzy or rough clustering algorithms, and general credal partitions can be ob- tained by evidential clustering procedures such a EVCLUS [14], ECM [26], BPEC [34], CCM [25], etc.
Base credal partitions cannot be combined directly, because there is not always a clear correspondence between clusters in different parti- tions; in particular, base credal partitions can have different numbers of clusters. To circumvent this difficulty, we proposed to convert each base credal partition to its relational representation, defined as the collection of pairwise mass functions describing the uncertain joint cluster- membership for each pair of objects. After the normalized relational


Table 2
Number of clusters in base partitions.



		


















Fig. 3. Simulated datasets: Aggregation (a), Compound (b), Flame (c), Spiral (d) and R15 (e).


Table 3
Average ARI results for simulated and real datasets. The best results are shown in bold.


representations have been computed, they can be aggregated using any combination rule of DS theory. The best results have been obtained with the averaging operator.
Using the similarity between relational representations and intui- tionistic fuzzy relations studied in Ref. [22], we have proposed a way to transitivize the combined relational representation by computing the transitive closures of two fuzzy relations, based on a t-norm. Our experimental results suggest that the minimum t-norm often yields the best results, especially for datasets with complex-shaped clusters. How- ever, making the relational representation transitive does not always improve the results, and may even degrade them in the case of datasets with many spherical clusters.
After the combined relational representation has been computed, the last step of our method consists in constructing a credal partition whose relational representation is as close as possible to the combined relational representation obtained in the previous step. We have proposed an error measure based on Jousselme's metric, which can be minimized using a grouped coordinate descent algorithm that solves a convex quadratic optimization problem at each step. After a normalized credal partition has been obtained, we “denormalize” it by assigning to the empty set the average of the masses assigned to the empty set by the base partitions, which provide useful information to signal outliers.
We have applied this method to a variety of simulated and real datasets. It has been shown to perform well in terms of adjusted Rand index as compared to the EAC method and to the ECM algorithm alone. It should also be emphasized that, in contrast with EAC and most existing ensemble clustering methods, our approach computes a credal partition, which constitutes a richer description of the clustering structure of a dataset, as compared to hard or fuzzy partitions.
Although very encouraging, these results are still preliminary. The determination of the number of clusters remains a crucial issue that re- mains to be thoroughly investigated. The application of this approach to very big datasets with a large number of clusters still represents a chal- lenge. Finally, we could apply this approach not only to combine credal partitions, but also to combine all kinds of partitions generated by all kinds of clustering algorithms. These research directions will be inves- tigated in future work.
CRediT authorship contribution statement

Feng Li: Methodology, Investigation, Writing - original draft. Shou- mei Li: Supervision, Funding acquisition. Thierry Denœux: Conceptu- alization, Methodology, Writing - review & editing, Supervision.

References

Akbari E, Dahlan HM, Ibrahim R, Alizadeh H. Hierarchical cluster ensemble selection. Eng Appl Artif Intell 2015;39:146–56.
Antoine V, Quost B, Masson M-H, Denoeux T. CECM: constrained evidential c- means algorithm. Comput Stat Data Anal 2012;56(4):894–914.
Antoine V, Quost B, Masson M-H, Denoeux T. CEVCLUS: evidential clustering with instance-level constraints for relational data. Soft Computing 2014;18(7):1321–35.
Boixader D, Jacas J, Recasens J. Fuzzy equivalence relations: advanced material. In: Dubois D, Prade H, editors. Fundamentals of fuzzy sets. Boston: Kluwer Academic Publishers; 2000. p. 261–90.
Bouchard M, Jousselme A-L, Dor´e P-E. A proof for the positive definiteness of the
Jaccard index matrix. Int J Approx Reason 2013;54(5):615–26.
Denœux T. evclust: evidential Clustering. R package version 1.0.3. URL, https://CRAN.R-project.org/package=evclust; 2016.
Denoeux T. Decision-making with belief functions: a review. Int J Approx Reason 2019;109:87–110.
Denœux T, Dubois D, Prade H. Representations of uncertainty in artificial intelligence: beyond probability and possibility. In: Marquis P, Papini O, Prade H, editors. A guided tour of artificial intelligence research. Springer Verlag; 2020. Ch. 4.
Denoeux T, Kanjanatarakul O, September. Beyond fuzzy, possibilistic and rough: an investigation of belief functions in clustering. In: Soft methods for data science (proc. Of the 8th international conference on soft methods in probability and statistics SMPS 2016). Vol. AISC 456 of advances in intelligent and soft computing. Rome, Italy: Springer-Verlag; 2016. p. 157–64.
Denoeux T, Kanjanatarakul O, Sriboonchitta S. EK-NNclus: a clustering procedure based on the evidential k-nearest neighbor rule. Knowl Base Syst 2015;88:57–69.
Denoeux T, Li S, Sriboonchitta S. Evaluating and comparing soft partitions: an approach based on Dempster-Shafer theory. IEEE Trans Fuzzy Syst 2018;26(3): 1231–44.
Denœux T, Masson M-H. EVCLUS: evidential clustering of proximity data. IEEE Trans Syst Man Cybern B Cybern 2004;34(1):95–109.
Denœux T, Masson M-H. Evidential reasoning in large partially ordered sets. Ann Oper Res 2011;195(1):135–61.
Denœux T, Sriboonchitta S, Kanjanatarakul O. Evidential clustering of large dissimilarity data. Knowl Base Syst 2016;106:179–95.
Dubois D, Prade H. Representation and combination of uncertainty with belief functions and possibility measures. Comput Intell 1988;4(3):244–64.



Fred ALN, Jain AK. Data clustering using evidence accumulation. In: International conference on pattern recognition; 2002. p. 276–80.
Fred ALN, Jain AK. Combining multiple clusterings using evidence accumulation. IEEE Trans Pattern Anal Mach Intell 2005;27(6):835–50.
Ghaemi R, Sulaiman m n, Ibrahim H, Mustapha N. A survey: clustering ensembles techniques. World Acad Sci Eng Technol 2009;38:644–53.
Iam-on N, Boongoen T, Garrett S. Refining pairwise similarity matrix for cluster ensemble problem with cluster relations. Discovery Science 2008:222–33.
Jousselme A-L, Grenier D, Bosse´ E. A new distance between two bodies of evidence.
Inf Fusion 2001;2(2):91–101.
Lee H-S. An optimal algorithm for computing the max-min transitive closure of a fuzzy similarity matrix. Fuzzy Set Syst 2001;123(1):129–36.
Li B, He W. The structures of intuitionistic fuzzy equivalence relations. Inf Sci 2014; 278:883–99.
Li F, Li S, Denoeux T. k-CEVCLUS: constrained evidential clustering of large dissimilarity data. Knowl Base Syst 2018;142:29–44.
Li F, Qian Y, Wang J, Liang J. Multigranulation information fusion: a Dempster- Shafer evidence theory-based clustering ensemble method. Inf Sci 2017;378: 389–409.
Liu Z, Pan Q, Dezert J, Mercier G. Credal c-means clustering method based on belief functions. Knowl Base Syst 2015;74:119–32.
Masson M-H, Denoeux T. ECM: an evidential version of the fuzzy c-means algorithm. Pattern Recogn 2008;41(4):1384–97.
Masson M-H, Denœux T. RECM: relational evidential c-means algorithm. Pattern Recogn Lett 2009;30(11):1015–26.
Masson M-H, Denoeux T. Ensemble clustering in the belief functions framework. Int J Approx Reason 2011;52(1):92–109.
Ovchinnikov S. An introduction to fuzzy relations. In: Dubois D, Prade H, editors. Fundamentals of fuzzy sets. Boston: Kluwer Academic Publishers; 2000. p. 233–59.
Peters G, Crespo F, Lingras P, Weber R. Soft clustering: fuzzy and rough approaches and their extensions and derivatives. Int J Approx Reason 2013;54(2):307–22.
Shafer G, et al. A mathematical theory of evidence. Princeton university press; 1976.
Smets P. The combination of evidence in the transferable belief model. IEEE Trans Pattern Anal Mach Intell 1990;12(5):447–58.
Smets P. Belief functions: the disjunctive rule of combination and the generalized bayesian theorem. Int J Approx Reason 1993;9(1):1–35.
Su Z-G, Denoeux T. BPEC: belief-peaks evidential clustering. IEEE Trans Fuzzy Syst 2019;27(1):111–23.
ter Braak CJF, Kourmpetis Y, Kiers HAL, Bink MCAM. Approximating a similarity matrix by a latent class model: a reappraisal of additive fuzzy clustering. Comput Stat Data Anal 2009;53(8):3183–93.
Vavasis SA. Complexity theory: quadratic programming. Boston, MA: Springer US; 2001. p. 304–7.
Vega-Pons S, Ruiz-Shulcloper J. Clustering ensemble method for heterogeneous partitions. Progress in pattern recognition, image analysis, computer vision, and applications, 5856; 2009. p. 481–8.
Vega-Pons S, Ruiz-Shulcloper J. A survey of clustering ensemble algorithms. Int J Pattern Recogn Artif Intell 2011;25(3):337–72.
Wang X, Han D, Han C. Ensemble clustering based on evidence theory. In: 2017 20th international conference on information fusion (fusion); 2017. p. 759–67.
Wang X, Yang C, Zhou J. Clustering aggregation by probability accumulation. Pattern Recogn 2009;42(5):668–75.
Yang L, Lv H, Wang W. Soft cluster ensemble based on fuzzy similarity measure. IMACS: multiconference on computational engineering in systems applications, 1; 2006. p. 1994–7. 2.
Zhan J-M, Chen J-T, Xing J-Q. Research advance of clustering ensemble algorithm. In: 2017 international conference on wavelet analysis and pattern recognition (ICWAPR); 2017. p. 109–14.
Zhou K, Martin A, Pan Q, Liu Z-G. Median evidential c-means algorithm and its application to community detection. Knowl Base Syst 2015;74:69–88.
