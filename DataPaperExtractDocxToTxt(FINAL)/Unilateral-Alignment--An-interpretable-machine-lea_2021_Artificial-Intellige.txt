Artificial Intelligence in Geosciences 2 (2021) 192–201

		




Unilateral Alignment: An interpretable machine learning method for geophysical logs calibration
Wenting Zhang a, Jichen Wang b, Kun Li c, Haining Liu e, Yu Kang a,c,d, Yuping Wu d,
Wenjun Lv a,c,*
a Department of Automation, University of Science and Technology of China, Hefei, 230027, China
b College of Control Science and Engineering, China University of Petroleum, Qingdao, 266580, China
c Institute of Advanced Technology, University of Science and Technology of China, Hefei, 230031, China
d Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, 230088, China
e Shengli Geophysical Research Institute, SINOPEC Group, Dongying, 257022, China



A R T I C L E I N F O

Keywords:
Interpretable machine learning Geophysical logs calibration Data distribution discrepancy
A B S T R A C T

Most of the existing machine learning studies in logs interpretation do not consider the data distribution discrepancy issue, so the trained model cannot well generalize to the unseen data without calibrating the logs. In this paper, we formulated the geophysical logs calibration problem and give its statistical explanation, and then exhibited an interpretable machine learning method, i.e., Unilateral Alignment, which could align the logs from one well to another without losing the physical meanings. The involved UA method is an unsupervised feature domain adaptation method, so it does not rely on any labels from cores. The experiments in 3 wells and 6 tasks showed the effectiveness and interpretability from multiple views.





Introduction

Artificial intelligence (AI) is increasingly becoming an important scientific and technological tool in solving geosciences issues such as earthquakes detection, spatial prediction, stratigraphic correlation, seismic processing and interpretation, etc. (Magrini et al., 2020; Foued- jio, 2020; Xu et al., 2022; Zhou et al., 2020; Birnie et al., 2021). It has reported that the rapidly evolving field of machine learning (ML), a popular way to realize AI, will play a key role in geosciences (Bergen et al., 2019). However, due to the black-box issue existing in many ML models, we cannot know the way how an ML model works, so cannot assess its reliability and have enough confidence to apply it in produc- tion. We believe that the interpretability of ML will play an extremely important role in geosciences in which there are too many risk-aware applications, e.g., exploration.
The geophysical logs interpretation means the prediction of borehole geology information (e.g., lithofacies, porosity) according the logs, which is a fundamental and worthy work to understand the undersurface earth. The recent years have witnessed the development of the applications of machine learning in logs interpretation. For example, a semi-supervised model named Laplacian support vector machine is proposed to solve the
problems of scarce labels (Li et al., 2020). The smoothnesses implying in the feature space and depth are utilized to propagate the labels form labelled samples to unlabelled ones, therefore increasing the classifica- tion accuracy. Although the Laplacian works in semi-supervised learning, the setting of Laplacian is still empirical so that the introduction of unlabelled data might result in a worse performance compared with just using labelled data. Hence, an ensemble mechanism is used to combine some candidate Laplacians and calculate an optimal one to guarantee the safety of using semi-supervision (Li et al., 2021). More similar work could be found in the literatures (Dunham et al., 2020; Imamverdiyev and Sukhostat, 2019; Zhu et al., 2018).
Although there has been a great deal of researches in this area, most of them work under the assumption of independent and identically dis- tribution (iid), i.e., the data from training wells should have similar distribution with those data from testing wells. However, such an assumption barely hold due to the differences in logging equipments, borehole conditions, etc. As illustrated in Fig. 1c, there are a significant distribution discrepancy between the training dataset and testing dataset, so iid does not hold anymore. As shown in Fig. 1a and b, the non-iid problem causes an accuracy decrease in interpreting the logs from another wells by the trained classifier. Both supervised and semi-




* Corresponding author. Department of Automation, University of Science and Technology of China, Hefei, 230027, China.
E-mail address: wlv@ustc.edu.cn (W. Lv).

https://doi.org/10.1016/j.aiig.2022.02.006
Received 27 December 2021; Received in revised form 27 February 2022; Accepted 27 February 2022
Available online 14 March 2022
2666-5441/© 2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. Illustration of data distribution discrepancy. (a) Logs, cores (i.e, labels), and predictions on Well A using the model trained on Well B; (b) Logs, cores, pre- dictions on Well B using the model trained on Well A; (c) t-SNE (van der Maaten and Hinton, 2008) visualization of data distribution for Well A and Well B. Gray, yellow, and green indicate mudstone, sandstone, and dolomite, respectively. Dots and boxes indicate the data from Well A and Well B, respectively.



Fig. 2. Illustration of the UA framework.



Table 1
Dataset description.


Wellsb	Lithologya


Mu	Si	Co	Sum
addition, data mining and processing techniques can be coupled with machine learning algorithms to solve the problem of data imbalance (Long et al., 2016; Zhong et al., 2020) Filtering techniques help us to remove some of the random non-stratigraphic responses (Lv et al., 2018; Ruckebusch, 1983), but do not have a significant effect on the distribu-


a Mu, mudstone; Si, siltstone; Co, conglomeratic sandstone.
b Logging curves of each well: AC, acoustic log; CAL, caliper log; COND, con- ductivity log; GR, gamma ray log; R25, 2.5 m bottom gradient resistivity; and SP, spontaneous potential log resistivity.


Table 2
Macro-average F1-score (%) on 6 tasks.

a Weighted ELM classifier trained with source-domain examples.
b Weighted ELM classifier trained with target-domain examples.

supervised classification methods cannot handle such a situation, so some pre-processing work should be done in advance of interpretation. In
As a transfer learning method, DA tries to find the relevance between two datasets and then adopts the knowledge from one domain to another. DA could works in an unsupervised manner which means that we only need the samples from two datasets. DA generally falls into three cate- gories: model-, sample-, and feature-based methods (Yang et al., 2020). The sample-based DA re-weights the samples overlapped in the feature space during the training phase to yield a more robust model in both domain, but it could only handle the small distribution discrepancy issue (Chang et al., 2021b; Chang et al., 2022). The model-based DA re-adjust the source-domain classifier to fit the target domain (Liu et al., 2020). For a neural network model, usually the last few layer are allowed to adjust, so the transferability is not strong enough. The feature-based DA (fDA) tries to map the samples to a feature space with higher dimension such that two domains align (Li et al., 2018). As we know that a deep neural network possesses multiple layers for feature extraction, so the fDA usually has more feasibility. Most fDA methods map both domains, so the physical meanings of the original features would loss (Chang et al., 2021a). Instead, we could just align one domain to another in the original feature space, therefore maintaining the physical meanings (Chen et al., 2018; Wu et al., 2022). Such a fDA method could be named Unilateral Alignment (UA) which might be an excellent tool for geophysical logs calibration to our opinions.




Fig. 3. F1-score of each class in six tasks.


In this paper, we would exhibit an interpretable machine learning method, i.e., UA, which could align the logs from one well to another without losing the physical meanings. In addition, the involved UA method is an unsupervised fDA method, so it does not rely on any labels from cores. The contributions are threefold. First, we formulate the geophysical logs calibration problem and give its statistical explanation. Second, we present the derivation of UA and illustrate its application in logs calibration. Third, we conduct massive experiments in 3 wells and 6 tasks to show the effectiveness and interpretability from multiple views. The above three contributions correspond to Sec. 2, 3, and 4, respectively.
Formulation of geophysical logs calibration problem

In the logs interpretation problem (e.g., lithofacies identification), the well logging feature vector (also named instance or sample) of d logs at a
certain depth is denoted by xi ¼ ½xi1; xi2; ⋯ ; xid]2 Rd, including acoustic log (AC), Gamma ray log (GR), caliper log (CAL), etc. The lithofacies
corresponding to the instance xi is denoted by

k
y ¼ ½0; …; 0; 1 ; 0; …; 0];
|ﬄﬄkﬄ{0 —zkﬄﬄﬄ}




Fig. 4. Visualization of Well A logs in the task A → B before and after calibration.


Fig. 5. Visualization of Well B logs and the corresponding predicted lithologies in the task A → B before and after calibration.



Table 3
The mean values of each log on the task A → B.
respectively, where Ps(⋅), Pt(⋅) are marginal probability distributions, and
Ps(⋅|⋅), Pt(⋅|⋅) are conditional ones. In general, the conditional probability distributions are shared across domains (Stojanov et al., 2021). Under the
assumption of Ps(ys|xs) = Pt(yt|xt), the problem turns to tackle the unequality of marginal probability distributions, i.e., Ps(xs) /= Pt(xt).
In order to improve the performance in the case of probability discrepancy, we propose a calibration method to learn a unilateral cali-
brating function f compelling P (f(xs)) ≈ P (f(xt)) and consequently

s	t




Table 4
The mean values of each log on the task A → C.
Ps(f(xs), ys) ≈ Pt(f(xt), yt). In this way, the classifier trained on Ds would have a good performance in the interpretation task on a target-domain well.

Unilateral Alignment for geophysical logs calibration








when xi belongs to the k-th (1 ≤ k ≤ k0) lithofacies type of the total k0
types. n examples (xi, yi) along the depth constitute a labelled dataset D =
{(xi; yi)}n from the well.
In most cases, the source-domain instances and the target-domain ones have different probability distributions. With a logs calibration, the accuracy of lithofacies identification might decrease when predicting the target-domain instances by the model trained on source-domain ex-
amples. In this case, the expert-interpreted dataset is denoted as the
Extreme Learning Machine (ELM) can be used to address classifica- tion and regression problems, which is proposed by G.-B. Huang (Huang et al., 2004, 2011). Its model training is efficient compared with some traditional learning algorithms, meantime completing regression and multi-class classification tasks effectively. For example, considering a classification task with training dataset {X; Y} = {(xi; yi)}n , ELM cor- responding to the training dataset with L hidden layer nodes is described as


s  s  ns
terpreted well is called target-domain dataset Dt = {xt }nt , where ns, nt represent the number of the instances in Ds, Dt, respectively. Besides, these instances of both domains belong to the same feature space F,
which means that xs;xt ∈ F. The label space Y of Ds is also identical with
Dt's, i.e., ys; yt ∈ Y.
Since the datasets of two domains are sampled from respective joint distributions, the discrepancies of probability distributions between two domains can be seen as the discrepancies of two joint distributions i.e.,
XL  β g(wTx + b )= o ; j = 1; …; n;	(1)

where g(⋅) is the nonlinear piecewise continuous activation function (e.g., Sigmoid function), wi = [wi1; wi2; ⋯ ; wid]∈ Rd and bi are the randomly- generated weight vector and bias of the i-th hidden layer node, βi is the
output weight of the i-th hidden layer node. oj is the prediction of the j-th instance.
To minimize the predicted error, the error function can be defined as
Xn   o — y  = 0;	(2)

are rewritten as Ps(xs, ys) = Ps(xs)Ps(ys|xs), Pt(xt, yt) = Pt(xt)Pt(yt|xt),

where yj denotes the j-th label among the n distances. Combining (1) and




Fig. 6. Frequency distribution histogram of SP log in the task A → B.




Fig. 7. Frequency distribution histogram of SP log in the task A → C.


Fig. 8. Frequency distribution histogram of AC log in the task A → B.


(2) yields Hβ = Y, where H = [h(x1); h(x2); ⋯ ; h(xn)] ∈ Rn×L represents the mapped instance matrix, and β ∈ RL×k0 denotes the output weight

min
β
1

β 2 +
C

Hβ — Y
2;	(5)

matrix, h(xi)= [g(xiw1 +b1); g(xiw2 +b2); …; g(xiwL +bL)] ∈ R1×L is the i- th mapped instance. Hence, the loss function is
where C is the penalty factor to balance the two terms. Furthermore, we have

min
β
Hβ — Y 2;	(3)
8 HT
>

HHT + I

 —1

Y; n < L

where β could be calculated by the Moore-Penrose (MP) generalized inverse, that is,
β* = H†Y;	(4)
β* = <
>>:
C
I —1
HTH + C

HTY; n ≥ L
(6)

where H† is the MP generalized inverse matrix of H. To avoid the over-
fitting issue, (3) could be augmented as
3.2. Maximum mean discrepancy (MMD)

Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) is




Fig. 9. Frequency distribution histogram of COND log in the task A → B.


Fig. 10. Frequency distribution histogram of GR log in the task A → B.


generally used to measure the similarity between two probability dis-


MMD2(X; Y	1 Xn  φ(x )—  1 Xm  φ(x )||2

(8)

distance is measured by mapping the instances into a Reproducing Kernel Hilbert Space (RKHS). Given two distributions P and Q, the formula of the distance based on the maximum mean discrepancy criterion between them is
where φ(⋅) is the kernel-induced feature mapping function.

3.3. Unilateral Alignment (UA)

MMD2(7 ; P; Q)= sup  EP[f (x)] — EQ[f (y)]2	(7)	s	t

f ∈7

where √ denotes an RKHS space, 7 denotes a class of functions. The continuous function f(⋅) is the mapping function, sup(⋅) is an upper bound function, and EP(x), EQ(x) are expectations. The empirical estimation of
MMD in an RKHS is
For the case that Ps(x ) /= Pt(x ), UA of geophysical logs calibration aims to adjust the logs of one well to align it with another. To achieve this goal, we propose the UA calibration method by combining ELM and MMD.
We introduce the random mapping used in ELM to our method, i.e.,
the instance xi is mapped by h(xi)∈ RL, where L is the dimension of mapped instance. Additionally, we define Hs ∈ Rns ×L and Ht ∈ Rnt ×L as




Fig. 11. Frequency distribution histogram of R25 log in the task A → B.


the random mapping matrices of all the instances in datasets Ds, Dt, respectively. In this case, MMD could be expressed by
Eventually, we obtain the solution of (13), that is,


MMD2(D ; D )= || 1

Xns

h(xs)— 1

Xnt

h(xt)||2	(9)
8 HT
<  s
In
+ HsHT +
γ
λ ΩHHT
γ
 —1
Xs; n < L

s  t	ns
i=1	i	nt
j=1	j
β* =
>>: IL

+ HTHs +
—1
HTΩH
HTXs; n ≥ L
(14)



min || 1 Xns
h(xs)β —  1 Xnt

h(xt)β||2 = min Tr(βTHTΩHβ)	(10)
where β* is the optimal transform matrix, and In ∈ Rn×n, IL ∈ RL×L are both identity matrices. Specifically, the UA calibrating function f could be

β	ns
i=1	i
nt	j=1	j	β
defined as f (xs)= Hsβ = X~ s ≈ Xs and f (xt ) = Ht β = X~ t , where X~ s ∈

where Tr(⋅) is the matrix trace, and Ω ∈ Rn×n denotes the marginal PMMD matrix, that is,
1	i j	n
s
>
Rns ×d ; X~ t ∈ Rnt ×d are the calibrated instance matrices corresponding to
Hs, Ht, respectively. In this case, this method could compel the proba- bility distributions Ps(~xs)≈ Pt (~xt ), and consequently achieve Ps(~xs; ys)≈ Pt (~xt ; yt ). Subsequently, we could use the source-domain calibrated
dataset (X~ s; Ys) to train an ELM and test on target-domain calibrated

Ω~ = <  1 ;	i; j > n
(11)
dataset X~ t with high accuracy, where Ys ∈ Rns ×k0 is the label matrix of

ij	n2	s
>>: — 1 ; others

Nevertheless, the random mapping function leads to the interaction of logs, thereby the mapped features lose the physical meanings. In this case, the regularization term of source domain maintenance (Chen et al., 2018) is introduced in the process of calculating β, which could maintain the physical meanings of aligned features. Hence, the process of logs calibration is interpretable through our method. Specifically, by intro- ducing the source domain maintenance term, the equation (10) can be written as
min λ Tr(βTHTΩHβ)+ γ X — H β 2	(12)
β  2	2
where the second term denotes the regularization term of source domain maintenance. Xs ∈ Rns ×L is the source-domain instance matrix. The
penalty factors λ and γ are used to control the contributions of each term. To prevent the overfitting issue, (12) is augmented with a complexity measure term as follows:
min 1  2  λ   T  T	γ	2
source-domain instances. The UA framework is illustrated in Fig. 2.

Experimental analysis

In this section, we conduct a large number of experiments using well- logging data collected from multiple wells in Jiyang depression, Bohai Bay Basin to verify the validity and interpretability of our method. First, we describe the dataset and the experimental settings. Second, we compare the performances of UA, S.O., and T.O., and present the accu- racy of each lithology via the bar graphs. Finally, we show the visualized results and discuss the interpretability of our method. Here, S.O., means the model trained on source-domain dataset only, which could be seen as the lower bound of UA. If UA has a lower accuracy than S.O., then negative transfer happens. T.O. means the model trained on target- domain dataset only, which could be seen as the upper bound of UA.

Experimental settings

The dataset consists of three wells, which is shown in Table 1. Wells A, B, and C are from the same area and have the same types of logs and
lithologies, that is, they have the same input and output space. To verify


β
β  + 2 Tr(β H ΩHβ)+ 2 Xs — Hsβ 
.	(13)
the calibration effect, a three-class ELM is trained to classify three types of lithologies and evaluated by average F1-Score which considers both



the accuracy and recall ratios of the classification model. F1-Score could be expressed by
CAL, so the calibrated R25 can be seen as that under the situation without
borehole expansion. Such a smoothness in R25 avoids many mis- classifications. In areas of c and d, although the calibrated logs change

averageF1 — Score = F1 + F2 + ⋯ + Fc
c
(15)
greatly, it keeps the basic variation trend of the original logs.
The following analysis are based on the statistic information of logs,

where c is the number of categories and the Fi is F1-Score of i-th category which can be expressed by
i.e., the differences of means and distribution histograms between two domains. Taking A → B and A → C as an examples, it can be seen from Table 3 and Table 4 that the mean of each log in the aligned (calibrated)

F = 2 × Pi × Ri
(16)
target domain and source domain are almost the same as those in the

i	Pi + Ri

where the Pi represents the i-th Precision and Ri is the Recall, which can be computed by
P =	TP
TP + FP
original source domain. This indicates that the means are aligned unilaterally. Subsequently, Fig. 6 presents the distribution histograms of Well A SP and Well B SP in two domains. It can be seen that the SP log distribution of target domain is obviously aligned to the one of source domain, while the original SP log distribution of Well A and Well B is quite different. This could be the reason why the F1-score in the task A →

R =		TP TP + FN
(17)
B could be increased greatly as shown in Table 2. According to Fig. 7, it can be seen that there is a small difference in SP log distribution between Well A and Well C, so S.O. method used to predict Well C can achieve a

where the TP, FP and FN represent true positive(an instance are positive classes and are judged to be positive classes), false positive(a false class is judged to be a positive class) and false negative(a false class is judged to be a negative class), respectively.
Due to the wide range of logs, we use min-max normalization to convert all logging data to those within 0 and 1. The number of hidden neurons are set 800, the weights and bias are generated randomly by the Gaussian distribution with variance equalling 3. The trade-off parameter γ is searched from 10—3 to 104, and λ from 102 to 106. S.O. and T.O. are realized by weighted ELM which could eliminate the influence of un- balanced classes. All experiments are implemented by Python 3.8 on a desktop computer with 2.90-GHz CPU and 16.00-GB RAM.

Comparative study

The performances of these methods are evaluated by different com- binations of wells on the experimental dataset. For example, we set Well A as the source domain for training and Well B as the target domain for testing (i.e., task A → B). Therefore, there are a total of six combinations to be compared, i.e., A → B, A → C, B → A, B → C, C → A, and C → B. Table 2 lists the macro-average F1-scores of these methods on 6 tasks. We have the following observations. (i) In all six tasks, the high F1-score was achieved by using T.O. method, while the F1-score was significantly decreased by using S.O. method. This indicates that the data distribution varies from well to well. (ii) The F1-scores of UA performing on all tasks are higher than S.O., which shows the effectiveness of UA in the case of data discrepancy. (iii) The F1-scores of UA is very close to those of the
T.O. baselines. Especially, it can reach 97.2% on the task A → B. (iv) In
the six tasks, the F1-scores of UA are improved by about 10% compared with S.O. baseline method, and even improved by 20% on the task B → C.
(v) Although the F1-score of S.O. baseline is relatively high, UA still have an improvement on task A → C. The F1-scores for each lithology are shown in Fig. 3. It is easy to find that S.O. usually overfits to the mudstone so that its F1-score might be high on some tasks. UA eliminates the dis- tribution discrepancies in the six tasks, so the classification after logs calibration could perform well on each lithology.

Deeper discussions

Taking experiment A → B as an example, Fig. 4 and Fig. 5 show the changes of logs of Well A and Well B. It is obvious that each log in source domain has a slight change as shown in Fig. 4. Most of the macroscopic characteristics are preserved. Some microscopic characteristics are weakened due to low-pass filtering function of the regularization term of source domain maintenance. As shown in Fig. 5, some logs has a great change and the predictions are significantly improved by UA Observing areas a and b in Fig. 5, we think that UA smoothen R25 by integrating
high F1-score of 92.3%. In this case, UA should align to Well A in the high-dimensional space while preserving the original information of Well C as much as possible. The frequency distribution histograms of other logs are shown in Figs. 8–11 which all support the superiority of UA

Conclusions and further work

In this paper, we formulated the geophysical logs calibration problem and give its statistical explanation, and exhibited an interpretable ma- chine learning method, i.e., UA, which could align the logs from one well to another without losing the physical meanings. Massive experiments in 3 wells and 6 tasks showed the effectiveness and interpretability from multiple views. The involved UA method is an unsupervised feature domain adaptation method, so it does not rely on any labels from cores. Actually, the source-domain labels could be used to pre-train a model and outputs some pseudo-labels for the target domain, such that we can obtain a conditional MMD instead of the marginal MMD used in the paper. With the conditional MMD, the conditional probability distribu- tion of both domain could be aligned, which might have a better cali- bration effect than the current work.

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 61903353, and in part by the SINOPEC Programmes for Science and Technology Development under Grant PE19008-8.

References

Bergen, K.J., Johnson, P.A., de Hoop, M.V., et al., 2019. Machine learning for data-driven discovery in solid earth geoscience. Science 363 (6433).
Birnie, C., Ravasi, M., Liu, S., Alkhalifah, T., 2021. The potential of self-supervised networks for random noise suppression in seismic data. Artif. Intell. Geosci. 2, 47–59.
Chang, J., Li, J., Kang, Y., Lv, W., Xu, T., Li, Z., Xing Zheng, W., Han, H., Liu, H., 2021a. Unsupervised domain adaptation using maximum mean discrepancy optimization for lithology identification. Geophysics 86 (2), ID19–ID30.
Chang, J., Kang, Y., Zheng, W.X., Cao, Y., Li, Z., Lv, W., Wang, X.-M., 2021b. Active domain adaptation with application to intelligent logging lithology identification. IEEE Trans. Cybern.
Chang, J., Kang, Y., Li, Z., Zheng, W.X., Lv, W., Feng, D.-Y., 2022. Cross-domain lithology identification using active learning and source reweighting. IEEE Geoscience and Remote Sensing Letters 19, 1–5.
Chen, Y., Song, S., Li, S., Yang, L., Wu, C., 2018. Domain space transfer extreme learning machine for domain adaptation. IEEE Trans. Cybern. 49 (5), 1909–1922.



Dunham, M.W., Malcolm, A., Kim Welford, J., 2020. Improved well-log classification using semisupervised label propagation and self-training, with comparisons to popular supervised algorithms. Geophysics 85 (1), O1–O15.
Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.
Artif. Intell. Geosci. 1, 11–23.
Gretton, A., Borgwardt, K., Rasch, M., Scho€lkopf, B., Smola, A., 2006. A kernel method for the two-sample-problem. Adv. Neural Inf. Process. Syst. 19, 513–520.
Huang, G.-B., Zhu, Q.-Y., Siew, C.-K., 2004. Extreme learning machine: a new learning scheme of feedforward neural networks. In: Proceedings of the IEEE International Joint Conference on Neural Networks, vol. 2. Ieee, pp. 985–990.
Huang, G.-B., Zhou, H., Ding, X., Zhang, R., 2011. Extreme learning machine for regression and multiclass classification. IEEE Trans. Syst. Man Cybern. B (Cybernetics) 42 (2), 513–529.
Imamverdiyev, Y., Sukhostat, L., 2019. Lithological facies classification using deep convolutional neural network. J. Petrol. Sci. Eng. 174, 216–228.
Li, S., Song, S., Huang, G., Wu, C., 2018. Cross-domain extreme learning machines for domain adaptation. IEEE Trans. Syst. Man Cybern.: Systems 49 (6), 1194–1207.
Li, Z., Kang, Y., Feng, D., Wang, X.-M., Lv, W., Chang, J., Zheng, W.X., 2020. Semi- supervised learning for lithology identification using Laplacian support vector machine. J. Petrol. Sci. Eng. 195, 107510.
Li, Z., Kang, Y., Lv, W., Zheng, W.X., Wang, X.-M., 2021. Interpretable semisupervised classification method under multiple smoothness assumptions with application to lithology identification. Geosci. Rem. Sens. Lett. IEEE 18 (3), 386–390.
Liu, H., Wu, Y., Cao, Y., Lv, W., Han, H., Li, Z., Chang, J., 2020. Well logging based lithology identification model establishment under data drift: a transfer learning method. Sensors 20 (13), 3643.
Long, W., Chai, D., Aminzadeh, F., 2016. Pseudo density log generation using artificial neural network. In: SPE Western Regional Meeting. OnePetro.
Lv, W., Kang, Y., Zhao, Y., 2018. Self-tuning asynchronous filter for linear Gaussian system and applications. IEEE/CAA J. Automat. Sin. 5 (6), 1054–1061.
Magrini, F., Jozinovi´c, D., Cammarano, F., Michelini, A., Boschi, L., 2020. Local earthquakes detection: a benchmark dataset of 3-component seismograms built on a global scale. Artif. Intell. Geosci. 1, 1–10.
Ruckebusch, G., 1983. A kalman filtering approach to natural gamma ray spectroscopy in well logging. IEEE Trans. Automat. Control 28 (3), 372–380.
Stojanov, P., Li, Z., Gong, M., Ca, R., Carbonell, J., Zhang, K., 2021. Domain adaptation with invariant representation learning: what transformations to learn? Adv. Neural Inf. Process. Syst. 34.
van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9 (Nov), 2579–2605.
Vapnik, V., 1999. The Nature of Statistical Learning Theory. Springer science & business media.
Wu, Y., Yang, Y., Lv, W., Chang, J., Li, Z., Feng, D., Xu, T., Li, J., 2022. Robust unilateral alignment for subsurface lithofacies classification. IEEE Trans. Geosci. Rem. Sens. 60, 1–13.
Xu, T., Chang, J., Kang, Y., Lv, W., Li, J., Han, H., Liu, H., 2022. Intelligent cross-well sandstone prediction based on convolutional neural network. IEEE Geoscience and Remote Sensing Letters 19, 1–5.
Yang, Q., Zhang, Y., Dai, W., Pan, S.J., 2020. Transfer Learning. Cambridge University Press.
Zhong, R., Johnson, R.L., Chen, Z., 2020. Using machine learning methods to identify coal pay zones from drilling and logging-while-drilling (lwd) data. SPE J. 25, 1241–1258, 03.
Zhou, R., Cai, Y., Zong, J., Yao, X., Yu, F., Hu, G., 2020. Automatic fault instance segmentation based on mask propagation neural network. Artif. Intell. Geosci. 1, 31–35.
Zhu, L., Li, H., Yang, Z., Li, C., Ao, Y., 2018. Intelligent logging lithological interpretation with convolution neural networks. Petrophysics 59, 799–810, 06.
