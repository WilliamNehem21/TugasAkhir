 AASRI Procedia  8 ( 2014 )  112 – 117 
Available online at www.sciencedirect.com
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.08.019 
ScienceDirect
2
Abst
This
trade
the tr
achie
VQ (
98.15
© 20
Sele
Keyw
1. In
V
exce
com
gain
A
the V
*
E
2014 AASR
N
tract
paper proposes
e-off and learnin
rade-off aspect
eved. In the lea
(FSVQ) as an i
5% were achiev
014. Published
ection and/or p
words: binary sear
ntroduction 
Vector quantiz
ellent rate-dist
mputation requ
ned by using th
Among existin
VQ search sp
Corresponding a
E-mail address: s9
RI Conferen
Bin
Ning-Yun 
a NTUT, Elect
s a fast search 
ng aspects (TLA
t, a slight loss 
arning aspect, th
inferred functio
ved, which conf
d by Elsevier 
peer review un
rch; G.729; line s
zation (VQ) is
tortion perform
uirements. Thu
he VQ approa
ng fast search 
pace. The sear
author. Tel.: +886
94310393s@hotm
nce on Spor
nary Sear
Kua, Shun
trical Engineering
algorithm for v
A) were used to
occurred in the
he binary search
on. In the exper
firmed the exce
B.V.
nder responsib
pectrum pair (LS
s a lossy and 
mance and sim
us, numerous
ach.
methods, the
rch spaces of
6930204341. 
mail.com. 
rts Engineer
rch Vect
n-Chieh Ch
g #1, Sec. 3, Chu
vector quantizat
o enhance the l
e quantization q
h space was dev
riment, comput
ellent performan
bility of Amer
SP); speech codec
powerful met
mple structur
 studies have
e tree-structure
f the TSVQ co
ring and Com
tor Quan
hanga, Shaw
ng-hsiao East Rd
tion (VQ) base
ine spectrum pa
quality; howev
veloped using t
tational savings
nce of the BSS-
rican Applied 
c; vector quantiza
thod for mult
re. However, f
e focused on t
ed VQ (TSVQ
ode-book are 
mputer Scie
ntization
w-Hwa Hw
d., Taipei, Taiwan
d on a binary s
air (LSP) encod
er, substantial 
the learning pro
s of 86.19% and
-VQ approach.
Science Rese
ation (VQ); 
timedia comm
full search alg
the computati
Q) method [2]
reduced to 2
ence (SECS
n
wanga,*
n, ROC 
search space (B
der of the G.729
computational 
ocess, which us
d a quantization
earch Institute 
munications be
gorithms have
ional savings 
] was propose
log2(N) which
S 2014) 
BSS-VQ). The 
9 standard. In 
savings were 
ses full search 
n accuracy of 
ecause of its 
e substantial 
that can be 
ed to reduce 
h makes the 
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
113
 Ning-Yun Ku et al. /  AASRI Procedia  8 ( 2014 )  112 – 117 
method powerful; however, the meth-od’s loss of quantization quality is problematic. The TSVQ is a binary 
search method that uses a single path to traverse the appropriate codewords. Thus, the TSVQ encoder does 
not ensure that the selected codeword is the closest to the input vector. 
A previous study [1] applied triangular inequality elimination (TIE) to VQ-based image coding, achieving 
computational savings of more than 90%. However, the TIE approach is dependent on the correlation 
characteristics of input signals. The noise-like (weak correlation characteristic) input vector reduces the 
performance of the TIE approach. 
In [3], the quasi-binary search (QBS) algorithm and trade-off aspect between TSVQ and TIE were 
proposed to reduce the computational complexity of the VQ algorithm. The performance was superior to that 
of TIE, particularly regarding the noise-like input signal. The QBS algorithm is not dependent on the 
correlation characteristics of input signals. Although the quantization accuracy of the QBS algorithm was 
99.16%, which was reasonably good, the computational savings of 59.43% for the LSP encoder of the G.729 
standard were unsatisfactory. 
Although VQ is a powerful method, it confronts a number of problems in gaining computational savings 
for the LSP encoder of the G.729 standard. First, a moving average (MA) filters the LSP parameter, and the 
redundant source data is then extracted and removed. The MA filter thereby eliminates the correlation 
characteristics of the bias of LSP. Thus, the performance of these TIE-based algorithms decreases in the 
G.729 standard. Second, a small codebook size is provided in the multistage VQ of the G.729 standard. All 
codewords are adjacent and close to each other; therefore, the TIE algorithm cannot work efficiently. Third, 
the TSVQ achieves considerable computational savings, but loses substantial quantization quality. The QBS 
approach loses slight quantization quality and achieves unsatisfactory computational savings. 
Another study [4] proposed the fast search method for VQ. However, none of those methods focus on the 
LSP encoder of the G.729 standard. This paper proposes a binary search space VQ (BSS-VQ) approach, 
which balances computational savings and quantization quality, and achieves considerable computational 
savings with only a slight loss in quantization quality. Moreover, the learning aspects of neural networks and 
VQ were also employed to improve the performance of the BSS-VQ approach. The LSP encoder with the 
G.729 standard was used to verify the performance of the BSS-VQ. The following section describes the BSS-
VQ algorithm. The last two sections present the experimental results and conclusion. 
2. BSS-VQ algorithm 
This section presents a fast search BSS-VQ algorithm that employs a fast locating approach to determine a 
small search space and fully search it to obtain an optimally matched codeword. A learning algorithm was 
proposed to build a BSS, which uses full search VQ (FSVQ) as an inferred function. The BSS was trained 
using large amounts of training data, which efficiently obtained codewords for each subspace. However, some 
trifling regions existed between splitting boundaries and codeword boundaries, which implied that the training 
data fell in trifling regions with little possi-bility. Therefore, a learning algorithm with insufficient training 
may lead to missed encoding in the trifling region which no training data fall in when the training process, 
resulting in a decline in quantization accuracy; conversely, each subspace can be fully trained using a learning 
algorithm with sufficient training data, but it with highly overlapped codewords; a BSS with highly 
overlapped codewords results in a decline in computational savings. Based on the aforementioned analysis, 
Algorithm 1 de-scribes the BSS generated by the learning process in the BSS-VQ algorithm. Algorithm 2, 
concerning the BSS-VQ encoding process, is also presented in detail. 
114  
 Ning-Yun Ku et al. /  AASRI Procedia  8 ( 2014 )  112 – 117 
2.1. Algorithm 1: BSS generation for BSS-VQ 
Step1: BSS determination. 
Step 1.1: Select 
�� � �������� � � � �� � � � � �� 
 
(1) 
which is the original codebook of VQ, to train the BSS. The term N is the codebook size and D is the 
codeword dimension. 
Step 1.2: Calculate 
�� �
�
� �
�����
�
���
� � � � � � 
 
(2) 
as the centroid of CB in the jth dimension. 
Step 1.3: Determine 
��� � ������� � � � ��� 
 
(3) 
as the BSS, which is generated from dichotomy splitting according to the centroid Cj. The BSS contains 2D
subspaces, which are empty of codewords at the initialization stage. 
Step 1.4: Empty 
��� � ��������� � � � ��� � � � � �� 
 
(4) 
as the training placement statistics, which are used to determine how often the inferred values fall into cwi of 
bssk.
Step 2: Learning process. 
Step 2.1: The term 
� � ����� � � � �� 
 
(5) 
is adopted as the training database, S is used to build the content codeword of the BSS, where si is the D-
dimensional vector, and M is the data number. The term M is a large number, indicating that the training data 
are sufficient to fully train each binary search subspace. 
Step 2.2: Obtain 
��� � �������� 
 
(6) 
115
 Ning-Yun Ku et al. /  AASRI Procedia  8 ( 2014 )  112 – 117 
The term FSVQ(si) denotes a full search VQ algorithm. It is used to determine the optimally matched 
codeword, cwn, from CB for input vector si.
Step 2.3:Determine 
���� � ��������� 
 
(7) 
� � �
�� � �����
�
���
� ����� � ��� ���� � ��
�� ���� � �� 
 
(8) 
The BSSVQ(si) is proposed in this paper, and is used to determine the nearest subspace bssp for input vector 
si. The BSSVQ(.) requires only D times comparison. 
Step 2.4: Set 
��� � ���� 
 
(9) 
and increase tpsp,n by one. The optimal matched codeword cwn is inserted into bssp if cwn does not belong to 
bssp.
Step 2.5: Repeat Steps 2.2 to 2.4 with each input vector si until i=M. The sum of TPS is M in the end. 
Step 2.6: Normalize 
��� � ������� ���������
�
�� � � � ��� � � � � ��� ��������� � �
������
�
���
 
(10) 
Step 3: BSS screening. 
Step 3.1: Denote 
��� � ��������� � � � ��� � � � � ��� 
 
(11) 
as the learned binary search space, where Ni is the number of codewords in bssi. Set the optimal thresholds 
�gsn and �vss.
Step 3.2:Sort bssi.The sorted 
���� � �������������� � ��������� � � � � ���  
(12) 
The codewords of bssi are sorted in descending order by tpsi.
Step 3.3: Screen bssi. The new 
���� � ��������� � � � ��� 
 
(13) 
116  
 Ning-Yun Ku et al. /  AASRI Procedia  8 ( 2014 )  112 – 117 
The term 
�� � �
��� �� � ����
�������� � ���� � ��������� �� � ����
 
 
(14) 
is the effective search subspace. The dismissed subspace ����� � ���������� � � � ��� is filtered out by 
threshold �vss, which contains less likely codewords of bssi.
Step 3.4: Repeat Steps 3.2 to 3.3 until i=2D.
2.2. Algorithm 2: A BSS-VQ encoding process 
Step 1: Denote 
� � ����� � � � �� 
 
(15) 
as the input signals, where xi is a D-dimensional vector. 
Step 2: Determine 
���� � ���������� � � �
�� � �����
�
���
 
 
(16) 
where 
����� � ��� ���� � ��
�� ���� � �� 
 
(17) 
The BSSVQ(.) function is used to determine the subspace bssq, and costs D times comparison. 
Step 3: Obtain 
��� � �������� 
 
(��) 
All codewords within subspace bssq are set as the new codebook, and the FSVQ(.) is subsequently used to 
obtain optimally matched codeword cwm from bssq. The codeword number of subspace bssq is lower than that 
of the original codebook. Thus, computational complexity is substantially reduced. 
Step 4: Repeat Steps 2 and 3 with each input vector xi until i = t. 
3. Experimental results 
This study used the LSP encoder in the G.729 standard with a 10-D codebook and 128 codewords to verify 
the performance of BSS-VQ. A speech database, which was recorded at a sampling rate of 8 kHz and a 
resolution of 16 bits, was used on the inside test; it contained male speech, female speech, background noise, 
and silence, and its data number M was 601 422 (45.8 MB). The trained binary search space whose average, 
minimum, and maximum number of codewords were 16.53, 8, and 26, respectively. Moreover, the 15.3 MB 
117
 Ning-Yun Ku et al. /  AASRI Procedia  8 ( 2014 )  112 – 117 
speech data for the outside test data were used to verify the performance of BSS-VQ. As shown in Table 1, 
computational savings of 86.19% and a quantization accuracy of 98.15% were achieved. The computational 
savings of BBS-VQ outperform that of the QBS-VQ approach when their quantization accuracies are similar. 
Moreover, the quantization accuracy of BSS-VQ outperforms that of TSVQ when their computational savings 
are similar. These experimental results confirmed the excellent performance of the BSS-VQ approach. 
Table 1. The computational saving of LSP encoder with G.729 standard for outside test with FSVQ, TIE, QBS, BSSVQ, and TSVQ 
algorithms 
 
Computational Savings
Quantization Accuracy
FSVQ
0% 
100% 
TIEVQ 
23.65% 
100% 
TSVQ
89.06% 
46.61% 
QBSVQ 
59.43% 
99.16% 
BSS-VQ 
86.19% 
98.15% 
4. Conclusion 
This paper presents a BSS-VQ algorithm to enhance the LSP encoder of the G.729 standard. The trade-off 
and learning aspects were used to achieve optimal performance. The BSS-VQ algorithm is not dependent on 
the correlation characteristics of input signals and outperforms most existing algorithms in the LSP encoder of 
the G.729 standard. 
References 
[1] Choi, S.Y. and Chae, S.I. Incremental-Search Fast Vector Quantiser Using Triangular Inequalities for 
Multiple Anchors. Electronics Letters, 1192-1193; 1998. 
[2] Djamah, M. and O’S, D. An efficient tree-structured codebook design for embedded vector quantization. 
IEEE International Conference onAcoustics Speech and Signal Processing (ICASSP), 4686-4689; 2010. 
[3] Yan, L.J. and Hwang, S.H. The binary vector quantisation. Proc. Third IEEE Int. Symp. on 
Communication, Control, Signal Processing (ISCCSP), 604–607; 2008. 
[4] Chen, S.X. and Li, F.W. Fast encoding method for vector quantisation of images using subvector 
characteristics and Hadamard transform.IET Image Processing, 18-24; 2011. 
