Array 15 (2022) 100192
Available online 30 May 2022
2590-0056/© 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/locate/array
A parameter-free graph reduction for spectral clustering and SpectralNet
Mashaan Alshammari a,∗, John Stavrakakis b, Masahiro Takatsuka b
a College of Computer Sciences and Engineering, University of Hail, Hail 81411, Saudi Arabia
b School of Computer Science, The University of Sydney, NSW 2006, Australia
A R T I C L E
I N F O
Keywords:
Spectral clustering
SpectralNet
Graph reduction
Local scale similarity
A B S T R A C T
Graph-based clustering methods like spectral clustering and SpectralNet are very efficient in detecting clusters
of non-convex shapes. Unlike the popular 𝑘-means, graph-based clustering methods do not assume that each
cluster has a single mean. However, these methods need a graph where vertices in the same cluster are
connected by edges of large weights. To achieve this goal, many studies have proposed graph reduction
methods with parameters. Unfortunately, these parameters have to be tuned for every dataset. We introduce
a graph reduction method that does not require any parameters. First, the distances from every point 𝑝 to its
neighbors are filtered using an adaptive threshold to only keep neighbors with similar surrounding density.
Second, the similarities with close neighbors are computed and only high similarities are kept. The edges
that survive these two filtering steps form the constructed graph that was passed to spectral clustering and
SpectralNet. The experiments showed that our method provides a stable alternative, where other methods’
performance fluctuated according to the setting of their parameters.
1. Introduction
The problem of detecting clusters of non-convex geometric shape,
has been long studied in the literature of pattern recognition. The solu-
tions of this problem could be broadly classified into two categories:
kernel- and graph-based methods. Kernel-based methods attempt to
map the points into a space where they can be separated. The em-
bedding function 𝜙 ∶ R𝐷 → R𝑀 maps points from the original space
to an embedding space. Defining the embedding function 𝜙 is usually
unknown and could be computationally expensive [1]. On the other
hand, graph-based methods use the graph 𝐺(𝑉 , 𝐸) whose set of vertices
represents the data points and its set of edges represents the similarity
between each pair of vertices. Finding non-convex clusters in a graph
could be done in three ways: (1) by iteratively coarsening and partition-
ing the graph [2], (2) by performing spectral clustering [3], and (3) by
feeding the graph 𝐺(𝑉 , 𝐸) to a neural network (SpectralNet) [4]. The
first way of detecting clusters in a graph is iterative which involves two
deficiencies: the risk of being trapped in a local minima and the need
for a stopping condition. This makes spectral clustering and SpectralNet
more appealing for studies conducting graph-based clustering.
Spectral clustering starts by constructing a graph 𝐺(𝑉 , 𝐸). The sets
of vertices 𝑉 and edges 𝐸 represent data points and their pairwise
similarities. Spectral clustering detects clusters by performing eigen-
decomposition on the graph Laplacian matrix 𝐿 and running 𝑘-means
on its top eigenvectors [5]. The computational bottleneck represented
∗ Corresponding author.
E-mail addresses: mashaan.alshammari@uoh.edu.sa (M. Alshammari), john.stavrakakis@sydney.edu.au (J. Stavrakakis), masa.takatsuka@sydney.edu.au
(M. Takatsuka).
by the eigen-decomposition would cost the algorithm computations in
the order of (𝑁3) [6]. This stimulated the research on reducing these
computations by reducing the graph vertices and/or edges. However,
the need for a memory efficient graph creates another problem related
to the number of parameters associated with the process of graph
construction. Deciding the number of reduced vertices and how the
edges are eliminated would create several parameters that need careful
tuning.
SpectralNet [4] uses Siamese nets to learn affinities between data
points. Then it feeds these affinities to a neural network to find a
map function 𝐹𝜃, which maps the graph vertices 𝑉 to an embedding
space where they can be separated using 𝑘-means. The Siamese nets
expect the user to label which pairs are positive (similar) and which
are negative (dissimilar). An unsupervised pairing uses the 𝑘-nearest
neighbors, where the nearest neighbors are the positive pairs and the
farthest neighbors are the negative pairs. The parameter 𝑘 requires
manual tuning. It also restricts the number of edges to be exactly 𝑘,
regardless of the surrounding density around the data point.
In her spectral clustering paper, von Luxburg [7] wrote about the
advantages of mutual k-nearest neighbor graph, and how it ‘‘tends not to
connect areas of different density’’. She highlighted the need for having a
‘‘heuristic to choose the parameter k’’. We introduce a graph reduction
method that does not require any parameters to produce a mutual
https://doi.org/10.1016/j.array.2022.100192
Received 21 January 2022; Received in revised form 14 May 2022; Accepted 22 May 2022
Array 15 (2022) 100192
2
M. Alshammari et al.
graph with a reduced number of edges compared to the size of the full
graph 𝐸 = 𝑁 × 𝑁, where 𝑁 is number of all vertices. It initially finds
the mean distance that best describes the density around a point. Then,
it computes the pairwise similarities based on: (1) the distance between
a pair of points, and (2) the mean distance of the surrounding density.
Finally, we construct a mutual graph where a pair of vertices must be
in each other’s nearest neighbors sets. We used two graph applications
for the experiments: spectral clustering and SpectralNet. The proposed
method provides a stable alternative compared to other methods where
their performance was determined by the selected parameters.
Our main contribution in this work, is eliminating manually tuning
parameters that affect the clustering accuracy when changed. The
graph partitioning methods used in this work are spectral clustering [7]
and SpectralNet [4].
2. Related work
The problem of detecting non-convex clusters has led to the develop-
ment of numerous clustering methods. These methods have abandoned
the assumption that a cluster has a single mean. Instead, they rely on
pairwise similarities to detect clusters. Graph-based clustering involves
two steps: (1) reducing the graph, and (2) partitioning the graph. The
proposed method in this paper falls under graph construction methods.
Spectral clustering uses eigen-decomposition to map the points into
an embedding space, then groups similar points. One of the impor-
tant application of spectral clustering is subspace clustering [8,9].
The performance of spectral clustering is determined by the similarity
metric used to construct the affinity matrix 𝐴. The earlier works of
subspace clustering used affinities based on principal angles [10]. But
recent studies have used sparse representation of points to measure
the similarity [8,11,12]. Spectral clustering requires computations in
order of (𝑁3), due to the eigen-decomposition step. A straightforward
solution to this problem is to reduce the size of the affinity matrix 𝐴.
This can be done in two ways: (1) reducing the set of vertices 𝑉 , or (2)
reducing the set of edges 𝐸.
Reducing the number of vertices is done by placing representatives
on top of the data points, and then using those representatives as
graph vertices. Placing a representative could be done by sampling
(like 𝑘-means++ [13]) or by vector quantization (like self-organizing
maps [14]). A well-known method in this field is ‘‘k-means-based ap-
proximate spectral clustering (KASP)’’ proposed by Yan et al. [6]. KASP
uses 𝑘-means to place representatives. Other efforts by Tasdemir [15]
and Tasdemir et al. [16] involved placing representatives using vector
quantization, and a nice feature of these methods is that the pairwise
similarities are computed during the vector quantization. The problem
with these methods is the parameter 𝑚, which is the number of repre-
sentatives. Specifically, how should we set 𝑚? And how would different
values of 𝑚 affect the clustering results?
Reducing the graph edges could be done by setting the neighbor-
hood conditions. For instance, let 𝑝 be the center of a ball 𝐵(𝑝, 𝑟) with
radius 𝑟 and 𝑞 be the center of a ball 𝐵(𝑞, 𝑟). 𝑝 and 𝑞 are connected if
and only if the intersection of 𝐵(𝑝, 𝑟) and 𝐵(𝑞, 𝑟) does not contain other
points [17]. Such graphs are called Relative Neighborhood Graphs
(RNGs). Correa and Lindstrom [18] used a 𝛽-skeleton graph for spec-
tral clustering. However, the parameter 𝛽 needs tuning. Alshammari
et al. [3] introduced a method to filter edges from a 𝑘-nearest neighbor
graph. However, it still needs an influential parameter, which was the
mean of the baseline distribution of distances 𝜇0. Another local method
to reduce the number of edges was proposed by Satuluri et al. [19]. The
authors measured the similarity between two vertices using adjacency
lists overlap, a metric known in the literature as shared nearest neigh-
bor similarity [20]. A graph sparsification method based on effective
resistance was proposed by Spielman and Srivastava [21,22]. Their
method was theoretically solid, but the definition of effective resistance
breaks the cluster structure of the graph. Vertices with more short
paths have low effective resistance and the method disconnects them.
Retaining the cluster structure of the graph requires connecting such
vertices [19].
In spectral clustering, the obtained spectral embedding cannot be
extended to unseen data, a task commonly known as out-of-sample-
extension (OOSE). Several studies have proposed solutions to this prob-
lem. Bengio et al. [23] used Nyström method to approximate the
eigenfunction for the new samples. But they have to check the similarity
between the training and new samples [24]. Alzate and Suykens [25]
proposed binarizing the rows of eigenvectors matrix where each row
corresponds to a single training data point. By counting row occur-
rences, one can find the 𝑘 most occurring rows, where each row
represents an encode vector for a cluster. To label a test sample,
its projection was binarized and it is assigned to the closest cluster
based on the minimum Hamming distance between its projection and
encoding vectors. Levin et al. [26] proposed a linear least squares
OOSE, which was very close to Bengio et al. [23] approach. They also
proposed a maximum-likelihood OOSE that produces a binary vector
⃖⃗𝑎 indicating whether the unseen sample has an edge to the training
samples or not.
All previous methods that provided an out-of-sample-extension
(OOSE) to spectral clustering have relied on eigen-decomposition,
which becomes infeasible for large datasets. The newly proposed Spec-
tralNet [4] is different from spectral clustering in a way that it does not
use eigen-decomposition step. Instead, SpectralNet passes the affinity
matrix 𝐴 into a deep neural network to group points with high similar-
ities. Yet, SpectralNet still needs a graph construction method. Previous
SpectralNet works have used 𝑘-nearest neighbor graph, but they have to
set the parameter 𝑘 manually. It also restricts the number of edges to be
exactly 𝑘, regardless of the surrounding density around the data point.
Dense clusters require more edges to be strongly connected. Strong
connections ensure closer positions in the embedding space. Also,
SpectralNet methods randomly choose the negative pairs. This random
selection makes the method inconsistent in independent executions.
Considering the literature on reduced graphs for spectral cluster-
ing and SpectralNet, it is evident that they have two deficiencies.
First, certain parameters are required to drive the graph reduction
process. Second, the involvement of random steps makes these methods
inconsistent over independent executions.
3. Reducing the graph size without the need for parameters
The motivation behind our work was to avoid the use of any
parameters during the graph reduction. The input for our method is a
𝑘-nearest neighbor graph. Although this 𝑘-nn graph is sparsified, it still
connects clusters with different densities. The value of 𝑘 has limited
influence on the final graph because it is not final, and most of the
unnecessary edges created by 𝑘-nn will be removed in the reduction
process. The method starts by finding the value of 𝜎𝑝 that best describes
the local statistics around a randomly chosen point 𝑝. Then, it filters
the edges with low weights. Finally, it checks the mutual agreement
for each edge.
3.1. Finding the value of 𝜎𝑝
To compute pairwise similarities, we used the similarity measure
introduced by [27], which is defined as follows:
𝐴𝑝𝑞 = exp
(−𝑑2 (𝑝, 𝑞)
𝜎𝑝𝜎𝑞
)
.
(1)
where −𝑑2 (𝑝, 𝑞) is the distance between points 𝑝 and 𝑞. 𝜎𝑝 and 𝜎𝑞 are
the local scales at points 𝑝 and 𝑞 respectively. What is good about
this similarity measure is that it uses two sources of information to
compute the pairwise similarity: (1) the distance between them, and
(2) the surrounding density for each point. Points belonging to clusters
with different densities would have a low similarity even if they are
Array 15 (2022) 100192
3
M. Alshammari et al.
Fig. 1. The process of computing 𝜎𝑝 for a point 𝑝. (Best viewed in color).
Fig. 2. After computing the pairwise similarities, we include highly similar edges for a point 𝑝.
(For interpretation of the references to color in this figure legend, the reader is
referred to the web version of this article.)
Fig. 3. Synthetic datasets used in the experiments.
separated by a small distance This makes this measure superior for
highlighting different clusters separated by a small distance.
One problem that arises from using this measure in Eq. (1) is how to
set the value of 𝜎𝑝 in the denominator. In previous studies, it was set as
the distance to the 7th neighbor [27,28]. However, there is no evidence
that the distance to the 7th neighbor would work in every dataset.
Using the data to select this parameter would be more practical.
The idea behind the parameter 𝜎𝑝 is to measure the sparseness of a
cluster. If 𝑝 lies in a sparse cluster, it would have a large 𝜎𝑝; whereas if
𝑝 lies in a dense cluster, it would have a small 𝜎𝑝. To achieve this, we
need to exclude neighbors with different local density than 𝑝 from being
included in computing 𝜎𝑝. We used a smooth histogram of distances to
characterize the local density for 𝑝 neighbors (as shown in Fig. 1). The
intuition is if a neighbor has different local density than 𝑝, this would
be represented as a peak on the histogram. The histogram bin values for
each point are smoothed using the moving weighted average (MWA).
The smoothing was designed as follows:
𝑀𝑊 𝐴𝑖 = 𝑣𝑖−1 + 𝑣𝑖 + 𝑣𝑖+1
𝑟𝑖−1 + 𝑟𝑖 + 𝑟𝑖+1
,
(2)
where 𝑣 is the value of the bin and 𝑟 is the rank of the bin, with
𝑟 = 1 being the bin containing the closest points to the point 𝑝. This
smoothing assigns weights to the bins based on their distance from 𝑝,
with high weights assigned to closer bins and low weights assigned to
bins further away.
The histogram threshold tells us that up to the Kth neighbor, the
local density of 𝑝 has not changed. Then, we compute 𝜎𝑝 as the mean
distance from the 1st to the Kth neighbor. This process is described in
statements 4 to 9 in Algorithm 1.
Array 15 (2022) 100192
4
M. Alshammari et al.
Fig. 4. Results with the synthetic data, all values are for 50 runs. (Best viewed in color).
3.2. Reducing the graph edges
Once we have 𝜎𝑝 for each point, we can calculate the pairwise
similarities using the formula in Eq. (1), as shown in statements 10 to
14 in Algorithm 1. Large values indicate highly similar points, whereas
low values indicate dissimilarity. We build another histogram of all the
pairwise similarities using the Freedman–Diaconis rule [29] as shown
in Fig. 2. For each point, similarities lower than the threshold 𝑇𝑝 are
eliminated. If the maximum similarity is larger than the mean plus the
standard deviation 𝜇 + 𝜎, the threshold is set as 𝑇 = 𝜇 + 𝜎. If not, the
threshold is set as 𝑇 = 𝜇 − 𝜎. Fig. 2 shows the included similarities as
blue bins and the excluded similarities as red bins. The graph edges are
defined as:
(𝑝, 𝑞) ∈ 𝐸(𝐺) ⇔ 𝐴𝑝𝑞 > 𝑇𝑝.
(3)
where (𝑝, 𝑞) is the edge between points 𝑝 and 𝑞. 𝐴𝑝𝑞 is the weight
assigned to the edge (𝑝, 𝑞). This process is described in statements 15
to 21 in Algorithm 1. The last step of our reduction method is to build
a mutual graph. In a mutual graph, a pair of points should agree to
accept an edge. This makes the graph 𝐺 to be defined as:
(𝑝, 𝑞) ∈ 𝐸(𝐺) ⇔ 𝐴𝑝𝑞 > 𝑇𝑝
and
𝐴𝑞𝑝 > 𝑇𝑞.
(4)
where 𝑇𝑝 is threshold of acceptance for the vertex 𝑝.
3.3. Integration with SpectralNet
Our graph filtering method can be seamlessly integrated to the
newly proposed spectral clustering using deep neural networks (Spec-
tralNet) [4]. SpectralNet uses Siamese nets [30] to learn affinities
between data points. Siamese nets expect the user to label which pairs
are positive and which are negative. An unsupervised pairing uses the
𝑘-nearest neighbors, where the nearest neighbors are positive pairs
and the farthest neighbors are negative pairs. Our graph filtering can
be used to obtain positive and negative pairs. It offers the advantage
of setting the number of pairs per point dynamically. This cannot be
achieved using 𝑘-nearest neighbors, where all the points are restricted
to have a fixed number of positive pairs. Also, we do not have to set 𝑘
Array 15 (2022) 100192
5
M. Alshammari et al.
Algorithm 1: Reducing a 𝑘-nearest neighbor graph
Input: 𝑘-nn graph where 𝑘 = 𝑘𝑚𝑎𝑥 of 𝑁 vertices.
Output: Reduced graph of 𝑁 vertices.
1 Construct distance matrix 𝐷(𝑁, 𝑘𝑚𝑎𝑥) of 𝑘-nn graph
2 Construct a histogram 𝐻𝐷 of all elements in 𝐷 using FD rule
3 Save bin width in 𝐻𝐷 to the variable 𝑏𝑖𝑛𝐷
/* The following loop has computations in order of
(𝑁𝑘𝑚𝑎𝑥)
*/
4 for 𝑝 = 1 to 𝑁 do
5
Construct a histogram 𝐻𝑝 of 𝐷𝑝,1 to 𝑘𝑚𝑎𝑥 using 𝑏𝑖𝑛𝐷
6
Apply MWA to bin values in 𝐻𝑝 (Eq. (2))
7
Set Kth as the first bin that exceeds MWA threshold
8
𝜎𝑝 = mean(𝐷𝑝,1 to Kth)
9 end
/* The following loop has computations in order of
(𝑁𝑘𝑚𝑎𝑥)
*/
10 for 𝑝 = 1 to 𝑁 do
11
for 𝑞 = 1 to 𝑘𝑚𝑎𝑥 do
12
𝐴𝑝,𝑞 = exp
(
𝐷(𝑝,𝑞)
𝜎𝑝𝜎𝑞
)
13
end
14 end
/* The following loop has computations in order of
(𝑁𝑘𝑚𝑎𝑥)
*/
15 for 𝑝 = 1 to 𝑁 do
16
if max(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) > 𝜇(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) + 𝜎(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) then
17
𝐴𝑝,1 to 𝑘𝑚𝑎𝑥 < 𝜇(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) + 𝜎(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) = 0
18
else
19
𝐴𝑝,1 to 𝑘𝑚𝑎𝑥 < 𝜇(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) − 𝜎(𝐴𝑝,1 to 𝑘𝑚𝑎𝑥) = 0
20
end
21 end
22 Construct a reduced graph using affinity matrix 𝐴(𝑁, 𝑘𝑚𝑎𝑥)
manually. We let our method assigns the positive and negative pairs for
each point. A pseudocode illustrating the steps of the proposed method
is shown in Algorithm 1.
4. Experiments and discussions
In the experiments we used four synthetic datasets, as shown in
Fig. 3. Dataset 1 to 3 were created by [27], while Dataset 4 was
created by us. We also used seven real datasets (see Table 1). Apart
from the MNIST dataset, all the real datasets were retrieved from UCI
machine learning. Each dataset was run with two parameter sets to
evaluate the effect.
Six methods were used for comparison and these are shown in
Table 2. Methods 1 to 5 [6,15,16] rely on the parameter 𝑚, which
is the number of representatives to build the graph 𝐺. They used
iterative algorithms like 𝑘-means and self-organizing maps to construct
the graph, which makes them produce a slightly different graph with
each run. Method 6 [3] relied on the parameter 𝜇0 to build the graph
𝐺, where 𝜇0 is the number of neighbors whose mean was used as a
threshold to include or exclude further neighbors. The code is available
at https://github.com/mashaan14/Spectral-Clustering.
All the methods were evaluated using three evaluation metrics: (1)
clustering accuracy (ACC) (2) the Adjusted Rand Index (ARI) [31], and
(3) the percentage of edges used compared to all the edges in a full
graph (E%).
ACC computes the percentage of hits between ground truth labels
𝑇𝑖 and labels obtained through clustering 𝐿𝑖. It is defined as [32]:
𝐴𝐶𝐶(𝑇 , 𝐿) =
∑𝑁
𝑖=1 𝛿(𝑇𝑖, 𝑚𝑎𝑝(𝐿𝑖))
𝑁
,
(5)
Table 1
The four synthetic and seven real datasets used in the experiments; 𝑁 is the
number of points, 𝑑 is the number of dimensions, 𝐶 is the number of clusters,
𝑚 is the size of the reduced set of vertices, and 𝜇0 is the number of neighbors
used as a threshold to include or exclude further neighbors.
where 𝑁 is the number of points and the function 𝛿(𝑥, 𝑦) is the Kro-
necker delta function, which equals one if 𝑥 = 𝑦 and zero otherwise. The
function 𝑚𝑎𝑝(𝐿𝑖) permutes the grouping obtained through clustering for
the best fit with the ground-truth grouping. ARI needs two groupings 𝑇
and 𝐿, where 𝑇 is the ground truth and 𝐿 is the grouping predicted by
the clustering method. If 𝑇 and 𝐿 are identical, ARI produces one, and
zero in case of random grouping. ARI is calculated using: 𝑛11: pairs in
the same cluster in both 𝑇 and 𝐿; 𝑛00: pairs in different clusters in both
𝑇 and 𝐿; 𝑛01: pairs in the same cluster in 𝑇 but in different clusters in
𝐿; 𝑛10: pairs in different clusters in 𝑇 but in the same cluster in 𝐿.
𝐴𝑅𝐼(𝑇 , 𝐿) =
2(𝑛00𝑛11 − 𝑛01𝑛10)
(𝑛00 + 𝑛01)(𝑛01 + 𝑛11) + (𝑛00 + 𝑛10)(𝑛10 + 𝑛11) .
(6)
The computational efficiency can be measured by the method’s running
time, but, this is influenced by the type of machine used. We chose to
measure the computational efficiency by the percentage of edges, E%:
𝐸% = 𝐸(𝐺𝑟𝑒𝑑𝑢𝑐𝑒𝑑)
𝐸(𝐺𝑓𝑢𝑙𝑙)
.
(7)
4.1. Experiments on synthetic data
In the synthetic datasets the proposed method delivered a perfor-
mance that ranked it as 2nd, 2nd, 1st, and 2nd for Datasets 1 to 4
respectively (see Fig. 4). Method 6 was the top performer on three
occasions. However, its performance dropped significantly when we
changed the parameter 𝜇0. For example, its performance dropped by
50% with Dataset 4 when we changed 𝜇0 = 3 to 𝜇0 = 7. This shows
how parameters could affect the performance. Another observation is
the consistency of ACC and ARI metrics over the 50 runs. By looking
at Fig. 4, the methods 1 to 5 have a wide standard deviation. This
is explained by the iterative algorithms used by methods 1 to 5 to
construct the graph. Method 6 and the proposed method do not have
this problem, and they have a small standard deviation. This is due to
their deterministic nature when constructing the graph, which makes
them consistent over independent executions.
In terms of the used edges, the proposed method used 6.32%,
1.45%, and 0.51% of the full graph edges for Datasets 2 to 4
respectively. But in Dataset 1 there was a sharp increase where the
proposed method used 16% of the full graph edges. This sharp increase
could be explained by the points in dense clusters being fully connected.
Array 15 (2022) 100192
6
M. Alshammari et al.
Fig. 5. Results with real data, all values are for 50 runs. (Best viewed in color).
Array 15 (2022) 100192
7
M. Alshammari et al.
Fig. 6. Testing the methods’ performance with the iris dataset under different settings of parameters 𝑚 and 𝜇0. (Best viewed in color).
Fig. 7. Datasets used in the SpectralNet experiments.
Fig. 8. Results of the experiments for integration with SpectralNet for 10 runs. (Best viewed in color).
Array 15 (2022) 100192
8
M. Alshammari et al.
Table 2
Methods used in the experiments. 𝑚 is the number of reduced vertices, 𝑁 is the number of all vertices, 𝑡 is the number of
iterations, 𝑘𝑚𝑎𝑥 is the parameter used to construct 𝑘-nn graph.
4.2. Experiments on real data
With real datasets in Fig. 5, the proposed method continued to be
the most consistent method over all tested methods. It kept a very small
standard deviation, while other methods had a wide standard deviation.
The performance of other methods was determined by their parameters.
For example, Method 3 was the best performer on iris dataset when
𝑚 = 16. However, when we changed 𝑚 to 32, its performance dropped
by more than 15%. Another observation with statlog and MNIST the
proposed method did not perform well. This indicated that a cluster
in these datasets does not have the same statistics across its regions.
Therefore, characterizing clusters using local 𝜎 might not be a good
choice. Instead, we should use CONN to discover clusters discontinuity,
rather than tracking local statistics.
4.3. Effect of the parameters on the spectral clustering performance
In this experiment, we investigated how a wide selection of parame-
ters could affect the accuracy of the spectral clustering. The parameters
𝑚 and 𝜇0 were given the following values: 𝑚 ∈ {10, 20, 30, 40, 50, 60, 70,
80, 90, 100} and 𝜇0 ∈ {3, 7, 10, 20, 30, 40, 50, 60, 70, 80}. In Fig. 6 (left),
the performance Methods 1 to 5 fluctuated with different values
of 𝑚, with a clear downward trend seen as 𝑚 increased. The dashed
horizontal line is the performance of the proposed method. In Fig. 6
(right), Method 6 started with low performance, peaking around 𝜇0 =
30, and then it took a downward trend. By eliminating the use of 𝜇0,
our method delivered a stable performance as shown by the horizontal
dashed line.
4.4. Experiments for integration with SpectralNet
The SpectralNet integration experiment was conducted using three
datasets shown in Fig. 7. The evaluation metrics are ACC shown
in Eq. (5), ARI shown in Eq. (6), and total pairs, which is the number
of pairs passed to the Siamese net. We used four methods to construct
positive and negative pairs. The first two methods used a 𝑘-nearest
neighbor graph with 𝑘 = 2 and 𝑘 = 4. Simply, the nearest 𝑘 neighbors
were set as the positive pairs, and 𝑘 random farthest neighbors were set
as the negative pairs. The third method used the parameter 𝜇0 proposed
by Alshammari, et al. [3] to construct pairs.
In Fig. 8, the proposed method delivered the best performance for
the cc and compound datasets. This good performance was coupled
with good computational efficiency, with an average of 8468 for the
total pairs passed to the Siamese net. Only 𝑘
=
2 could deliver
fewer total pairs, but with a massive loss in performance. For the
aggregation dataset, 𝑘 = 2 delivered the best performance. This
experiment highlighted the need for setting the number of positive pairs
dynamically. The methods following this approach (the 𝜇0 method and
our method) were the best performers for two of the three datasets.
5. Conclusion
The problem of detecting non-convex clusters has led to the de-
velopment of numerous clustering methods. One of the well-known
graph-based clustering methods is spectral clustering and SpectralNet.
Both spectral clustering and SpectralNet require a graph that connects
points in the same cluster with edges of high weights. The intuition is
simple, strongly connected points will become closer in the embedding
space and can be easily detected.
Graph reduction requires extensive use of parameters that need
careful setting for each dataset. The graph reduction algorithm pro-
posed in this study does not require any parameters to reduce the
graph, yet it is able to maintain spectral clustering and SpectralNet
accuracies. It takes an input as a full graph or a 𝑘-nearest neighbor
graph (in the case of a large number of points). Then, it reduces the
graph edges using statistical measures that require low computations.
The experiments revealed that the proposed method provides a stable
alternative compared to other methods that require parameters tuning.
The proposed method does not reduce the graph vertices, which
could boost the computational efficiency. A useful extension of the
proposed method would be a vertices reduction component that is
aware of local statistics. Another potential improvement of this work is
to use a different kernel other than gaussian kernel to compute pairwise
similarities.
Array 15 (2022) 100192
9
M. Alshammari et al.
CRediT authorship contribution statement
Mashaan Alshammari: Conceptualization, Methodology, Software,
Visualization, Writing – original draft, Project administration. John
Stavrakakis: Conceptualization, Investigation, Visualization, Writing –
review & editing. Masahiro Takatsuka: Conceptualization, Writing –
review & editing, Supervision.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
References
[1] Qin Y, Yu ZL, Wang C-D, Gu Z, Li Y. A novel clustering method based on hybrid
K-nearest-neighbor graph. Pattern Recognit 2018;74:1–14. http://dx.doi.org/10.
1016/j.patcog.2017.09.008.
[2] Kim Y, Do H, Kim SB. Outer-points shaver: Robust graph-based clustering
via node cutting. Pattern Recognit 2020;97:107001. http://dx.doi.org/10.1016/
j.patcog.2019.107001.
[3] Alshammari M, Stavrakakis J, Takatsuka M. Refining a k-nearest neighbor
graph for a computationally efficient spectral clustering. Pattern Recognit
2021;114:107869. http://dx.doi.org/10.1016/j.patcog.2021.107869.
[4] Shaham U, Stanton K, Li H, Nadler B, Basri R, Kluger Y. SpectralNet: Spectral
clustering using deep neural networks. In: 6th international conference on
learning representations, ICLR 2018 - Conference track proceedings. 2018, http:
//dx.doi.org/10.48550/ARXIV.1801.01587.
[5] Ng AY, Jordan MI, Weiss Y. On spectral clustering: Analysis and an algorithm.
Adv Neural Inf Process Syst 2002.
[6] Yan D, Huang L, Jordan MI. Fast approximate spectral clustering. In: Proceedings
of the 15th ACM SIGKDD international conference on knowledge discovery and
data mining. 2009, p. 907–16. http://dx.doi.org/10.1145/1557019.1557118.
[7] von Luxburg U. A tutorial on spectral clustering. Stat Comput 2007;17(4):395–
416. http://dx.doi.org/10.1007/s11222-007-9033-z.
[8] Liu G, Lin Z, Yan S, Sun J, Yu Y, Ma Y. Robust recovery of subspace structures by
low-rank representation. IEEE Trans Pattern Anal Mach Intell 2013;35(1):171–84.
http://dx.doi.org/10.1109/TPAMI.2012.88.
[9] Elhamifar E, Vidal R. Sparse subspace clustering: Algorithm, theory, and ap-
plications. IEEE Trans Pattern Anal Mach Intell 2013;35(11):2765–81. http:
//dx.doi.org/10.1109/TPAMI.2013.57.
[10] Wolf L, Shashua A. Learning over sets using kernel principal angles. J Mach
Learn Res 2003;4(null):913–31. http://dx.doi.org/10.1109/TPAMI.2012.88.
[11] Peng C, Kang Z, Li H, Cheng Q. Subspace clustering using log-determinant
rank approximation. In: Proceedings of the 21th ACM SIGKDD international
conference on knowledge discovery and data mining. KDD ’15, 2015, p. 925–34.
http://dx.doi.org/10.1145/2783258.2783303.
[12] Peng C, Zhang Q, Kang Z, Chen C, Cheng Q. Kernel two-dimensional ridge
regression for subspace clustering. Pattern Recognit 2021;113:107749. http://
dx.doi.org/10.1016/j.patcog.2020.107749.
[13] Arthur D, Vassilvitskii S. K-means++: The advantages of careful seeding.
In: Proceedings of the annual ACM-SIAM symposium on discrete algorithms
07-09-January-2007. 2007, p. 1027–35.
[14] Kohonen T. The self-organizing map. Proc IEEE 1990;78(9):1464–80. http://dx.
doi.org/10.1109/5.58325.
[15] Tasdemir K. Vector quantization based approximate spectral clustering of
large datasets. Pattern Recognit 2012;45(8):3034–44. http://dx.doi.org/10.1016/
j.patcog.2012.02.012.
[16] Tasdemir K, Yalcin B, Yildirim I. Approximate spectral clustering with utilized
similarity information using geodesic based hybrid distance measures. Pattern
Recognit 2015;48(4):1465–77. http://dx.doi.org/10.1016/j.patcog.2014.10.023.
[17] Marchette DJ. Random graphs for statistical pattern recognition. Wiley series in
probability and statistics, Hoboken, N.J: Wiley-Interscience; 2004, http://dx.doi.
org/10.1002/047172209X.
[18] Correa C, Lindstrom P. Locally-scaled spectral clustering using empty region
graphs. In: Proceedings of the 18th ACM SIGKDD international conference on
knowledge discovery and data mining. 2012, p. 1330–8. http://dx.doi.org/10.
1145/2339530.2339736.
[19] Satuluri V, Parthasarathy S, Ruan Y. Local graph sparsification for scalable clus-
tering. SIGMOD ’11, New York, NY, USA: Association for Computing Machinery;
2011, p. 721–32. http://dx.doi.org/10.1145/1989323.1989399.
[20] Jarvis R, Patrick E. Clustering using a similarity measure based on shared
near neighbors. IEEE Trans Comput 1973;C-22(11):1025–34. http://dx.doi.org/
10.1109/T-C.1973.223640.
[21] Spielman DA, Srivastava N. Graph sparsification by effective resistances. SIAM
J Comput 2011;40(6):1913–26. http://dx.doi.org/10.1137/080734029.
[22] Spielman DA, Teng S-H. Spectral sparsification of graphs. SIAM J Comput
2011;40(4):981–1025. http://dx.doi.org/10.1137/08074489X.
[23] Bengio Y, Paiement J-f, Vincent P, Delalleau O, Roux N, Ouimet M. Out-of-sample
extensions for LLE, isomap, MDS, eigenmaps, and spectral clustering. In: Thrun S,
Saul L, Schölkopf B, editors. Advances in neural information processing systems,
Vol. 16. MIT Press; 2003.
[24] Nie F, Zeng Z, Tsang IW, Xu D, Zhang C. Spectral embedded clustering: A
framework for in-sample and out-of-sample spectral clustering. IEEE Trans Neural
Netw 2011;22(11):1796–808. http://dx.doi.org/10.1109/TNN.2011.2162000.
[25] Alzate C, Suykens JAK. Multiway spectral clustering with out-of-sample ex-
tensions through weighted kernel PCA. IEEE Trans Pattern Anal Mach Intell
2010;32(2):335–47. http://dx.doi.org/10.1109/TPAMI.2008.292.
[26] Levin K, Roosta F, Mahoney M, Priebe C. Out-of-sample extension of graph
adjacency spectral embedding. In: Dy J, Krause A, editors. Proceedings of the
35th international conference on machine learning. Proceedings of machine
learning research, vol. 80, PMLR; 2018, p. 2975–84.
[27] Zelnik-Manor L, Perona P. Self-tuning spectral clustering. Adv Neural Inf Process
Syst 2005;1601–8.
[28] Sugiyama M. Dimensionality reduction of multimodal labeled data by local fisher
discriminant analysis. J Mach Learn Res 2007;8(May):1027–61.
[29] Freedman D, Diaconis P. On the histogram as a density estimator L2 theory. Z
Wahrscheinlichkeitstheor Verwandte Geb 1981;57(4):453–76. http://dx.doi.org/
10.1007/BF01025868.
[30] Bromley J, Guyon I, LeCun Y, Säckinger E, Shah R. Signature verification using
a ‘‘Siamese’’ time delay neural network. In: Proceedings of the 6th international
conference on neural information processing systems. NIPS’93, San Francisco, CA,
USA: Morgan Kaufmann Publishers Inc.; 1993, p. 737–44. http://dx.doi.org/10.
1142/9789812797926_0003.
[31] Hubert L, Arabie P. Comparing partitions. J Classification 1985;2(1):193–218.
http://dx.doi.org/10.1007/BF01908075.
[32] Cai D, He X, Han J. Document clustering using locality preserving indexing. IEEE
Trans Knowl Data Eng 2005;17(12):1624–37. http://dx.doi.org/10.1109/TKDE.
2005.198.
Dr. Mashaan Alshammari is an assistant professor at University of Hail. His research
interests include unsupervised learning and image analysis. Mashaan holds a MSc in
computer science from King Fahd University of Petroleum and Minerals (KFUPM), Saudi
Arabia, and a PhD from the University of Sydney, Australia.
Dr. John Stavrakakis has strong interests in 3D computer graphics, remote rendering
and computer security. He holds a PhD in Computer science and is an academic fellow
at the University of Sydney, Australia.
Dr. Masahiro Takatsuka received his MEng degree at Tokyo Institute of Technology
in 1992, and received his PhD at the Monash University in 1997. In 1997–2002, he
worked at GeoVISTA Center, The Pennsylvania State University as a senior research
associate. He joined the School of Computer Science, the University of Sydney in 2002.
