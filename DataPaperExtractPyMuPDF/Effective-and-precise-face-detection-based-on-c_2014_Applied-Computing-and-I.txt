ORIGINAL ARTICLE
Eﬀective and precise face detection
based on color and depth data
Loris Nanni a,*, Alessandra Lumini b, Fabio Dominio a,
Pietro Zanuttigh a
a DEI, University of Padova, via Gradenigo, 6, 35131 Padova, Italy
b DISI, University of Bologna, via Venezia, 52, 47521 Cesena, Italy
Received 3 February 2014; revised 31 March 2014; accepted 7 April 2014
Available online 21 April 2014
KEYWORDS
Face detection;
Skin detection;
Depth map;
Viola–jones detector
Abstract
In this work an effective face detector based on the well-known Viola–
Jones algorithm is proposed. A common issue in face detection is that for max-
imizing the face detection rate a low threshold is used for classifying as face an
input image, but at the same time using a low threshold drastically increases the
number of false positives. In this paper several criteria are proposed for reducing
false positives: (i) a skin detection step is used to reject a candidate face region
that does not contain the skin color, (ii) the size of the candidate face region is
calculated according to the depth data, removing the too small or the too large
faces, (iii) images of ﬂat objects (e.g. candidate face found in a wall) or uneven
objects (e.g. candidate face found in the leaves of a tree) are removed using
the depth map and a segmentation approach based both on color and depth
data.
The above criteria permit to drastically reduce the number of false positives with-
out decreasing the detection rate. The proposed approach has been validated on
three datasets composed of 180 samples including both 2D and depth images.
The face position inside samples has been manually labeled for testing.
* Corresponding author. Tel.: +39 3493511673.
E-mail address: nanni@dei.unipd.it (L. Nanni).
Peer review under responsibility of King Saud University.
Production and hosting by Elsevier
Applied Computing and Informatics (2014) 10, 1–13
King Saud University
Applied Computing and Informatics
www.ksu.edu.sa
www.sciencedirect.com
2210-8327 ª 2014 King Saud University. Production and hosting by Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.aci.2014.04.001
A Matlab version of the system for face detection and the full testing dataset will
be freely available from http://www.dei.unipd.it/node/2357.
ª 2014 King Saud University. Production and hosting by Elsevier B.V. All rights
reserved.
1. Introduction
Face detection has attracted the attention of many research groups due to its
widespread application in many ﬁelds as surveillance and security systems, as
human–computer interface, face tagging, behavioral analysis, content-based image
and video indexing, and many others (Zeng et al., 2009). Face detection is the ﬁrst
crucial step for facial analysis algorithms (i.e. face recognition/veriﬁcation, head
tracking, and facial expression recognition) whose goal is to determine whether
or not faces are present in an image and eventually return their location and extent
(i.e. a bounding box). It is a more challenging problem than face localization in
which a single face is assumed to be inside an image.
Most of the literature in this ﬁeld deals with frontal face detection from two-dimen-
sional (2D) images: the problem is often formulated as a two-class pattern recognition
problem aimed at classifying each sub-window of a given size of the input image as
either containing or not containing a face (Jin et al., 2004). Then the classiﬁcation
is performed by common technologies for 2D facial recognition such as Eigenface,
Fisherface, waveletface, PCA (Principal Component Analysis), LDA (Linear Dis-
criminant Analysis), Haar wavelet transform, and so on. The Viola–Jones detector
(Viola and Jones, 2001) is probably the most famous approach for frontal 2D detec-
tion: it involves exhaustively searching an entire image for faces, with multiple scales
explored at each pixel using Haar-like rectangle features boosting classiﬁcation. Two
different face detection strategies based on slightly modiﬁed Viola–Jones are proposed
in Anisetti (2009). In Ku¨ blbeck and Ernst, 2006 boosting has also been used in con-
junction with Modiﬁed census transform (MCT), to improve illumination invariance.
In (Huang et al., 2007) a method able to detect faces with arbitrary rotation-in-plane
and rotation–off-plane angles in still images or video sequences is proposed. In
(Jianxin et al., 2008) is designed a classiﬁer that explicitly addresses the difﬁculties
caused by the asymmetric learning goal (the minority class is the face class).
Despite the success of these and of several other methods, designed to provide
accurate detection performance under variable conditions (Zhang and Zhang,
2010), most of difﬁculties in precise face detection still arises in the presence of
illumination changes and occlusions. One possible way to improve algorithms
for face detection is to incorporate models of the image processing that efﬁciently
integrates multiple cues, such as stereo disparity, texture and motion.
For example, Microsoft Kinect is a depth sensing device that couples the 2D
RGB image with a depth map (RGB-D) which can be used to determine the depth
of every object in the scene. Each pixel in Kinect’s depth map has a value
2
L. Nanni et al.
indicating the relative distance of that pixel from the sensor at the time of image
capture. Depth information captured by Kinect is not useful to differentiate
among different individuals at a distance, due to its very high inter-class similarity,
but thanks to its low intra-class variation may be useful to improve the robustness
of a face detector by reducing sensitivities to illumination, occlusions, changing of
expression and pose. Kinect devices have been extremely popular recently, due to
their low-cost and availability, and the ﬁrst benchmark datasets have been
collected for 3D face recognition (Tsalakanidou et al., 2003) or detection (Hg
et al., 2012).
Several recent approaches use depth map or other 3D information for face
detection. For instance, the classic Viola–Jones face detection algorithm is
extended in (Dixon et al., 2007; Burgin et al., 2011) to simultaneously consider
depth and color information in face detection. In (Shieh and Hsieh, 2013) Haar
wavelets on 2D are ﬁrst used to detect the human face and then its position is
reﬁned by structured light analysis. Other depth-based detectors are proposed in
Shotton et al. (2011), Mattheij et al. (2012): the approach by Shotton et al.
(2011) employs depth-comparison features deﬁned as pixel pairs in depth images,
to quickly and accurately classify body joints and parts from single depth images,
a similar method based on square regions comparison is coupled to Viola Jones
face detector in Mattheij et al. (2012) for robust and accurate face detection. In
(Jiang et al., 2013) biologically inspired integrated representation of texture and
stereo disparity information is used for a multi-view facial detection task with
the valuable result of improved detection performance and reduced computational
complexity. Disparity information extracted from stereo images allows to strongly
reduce the number of locations to be evaluated during the search process. In
(Goswami et al., 2013) the authors use the additional information obtained by
the depth map to improve face recognition: their approach extracts textural
descriptors (the Histogram of Oriented Gradients) from four entropy maps corre-
sponding to RGB and depth information with varying patch sizes and uses Ran-
dom Forest as a classiﬁer. Another face recognition approach designed speciﬁcally
for low resolution 3D sensors is proposed in Li et al. (2013), which uses efﬁcient
Iterative Closest Point method and facial symmetry for estimating a canonical
frontal view from non-frontal views.
This work, similar to other approaches proposed in the literature (Anisetti et al.,
2008), is aimed at using depth information to reduce the number of false positive
detections and improve the percentage of correct detections. In (Anisetti et al.,
2008) the authors use a 2D multi-step algorithm to obtain a coarse-to-ﬁne classiﬁ-
cation, then reﬁne the quality of the face location by a 3D tracking approach.
In this paper an effective and precise face detector designed for upright and
frontal faces is presented based on both gray-level image and depth map: depth
information is used to ﬁlter the regions of the image where a candidate face region
is found by the Viola–Jones (VJ) detector Viola and Jones, 2001. A main drawback
of VJ is that several false positives occur setting a low threshold for face
Effective and precise face detection based on color and depth data
3
classiﬁcation; in this work several criteria, mainly evaluated on the depth map, are
used to drastically reduce the number of false positives:
- theﬁrstﬁlteringruleisdeﬁnedonthecoloroftheregion;sincesomefalsepositives
have colors not compatible with the face (e.g. shadows on jeans) a skin detector is
applied to remove the candidate face regions that do not contain skin pixels;
- the second ﬁltering rule is deﬁned on the size of the face: using the depth map
it is quite easy to calculate the size of the candidate face region, which is use-
ful to discard smallest and largest faces from the ﬁnal result set;
- the third ﬁltering rule is deﬁned on the depth map to discard ﬂat objects (e.g.
candidate faces found in a wall) or uneven objects (e.g. candidate face found
in the leaves of a tree). Combining color and depth data the candidate face
region can be extracted from the background and measures of depth and reg-
ularity are used for ﬁltering out false positives.
Unfortunately, in the literature there are no freely available large datasets for
face detection (with difﬁcult images as complex background, the datasets used
in (Shieh and Hsieh, 2013; Mattheij et al., 2012) are quite easy) that contains both
the color and the depth map. There are several datasets for face recognition using
the depth map, but the face detection step in those datasets is easy. Therefore, the
proposed approach has been evaluated on a collected dataset that will be made
freely available for further comparisons.
2. Base face detector
The proposed method is based on the widely used VJ face detector (Viola and
Jones, 2001), characterized by a slow training, but very fast classiﬁcation. VJ
involves a very simple image representation, based on Haar wavelets, an integral
image for rapid feature detection, the AdaBoost machine-learning method for
selecting a small number of important features, and a cascade combination of weak
learners for classiﬁcation. The detection performance of VJ strictly relies on the
threshold used to classify an input region as face, this value deﬁnes the criteria to
declare a ﬁnal face detection in an area where there are multiple detections around
an object. Groups of candidate face regions that meet the threshold are merged to
produce one bounding box around the target object. Increasing this threshold may
help suppress false detections by requiring that the target object be detected multi-
ple times during the multiscale detection phase. Since the original VJ implementa-
tion is designed for upright frontal image, in order to handle non upright faces in
the work, the original images are also rotated of {20�,�20�} before detection.
In the complete system, outlined in Figure 1, ﬁrst the VJ detector is applied to
input and rotated images using a low classiﬁcation threshold, then all the
candidate face regions are ﬁltered out according to three criteria (detailed in the
following sub-sections) with the aim of reducing false positives:
4
L. Nanni et al.
- skin detection;
- size of the image;
- ﬂatness/unevenness of the image.
Figure 2 shows the result of the three ﬁltering steps on two sample images.
Depth data acquired by the Kinect are projected over the color images contain-
ing the faces in order to obtain a set of aligned color images and depth maps. For
this purpose, the calibration data for the depth and color cameras of the Kinect are
computed using the method proposed in Herrera et al. (2012). This approach com-
putes both the intrinsic parameters of the depth and color cameras and the extrinsic
parameters between the two cameras. The depth samples positions of the image in
the 3D space are ﬁrst computed by using the intrinsic parameters of the depth cam-
era and the 3D samples are then reprojected in the 2D color image reference system
by using both the color camera intrinsic parameters and the extrinsic ones. By the
end of this procedure, a color and a depth value are associated to each sample.
2.1. Skin detection ﬁlter
The presence of skin is a good indicator to assert the detection of a face. In this
work a skin ﬁlter is applied to candidate face regions which is a simpliﬁed version
of the ensemble proposed in Nanni et al. (2013). It is the combination by the sum
rule of the methods proposed in Jones (2002), O´ Conaire et al. (2007) with three
other skin detectors based on the idea proposed in Khan et al. (2011); after the
VJ Face detector
(low threshold)
Size ﬁltered 
regions
ﬂatness\unevenness
ﬁltered regions
Input image 
()
±20° Rotated images
Depth map alignment
Set of candidate face 
regions
Skin ﬁltered
regions
3 steps ﬁltering
Detected faces
Figure 1
Outline of our complete system.
Effective and precise face detection based on color and depth data
5
training phase performed according to the above cited approaches, pixels are
classiﬁed by a set of lookup tables, built using SVMs trained considering different
features (i.e., different pre-processing and color spaces):
� max-RGB color constancy and RGB color space;
� max-RGB color constancy and YUV1 color space;
� RGB color space.
The color constancy problem is the ability to estimate the unknown light of a
scene from an image. The max-RGB color constancy approach is based on the
assumption that the reﬂectance which is achieved for each of the three color
channels is equal (van de Weijer et al., 2007).
Since lookup tables2 are used for the classiﬁcation task this method allows to
classify skin pixels of a given image in real time. Please note that since lookup
tables have been calculated from several large datasets (Nanni et al., 2013), the
system is not over-trained in the dataset tested in this paper.
2.2. Filter by the size of the image
The size criteria simply remove the candidate faces not included in a ﬁxed range
size ([12.5,30] cm). The size of a candidate face region is extracted from the depth
map according to the following approach.
Assuming that the face detection algorithm returns the 2D position and
dimension in pixels (w2D, h2D) of a candidate face region, its 3D physical
dimension in mm (w3D; h3D) can be estimated as:
Candidate face 
region ﬁltered 
by skin  
True posi�ve 
discovered only 
in the rotated 
version of the 
input image 
Candidate face 
regions ﬁltered 
by size 
Candidate face 
regions ﬁltered 
by size 
Candidate face 
region ﬁltered 
by low std 
True faces 
correctly 
labelled 
Figure 2
Result of the ﬁltering steps on some sample images.
1 The color space conversion is performed using the ‘‘Colorspace’’ Matlab toolbox http://www.math-
works.com/matlabcentral/ﬁleexchange/28790-colorspace-transformations.
2 A lookup table is the result of the pre-calculation by SVM of classiﬁcation scores of all the combinations of
pixel values (2563) from a given color space.
6
L. Nanni et al.
w3D ¼ w2D
�d
fx
h3D ¼ h2D
�d
fy
where fx and fy are the Kinect camera focal lengths computed by the calibration
algorithm of Herrera et al. (2012) and �d is the average depth of the samples within
the face candidate bounding box. Note how �dis indeed deﬁned as the median of
the depth samples in order to reduce the impact of noisy samples in the average
computation.
2.3. Filter by ﬂatness/unevenness of the image
Another signiﬁcant information that can be obtained from the depth map is the
ﬂatness/unevenness of the candidate face regions. For this ﬁlter ﬁrst a segmenta-
tion procedure is applied, then from each face candidate region the standard devi-
ation (std) of the pixels of the depth map that belongs to the larger segment is
calculated. Regions having a std out of a ﬁxed range [0.15,4] are removed.
The segmentation of both color and depth map is performed according to the
approach of Dal Mutto et al. (2012). This segmentation scheme is based on the
normalized cuts spectral clustering algorithm (Shi and Malik, 2000) and jointly
exploits the geometry and color information for optimal performances.
The basic architecture of the segmentation scheme is shown in Figure 3. The
procedure has two main stages: ﬁrstly a six-dimensional representation of the
scene samples is built from the geometry and color data and then the obtained
point set is segmented using spectral clustering.
Each sample in the acquired depth map corresponds to a 3D point of the scene
pi; i ¼ 1; . . . ; N. After the joint calibration of the depth and color cameras it is pos-
sible to compute the 3D coordinates x, y and z of pi and to associate to it a 3-D
vector containing the R, G, and B color components. Geometry and color then
need to be uniﬁed in a meaningful way. Color values are converted to a perceptu-
ally uniform space in order to give a perceptual signiﬁcance to the distance
between colors that will be used in the clustering algorithm. The CIELab space
has been used for this purpose, i.e., the color information of each scene point is
the 3-D vector:
pc
i ¼
LðpiÞ
aðpiÞ
bðpiÞ
2
64
3
75; i ¼ 1; . . . ; N
The geometry is simply represented by the 3-D coordinates of each point, i.e.:
pg
i ¼
xðpiÞ
yðpiÞ
zðpiÞ
2
64
3
75; i ¼ 1; . . . ; N
Effective and precise face detection based on color and depth data
7
The scene segmentation algorithm should be insensitive to the relative scaling of
the point-cloud geometry and should bring geometry and color distances into a
consistent framework. Therefore all the components of pg
i are normalized with
respect to. the average of the standard deviations of the point coordinates
rg ¼ ðrx þ ry þ rzÞ=3. The adopted geometry representation is thus the vector:
�xðpiÞ
�yðpiÞ
�zðpiÞ
2
64
3
75 ¼
3
rx þ ry þ rz
xðpiÞ
yðpiÞ
zðpiÞ
2
64
3
75 ¼ 1
rg
xðpiÞ
yðpiÞ
zðpiÞ
2
64
3
75
In order to balance the relevance of color and geometry in the merging process,
the color information vectors are also normalized by the average of the standard
deviations of the L, a and b components. The ﬁnal color representation therefore is:
�LðpiÞ
�aðpiÞ
�bðpiÞ
2
64
3
75 ¼
3
rL þ ra þ rb
LðpiÞ
aðpiÞ
bðpiÞ
2
64
3
75 ¼ 1
rc
LðpiÞ
aðpiÞ
bðpiÞ
2
64
3
75
From the above normalized geometry and color information vectors, each point
is ﬁnally represented as
p f
i ¼
�LðpiÞ
�aðpiÞ
�bðpiÞ
k�xðpiÞ
k�yðpiÞ
k�zðpiÞ
2
666666664
3
777777775
where k is a parameter balancing the contribution of color and geometry. High
values of k increase the relevance of geometry, while low values of k increase
the relevance of color information. A complete discussion on the effect of this
parameter and on how to automatically set it to the optimal value is presented
in Dal Mutto et al. (2012).
Figure 3
Architecture of the proposed segmentation scheme.
8
L. Nanni et al.
The computed vectors p f
i are then clustered in order to segment the acquired
scene. Among the various clustering techniques, methods based on pairwise afﬁn-
ity measures computed between all the possible couples of points allows to obtain
very accurate and robust results because they do not assume a Gaussian model for
the distribution of the points. On the other side they have the drawback that they
need to compare all the possible pairs of points and are so very expensive in terms
of both CPU and memory resources. Normalized cuts spectral clustering (Shi and
Malik, 2000) is an effective example of this family. This method is based on the
partition of a graph representing the scene according to spectral graph theory cri-
teria. The minimization is done using normalized cuts and accounts both for the
similarity between the pixels inside the same segment and the dissimilarity between
the pixels in different segments. The minimization problem is very computation-
ally expensive and several methods have been proposed for its efﬁcient approxima-
tion. In the method based on the integral eigenvalue problem proposed in Fowlkes
et al. (2004), the set of points is ﬁrst randomly subsampled and then the subset is
partitioned and the solution is propagated to the whole points set by a speciﬁc
technique called the Nystro¨ m method. In order to avoid small regions due to noise
a ﬁnal reﬁnement stage removing regions smaller than a pre-deﬁned threshold is
ﬁnally applied. In Figure 4 an example of segmented image is reported.
3. Experimental results
The experimental evaluation of the proposed face detection system has been car-
ried out on a dataset composed of three subsets, all containing frontal images:
� Microsoft hand gesture (Ren et al., 2011), it is composed of images of 10 dif-
ferent people performing gestures; each image contains only one face; since
the images in the datasets are quite similar to each other we have chosen
and labeled for face detection a subset of 42 images.
� Padua Hand gesture (Dominio et al., 2013), it is another gesture dataset
composed of images from 10 different people; each image contains only
one face; since the images are quite similar we have chosen a subset of 59
images.
Figure 4
Segmentation map, color image and depth map.
Effective and precise face detection based on color and depth data
9
� Padua FaceDec, it is a new dataset collected and labeled for the purpose of
this work. It contains 132 images acquired with the Kinect sensor at the Uni-
versity campus in Padova. It includes both outdoor and indoor scenes,
framed in different hours during the day, in order to account for the varying
lighting conditions. The images capture one or several people performing
various daily activities, e.g., working, studying, walking, chatting and so
on. Note how most people are not directly looking into the camera, i.e., they
did not pose for the frame acquisition but they were doing their activities
without being aware of the camera shooting them. Some faces are also par-
tially occluded by objects or other people. For these reasons, this dataset is
more challenging than the previous ones.
The three sets have been merged to form a single dataset consisting of 233
images containing 251 faces3 (only upright frontal faces with a maximum rotation
of ±30� have been considered). Notice that the parameters of the method have
been manually selected and are the same for all the testing images despite their dif-
ferent origin. The dataset is not ‘‘easy’’: in Figure 5 some samples which are not
detected by the VJ method4 are shown.
The aim of the experiment reported in Table 1 is to evaluate the effectiveness of
the proposed approach considering the different ﬁltering steps and the use of
depth image; the following approaches are compared according to the detection
rate (percentage of faces detected), the number of false positives and the F-mea-
sure5 evaluated on the whole dataset:
� VJ(k), Viola–Jones detector in the 2D image with a threshold of k;
� VJ(k)-Sz, the above approach ﬁltered considering the size of the candidate
face region;
Figure 5
Sample images from the dataset which contain faces not detected from the VJ method.
3 Some images contain more than one face and some no faces.
4 VJ is executed with a very low recognition threshold (k = 2).
5 The F-measure is the harmonic mean of recall and precision, often used in document retrieval, it is deﬁned as:
2 · precision · recall/(precision + recall).
10
L. Nanni et al.
� VJ(k)-SzSk, the base VJ detector ﬁltered considering size and skin;
� VJ(k)-SzSkStd, the base VJ detector ﬁltered considering size, skin and pres-
ence of ﬂat/uneven regions (considering the depth map). In this method std is
calculated on the whole candidate face region (without the segmentation, to
reduce the computation time).
� VJ(k)-Fin, the whole approach described in this paper (including segmenta-
tion to calculate std)
� VJ(k)-Fin-No, as VJ(k)-Fin but without considering the skin ﬁlter step.
It is clear that the size is a useful criterion for removing the high number of false
positives candidates found by VJ using a low threshold (required to reach a high
detection rate). It allows to greatly reduce the number of false positives, the other
two ﬁltering criteria allow to further reducing the number of false positives. The
proposed approach allows to diminish the number of false positives on the consid-
ered dataset from 1063 to just 143 almost without affecting the detection
performances.
The depth map allows to remove the false positives in many critical situations.
In particular it allows to get the actual size of the candidate face allowing to
remove objects too small or too large to be a face. It also aids the segmentation
step in the proposed method that is a critical point to ensure proper processing
in the remaining steps.
Finally, even if the experiments reported in this paper are related to data
acquired by the Kinect, there are several other depth acquisition schemes and sen-
sors that can be exploited. For example stereo vision systems that get the 3D data
from two standard cameras, allow to work at big distances by choosing a suitable
baseline. Also there is a wide range of 3D sensors that can work at different dis-
tances and with different accuracies. The Kinect is one of the most widespread and
the cheapest acquisition sensors but not the best.
Table 1
Comparison of methods in terms of detection rate and number of false positives. Rows
corresponding to the optimal setting of the VJ threshold have been highlighted. Bold is the best performance.
Detection rate (%)
# False positives
F-measure
VJ(4)
88.05
193
0.665
VJ(3)
92.03
375
0.539
VJ(2)
95.62
1063
0.308
VJ(1)
95.62
8017
0.056
VJ(3)-Sz
92.03
123
0.725
VJ(2)-Sz
95.22
310
0.601
VJ(1)-Sz
95.62
2062
0.189
VJ(2)-SzSk
95.22
196
0.706
VJ(2)-SzSkStd
95.22
165
0.730
VJ(2)-Fin
94.82
143
0.758
VJ(2)-Fin-No
94.82
212
0.679
Effective and precise face detection based on color and depth data
11
4. Conclusion
In this work a face detector for frontal faces is proposed. The Viola–Jones face
detector is coupled with three heuristic criteria calculated using the depth map
with the main goal of obtaining accurate face detection with few false positives.
The proposed system makes use of several criteria for ﬁltering the false positives
found by the face detector:
� A skin detection ﬁlter is used to remove the candidate face regions that con-
tain enough skin pixels;
� The size of the candidate face is calculated using the depth map to remove
regions whose size is out of a ﬁxed range;
� The depth map is used to design a ﬁlter rule to discard ﬂat objects (e.g. can-
didate faces found in a wall) or uneven objects (e.g. candidate face found in
the leaves of a tree).
Experimental results show that the proposed system works well in a collected
dataset built by color images and depth map.
We are aware that the dataset used for testing is small with respect to those
available for 2D face detection, but in our opinion the results clearly conﬁrm that
the depth map permits to deﬁne criteria for a drastical reduction of the number of
false positives. However, it is our intention to collect new images to build a larger
dataset.
Future works include collecting a larger dataset and extend the system to deal
also with non frontal upright faces. Another future work will be testing different
and more performing face detectors, as (Nanni and Lumini, 2012), for reducing
the number of false negatives.
References
Anisetti, M., 2009. ‘‘Fast and robust Face Detection’’, Multimedia Techniques for Device and Ambient
Intelligence, ISBN: 978-0-387-88776-0, Springer, US (Chapter 3).
Anisetti, M., Bellandi, V., Damiani, E., Arnone, L., Rat, B., 2008. A3FD: accurate 3D face detection. In: Signal
Processing for Image Enhancement and Multimedia Processing. Springer, US, pp. 155–165.
Burgin, W., Pantofaru, C., Smart, W.D., 2011.‘‘Using depth information to improve face detection’’. In:
Proceedings of the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI ’11), pp.
119–120.
Dal Mutto, C., Zanuttigh, P., Cortelazzo, G.M., 2012. ‘‘Fusion of geometry and color information for scene
segmentation’’. In: IEEE Journal of Selected Topics in Signal Processing, vol. 6, No. 5, pp. 505–521.
Dixon, M., Heckel, F., Pless, R., Smart, W.D., 2007. Faster and more accurate face detection on mobile robots
using geometric constraints. In: IEEE/RSJ International Conference on Robots and Systems (IROS 2007), pp.
1041–1046.
Dominio, F., Donadeo, M., Zanuttigh, P., 2013. Combining multiple depth-based descriptors for hand gesture
recognition, Pattern Recogn. Lett., (accepted for publication), available online 24 October 2013.
Fowlkes, C., Belongie, S., Chung, F., Malik, J., 2004. Spectral grouping using the Nystro¨ m method. IEEE Trans.
Pattern Anal. Mach. Intell. 26 (2), 214–225.
Goswami, G., Bharadwaj, S., Vatsa, M., Singh, R., 2013. On RGB-D face recognition using Kinect. In: IEEE
Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS), pp. 1–6.
12
L. Nanni et al.
Herrera, D., Kannala, J., Heikkila¨ , J., 2012. Joint depth and color camera calibration with distortion correction.
IEEE Trans. Pattern Anal. Mach. Intell. 34, 2058–2782.
Hg, R.I., Jasek, P., Roﬁdal, C., Nasrollahi, K., Moeslund, T.B., Tranchet, G., 2012. An RGB-D database using
Microsoft’s Kinect for Windows for face detection. In: Eighth International Conference on Signal Image
Technology and Internet Based Systems (SITIS), pp. 42–46.
Huang, Chang, Ai, Haizhou, Li, Yuan, Lao, Shihong, 2007. High-performance rotation invariant multiview face
detection. IEEE Trans. Pattern Anal. Mach. Intell., 671–686.
Jiang, F., Fischer, M., Ekenel, H.K., Shi, B.E., 2013. Combining texture and stereo disparity cues for real-time
face detection. Sig. Process. 28 (9), 1100–1113.
Wu, Jianxin, Charles Brubaker, S., Mullin, Matthew D., Rehg, James M., 2008. Fast asymmetric learning for
cascade face detection. IEEE Trans. Pattern Anal. Mach. Intell. 30, 369–382.
Jin, H.L., Liu, Q.S., Lu, H.Q., 2004. ‘‘Face detection using one-class based support vectors’’. In: Proc. 6th IEEE
Int. Conf. Autom. Face Gesture Recog., pp. 457–462.
Jones, M.J. et al, 2002. Statistical color models with application to skin detection. IJCV 46 (1), 81–96.
Khan, R., Hanbury, A., Sto¨ ttinger, J., Bais, A., 2011. Color based skin classiﬁcation. Pattern Recogn. Lett., 157–
163.
Ku¨ blbeck, C., Ernst, A., 2006. Face detection and track in video sequences using the modiﬁed census
transformation. Image Vision Comput. 24 (6), 564–572.
Li, B.Y., Mian, A.S., Liu, W., Krishna, A., 2013. Using kinect for face recognition under varying poses,
expressions, illumination and disguise. In: IEEE Workshop on Applications of Computer Vision (WACV),
pp. 186–192.
Mattheij, R., Postma, E., Van den Hurk, Y., Spronck, P., 2012. Depth-based detection using Haarlike features.
In: Proceedings of the BNAIC 2012 conference. Maastricht University, The Netherlands, pp. 162–169.
Nanni Loris, Lumini Alessandra, 2012. ‘‘Combining face and eye detectors in a high-performance face-detection
system’’. In: IEEE Multimedia, vol. 19, No. 4, Oct.–Dec. 2012, pp. 20–27. doi:10.1109/MMUL.2011.57.
Nanni, L., Lumini, A., Migliardi, M., 2013. Learning based Skin Classiﬁcation, submitted to Applied Soft
Computing.
O´ Conaire, Ciara´ n, O’Connor, Noel E., Smeaton, Alan F., 2007. ‘‘Detector adaptation by maximising agreement
between independent data sources’’. In: IEEE International Workshop on Object Tracking and Classiﬁcation
Beyond the Visible, Spectrum.
Ren, Z., Meng, J., Yuan. J., 2011. Depth camera based hand gesture recognition and its applications in human–
computer-interaction. In: Proc. of ICICS, pp. 1–5.
Shi, J., Malik, J., 2000. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 22 (8),
888–905.
Shieh, M.Y., Hsieh, T.M., 2013. Fast facial detection by depth map analysis. Math. Prob. Eng., 1–10.
Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., Blake, A., 2011. Real-
time human pose recognition in parts from single depth images. CVPR 2, 3.
Tsalakanidou, F., Tzovaras, D., Strintzis, M.G., 2003. Use of depth and colour Eigenfaces for face recognition.
Pattern Recogn. Lett. 24 (910), 1427–1435.
van de Weijer, J., Gevers, T., Gijsenij, A., 2007. Edge-based color constancy. IEEE Trans. Image Process. 16,
2207–2214.
Viola, Paul, Michael, J. Jones, 2001. ‘‘Rapid Object Detection using a Boosted Cascade of Simple Features’’,
CVPR.
Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S., 2009. A survey of affect recognition methods: audio, visual,
and spontaneous expressions. IEEE Trans. Pattern Anal. Mach. Intell. 31 (1), 39–58.
Zhang C., Zhang Z., 2010. ‘‘A Survey of Recent Advances in Face Detection’’, Microsoft Research Technical
Report, MSR-TR-2010-66.
Effective and precise face detection based on color and depth data
13
