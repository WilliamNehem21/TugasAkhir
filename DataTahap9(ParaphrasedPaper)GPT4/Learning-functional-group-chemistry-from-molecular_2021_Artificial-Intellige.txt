This academic paper describes an approach for identifying and extracting functional groups (FGs) from chemical compounds using an algorithm introduced by Ertl. The algorithm detects various atomic constituents, including heteroatoms, carbon atoms involved in double or triple bonds, acetal carbons, and certain rings. It then clusters connected atoms into FGs, considering the surrounding atomic environment, such as bonded carbon or hydrogen atoms.

To analyze these compounds, a deep learning architecture called a Structurally Constrained Neural Network (SCNN) was trained using images or a condensed graph of reaction (CGR) representations with a stride of 2 for the convolution layer. The Inception-V3 architecture was adapted for transfer learning by modifying the final layer to include three fully connected layers with 500, 1000, and 2000 neurons, respectively. Different activation functions, sigmoid for FG multi-label classification and softmax for activity classification (AC), were used in the output layer.

Adam optimizer was employed to minimize binary cross-entropy loss during training. Additionally, a max-pooling layer was incorporated to determine the maximum value across convolved feature map patches, while dropout layers were utilized to prevent overfitting.

Matched Molecular Pairs (MMPs) were visualized in a single consolidated graph through the CGR approach, which combines reactants and products into a single representation based on the superposition of invariant parts. MMP CGRs were created using a custom Python script and the RDKit API. Each pseudo-molecule in a CGR consists of larger fragments connected by single bonds and smaller fragments by zero-order bonds.

The paper discusses two models: one (I-IN) detects general chemical features including the core and FGs; the other (I-FG) focuses on identifying the FGs. Successful transfer learning by the I-FG model is attributed to its capability to recognize specific FGs that differentiate between compounds that form ACs and non-AC MMPs.

Lastly, the paper references TensorFlow, a system for large-scale machine learning, citing its presentation at the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16) in Savannah, Georgia. TensorFlow was mentioned as part of the technical environment supporting the research.