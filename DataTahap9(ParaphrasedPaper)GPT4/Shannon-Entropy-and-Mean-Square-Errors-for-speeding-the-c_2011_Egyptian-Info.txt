Paraphrasing the provided text from the academic paper:

Abstract:
There is an ongoing effort to enhance the efficiency and convergence speed of multilayer backpropagation neural network algorithms. In recent years, there has been growing interest in using entropy-based metrics in adaptive systems. Proposals have been made to improve learning by maximizing or minimizing entropy-based cost functions. Our study suggests improving multilayer backpropagation neural networks by replacing the typical mean square error (MSE) minimization with the minimization of Shannon entropy (SE) of the output differences against the intended target. We compare the effectiveness of using the Shannon entropy cost function to the MSE cost function, alongside testing with two types of activation functions: Cauchy and hyperbolic tangent. Our findings indicate that the Shannon entropy cost function provides better convergence than MSE, but MSE leads to faster convergence than Shannon entropy.

Neural networks have several theoretical advantages: they are data-driven and self-adaptive, they can approximate any function to a high degree of accuracy, they suit nonlinear modeling of complex relationships, and they can estimate posterior probabilities for classification and statistical analysis.

The conventional error backpropagation uses MSE as the cost function, which can sometimes lead to slow error reduction stages, affecting learning times. To address this issue, we propose using entropy error functions, which simulations have shown result in improved network performance with reduced stagnation periods. We suggest using error entropy minimization instead of MSE for classification.

The paper is structured with: Section 2 covering related work, Section 3 detailing multilayer backpropagation neural networks, Section 4 discussing MSE, Section 5 analyzing Shannon entropy, Sections 6 and 7 presenting simulation results for Shannon entropy and MSE respectively, Section 8 comparing the two cost functions, and Section 9 providing conclusions.

In controlling weight adjustments and dampening oscillations during the backpropagation learning process, it is observed that the convergence rate can be slow, particularly due to the saturation behavior of the activation function, which results in minimal descent gradient values and thus slow weight adjustments. We describe an algorithm that measures error based on a binary target variable, seeking to minimize this error but not by using the direct expression for the error.

(Note: The paraphrasing is indicative of the overall content. For academic purposes, make sure to read and cite the original paper directly to capture the technical details accurately.)