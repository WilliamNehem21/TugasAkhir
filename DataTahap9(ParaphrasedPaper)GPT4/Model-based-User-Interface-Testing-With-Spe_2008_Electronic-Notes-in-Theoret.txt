This academic paper investigates the possibility of using task models to create test oracles for the model-based testing of user interfaces, with an emphasis on making the testing process more abstract and reducing the construction effort. The authors chose ConcurTaskTrees (CTT) as the task modeling notation due to its hierarchical structure, popularity, and existing tool support, specifically the TERESA tool.

The TERESA tool is used to animate the CTT task models and generate presentation task sets (PTS), which outline the enabled tasks at each stage of interaction. This is coupled with the use of the FIT framework's keywords to add detailed task information, such as window management, into the CTT models to generate low-level test oracles. The generated oracles are described using Spec#, a formal language for behavior specification.

The paper illustrates the approach with the Windows Notepad editor's "find" function. The authors created a task model based on prior use and extended testing. Conditions are incorporated into this model, and a tool named TOM automates the enrichment of the initial finite state machine (FSM) expressed with PTS into a more detailed Spec# oracle.

Manual refinement of the Spec# oracles may be necessary, as the CTT model might be kept at a higher abstraction level. Subsequently, adjustments can be made efficiently, taking only a short time for a tester to complete.

The larger vision is to create comprehensive task models that could encompass all interactions with a system, enabling the testing of actual system behavior against the intended design captured in the task models. However, oracles based on task models may not capture the full range of user behaviors or GUI possibilities since users can interact with systems in various ways.

The challenge lies in the information missing from the task models related to specific implementation choices. To address this, the authors suggest annotating task models with implementation-specific details. To test behaviors outside the task model, the authors propose altering the task model to include potential user errors or applying transformations to test against different scenarios.

Future work intends to explore the GUI's resilience against user errors or unexpected behaviors, either by adapting the task models to include such behaviors or using a more dialogue-controlled based oracle.

The paper concludes that, despite some limitations, task models can effectively be used to generate oracles for user interface testing in a model-based testing context, with tool support that enables automated oracle generation from task models, and discusses the benefits and drawbacks of this approach.