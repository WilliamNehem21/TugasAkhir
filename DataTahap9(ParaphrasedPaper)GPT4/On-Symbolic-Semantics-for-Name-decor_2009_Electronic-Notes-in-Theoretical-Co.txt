Concurrent and distributed systems are increasingly operating within open environments, allowing their various components—be they components, agents, or services—to form dynamic connections. For example, in service-oriented architectures, computational resources can be accessed via interactive sessions that are created and dissolved as needed. These systems, with dynamically binding components, can even continue to evolve during runtime, resulting in a system that is partially defined even while in operation. There are challenges in characterizing and analyzing the behavior of such systems when there is incomplete information about them.

Web crawlers, also known as bots, spiders, or scutters, are automated programs that navigate the World Wide Web to collect (or sometimes generate) information. Noteworthy examples include search engine crawlers like Googlebot and malicious crawlers like spambots that harvest email addresses for spamming or phishing purposes.

Certain protocols have been designed to manage interactions between web crawlers and sites; for example, the robots.txt file and sitemaps serve as de facto standards for indicating which sections of a website crawlers should ignore or prioritize. Although crawlers are not obligated to follow these protocols, web servers often have methods to distinguish between automated crawlers and human users, such as analyzing navigation speed or patterns, and enforce compliance with these rules accordingly.

The paper also delves into the behavior of three types of crawlers—careful, cautious, and rash—within static networks (i.e., networks in which pages are neither added nor removed during the crawling process). Careful crawlers validate the existence of a page before storing or acting on its URL; cautious crawlers change their target page without such verification; and rash crawlers do not check the existence of pages at all. Despite their differences, all three types can analyze an existing page.

A simple name-based calculus is proposed to model the actions of crawlers within a web of links. Here, the web system can consist of various elements like crawlers and links, and is considered dynamic, with the capability for crawlers to learn new site addresses by analyzing links from their current site. This learning process is similar for all types of crawlers, abstracting away the detailed interactions of real-world crawlers.

The paper also uses certain theoretical frameworks (like theorem 4.9) to understand how the processes of a crawler extend their knowledge about a web system by observing and interacting with the links within it. The focus is placed on how a crawler might handle a link leading to a non-existent (missing) page, termed a broken link, and how this influences the crawler's knowledge of the web.