To address the challenges in multi-objective optimization, researchers have developed descent methods based on traditional single-objective optimization techniques for both constrained and unconstrained problems. This study focuses on a particular algorithm that extends the scalar Augmented Lagrangian method to multi-objective optimization as introduced by Cocchi and Lapucci (2020).

The paper provides a comprehensive theoretical analysis of their multi-objective algorithm, asserting its well-defined nature and laying out its convergence properties. It also includes an important underlying assumption for the analysis.

For experimental validation, the algorithm was tested using 10 different seeds for the random number generator, and each test had a 2-minute time limit. The resulting Pareto fronts were evaluated using the purity metric, and the best performance was selected for comparison against a reference front.

It is worth noting that the results may slightly favor the NSGA-II algorithm because it assumes an ideal scenario for this algorithm, which is inherently stochastic. Contrarily, deterministic methods like FRONT-ALaMO, DMS, and MOSQP were only executed once each.

The benchmarks include tweaked versions of the BNH and LAP problems from Cocchi and Lapucci (2020), along with a revised version of the OSY problem, which is detailed in the appendix.

This paper centers on smooth, constrained, and convex multi-objective optimization problems, aiming to create high-quality Pareto front approximations. After reviewing related work, the authors propose an Augmented Lagrangian method tailored to multi-objective scenarios.

The method put forward by Cocchi and Lapucci (2020) is capable of finding an entire Pareto-stationary set rather than just a single solution. The algorithm iteratively manages a list of mutually non-dominated and Pareto-stationary points with respect to the current multi-objective Augmented Lagrangian. Line searches in various descent directions are conducted to explore the objective space while regularly updating the penalty parameter and Lagrange multipliers in response to constraint violations by the points in the list.

Extensive computational experiments demonstrated that the new method surpasses the SQP algorithm by popular multi-objective optimization metrics. The authors also compared their method with other leading techniques, including derivative-free (DMS) and genetic (NSGA-II) methods, and found that their approach achieved better results.