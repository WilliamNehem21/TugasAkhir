Modern life is increasingly intertwined with digital simulated environments like virtual environments (VE) and virtual reality (VR). Defined as computer-generated simulations, they engage users in a world akin to reality through multiple senses, interaction devices, and simulated scenarios. This spectrum of digital experiences is categorized in a Reality-Virtuality Continuum, ranging from pure reality to pure virtuality, with all intermediate stages labeled mixed reality (MR), where augmented reality adds virtual elements to the real world, and augmented virtuality incorporates real elements into a virtual setting. Though typically focused on visual elements, these immersive technologies can also engage other senses like sound, movement, and even taste or smell.

Two central theories in educational VE design are Mayer's Cognitive Theory of Multimedia Learning, proposing that learners construct knowledge by selecting and linking visual and verbal information, and the widely-utilized Technology Acceptance Model (TAM), which predicts technology adoption based on perceived ease of use and usefulness. Multiple learning outcomes from educational VEs are tracked; knowledge is often gauged with tests, while skills are measured, sometimes with an emphasis on their transferability to real-world application.

This paper suggests that despite growing interest, research in VE effectiveness often suffers from methodological flaws with a need for better power analysis and sample size justification. Studies frequently lack control groups or use inadequate reference groups, which obscures the actual effectiveness of VE methods. A variety of research methods and metrics are in use, making comparisons difficult, and there is a publication bias towards positive findings. The paper recommends a formal guideline for evaluating training effectiveness in VEs with at least two, preferably three, measurement time points (pretest, immediate post-test, and retention test), thoughtful selection of learning outcomes, and consideration of potential conflicts of interest.

As an empirical example, Yang, Chang, Hwang, and Zou's (2020) study was cited, which explored how a cognitive complexity-based competition game affects English vocabulary learning, anxiety, and behaviors in non-native speakers.