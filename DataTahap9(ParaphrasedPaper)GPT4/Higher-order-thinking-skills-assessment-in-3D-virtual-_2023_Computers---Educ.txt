In the 80s and 90s, technological advancements led to the rise of virtual learning environments (VLEs) similar to the online educational tools used extensively today. Such platforms deliver educational materials, facilitate communication, and enhance skill development, and they come in various formats with different degrees of user interaction, game-like features, and immersive experiences.

Queiroz et al. (2019) noted that VLE assessments traditionally focus on concrete skills in fields such as biology, computer science, and medicine, while the assessment of higher-order thinking skills and the integration of AI in learning pattern analysis is less explored. Spector & Ma (2019), however, cautioned against overly relying on AI for education, emphasizing the importance of human intelligence and the assessment of higher-order thinking through simulations and games.

Further research is needed to help educators provide more nuanced assessments that identify student strengths and weaknesses. This could be done through assessing smaller tasks, using expert actions to manage data size. Reviewing literature has revealed gaps in the effectiveness of series-based assessments, the use of expert data for smaller tasks, and the selection of appropriate measures for similarity analysis.

Our research was motivated by identifying assessment methods offering specific feedback on smaller activities without large datasets. We suggested an assessment method using motifs—smaller activities—and their similarity to expert actions, allowing for more direct educator involvement in designing assessments.

We conducted a study in a 3D VLE representing a chemistry laboratory using both desktop and head-mounted displays to train student safety procedures. Traditional safety training often uses text and video material followed by question-and-answer sessions. A 3D VLE offers a safe environment to simulate emergencies like fires or chemical spills, which cannot be safely created in reality.

During the winter 2021 semester, we recruited 36 university students (20 males and 16 females) with an average age of 25 and a standard deviation of 6.45 from chemistry or related science and engineering programs to participate in the study via social media and colleague referrals. Despite their previous traditional safety training, many students still faced challenges in actual labs. Many participants also had experience with immersive VR gaming.

Following the granular analysis, large action patterns were converted into motifs, representing specific skills across six laboratory chambers. In this study, the instructor sought three skills within three tasks, each with distinct activities and orders. We tested various similarity measures to determine which would yield the maximum similarity index when comparing students to an expert, hypothesizing that different high-order thinking skills might require different measures.

We collected extensive student interaction data in the VLE and eliminated non-task-related actions from analysis. Cosine similarity index provided the highest correlation for assessing task performance that was not sensitive to action order.

The outcomes of the study indicated that motifs and expert data comparison are useful in series-based assessment of higher-order thinking skills, providing precise feedback and overcoming the constraints of traditional methods. Different similarity measures might be better suited for different tasks or skills, suggesting a need for additional research into these measures' relative effectiveness.

In conclusion, this paper examined the effectiveness of motif-based assessments for higher-order thinking skills in 3D virtual learning environments, highlighting the utility and potential of virtual platforms when physical classrooms are unavailable. The research affirms the value of using small, meaningful learning elements (motifs), expert comparative data, and a range of similarity indexes.