In this scholarly paper, the goal of the optimization process is to identify critical points where the gradient, denoted as l, equals zero. The rate at which the optimal solution is approached is influenced by the second derivative of the response matrix, referred to as m. Whether the critical point is a minimum or a saddle point depends on the eigenvalues of matrix m, which reflect the curvature of the function being optimized (f) along the direction of the corresponding eigenvector.

The gradient (l) can be described as outlined in the referenced chapter, and further analysis is conducted to set the determinant of an augmented matrix m+ to zero. This creates a theoretical possibility for solutions to intersect within a particular submanifold. The paper highlights the necessity of determining the precise conditions under which these intersections, or solution crossings, might occur.

An approach to calculate the function h(u) is outlined, involving the differentiation of a particular expression. For the specific optimization task at hand, it's noted that there is no need to track the full trajectory or streamline. Rather, it is sufficient to ascertain a series of intermediate points situated between two critical locations. To approximate these points, the authors suggest employing the method of steepest descent: initiating with an estimated value for 'a' that is derived by linear interpolation between a saddle point and a local minimum, followed by a number of iterations employing the steepest descent method, as detailed on page 121 of the referenced text.