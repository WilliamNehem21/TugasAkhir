The paper addresses the limitations of traditional machine learning (ML) and deep learning (DL) systems that utilize the conventional von Neumann computing architecture, where memory and processor are separate entities. This separation causes data bottlenecks during the movement of information between the two. In this context, in-memory computing (IMC) emerges as an innovative approach that integrates computational tasks directly into the memory, helping to overcome these bottlenecks.

The paper focuses on memristive in-memory computing systems (MIMDLS) for DL deployment and offers an overview of simulation frameworks and tools used to model these large-scale systems. It outlines modern computer-aided design (CAD) simulation frameworks that leverage current software engineering techniques, enable high-level language APIs, and can accurately represent non-ideal device characteristics and peripheral circuitry.

The paper is organized into six sections: Section 2 discusses the basics of modeling and simulation for in-memory MIMDLS, Section 3 reviews existing CAD tools designed for this purpose, and Section 4 compares simulation frameworks through the simulation of two MIMDLS architectures. Section 5 gives future perspectives for MIMDLS simulation frameworks, and Section 6 concludes the paper.

Detailed architectures such as tiled crossbar architectures, modular crossbar tiles, and peripheral circuits are described, with a comprehensive overview of IMC accelerators for DL provided elsewhere. The paper conducts simulation studies by training the VGG-8 network and performs inference using the GoogLeNet network on the CIFAR-10 dataset. Notably, simulations are limited by the capacity of current frameworks and GPU memory constraints, restricting the size and complexity of networks that can be trained.

The simulations assume devices with a finite number of conductance states and analog-to-digital converters (ADCs) with 6-bit resolution. Simulation outcomes are presented as averages with standard deviations for both training and inference, over multiple runs or training epochs. All the code used for simulations is publicly accessible, allowing for custom comparisons using different hardware, network architectures, and hyperparameters.

Furthermore, the paper compares the performance of training and inference simulations between proprietary simulators like MemTorch, DNN_NeuroSim_v2.1, and IBM's Analog Hardware Acceleration Kit against baseline simulations conducted using the native PyTorch ML library. To enhance performance, network parameters are quantized to 16-bits using the torch.cuda.amp toolkit in the baseline PyTorch simulations.