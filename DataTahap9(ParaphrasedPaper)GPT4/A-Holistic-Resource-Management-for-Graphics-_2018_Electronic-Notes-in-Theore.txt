This paper outlines significant contributions to the field of cloud computing, with a focus on the effective management of GPU resources for general purposes. The highlights include:

1. A systematic architecture designed to oversee GPU resource sharing in cloud environments, aimed at reducing application deployment time while ensuring quality of service (QoS) during operation. The architecture prioritizes both performance and energy consumption.
2. An experimental study comparing the influence of the architecture on performance, as well as power and energy utilization across different GPU models.

The paper is organized as follows:
- Section 2 gives an overview of work related to the topic.
- Section 3 describes the proposed architectural system.
- Section 4 details the benchmarking and analysis of heterogeneous GPUs.
- Section 5 presents experiments and results about GPU benchmarking.
- Section 6 concludes and outlines directions for future research.

The paper also delves into GPU occupancy, a crucial metric for measuring performance in general-purpose GPU usage, where occupancy is the ratio of active threads to the maximum capacity of a streaming multiprocessor (SM). This ratio ranges between 0 and 1 and is calculated by varying the number of threads per block while keeping the blocks constant to ensure simultaneous SM operation. The study observes changes in power consumption and execution time for different matrix multiplication scenarios and notes distinct power usage behaviors as the GPU workload changes, particularly on a Fermi C2075 GPU.

Further analysis shows a correlation between GPU occupancy, memory types, hardware block scheduling, and power consumption, revealing variations dependent on the GPU architecture. For instance, increasing the number of blocks improves performance but also raises energy consumption on the Fermi C2075 GPU, whereas on the Kepler K40c GPU, it enhances performance and energy efficiency.

The paper presents an adaptive architecture to manage heterogeneous GPU resources in cloud computing, with the goal of optimizing for performance and energy consumption. An initial step in developing this architecture is the introduction of a heterogeneous GPU analyzer tool.

The proposed future work includes creating an energy consumption prediction model based on influential factors to guide an energy-efficient scheduling policy for GPU applications. Additionally, an adaptive management framework is planned to automatically balance the trade-off between energy efficiency, performance, and cost to maintain the QoS for GPU applications during operation time.