The increasing significance of deep learning (DL) workloads in high-performance computing (HPC) systems necessitates consideration of their performance for the design and deployment of next-generation computing infrastructure. DL applications heavily depend on both DL frameworks and the underlying compute hardware like CPUs and GPUs. Therefore, comprehensive understanding and assessment of compute kernels, models, and frameworks within DL stacks is crucial, particularly for their impact on scientific and mission-critical tasks.

At the Oak Ridge Leadership Computing Facility (OLCF), a collaboration with Argonne and Livermore labs (CORAL) has led to the development of a set of DL benchmarks. These benchmarks are employed to evaluate the AI capabilities of cutting-edge supercomputers. The paper shares initial findings and performance benchmark comparisons of an NVIDIA V100-based Summit system (employing the CUDA stack) and an AMD MI100-based testbed system (utilizing the ROCm stack). It emphasizes a layered approach to DL benchmarking and suggests potential optimization avenues for the technologies under review.

As DL applications play a larger role in HPC center allocations, evaluating the performance of DL stacks becomes essential for the procurement of future HPC infrastructures. To this end, the CORAL collaboration has included DL workloads in their benchmark suite (CORAL-2) for the first time, directly influencing the evaluation and acquisition of systems like Frontier, Aurora, and El Capitan. The benchmarks range from elementary DL kernels to comprehensive training tasks and are more focused on throughput and fundamental components compared to industry-led efforts like MLCommons HPC.

Although DL models are becoming increasingly complex, the crucial computations boil down to certain mathematical kernels: general matrix multiply (GEMM), convolution, and recurrent operations along with vital communication operations, particularly in distributed training scenarios. The paper's subsequent sections provide more context on traditional HPC workloads versus emerging DL workloads, a detailed methodology for performance evaluation, and present results from this approach offering a comprehensive perspective on key performance metrics.

The paper also discusses the roofline model as an illustrative metric for identifying computational or memory bottlenecks and outlines resource utilization monitoring as a means to detect operational inefficiencies. Communication kernels are underscored for their growing importance in distributed training setups. Final conclusions point out that the MI100-based system excels in single precision tasks, but there are areas for improvement, particularly in the ROCm stack's maturity. Machine learning techniques have been used to model benchmark performance relations with input parameters, providing insights into the performance differences between the two stacks.

The research emphasizes that as kernels and frameworks evolve, particularly for the ROCm ecosystem, the specific findings may change over time but the systematic approach to DL benchmarking presented will remain relevant. This work was supported by the OLCF, a DOE Office of Science user facility located at the Oak Ridge National Laboratory and funded by the US Department of Energy.