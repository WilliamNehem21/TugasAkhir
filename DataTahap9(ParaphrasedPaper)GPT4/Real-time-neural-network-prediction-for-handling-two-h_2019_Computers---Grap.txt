Our research targets improving marker-based motion capture, particularly for hands and fingers, by utilizing machine learning to correct for occlusions, which can inhibit the accurate embodiment of an avatar in virtual reality (VR). Typical inertial measurement units (IMUs) used for motion tracking offer short-term precision but are prone to long-term drift due to their reliance on relative positioning. Furthermore, common problems in full-body VR experiences include self-occlusions and static hand movements, which break the sense of body ownership. Traditional solutions, such as adding more cameras, don't fully solve these issues.

We propose a machine-learning algorithm to predictively model and correct occlusions in real-time, which we introduce after reviewing the current state of related technology and methods. Our training dataset's attributes are detailed, as well as three baseline methods to address occlusions. We also survey research that assumes a skeleton model is in place for constraining solutions, and specific studies on passive marker systems for fingers and face that use Gaussian Mixture Models (GMMs) for swift recovery from occlusions without considering predictions during occlusions.

Prior art includes modeling of hand postures and using multiple neural networks for hand localization and joint position regression. However, these techniques have motion range limitations. In motion capture, keypoint estimation with CNNs and sensor fusion from multiple data sources have been utilized, albeit with notable shortcomings when facing multiple occlusions.

Our method improves on related work by incorporating a deep learning model with a forward kinematics layer that constrains outputs to feasible postures, which could offer higher precision and range of motion compared to techniques like those used with Microsoft's Kinect.

For tracking, we integrate absolute positional data from an active motion capture system and apply sensor fusion to synchronize different data streams, ensuring plausible results. To approximate the solution to the marker transformation problem in cases with excess markers, we use L2 regularization.

We compare our approach to simpler baselines, like maintaining last known positions of occluded markers and using moving averages of marker velocities to account for noise and maintain temporal consistency. The model focuses on minimizing reconstruction errors on the validation set while considering performance and latency, with automatic learning rate adjustments during training.

Adding both hands in VR is shown to enhance user interaction with the virtual environment, demonstrated by a video exhibiting various complex actions. Our data-driven approach is tailored for hand and finger reconstruction using a minimal number of markers per finger and surpasses analytic inverse kinematics which fare poorly without intermediate markers.

Moving forward, we aim to explore advanced neural network architectures to handle temporal consistency during occlusions and integrate passive motion capture systems. Moreover, we plan to develop a dual-hand network for interaction handling and to enforce improved self-contact consistency to enhance avatar body ownership.