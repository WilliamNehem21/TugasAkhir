The paper discusses the importance of predicting defect-prone software components to direct testing resources efficiently and enhance the business value of software projects. Most existing research in this field uses supervised machine learning but often face issues with imbalanced datasets. Furthermore, understanding what features drive the predictions of these models is crucial.

The authors introduce a new prediction framework employing eleven machine learning classifiers and twelve datasets, utilizing four nature-inspired algorithms for feature selection--particle swarm optimization, genetic algorithm, harmony algorithm, and ant colony optimization. To tackle dataset imbalance, they use the Synthetic Minority Oversampling Technique (SMOTE) and employ the Shapley additive explanation model to identify the most important features. The results indicated that gradient boosting, stochastic gradient boosting, decision trees, and categorical boosting performed best, achieving over 90% accuracy and ROC-AUC, with ant colony optimization being the superior feature selection method.

The paper is structured into six sections: Section 2 reviews related research, Section 3 presents the proposed prediction framework and methods, Section 4 details evaluation metrics and tuning of hyperparameters, and Section 5 discusses experimental outcomes and explanations for classifier outputs using SHAP values. Finally, Section 6 concludes and suggests future research directions.

The paper provides an account of various studies using machine learning for software defect prediction, including experiments with NASA datasets and different classification algorithms. Results from these studies show improved performance when advanced techniques like deep learning or bio-inspired feature selection were employed.

The issue of imbalanced data is addressed using SMOTE, emphasizing its effectiveness when dealing with uneven distributions in data classes, which is common in NASA datasets.

The study experiments with various machine learning models and metaheuristic feature selection methods, finding improvement in testing accuracy and particularly in classifiers like gradient boosting and categorical boosting. To understand the impact of features, the authors used the Shapley additive explanation model, noting that metrics like lines of code, comments, and complexity have significant predictive power.

The paper concludes that some classifiers performed exceptionally well on certain datasets, with average accuracies reaching up to 99% after applying SMOTE for data balancing and optimization techniques like ant colony optimization and particle swarm optimization for feature selection. The most influential features in these classifiers were found to be metrics related to code and design complexity.