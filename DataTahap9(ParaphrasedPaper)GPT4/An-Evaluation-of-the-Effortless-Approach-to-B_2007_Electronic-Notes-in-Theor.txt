Naps et al. present two primary methods for constructing engagement level in learning algorithms: directly through automated generation and manually through hand construction. Their study utilizes a hybrid approach incorporating both methods. Visualizations of the algorithm's execution phases are created automatically by dedicated software, facilitating direct generation.

The paper is organized into several sections. Section two details the evaluation process, including participant selection, variables assessed, methods, and procedures. Section three presents the results of this evaluation, followed by a discussion in section four. Conclusions and prospects for future research are outlined in section five.

The study participants were randomly assigned to two groups: viewers (VG) and builders (BG). Both groups were surveyed on their pre-existing knowledge of the algorithm, with only one student being familiar with it. Therefore, it was confirmed that both groups came from the same baseline population, ensuring comparability of subsequent findings.

The evaluation comprised two phases: an initial training session to familiarize participants with the Integrated Development Environment (IDE), and an experimental session assessing their understanding of the algorithm. Ten and thirteen students participated in each session, respectively. During the two-hour training session, an instructor demonstrated the use of the software tool and generated two animations as examples, which the students then replicated. These animations were not related to the algorithm that would be studied in the experimental session. Students completed a questionnaire afterward, providing their initial impressions of the tool.

Two weeks later, the experimental session took place, also lasting two hours. Students were informed that their participation was voluntary before being assigned to either the VG (7 participants) or the BG (6 participants). Each group received a textual description of the algorithm to study; however, it is implied that the BG group received additional materials, possibly including the means to generate or interact with visualizations.

The study revealed substantial differences in the learners' abilities to utilize the algorithm in practice. Only 14% of the VG could identify a key concept, whereas 83% of the BG could do so, amounting to a 16% improvement in the average understanding levels (BG scoring 0.88 and VG 0.73). Even more significant was the difference in their ability to apply the knowledge, with the VG scoring an average of 0.33 while the BG scored 0.77â€”a 60% improvement in learning efficiency. Both differences were statistically significant.