Biometrics is a crucial field within pattern recognition, and face recognition stands out as a particularly appealing biometric approach. However, effective face recognition in real-world scenarios presents numerous obstacles. This is largely attributable to the face being a deformable structure that can appear differently due to a range of factors such as facial expressions, age variations, viewing angles, and notably, lighting conditions. The advent of deep learning has significantly impacted computer vision, with AlexNet, created by Alex Krizhevsky, winning the ILSVRC-2012 competition by a large margin.

For engineering applications of deep learning, substantial training data is essential. Researchers from Baidu have underscored the critical role ample training data plays in enhancing deep networks' efficacy. By expanding their training data set with additional images, they demonstrated an improved accuracy in face verification tasks. This indicates that more extensive training data can indeed enhance deep networks' performance.

To address issues related to lighting changes in face recognition using deep networks, it's essential to have a vast collection of training data covering a broad range of lighting conditions. Collecting such diverse datasets in the real world is challenging. This study proposes a deep network that utilizes both visible light and near-infrared images, capitalizing on the strengths of each. Publicly available visible light images are used to train an initial deep model, which is then refined using near-infrared images. This completed model can extract features from near-infrared images and is used with cosine distance to measure similarity and an adaptive score fusion approach to classify faces.

The system employs two imaging devices—a visible light and a near-infrared camera—to simultaneously capture dual-modality facial images. Feature extraction is performed on both image types using corresponding models. A combined scoring system, which leverages the complementary features derived from each modality, is then applied to accomplish the final classification.

Face recognition techniques have been traditionally categorized into local feature methods, subspace-based methods, and sparse representation methods. Each methodology addresses various challenges associated with facial recognition, such as expression changes, occlusions, and disguises, using different focus areas such as local features or transformation into sparse low-dimensional spaces.

The paper references the architecture of the VGG net model, which includes five blocks of convolutional layers and nonlinear operations, with pooling layers at the end of each block. Additionally, the Sunwin face database, comprising both visible light and near-infrared images under various lighting conditions, is used in the study.

Three fusion strategies mentioned in the paper—pixel-based, feature-level, and score-level—show that score-level fusion is superior for face recognition, as it prevents information loss common in feature-level fusion. The proposed system utilizes score fusion for its final classification step.

Upon comparing traditional face recognition methods, such as Local Binary Patterns (LBP), to the deep network model, a substantial performance improvement is observed with the latter. The paper's CNN-based model, using both visible light and near-infrared images coupled with an adaptive score fusion strategy, proves effective in dealing with illumination variance. This is confirmed by superior performance on various datasets, signaling that the newly proposed method offers a robust solution for feature extraction in face recognition tasks.