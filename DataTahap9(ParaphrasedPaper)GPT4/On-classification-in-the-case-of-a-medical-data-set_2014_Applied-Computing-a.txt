In previous research, we developed a method for analyzing nystagmic eye movements essential in otoneurological assessments. Our goal was to automate the differentiation between invalid and valid nystagmic movements because only valid movements contribute to accurate diagnosis in otoneurology. Noisy or artifact-laden movements typify invalid ones. We explored the application of machine learning to classify these movements into 'rejected' (invalid) and 'accepted' (valid) groups. This turned out to be challenging because of the complex nature of the data distribution. We discovered that efforts to improve classification by removing some of the larger 'rejected' group led to unexpectedly poor results with certain machine learning techniques, due to the intricate data distribution.

Initially, we operated under the assumption that the data would form two distinct classes, with invalid eye movements being rejected based on exceeding predetermined thresholds. The manual selection process categorized 2,171 movements as accepted and 3,818 as rejected. In contrast, an automated approach yielded 2,517 accepted and 3,472 rejected movements. When both methods were used, the result was 1,645 accepted versus 4,344 rejected. Following the application of 'cleaning' to reduce the larger rejected class, the numbers, based on the selection method, were 2,171, 2,517, and 1,645 accepted movements respectively.

The automated selection process occasionally outperformed the combination of manual and automatic techniques due to its consistency, as manual criteria can fluctuate slightly over time. The study also revealed that using a larger number of nearest neighbors (5, 7, 9, or 11 instead of just 3) in the classification slightly improved the results by cleaning more elements from the majority (rejected) class. Nonetheless, this wasn't a universal finding, suggesting that the characteristics and distribution of the data remain pivotal in determining the classification outcome.