In the last ten years, plug-in hybrid electric vehicles (PHEVs) have made notable strides in reducing fuel consumption and CO2 emissions. Nevertheless, the effectiveness of these improvements is highly dependent on the sophistication of the on-board energy management strategy (EMS). As technologies like connected vehicles and automated driving become more widespread, the complexity of controlling EMS objectives has increased. Traditional rule-based methods are known for being robust, efficient, and easy to compute, but they may not offer the best outcomes for complex, multi-objective, nonlinear, and dynamic systems. This has highlighted the necessity for smarter control systems in future vehicles.

Compared to the charge-depletion charge-sustaining (CDCS) strategy, there can be up to a 10% fuel saving. Although current approaches can work well with real-time hybrid control units (HCUs), it's challenging to balance multiple factors such as objectives, real-time performance, tuning efforts, and vehicle hardware limitations. Reinforcement learning (RL)-based EMS approaches in PHEVs have captured substantial interest from the AI community due to their capacity to learn optimal control policies through interaction, rather than being programmed with a specific strategy.

The study distinguishes three drive modes: 1) Conventional Drive (CD) with internal combustion engine (ICE) as the sole propulsion source, 2) Optimum Generation (OG) where required torque is low and ICE efficiency can be improved by using excess power to charge the battery, and 3) Electric Drive (ED) enabling the electric motor (EM) to power the vehicle.

The RL agent gathered 50,000 experience tuples following a random policy, which were processed to evaluate rewards and estimate Q-values. Various neural network (NN) architectures were trained and the chosen one illustrated the lowest root mean square error (RMSE), indicating superior Q-function approximation.

The Double Deep Q-Network (DDQN) algorithm mitigates the overestimation of Q-values and assures stable training by using two separate NNs: one for policy and another for target Q-value estimation. Unlike some methods that only involve the agent in free modes, this approach allows full observability/control, as excluding the agent in some modes led to convergence issues.

This research demonstrates RL-based EMS can greatly enhance PHEV fuel efficiency, with an adaptive online learning RL agent incorporated into an existing HCU framework. The study started by setting up the control issue as an infinite-horizon optimal control problem, considering various vehicle-related factors, with the goal of minimizing total fuel consumption and engine switches.

The study suggests a future trend of integrating these systems with intelligent transportation to create smarter cities and grids. As vehicles become more connected, distributed multi-agent RL systems will be essential for vehicles to cooperatively learn. The transition to intelligent control systems is critical for automotive manufacturers to stay ahead of challenges.

Acknowledgments were given to Prof. Thomas Schlechter and Prof. Stefan Winkler for their support, as well as to the AVL DSP team—Patrick Teufelberger, Huan Chen, Evgeny Korsunsky, and Bhargav Adabala—for their collaboration and valuable input. Additionally, the author thanked editors and anonymous reviewers for their constructive critiques and suggestions which contributed to improving the research.