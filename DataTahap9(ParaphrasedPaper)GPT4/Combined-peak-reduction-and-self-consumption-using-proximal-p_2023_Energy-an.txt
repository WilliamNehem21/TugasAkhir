In recent times, Demand Response (DR) programs, which encourage homeowners to adjust their electricity use, have started to leverage Reinforcement Learning (RL) methods, especially for allowing flexibility in energy consumption. However, RL is often criticized for not being data-efficient. To tackle this, algorithms like Proximal Policy Optimization (PPO) have been developed to better use data, and researchers are exploring the pairing of RL with transfer learning to further improve results. This study builds on these advancements by integrating specific DR knowledge to enhance transfer learning, resulting in better performance. The research focused on a DR scenario aimed at reducing energy use during peak times and increasing self-use of energy, which had financial benefits under a capacity tariff systemâ€”as introduced by the Flemish energy regulator (VREG) in Belgium. This approach was tested using real household data from the Netherlands.

The paper consists of four sections, detailed as follows:
1. An in-depth explanation of the capacity tariff design, the formulation of the Markov Decision Process (MDP) used for RL, and the specifics of the adapted RL algorithm.
2. A section on experimental setup and the detailed analysis of the results achieved.
3. A conclusion summarizing findings and proposing future research directions.

To address the challenge of minimizing reliance on extensive forecasts of local solar PV production and demand, the study aimed to develop control policies that would be applicable across various households, irrespective of specific environmental parameters like inverter capacities. Transfer learning was employed so that policies learned in one context could be effectively applied in another without significant loss of performance.

The pre-training phase used simulated sunshine data, simulated electrical load data, and domestic hot water consumption data to train the RL agent over the equivalent of 15 simulation years. The subsequent results illustrated that incorporating domain expertise in the RL pipeline led to cost savings (14.51% compared to a standard hysteresis controller and 6.68% compared to traditional PPO) and mitigated some of the disadvantages of model-free learning methods, echoing trends in physics-informed machine learning.

The paper suggests that RL methods, particularly when enhanced by domain knowledge, are promising for DR settings. The implementation of capacity tariffs in Flanders could motivate residential users to adopt smarter control strategies for their energy appliances, like electric water heaters. Looking to the future, the researchers plan to investigate integrating local control methods within an aggregator framework to further reduce peak power consumption in residential areas.