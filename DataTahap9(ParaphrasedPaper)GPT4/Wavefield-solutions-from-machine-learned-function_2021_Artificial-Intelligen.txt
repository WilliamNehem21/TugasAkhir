A more extensive network can produce more precise wavefields. However, for the purpose of computing gradients to update the velocity model, a flawless representation of the scattered wavefield isn't crucial. Training a network with 12 layers is roughly 50% more expensive than one with 10 layers, despite the fact that the network's model parameters only grow by 4%. Conversely, training a network with 8 layers costs about two-thirds of what it does for a 10-layer network, but it has only a quarter of the model parameters. In conclusion, with the current network design, training costs go up by approximately 50% when two additional layers, each twice as big as the initial largest layer, are included, resulting in a wavefield with higher resolution.