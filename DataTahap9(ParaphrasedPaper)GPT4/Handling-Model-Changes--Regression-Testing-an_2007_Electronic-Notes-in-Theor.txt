Recent advancements have led to the development of methods for automatically generating test cases using model-checkers, with their effectiveness and scope largely hinging on the complexity of the underlying models. Fully generating new test suites can be costly, particularly when only portions of a test suite may need updating due to alterations in the model. This paper explores strategies for optimizing the process of updating test suites post-model modifications, aligning with the principles of regression testing to curtail the quantity of necessary test cases after a change.

Utilizing model-checkers for generating test cases is gaining relevance, as they can produce examples of software traces to be employed as test cases. The paper introduces various techniques for crafting novel test cases in response to model changes, which can serve as regression tests or for refreshing pre-existing test suites.

The document is structured in the following manner: Section 2 examines different types of model changes along with their relation to a specific model-checker input language, before detailing our approaches for identifying obsolete test cases and generating new ones. Section 3 details the experimental design, the methods used for measurement, and the outcomes of these experiments. The final section, Section 4, discusses these results and concludes the paper.

In practical terms, the Kripke structure is delineated through the model-checker's input language. To evaluate whether a test case is still applicable to an updated model, it must be translated into a verifiable form in which variables' transition relations rely on a special state counting variable, as per the recommendations of Ammann and Black.

The task of assessing the relevance of a test suite to a changed model boils down to model-checking each test case with the revised model. Test cases resulting in counter examples are deemed obsolete, while those not leading to counter examples remain valid. Nevertheless, this process can be computationally demanding for intricate models.

On a theoretical level, if a model change involves substitutions or removals of transitions that are then replicated by identical behaviors, false positives may be reported, though this can be circumvented by testing the altered model against a change property.

Old test cases nullified by model changes can be repurposed as negative regression tests in the implementation phase against the altered model. Following the removal of outdated test cases, new test cases must be created to cover the newly introduced model behaviors.

Additionally, by utilizing symbolic execution methods for checking test cases, the counter examples generated during this process can be directly used as new test cases.

This study has been put into practice using Python and the model-checker NuSMV, with experiments performed on a typical computer setup. Changes between model versions were determined via abstract syntax tree analysis, and a cruise control application example model was employed to evaluate the proposed methods. Mutation scores and creation times for updated test suites were monitored across model changes, averaging multiple rounds of experiments.

In conclusion, the paper presents solutions for retaining valid test cases after model changes and introduces various means for generating change-specific test cases. These methods not only update test suites effectively but also present a balance between enhancing performance and potential reductions in test quality. The study reveals the importance of optimizing model-checker based test case generation, especially when dealing with convoluted models, to attain substantial time savings during test suite recreation for model changes. Future work could include evaluating these techniques on more sophisticated models and realistic changes to better gauge their performance impact.