This academic paper delves into the specialization of neurons and the evolution of neural network models.

Initially, all neurons serve generic functions but become specialized with training. Dendrites receive signals while axons transmit electrical impulses over distances. Neuronal inputs prompt slow potential changes governed by membrane properties, enabling temporal summation of signals.

Research indicates that the cerebral cortex is organized into columns with similar characteristics. Human brain learning relies on the ability to adapt connections through exposure and regulated neuron activity. Information processed by the cortex strengthens connections over time, shaping mental representations. Conversely, the inhibition of neuron activity plays a role in forgetting.

Early artificial neural networks (ANN) were based on the basic McCulloch-Pitts neuron model, featuring a summation unit and binary activation. Over time, these models grew more complex, incorporating various activation functions and outputs, transitioning from deterministic to stochastic neurons to better simulate biological processes.

Edelman and Mountcastle's significant work shifted the focus from simple neuron models to cortical columns as the basic units of learning architectures. This was a more biologically plausible mechanism for learning and pattern recognition.

Deep neural networks (DNN) are more involved versions of simpler ANN designs, utilizing a basic neuron model but not strictly emulating the human brain. DNNs are based on a biologically inspired model called the neocognitron, aimed at achieving hierarchical knowledge abstraction. They learn different levels of features, but lack innate logical reasoning skills.

Columnar architectures (CA) are another deep ANN model inspired by brain structure. These networks employ concepts from cortical organization and synaptic modulation to form a training algorithm that captures sensory information and creates constant pattern representations. For more on CA's biological influences, see the cited references.

In summary, the computational cost of deep belief networks (DBN) and CA depends on the network's architecture—layer count and neurons per layer—which determines connection density. Both might employ a sigmoid activation function and post-training pruning results in some connections with zero weight. CA networks tend to have fewer non-zero weights than DBNs due to pruning. The effectiveness of network depth varies with the dataset, and weight thresholding is not simplified to a fixed value to accommodate varying input data weights, as some weights, while non-zero, are still relatively insignificant.