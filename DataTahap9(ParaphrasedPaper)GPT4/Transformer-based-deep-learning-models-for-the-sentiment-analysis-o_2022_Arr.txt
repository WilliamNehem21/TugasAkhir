To overcome the limitations of traditional feature extraction methods, word-embedding techniques were introduced. These techniques focus on capturing both semantic and syntactic information from words and have prompted renewed interest in neural network research across multiple studies. Many research efforts have concentrated on sentence-level classification challenges within sentiment analysis (SA). Two widely adopted word-embedding approaches are Word2Vec and GloVe. Word2Vec operates through two architectures: Continuous Bag-of-Words (CBOW), which predicts a word from its surrounding context, and Skip-Gram, which predicts surrounding words from a central word. In contrast, GloVe constructs word vectors through a global log-bilinear regression model that leverages word co-occurrence and matrix factorization.

This academic paper includes a literature review in two segments: the first involves word embeddings and cutting-edge transformers, while the second addresses SA classification models. Traditional models struggle with out-of-vocabulary (OOV) words and contextual disambiguation, often missing sentiment nuances. In the past two years, transformer-based models like BERT have set new standards in text classification, including SA. One study leverages a Chinese-language BERT trained on Wikipedia for stock review analysis, incorporating additional layers like BiGRU. Another study developed BERT-based personality recognition, yielding substantial accuracy improvements. Various neural networks using different embeddings have been compared for drug review SA, where BERT with LSTM provided promising outcomes. Word2Vec and BERT have been analyzed for Tunisian SA, with BERT outperforming when combined with a convolutional neural network (CNN).

The BERT-mini model relies on encoders from the transformer architecture to encode text, featuring four layers with multi-head attention and feed-forward mechanisms. Tokenization checks vocabulary occurrence and breaks down OOV words into subwords or individual characters when necessary. In classification, reviews are labeled based on intensity scores: negative if the negative score surpasses others, positive if it has the highest positive score, with a final pre-processing step for numerical conversion.

In this context, max-pooling is used during dimension filtering to highlight key features by selecting the largest values within filter patches, resulting in a feature map aggregating the most relevant information.

The proposed scheme's evaluation spans different datasets, covering various domains and sizes, focusing on positive and negative reviews. Performance metrics include recall, precision, F-score, and accuracy derived from confusion matrices, supplemented by ROC and AUC measurements. The BERT-based convolutional bidirectional recurrent neural network (CBRNN) model has been scrutinized against other deep learning models with various embeddings. This model exhibits superior precision and equivalent recall to Word2Vec-based LSTM and exhibits incremental improvements in F1-score, accuracy, and AUC compared to models using GloVe and Word2Vec embeddings. Ultimately, the superior performance of the BERT-based CBRNN model suggests its industrial applicability in conducting SA of products.