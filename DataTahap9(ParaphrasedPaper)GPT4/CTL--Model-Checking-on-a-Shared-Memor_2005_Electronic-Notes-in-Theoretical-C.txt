In this study, we introduce a parallel algorithm for CTL* model checking, which is designed to be used on a virtual shared-memory high-performance computing architecture. The method is based on the idea of a game played with automata, where one participant wins if the automaton is empty, and the other wins if the automaton has at least one element. We demonstrate how to perform this game in parallel by applying dynamic load balancing to distribute the workload evenly among processors. Performance charts are included to demonstrate the algorithm's practicality and its ability to effectively speed up the process.

The research investigates the practicality of parallelizing model checking for shared-memory multiprocessors by focusing on explicit-state on-the-fly model checking for CTL* with regards to safety and liveness properties. We created a parallel model checker and confirmed its practicality through both theoretical and experimental analysis, which also tested the parallel model checkerâ€™s correctness.

The paper presents an overview of our previously developed parallel algorithm for reachability analysis on a shared-memory architecture and discusses how this dynamic load balancing technique has been incorporated into a new parallel CTL* model checking algorithm. An explanation of the parallel algorithm is detailed in subsequent sections.

Our parallel reachability analysis algorithm operates on multiple processes, each on its own processor, sharing a common memory for visiting states. Each process has a local private stack and a bounded shared stack to manage unexpanded states, allowing it to add states to its own stacks. When a process's stacks are empty, it may 'steal' a state from another's shared stack.

An accepting run in the context of model checking is defined as a sequence wherein all infinite paths fulfill a given acceptance condition. Different alternating automata types possess varied acceptance criteria. For hesitant alternating automata (HAAs), the condition consists of a pair of states and hinges on specific restrictions in the transitions.

The parallel algorithm performs impressively and shows excellent scalability with more processors. For example, reachability analysis of a graph with 100 million states takes roughly four minutes on a single processor and only about twenty seconds on sixteen processors. The performance of model B is consistent with our algorithm's experimental results, showing that a process waiting to steal a state only experiences minimal delay before accessing the shared stack.