Research on cybersecurity often zeroes in on the technical functions of systems. However, we shift the focus to user-centered concerns, examining how human cognitive processes affect the secure transfer of information from users to computer systems. We build upon a framework we previously developed for checking usability attributes to better address these issues. Through several small case studies, we illustrate how some confidentiality breaches—stemming from a mix of poor design and particular cognitive traits—can be identified within our expanded framework.

Our study straddles the line between the intricacies of technical information-flow security and the broader, more human-centric dimensions of user-focused security. Specifically, we are concerned with the security of the information users input into computer systems, rather than the computer systems themselves. Instead of the social aspects of human-computer interaction, we put the spotlight on cognitive processes governing the flow of user information into systems.

We have found our cognitive architecture helpful in pinpointing systematic user errors in usability and task fulfillment contexts. We aim to demonstrate that the same behaviors that our architecture models can also unearth security concerns, thereby helping to refine user interaction designs from a security standpoint. As an initial step, we revisit several instances of user errors from our prior studies through a security lens. Then we use an example with the model-checking tool SAL to identify potential confidentiality breaches triggered by user interpretation of system cues. We particularly examine how user habits and the placement of input fields in authentication screens might give rise to security issues. While our examples are modest in scope, they serve to prove the broader applicability of our ideas and methods.

In our modeling, "user goals" represent knowledge users bring into an interaction regarding specific sub-goals tied to the task at hand, like inputs required by a device. Users may act on these ingrained objectives, regardless of the immediate prompts from the device. No set sequence is presumed for the accomplishment of user goals, which are modeled within the user module as an array. Three parts constitute the state space of the user model: input, output, and global (memory) variables. We simulate the environment with a global variable and define all these using type variables for each unique interactive system instance.

To prevent users from repeating the same goal-oriented actions without genuine prompts, we include a "goalcommit" command in our model, which triggers when specific preconditions—defined within the "goals" array—are met. Additionally, our model captures the phenomenon where users may terminate interactions upon achieving their primary goal but neglect secondary tasks. We use the SAL tool to represent these behaviors and test them for security implications. For instance, an old but relevant real-world problem like users forgetting their bank card at an ATM after withdrawing money is an example of a "post-completion error" that can also be a security risk.

Lastly, we study the potential for security issues arising from the interplay between system prompts and user interpretations using our cognitive framework formalized in SAL. We suggest that user inclinations, coupled with specific interface designs, might cause incorrect input—such as mixing up username and password fields—leading to unintended disclosure of private information. In sum, our paper extends our prior research to identify and address security problems that arise at the intersection of system design and human cognitive habits, utilizing formal verification tools to expose and understand these vulnerabilities.