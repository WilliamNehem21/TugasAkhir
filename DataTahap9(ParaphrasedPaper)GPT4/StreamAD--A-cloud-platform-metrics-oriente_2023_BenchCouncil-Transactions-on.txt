As cloud platforms grow, they present challenges for site reliability engineers (SREs) who must detect and diagnose faults in increasingly large and complex systems. These platforms need to maintain service level agreements (SLAs) with customers, as service outages can have serious financial repercussions.

To address this, effective online anomaly detection algorithms are needed that can process data streams continuously, update models in real-time to ensure accurate detections, and adapt to concept drift—significant changes in data distribution—to avoid false alarms.

Cloud platform metrics, such as CPU usage, system health, memory, block storage, and network, are key indicators of system state and varied across different platforms.

StreamAD is a tool that employs machine learning for real-time metric monitoring, analyzing data from message queues, and alerting SREs to anomalies through a dashboard.

Benchmarks like TODS provide automated machine learning systems for anomaly detection with diverse datasets and algorithms, whereas benchmarks such as NAB focus on online anomaly detection but are limited to univariate data and not maintained. Exathlon concentrates on time series anomaly detection with a spark for explainable results, while UTSD offers visual interfaces for univariate series. 

XStream is an algorithm for evolving data streams, and other methods such as RSHash, HSTree, and LODA offer different approaches to ranking and scoring anomalies.

A range of datasets, such as AIOps_KPI, Micro, and GAIA, provides diverse data for anomaly detection evaluation.

StreamAD evaluates detection methods using both point-aware and series-aware evaluations to consider precision and recall, accommodating different types of anomalies and data distributions.

After initializing algorithms with a portion of the data, benchmarking continues with different hyperparameter settings according to the respective algorithm.

Selecting the best algorithm for a particular use case remains a challenge, and while the benchmark suggests using SPOT for univariate and XStream for multivariate streams, user-specific trials are still necessary. 

The construction of benchmarks like StreamAD is ongoing, with plans to incorporate state-of-the-art methods and broader evaluation criteria, including the effect of hyperparameters and the interpretability of algorithms. 

Finally, the proposed StreamAD benchmark facilitates unsupervised online anomaly detection for cloud metrics with eleven detection algorithms tested across five public datasets. It assesses the effectiveness, efficiency, and memory resource consumption of these methods and provides a user-friendly API for SREs, supporting both practical applications and future research.