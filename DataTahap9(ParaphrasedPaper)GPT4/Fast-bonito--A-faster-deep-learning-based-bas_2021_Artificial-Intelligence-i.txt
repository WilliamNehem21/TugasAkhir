The core architecture of Bonito, a neural network designed for decoding raw electronic DNA sequencing data, draws inspiration from QuartzNet, which was initially crafted for speech recognition tasks. However, concerns arose about QuartzNet's suitability for the specific task of translating electronic signals to DNA bases. The team investigated the Bonito architecture for potential performance bottlenecks. Bonito's structure contains a series of TCSConv-BN-ReLU modules, with each consisting of time-channel separable convolutions (TCSConv), batch normalization (BN), and a ReLU activation function. While this design significantly cuts down the model's parameter count, suboptimal hardware inference library writing impairs its speed somewhat.

In the neural architecture search (NAS) process, the controller module's configuration is shaped by the search space. The controller is trained through a cyclic process of sampling, evaluating, and updating. During each iteration, a batch of models is generated by the controller, which are then trained for a set number of epochs. Training is followed by measuring each model's inference latency and accuracy. A multiobjective reward based on these two metrics is calculated, fed into the controller, and used to adjust its parameters by maximizing the anticipated reward.

For the research, Bonito version 0.2.3 was utilized. Additionally, an improved version called Fast-Bonito was developed, employing the identical training and validation datasets as Bonito. Bonito is continuously being enhanced with new updates, and the team plans to incorporate these advancements into Fast-Bonito moving forward.