This paper reviews the latest advancements in global optimization, emphasizing the dynamism and breadth of contemporary computational techniques and theoretical insights into nonconvex optimization. The authors provide an extensive collection of recent studies they believe to represent the field well. They begin by tracing the origins of global optimization, highlighting what aspects have persisted or been neglected and discussing methods that have been rediscovered, particularly in the context of machine learning.

The paper is structured into several sections. Section 3 focuses on various heuristic computational methods that have been published since the authors' previous book in 2013 and considers this work a continuation. Section 4 surveys literature on structured optimization problems amenable to exact solution methodologies. Section 5 touches on computational considerations, recommending resources for comprehensive listings of test problems and solving tools, and concludes with some remarks.

One notable discussion involves the use of modified basin hopping (BH) techniques in chemical physics, where global optimization (GO) methods are often combined with machine learning to guide the search process. For instance, machine learning can help to simplify atomic clustering problems, enabling more effective GO algorithms.

The authors mention the paper by Kawaguchi (2016), which addresses a conjecture regarding the absence of non-global local optima in training deep neural networks with quadratic loss functions. This could shed light on why local optimization algorithms often find global solutions in neural network training.

The paper references a work by Bomze and de Klerk (2002), which presents a polynomial time approximation scheme (PTAS) for certain optimization problems, demonstrating that while some problems may not have efficient exact solutions, there may be efficient approximation methods.

Branch & Cut (B&C) methodology developments are also covered, focusing on improvements not typically applied to standard operations like upper bounding and fathoming. Instead, they explore novel fathoming rules based on optimality conditions.

Different geometrical forms have been recommended for underestimating and overestimating functions in optimization problems, tied to linear sum-of-ratios issues. The authors cite use of rectangles, triangles, and trapezoids for creating tight estimators in relevant optimization scenarios.

The paper highlights methods based on interval arithmetic for addressing poorly structured global optimization problems, acknowledging the value of such approaches, despite their limited number in recent decades.

There is a brief mention of mixed socp-sdp relaxations to strike a balance between solution quality and calculation time, and an alternative approach to representing sos polynomials that may be more cost-effective.

The authors note that no single optimization method excels for every problem type, making it crucial to choose an appropriate method based on the problem's characteristics.

The paper concludes by acknowledging that while it does not delve into computational experiences or compare different solvers, other publications and resources provide such analyses. It also suggests that heuristic approaches may fare best with molecular conformation problems due to the inherent complexity of precisely characterizing these systems.

Overall, the paper surveys the recent literature to assess the state-of-the-art in global optimization, offering a comprehensive resource on heuristics, exact methods, and influences of fields like machine learning on current GO strategies.