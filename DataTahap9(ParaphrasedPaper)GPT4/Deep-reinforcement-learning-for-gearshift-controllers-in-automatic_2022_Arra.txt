Automated systems for controlling gear shifts have been developed as alternatives to the manual and time-consuming road tests where engineers adjust control parameters to achieve desired shift characteristics. Various measurable criteria, like the vibration dose value (VDV) and the amplitude and root mean squared (RMS) of longitudinal acceleration, facilitate subjective shift quality assessments. Utilizing these criteria, different methods for automating the calibration process have emerged, including the use of fuzzy logic, evolutionary algorithms, and gradient-based optimization techniques.

Sommer Obando worked on reducing clutch judder by applying Q-learning and SARSA algorithms to manage the normal force on a friction clutch. However, the research had limitations due to an oversimplified control model that did not account for certain complexities found in real-world automotive transmissions, such as nonlinear actuator dynamics and partial observability. Their approach also showed performance degradation when transitioning from simulations to real-world experiments.

Lampe et al. employed a deep deterministic policy gradient (DDPG) agent in a simplified simulation to learn policies for clutch engagement in automated manual and dual-clutch transmissions. Nonetheless, their study lacked validation in a physical vehicle or transmission system.

Typically, gear shift control in vehicles is managed using an open-loop control system with an underlying PI-controller that adapts based on prior shifts to achieve the desired clutch slip speed trajectory. The gear shift process involves a filling phase, where no measurable variable changes occur until the piston contacts the friction plates, and a synchronization phase, where the controller uses feedback to manage the actuation current based on observed deviations, thus controlling the slip speed and minimizing jerks. This conventional strategy has limitations, like a constrained actuation range and the inability to preempt errors, as it reacts only to existing ones.

The paper introduces a control task for gear shifts that aims to be addressed through deep reinforcement learning (DRL) algorithms, especially Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). SAC, which is an off-policy actor-critic algorithm, uses a soft MDP formulation that prioritizes both expected return and policy entropy, benefitting from various DRL concepts such as replay buffers and target networks.

The phases of gear shifting are defined based on expert knowledge, with robust thresholds in place to overcome sensory noise. The learning agents are designed to be resistant to variation and noise, using standard simulations to become robust through domain adaptation, ensuring effectiveness across different operation conditions.

The paper acknowledges the contributions of individuals like Bernd Frauenknecht, who worked on PPO implementation, as well as the overall support from management and colleagues, emphasizing the collaborative effort in advancing the physics simulator and transmission control techniques.