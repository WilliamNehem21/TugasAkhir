Gesture-based communication is a vital means of interaction, particularly for people with hearing impairments. Sign language is a natural form of communication widely used by individuals with speech disabilities, including roughly 1% of the Indian population. Understanding Indian Sign Language (ISL) therefore has significant potential for enhancing these individuals' lives. This paper introduces a technique for ISL recognition using a Bag of Visual Words model with background subtraction and Speeded Up Robust Features (SURF) for feature extraction. Sign images are represented as histograms and matched to labels using classifiers like Support Vector Machine (SVM) and Convolutional Neural Networks (CNN). An interactive Graphical User Interface (GUI) simplifies system access.

Effective communication, encompassing our unique methods of interaction, is a universal human need. The paper notes that ISL was standardized in 2003, which sparked research interest. ISL is complex, with variations across regions and sign types, posing challenges for creating a uniform recognition system. Although deep learning advancements have improved image recognition capabilities, custom datasets and algorithms are necessary for high accuracy in video detection.

The authors faced the challenge of no standard ISL datasets and decided to build a custom dataset. During data collection, images were captured using two methods: one for still backgrounds using skin segmentation and HSV color space, and another for dynamic backgrounds using frame differencing and grayscale conversion. Hand segmentation and noise removal were addressed to prepare the data.

For feature quantization, the authors adopted the Bag of Visual Words model, while an SVM with a linear kernel was used for classification. CNNs provided enhanced image classification due to their structure. An accuracy of 99% was achieved for ISL static alphabets and digits, surpassing previous models' capabilities.

The paper describes the development of a real-time recognition system including a custom dataset, rotation invariance, and independence from a controlled background. Moving forward, the authors propose expanding the dataset to include more signs from various languages, enhancing response times, and supporting the formation of simple words and expressions for both isolated and continuous recognition tasks. The paper indicates that further development of real-time applications is crucial, highlighting the importance of quick and accurate sign recognition systems.