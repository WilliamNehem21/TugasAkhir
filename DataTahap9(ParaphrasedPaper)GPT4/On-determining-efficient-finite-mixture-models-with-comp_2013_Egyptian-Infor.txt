In this paper, an innovative algorithm named EMCE (Efficient Model with Compact and Essential components) is introduced for clustering data using finite mixture models (FMMs). This algorithm identifies the optimal FMM based on a new selection criterion, which is designed to select a model that is both efficient -- in terms of complexity -- and composed of compact, essential components with minimal mutual information and overlap.

The study compares the EMCE algorithm's performance against other established methods, demonstrating its superiority, particularly with small and sparse datasets and those with overlapping clusters. Notably, the EMCE algorithm suffers less from the curse of dimensionality, as evidenced by its lessened sensitivity to the number of features within a dataset.

Conventional approaches like BIC and MML criteria often overestimate when clusters are non-Gaussian or when the data is sparse and tend to underestimate in the face of overlap or a small number of feature vectors. The EMCE, on the other hand, is robust to these issues by focusing on the intrinsic properties of the input data set, rather than being heavily influenced by the dimensionality of the data.

The paper details the execution and testing of the EMCE algorithm using MATLAB software on various datasets, with a focus on both initialization and convergence within the learning process. Quantification of the clustering results' accuracy is also addressed, employing normalized mutual information (NMI) as the primary metric.

The EMCE algorithm achieves the highest NMI values and accurately determines the number of mixture components across the tested datasets. The overall findings suggest that the EMCE algorithm offers a more effective framework for model estimation and selection in data clustering compared to existing methods, reinforcing its applicability to small, sparse, or overlapping datasets.