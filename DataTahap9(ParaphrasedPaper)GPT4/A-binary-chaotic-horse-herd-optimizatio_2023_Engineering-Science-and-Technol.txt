This academic paper presents a specialized algorithm, the Horse Herd Algorithm (HOA), which has been adapted to operate effectively on large datasets. The adaptation involved transforming the HOA into a binary format suitable for a binary search space, after which the new Binary Horse Herd Algorithm (BHOA) was tailored to address the feature selection problem using a wrapper method (BHOAFS). The process involved an extensive review of literature to develop a binary variant of the HOA suitable for feature selection tasks.

The aim of the study was twofold: to maximize classification accuracy and to minimize the number of features selected. Multiple metrics were considered to evaluate the algorithm's performance, including average fitness, accuracy, number of features chosen, and computational processing time.

The paper is structured with Section 2 outlining the theoretical underpinnings and the detailed description of the Binary BHOAFS method. Section 3 presents the chaotic versions of the BHOAFS tailored for feature selection, namely the Binary Chaotic BCHOAFS methods. Section 4 delves into experimental results and discussions, and Section 5 wraps up with the studyâ€™s conclusions.

The BHOA adapts an animal behavior-based ranking system for the selection of 'horses' within the algorithm, wherein the top 10% are classified as 'A' horses while the next 20%, 30%, and 40% are categorized as 'B', 'C', and 'D' horses respectively based on their fitness values.

Since the original HOA operates in a continuous space and the feature selection problem exists in a discrete space, a U-shaped transfer function was used to convert continuous values to binary so the HOA could be applied to the discrete feature selection space.

In the proposed algorithm, a Specific Matching Function (SMF) is designed for local search strategy, which helps to improve individuals' positions by calculating the similarity of candidate solutions and generating potentially better positions for the next iteration.

Random population generation for the feature selection (FS) process follows the Pareto principle, setting the initial binary vectors to 80% zeros and 20% ones. Datasets get randomly split into training (90%) and testing (10%) sets. Classifiers such as k-NN and SVM are used, and cross-validation is employed to validate results with the fitness function considering both classification accuracy and the number of features selected.

The robustness of results is ensured by repeatedly dividing datasets into training and test subsets and applying various statistical measures.

Upon comparing results based on chaotic maps, it was found that piecewise and singer chaotic maps were the most effective for accuracy and feature selection. These maps were integrated into BCHOAFS3 and BCHOAFS4 versions of the algorithm and surpassed other chaotic maps like logistic, tent, and sinusoidal versions in performance.

A comprehensive experimental evaluation using 18 standard UCI datasets of various sizes and characteristics was conducted to assess the performance of the BHOAFS and various chaotic BCHOAFS versions. These were compared against both traditional optimization algorithms and binary optimization counterparts. Among the various algorithms tested, BCHOAFS3 and BCHOAFS4 (BCHOAFSpiecewise and BCHOAFSsinger) were found to deliver the best performance in terms of classification accuracy and feature selection.