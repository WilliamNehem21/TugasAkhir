Cloud computing services have been available from various vendors over an extended period, delivering on-demand infrastructure with a variety of virtual machine configurations, yet typically without explicit performance metrics. Benchmarking these virtual machine instances allows for a comparison of the diverse services offered by distinct providers. This paper introduces a software tool designed for the automated gathering of performance data from cloud infrastructure, enabling an evaluation of service cost-effectiveness.

The underlying aim is to uncover actual performance variations amongst cloud service offerings, transcending reliance on advertised hardware specifications or past performance data. More accurate predictions about job completion times could minimize waiting periods for resources and optimize the scheduling of time-sensitive tasks. Moreover, precise estimations might allow for efficient job queuing within the allocated instance time frames. To achieve this, a tool was developed to efficiently conduct performance assessments automatically on cloud instances from various suppliers. It manages the complete testing cycle, starting from instance deployment to result retrieval, and ensures the termination of instances post-testing to prevent unnecessary costs. Initially, the focus is placed on assessing CPU and memory performance, with future plans to integrate additional benchmark suites. Unlike other solutions, some of which are not publically available or are tailored for specific use cases, this tool is offered as an open-source option for broader accessibility.

Using this tool, substantial performance differences were noted across instance types offered by cloud providers like Microsoft Azure and Amazon Web Services. Unexpected discrepancies in cost-effectiveness were observed, especially in instances where higher prices did not necessarily equate to better performance.

The testing method involves deploying a fresh VM, running the benchmark suite (initially, SPECjvm2008 was chosen), capturing the results, and subsequently disassembling the cloud service to avoid further costs. Python's Paramiko package is employed for SSH connections, enabling remote execution of benchmarks within the VM, as well as for file transfer post-testing.

Instances within Azure's basic A-series demonstrated nonlinear scaling in performance relative to their hardware enhancements, suggesting potential anomalies in the expected performance gains. AWS T2 instances showed variable performance based on the availability of CPU credits, which permit temporary performance boosts.

Multi-core cloud instances can be advantageous even for single-threaded workloads by allocating tasks across multiple cores, assuming the performance-to-cost ratio justifies such deployment. The study also found that performance consistency varies amongst instances, making it difficult to predict an instance's efficiency based on initial test runs. Consequently, extended testing or higher tier instances might yield more reliable performance metrics.

Ultimately, this research seeks to offer a benchmarking tool to facilitate informed decisions for cloud resource allocation, with aspirations to integrate a broader spectrum of benchmarking utilities. Despite initial limitations in supported benchmarks, the objective is to provide a versatile, open-source tool for the community to better assess and compare cloud performance against service costs.