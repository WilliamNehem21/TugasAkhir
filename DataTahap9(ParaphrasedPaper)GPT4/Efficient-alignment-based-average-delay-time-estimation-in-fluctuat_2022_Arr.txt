In this paper, the authors introduce a new algorithm intended to estimate the average delay between two time series where the delays can change over time. Unlike typical methods that only consider a fixed number of minimum cost alignments, leading to exponential computational effort relative to the series length, the new algorithm comprehensively incorporates every such alignment. The algorithm achieves this efficiency by operating in linear time with respect to the number of nodes in the graph for minimum cost alignment—potentially up to the square of the time series length. The paper demonstrates the algorithm’s superior performance over basic recursive enumeration methods through numerical experiments.

Time delay estimation is crucial in various scientific and engineering domains, including sonar, radar, seismology, and geophysics. Traditional studies in these areas frequently assume a constant delay and use cross-correlation techniques for estimations. But recent improvements have focused on more sophisticated models that offer improved spatial predictions.

The proposed algorithm computes an average delay across all points where the time series align using the concept of minimum-cost alignments, referred to as the mean time delay by minimum-cost alignments. Directly enumerating all these alignments to calculate delay time is infeasible, as the number can grow exponentially with the length of the time series.

The detailed method for calculating the warp-based cost, which applies to representative examples, is thoroughly discussed in the paper, along with the algorithm’s time and space complexities. Numerical experiments demonstrate the proposed method's effectiveness compared with the basic approach. The paper concludes with a summary and potential future research directions. Additionally, an appendix provides clarification on calculating gap-based costs, which slightly differ from the main cost calculations discussed in the paper.