This paper investigates the use of black box testing techniques in which testers are not privy to the internal workings of the System Under Test (SUT). It explores the utility of simple coverage metrics to identify errors, specifically focusing on measuring the coverage of the system's specified behavior rather than its internal state coverage.

The proposed method is called exploration testing, an automated process wherein a test engine explores a deterministic Labelled Transition System (LTS) representing the expected behavior of the system. The test engine navigates through the LTS and decides when to send inputs, listen for outputs, or reset the system throughout the testing phase, restricted to the actions available in the current LTS state.

The paper aims to introduce and evaluate heuristic algorithms designed to quickly uncover errors and describes how one can create specification coverage-aided test selection algorithms by separating the problem into parts. It compares these algorithms by applying them to conference protocol systems with varying numbers of entities, analyzing the number of test steps necessary to find an error.

Previously tested at the University of Twente, both correct and mutated versions of protocol entities were used, with this study distinguishing itself by testing the collaborative service of multiple clients to showcase the dynamics of concurrent and reactive systems.

One testing approach highlighted is the transition tour method, which endeavors to trigger every transition in the LTS at least once. This paper elaborates on this method, applying additional coverage criteria to direct test runs.

The paper is structured as follows: Section 2 formalizes LTSs and coverage criteria. Section 3 describes the test engine architecture. Section 4 delves into heuristic algorithms, with Section 5 presenting the test setup and comparison results.

Three coverage classes, 111, 001, and 010, are used to explain how to achieve LTS coverage: 111 requires executing every transition, 001 focuses on reaching every state, and 010 ensures that every action in the LTS is performed at least once.

The paper explains the test engine's handling of successful inputs, resulting in transition executions within the LTS, while refused input actions are processed if possible. Outputs are verified against the current LTS state, and any illegal responses prompt the cessation of testing.

It introduces algorithms with playful, game-like strategies where the test engine decides on inputs or waits for outputs, aiming to maximize a score based on transition values.

The brief overview gives a snapshot of featured strategies, including random heuristics, pessimistic and adaptive player algorithms, each with unique approaches to estimating probabilities and decision making.

The paper specifies the reliable delivery of messages within the conference system and discusses testing against different mutated client process versions, identifying the mutants and the types of errors they produced.

Finally, the paper concludes that heuristic algorithms incorporating the 010 coverage criterion and certain evaluation functions outperformed simpler algorithms, although some algorithms still displayed suboptimal performance.