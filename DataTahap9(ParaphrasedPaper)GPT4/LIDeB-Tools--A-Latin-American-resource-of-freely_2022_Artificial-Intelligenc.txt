Cheminformatics employs information technology to manage and process chemical data, a practice deeply embedded in the chemical sciences for many years, but the term became prevalent with the growth of drug discovery data. Core tasks in cheminformatics include managing chemical databases, storing and retrieving information, analyzing structure-property relationships, conducting in silico screening, and designing libraries.

Data science and machine learning have given rise to platforms like KNIME, Pipeline Pilot, and Alteryx that allow users with minimal coding skills to build data analysis pipelines. However, most advanced functions or third-party applications require commercial licenses, limiting access for smaller research groups, especially in developing nations.

The LIDEB tools suite is accessible as web apps on Streamlit, offering a user-friendly interface and cloud-based computational resources, or as customizable standalone Python applications using open-source libraries. These tools can integrate into existing cheminformatics workflows.

For molecular clustering, molecules are inputted in SMILES format via a CSV file. After optional standardization, 1613 molecular descriptors are calculated using MolVS and Mordred, or users can provide their own descriptors. The embedded UMAP algorithm reduces the high-dimensional fingerprint space, and the GMM algorithm clusters molecules in this space, guided by the silhouette score to estimate optimal cluster numbers. CHICA is re-executed with chosen parameters to assign molecules to clusters.

When selecting chemically diverse molecules, if less than 400 molecules are found, property ranges expand by 1.5 times, up to five increases. A preliminary list of decoys is formed with a Physicochemical Similarity Score (PSS) based on six properties. The quality of decoys is assessed using the Deviation from Optimum Embedding (DOE) score and Doppelganger score to ensure physicochemical match and minimize latent active compounds.

For protein classification, 70% of the proteins are used for model training, with the rest for external validation. A clustering strategy employs PCA for dimensionality reduction followed by k-means clustering. Randomization tests check model robustness by scrambling class labels and retraining, expecting lower accuracy from randomized models compared to real ones.