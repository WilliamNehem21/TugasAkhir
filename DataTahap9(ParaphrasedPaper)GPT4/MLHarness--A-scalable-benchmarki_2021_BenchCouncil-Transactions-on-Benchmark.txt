Innovations in machine learning (ML) and deep learning (DL) are rapidly emerging, encompassing diverse applications, datasets, frameworks, models, software, and hardware systems. However, traditional methods of sharing these innovations often involve ad-hoc scripting and manual workflow descriptions, leading to challenges in reproducing results and adapting innovations to various environments.

To address these issues, a standard benchmarking platform like MLCommons (formerly MLPerf) is crucial, as it can provide a uniform ground for fair comparison and evaluation of ML/DL innovations. Focusing particularly on MLCommons Inference—which measures system performance in processing inputs and delivering outputs using a trained model—this paper underscores the platform's capacity to evaluate systems across scales, from mobile devices to data centers.

MLCommons Inference not only assesses traditional model metrics like accuracy but also system-related metrics, such as percentile-latency and throughput. A critical part of MLCommons Inference is the LoadGen, which generates queries for the Inference System Under Test (SUT). This SUT encompasses various ML/DL frameworks, models, software libraries, and hardware systems.

The paper introduces a modified MLCommons Inference platform, named MLHarness, which is augmented with MLModelScope. MLModelScope is an inference platform providing a clear exchange specification and tool for cross-stack profiling and analysis. MLHarness, as a scalable benchmarking system, extends the applicability of MLCommons Inference and supports models beyond its original scope.

While MLModelScope initially supported only computer vision tasks, this paper presented implementations of user-defined pre-processing and post-processing capabilities, thus enabling support for different types of models, such as question answering and medical 3D image segmentation models.

To facilitate efficient data transfer between Go and Python within MLHarness, the researchers proposed in-memory copying techniques. They addressed data type discrepancies and garbage collection synchronization issues to ensure data integrity and efficient processing.

The experimental results reported in the paper demonstrated MLHarness's capabilities to benchmark models according to MLCommons Inference standards for different applications. Not only were various MLCommons Inference scenarios (such as server and multistream) supported, but MLHarness could also successfully import and accurately assess models from outside the MLCommons suite.

MLHarness's extended exchange specifications enable researchers to easily incorporate their innovations while collecting reliable metrics. Looking ahead, the authors aim to expand MLHarness to support MLCommons training benchmarks.

The paper concludes by emphasizing the crucial role of a standardized benchmarking harness system like MLHarness for the flourishing ML/DL community. The system's flexibility and scalability promote reproducibility and portability, contributing to a broader impact that could potentially extend to other computing research fields beyond ML/DL.