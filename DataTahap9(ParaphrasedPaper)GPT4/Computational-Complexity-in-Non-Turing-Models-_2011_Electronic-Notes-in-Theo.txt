The paper suggests that, like conventional algorithms, the system in question has a maximum capacity for the size of numbers it can factor. However, unlike traditional algorithms, the computational time does not increase when the number size nears this maximum capacity. The limitation on the system's factorization ability arises not from the typical computational considerations like processing time or memory constraints, but from the increased level of precision required from the system's user.

Furthermore, the authors argue for a broader perspective in evaluating computational resources. They note that in addition to the usual metrics of time and space used in algorithmic evaluation, other factors, such as the precision required in physical systems, should also be taken into account.

To enhance these limits, it is suggested to explore as many solution methods as possible for a given problem. However, in practice, the methods considered typically come from a singular computation model (often the Turing machine), due to the challenge in comparing complexity across different computational models. This reliance on a single model is seen as an unfortunate necessity because of our current limitations in contrasting the complexities from diverse computation models.