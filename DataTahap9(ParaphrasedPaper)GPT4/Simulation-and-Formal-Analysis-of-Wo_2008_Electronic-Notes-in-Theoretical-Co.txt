Historically, data was recorded on paper, which when used in operational processes can be considered a token that passes through various stages of work activity. This document or "work item" is the substance on which colleagues perform their tasks, composing the entirety of the workflow, which dictates the sequence in which these tasks should be completed.

Dataflow networks are a conceptual model designed to represent intricate, distributed computing environments, providing clear-cut meanings and sitting in complexity between the BPEL and PROMELA verification models. This methodology marries the state-oriented perspectives of finite state machines with the data (or token) movement inherent in Petri nets, while additionally facilitating the simulation of faults by incorporating extra rules and node states.

A workflow execution engine can organize the efforts of numerous individuals and interfaces with assorted computer systems across different organizations. These systems are only loosely interconnected and their operation can't be guaranteed, necessitating the integration of redundancy into these workflows to ensure their reliability.

In our research, we introduced a technique for verifying the correctness of workflows structured in BPEL. We employ dataflow networks to articulate the formal operational definitions of these workflows, transforming the BPEL framework into a dataflow network, which is in turn translated into a PROMELA model for validation purposes.