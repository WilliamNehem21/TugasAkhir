In this paper, we introduce a stochastic perspective on Lambek categorial grammars, which have proven useful in the field of natural language processing. The central idea arises from the observation that parsing in Lambek calculus relies on an inherently non-deterministic algorithm. We suggest that adopting a probabilistic approach could effectively address this non-determinism.

We explain how, in Lambek calculus, a constituent of type B on the right side of the turnstile does not need to be directly associated with an axiom link's left conclusion. This concept contrasts with traditional assumptions and is demonstrated with a proof-net example from an earlier section of our text, where the word "lit" is given the type (NP \ N / S) = NP. Here, the second occurrence of NP is not the left, but the right conclusion of an axiom link.

We then turn our attention to discussing the automaton's stack content as described in Section 4, particularly just before it processes the word "le." Depending on the scenario, the stack might contain either [NP] or [S NP?], displaying the unresolved dependencies up to that point. This stack content can help predict the appropriate type for the word "le," and our method has the advantage of accommodating long-distance dependencies, a feature not typically supported by taggers that rely on n-gram models or Hidden Markov Models.