In this paper, we continue to use the concept of soft margin consistency in semi-supervised settings, leading to the creation of a new approach called Semi-Supervised Soft Margin Consistency based Multi-View Maximum Entropy Discrimination (SSMVMED). However, the challenge arises in circumstances where there are a limited number of labeled data points and an abundance of unlabeled data. Since labeled data typically provides more valuable information for classification tasks, but is often scarce and costly to obtain in practical scenarios, the effectiveness of semi-supervised learning algorithms is heavily dependent on the dataset composition.

A common strategy to mitigate this problem is to generate synthetic unlabeled data from existing labeled and unlabeled data, which then inherits some of the discriminative characteristics of the original dataset. Universum learning is one method that employs this technique. Despite its utility, this approach faces two main issues: first, the varying importance of different views and features is often ignored when creating the universum set; second, the traditional methods for generating additional unlabeled instances mainly rely on either labeled or unlabeled data, rather than utilizing both.

To address the first issue, we implement Weighted Multi-View Clustering (WMVC) to better understand the contribution of each view and feature. For the second issue, we develop strategies that utilize both labeled and unlabeled instances in generating new unlabeled data.

SMVMED differs from other related methods, such as MVmed and AMVmed, due to its incorporation of a soft margin. It ensures margin consistency by minimizing the Kullback-Leibler divergence between the margin parameter posteriors from multiple views, and it includes a balancing parameter that allows for flexibility between achieving a large margin and maintaining margin consistency.

We demonstrate the application of our method using diverse datasets: (1) A course dataset for identifying course web pages, (2) Citeseer and Cora datasets with multiple views, focusing on content and citation views, (3) WebKB dataset with web pages from four different universities categorized into five types, and (4) a subset of the 20-newsgroup dataset.

We employ statistical tests, specifically the paired t-test and the Nemenyi statistical test, to assess the significance of performance differences between learning machines. The paired t-test examines differences on individual datasets, while the Nemenyi test compares performance across multiple datasets. We prioritize evaluating test accuracy in our analysis.

Lastly, we discuss the theoretical computational complexity of SSMVMED, breaking it down into its constituent steps and considering the number of labeled, original unlabeled, and additional unlabeled instances.