Array 11 (2021) 100074

Contents lists available at ScienceDirect

Array

journal homepage: www.elsevier.com/journals/array/2590-0056/open-access-journal

Wild Geese Algorithm: A novel algorithm for large scale optimization based
on the natural life and death of wild geese

Mojtaba Ghasemi a, Abolfazl Rahimnejad b, Rasul Hemmati c, Ebrahim Akbari d,
S. Andrew Gadsden b, *

a Department of Electronics and Electrical Engineering, Shiraz University of Technology, Shiraz, Iran
b Department of Engineering Systems and Computing, University of Guelph, Guelph, Canada
c Department of Electrical and Computer Engineering, Marquette University, Milwaukee, USA
d Department of Electrical Engineering, Faculty of Engineering, University of Isfahan, Isfahan, Iran

A R T I C L E I N F O

A B S T R A C T

Keywords:
Large-scale global optimization (LSGO)
Wild Geese Algorithm (WGA)
Swarm-based method
Engineering optimization

In numerous real-life applications, nature-inspired population-based search algorithms have been applied to solve
numerical optimization problems. This paper focuses on a simple and powerful swarm optimizer, named Wild
Geese Algorithm (WGA), for large-scale global optimization whose efﬁciency and performance are veriﬁed using
large-scale test functions of IEEE CEC 2008 and CEC 2010 special sessions with high dimensions D ¼ 100, 500,
1000. WGA is inspired by wild geese in nature and models various aspects of their life such as evolution, regular
cooperative migration, and fatality. The effectiveness of WGA for ﬁnding the global optimal solutions of high-
dimensional optimization problems is compared with that of other methods reported in the previous literature.
Experimental results show that the proposed WGA has an efﬁcient performance in solving a range of large-scale
optimization problems, making it highly competitive among other large-scale optimization algorithms despite its
simpler structure and easier implementation. The source code of the proposed WGA algorithm is publicly
available at github.com/ebrahimakbary/WGA.

1. Introduction

Many practical optimization problems, which are called Large Scale
Global Optimization (LSGO) problems, deal with a lot of decision vari-
ables. Some practical LSGO problems are large-scale electronic systems
design, scheduling problems, vehicle routing in large-scale trafﬁc net-
works, and inverse problem chemical kinetics. Many real-world optimi-
zation problems involve optimization of a large number of control
variables with various constraints. However, the classical mathematical
programming methods do not generally provide good solutions for
different optimization problems with different real-world complexities,
due to the huge size of the problems [1]. The global optimization per-
formance of the population-based algorithms often becomes weaker in
such problems with increasing the dimension and complexity of the
problem [2–4]. The practical large-scale optimization problems have
been modeled with different benchmark test functions such as those
presented in the CEC 2008 [5] and CEC 2010 [6].

Recently, many nature-inspired and population-based meta-heuristic
optimization algorithms have been presented to deal with LSGO

* Corresponding author.

E-mail address: gadsden@uoguelph.ca (S.A. Gadsden).

problems with different real-world complexities such as nonlinearity,
non-smoothness,
non-
non-convexity, mixed-integer
differentiability, etc. Some new nature-inspired optimization algo-
rithms for solving the practical large-scale optimization problems are
listed in Table 1. It should be mentioned that, the boldface rows of this
table, show the methods which were used in the comparative study with
the proposed WGA.

nature,

Wild geese have a long-distance, coordinated and organized travel,
which can be used as an inspiration for a very appropriate optimization
algorithm for high-dimension problems. Based on the general model of
wild geese’ lives, a novel algorithm called Wild Geese Algorithm (WGA)
is introduced in this paper, which have some main prominent charac-
teristics compared to the previous algorithms including:

(cid:2) It is simple with low computational burden, and its implementation is

easily performed.

(cid:2) It has proper and satisfactory power for different test functions, from

different groups.

https://doi.org/10.1016/j.array.2021.100074
Received 23 December 2020; Received in revised form 1 May 2021; Accepted 16 June 2021
Available online 25 June 2021
2590-0056/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).

M. Ghasemi et al.

Table 1
Summary of some new nature-inspired optimization algorithms for solving the
practical large-scale optimization problems.

Table 1 (continued )

Ref.

Year

Abbreviation

Short Description

Ref.

Year

Abbreviation

Short Description

[7]

2008 MLCC

[8]

2008

EPUS-PSO

[9]

2008

sep-CMA-ES

[10]

2010

SOUPDE

[11]

2010

CCVIL

[12]

2010

(cid:2)DECC-D
(cid:2)DECC-DML

[13]

2010

GOBL

[14]

2011

TSVP

[15]

2011

SP-UCI

[16]

2012

LMDEa

[17]

2012

DE-CCS

[2]

2012

CCPSO2

[18]

2012

LSCBO

Multilevel
Cooperative
Coevolution
Efﬁcient Population
Utilization Strategy
for Particle Swarm
Optimizer (PSO)
Covariance Matrix
Adaptation
Evolution Strategy
having diagonal
covariance matrix
Shufﬂe or update
parallel differential
evolution
Cooperative
Coevolution with
Variable Interaction
Learning
(cid:2)Differential
Evolution with
Cooperative Co-
evolution using
Delta-Grouping
(cid:2)Differential
Evolution with
Multilevel
Cooperative Co-
evolution using
Delta-Grouping
Generalized
Opposition-Based
Learning
Tabu Search with
Variable Partitioning
Shufﬂed complex
evolution with
principal
components
analysis–University
of California at Irvine
Differential
Evolution with
Landscape Modality
Detection and a
Diversity Archive
Differential
Evolution Algorithm
with Cooperative
Coevolutionary
Selection Operator
A new Cooperative
Coevolving Particle
Swarm
Optimization with a
new position
update rule based
on Cauchy and
Gaussian
distributions
Large Scale
Optimization Based
on Co-ordinated
Bacterial Dynamics
and Opposite
Numbers

Dimensions
under study

Real-
world
problem

100, 500,
1000

100, 500,
1000

No

No

100–1000

No

50, 100,
200, 500,
100
1000

100, 500,
1000

50, 100,
200, 500,
100
100, 400,
1000
10, 50, 100,
1000

No

No

No

No

No

No

1000

No

500,1000

No

[19]

2013

GOjDE

[20]

2013

EOEA

[21]

2014

FT-DNPSO

[22]

2014

CBCC1-DG
CBCC2-DG
DECC-DG

[23]

2015

CDE

[24]

2015

CSO

[25]

2016

SOMAQI

[26]

2018

MWOA

[27]

2019

EHO

[28]
[29]

2019
2019

SFO
PRO

[30]

2019

EBA

1000

No

[31]

2019

EO

[32]

2020

NPO

[33]

2020

ISSA

A Generalized
Opposition based
Differential
Evolution enhanced
with a self-adapting
parameter tuning
strategy
A two-stage based
ensemble
optimization
evolutionary
algorithm
PSO with dynamic
neighborhood based
on kernel fuzzy
clustering and
variable trust region
methods
Two different
versions of
Contribution Based
Cooperative Co-
evolution and
Differential
Evolution with
Cooperative Co-
evolution, all with
differential
grouping
Continuous
Differential
Evolution
A Competitive
Swarm Optimizer

Self Organizing
Migrating Algorithm
with Quadratic
Interpolation
A Modiﬁed Whale
Optimization
Algorithm
Enhanced Elephant
Herding
Optimization with
Novel Individual
Updating Strategies
Sailﬁsh Optimizer
Poor and rich
optimization
algorithm
Ensemble Bat
Algorithm
Equilibrium
optimizer
Nomadic People
Optimizer
An improved Social
Spider Algorithm

Array 11 (2021) 100074

Dimensions
under study

100, 200,
500, 1000

Real-
world
problem

No

1000

Yes

30, 100,
1000

No

1000

No

200, 500,
1000

100, 500,
1000, 2000,
5000
100, 500,
1000, 2000,
3000

100, 300,
500, 1000

50, 100,
200, 500,
100

300
300

100, 500,
1000
10–200

100, 500,
2000
100, 500,
1000

No

No

No

No

No

Yes
Yes

No

Yes

No

No

100, 500,
1000

No

It is worth mentioning that although the proposed WGA may seem
similar to PSO, especially due to the existence of personal best and global
best concepts,
it has some thorough distinctions of structure and
formulation, the main of which can be listed as follows:

2

M. Ghasemi et al.

Array 11 (2021) 100074

1 In WGA, all solutions are sorted based on their objective values so that
each member of population moves using information from its adja-
cent members in the sorted population.

2 In the proposed method, the formulation for calculating the velocity
of each goose is completely different from the PSO and is based on the
positions, velocities, and best positions of the goose and its adjacent
geese in the sorted population, as well as the global best solution's
position. While in PSO, the only parameter that is shared among all
solutions is the position of the global best solution.

3 In the proposed WGA, two different solutions are generated per so-
lution and are used for creating the next iteration's goose based on a
mechanism similar to the crossover operator of differential evolution.
4 Finally, in the proposed algorithm, a population reduction policy is
implemented which is accomplished by fatality (elimination) of the
weakest goose of the population.

The rest of this paper is organized as follows. Section 2 presents the
new proposed algorithm for large-scale optimization problems. Section 3
shows
the
conclusions.

results. Finally, Section 4 presents

the experimental

2. The proposed algorithm: Wild Geese Algorithm

In recent years, some new algorithm inspired from group movement
and group search by animals have been proposed for large-scale global
continuous optimization [1]. In this paper, based on the different phases
of wild geese's lives, including their rhythmic and coordinated group
migration, reproduction and evolution and also deaths in the population
of geese, a new efﬁcient algorithm, named as Wild Geese Algorithm
(WGA), is presented for high-dimensional optimization problems. In
Fig. 1, a group ordered migration based on the position of wild geese is
shown. In general, the proposed WGA phases are as follows:

1 Ordered and coordinated group migration (or migration and

displacement velocity phase)

2 Walking and searching for food by wild geese.
3 Reproduction and evolution of wild geese.
4 Death, migration and ordered evolution of wild geese.

First, an initial population of wild geese are created, so that the po-
sition vector of the i-th wild goose is equal to xi. The best local position or
personal best solution pi and migration velocity viare determined. Then,
all wild geese populations are sorted from the best to the worst according
to their target function.

In this modeling strategy, each wild goose exploits information from
its adjacent wild geese in the ordered population, and is directed by those
individuals. The phases of WGA are further discussed in the subsequent
subsections.

2.1. An ordered and coordinated group migration (or migration and
displacement velocity phase)

As it is observed in Fig. 1, migration of wild geese is a group, coor-
dinated, ordered and under control migration, which is based on reach-
ing the upfront and adjacent individuals in the sorted population.
Velocity and displacement equations according to the coordinated ve-
locity of the geese are given in Eq. (1) and Eq. (2).

vIterþ1
i;d

¼

(cid:1)
r1;d (cid:3) vIter
i;d
(cid:1)

(cid:1)

(cid:3)(cid:3)

þ r2;d (cid:3)
(cid:3)

(cid:4) vIter
vIter
iþ1;d
i(cid:4)1;d
(cid:1)

(cid:3)

þr3;d (cid:3)

(cid:1)

pIter
i;d

(cid:4) xIter
i(cid:4)1;d

þ r4;d (cid:3)
(cid:3)

pIter
iþ1;d
(cid:1)

(cid:4) xIter
i;d

(cid:3)

þr5;d (cid:3)

pIter
iþ2;d

(cid:4) xIter
iþ1;d

(cid:4) r6;d (cid:3)

pIter
i(cid:4)1;d

(cid:4) xIter
iþ2;d

Fig. 1. An ordered and coordinated migration of wild geese.

where xi;d, pi;d, and vi;d are the dth dimension of the current position, the
best position, and the current velocity of the ith wild goose, respectively.
Note that in this study, rk;d; k ¼ 1; 2; :::; 11 are uniformly distributed
random numbers between 0 and 1.

As observed in Eq. (1), the velocity and position changes of each wild
goose (for instance i-th wild goose) depend on the velocities of their
Þ , and also to the positions of its
upfront and rear members, i.e ð vIter
iþ1
adjacent members.

(cid:4)vIter
i(cid:4)1

According to the model from the migration of wild geese in Fig. 2 and
Eq. (1), the wild geese use information from their adjacent individuals in
the sorted population, as patterns for their movement and navigation,
→
and tend to reach those members (reduce their distances), i.e. xIter
i(cid:4)1
pIter
i

; xIter
i
Additionally, the global best member is used as another guide for the
movements of the whole ﬂock; which is reﬂected in Eq. (2). This position
change is carried out in an ordered form and coordinated with the
upfront members in order to model the movement of all members as an
ordered series, as shown in Figs. 1 and 2.

iþ2, and xIter
iþ2

→ (cid:4) pIter
i(cid:4)1.

→ pIter
iþ1

; xIter
iþ1

→ pIter

xv
i;d

¼ pIter
i;d

þ r7;d (cid:3) r8;d (cid:3)

gIter
d

þ pIter
iþ1;d

(cid:4) 2 (cid:3) pIter
i;d

þ vIterþ1
i;d

(2)

(cid:1)(cid:1)

(cid:3)

(cid:3)

where gd is the global best position among all members.

2.2. Walking and searching for food by wild geese

This step is modeled in such a way that the i-th wild goose moves
towards its upfront member, i.e. the (iþ1)-th goose (pIter
iþ1 ). In
another word, the i-th goose tries to reach the (iþ1)-th goose (pIter
(cid:4) pIter
).
iþ1
i
The equation for walking and searching for food by the wild goose, xW
is
i
as follows:

→ pIter

i

xw
i;d

¼ pIter
i;d

þ r9;d (cid:3) r10;d (cid:3)

pIter
iþ1;d

(cid:4) pIter
i;d

(cid:1)

(cid:3)

(3)

2.3. Reproduction and evolution of wild geese

Another stage of wild geese's life is reproduction and evolution. In this
paper, its modeling is performed so that a combination between migra-
tion equation (xV
i ) is used.
The Cr value for the proposed WGA algorithm is 0.5 in total simulations.

i ) and walking and search for food equation (xW

(1)

(

xIterþ1
i;d

¼

if

r11;d (cid:5) Cr

otherwise:

xv
i;d
xw
i;d

(4)

3

M. Ghasemi et al.

Array 11 (2021) 100074

Fig. 2. The model of ordered and coordinated group migration of wild geese.

2.4. Death, migration and ordered evolution

Algorithm 1 (continued )

The previous experiments from the literature show that for different
optimization algorithms the population number and the iteration number
do not have the same level of inﬂuence on solving every types of prob-
lems. For some functions, the size of algorithm's population is more
important and more effective than the number of algorithm's iterations
(e.g. F2 and F3 functions), and for some other functions the number of
algorithm's iterations is more important and more effective than the size
of WGA algorithm's population (e.g. F7 and F8 functions). In this paper,
to overcome this problem and establish a compromised solution, the
death phase is employed in order to balance algorithm performance for
all test functions. In this phase, the algorithm starts with the maximum
population number Npinitial and during the algorithm iterations, the
weaker members will be removed from the population based on Eq. (5)
and the population size will decrease linearly so that it reaches its ﬁnal
value Npfinal in the ﬁnal iteration.

0

Np ¼ round

B
@

Npinitial
(cid:4)
(cid:5)

(cid:4)

Npinitial (cid:4) Npfinal

*

(cid:4)

(cid:6)

FEs
FEsmax

1

C
A

(cid:7)(cid:7)

(5)

where FEs and FEsmax are the number of function evaluations and its
maximum.

Algorithm 1
Demonstrates the optimization process of WGA.

Algorithm 1:
1: to set values of the control parameters of WGA;
2: to generate the initial population (whose number are equal to Npinitial) and V Iter¼1

¼

i

½0(cid:6);

3: to evaluate the ﬁtness of each population individual and FEs ¼ Npinitial;
4: to ﬁnd the personal best position of all particles Npinitial (i ¼ 1, 2, …, Npinitial) in

swarm Piand the global best position G;

5: while the FEs till FEsmax do

6: Wild Goose populations are arranged from the best to the worst according to
Fig. 2;
7: for i ¼ 1 (best) to Np (worst) do

8: Select the sorted members i (cid:4) 1th; i þ 1th; and i þ 2th;
{** An ordered and coordinated group migration based on Eq. (1) and Eq. (2) **}
9: for d ¼ 1 to D do

← Eq. (1);

10: V Iterþ1
i
11: end for
12: for d ¼ 1 to D do

← Eq. (2);

13: xV
i;d
14: end for
{** Walking and search geese Eq. (3) **}
15: for d ¼ 1 to D do

← Eq. (3);

16: xW
i;d
17: end for
{** Reproduction and evolution Eq. (4) **}
18: for d ¼ 1 to D do

19: XIterþ1
i
20: end for
21: if xIterþ1
i;d
22: xIterþ1
i;d

23: end if
24: if xIterþ1
i;d
25: xIterþ1
i;d

← Eq. (4);

< xmin
d
← xmin
d

;

> xmax
d
← xmax
d

;

26: end if
27: to evaluate the ﬁtness of XIterþ1
28: if f ðXIterþ1
Þ (cid:5) f ðPIter
i
← XIterþ1
29: PIterþ1
;
i
i

Þ

i

i

30: end if
31: if f ðPIterþ1

i
32: G ← PIterþ1

Þ (cid:5) f ðGÞ
;

i

33: end if
34: end for
35: FEs ¼ FEs þ Np;
36: Np ← Eq. (5);

37: end while

3. Results and analysis of experimental evaluation studies

In this section, 20 widely used large scale test functions are exploited
to show the efﬁciency and performance of the proposed algorithm. The
formulation and characteristics of all CEC 2010 benchmark test functions
are listed in Ref. [6].

The performance and robustness of WGA for solving real and large-
scale optimization problems are characterized by two indices: 1) the
mean of best values of test function (Mean), and 2) the standard deviation
(Std) indices.

Test functions include 1. Separable functions (F1 (cid:4) F3), 2. Single-
group m-nonseparable functions (F4 (cid:4) F8), 3. D
2m-group m-nonseparable
functions (F9 (cid:4) F13), 4. D
m-group m-nonseparable functions (F14 (cid:4) F18),
and 5. non-separable functions (F19 (cid:4) F20), where m is the number of
variables in each non-separable subcomponent, and D and m are assumed
as 1000 and 50, respectively. To show the efﬁciency of WGA, in all
simulations of this paper, 25 independent simulations are used in each
section for every test function, as in Refs. [6,22]. Furthermore, in all
simulations, the maximum number of ﬁtness evaluations FEsmax is 3 (cid:3)
106. In all tables, the þ sign means the algorithm outperforms WGA, the
– sign means WGA outperforms the algorithm, and the ¼ sign means
WGA and the considered algorithm yield the same solution for the given
problem. It should be mentioned that, in all results tables, the boldface is
used to emphasize the algorithm that achieves the best Mean index value
for each problem.

3.1. Experimental setup

(continued on next column)

At ﬁrst, to show the performance of the population reduction by death

3.1.1. Inﬂuence of death phase on WGA performance

4

M. Ghasemi et al.

Array 11 (2021) 100074

Table 2
Average ﬁtness values and standard deviations of results for test functions over
25 independent runs.

F

F1

F2

F3

F4

F5

F6

F7

F8

F9

F10

F11

F12

F13

F14

F15

F16

F17

F18

F19

F20

Nb/Nw/Mr

WGA, Np ¼ 30

WGA, Np ¼ 120

WGA

1.68E-21
7.71E-22
3
7.78Eþ03
7.95Eþ01
3
1.00Eþ01
1.25Eþ01
3
3.81E þ 11
1.63E þ 11
1
9.55Eþ07
7.04Eþ06
3
1.98Eþ01
2.50E-02
3
8.01E-02
2.00E-02
1
8.60E þ 06
3.16E þ 05
1
2.54Eþ07
1.33Eþ06
2
4.67Eþ03
1.60Eþ02
3
8.94Eþ01
7.77Eþ00
3
1.62E þ 03
1.30E þ 02
1
9.11Eþ02
1.93Eþ02
2
7.51E þ 07
5.36E þ 06
1
5.28Eþ03
3.79Eþ02
3
2.69Eþ02
1.37Eþ01
3
1.41E þ 04
6.23E þ 02
1
2.11Eþ03
1.47Eþ03
2
8.73E þ 05
1.03E þ 05
1
1.58Eþ03
7.71Eþ01
3
7/10/2.15

2.33E-24
1.58E-24
2
2.18E þ 03
1.14E þ 01
1
1.17E-13
7.40E-15
1
9.99Eþ11
1.05Eþ11
3
5.74Eþ07
3.68Eþ06
2
3.56E-09
1.40E-15
2
4.47Eþ03
1.69Eþ03
3
4.30Eþ07
2.74Eþ07
3
4.55Eþ07
5.50Eþ06
3
1.76E þ 03
2.48E þ 01
1
2.34E-13
1.07E-14
1
3.25Eþ04
1.40Eþ03
3
9.87Eþ02
1.50Eþ02
3
1.52Eþ08
1.24Eþ07
3
4.21Eþ03
1.01Eþ02
2
7.63Eþ00
2.95Eþ00
2
1.47Eþ05
7.77Eþ03
3
4.15Eþ03
1.56Eþ03
3
1.35Eþ06
5.17Eþ04
3
1.15Eþ03
2.42Eþ01
2
4/10/2.3

1.05E-26
2.56E-26
1
2.28Eþ03
4.58Eþ01
2
1.47E-13
8.94E-15
2
5.15Eþ11
7.89Eþ10
2
5.47E þ 07
7.93E þ 06
1
3.55E-09
5.48E-14
1
4.60Eþ00
6.28Eþ00
2
9.16Eþ06
8.79Eþ06
2
2.21E þ 07
1.51E þ 06
1
2.64Eþ03
2.70Eþ01
2
3.06E-13
5.48E-14
2
4.15Eþ03
2.40Eþ02
2
6.87E þ 02
2.63E þ 01
1
7.67Eþ07
4.55Eþ06
2
3.14E þ 03
5.42E þ 01
1
3.79E þ 00
6.26E-01
1
3.74Eþ04
1.36Eþ02
2
1.52E þ 03
2.93E þ 02
1
1.04Eþ06
2.85Eþ04
2
1.04E þ 03
8.18E þ 01
1
10/0/1.55

of Wild Geese, WGA is tested without considering the death phase and is
tested with a large population Np ¼ 120 and a small population Np ¼ 30.
The suitable results were compared with those of WGA (considering
population reduction from Np ¼ 120 (Npinitial¼120) to Np ¼ 30
(Npfinal¼30) using Eq. (5), where the results obtained for each function
are listed in Table 2. The results demonstrate that the proposed death
phase improves the efﬁciency of WGA for high-dimensional problems.
The positive inﬂuence of death phase can be especially observed for test

5

functions F3, F6, F7, F11, F12, F16, and F17. Moreover, the convergence
characteristics of this algorithm for 6 different functions of various types
are depicted in Fig. 3, which verify the effectiveness of implementing
death phase in WGA.

3.1.2. Why Cr ¼ 0.5 in WGA for all test functions?

In this paper Cr ¼ 0.5 is used for all simulations. To select a suitable
value for Cr four different constant values other than 0.5, i.e. 0.1, 0.25,
0.75 and 0.9 are tested, whose results are presented in Table 3. As
observed, the constant value 0.5 is the best value for different test
functions of CEC 2010. It should be mentioned that in all simulation
results tables, three values are reported for optimizing each test function
with each algorithm; the ﬁrst two demonstrate the average and standard
deviation of ﬁtness values of the obtained results. The third value shows
the rank of that algorithm in terms of the mean index. Furthermore, three
parameters are reported for each algorithm in all tables, i.e. Nb, Nw, Mr.
Nb and Nw are the number of times the algorithm yields the best and the
worst mean index, respectively; and Mr is the average rank of the algo-
rithm achieved in solving all considered test functions.

3.2. Comparing WGA with recent optimization algorithms

3.2.1. CEC 2008 test functions

In this section, the results of WGA are compared with those of a series
of the recently proposed optimization algorithms for large-scale prob-
lems from CEC 2008 test functions with different high dimensions
including D ¼ 100, D ¼ 500 and D ¼ 1000. The formulation and char-
acteristics of CEC008 benchmark test functions are listed in Ref. [5] and
Table 4:

Two indices are exploited in this study to characterize the perfor-
mance and robustness of WGA for solving real and large-scale optimi-
zation problems with different dimensions: 1) mean of the best values of
test function (Mean), and 2) standard deviation (Std). Tables 5–7 show
the ﬁnal best solutions of test functions’ optimization by WGA and those
of large scale optimization algorithms including CSO [24], CCPSO2 [2],
sep-CMA-ES [9], MLCC [7], and EPUS-PSO [8]. As seen, the proposed
WGA is able to provide very efﬁcient and competitive results in solving
real and large-scale problems compared with the previously proposed
algorithms. WGA proves itself as a promising technique for real and large
scale shifted unimodal and multimodal optimization problems.

3.2.2. CEC 2010 test functions

As mentioned in the introduction section of this paper, numerous
researches have been recently performed to achieve some algorithms and
meta-heuristic optimization methods for high-dimension optimization
problems. These studies and many other methods have been introduced
to ﬁnd a simple and quick method with the low computational burden. In
Table 8, the results of previous researches for 20 different test functions
of CEC 2010 with D ¼ 1000 are summarized [22], which was obtained
with the same conditions as those of WGA. The summarized algorithms in
Table 8 include MLCC algorithm [7], differential evolution with coop-
erative co-evolution and delta grouping DECC-D and DECC-DML [12],
contribution based cooperative co-evolution and differential grouping
CBCC1-DG and CBCC2-DG [22], differential evolution with cooperative
co-evolution and random grouping (DECC-DG) [22]. The last two rows of
Table 8 present the comparative indices for these algorithms.

The WGA algorithm has achieved the best results in 12 of 20 func-
tions, i.e. F4, F5, F6, F7, F9, F10, F13, F14, F15, F17, F18, and F19. In
addition, for none of the test functions WGA has the worst results.
Moreover, WGA reaches the best average rank (Mr). The proposed al-
gorithm (WGA) outperforms MLCC algorithm in 18 out of 20 functions;
only for the ﬁrst two functions MLCC algorithm performs better. For the
ﬁrst function the average value of WGA is very close to that of MLCC
algorithm. MLCC algorithm has different results for different test func-
tions and has the worst results for 6 out of 20 functions. However, the
proposed algorithm has acceptable and suitable results for most of the

M. Ghasemi et al.

Array 11 (2021) 100074

Fig. 3. Average convergence of WGA on nine selected test functions over 25 independent runs.

test functions and dispersion of its results are less than those of the other
algorithms. The comparison between WGA and DECC-D algorithm shows
that WGA performs better for 18 out of 20 functions. Nonetheless, for
functions F2 and F20, it gives a worse result compared to that of DECC-D.
For function F2, the average value of WGA is very close to that obtained
from DECC-D algorithm. Furthermore, DECC-D algorithm does not pro-
vide a good quality solution for different test functions, for example for
F2 and F20 it has suitable results, but for F5 (cid:4) F8, F10 (cid:4) F12, and F15(cid:4)
F17 its results are not acceptable compared to those of other algorithms.
Although DECC-DML algorithm outperforms WGA for ﬁve test functions,
it has the worst solution for six functions. CBCC1-DG and CBCC2-DG
algorithms are more successful than WGA for two and three functions,
respectively; however, CBCC1-DG gives the best result for none of the
functions and CBCC2-DG yields the best result for only function. DECC-
DG algorithm performs better than WGA for 2 out of 20 test functions;
however, it gives the worst solution for 4 test functions among all
algorithms.

3.2.3. Test on real-world optimization problems

Here, the effectiveness of the proposed algorithm (WGA) was inves-
tigated compared to genetic algorithm (GL-25) [34], DE with strategy
adaptation (SaDE) [35], DE with control components and composite trial
vector generation approaches (CoDE) [36], Standard particle swarm
optimization (SPSO2013) [37], and heterogeneous comprehensive
learning PSO with improved exploitation and exploration (HCLPSO) [38]
on
for
frequency-modulated (FM) sound waves [39] and large-scale reliabili-
ty-redundancy allocation optimization (RRAO) of a gas turbine [40].

estimating

real-world

including

usages

factor

the

1) Estimating the factor for frequency modulated sound waves

The greatly complex multimodal frequency-modulated (FM) sound
synthesis optimizing problem plays a key role in various modern music
systems for estimating the optimal factors of a FM sound wave synthesis
[39]. The estimation of optimal factors of an FM sound wave synthesis is

6

M. Ghasemi et al.

Array 11 (2021) 100074

Table 3
Average ﬁtness values and standard deviations on test functions over 25 inde-
pendent runs.

sound waves for t deﬁned in range of 1–100 are as follows [42]:

yðtÞ ¼ x1 sinðx2tθ þ x3 sinðx4tθ þ x5 sinðx6tθÞÞÞ;

F

F1

F4

F9

F14

F20

(cid:4)/þ/ ¼
Nb/Nw/Mr

Cr ¼ 0.1

Cr ¼ 0.25

Cr ¼ 0.75

Cr ¼ 0.9

Cr ¼ 0.5

3.12Eþ07
7.41Eþ07
3,–
1.17Eþ12
7.40Eþ11
3,–
1.39Eþ10
8.22Eþ09
4,–
3.04Eþ10
2.61Eþ10
4,–
1.50Eþ10
4.20Eþ09
3,–
5/0/0
0/0/3.4

3.77E-06
1.63E-07
2,–
7.26Eþ11
9.64Eþ10
2,–
9.04Eþ07
2.76Eþ08
2,–
2.71Eþ09
1.26Eþ09
2,–
1.03Eþ03
5.15Eþ01
1,þ
4/1/0
1/0/1.8

5.24Eþ09
1.03Eþ09
4,–
4.96Eþ13
8.22Eþ13
4,–
1.03Eþ10
7.50Eþ09
3,–
3.19Eþ09
4.54Eþ09
3,–
1.17Eþ11
3.73Eþ09
4,–
5/0/0
0/0/3.6

5.00Eþ10
2.29Eþ10
5,–
2.48Eþ14
5.31Eþ13
5,–
6.71Eþ10
2.56Eþ10
5,–
1.55Eþ11
1.23Eþ11
5,–
6.53Eþ11
3.90Eþ09
5,–
5/0/0
0/0/5

1.05E-26
2.56E-26
1
5.15Eþ11
7.89Eþ10
1
2.21Eþ07
1.51Eþ06
1
7.67Eþ07
4.55Eþ06
1
1.04Eþ03
8.18Eþ01
2
–

4/0/1.2

Table 4
Summary of CEC 08 Special Session benchmark test functions [5] for large scale
global optimization.

Function

Name

Properties

f1

f2

f3

f4

f5

f6

Shifted Sphere

Shifted
Schwefel's
Shifted
Rosenbrock's
Shifted
Rastrigin's
Shifted
Griewank's
Shifted Ackley's

Unimodal, separable,
scalable
Unimodal, non-
separable, scalable
Multimodal, non-
separable, scalable
Multimodal,
separable, scalable
Multimodal, non-
separable, scalable
Multimodal,
separable, scalable

Search
space

[-100,
100]
[-100,
100]
[-100,
100]
[-5, 5]

[-600,
600]
[-32, 32]

Global
optimum

0

0

0

0

0

0

an optimization problem with D decision variables. In this work, the case
of D ¼ 6 is only considered in accordance with [41,42]. Six components
are included in the 6-dimensional parameter vector as x ¼ [x1(a1),
x2(ω1), x3(a2), x4(ω2), x5(a3), x6(ω3)] ranging between 6.35 and 6.5 for
all variables. The equations provided for the target and approximated

(6)

(7)

y0ðtÞ ¼ 1:0*sinð0:5tθ (cid:4) 1:5 * sinð4:8tθ þ 2:0 * sinð4:9tθÞÞÞ;

where θ ¼ 2π
100

The optimization problem objective function is considered as the sum
of squared errors between yðtÞ(the approximated wave) and y0ðtÞ (the
target wave) with optimal value f(x) ¼ 0 as follows:

f ðxÞ ¼

X100

t¼0

ðyðtÞ (cid:4) y0ðtÞÞ2:

2) RRAO constrained problem:

(8)

The nonlinear

reliability-redundancy constrained optimization
problems are mainly aimed at enhancing the system reliability (maxi-
mizing the overall system reliability) through optimizing element re-
liabilities vector (r ¼ (r1, r2, …, rm)) and redundancy assignment numbers
vector (n¼ (n1, n2, …, nm)) for subsystems of the system. It is possible to
formulate this problem as a nonlinear mixed-integer programming model
by choosing the system reliability as the objective function to be maxi-
mized subjecting to several nonlinear constraints as follows [40]:

Maximize Rs ¼ f ðr; nÞ;

subject to gðr; nÞ (cid:5) l;
0 (cid:5) rd (cid:5) 1; nd 2 Zþ; 0 (cid:5) d (cid:5) m:

(9)

(10)

þ

where Z
is the set of positive integers, Rs represents the reliability of
various systems, f(.) and g(.) denote for the objective and constraint
functions of RRAO problem for the total parallel-series systems, respec-
tively, from which g(.) is usually related to the system cost, weight and
volume. n¼ (n1, n2, …, nm) and r ¼ (r1, r2, …, rm) show the redundancy
allocation numbers and component reliabilities vectors for system's
subsystems including m subsystems, respectively. Moreover, l shows the
limit of the system resources.

The overspeed detection was continually offered by the mechanical
and electrical systems. By occurring an overspeed, the fuel source must
be stopped through control valves (V1 to Vm). Fig. 4 represents a gas
turbine's overspeed protection system for RRAO optimizing the mixed-
integer non-linear problem. The large-scale test structure involves 40

Table 5
Results obtained by optimization algorithms for dimension 100 over 25 independent runs.

F

F1

F2

F3

F4

F5

F6

(cid:4)/þ/ ¼
Nb/Nw/Mr

D ¼ 100

CCPSO2 [2]

CSO [24]

sep-CMA-ES [9]

MLCC [7]

EPUS-PSO [8]

ISSA [33]

EO [31]

WGA

7.73E-14
3.23E-14
6,-
6.08Eþ00
7.83Eþ00
2,-
4.23Eþ02
8.65Eþ02
7,-
3.98E-02
1.99E-01
2,þ
3.45E-03
4.88E-03
4,-
1.44E-13
3.06E-14
4,-
5/1/0
0/0/4.167

9.11E-29
1.10E-28
2,-
3.35Eþ01
5.38Eþ00
6,-
3.90Eþ02
5.53Eþ02
6,-
5.60Eþ01
7.48Eþ00
4,þ
0
0
1, ¼
1.20E-014
1.52E-015
1,þ
3/2/1
2/0/3.333

9.02E-15
5.53E-15
4,-
2.31Eþ01
1.39Eþ01
4,-
4.31E þ 00
1.26E þ 01
1, þ
2.78Eþ02
3.43Eþ01
6,-
2.96E-04
1.48E-03
3,-
2.12Eþ01
4.02E-01
8,-
5/1/0
1/1/4.333

7.47E-01
1.70E-01
7,-
1.86Eþ01
2.26Eþ0
3,-
4.99Eþ03
5.35Eþ03
8,-
4.71Eþ02
5.94Eþ01
7,-
3.72E-01
5.60E-02
6,-
2.06Eþ00
4.40E-01
6,-
6/0/0
0/4/6.167

6.82E-14
2.32E-14
5,-
2.53Eþ01
8.73Eþ00
5,-
1.50Eþ02
5.72Eþ01
4,-
4.39E-13
9.21E-14
1,þ
3.41E-14
1.16E-14
2,-
1.11E-13
7.87E-15
3,-
5/1/0
1/0/3.333

7

0
0
1, ¼
8.31Eþ01
1.91 Eþ01
8,-
1.68Eþ02
9.46Eþ01
5,-
5.00Eþ00
6.60Eþ00
3,þ
0
0
1, ¼
2.09Eþ01
2.99E-02
7,-
3/1/2
2/1/4.167

1.31E-20
5.01E-20
3.-
4.29Eþ01
3.69Eþ00
7,-
9.21Eþ01
8.97Eþ01
2,þ
6.04Eþ02
8.52Eþ01
8,-
9.58E-02
1.02E-01
5,-
2.05Eþ01
1.73E-01
5,-
5/1/0
0/1/5

0
0
1
2.14E-05
3.08E-05
1
1.04Eþ02
4.01Eþ01
3
1.25Eþ02
1.41Eþ01
5
0
0
1
1.39E-014
1.23E-015
2
–

3/0/2.333

M. Ghasemi et al.

Array 11 (2021) 100074

Table 6
Results obtained by optimization algorithms for dimension 500 over 25 independent runs.

F

F1

F2

F3

F4

F5

F6

(cid:4)/þ/ ¼
Nb/Nw/Mr

D ¼ 500

CCPSO2 [2]

CSO [24]

sep-CMA-ES [9]

MLCC [7]

EPUS-PSO [8]

ISSA [33]

EO [31]

WGA

3.00E-13
7.96E-14
5,-
5.79Eþ01
4.21Eþ01
4,-
7.24Eþ02
1.54Eþ02
6,-
3.98E-02
1.99E-01
2,þ
1.18E-03
4.61E-03
5,-
5.34E-13
8.61E-14
3,-
5/1/0
0/0/4.167

6.57E-23
3.90E-24
3,-
2.60E þ 01
2.40E þ 00
1,þ
5.74Eþ02
1.67Eþ02
4,-
3.19Eþ02
2.16Eþ01
4,-
2.22E-16
0.00E þ 00
1,þ
4.13E-13
1.10E-14
2,-
4/2/0
2/0/2.5

2.25E-14
6.10E-15
4,-
2.12Eþ02
1.74Eþ01
7,-
2.93E þ 02
3.59E þ 01
1,þ
2.18Eþ03
1.51Eþ02
6,-
7.88E-04
2.82E-03
4,-
2.15Eþ01
3.10E-01
7,-
5/1/0
1/2/4.833

4.30E-13
3.31E-14
6,-
6.67Eþ01
5.70Eþ00
5,-
9.25Eþ02
1.73Eþ02
7,-
1.79E-11
6.31E-11
1,þ
2.13E-13
2.48E-14
3,-
5.34E-13
7.01E-14
3,-
5/1/0
1/1/4.167

8.45Eþ01
6.40Eþ00
8,-
4.35Eþ01
5.51E-01
2,þ
5.77Eþ04
8.04Eþ03
5,-
3.49Eþ03
1.12Eþ02
7,-
1.64Eþ00
4.69E-02
8,-
6.64Eþ00
4.49E-01
4,-
5/1/0
0/3/5.667

9.90E-28
9.95E-28
2,-
2.66Eþ02
1.92Eþ01
8,-
8.31Eþ14
3.11Eþ14
8,-
2.07Eþ03
5.38Eþ02
5,-
4.48E-02
1.29E-01
6,-
2.14Eþ01
1.70E-02
6,-
6/0/0
0/2/5.833

4.14E-04
3.87E-04
7,-
9.34Eþ01
3.01E-01
6,-
1.95Eþ03
1.04Eþ03
3,-
3.78Eþ03
1.46Eþ02
8,-
2.42E-01
6.11E-01
7,-
2.06Eþ01
3.35E-01
5,-
6/0/0
0/1/6

0.00E þ 00
0.00E þ 00
1
5.73Eþ01
8.72Eþ00
3
5.22Eþ02
3.60Eþ01
2
1.25Eþ02
1.41Eþ01
3
4.12E-16
5.36E-17
2
5.77E-14
1.58E-15
1,þ
–

2/0/2

Table 7
Results obtained by optimization algorithms for dimension D ¼ 1000 over 25 independent runs.

F

F1

F2

F3

F4

F5

F6

(cid:4)/þ/ ¼
Nb/Nw/Mr

D ¼ 1000

CCPSO2 [2]

CSO [24]

sep-CMA-ES [9]

MLCC [7]

EPUS-PSO [8]

ISSA [33]

EO [31]

WGA

5.18E-13
9.61E-14
5,-
7.82Eþ01
4.25Eþ01
4,-
1.33Eþ03
2.63Eþ02
4,-
1.99E-01
4.06E-01
2,þ
1.18E-03
3.27E-03
5,-
1.02E-12
1.68E-13
2,-
5/1/0
0/0/3.667

1.09E-21
4.20E-23
2,-
4.15E þ 01
9.74E-01
1,þ
1.01Eþ03
3.02Eþ01
3,-
6.89Eþ02
3.10Eþ01
3,þ
2.26E-16
2.18E-17
1,þ
1.21E-12
2.64E-14
4,-
3/3/0
2/0/2.33

7.81E-15
1.52E-15
4,-
3.65Eþ02
9.02Eþ00
8,-
9.10E þ 02
4.54E þ 01
1,þ
5.31Eþ03
2.48Eþ02
5,-
3.94E-04
1.97E-03
4,-
2.15Eþ01
3.19E-01
5,-
5/1/0
1/1/4.5

8.46E-13
5.01E-14
6,-
1.09Eþ02
4.75Eþ00
5,-
1.80Eþ03
1.58Eþ02
5,-
1.37E-10
3.37E-10
1,þ
4.18E-13
2.78E-14
3,-
1.06E-12
7.68E-14
3,-
5/1/0
1/0/3.833

5.53Eþ02
2.86Eþ01
7,-
4.66Eþ01
4.00E-01
2,þ
8.37Eþ05
1.52Eþ05
6,-
7.58Eþ03
1.51Eþ02
6,-
5.89Eþ00
3.91E-01
7,-
1.89Eþ01
2.49Eþ00
6,-
5/1/0
0/0/5.667

2.09E-18
3.95E-18
3,-
3.10Eþ02
1.38Eþ01
7,-
2.17Eþ15
6.89Eþ13
8,-
1.49Eþ04
1.93Eþ03
8,-
3.10E-01
4.51E-01
6,-
2.15Eþ01
7.70E-03
8,-
6/0/0
0/3/6.667

1.35Eþ04
6.94Eþ03
8,-
1.64Eþ02
1.16Eþ02
6,-
2.58Eþ09
2.63Eþ09
7,-
7.79Eþ03
1.01Eþ02
7,-
4.07Eþ01
5.39Eþ01
8,-
2.05Eþ01
1.40E-01
7,-
6/0/0
0/2/7.167

1.75E-28
1.27E-28
1
7.43Eþ01
4.89Eþ00
3
1.00Eþ03
8.25Eþ01
2
2.52Eþ03
1.34Eþ02
4
1.22E-15
2.91E-16
2
1.21E-13
5.18E-15
1
–

2/0/2.167

decision variables (m*2 ¼ 40). The input factors and data for the large-
scale test system are provided in Ref. [43] with 20 subsystems.

It is possible to formulate this reliability optimization problem as:

Maximize f5ðr; nÞ ¼

Ym

½1 (cid:4) ð1 (cid:4) rdÞnd (cid:6):

represents the upper volume limit of the products of the subsystem.

2) The system cost limitationg2ðr; nÞ:

g2ðr; nÞ ¼

Xm

(cid:8)
nd þ e0:25nd
CðrdÞ

(cid:9)

(cid:5) C;

(cid:5)
1 (cid:4) 10(cid:4)6

(cid:6)
d¼1

0:5 (cid:5) rd (cid:5)
1 (cid:5) nd (cid:5) 10; 2 Zþ:

; 0 (cid:5) d (cid:5) m

The system constraints include:

(11)

d¼1
(cid:4)
(cid:4) T
ln rd

(cid:7)β

d

:

CðrdÞ ¼ αd

(13)

1) The combined weight, volume, and redundancy allocation con-

straintg1ðr; nÞ:

g1ðr; nÞ ¼

Xm

d¼1

dn2
v2

d

(cid:5) V

where vd shows the volume of dth subsystem for all components and V

where, C shows the upper cost limit of the system, CðrdÞ is the cost for all
element with reliability rd at dth stage, and T is the operating time in
which the components are working.

(12)

3) The system weight limitationg3ðr; nÞ:

g3ðr; nÞ ¼

Xm

d¼1

wdnde0:25nd (cid:5) W

8

(14)

M. Ghasemi et al.

Array 11 (2021) 100074

Table 8
Average ﬁtness values and standard deviations on CEC 2010 functions over 25 independent runs.

F

F1

F2

F3

F4

F5

F6

F7

F8

F9

F10

F11

F12

F13

F14

F15

F16

F17

F18

F19

F20

(cid:4)/þ/ ¼
Nb/Nw/Mr

MLCC [7]

DECC-D [12]

DECC-DML [12]

CBCC1-DG [22]

CBCC2-DG [22]

DECC-DG [22]

WGA

1.53E-27
7.66E-27
1,þ
5.57E-01
2.21E þ 00
1,þ
9.88E-13
3.70E-12
4,-
9.61Eþ12
3.43Eþ12
7,-
3.84Eþ08
6.93Eþ07
6,-
1.62Eþ07
4.97Eþ06
6,-
6.89Eþ05
7.37Eþ05
5,-
4.38Eþ07
3.45Eþ07
7,-
1.23Eþ08
1.33Eþ07
7,-
3.43Eþ03
8.72Eþ02
2,-
1.98Eþ02
6.98E-01
6,-
3.49Eþ04
4.92Eþ03
5,-
2.08Eþ03
7.27Eþ02
4,-
3.16Eþ08
2.77Eþ07
4,-
7.11Eþ03
1.34Eþ03
4,-
3.76Eþ02
4.71Eþ01
7,-
1.59Eþ05
1.43Eþ04
5,-
7.09Eþ03
4.77Eþ03
4-
1.36Eþ06
7.35Eþ04
2,-
2.05Eþ03
1.80Eþ02
3,-
18/2/0
2/6/4.5

1.01E-24
1.40E-25
4,-
2.99Eþ02
1.92Eþ01
3,þ
1.81E-13
6.68E-15
3,-
3.99Eþ12
1.30Eþ12
5,-
4.16Eþ08
1.01Eþ08
7,-
1.36Eþ07
9.20Eþ06
5,-
6.58Eþ07
4.06Eþ07
6,-
5.39Eþ07
2.93Eþ07
6,-
6.19Eþ07
6.43Eþ06
4,-
1.16Eþ04
2.68Eþ03
6,-
4.76Eþ01
9.53Eþ01
5,-
1.53Eþ05
1.23Eþ04
6,-
9.87Eþ02
2.41Eþ02
2,-
1.98Eþ08
1.45Eþ07
3,-
1.53Eþ04
3.92Eþ02
5,-
1.88Eþ02
2.16Eþ02
6,-
9.03Eþ05
5.28Eþ04
6,-
2.12Eþ03
5.18Eþ02
2,-
1.33Eþ07
1.05Eþ06
4,-
9.91E þ 02
2.61E þ 01
1,þ
18/2/0
1/1/4.45

1.93E-25
1.86E-25
3,-
2.17Eþ02
2.98Eþ01
2,þ
1.18E-13
8.22E-15
1,þ
3.58Eþ12
1.54Eþ12
4,-
2.98Eþ08
9.31Eþ07
5,-
7.93Eþ05
3.97Eþ06
4,-
1.39Eþ08
7.72Eþ07
7,-
3.46Eþ07
3.56Eþ07
5.-
5.92Eþ07
4.71Eþ06
2,-
1.25Eþ04
2.66Eþ02
7,-
1.80E-13
9.88E-15
1,þ
3.79Eþ06
1.50Eþ05
7,-
1.14Eþ03
4.31Eþ02
3,-
1.89Eþ08
1.49Eþ07
2,-
1.54Eþ04
3.59Eþ02
6,-
5.08E-02
2.54E-01
4,þ
6.54Eþ06
4.63Eþ05
7,-
2.47Eþ03
1.18Eþ03
3,-
1.59Eþ07
1.72Eþ06
5,-
9.91E þ 02
3.51E þ 01
1,þ
15/5/0
3/6/3.95

1.32Eþ04
6.25Eþ04
7,-
4.44Eþ03
1.60Eþ02
6,-
1.66Eþ01
3.79E-01
5,-
2.31Eþ12
7.43Eþ11
2,-
1.35Eþ08
2.18Eþ07
2,-
1.65Eþ01
3.99E-01
3,-
1.81Eþ04
4.59Eþ04
4,-
3.34Eþ06
2.29Eþ06
2,þ
6.79Eþ07
6.92Eþ06
5,-
4.01Eþ03
1.37Eþ02
3,-
1.05Eþ01
9.31E-01
4,-
4.19Eþ03
1.25Eþ03
4,-
9.10Eþ03
3.75Eþ03
6,-
3.64Eþ08
2.61Eþ07
6,-
5.89Eþ03
9.10Eþ01
3,-
3.08E-12
3.19E-12
2,þ
4.50Eþ04
3.18Eþ03
3,-
1.34Eþ09
4.94Eþ08
6,-
1.74Eþ06
8.46Eþ04
3,-
9.53Eþ04
1.02Eþ05
5,-
18/2/0
0/2/4.05

8.34Eþ03
3.41Eþ04
6,-
4.44Eþ03
1.80Eþ02
6,-
1.67Eþ01
3.28E-01
6,-
2.36Eþ12
7.92Eþ11
3,-
1.36Eþ08
2.46Eþ07
3,-
1.64Eþ01
3.46E-01
2,-
1.35Eþ04
3.92Eþ04
3,-
8.70E þ 05
1.71E þ 06
1,þ
7.97Eþ07
1.08Eþ07
6,-
4.04Eþ03
1.21Eþ02
4,-
1.03Eþ01
8.47E-01
3,-
4.00Eþ03
8.63Eþ02
2,þ
4.54Eþ03
1.91Eþ03
5,-
3.69Eþ08
2.42Eþ07
7,-
5.88Eþ03
8.81Eþ01
2,-
4.44E-12
4.22E-13
3,þ
4.73Eþ04
2.77Eþ03
4,-
3.47Eþ08
1.39Eþ08
5,-
1.74Eþ06
8.46Eþ04
3,-
8.42Eþ03
2.36Eþ03
4,-
17/3/0
1/3/3.9

5.47Eþ03
2.02Eþ04
5,-
4.39Eþ03
1.97Eþ02
5,-
1.67Eþ01
3.34E-01
6,-
4.79Eþ12
1.44Eþ12
6,-
1.55Eþ08
2.17Eþ07
4,-
1.64Eþ01
2.71E-01
2,-
1.16Eþ04
7.41Eþ03
2,-
3.04Eþ07
2.11Eþ07
4,-
5.96Eþ07
8.18Eþ06
3,-
4.52Eþ03
1.41Eþ02
5,-
1.03Eþ01
1.01Eþ00
3,-
2.52E þ 03
4.86E þ 02
1,þ
4.54Eþ06
2.13Eþ06
7,-
3.41Eþ08
2.41Eþ07
5,-
5.88Eþ03
1.03Eþ02
2,-
7.39E-13
5.70E-14
1,þ
4.01Eþ04
2.85Eþ03
2,-
1.11Eþ10
2.04Eþ09
7,-
1.74Eþ06
9.54Eþ04
3,-
4.87Eþ07
2.27Eþ07
6,-
18/2/0
2/4/3.95

1.05E-26
2.56E-26
2
2.28Eþ03
4.58Eþ01
4
1.47E-13
8.94E-15
2
5.15E þ 11
7.89E þ 10
1
5.47E þ 07
7.93E þ 06
1
3.55E-09
5.48E-14
1
4.60E þ 00
6.28E þ 00
1
9.16Eþ06
8.79Eþ06
3
2.21E þ 07
1.51E þ 06
1
2.64E þ 03
2.70E þ 01
1
3.06E-13
5.48E-14
2
4.15Eþ03
2.40Eþ02
3
6.87E þ 02
2.63E þ 01
1
7.67E þ 07
4.55E þ 06
1
3.14E þ 03
5.42E þ 01
1
3.79Eþ00
6.26E-01
5
3.74E þ 04
1.36E þ 02
1
1.52E þ 03
2.93E þ 02
1
1.04E þ 06
2.85E þ 04
1
1.04Eþ03
8.18Eþ01
2
-
12/0/1.75

The proposed WGA algorithm and the other 5 algorithms are applied
in these two real-world problems. For comparative studies, FEsmax are
adjusted to 5.00Eþ04 and a large enough population size is chosen for all
algorithms. Table 9 presents the optimization results (mean and standard

deviation) of different algorithms executed in 30 runs for solving the two
problems. The best results are shown in boldface, which indicate that
WGA provides efﬁcient and better performance compared to the other 5
advanced algorithms for real-world optimization problems.

9

M. Ghasemi et al.

Array 11 (2021) 100074

Acknowledgements

We would like to acknowledge funding and support from the Natural
Sciences and Engineering Research Council (NSERC) Discovery Grant
(Gadsden).

References

[1] Mahdavi S, Shiri ME, Rahnamayan S. Metaheuristics in large-scale global continues

optimization: a survey. Inf Sci 2015;295:407–28. https://doi.org/10.1016/
j.ins.2014.10.042.

[2] Li X, Yao X. Cooperatively coevolving particle swarms for large scale optimization.

IEEE Trans Evol Comput 2012;16:210–24. https://doi.org/10.1109/
tevc.2011.2112662.

[3] MacNish C, Yao X. Direction matters in high-dimensional optimisation. In: IEEE

congr evol comput (IEEE world congr comput intell 2008; 2008. https://doi.org/
10.1109/cec.2008.4631115.

[4] Ali MZ, Awad NH, Suganthan PN. Multi-population differential evolution with

balanced ensemble of mutation strategies for large-scale global optimization. Appl
Soft Comput 2015;33:304–27. https://doi.org/10.1016/j.asoc.2015.04.019.
[5] Tang K, Y(cid:1)ao X, Suganthan PN, MacNish C, Chen Y-P, Chen C-M, et al. Benchmark
functions for the CEC’2008 special session and competition on large scale global
optimization. Nat Inspired Comput Appl Lab USTC, China 2007;24:1–18.

[6] Tang K, Li X, Suganthan PN, Yang Z, Weise T. Benchmark functions for the CEC
2010 special session and competition on large-scale global optimization. Nat
Inspired Comput Appl Lab USTC, China n.d 2010.

[7] Yang Z, Tang K, Yao X. Multilevel cooperative coevolution for large scale

optimization. In: IEEE congr evol comput (IEEE world congr comput intell 2008;
2008. https://doi.org/10.1109/cec.2008.4631014.

[8] Hsieh S-T, Sun T-Y, Liu C-C, Tsai S-J. Solving large scale global optimization using
improved Particle Swarm Optimizer. IEEE Congr Evol Comput. 2008. https://
doi.org/10.1109/cec.2008.4631030. IEEE World Congr Comput Intell 2008.
[9] Ros R, Hansen N. A simple modiﬁcation in CMA-ES achieving linear time and space
complexity. Parallel Probl Solving from Nat – PPSN X 2008:296–305. https://
doi.org/10.1007/978-3-540-87700-4_30.

[10] Weber M, Neri F, Tirronen V. Shufﬂe or update parallel differential evolution for
large-scale optimization. Soft Comput 2010;15:2089–107. https://doi.org/
10.1007/s00500-010-0640-9.

[11] Chen W, Weise T, Yang Z, Tang K. Large-scale global optimization using cooperative
coevolution with variable interaction learning. Parallel probl solving from nature.
PPSN XI 2010. https://doi.org/10.1007/978-3-642-15871-1_31. 300–9.

[12] Omidvar MN, Li X, Yao X. Cooperative Co-evolution with delta grouping for large
scale non-separable function optimization. In: IEEE congr evol comput; 2010.
https://doi.org/10.1109/cec.2010.5585979.

[13] Wang H, Wu Z, Rahnamayan S. Enhanced opposition-based differential evolution
for solving high-dimensional continuous optimization problems. Soft Comput 2010;
15:2127–40. https://doi.org/10.1007/s00500-010-0642-7.

[14] Hedar A-R, Ali AF. Tabu search with multi-level neighborhood structures for high
dimensional problems. Appl Intell 2011;37:189–206. https://doi.org/10.1007/
s10489-011-0321-0.

[15] Chu W, Gao X, Sorooshian S. A new evolutionary search strategy for global

optimization of high-dimensional problems. Inf Sci 2011;181:4909–27. https://
doi.org/10.1016/j.ins.2011.06.024.

[16] Takahama T, Sakai S. Large scale optimization by differential evolution with

landscape modality detection and a diversity archive. In: IEEE congr evol comput;
2012. https://doi.org/10.1109/cec.2012.6252911. 2012.

[17] Wang C, Gao J-H. A differential evolution algorithm with cooperative

coevolutionary selection operation for high-dimensional optimization. Opt Lett
2012;8:477–92. https://doi.org/10.1007/s11590-012-0592-3.

[18] Chowdhury JG, Chowdhury A, Sur A. Large scale optimization based on Co-

ordinated bacterial dynamics and opposite numbers. Swarm. Evol Memetic Comput
2012;770–7. https://doi.org/10.1007/978-3-642-35380-2_90.

[19] Wang H, Rahnamayan S, Wu Z. Parallel differential evolution with self-adapting

control parameters and generalized opposition-based learning for solving high-
dimensional optimization problems. J Parallel Distr Comput 2013;73:62–73.
https://doi.org/10.1016/j.jpdc.2012.02.019.

[20] Wang Y, Huang J, Dong WS, Yan JC, Tian CH, Li M, et al. Two-stage based ensemble
optimization framework for large-scale global optimization. Eur J Oper Res 2013;
228:308–20. https://doi.org/10.1016/j.ejor.2012.12.021.

[21] Fan J, Wang J, Han M. Cooperative coevolution for large-scale optimization based
on Kernel Fuzzy clustering and variable trust region methods. IEEE Trans Fuzzy Syst
2014;22:829–39. https://doi.org/10.1109/tfuzz.2013.2276863.

[22] Omidvar MN, Li X, Mei Y, Yao X. Cooperative Co-evolution with differential

grouping for large scale optimization. IEEE Trans Evol Comput 2014;18:378–93.
https://doi.org/10.1109/tevc.2013.2281543.

[23] Segura C, Coello Coello CA, Hern(cid:1)andez-Díaz AG. Improving the vector generation

strategy of Differential Evolution for large-scale optimization. Inf Sci 2015;323:
106–29. https://doi.org/10.1016/j.ins.2015.06.029.

[24] Cheng R, Jin Y. A competitive swarm optimizer for large scale optimization. IEEE
Trans Cybern 2015;45:191–204. https://doi.org/10.1109/tcyb.2014.2322602.

[25] Singh D, Agrawal S. Self organizing migrating algorithm with quadratic

interpolation for solving large scale global optimization problems. Appl Soft
Comput J 2016;38:1040–8. https://doi.org/10.1016/j.asoc.2015.09.033.

Fig. 4. The diagram block for a gas turbine's overspeed protection system.

Table 9
Average ﬁtness values and standard deviations on real-world optimization
problems.

Algorithms

Problem 1

Problem 2

Mean

Std

Mean

Std

GL-25
SaDE
CoDE
SPSO2013
HCLPSO
WGA

4.05Eþ000
2.72Eþ000
3.19Eþ000
7.64Eþ000
5.38Eþ000
1.23E-007

9.83Eþ000
6.65Eþ000
8.54Eþ000
1.15Eþ001
1.29Eþ001
1.08E-007

8.634E-001
8.898E-001
8.882E-001
8.730E-001
8.875E-001
8.915E-001

8.114E-001
2.875E-002
6.155E-001
6.058E-001
1.464E-001
9.628E-004

4. Conclusion

The proposed Wild Goose Algorithm (WGA) is a simple and effective
algorithm that has been designed and proposed for optimization of high-
dimensional problems. This algorithm, which is inspired by wild geese
found in nature, includes ordered and coordinated group migration,
reproduction and evolution of geese, and also death in the population of
geese. To show the performance of the proposed WGA algorithm for
optimization of high-dimension problems, it is tested and compared with
sep-CMA-ES, CCPSO2, CSO, EPUS-PSO, MLCC, DECCD, DECC-DML,
CBCC2-DG, CBCC1-DG and DECC-DG algorithms based on the func-
tions of CEC 2008 and CEC 2010. One of the advantages of WGA is that it
has only one control parameter, Cr. It is experimentally shown that WGA
has better competitive results with respect to other mentioned algo-
rithms, and outperforms all other algorithms for most of the test func-
tions. Furthermore, WGA is a simple and basic algorithm for large-scale
optimization which can be used for various real-world optimization
problems. In recent years, numerous studies have been carried out in the
area of high-dimension optimization, the most of which focused on
cooperative co-evolution technique. In future, WGA may be embedded
into the frameworks of different CC methods with various categories in
order to improve its performance. Furthermore, WGA can be used for
solving other real-world large-scale optimization problems.

Credit author statement

Mojtaba Ghasemi: Conceptualization, Methodology, Software,
Writing – original draft preparation. Abolfazl Rahimnejad: Data curation,
Software, Writing – original draft preparation. Rasul Hemmati: Writing –
original draft preparation, Visualization, Investigation. Ebrahim Akbari:
Software, Validation. S. Andrew Gadsden: Writing- Reviewing and
Editing.

Declaration of competing interest

The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂuence
the work reported in this paper.

10

M. Ghasemi et al.

Array 11 (2021) 100074

[26] Sun Y, Wang X, Chen Y, Liu Z. A modiﬁed whale optimization algorithm for large-

scale global optimization problems. Expert Syst Appl 2018;114:563–77. https://
doi.org/10.1016/j.eswa.2018.08.027.

[35] Qin AK, Huang VL, Suganthan PN. Differential evolution algorithm with strategy
adaptation for global numerical optimization. IEEE Trans Evol Comput 2009;13:
398–417. https://doi.org/10.1109/tevc.2008.927706.

[27] Li J, Guo L, Li Y, Liu C. Enhancing elephant herding optimization with novel

[36] Wang Y, Cai Z, Zhang Q. Differential evolution with composite trial vector

individual updating strategies for large-scale optimization problems. Mathematics
2019;7:395. https://doi.org/10.3390/math7050395.

[28] Shadravan S, Naji HR, Bardsiri VK. The Sailﬁsh Optimizer: a novel nature-inspired
metaheuristic algorithm for solving constrained engineering optimization problems.
Eng Appl Artif Intell 2019;80:20–34. https://doi.org/10.1016/
j.engappai.2019.01.001.

[29] Samareh Moosavi SH, Bardsiri VK. Poor and rich optimization algorithm: a new

human-based and multi populations algorithm. Eng Appl Artif Intell 2019;86:
165–81. https://doi.org/10.1016/j.engappai.2019.08.025.

[30] Cai X, Zhang J, Liang H, Wang L, Wu Q. An ensemble bat algorithm for large-scale
optimization. Int J Mach Learn Cybern 2019;10:3099–113. https://doi.org/
10.1007/s13042-019-01002-8.

[31] Faramarzi A, Heidarinejad M, Stephens B, Mirjalili S. Equilibrium optimizer: a

novel optimization algorithm. Knowl Base Syst 2020;191:105190. https://doi.org/
10.1016/j.knosys.2019.105190.

[32] Salih SQ, Alsewari ARA. A new algorithm for normal and large-scale optimization

problems: nomadic People Optimizer. Neural Comput Appl 2020;32:10359–86.
https://doi.org/10.1007/s00521-019-04575-1.

[33] Bas¸ E, Ülker E. Improved social spider algorithm for large scale optimization. Artif

Intell Rev 2020:1–36. https://doi.org/10.1007/s10462-020-09931-5.

[34] García-Martínez C, Lozano M, Herrera F, Molina D, S(cid:1)anchez AM. Global and local

real-coded genetic algorithms based on parent-centric crossover operators. Eur J
Oper Res 2008;185:1088–113. https://doi.org/10.1016/j.ejor.2006.06.043.

generation strategies and control parameters. IEEE Trans Evol Comput 2011;15:
55–66. https://doi.org/10.1109/tevc.2010.2087271.

[37] Zambrano-Bigiarini M, Clerc M, Rojas R. Standard particle swarm optimisation

2011 at CEC-2013: a baseline for future PSO improvements. In: IEEE congr. Evol.
Comput., IEEE; 2013; 2013. p. 2337–44. https://doi.org/10.1109/
CEC.2013.6557848.

[38] Lynn N, Suganthan PN. Heterogeneous comprehensive learning particle swarm
optimization with enhanced exploration and exploitation. Swarm Evol Comput
2015;24:11–24. https://doi.org/10.1016/j.swevo.2015.05.002.

[39] Das S, Suganthan PN. Problem deﬁnitions and evaluation criteria for CEC 2011
competition on testing evolutionary algorithms on real world optimization
problems. Jadavpur Univ Nanyang Technol Univ Kolkata; 2010. p. 341–59.
[40] Chen T-C. IAs based approach for reliability redundancy allocation problems. Appl
Math Comput 2006;182:1556–67. https://doi.org/10.1016/j.amc.2006.05.044.

[41] Das S, Abraham A, Chakraborty UK, Konar A. Differential evolution using a

neighborhood-based mutation operator. IEEE Trans Evol Comput 2009;13:526–53.
https://doi.org/10.1109/tevc.2008.2009457.

[42] Wang H, Rahnamayan S, Sun H, Omran MGH. Gaussian bare-bones differential
evolution. IEEE Trans Cybern 2013;43:634–47. https://doi.org/10.1109/
tsmcb.2012.2213808.

[43] Zhang H, Hu X, Shao X, Li Z, Wang Y. IPSO-based hybrid approaches for reliability-
redundancy allocation problems. Sci China Technol Sci 2013;56:2854–64. https://
doi.org/10.1007/s11431-013-5372-5.

11

