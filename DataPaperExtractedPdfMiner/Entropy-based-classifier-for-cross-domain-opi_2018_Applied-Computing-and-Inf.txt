Applied Computing and Informatics 14 (2018) 55–64

Contents lists available at ScienceDirect

Applied Computing and Informatics

j o u r n a l h o m e p a g e : w w w . s c i e n c e d i r e c t . c o m

Original Article

Entropy based classiﬁer for cross-domain opinion mining
Jyoti S. Deshmukh a, Amiya Kumar Tripathy b,⇑

a Department of Computer Engineering, PAHER University, Udaipur, India
b Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 August 2016
Revised 11 February 2017
Accepted 20 March 2017
Available online 22 March 2017

Keywords:
Data mining
Opinion mining
Knowledge discovery
Expert systems
Information systems
Machine learning

1. Introduction

In recent years, the growth of social network has increased the interest of people in analyzing reviews
and opinions for products before they buy them. Consequently, this has given rise to the domain adap-
tation as a prominent area of research in sentiment analysis. A classiﬁer trained from one domain often
gives poor results on data from another domain. Expression of sentiment is different in every domain. The
labeling cost of each domain separately is very high as well as time consuming. Therefore, this study has
proposed an approach that extracts and classiﬁes opinion words from one domain called source domain
and predicts opinion words of another domain called target domain using a semi-supervised approach,
which combines modiﬁed maximum entropy and bipartite graph clustering. A comparison of opinion
classiﬁcation on reviews on four different product domains is presented. The results demonstrate that
the proposed method performs relatively well in comparison to the other methods. Comparison of
SentiWordNet of domain-speciﬁc and domain-independent words reveals that on an average 72.6%
and 88.4% words, respectively, are correctly classiﬁed.
(cid:1) 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Opinionated text has created a new area of research in text
analysis. Traditionally, fact and information-centric view of text
was expanded to enable sentiment-aware applications. Nowadays,
increased use of the Internet and online activities like ticket book-
ing, online transactions, e-commerce, social media communica-
tions, blogging, etc. has led to the need for the extraction,
transformation and analysis of huge amount of information. There-
fore, new approaches need to applied to analyze and summarize
the information [14].

Organizations take the review of product given by users seri-
ously, as it adversely affects the sales of the product. Consequently,
organizations take the effort to respond to the reviews, as well as
monitor the effectiveness of its advertising campaigns. In this
regard, sentiment analysis, a popular method, is used to extract
and analyze sentiments [5,4].

⇑ Corresponding author at: School of Science, Edith Cowan University, Perth,
Australia.

E-mail addresses: jyoja2007@gmail.com (J.S. Deshmukh), amiya@dbit.in (A.K.

Tripathy).

Peer review under responsibility of King Saud University.

Production and hosting by Elsevier

Opinion mining is constantly growing due to the availability of
views, opinions and experiences about a product/service online, as
people are shedding their inhibition to express their opinions
online. However, automatic detection and analysis of opinions
about products, brands, political issues, etc. is a daunting task.
Opinion mining involves three chief elements:
feature and
feature-of relations, opinion expressions and the related opinion
attributes (e.g., polarity), and feature-opinion relations. An opinion
lexicon is a list of opinion expressions or a set of adjectives, which
are used to indicate opinion/sentiment polarity like positive, nega-
tive and neutral. This lexicon arises from synonyms in the Word-
Net, while antonyms are used to expand lexicon in the form of
graphs. Such a dictionary-based approach has been used to par-
tially disambiguate the results of parts of speech tagger. Further,
fuzzy logic is used to determine opinion boundaries and to adopt
syntactic parsing to learn and infer propagation rules between
opinions and features [24,13].

Medhat et al. [18] conducted a survey on sentiment algorithms
and its applications and found that sentiment classiﬁcation and
feature selection are more prominent areas in recent research.
They also reported that Support vector machine and Naïve Bayes
algorithms are the generally used algorithms to classify senti-
ments, and English is the language used in many resources like
WordNet. Opinions and reviews given on social networking sites
are used to generate datasets for the experiments.

The WordNet is a generalized lexicon and cannot be used for
sentiment analysis; therefore, a need arose for the development

http://dx.doi.org/10.1016/j.aci.2017.03.001
2210-8327/(cid:1) 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

56

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

of sentiment lexicon. SentiWordNet evolved out of WordNet was
created as a lexical resource for opinion mining. It assigns to each
synset of WordNet three sentiment scores: positive, negative and
neutral [19,11].

Manufacturers, as well as consumers, require opinion mining
tools to collect opinions about a certain product. The opinion anal-
ysis tools can be used by manufacturers to decide a marketing
strategy for estimating production rate. On the other hand, con-
sumers can use these tools to make decision on buying a new pro-
duct or take a trip to vacation locations, or select hotel, etc.

Labeled opinions are used to analyze the classiﬁer. Practically,
labeled opinions for every domain is not possible, as it delimited
by time and cost, while domain adaptation or transfer learning
could be used to circumvent this limitation. In this paper, we pro-
pose the approach of domain adaptable lexicon which predicts the
polarity of lexicon of one domain using a set of labeled lexicon of
another domain using a modiﬁed entropy algorithm. This algo-
rithm uses enhanced entropy with modiﬁed increment quantity
instead of traditional entropy algorithm. Dataset of different types
of products containing textual reviews has been used for evalua-
tion. Multiple experiments were carried out to analyze the algo-
rithm using accuracy and F-measure. We designed the approach
in two phases: (i) preprocessing of dataset and (ii) applying classi-
ﬁer and clustering on dataset.

The rest of the paper is structured as follows. In Section 2, we
describe the related work on domain adaptation approaches. In
Section 3, we introduce our new improved entropy based semi-
supervised approach. In Section 4, we evaluate our approach using
cross-domain sentiment classiﬁcation tasks, and compare it with
other baseline methods. Finally, in Section 5 we draw conclusions
on the proposed approach and set directions for future work.

2. Related work

The text documents containing opinions or sentiments were
classiﬁed based on their polarity, i.e. whether a document is writ-
ten with a positive approach or a negative approach. Although
machine learning approach uses a word’s polarity as a feature,
the polarity of some words cannot be determined without domain
knowledge. Hence, the reusability of learned result of a domain is
essential. Transfer learning, also known as domain adaptation, can
be used to address this challenge. Transfer learning utilizes the
results learned in a source domain to solve a similar problem in
another target domain [22]. Approaches used to classify single
and cross-domain polarity opinions are usually a bag of words,
n-grams or lexical resource-based classiﬁers.

The main aim of domain adaptation is to transfer knowledge
across domains or tasks. Tagging the opinion word and building
a classiﬁer is time consuming and expensive, as opinions are
domain dependent. Normally, users express their opinions speciﬁc
to a particular domain. An opinion classiﬁer trained in one domain
may not work well when directly applied to another domain due to
mismatch between domain-speciﬁc words. Thus, domain adapta-
tion algorithms are extremely desirable to reduce domain depen-
dency and labeling costs. Sentiment classiﬁcation problem are
considered as a feature expansion problem, in which related fea-
tures are appended to reduce mismatch of features between the
two domains. To overcome this problem, sentiment-sensitive the-
saurus, which contains different words and their orientation in dif-
ferent domains, has been created. Bollegala et al. [7] used labeled,
as well as unlabeled data, for evaluation. The results suggested that
method performs signiﬁcantly well compared to baseline.

To overcome domain adaptation issue, various adaptation
methods have been proposed in the past, e.g., ensemble of classi-
ﬁers. Combination of various feature sets and classiﬁcation tech-

niques yielded in the ensemble framework was proposed by Xia
et al. [26]. They used two types of feature sets, namely, Parts-of-
speech information and Word-relations and Naïve Bayes, Maxi-
mum Entropy and Support Vector Machines classiﬁers. For better
accuracy, ensemble approaches like ﬁxed combination, weighted
combination and Meta-classiﬁer combination, were applied. Li
et al. [29] proposed active learning in which source and target clas-
siﬁers were trained separately. Using Query By Committee (QBC)
selection strategy, informative samples were selected, and classiﬁ-
cation decision were made by combining classiﬁers. Label propaga-
tion was used to train both classiﬁers. The result demonstrated
that signiﬁcantly outperformed the baseline methods.

Most often, opinions are given in the natural language. One
major issue with natural language is the ambiguity of words. Fer-
sini et al. [10] applied Bayesian ensemble model in which uncer-
tainty and reliability was taken care. Greedy approach was used
for classiﬁer selection, while gold standard datasets were used
for experimental analysis. However, classiﬁcation performance is
frequently affected by the polarity shift problem. Polarity shifters
are words and phrases that can change sentiment orientation of
texts. Xia et al. [28] addressed this issue using three-stage models
which include detection of polarity shift, removal of polarity shifts
and sentiment classiﬁcation. Onan et al. [2] proposed the weight
based ensemble classiﬁer, in which weighted voting scheme was
used to assign weight to classiﬁer. As a base learner Bayesian logis-
tic regression, Naïve Bayes, linear discriminant analysis, logistic
regression and Support vector machine are used. A different type
of experimental analysis shows better result than conventional
ensemble learning. Da Silva et al. [8] used classiﬁer ensembles
formed by different classiﬁer which is applicable to ﬁnd products
on the web. Augustyniak et al. [16] demonstrated Twitter dataset
to have good accuracy only for positive and negative queries. They
found that Bag of Words with ensemble classiﬁer performs better
than supervised approach.

Identiﬁcation of feature and weighting is an important step in
opinion mining. Khan et al. [12] proposed a new approach that
identiﬁed features and assigned term label using SentiWordNet.
In this method, point wise mutual information and chi square
approaches were used to select features to SentiWordNet that
were weighted. Support vector machine was used as classiﬁer.
Experimental evaluation on benchmark dataset shows effective-
ness of approach.

Social networking sites contains text data in long format as well
as short messages with symbols, emoticons etc. Opinion detection
in long reviews is easy than short reviews, as short reviews contain
fewer features, and more symbols, idioms etc. hence difﬁcult to
extract opinion. Lochter et al. [15] proposed ensemble approach
to tackle this issue. This approach used text normalization methods
to improve the quality of features. The features thus ﬁltered and
enhanced served as the input for machine learning algorithms. Pro-
posed framework was evaluated using real and non-coded datasets
and concluded that this approach was superior to other methods
with a 99.9% conﬁdence level. However, this approach was sug-
gested to be expensive for ofﬂine processes due to higher cost of
computing power. Hence, parallelization of this process has been
stated as future work by the authors.

Sparseness is another issue in short text data. Word co-
occurrence and context information approaches are generally used
for solving sparseness issue. These approaches are less efﬁcient. To
address this problem, Chutao et al. [32] considered probability dis-
tribution of terms as the weight of terms.

Similar to ensemble classiﬁers, graph-based methodology are
also used for domain adaptation. Dhillon et al. [25] proposed the
graph-based domain adaptation method. Similarity graphs were
constructed between features from all domains, if these features
were similar then it demonstrated the presence of edge between

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

57

well as the generalization of the information. Hence, generalized
domain adaptable algorithms are needed for the automatic identi-
ﬁcation and classiﬁcation of opinion lexicons.

Domain adaptability is a major issue in sentiment analysis or
opinion mining, which has been addressed in the proposed frame-
work. There are many resources and training corpora available in
English with proven results. A proposed model will be trained from
a training dataset, which will be used for sentiment classiﬁcation.
SentiWordNet resource will be used for this research as it is a pub-
licly available for opinion lexicons with polarity.

An opinion lexicon is one or more words with positive or nega-
tive orientation. Lexicons are used when no training data are avail-
able because the training data contain prior knowledge about the
sentiment of a feature. It is a vital component of unsupervised sen-
timent classiﬁcation methods. The construction of a large sized lex-
icon is an expensive and time-consuming task. Hence, building
automated methods that inﬂuence existing resources to expand
existing lexicons are needed.

Domain adaptation of sentiment models from a domain with
sufﬁcient labeled data to a new domain with less labeled data is
a challenge that requires new and efﬁcient algorithms to solve it.
The proposed system has constructed a domain adaptable lexicon
which can adapt seamlessly from one domain to another. The
expected outcome would be a set of lexicons with polarities for dif-
ferent domains with development of robust model.

Labeled set of documents from source and labeled or unlabeled
documents from target domain is taken as input (Fig. 1). Prepro-
cessing is done to eliminate unnecessary words called as stop
words. The irrelevant data would be eliminated by this process.

Most of the English sentences include words like ‘‘a, an, of, the, I,
it, you, etc.” Such words do not carry any particular meaning. Infor-
mation extraction from natural language can be done effectively
and clearly by avoiding those words which occur frequently. To

them. All labeled features were used in metric-learning algorithms.
Graph was constructed using data-dependent metric and the
weight was calculated for each edge. Experimental results demon-
strated the reduction of classiﬁcation error.

Pan and Yang [22] and Wang and Shi [31] focused on bridging
the gap between domain-speciﬁc and domain-independent lexi-
cons, as these approaches do not work well when applied to two
extremely different domains. Singh and Husain [30] presented dif-
ferent datasets used in sentiment analysis as well as classiﬁcation
and clustering methods. This review reveals that same method is
not applicable for all domains. From the analysis of the literature,
it can be summarized that Naïve Bayes works well for or text clas-
siﬁcation, clustering for consumer services, and SVM for biological
review and analysis.

Training and testing data from same feature space and same
distribution has been reported to give good results for machine
learning algorithms. Estimating the effect of distribution changes
through statistical models is reported to be very expensive, as it
has to be rebuilt from scratch. In many real world applications, it
is expensive or impractical to recollect the needed training data
and rebuild the models. In such cases, domain adaptation or trans-
fer learning between task domains would be desirable.

To overcome the problem of feature distribution variance across
domains, Xiao and Guo [21] proposed a feature space independent
semi-supervised kernel matching method, based on a Hilbert-
Schmidt Independence Criterion. Two kernel matrices were cre-
ated over the instances in the source domain and the instances
in the target domain. Each labeled instance in the target domain
was deﬁnitely mapped into a source instance with the same class
label through prediction function. Evaluation of the proposed
method performed on Amazon product reviews and Reuters’ mul-
tilingual newswire stories showed reduction in human annotation
efforts.

The Lexicon based approach works with the polarities of the
opinion-oriented words and relies on a lexicon. A collection of
known terms that contribute to the sentiment of a text is called
sentiment lexicon. Many open source lexicons are available which
serve as a database for extracting the polarity values of opinion
words. But these generic polarity lexicons reﬂect the most generic
sentiment of opinion words. An opinion word need not express the
same sentiment everywhere, i.e., opinion words could be context-
dependent or domain-speciﬁc. The word ‘‘small” in ‘‘room is too
small” indicates a negative opinion, whereas in ‘‘small screen size”
as seen in the mobile domain indicates a positive opinion.

The variation of opinion found for the same word in different
domains restricts the usage of generic lexicons as it generalizes
the polarity of a word. Therefore, lexicons with updated polarity
values that can give polarity of a same word in different domains
using same lexicon database will have to be built. The proposed
work attempts in building such an enhanced polarity lexicon using
the maximum entropy algorithm with modiﬁcation being made in
increment quantity which helps in reﬁning the classiﬁcation gran-
ularity from document to word level. The knowledge gained from
one domain is used to predict and classify the polarity of opinion
words from another domain, resulting in an improved lexicon
using semi-supervised approach. The common words from all
domains having same polarity orientation are treated as domain-
independent words and remaining as domain-speciﬁc words.

3. Proposed framework

3.1. Generic processes

Most of the existing research regarding opinion mining is
domain dependent, which limits the scope of the application as

Fig. 1. Workﬂow of proposed system.

58

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

remove stop words from sentences, a text ﬁle that consists of list of
English stop words is used.

After the removal stop word, parts of speech like noun, adjec-
tive, adverb, verb, etc. are extracted using the parser. Parsing is a
vital step as it gives opinion words as an output. Sentence parsing
involves assigning different parts of speech tags to a given text.
This process is known as Part-Of-Speech (POS) tagging. For infor-
mation extraction, POS tagging is important because each category
plays a speciﬁc role within a sentence. Nouns give names to
objects, or entities from reviews. An adjective describes opinion.
Also, some verbs and adverbs can play an important role as an
adjective.

Examples:.

the/DT battery/NN life/NN on/IN the/DT iphone/JJ 4S/CD is/VBZ
amazing/JJ
this/DT phone/NN is/VBZ very/RB slow/JJ

In pre-processing step, text review is ﬁrst divided into sen-
tences. Stanford parser is used to generate the POS tagging of each
word present in the sentence [9], as it is essential to ﬁnd general
language patterns.

Adjectives and adverbs are good indicators of opinion, hence are
extracted from each review. Some verbs are also considered as
opinion, e.g., like, love, recommend, etc. Two consecutive words,
i.e., adverb-verb, adverb-adjective also extracted from processed
tagged reviews as a verb alone does not indicate opinion. Nouns
are not considered in framework.

All tagged words after POS tagging phase tagged words are clas-

siﬁed using an algorithm explained in Section 3.2.

3.2. Algorithm

Classiﬁcation of opinions can be done using a modiﬁed maxi-
mum entropy algorithm. The increment quantity is modiﬁed
according to the importance of the measure of words as speciﬁed
in Eq. (3). The maximum entropy classiﬁer is closely related to
the Naïve Bayes classiﬁer, except that it uses a search-based opti-
mization to ﬁnd weights for the features that maximize the likeli-
hood of the training data. It can handle mixture of boolean, integer,
and real-valued features [17]. It is also used when the conditional
independence of the features cannot be assumed, i.e., in problems
like text classiﬁcation where features are words and are not inde-
pendent [1].

The main aim of the study is to construct a stochastic model
that accurately represents the behavior of the random process.
Let d be document in a dataset; w, the word present in document;
and c, the class.

1. For each word w and class c 2 C, a joint feature pðw; cÞ ¼ f (w, c)
= Nis deﬁned, where N is the number of times that w occurs in a
document in class c. (N could also be boolean, registering pres-
ence vs. absence.)

2. Empirical distribution is used to build the statistical model of
the random process, which distributes text to speciﬁc class.

Pðcjd; kÞ¼def

P

P

exp
c02C exp

kif i
P
i

ðc; dÞ
kif i

i

ðc0; dÞ

ð2Þ

where k ¼ ki þ di ðkis calculated by an iterative scaling algorithmÞ
di is increment quantity

As the granularity of classiﬁcation is reﬁned from document-
level to word level, the increment quantity (di) is modiﬁed. The
modiﬁed quantity (dmi) is deﬁned as

!

dmi ¼ 1
M

Xk

log

idfi

i

;

ð3Þ

where M ¼ max

P
k
i f i

ðd; cÞ.

Eq. (3) calculates inverse document frequency of each word,
which is a popular measure of words’ importance. It is deﬁned as
the logarithmic ratio of the number of documents in a collection
to the number of documents containing the given word. This sug-
gests that uncommon words have higher idfi and common function
words have lower idfi, where idfi is inverse document frequency.
This is useful to measure the words ability to discriminate between
documents. M is the sum of all features in training instance. Fea-
ture value is taken as tﬁdfi.

Algorithm works on word level. POS tagged words are extracted
from preprocessing steps are used. Each word acts as feature. Fea-
ture value fiðd; cÞ is calculated as tﬁdfi. of each word. As per Eq. (3)
inverse document frequency of each POS tagged word is calculated.
Algorithm is executed for total number of features provided in
input dataset. According to this weight words are classiﬁed into
two categories. Classiﬁed words are having POS tag, polarity tag,
and weight value. From this list, common and uncommon words
are picked and used for bipartite graph clustering explained below.
Probability distribution of class c is calculated based on term
frequency. Classiﬁcation process predicts the polarity of the target
domains lexicon from source domain. The clustering algorithm is
applied on classiﬁed word lists and documents until it reaches con-
vergence. All extracted words from source domain are tagged, and
weight is calculated for each word using mutual information avail-
able for words. Target words are extracted and compared with the
source. If they match then they will be categorized as domain-
independent, otherwise domain-speciﬁc. Domain-independent
words are from both source and target domains; whereas,
domain-speciﬁc are from target domain only. A graph is con-
structed between domain dependent and domain-independent
words. Co-occurrence relationship between these words repre-
sents edge. Occurrence of domain-speciﬁc word along with
domain-independent word means that both a related to each other
and assign edge. Using domain-independent words weight is
assigned to domain-speciﬁc words and classiﬁed accordingly. Each
ﬁle form target domain is assigned score on which basis it is clas-
siﬁed as positive or negative. Each word has weight assigned to it.
Summation of weights of words in each sentence gives score to
sentence. Then addition of all sentence score is nothing but score
of ﬁle. On the basis of this, ﬁle is classiﬁed.

Clustering helps in reducing mismatch between domain-
speciﬁc words of source and target domains. Two sets of lexicons
are extracted as an output with polarity which is compared with
SentiWordNet (Fig. 2).

4. Result and discussions

ðd; cÞ ¼

f i

Z

1

0

if c ¼ ci&d contains wk otherwise

ð1Þ

4.1. Experiment1

Above indicator function called as feature. Via iterative optimiza-
tion, assign a weight to each joint feature so as to maximize the
log-likelihood of the training data.
3. The probability of class c given a document d and weights k is

The dataset from John et al. [6] was used for experiments. It
contains a collection of product reviews from Amazon.com. This
dataset contains three types of ﬁles positive, negative and unla-
beled in XML format. Each line in form of: feature:<count>. . .. fea

 
J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

59

Table 2
Comparative analysis of accuracy of proposed method and baseline methods.

Source ! Target Accuracy (%)

Proposed Method

Accuracy
(%) SS-FE

Accuracy
(%) SFA

Accuracy
(%) SWC

B ! D
B ! E
B ! K
D ! B
D ! E
D ! K
E ! B
E ! D
E ! K
K ! B
K ! D
K ! E

82.45
78
78.65
74.35
79.78
84.21
82.15
87.8
81.44
81.05
70
88.35

79.10
74.24
78.07
80.38
77.07
77.82
72.86
74.60
84.87
72.94
75.70
82.93

82.55
72
78
77
77
81
75.5
77
87.1
74
77
84.6

81.66
77.04
82.26
79.95
76.98
82.13
72.11
73.81
85.33
75.78
76.88
84.78

Fig. 2. Flow of proposed algorithm.

ture:<count>#label#:<label>, e.g., old_boy:1 i_am:1 the: 1 boy_-
had:1 so_i:1 #label#: negative. These ﬁles were extracted using
XML ﬁle splitter and reviews were converted into text ﬁle. The
dataset contains 1000 positive ﬁles and 1000 negative ﬁles for each
domain. The reviews are about four product domains: Books (B),
DVDs (D), Electronics (E) and Kitchen appliances (K) and are writ-
ten in English language. For experiment, labeled dataset of 1000
positive and 1000 negative ﬁles was used. An instance in each
domain is recorded in Table 1. Except book domain other domains
had more number of positive instances.

From this dataset, 12 cross-domain sentiment classiﬁcation
constructed: B ! D; B ! E; B ! K; D ! B; D ! E;
tasks were
D ! K; E ! B; E ! D; E ! K; K ! B; K ! D; K ! E, where
the
word before an arrow corresponds to the source domain and the
word after an arrow corresponds to the target domain.

From Table 2, it is evident that Book and DVD, if considered as a
source domain, achieve a good compatibility with electronics and
kitchen domain, which is considered as target domain. Besides,
electronic and kitchen are compatible domains.

Baseline methods use in this study are Feature Ensemble plus
Sample selection (SS-FE) [27], Spectral feature alignment (SFA)
[23], and Supervised word clustering (SWC) [20]. SFA achieved
was between 72.5% and 86.75%, SS-FE was between 72.94% and
84.87% and SWC was between 72.11% and 85.33%, whereas accu-
racy of proposed algorithm was between 70% and 88.35%.

Only the DVD, electronics, and kitchen were considered as the
source domain, while book, kitchen and DVD as a target domain,
producing comparatively less accurate results than the baseline
method (Fig. 3). There are two key points in proposed framework:

Table 1
Negative and positive instances for multi-domain dataset.

Domain Name

Negative Instances

Positive Instances

Book
DVD
Electronics
Kitchen appliances

73,500
66,126
43,806
36,106

72,794
76,759
44,321
36,733

Fig. 3. Accuracy analysis.

ﬁrst it classiﬁes the words and documents, and then clusters them.
After classiﬁcation step, opinionated words are extracted with
weight value as well as polarity. These weights are very important
factor as it increases the importance of discriminative terms. Pro-
posed approach also identiﬁes domain-independent and speciﬁc
features which are used for clustering. All classiﬁed words clus-
tered again leads to acceptable results. One of the reasons for
low accuracy for some domain is imbalance of class labels and
the presence of word disambiguation.

Accuracy is used as an evaluation measure. Accuracy is the pro-
portion of correctly classiﬁed examples to the total number of
examples; on the other hand, error rate refers to incorrectly classi-
ﬁed examples to correctly classiﬁed examples. F-measure or preci-
sion and recall can be used as evaluation measures.

F-measure is only deﬁned in terms of true positive (TP), false
positive (FP) and false negative (FN), while true negative (TN) is
not considered. Accuracy and F-measure is compared for proposed
approach which shows that, in general, F-measure is similar to
accuracy. But only single class is considered in F-measure as posi-
tive class (Fig. 4). On the other hand, when calculating accuracy
equal weight is given to both the classes.

Classiﬁed words are used to ﬁnd domain-independent and
domain-speciﬁc words from the respective domains. Domain-
independent and domain-speciﬁc words are compared to the Sen-
tiWordNet [3], in order to ﬁnd out how many words match with
them (Tables 3–6). From the tables, it has been observed that on
an average 72.6% domain-speciﬁc words are correctly classiﬁed
for different domains; while88.4% words
from domain-
independent word list
classiﬁed. Domain-
independent words typically occur in every domain; hence, match-
ing percentage is more than the matching percentage of domain-

correctly

are

60

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

Fig. 4. Analysis of accuracy vs F-measure.

Table 3
Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering book (B) as a source domain.

Domains

Domain-speciﬁc words

Domain-independent words

No. of words matching SentiWordNet

B ? D
B ? E
B ? K

11,503
5250
4200

9744
4796
4325

Domain-speciﬁc words

Domain-independent words

8934
4077
3262

8972
4395
3979

Table 4
Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering DVD(D) as a source domain.

Domains

Domain-speciﬁc words

Domain-independent words

No. of words matching SentiWordNet

D ? B
D ? E
D ? K

11,130
5238
4205

9744
4781
4320

Domain-speciﬁc words

Domain-independent words

8644
4068
3266

9009
4418
3980

Table 5
Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering Electronics (E) as a source domain.

Domains

Domain-speciﬁc words

Domain-independent words

No. of words matching SentiWordNet

E ? B
E ? D
E ? K

16,105
16,466
4802

4769
4781
3723

Domain-speciﬁc words

Domain-independent words

12,508
12,789
3729

4430
4462
3454

Table 6
Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering Kitchen (K) as a source domain.

Domains

Domain-speciﬁc words

Domain-independent words

No. of words matching SentiWordNet

K ? B
K ? D
K ? E

16,549
16,927
6296

4325
4320
3723

Domain-speciﬁc words

Domain-independent words

12,853
13,147
4890

3980
3987
3449

speciﬁc words. As SentiWordNe thas generalized opinion lexicons,
percentage of matching domain-speciﬁc words is relatively less.

4.2. Experiment 2

To evaluate accuracy of proposed algorithm for unlabeled target
dataset, we have performed 12 cross-domain tasks with labeled
source dataset and unlabeled target dataset. Fig. 5 illustrates the
accuracy analysis. Accuracy achieved by proposed method for
unlabeled target lies between 65.65% and 98.0%; whereas, labeled
target achieves accuracy between 70.0% and 88.35%. Performance
of classiﬁer, therefore, increases signiﬁcantly. It shows that elec-

tronics, kitchen and DVD domains are compatible with each other
due to their more similar features. In general, the features of these
three domains are more or less similar. Also kitchen as source and
electronics as target and vice versa gives better accuracy as both
domains share more common features. Kitchen appliances domain
also shares electronics appliances; hence, major information of
both domains is similar. But kitchen as source and DVD as target
does not give good results for both labeled and unlabeled dataset,
because kitchen and DVD are not similar to each other, as that of
kitchen and electronics. Usually, if two domains are more similar
then a number of features transferred from source to target are
also more because source data is used as training dataset and tar-

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

61

for some domains. The result shows that the proposed algorithm
was better than both SVM and Naïve Bayes.

Figs. 6 and 7 provides accuracy and F-measure analysis for each
of 12 cross-domain sentiment classiﬁcation tasks on Amazon pro-
duct reviews. For this study, 1000 positive and 1000 negative
reviews were taken as the source and target domains. Comparison
of proposed approach with baseline approaches shows that
domain adaptation from book as source domain to the DVD as tar-
get domain rather than kitchen as source to DVD as target domain
is more feasible. It also shows that relatedness between source and
target domain reviews are more important factors for the effective-
ness of domain adaptation.

Electronics as a source domain is more compatible with every
domain. This suggests that more number of features are relevant
in both source and target domains. The proposed approach pro-
vides the highest accuracy for electronics as compared to baseline
as well as SVM and Naïve Bayes. Compared to accuracy, F-measure
results are improved but these are only related with positive doc-
uments. Results show that the proposed approach have better
accuracy and F-measure. Further, Naïve Bayes provides better
results than SVM. Some of the major drawback of Naïve Bayes is
assumption of independent attributes and difﬁculty in interpreta-
tions of SVM results. In proposed framework, maximum entropy
that is used provides a natural mechanism of multiclass classiﬁca-
tion. The results are better as increment quantity focuses on term
frequency and inverse document frequency. It concentrates on fea-
tures and its presence which is not focused in Naïve Bayes or SVM.
Random Trees area collection of individual decision trees, in
which each tree is generated from different samples and subsets
of the training data. Classiﬁcation of dataset based on random
sub selection of training samples result in many decision trees,
hence this method is called Random trees. Each tree can be voted
to make ﬁnal decision.

An experiment was carried using Rapid Miner 5.3.015 software.
Results are recorded in Fig. 8, which shows the comparison
between the proposed approach and the Random tree. The pro-
posed approach gives better accuracy than the Random tree. The
random trees classiﬁer takes the input feature vector, classiﬁes it
with every tree in the forest, and produces the class label that
‘‘votes” as output. Using bootstrap
received the majority of
approach, training sets are generated. Vectors are randomly
selected, hence some vectors will occur more than once or some
will be absent. All variables are not used to ﬁnd the best split.

In contrast, the proposed approach works at word level where
each word acts as a feature. Later, importance of word is analyzed
using term frequency and inverse document frequency of each
word consequently producing better accuracy. Using Random tree
higher accuracy achieved in Electronics as source and kitchen as
target domain wherein the proposed approach it is reverse way.

Fig. 5. Accuracy analysis of unlabeled and labeled target.

get features are derived from it. The impact of unlabeled target
data is more than that of labeled target data. It states that unla-
beled target data can be used for the accuracy gain, as well as
reduce the annotation cost signiﬁcantly.

4.3. Experiment 3

Generally, Naïve Bayes and SVM algorithms are used for text
classiﬁcation. Experiment 3 was conducted to evaluate proposed
framework. using Rapid Miner 5.3.015 software for Naïve Bayes
and SVM algorithm,. The software contains text mining plug-in
which converts non-structured textual data into structured format
for further analysis. This study adopted implemented linear SVM as
most of the text classiﬁcation problems are linearly separable. Fur-
ther, as text classiﬁcation contains large number of features, linear
kernel was found to be better suited for this purpose. Results were
obtained from Blitzer dataset for four different domains of uni-
grams and for applying word frequency in document and in entire
corpus.

Naïve Bayes classiﬁer is a probabilistic classiﬁer based on prob-
ability models that incorporate strong independence assumptions
among the features. Independence assumption of features is a sub-
tle issue with Naïve Bayes. If certain feature and class label value
do not occur together then the frequency-based probability esti-
mate will become zero. When all the probabilities are multiplied,
the answer will be zero and this affects the posterior probability
estimate. Hence, it provides poor accuracy compared to the SVM

Fig. 6. Accuracy analysis for 12 cross-domain classiﬁcation tasks.

62

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

Fig. 7. F-measure analysis for 12 cross-domain classiﬁcation tasks.

Fig. 8. Accuracy comparison with Random Tree.

Therefore, it is clear that electronics and kitchen are more compat-
ible domains as they are having similar features.

4.4. Experiment 4

For testing the model, Amazon’s balanced 6cats dataset col-
lected by Mark Drezde and processed by Richard Johansson in
2012 has been used for this study. The review collection is divided

into six topic subdirectories: Books, Camera, DVD, Health, Music
and Software. The document in each topic directory is divided into
positive and negative subdirectories. From this dataset, 30 classiﬁ-
cation tasks were constructed. For 1000 positive and 1000 negative
ﬁles of each domain, the accuracy was achieved between 70% and
97% (Fig. 9).

Highest accuracy achieved in software as source and camera as
target domain. B ! C; C ! B; D ! M; H ! S; M ! B; S ! C clas-

Fig. 9. Accuracy analysis for Amazon balanced 6 cats dataset.

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

63

Table 7
Domain-independent, Domain-speciﬁc and SWN matched words for Book as source
domain.

to each other or relatedness between these domains is less, hence
the results are lower.

Domains Domain-speciﬁc
words

Domain-independent
words

SWN matched
words

5. Conclusions

B ? C
B ? D
B ? H
B ? M
B ? S

3942
12,044
4356
10,304
4712

4459
9896
4468
7438
5222

4533
10,358
4444
7910
5356

Table 8
Domain-Independent, Domain-Speciﬁc and SWN matched words for DVD as source
domain.

Domains Domain-speciﬁc
words

Domain-independent
words

SWN matched
words

D ? B
D ? C
D ? H
D ? M
D ? S

10,519
3770
4306
9305
4741

9896
4631
4518
8437
5193

10,259
4670
4481
8883
5342

Table 9
Domain-independent, Domain-speciﬁc and SWN matched words for Camera as
source domain.

Domains Domain-speciﬁc
words

Domain-independent
words

SWN matched
words

C ? B
C ? D
C ? H
C ? M
C ? S

15,956
17,309
5186
13,845
6029

4459
4631
3638
3897
3905

5049
5264
3652
4410
4142

siﬁcation tasks are giving high accuracy. From the results, it was
found that book, camera and music domains are having more com-
mon features.

Tables 7–9 present domain-speciﬁc, domain-independent and
matching words with SentiWordNet. Domain-independent words
are from both source and target domains. Domain-speciﬁc are only
from target domains. With SentiWordNet matching percent is
average 55.7%.

4.5. Discussion

For our experiments, two different datasets were used. Each
dataset consisted of labeled positive and negative text review doc-
uments. All the results from above sections reveal that the pro-
posed approach gives better accuracy than baseline methods.
Word is important entity as it indicates sentiment or opinion of
object. The proposed framework is based on modiﬁed entropy clas-
siﬁer. Opinionated words are extracted based on the term fre-
quency and inverse document frequency. Increment quantity is
modiﬁed as granularity reﬁned from document to word level
which shows drastic difference between traditional maximum
entropy and modiﬁed entropy. Bipartite graph clustering is applied
on classiﬁed data which has enhanced the results.

As compared to baseline methods, moderate accuracy was
achieved by the proposed method. Relatedness between source
and target domain is important factor in domain adaptation. Also
for unlabeled target dataset better accuracy was achieved. It means
that it can signiﬁcantly reduce the annotation cost also. F-measure
is also taken as evaluation measure which shows better results of
proposed framework over base line methods. But it does not con-
sider the true negative features. Some domains are not compatible

Opinion mining is a popular research area; yet, researchers have
mainly focussed on domain adaptation. This work addressed the
major issue of domain adaptation. In this work, semi-supervised
approach was used which holds maximum entropy classiﬁer with
modiﬁed increment value and bipartite clustering. Labeled as well
as unlabeled set of lexicons from different domains were collected
from Amazon are used for experimental analysis of the proposed
approach. Pre-processing step was used to remove noise from
dataset. Each word from dataset is tagged for parts of speech using
the Stanford parser. This tagged data is used by classiﬁer which is
based on features tﬁdfi and idfi,. value which is useful to measure
the words’ ability to discriminate between documents. Domain-
speciﬁc and domain-independent lexicons are used for clustering.
Classiﬁed lexicons are compared with SentiWordNet 3.0 to ﬁnd
matching percentage as SentiWordNet is publicly available lexicon
resource.

This work was able to produce relatively good results for some
of the domain, and it was able to handle only two classes with an
acceptable accuracy. Domain-speciﬁc and domain-independent
words compared to SentiWordNet 3.0 shows average matching
percent 68.25%. The experimental results of proposed approach
have shown a signiﬁcant increase in accuracy for different domains
over baseline approach as the proposed framework emphasizes on
granularity of the word. This is the major change in classiﬁer in
comparison to traditional approach. Importance of each word that
has more impact on results of classiﬁer was classiﬁed. Testing of
approach carried on Amazon cat6 dataset, which shows a signiﬁ-
cant improvement in accuracy ranging from 3 to 6 points com-
pared to dataset from Blitzer. In comparison to SVM and Navie
Bayes, we have proposed an algorithm that could provide better
accuracy. It shows that relatedness between domains is a major
factor for effectiveness of domain adaptation.

In the proposed system, bipartite graph clustering was used to
reduce the mismatch between domain speciﬁc words of source
domain and target domain. Domain-independent words were used
to cluster domain-speciﬁc words from source and target domains.
To train classiﬁer for target domain, clustering was used as it
reduced the gap between domain-speciﬁc words of different
domains. Future studies can be taken up to determine the co-
clustering of words and documents from different domains. The
proposed system focuses on only words, in future non-word fea-
tures like the age of document, the recommendation counts of doc-
ument can be considered. At present, framework considers only
unigrams and reviews are in English language. Also in future this
work can be extended for other languages as well as n-grams.

References

[1] Abinash Tripathy, Ankit Agrawal, Santanu Kumar Rath, Classiﬁcation of
in: Expert

sentiment reviews using n-gram machine learning approach,
Systems with Applications, vol. 57, 2016, pp. 117–126.

[2] Aytug Onan, Serdar Korukoglu, Hasan Bulut, A multiobjective weighted voting
ensemble classiﬁer based on differential evolution algorithm for
text
sentiment classiﬁcation, in: Elsevier Expert Systems With Applications, vol.
62, 2016, pp. 1–16.

[3] Baccianella Stefano, Esuli Andrea, Sebastiani Fabrizio, SentiWordNet 3.0: An
Enhance Lexical Resource for Sentiment Analysis and Opinion Mining, in:
Proceedings of the 7th Language Resources and Evaluation Conference (LREC
2010), Valletta, Malta, May 17–23, 2010, pp. 2200–2204.

[4] A. Bermingham, M. Conway, L. McInerney, N. O’Hare, A. Smeaton, Combining
Social network analysis and sentiment analysis to explore the potential for
online radicalisation, in: Proc. of Int’l Conf. on Advances in Social Network
Analysis and Mining, Athens, Greece, July 20–22, 2009, pp. 231–236.

64

J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64

[5] Bing Liu, Sentiment Analysis & Opinion Mining, Kindle Edition., Morgan &

[18] W. Medhat, A. Hassan, H. Korashy, Sentiment analysis algorithms and

Claypool Publishers, 2012.

[6] John Blitzer, Mark Dredze, Fernando Pereira, Biographies, Bollywood, boom-
boxes and blenders: domain adaptation for sentiment classiﬁcation,
in:
Association of Proceedings of the 45th Annual Meeting of the Computational
Linguistics (ACL), Prague, Czech Republic, June 2007, pp. 440–447.

[7] D. Bollegala, D. Weir, J. Carroll, Cross-domain sentiment classiﬁcation using a
sentiment sensitive thesaurus, in: Knowledge and Data Engineering, IEEE
Transactions, vol. 25(8), 2013, pp. 1719–1731.

[8] N.F. Da Silva, E.R. Hruschka, E.R. Hruschka, Tweet sentiment analysis with
classiﬁer ensembles, in: Elsevier Decision Support Systems, vol. 66, 2014, pp.
170–179.

[9] M.C. De Marneffe, B. MacCartney, C.D. Manning, Generating typed dependency
parses from phrase structure parses, in: Proceedings of LREC, vol. 6, 2006, pp.
449–454.

[10] E. Fersini, E. Messina, F.A. Pozzi, Sentiment analysis: Bayesian Ensemble

Learning, Elsevier Decision Support Syst. 68 (2014) 26–38.

[11] A. Esuli, F. Sebastiani, Senti-WordNet: A Publicly Available Lexical Resource for
Opinion Mining, in: Proc. of the 05th Conf. on Language Resources and
Evaluation, Genova Italy, May 22–28, 2006, pp. 417–422. Available at <http://
sentiwordnet.isti.cnr.it/>.

[12] Farhan Hassan Khan, Usman Qamar, Saba Bashir, S WIMS: semi-supervised
subjective feature weighting and intelligent model selection for sentiment
analysis, Knowledge-Based Syst. 100 (2016) 97–111.

[13] Haiqing Zhang, Aicha Sekhari, Yacine Ouzrout, Abdelaziz Bouras,

Jointly
identifying opinion mining elements and fuzzy measurement of opinion
intensity to analyze product features, Eng. Appl. Artiﬁcial Intelligence 47
(2016) 122–139.

[14] Kumar Ravi, Vadlamani Ravi, A survey on opinion mining and sentiment
analysis: tasks, approaches and applications, Knowledge Based Syst. 89 (2015)
14–46.

[15] J.V. Lochter, R.F. Zanetti, D. Reller, T.A. Almeida, ShortText opinion detection
using ensemble of classiﬁers and semantic indexing, Expert Syst. Appl. 62
(2016) 243–249.

[16] Lukasz Augustyniak, Tomasz Kajdanowicz, Piotr Szyma´ nski, Włodzimierz
Tuligłowicz, Przemyslaw Kazienko, Reda Alhajj, Boleslaw Szymanski, Simpler
is better? lexicon-based ensemble sentiment classiﬁcation beats supervised
methods, in: International Workshop on Curbing Collusive Cyber-gossips in
Social Networks (C3-2014), August 17, 2014 Proc.
IEEE/ACM Int. Conf.
Advances in Social Network Analysis and Mining, ASONAM, Beijing, China,
2014.

[17] McCallum Andrew, Freitag Dayne, Pereira Fernando, Maximum entropy
markov models for information extraction and segmentation,
in: 17th
International Conf. on Machine Learning, Stanford University, June 29-July 2,
2000.

applications: a survey, Ain Shams Eng. J. 5 (4) (2014) 1093–1113.

[19] G.A. Miller, WordNet: a lexical database for English, Commun. ACM 38 (11)

(1995) 39–41.

[20] Min Xiao, Feipeng Zhao, Yuhong Guo, Learning latent word representations for
domain adaptation using supervised word clustering, in: Proceedings of the
2013 Conference on Empirical Methods in Natural Language Processing, pages
152–162, Seattle, Washington, USA, 18–21 October, 2013.

[21] Min Xiao, Yuhong Guo, Feature space independent semi-supervised domain
adaptation via kernel matching, IEEE Trans. Pattern Anal. Mach. Intelligence 37
(1) (2015) 52–66.

[22] S. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. Knowledge Eng. 22

(10) (2009) 1345–1359.

[23] Pan SinnoJialin, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, Zheng Chen, Cross-
domain sentiment
in:
Proceedings of the 19th International World Wide Web Conference, ACM,
Raleigh, USA, April 26–30, 2010.

classiﬁcation via

alignment,

spectral

feature

[24] B. Pang, L. Lee, Opinion mining and sentiment analysis, Found. Trends Inform.

Retrieval 2 (1–2) (2008) 1–135.

[25] Paramveer S. Dhillon, Partha Talukdar, Koby Crammer, Metric Learning for
Graph-Based Domain Adaptation, University of Pennsylvania Department of
Computer and Information Science Technical Report No. MS-CIS-12-17,
January 2012.

[26] Rui Xia, Chengqing Zong, Shoushan Li, Ensemble of
classiﬁcation,

classiﬁcation algorithms
Sciences, vol. 181(6), 15 March 2011, pp. 1138–1152.

sentiment

for

feature sets and
Information

in:

[27] Rui Xia, Chengqing Zong, Xuelei Hu, E. Cambria, Feature ensemble plus sample
selection: domain adaptation for sentiment classiﬁcation, IEEE Intelligent Syst.
28 (3) (2013) 10–18.

[28] Rui Xia, Feng Xu, Jianfei Yu, Yong Qi, Erik Cambriac, Polarity shift detection,
elimination and ensemble: a three-stage model for document-level sentiment
analysis, Inform. Process. Manage. 52 (2016) 36–45.

[29] Shoushan Li, Yunxia Xue, Zhongqing Wang, Guodong Zhou, Active learning for
cross-domain sentiment classiﬁcation, in: Proceedings of IJCAI’13 the Twenty-
Third International Joint Conference on Artiﬁcial Intelligence, 2013, pp. 2127–
2133.

[30] P.K. Singh, M.S. Husain, Methodological study of opinion mining and

sentiment analysis techniques, Int. J. Soft Comput. 5 (1) (2014) 11.

[31] M. Wang, H. Shi, Research on sentiment analysis technology and polarity
computation of sentiment words,
in: Proc. of Int’l Conf. on Progress in
Informatics and Computing, Shanghai, vol. 1, December 10–12, 2010, pp. 331–
334.

[32] Zheng Chutao, Liu Cheng, Wong Hau-San, Iterative term weighting for short
in: Proceedings of the 2015 IEEE International Conference on

text data,
Systems, Man, and Cybernetics, Hong Kong, 9–12 October 2015.

