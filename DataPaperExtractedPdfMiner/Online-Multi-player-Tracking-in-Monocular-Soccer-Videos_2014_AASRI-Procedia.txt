Available online at www.sciencedirect.com
ScienceDirect

 AASRI Procedia   8  ( 2014 )  30 – 37 

2014 AASRI Conference on Sports Engineering and Computer Science (SECS 2014)

Online Multi-player Tracking in Monocular Soccer Videos

Michael Herrmanna,*, Martin Hoerniga, Bernd Radiga

aTechnische Universität München, Image Understanding and Knowledge-Based Systems, Boltzmannstr. 3, D-85748 Garching, Germany

Abstract

The tracking of players in monocular soccer videos is a challenging task because of numerous difficulties that can occur
especially in TV broadcasts, such as camera motions, severe occlusion of players, or inhomogeneous lightning conditions.
We propose a new robust method for multi-player tracking, which is based on finding local maxima on a confidence map.
This map represents an ensemble of visual evidences, such as colors of the team outfits, responses of a HOG human
detector, and grass regions in images. This combination of features allows for a robust online tracking procedure that does
not require any further information about the camera calibration or other user input. In the evaluation using four
representative datasets, our algorithm shows remarkable accuracy and outperforms a state-of-the-art pedestrian tracker.

© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
© 2014 Herrmann, Hoernig, Radig. Published by Elsevier B.V.
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Selection and/or peer review under responsibility of American Applied Science Research Institute
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute

Keywords: computer vision, soccer analysis, player tracking

1. Introduction

We aim to develop a system for an automatic analysis of soccer matches that extracts match statistics and
performs tactical analysis from monocular recordings, such as TV broadcasts, which usually exhibit numerous
difficulties. One important task of such a system is the 2D tracking of players in the video images. For this
purpose, we propose a new robust unsupervised online method that provides three main contributions:

* Corresponding author. Tel.: +49-89-289-17779; fax: +49-89-289-17757.
E-mail address: michael.herrmann@tum.de

2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license 
(http://creativecommons.org/licenses/by-nc-nd/3.0/).
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.08.006 

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

31

Figure 1: From left to right: original image, segmentation results, confidence map, and detection results (with confidence values).

 An observation model that is based on a combination of soccer-specific features (grass color), match-
specific features (colors of team outfits), and universal features (HOG detector) resulting in a robust
confidence map for player positions (see sections 2 and 3 and Figure 1).

 An efficient measurement model that finds optimal locations on a confidence map starting at predicted

positions of single-target Kalman filters [1] (see section 4).

 An efficient player detection, avoiding time-consuming tracking-by-detection (see section 4).

In human tracking, the tracking-by-detection approach is widely used for multi-target tracking (see i.e. [2],
[3]). Human detectors often show low accuracy and poor computational performance for small objects, while
in soccer videos, especially in low-resolution records and wide-angle scenes, player heights of 40 pixels and
less are quite common. In contrast, kernel-based approaches, such as the mean-shift tracking [4], often require
an initialization of the templates and risk drifting away, due to object-specific template adaption.

Zhang et al. [5] proposed an online method with a combination of tracking-by-detection and kernel-based

tracking and achieved state-of-the-art results using standard pedestrian tracking data sets.

In principle, our approach is similar, but we use soccer-specific knowledge to automatically detect players
without an exhaustive sliding-window approach. Our observation model is partially based on the ideas of
ASPOGAMO [6], but incorporates more universal features, such as the HOG detector, and does not need any
human-guided initialization. It is similar to the method proposed by [7], which is, however, a stand-alone
player detection and does not incorporate tracking information over time.

2. Preliminaries

Player positions. In our approach, the position of a player (cid:1868)(cid:3036) is modeled within an image (cid:2165)(cid:2202) at time (cid:1872) using
an axis-aligned bounding box (cid:2158)(cid:2191) ≔ {((cid:1876), (cid:1877)) ∈ ℕ(cid:2870) | ((cid:1876)(cid:3036) ≤ (cid:1876) < (cid:1876)(cid:3036) + (cid:1875)(cid:3036)) ∧ ((cid:1877)(cid:3036) ≤ (cid:1877) < (cid:1877)(cid:3036) + ℎ(cid:3036))}, where ((cid:1876)(cid:3036), (cid:1877)(cid:3036))
is the upper-left corner and (cid:1875)(cid:3036) and ℎ(cid:3036) are width and height, respectively. A bounding box describes the extent
of a player in the image and in our method the aspect ratio is fixed to (cid:1875)(cid:3036) ≔ 0.41ℎ(cid:3036) (see [8]).

Player segmentation. Our confidence map is based on segmented player regions in the image and we use
a robust grass segmentation method following [9]. This procedure takes as input a RGB image (cid:2165)(cid:2202) at time (cid:1872) and
returns an image region (cid:2164)(cid:2202), describing the enclosing hull of the playing field, and a foreground region (cid:2162)(cid:2202),
which, under ideal circumstances, covers the image region of all players in the visible part of the playing field

32  

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

(see Figure 1). Due to noise and movement artifacts usually not all players are segmented properly, whereas
parts of other foreground objects, such as field lines, can be included.

Unsupervised generation of color templates. According to the FIFA rules [10], the colors of the jerseys
have to be chosen so that the players from different teams, the referees, and the goal keepers are all pairwise
distinguishable. We make use of this fact by initially determining color templates for each type of outfit,
based on color histograms.

Given the first frame (cid:2165)(cid:2777) of a video sequence, our automatic detection procedure (see section 5) returns
player positions represented by a set of (cid:1866)(cid:2868) bounding boxes (cid:2158)(cid:2777) ≔ (cid:3419)(cid:2158)(cid:2778)
(cid:2777) (cid:3423). We extract the set of RGB
(cid:2777) (cid:3439) ∩ (cid:2162)(cid:2777) and cluster this set by applying the k-means++
color vectors of (cid:2165)(cid:2777) for the pixels within (cid:3435)(cid:2158)(cid:2778)
algorithm [11] using the Euclidean distance. The cluster centers are the (cid:1863)(cid:3004) dominant color vectors that
represent the (cid:1863)(cid:3004) bins of our color histograms. A color vector is assigned to the bin that is represented by the
(cid:2777),
(cid:2777) to its equal-sized top, mid, and bottom parts (cid:2176)(cid:2158)(cid:2191)
nearest dominant color. We divide each bounding box (cid:2158)(cid:2191)
(cid:2777) ∩ (cid:2162)(cid:2777). This takes
(cid:2777) ∩ (cid:2162)(cid:2777), and (cid:2158)(cid:2158)(cid:2191)
(cid:2169)(cid:2158)(cid:2191)
into account that a player’s outfit changes from top to bottom, regarding jersey, shorts and socks. Stacking
and normalizing result in a feature unit vector with 3(cid:1863)(cid:3004) entries for each bounding box.

(cid:2777) and calculate three histograms with respect to (cid:2176)(cid:2158)(cid:2191)

(cid:2777) ∪ … ∪ (cid:2158)(cid:2196)(cid:2777)

(cid:2777) ∩ (cid:2162)(cid:2777), (cid:2169)(cid:2158)(cid:2191)

(cid:2777), and (cid:2158)(cid:2158)(cid:2191)

(cid:2777), … , (cid:2158)(cid:2196)(cid:2777)

To calculate the distance of two normalized histogram vectors (cid:2190)(cid:2778) and (cid:2190)(cid:2779), we use the Hellinger distance as

proposed by [4] and define (cid:1856)(cid:3009)((cid:2190)(cid:2778), (cid:2190)(cid:2779)) ≔ (cid:3493)(cid:1856)(cid:3009)

(cid:2870) ((cid:2190)(cid:2778), (cid:2190)(cid:2779)) with (cid:1856)(cid:3009)

(cid:2870) ((cid:2190)(cid:2778), (cid:2190)(cid:2779)) ∶= 1 − Σ(cid:3036)(cid:2880)(cid:2869)

(cid:2871)(cid:3038)(cid:3252)(cid:3493)(cid:2190)(cid:2778)((cid:1861))(cid:2190)(cid:2779)((cid:1861)).

We try to estimate the number of different outfits among the given bounding boxes by clustering the set of
histogram vectors using k-means++ with (cid:1863) = 1, … , 5 (there are five different possible outfits: 2 x players, 2 x
goalkeepers, 1 x referees) using the distance (cid:1856)(cid:3009). We choose (cid:1863)(cid:3040)(cid:3036)(cid:3041) as the smallest (cid:1863) for which the result of the
clustering fulfills our decision criterion: for a given set of (cid:1864) histogram vectors (cid:2190)(cid:2778), … , (cid:2190)(cid:2194) and their (cid:1863) cluster
centers (cid:2197)(cid:2778), … , (cid:2197)(cid:2193), each vector (cid:2190)(cid:2191) is assigned to its cluster center (cid:2197)((cid:2190)(cid:2191)) and we define the minimum distance as
(cid:1856)(cid:3040)(cid:3036)(cid:3041) ∶= min(cid:2919)(cid:1856)(cid:3009)((cid:2190)(cid:2191), (cid:2197)((cid:2190)(cid:2191))) and the maximum distance as (cid:1856)(cid:3040)(cid:3028)(cid:3051) ∶= max(cid:2919)(cid:1856)(cid:3009)((cid:2190)(cid:2191), (cid:2197)((cid:2190)(cid:2191))). With the threshold
parameters (cid:1872)(cid:3040)(cid:3036)(cid:3041) and (cid:1872)(cid:3040)(cid:3028)(cid:3051) , our decision criterion is ((cid:1856)(cid:3040)(cid:3036)(cid:3041) ≤ (cid:1872)(cid:3040)(cid:3036)(cid:3041)) ∧ ((cid:1856)(cid:3040)(cid:3028)(cid:3051) ≤ (cid:1872)(cid:3040)(cid:3028)(cid:3051)). During tracking each
tracked bounding box is assigned to the outfit class with nearest distance (cid:1856)(cid:3003) at the moment of detection.

HOG-based human detection. We trained a human detector based on histograms of oriented gradients
(HOG) according to [12] with some slight modifications to their default detector. We use a 64×128 pixels
detection window with a human size of 41×100 pixels (see [8]). To allow for an efficient calculation, we do
not apply a Gaussian spatial window during the accumulation of the histograms. To avoid soccer-specific
overfitting, the classifier is trained using the INRIA pedestrian data set [12].

3. Confidence map for player positions

(cid:2202) (cid:3423) of bounding boxes at time (cid:1872). We assign each bounding box (cid:2158)(cid:2191)

We construct a function that describes a degree of quality of the image evidence (cid:2165)(cid:2202), given a set (cid:2158)(cid:2202) ≔
(cid:2202) ∈ (cid:2158)(cid:2202) to the connected component
(cid:2202) , … , (cid:2158)(cid:2196)(cid:2202)
(cid:2202) to an empty region. Hence, the
(cid:2202) ∈ (cid:2175)(cid:2202) has a set of assigned bounding boxes
(cid:2202) (cid:3423) . In the following, fractions are only calculated if the denominator is not zero.
(cid:2202) and its assigned bounding boxes

(cid:3419)(cid:2158)(cid:2778)
of the foreground (cid:2162)(cid:2202) with the greatest overlap. If (cid:2158)(cid:2191)
result is a set of image regions (cid:2175)(cid:2202) ≔ {(cid:2174)(cid:2778)
(cid:3037)(cid:2158)(cid:3561) (cid:3047) ≔ (cid:3419)(cid:2192)(cid:2158)(cid:3561)
(cid:2778)
Otherwise, they are omitted. We perform all calculations for each region (cid:2174)(cid:2192)

(cid:2202) ∩ (cid:2162)(cid:2202) = ∅, we assign (cid:2158)(cid:2191)

(cid:2202) }, where each (cid:2174)(cid:2192)

(cid:2202) , … , (cid:2192)(cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202) , … , (cid:2174)(cid:2195)(cid:2202)

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

33

(cid:2202)) ∈ ℕ ∖ {0}. In most cases
independently. For the purpose of clearness we omit the index (cid:1862) and set (cid:1866)(cid:3019) ≔ (cid:1866)((cid:2174)(cid:2192)
(cid:1866)(cid:3019) = 1. Cases with (cid:1866)(cid:3019) > 1 occur for example if one player is partly occluded by another player in the image.
Region-based evidence. Our region-based features are inspired by the compactness constraint and the size
constraint of [6] and are based on the assumption that an image region of one or more players should be fully
covered by bounding boxes. The coverage feature (cid:1855)(cid:2869) describes the degree of coverage of the connected
component (cid:2174)(cid:2202) by the union of the bounding boxes and is defined as

(cid:1855)(cid:2869)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

, (cid:2174)(cid:2202)) ≔ area(cid:3435)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) ∪ … ∪ (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202) ) ∩ (cid:2174)(cid:2202)(cid:3439) /area((cid:2174)(cid:2202)).

The over-coverage feature (cid:1855)(cid:2870) penalizes regions inside the bounding boxes that are not covered by the
foreground region. To favor positions where (cid:2174)(cid:2202) is evenly spread along the horizontal axis, each bounding box
(cid:2202) and the lower half (cid:2194)(cid:2158)(cid:3561)
(cid:2202) is divided to the upper half (cid:2203)(cid:2158)(cid:3561)
(cid:2158)(cid:3561)
(cid:2202) and we define
(cid:2191)
(cid:2191)
(cid:2191)

(cid:1855)(cid:2870)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

(cid:2202)\(cid:2174)(cid:2202))/area((cid:2197)(cid:2158)(cid:3561)
, (cid:2174)(cid:2202)) ≔ 1 − max(cid:2919) max(cid:2925)∈{(cid:2931),(cid:2922)} area((cid:2197)(cid:2158)(cid:3561)
(cid:2202)).
(cid:2191)
(cid:2191)

The overlap feature (cid:1855)(cid:2871) takes into account that in general the bounding boxes of two players do not fully

overlap (except in the rare case of full occlusion). It is defined as

(cid:1855)(cid:2871)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

(cid:2202) ∪ (cid:2158)(cid:3561)
(cid:2202))/area((cid:2158)(cid:3561)
(cid:2202) ∩ (cid:2158)(cid:3561)
, (cid:2174)(cid:2202)) ≔ 1 − max(cid:2919),(cid:2920) area((cid:2158)(cid:3561)
(cid:2202)).
(cid:2192)
(cid:2191)
(cid:2192)
(cid:2191)

Color-based evidence. The color-based confidence (cid:1855)(cid:2872) is based on the dominant colors and the outfit
classes (see section 2). The normalized color histogram vector (cid:2190)((cid:2158)(cid:3561)
(cid:2202)) ∈ ℝ(cid:2871)(cid:3038)(cid:3252) is calculated with respect to
(cid:2191)
(cid:2202) is the largest axis-aligned inner ellipse of (cid:2158)(cid:3561)
(cid:2158)(cid:3561)
(cid:2202). Each
(cid:2202) ∩ (cid:2174)(cid:2185)
(cid:2191)
(cid:2191)
bounding box has an assigned outfit class with histogram vector (cid:2197)((cid:2158)(cid:3561)
(cid:2202)) ∈ ℝ(cid:2871)(cid:3038)(cid:3252) and we define
(cid:2191)

(cid:2202) ∶= (cid:2174)(cid:2202), if area((cid:2174)(cid:2202)) > 0. Otherwise, (cid:2174)(cid:2185)

(cid:2202). We set (cid:2174)(cid:2185)

(cid:1855)(cid:2872)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

, (cid:2174)(cid:2202)) ≔ 1 − (cid:3495)(cid:1998)(cid:3036)(cid:2880)(cid:2869)

(cid:3041)(cid:3267) (cid:1856)(cid:3009)(cid:3435)(cid:2190)((cid:2158)(cid:3561)

(cid:2202)), (cid:2197)((cid:2158)(cid:3561)
(cid:2202))(cid:3439)
.
(cid:2191)
(cid:2191)

(cid:3289)(cid:3267)

HOG-based evidence. We use a human detector, trained as described in section 2. A resized sub-image
with size of 64×128 pixels is generated for each bounding box (cid:2158)(cid:3561)
(cid:2202), so that the height of the bounding box
(cid:2191)
corresponds to 100 pixels in the sub-image, the sub-image has the same center point as the bounding box, and
the aspect ratio of the pixels remains unchanged. This sub-image is used to perform a classification of the
human detector. The result of the classifier is a decision value (cid:1856)(cid:3005)((cid:2158)(cid:3561)
(cid:2202)) ∈ ℝ, which is mapped using the
(cid:2191)
(cid:4593) (cid:3435)(cid:2158)(cid:3561)
(cid:2202)|(cid:1873)(cid:3005), (cid:1874)(cid:3005)(cid:3439) ≔ ((cid:1856)(cid:3005)((cid:2158)(cid:3561)
(cid:2202)) − (cid:1873)(cid:3005))/((cid:1874)(cid:3005) − (cid:1873)(cid:3005)) . The confidence is defined by the
parameters (cid:1873)(cid:3005) and (cid:1874)(cid:3005) to (cid:1856)(cid:3005)
(cid:2191)
(cid:2191)
geometric mean

(cid:1855)(cid:2873)(cid:3435)(cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

, (cid:2174)(cid:2202)(cid:3627)(cid:1873)(cid:3005), (cid:1874)(cid:3005)) ≔ (cid:3495)(cid:1998)(cid:3036)(cid:2880)(cid:2869)

(cid:3041)(cid:3267) max (1, min(0, (cid:1856)(cid:3005)

′ (cid:3435)(cid:2158)(cid:3561)
(cid:2202)|(cid:1873)(cid:3005), (cid:1874)(cid:3005)(cid:3439))
.
(cid:2191)

(cid:3289)(cid:3267)

Gating. During tracking we additionally incorporate a gating term, based on the distance to the predicted
location of the tracker (if available). For each bounding box (cid:2158)(cid:3561)
(cid:2202), we calculate the Euclidean distance of its
(cid:2191)
center point to the center point of the corresponding predicted bounding box (cid:2172)(cid:3561)
(cid:2202), normalized by the length of
(cid:2191)

34  

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

the diagonal of (cid:2172)(cid:3561)
the mean distance of the bounding boxes (cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

(cid:2202) and trimmed to a maximum value of 1. The gating confidence (cid:1855)(cid:2874) is defined by one minus
(cid:2191)

and their corresponding predictions.

Ensemble averaging. The single features (cid:1855)(cid:3036), (cid:1861) ∈ {1, … 6} are combined by weighted ensemble averaging
, (cid:2174)(cid:2202)), which we

using the weights (cid:1875)(cid:3036), (cid:1861) ∈ {1, … 6}, resulting in the overall confidence function (cid:1855)((cid:2158)(cid:3561)
(cid:2778)
define as follows:

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

(cid:1855)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

, (cid:2174)(cid:2202)) ≔ (cid:2001)(cid:1875)(cid:3036)(cid:1855)(cid:3036)((cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202)

, (cid:2174)(cid:2202))/(cid:2001)(cid:1875)(cid:3036).

Note that (cid:1855)((cid:2158)(cid:3561)
(cid:2778)
(cid:1875)(cid:2870) ≔ 0 and if there is only one object ((cid:1866)(cid:3019) = 1), we omit the overlap feature and set (cid:1875)(cid:2871) ≔ 0.

, (cid:2174)(cid:2202)) ∈ [0; 1] because ∀(cid:1861): (cid:1855)(cid:3036)(cid:3435)(cid:2158)(cid:3561)
(cid:2778)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

(cid:2202) , … , (cid:2158)(cid:3561) (cid:2196)(cid:2174)

, (cid:2174)(cid:2202)(cid:3439) ∈ [0; 1]. For (cid:2174)(cid:2202) = ∅, we set (cid:1875)(cid:2869) ≔ 0 and

(cid:2202)

(cid:2202)

Maximization of the confidence. For every new time step, we search for positions with a local maximum
on the confidence map near the Kalman predictions. To keep the number of calculations low, we propose a
simple greedy heuristic: we iteratively optimize the confidence with respect to the positions of the bounding
boxes, while their sizes and the region (cid:2174)(cid:2202) remain fixed, thus for (cid:1866)(cid:3047) bounding boxes, we have 2(cid:1866)(cid:3047) variables. In
each iteration of the maximization procedure, we evaluate the confidence map at 2(cid:1866)(cid:3047) + 1 positions near the
current position and determine the position with the highest confidence value. If this value is higher than the
value at the current position, the iteration continues, starting at the best position. Otherwise, the iteration stops
with the current position as the result. Usually, our efficient greedy approach stops after 2-3 iterations and
shows, however, satisfactory results.

The 2(cid:1866)(cid:3047) + 1 positions in the neighborhood arise as follows: first the gradient of (cid:1855) is approximated using a
finite forward difference (with step size Δ(cid:1876) ≔ 8), which results in 2(cid:1866)(cid:3047) evaluations of the confidence map
(instead of 4(cid:1866)(cid:3047) using central difference). Then one step (likewise with step size Δ(cid:1876)) in the direction of the
gradient is taken for the remaining evaluation.

Measurement of object size. Mainly due to perspective projection the size of a player in the image
depends on the image position. To enforce consistency, we normalize the sizes of the predicted bounding
boxes before the maximization step. For this purpose, we fit a linear model using a least-squares approach,
where the height ℎ is the dependent and the coordinate (cid:1877) is the independent variable. Afterwards, the height
of each bounding box is adapted according to this model and the width is determined by (cid:1875) ≔ 0.41ℎ (see [8]).
After the maximization step, for each bounding box (cid:2158)(cid:3561)
(cid:2202) we iterate several steps of scale with respect to the
(cid:2191)
original size and take the size with the best confidence value (cid:1855)(cid:3029)(cid:3032)(cid:3046)(cid:3047) as a resulting measurement, which results
in the final measurement quality (cid:1869)(cid:3435)(cid:2158)(cid:3561)
(cid:2202)(cid:3439) ≔ (cid:1855)(cid:3029)(cid:3032)(cid:3046)(cid:3047).
(cid:2191)

4. Player tracking

We perform multi-target tracking by applying a single-target Kalman filter [1] for each tracked bounding
box. A new measurement is generated by an optimization step with respect to the confidence map using the
Kalman prediction as the starting position. The detection of new and lost targets is performed with the help of
deterministic heuristics, which are guided by the confidence map.

State

and measurement model. The

six-tuple
(cid:1867)(cid:3036) ≔ ((cid:1876)(cid:3036) (cid:1877)(cid:3036) (cid:1876)̇(cid:3036) (cid:1877)̇(cid:3036) (cid:1875)(cid:3036) ℎ(cid:3036))(cid:3021), where (cid:1876)(cid:3036), (cid:1877)(cid:3036), (cid:1875)(cid:3036), and ℎ(cid:3036) are the parameters of the bounding box, and (cid:1876)̇ (cid:3036) and (cid:1877)̇ (cid:3036) are
the velocity in (cid:1876) and (cid:1877), respectively. We apply a constant size and constant velocity model. Thus, the changes

state of player (cid:1868)(cid:3036)

represented

by a

is

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

35

in size and the acceleration of the objects are implicitly modeled by the process noise. Measurements are
performed for the position and the size variables.

Loss and detection of players during tracking. During tracking a tracked bounding box (cid:2158)(cid:2191)

(cid:2202) is removed if
(cid:2202)) is below a threshold (cid:1872)(cid:3039)(cid:3042)(cid:3046)(cid:3046) in two frames in a row or if it has a significant
its measurement quality (cid:1869)((cid:2158)(cid:2191)
overlap with another tracked bounding box (with higher measurement quality) in more than four frames in a
row.

In return, connected components that fulfill some size constraints according to the linear size model are
considered as single player candidates and for these positions the confidence map is evaluated and
(cid:2202)) greater than or equal to a
maximized. Single player candidates that have a measurement quality (cid:1869)((cid:2158)(cid:2191)
threshold (cid:1872)(cid:3031)(cid:3032)(cid:3047)(cid:3032)(cid:3030)(cid:3047) in two frames in a row are added to the tracked bounding boxes.

Initial player detection. For the initial player detection, we select connected components of the
foreground region (cid:2162)(cid:2777) with an appropriate orientation and aspect ratio as single player candidates. In the
neighborhood of each candidate, we perform a sliding-window human detection with a scale space which
depends on the size of the candidate region. The positive detections are used to fit our linear size model. The
human detection is repeated in the neighborhood of all connected components of (cid:2162)(cid:2777) with a restricted scale
space depending on the position of the region in the image. Again, the positive detections are used to fit the
size model and to estimate the team histograms (see section 2). These detections are the starting point for the
first maximization step resulting in the initial player positions.

5. Implementation details and evaluation results

In our implementation we set the number of dominant colors (cid:1863)(cid:3004) ≔ 64 as an estimated upper bound for
five outfits with each three four-colored parts (jersey, short, socks). The other parameters are determined
empirically and set as follows: (cid:1872)(cid:3040)(cid:3036)(cid:3041) ≔ 0.1 , (cid:1872)(cid:3040)(cid:3028)(cid:3051) ≔ 0.25 , (cid:1873)(cid:3005) ≔ −4 , (cid:1874)(cid:3005) ≔ 0.5, (cid:1875)(cid:2869) ≔ 0.75 , (cid:1875)(cid:2870) ≔ 0.4 ,
(cid:1875)(cid:2871) ≔ 0.05, (cid:1875)(cid:2872) ≔ 1.0, (cid:1875)(cid:2873) ≔ 0.5, (cid:1875)(cid:2874) ≔ 0.05, (cid:1872)(cid:3039)(cid:3042)(cid:3046)(cid:3046) ≔ 0.35, and (cid:1872)(cid:3031)(cid:3032)(cid:3047)(cid:3032)(cid:3030)(cid:3047) ∶= 0.7. We evaluate our system with
the help of MOTA and MOTP [13] and use manually annotated ground truth of the following video
sequences:
 German – Holland, TV broadcast, SD, 37s, 925 frames (GH)
 Bayern Munich – OSC Lille, TV broadcast, HD, 15s, 752 frames (BL)
 ISSIA-CNR Camera 3, static camera, Full-HD, 120s, 3000 frames, online available [14] (ISSIA)
 VS-PETS Camera 3 Test, static camera, 720×576, 100s, 2500 frames, online available [15] (VS)

Ground-truth and tracked bounding boxes are fixed to an aspect ratio 0.41:1 as proposed in [8]. Some
annotated targets that are outside the field (like coaches and linesman), as well as ground-truth bounding
boxes that are cropped by the image boundaries, are added to an ignore list, i.e. they don’t need to be
matched, but it is not an error if they are matched.

Our baseline is the publicly available tracker of Zhang et al. [5], which achieves state-of-the-art tracking
results on pedestrian tracking datasets. We used the standard parameter values which come with the source
code, except the HOG_DETECT_FRAME_RATIO, which we set to 2 for BL and ISSIA, and to 3 for GH and
VS allowing to detect small players. As the detector of this tracking procedure generates a lot of false positive
detections in the audience area, we ignore all its tracked objects outside our field hull (cid:1834)(cid:3047) at time (cid:1872).

36  

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

Figure 2: MOTA (top row) and MOTP (bottom row) results (in dependence of the overlap threshold) of the proposed tracking procedure
in comparison with the procedure of Zhang et al. [5]. Results are given for each data set (GH, BL, ISSIA, VS) from left to right.

Figure 2 shows the comparison of the MOTA / MOTP of our proposed tracking procedure and the
procedure of Zhang et al. depending on the overlap threshold for the assignment of the tracked bounding
boxes to the ground truth. Our system achieves satisfactory MOTA scores for the standard overlap threshold
of 0.5. With increasing overlap threshold, the accuracy becomes remarkable (up to 0.9 and more). In all cases,
our results outperform the results of the baseline. Our non-optimized implementation processes about 1-2
frames per second on an Intel Core2 Quad Q9650. Because of its highly parallel structure, we believe that a
near real-time performance could be possible. In contrast, the method of Zhang et al. processes 0.1 frames per
second.

We provide a publicly available visualization of our tracking results for ISSIA and VS (see [16] and [17]).

6. Conclusion

We proposed an unsupervised online 2D tracking procedure for players in monocular soccer videos that
applies an efficient determination of local maxima in a confidence map. This map is based on a robust
combination of soccer-specific (grass color), match-specific (team outfit colors) and general (HOG detector)
image features. Avoiding a time-consuming sliding-window approach our system allows for a fast player
tracking that in addition does not require any further input, such as user input or camera parameters. Our
tracking results achieve high accuracy and outperform a state-of-the-art pedestrian tracker.

Acknowledgements

The funding for this research was provided by German Research Foundation (DFG) grant no. RA359/12-1.

References

[1] R. E. Kalman, “A New Approach to Linear Filtering and Prediction Problems.,” Transactions of the
ASME - Journal of Basic Engineering, vol. 82, no. 1, pp. 35–45, Mar. 1960.

 Michael Herrmann et al.  /  AASRI Procedia   8  ( 2014 )  30 – 37 

37

S. Gerke, S. Singh, A. Linnemann, and P. Ndjiki-Nya, “Unsupervised Color Classifier Training for

J. Zhang, L. L. Presti, and S. Sclaroff, “Online Multi-person Tracking by Tracker Hierarchy,” in IEEE

P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection: An Evaluation of the State of the

[2] M. D. Breitenstein, F. Reichlin, B. Leibe, E. Koller-Meier, and L. Van Gool, “Online Multiperson
Tracking-by-Detection from a Single, Uncalibrated Camera,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 33, no. 9, pp. 1820–1833, Sep. 2011.
[3] H. Izadinia, I. Saleemi, W. Li, and M. Shah, “(MP)2T: Multiple People Multiple Parts Tracker,” in
Computer Vision – ECCV 2012, vol. 7577, Berlin, Heidelberg: Springer, 2012, pp. 100–114.
[4] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based Object Tracking,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 25, no. 5, pp. 564–577, May 2003.
[5]
9th International Conference on Advanced Video and Signal-Based Surveillance, 2012, pp. 379–385.
[6] M. Beetz, S. Gedikli, J. Bandouch, B. Kirchlechner, N. von Hoyningen-Huene, and A. C. P. Perzylo,
“Visually Tracking Football Games Based on TV Broadcasts,” in 20th International Joint Conference on
Artificial Intelligence, 2007, pp. 2066–2071.
[7]
Soccer Player Detection,” in Visual Communications and Image Processing, 2013, pp. 1–5.
[8]
Art,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 743–761, Apr.
2012.
[9] M. Hoernig, M. Herrmann, and B. Radig, “Real Time Soccer Field Analysis from Monocular TV Video
Data,” in 11th International Conference on Pattern Recognition and Image Analysis (PRIA-11-2013),
Samara, 2013, vol. 2, pp. 567–570.
[10] FIFA, “Laws of the Game,” 2014. [Online]. Available:
http://www.fifa.com/aboutfifa/footballdevelopment/technicalsupport/refereeing/laws-of-the-game/.
[Accessed: 30-Apr-2014].
[11] D. Arthur and S. Vassilvitskii, “k-means++: the Advantages of Careful Seeding,” in 18th annual ACM-
SIAM symposium on Discrete algorithms, Philadelphia, PA, USA, 2007, pp. 1027–1035.
[12] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,” in IEEE Conference
on Computer Vision and Pattern Recognition (CVPR 2005), 2005, vol. 1, pp. 886–893.
[13] K. Bernardin and R. Stiefelhagen, “Evaluating Multiple Object Tracking Performance: The CLEAR
MOT Metrics,” EURASIP Journal on Image and Video Processing, vol. 2008, pp. 1–10, May 2008.
[14] T. D’Orazio, M. Leo, N. Mosca, P. Spagnolo, and P. L. Mazzeo, “A Semi-automatic System for
Ground Truth Generation of Soccer Video Sequences,” in 6th IEEE International Conference on Advanced
Video and Signal Based Surveillance, 2009, pp. 559–564.
[15] University of Reading, “VS-PETS Football Dataset,” Third IEEE International Workshop on Visual
Surveillance and Performance Evaluation of Tracking and Surveillance, 2002. [Online]. Available:
http://www.cvg.reading.ac.uk/VSPETS/vspets-db.html. [Accessed: 30-Apr-2014].
[16] M. Herrmann, M. Hoernig, and B. Radig, “Player Tracking Results ISSIA-CNR,” 2014. [Online].
Available: http://www.youtube.com/watch?v=L9t7ei6gAjk. [Accessed: 30-Apr-2014].
[17] M. Herrmann, M. Hoernig, and B. Radig, “Player Tracking Results VS-PETS,” 2014. [Online].
Available: http://www.youtube.com/watch?v=SL1LjRAbPgI. [Accessed: 30-Apr-2014].

