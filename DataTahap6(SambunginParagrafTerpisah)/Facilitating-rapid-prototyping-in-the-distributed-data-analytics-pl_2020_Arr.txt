OODIDA [1] is a modular system for concurrent distributed data analytics, with a particular focus on the automotive domain. It processes in-vehicle data at its source instead of transferring all data over the network and processing it on a central server. A data analyst interacting with this system uses a Python library that assists in creating and vali- dating assignment specifications which consist of two parts: on-board tasks carried out by the on-board unit (OBU) in a reference vehicle, and an off-board task that is executed on a central cloud server. Several domain-specific algorithms and methods of descriptive statistics have been implemented in OODIDA. However, updating this system is time-consuming and disruptive as it necessitates terminating and rede- ploying software. Instead, we would like to perform an update without terminating ongoing tasks. We have therefore extended our system with the ability to execute custom code, without having to redeploy any part of the installation. This enables users to define and execute custom computations both on client devices and the server. This is an example of a dynamic code update. With this feature, users of our system are able to carry out their work, which largely consists of either tweaking existing methods for data analytics or developing new ones, with much faster turnaround times, allowing them to reap the benefits of rapid prototyping.

In this paper, we describe the active-code replacement feature of OODIDA. We start off with relevant background information in Sect. 2, which includes a brief overview of our system. In Sect. 3 we cover the implementation of this feature, showing how Erlang/OTP and Python interact. We elaborate on the reasoning behind our design consider- ations, including deliberate limitations, and show how it enables rapid prototyping. Afterwards, we show a quantitative as well as a qualitative

A condensed version of this paper has been previously published [2]. That paper presents a quick summary of the active-code replacement feature of the OODIDA platform. In contrast, this paper provides both more depth, covering various implementation details and extensive technical background, as well as increased breadth by giving a more thorough description of relevant parts of our system.

In this section, we describe the relevant background of active-code replacement in the OODIDA platform. We start with a brief overview of OODIDA (Sect. 2.1), including a description of assignment specifica- tions and the user front-end application, before we show how our system can be extended with new computational methods (Sect. 2.2). This leads to the motivating use case that describes on-the-fly updating of the sys- tem without taking any part of it down (Sect. 2.3).

The problem our system solves is that data generation of a connected vehicle outpaces increases in bandwidth. There is simply too much data to transfer to a central server for processing. Instead, with OODIDA, data is primarily processed on clients and in real time, which leads to cost savings as transmission and storage costs can be greatly reduced. Furthermore, data analysts can get insights a lot faster, which is valuable for business.

Assignments consist of an on-board task, performed on a central server, and an off-board task, performed by each client that is contained in the selected subset of clients. An example of an assignment specifi- cation is provided in Listing 1.1, which shows an example of a relatively basic assignment and its definition as an object in Python. The on-board task is executed on the chosen subset of clients, and the off-board task on the central cloud server. The provided example shows an instance of anomaly detection. The entire fleet of vehicles is monitored, with the goal of detecting whenever a vehicle exceeds a speed threshold value of

Unfortunately, this is a potentially disruptive procedure, not even taking into account potentially long-winded software development pro- cesses in large organizations. OODIDA has been designed with rapid prototyping in mind, but although it can be very quickly deployed and restarted, the original version cannot be extended while it is up and running. This was possible with ffl-erl [3], a precursor that was fully implemented in Erlang, which allows so-called hot-code reloading. There are some workarounds to keep OODIDA up-to-date, for instance by automatically redeploying it once a day. As we are targeting a compar- atively small fleet of reference vehicles, this is a manageable inconve- nience. Yet, users interacting with our system would reap the benefits of a much faster turnaround time if they were able to add computational methods without restarting any of the nodes at all.

The verification process of the user front-end application consists of two steps. First, the provided module has to be syntactically correct, which is done by loading it in Python. The second check targets the prescribed function custom_code. That function is called with the ex- pected input format, depending on whether it is called on the client or the cloud. We also verify that the returned values are of the expected type. If any of these assertions fail, the assignment is discarded. Otherwise, the custom Python module is sent to the cloud or to clients, depending on the provided instructions. This step is preceded by producing another assignment specification that, in either case, contains the entries user_id and custom_code. The value of the latter is an encoding of the user-provided Python module. The value of the key mode is either deploy_offboard or deploy_onboard. Once custom code has been deployed, it can be referred to in assignments by setting the value of the keys onboard or offboard to custom.

task specifications are sent to the designated client processes. Each client process spawns a task handler for the current task. Its purpose is to monitor task completion, besides alleviating the edge process from that burden and enabling it to process further task specifications concur- rently. In our case, the task handler sends the task specification in JSON to an external Python application, which turns the given code into a file, thus recreating the Python module the data analyst initially provided. The name of the resulting file also contains the ID of the user who pro- vided it. After the task handler is done, it notifies the assignment handler and terminates. Similarly, once the assignment handler has received re- sponses from all task handlers, it sends a status message to the cloud node and terminates. The cloud node sends a status message to inform the user that the custom code has been successfully deployed. Deploying custom code to the cloud is similar, the main difference being that b0 commu- nicates with the external Python worker application running on the cloud.

Computations are performed only after the specified amount of data has been gathered. This implies that a custom code module can be safely replaced as long as data collection is ongoing. The case where an update collides with a function call to custom code is discussed in Sect. 3.4. If a custom on-board or off-board computation is triggered by the keyword custom, Python loads the user-provided module using the function reload from the standard library. This happens in a separate process using the multiprocessing library. The motivation behind this choice is to enable concurrency in the client application as well as to avoid some technical issues with reloading in Python, which would retain definitions from a previously used custom module. Instead, our approach creates a blank slate for each reload.

It is possible that a new custom code version arrives at the same time the client application wants to load it. This is one example where the standard approach would be to roll back the update and instead use the previous custom code version. However, this scenario is less of a concern for us as we replace computational methods instead of system-level software. There is deliberately no mechanism for a rollback as the old version of the custom code was supposed to be replaced by new custom code, which implies that any results that could be generated by the old code instead of the not-yet-available new one are not of any interest to the user anymore. The only exception where a rollback would be helpful correct inputs and outputs are consumed and produced. What may not be immediately obvious, however, is that we can now even create ad hoc implementations of the most complex OODIDA use cases, an example of which is federated learning [8]. A key aspect of federated learning, compared to many standard types of assignments on our system, is that the results of one iteration are used as the input of the next one. The original implementation is discussed in the paper on OODIDA [1]. With federated learning, clients update machine learning models, which the server uses as inputs in order to create a new global model. This global model is the starting point for the next iteration of training on clients.

The main benefit of active-code replacement is that code for new computational methods can be deployed right away and executed almost instantly, without affecting other ongoing tasks. In contrast, a standard update of the cloud or client installation necessitates redeploying and restarting the respective components of the system. In order to quanti- tatively evaluate the performance difference, we executed OODDIA in an idealized scenario where the user and server were executed on one workstation each and one client device on another. We deployed iden- tical code to both the client and the server and took the average of five runs. The standard deployment procedure assumes that a minimal Linux installation with all necessary third-party applications and libraries is available. A complete re-installation would take considerably more time. The total amount of data that needs to be transferred in order to deploy OODIDA is around 1 MB. This is zipped data.

Custom code that is to be deployed remotely is turned into a payload for an assignment, which is a lightweight JSON format. We took a standard real-world example consisting of 20 lines of Python source code. The source code file has a size of 0.45 KB. When turned into the payload of a JSON assignment specification, the size of the file to be sent via the network is around 0.52 KB. The amount of custom code used may seem like a rather small amount of code, but this is beyond typical assignments that consist largely of glue code, tying together various calls to scikit- learn methods. These are often in the order of 10 lines of code or even below that.

is not the case that this approach fully sidesteps the need to update the library of computational methods on the cloud or on clients as OODIDA enforces restrictions on custom code. For instance, some parts of the Python standard library are off limits. Also, the user cannot install external libraries. Yet, for typical algorithmic explorations, which users of our system regularly conduct, active-code replacement is a vital feature that increases user productivity far more than the previous comparison may imply. That being said, due to the limitations of active- code replacement, it is complementary to the standard update procedure rather than a competitive approach.

Starting with MUC, the major difference is that this system exclu- sively targets cloud-only updates, while active-code replacement also addresses a potentially very large number of client devices. In MUC, each update leads to forking a new process that is executed concurrently. These processes are synchronized to ensure consistency. In contrast, in OODIDA, assignments are executed concurrently. Yet, as there is no continual processing, our problem is simplified. We thus replace code at any point but only execute it the next time it is called, which is after a batch of results has arrived. There is no need to concurrently execute an old version of the code as it would taint the state a computation if we combined results that were arrived at with non-identical code.

procedure than updating custom code for data analytics. In addition, Polus operates in a multi-threading environment instead of the highly concurrent message-passing environment of OODIDA. This is a funda- mental difference, which means that the approach of Polus could not be easily replicated in OODIDA. In essence, we spawn a separate process that uses custom code. In order to replicate the approach of Polus, we would need to update the existing process, which would make sand- boxing much more difficult. In our case, a spawned process handler that crashes due to illegal custom code, which should not happen to begin with, cannot take down the entire system. Instead, this process is terminated and the custom code discarded.

In addition to plans for future work on the OODIDA platform in general, we also have concrete ambitions for active-code replacement. The feature, as described, works as intended. Yet, we did point out some limitations and workarounds. The guiding idea is that the execution of custom code should be safe. One way of achieving this is by placing re- strictions on the provided code, such as only allowing certain return types. As of now, we only allow numerical values or lists of numerical values. This is sufficient for a very large number of data analytics tasks. Yet, in order to make active-code replacement more useful, we should extend the set of allowed return types. Related is the problem that we exclude certain Python libraries from being called. A further investiga- tion is needed in order to determine if a more fine-grained approach would make sense.

A major limitation of the current iteration of active-code replacement in OODIDA is that the user is not allowed to deploy additional libraries. However, as this is a potentially very useful feature, we would like to explore the possibility of creating sandboxed environments on the client, perhaps a solution based on lightweight containers via Docker [16]. This also refers back to an earlier note (cf. Sect. 3.7) on the current difficulties of reproducing deployments with custom code [17]. In fact, the idea of deploying a custom lightweight container to the client, which contains a custom sandboxed environment, seems promising. While this would arguably make custom deployments take more time, it would still be incomparably faster than the standard workflow for updating the client installation, as that is a decision the user cannot take by themselves and may take days or weeks, depending on organizational processes.

prototyping, largely achieved by automated deployment of the client installation. In reality, however, organizational bureaucracy and the demands of industry best-practices slow down this process a lot. In contrast, with the help of the active-code replacement feature, we pro- vide a safe and easy-to-use alternative that sidesteps such hurdles. Of course, we had to make some concessions by placing limitations on user- defined custom-code modules. Nonetheless, active-code replacement is eminently useful in practice as it enables data analysts working with OODIDA to pursue a highly interactive workflow, given how quickly custom code can be deployed on the system.

