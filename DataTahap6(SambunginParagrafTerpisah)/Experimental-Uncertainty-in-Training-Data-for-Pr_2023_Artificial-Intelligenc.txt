The accuracy of machine learning models for protein-ligand binding affinity prediction depends on the quality of the experimental data they are trained on. Most of these models are trained and tested on different subsets of the PDBbind database, which is the main source of protein-ligand complexes with annotated binding affinity in the public domain. However, estimating its experimental uncertainty is not straightforward because just a few protein-ligand complexes have more than one measurement associated. In this work, we analyze bioactivity data from ChEMBL to estimate the experimental uncertainty associated with the three binding affinity measures included in the PDBbind (Ki, Kd, and IC50), as well as the effect of combining them. The experimental uncertainty of combining these three affinity measures was characterized by a mean absolute error of 0.78 logarithmic units, a root mean square error of 1.04 and a Pearson correlation coefficient of 0.76. These estimations were contrasted with the performances obtained by state-of-the-art machine learning models for binding affinity prediction, showing that these models tend to be overoptimistic when evaluated on the core set from PDBbind.

Scoring functions can be classified into two main groups: classical and machine-learning based scoring functions [4,5]. The first group assumes a functional form to establish the relationships between fea- tures characterizing a protein-ligand complex and its binding affinity (force fields, empirical scoring functions, statistical potentials). In contrast, machine-learning based scoring functions learn the connection between the features describing the system and its binding affinity through a machine-learning algorithm. Machine-learning scoring func- tions have shown to outperform classical scoring functions regarding protein-ligand complexes with annotated binding affinity in the public domain, there are a couple of problems that cannot be overlooked. On the one hand is its documented bias towards preferred crystallographic targets [10], that will diminish only as more data become available. On the other hand, is the fact that the binding affinity data for distinct protein-ligand complexes comes from different sources (assays, labora- tories, experimental conditions), leading to an inherent experimental uncertainty, which in principle delimits the achievable performance of the machine-learning models trained on such data.

Trying to estimate this experimental uncertainty in the PDBbind database is not straightforward because just a few protein-ligand com- plexes have more than one measurement associated and the bias to- wards preferred crystallographic targets is also present. An alternative is to estimate the experimental uncertainty by analyzing data from larger bioactivity databases where no structural information of the protein- ligand complexes is available, such as ChEMBL [11]. In [12], for instance, the authors analyze the experimental uncertainty on Ki mea- surements from ChEMBL version 12. However, data from the PDBbind database includes not only Ki, but also Kd and IC50 measurements, and most machine-learning models derived from such data treat them as the same target variable / endpoint. Although this is a reasonable approxi- mation, combining different types of binding affinity estimation would only increase the uncertainty of estimations.

Single measures of binding affinity. The partially curated dataset was split into three subsets according to the binding affinity measure included (pKi, pKd or pIC50). In the context of each subset, to exclude data coming from citations of previously reported values, if a protein- ligand pair had the same bioactivity value reported multiple times, only one was preserved. Finally, only protein-ligand pairs with at least two independent measures were considered for further analysis.

Multiple measures of binding affinity. To study the effect of combining binding affinity measures in the experimental uncertainty, subsets containing pairwise combinations between binding affinity measures were generated (pKi-pKd, pKi-pIC50, pKd-pIC50). For consistency with the previous methodology, if a protein-ligand pair contained more than one measurement for an affinity measure, only the highest affinity was preserved, and only protein pairs with at least one measurement of each affinity measure were considered for further analysis. Finally, a dataset containing data from the three binding affinity measures was built (pKi- pKd-pIC50), which represents the worst-case scenario where all possible affinity measures are combined. In this last case, protein pairs with measurements for at least two affinity measures were considered for further analysis.

models can be contrasted with the Rp and RMSE reported herein. As the training data for these models comes from the PDBbind database, which combines pKi, pKd, and pIC50 as affinity measures, the closest estima- tions of their achievable performance are in principle the statistics calculated for the pKi-pKd-pIC50 dataset. The Rp and the RMSE values reported for these models are higher than those for the dataset herein built, especially for Rp. These findings indicate that machine learning models tend to be overoptimistic when assessing the general behavior of binding affinity values for protein ligand pairs (high Rp) but the indi- vidual predictions are already very close to the experimental uncertainty observed for the pKi-pKd-pIC50 dataset. It is worth noting that the comparison between these models and the herein obtained statistics is not straightforward, as the size and composition of the dataset where they are computed is different. So, considering the size of the core set from PDBbind and its known biases, it is not surprising to obtain such statistics.

The experimental uncertainty of binding affinity measures from different sources was estimated. Our results suggested that combining pKd data from distinct sources results in the lowest experimental un- certainty, with a MAE of 0.69 logarithmic units, a RMSE of 1.03, and an Rp of 0.76. Combination of distinct binding affinity measures brings about an increase in uncertainty, where the lowest uncertainty was associated to the combination of pKi and pIC50 measures, with a MAE of achieving predictions with almost experimental precision. This agrees with the results obtained by modeling different levels of noise on syn- thetic numerical data and comparing how well these levels of noise are detected by distinct metrics. Such study suggests that Rp is not sensitive enough to the error present in the data, as modelled data with up to 50% of noise ratio achieve Rp of up to 0.80. According to that, metrics such as the Fractional Gross Error, the Variance Accounted For, and the Mean Normalized Bias seem as promising choices for estimating the perfor- mance of predictive models, as they showed to be more sensitive to the noise present in the data [21]. We hope this brief study will contribute to the ongoing discussion within the scientific community about the limits and applicability of machine learning models for binding affinity pre- diction trained on heterogeneous binding affinity data.

