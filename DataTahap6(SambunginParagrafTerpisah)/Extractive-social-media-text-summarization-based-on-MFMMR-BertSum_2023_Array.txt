The advancement of computer technology has led to an overwhelming amount of textual information, hindering the efficiency of knowledge intake. To address this issue, various text summarization techniques have been developed, including statistics, graph sorting, machine learning, and deep learning. However, the rich semantic features of text often interfere with the abstract effects and lack effective processing of redundant information. In this paper, we propose the Multi-Features Maximal Marginal Relevance BERT (MFMMR-BertSum) model for Extractive Summarization, which utilizes the pre-trained model BERT to tackle the text summarization task. The model incorporates a classification layer for extractive summarization. Additionally, the Maximal Marginal Relevance (MMR) component is utilized to remove information redundancy and optimize the summary results. The proposed method outperforms other sentence-level extractive summarization baseline methods on the CNN/DailyMail dataset, thus verifying its effectiveness.

In the era of big data, the rapid development of social media constantly supplies a bulk of information. Text content, as a dominant medium in social media, is an efficient approach to conveying real- time news and opinions. However, the abundance of descriptions and interpretations often obscures the concrete opinion in a content, hin- dering the timely acquisition of vital information. Text summarization is a technique used to condense long text into a shorter abstract while retaining its original meaning [1]. Automatically generating key information from massive text can significantly improve efficiency compared to traditional manual summarization [2]. Automatic text summarization methods can be divided into two categories based on the relationship between the abstract and the original text: extractive summarization and abstractive summarization [3]. Extractive sum- marization extracts keywords from the source document to form a summary. However, this approach may result in a final summary that lacks coherence between sentences and may contain redundant information. While abstractive summarization generates new words to form a summary based on the content of the source document.

do not take contextual information into account. The development of deep learning techniques has facilitated breakthroughs in natu- ral language processing. The BERT model [7] is a pre-trained model that has been trained on large-scale datasets, demonstrating powerful generalization capabilities. BERT uses word-level inputs while extrac- tive summarization is a sentence-level task. Therefore it is impossible to fine-tune the BERT pre-trained model directly for automatic text summarization tasks.

The emergence of deep learning technology has revolutionized the field of extractive text summarization, bringing about significant progress and advancements. Liu [12] applied deep learning to the field of text summarization for the first time and proposed a text summa- rization method based on RBM. The emergence of pre-trained models like BERT [7] has brought natural language processing into a new era. BERT uses the encoder part of the Transformer as the main framework of the model. Through the joint adjustment of the context of each layer to predict the deep bidirectional representation, capturing the bidirectional context relationship in the statement. The BertSum model proposed by Liu [13] is the first BERT-based text summarization model. Some modifications have been made to the embedding of the BERT model for the purpose of extractive summarization. Yuan [14] added the hierarchical graph mask to BERT to make full use of the structural information between different semantic levels and extract the semantic units at the fact level to obtain a better summary. Srikanth [15] used the K-means algorithm to cluster the sentence representations output by the BERT model and introduced a dynamic method to determine the appropriate number of sentences from the cluster. Ma [16] propose a topic-aware extractive and abstractive summarization model based on

The pre-trained model BERT boasts semantically-rich representation capabilities that effectively address challenges such as polysemy and long-distance dependencies in natural language processing. The input representation of the BERT model consists of token embeddings, seg- ment embeddings, and position embeddings. The body of the model is a multi-layer bidirectional transformer structure and the output can be trained on downstream tasks through fine-tuning by connecting it to neural networks.

the sentence score term and redundancy term weights are best assigned. Furthermore, the ROUGE score changes very little under different word vector dimensions. Considering that a word vector with too large a dimension increases the complexity of the model and thus the running time, it is more appropriate when the word vector dimension is taken as 100. Through this experimentation, we determined that the MFMMR algorithm yielded the best summary performance with a hyperparame-

ment in the alignment with reference summaries across different sliding window scales, word order, and sentence structure, thus validating the effectiveness of the proposed MFMMR-BertSum model. Meanwhile, the hybrid model SummaRuNNer-PGN achieves better results than the separate one. Accordingly, we speculate that hybrid models combining the advantages of extractive and abstractive summarization have plenty of potential for growth.

In this research paper, we propose the MFMMR algorithm, con- sidering multiple features in the conventional MMR sentence scoring process. Significantly mitigates the adverse effects of relying on single feature for calculating sentence scores. The MFMMR-BertSum model is proposed by modifying the input representation of Bert and adding a classification layer with MMR components to reduce the redun- dancy problem in extractive summarization. Tests were conducted on the CNN/DailyMail dataset, and the results indicate MMR-BertSum has significant improvements compared to the baseline approach on

Junqing /an received his Ph.D. degree from China Univer- sity of Geosciences. He is currently an Associate Professor at the School of Computer Science, China University of Geosciences. He is a CCF member. His research interests include Data Mining, Natural Language Processing, High Performance Computing, and Optimization.

Yuewei Wang received an M.S. degree in Computer Sci- ence and Technology in 2018 from China University of Geosciences, Wuhan, China, where he is currently working toward a doctoral degree in Geographic Information System. His research interests include Machine Learning, Big Data Analyze, Digital Earth, and High Performance Computing.

