Neoplasms are defined by abnormal tissue growth and can be classi- fied as benign or malignant. Benign (noncancerous) neoplasms are char- acterized by slow and organized spread, the presence of well-defined borders, and the absence of an invasive character at both the tissue and organ levels. Malignant (cancerous) neoplasms, on the other hand, are characterized by often rapid and disorganized growth, with poorly de- fined borders and possible invasion of adjacent tissues and organs, im- plying the possibility of metastatic cancer [1,2].

The original dataset contains four categories regarding the last avail- able information about the patient: 1) alive without cancer, 2) alive with cancer, 3) death from cancer, and 4) death without further infor- mation. Our outcome of interest was patients with a confirmed cancer death between 12 and 24 months after the date of diagnosis. For the negative outcome, we included patients 1) alive without cancer or 2) alive with cancer between 12 and 36 months after the date of diag- nosis. Patients were removed if categorized as 4) death without other information.

For quantitative variables, we performed normalization using the z- score (separately in training and test). For all qualitative variables, we separated each category using one-hot encoding. The variable type of di- agnosis presented categorized missing (value 9) for twenty-four patients. We considered this a new category for the one hot encoding procedure. We also removed 75 patients due to the lack of any information in two variables: cancer stage (two patients) and difference in days between first medical appointment dates and diagnosis (73 patients).

We tested the predictive performance of six different machine learn- ing algorithms: catboost [10], xgboost [11], lightgbm [12], gradient boosting classifier, random forest, logistic regression. For catboost, xg- boost and lightgbm, we used their own Python packages. For the other algorithms, we used the scikit-learn library [13].

We used 10-fold cross-validation to select the hyperparameters in the training set with Hyperopt [14], which applies a Bayesian strategy for optimization, and RandomSearch. In the case of high class imbalance (minority class representing under 25% of total outcomes), we applied the Synthetic Minority Oversampling Technique (SMOTE). Also in the training set, we applied the BORUTA [15] method for variable selection. We then selected the best performing models from the training set (70% of the data) to evaluate their performance in the test set (30%).

The predictive performance of the models was evaluated on the test set using metrics such as area under the ROC curve (AUC-ROC), area un- der the precision-recall curve (AUC-PR), precision, recall, positive pre- dicted value, negative predicted value, and F1-score. We also evaluated the performance of the algorithms in the 20% highest risk patients (20% k-tops), with metrics such as true positive, false positive, precision and recall. Finally, the interpretation and evaluation of the contribution of each variable to the outcome was obtained by calculating the Shapley values [16,17,18] for the test set. We followed the guidelines of the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD) [19].

nine models. All models presented AUC-ROC values of at least 0.871 and six of them reached values above 0.900. The general model presented the best overall performance, with AUC-ROC of 0.946 and AUC-PR of 0.932. The model for the five main causes of death (top-5) presented an AUC-ROC of 0.945 and an AUC-PR of 0.937.

tion (code 12, UNACON with Hematology and Pediatric Oncology Ser- vices). A second patient (b) classified as a false negative was selected. The total risk score was 0.4782, which led the algorithm to classify the patient incorrectly as alive during the period. We observed that there was balance in the aggregate of the contribution of the predictors, high- lighting the importance of cancer stage IV to increase Shapley value and the non-public health service to decrease it. For patient c, a true negative classified as low risk, the most important characteristic to a low expected Shapley value were cancer stage I and non-public health

We found that all models achieved an AUC-ROC higher than 0.86 to predict cancer mortality using only routinely-collected data. Our results also indicated that a general algorithm, that included all cancer mortal- ity, performed in most cases better than cancer-specific algorithms.

We obtained a high predictive performance without the use of omics or image data, which is a promising result in the field of oncology espe- cially in low-income regions. We were able to develop an approach to compare the use of a general model with a model for the main causes of death, and with models specific to each type of cancer. Most of the studies developed in the literature are specific for a given type of cancer, due to the selected data sets, using either from image data or through structured data [20,21]. The use of data from a cancer registry allowed for achieving consistent results in all proposed models, while at the same time providing a real-world dataset, with recent cases and dif- ferent types of cancer.

In conclusion, the nine final models developed for predicting risk of death in cancer patients presented high predictive performance. The al- gorithms can be an important tool to help prioritize treatment decisions and patient allocation in cancer treatments, especially in low-income re- gions. Future work should explore the proposed methodological struc- ture and evaluate its predictive performance in new settings with differ- ent routinely collected data.

