In the area of NLU, a few concurrent methods were proposed based on continuous prompts, fo- cusing on improving knowledge probing (Qin and Eisner, 2021; Zhong et al., 2021). Lester et al. (2021) showed that with large pretrained models, only tuning continuous prompts with a frozen lan-

In this paper, we present a method P-Tuning that uses continuous prompts in concatenation with dis- crete prompts. P-Tuning improves performance and stabilizes training for pretrained language model adaptation. P-Tuning is effective with both tuned and frozen language models under both the few-shot and fully-supervised setings.

