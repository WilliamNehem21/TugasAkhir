This empirical paper examines the time delays that occur between the publication of Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD) and the Common Vulnerability Scoring System (CVSS) information attached to published CVEs. According to the empirical results based on regularized regression analysis of over eighty thousand archived vulnerabilities, (i) the CVSS content does not statistically influence the time delays, which, however, (ii) are strongly affected by a decreasing annual trend. In addition to these results, the paper contributes to the empirical research tradition of software vulnerabilities by a couple of insights on misuses of statistical methodology.

Software vulnerabilities are software bugs that expose weak- nesses in software systems. The CVSS standard is used to classify the severity of known and disclosed vulnerabilities. Once the clas- sification and evaluation work has been completed for a vulnera- bility identified with a CVE, the structured and quantified severity information is stored to vulnerability databases. Moti- vated by a recent empirical evaluation [16], this paper examines the time delays between the publication of CVEs and the usually later publication of CVSS information. The scope is restricted to NVD and the second revision of the CVSS standard.

The use of CVSS is mandated and recommended by many state agencies for assessments in different security-critical domains [36], including but not limited to medical devices [38] and the payment card industry [2]. The standard has been also incorporated into different governmental security risk, threat, and intelligence systems. Furthermore, CVSS information is used in numerous different commercial products [16], ranging from vul- nerability scanners and compliance assessment tools to automated penetration testing and intrusion detection systems.

CVSS is also widely used in academic research. Typical applica- tion domains include risk analysis [2,14], security audit frame- works [4], so-called attack graphs [7,26], and empirical assessments using CVSS for different purposes [1,25,31,33]. To these ends, a lot of work has been done to improve CVSS with different weighting algorithms [17,40], among other techniques [9,30]. With some rare exceptions [13], limited atten- tion has been given for examining how severity assessments are done in practice.

software engineering, MITRE presumably maintains a backlog for the CVEs assigned, some of which may be even rejected for inclu- sion to NVD. Although the structure of the backlog is unknown, a simple FIFO (first-in, first-out) might be considered in order to con- nect the speculation to a recent theoretical work [10]. In any case, eventually the vulnerabilities accepted for archiving are published in NVD. In parallel to the coordination and archiving work related to CVEs, vulnerabilities are evaluated for their severity by the NVD

According to the empirical results, only the answer to RQ1 is positive. For predicting the time delays, the CVSS content is largely noise. The statistical effect (RQ2) also fades away once the annual trend is controlled for (RQ3). To elaborate how these conclusions are reached, the remainder of this paper is structured into three sections. Namely: Section 2 introduces the dataset and the opera- tionalization of the variables used, Section 3 outlines the statistical methodology and presents the empirical results along the way, and Section 4 finally discusses the findings.

excluded. The same applies to CVEs without severity records. At the time of retrieving the NVD content [27], there were 2,218 vul- nerabilities that were published but still lacked CVSS records. Most of these cases relate either to new vulnerabilities that are still in the pipeline for severity assessments, or to already published CVEs

Two types of covariates are used for modeling the time delays in (1). The first contains the CVSS information itself. The CVSS (v. 2) standard [6] classifies the impact of vulnerabilities according to confidentiality, integrity, and availability (CIA). Each letter in the CIA acronym further expands into three categories that characterize the impact upon successfully exploiting the vulnera- bility in question. Thus, the analytical structure behind the impact dimension can be illustrated with a diagram: possibly regardless of the impact upon confidentiality, integrity, and availability. There exists also some empirical evidence along these lines [1]. However, the impact and exploitability dimensions both relate to intrinsic characteristics of vulnerabilities; they are constant across time and environments. For instance, EXPLOIT- ABILITY cannot answer to a temporal question about whether an exploit is known to exists for the vulnerability in question [30,43]. The same point extends toward NVD in general [8]. For these and other reasons, the new (v. 3) standard for CVSS enlarges the dimensions toward temporal and environmental metrics.

The three impact metrics measure the severity of a vulnerability on a system after the vulnerability has already been exploited. However, not all vulnerabilities can be exploited; therefore, the CVSS standard specifies also an exploitability dimension for vul- nerabilities. Like with the impact dimension, exploitability expands into three metrics (access vector, complexity, and authen- tication) that can each take three distinct values. The analytical meaning can be again summarized with the following diagram: so-called dummy variables. For each metric, the reference category is marked with a star in the previous two diagrams. For instance, INTEGRITY is expanded into two dummy variables, INTEGRITY (PARTIAL) and INTEGRITY(COMPLETE), say, the effects of which are compared against INTEGRITY(NONE), which cannot be included in the models due to multicollinearity. The same strategy applies to the metrics used for evaluating RQ1. Namely, the annual

The strong decreasing trend is likely to support a positive answer to the research question RQ1. Given this prior expectation, the main interest in the forthcoming analysis relates to the statis- tical effect of the impact and exploitability metrics when also the annual trend is modeled. One strategy for evaluating the research question RQ3 is to compare the models M1 and M2 against the full information model M3. If the CVSS metrics provide statistical power for predicting D, this power should be visible also when the decreasing annual trend is controlled for.

the joint significance of the dummy variable groups with a F-test, all groups are significant at a p < 0.001 level. Also the combined forward-stepwise and backward-stepwise algorithm (as imple- mented in the step function for R) retains all coefficients in b^a .

This short empirical paper examined the time delays that affect CVSS scoring work in the context of NVD. Three research questions were presented for guiding the empirical analysis based on regres- sion methods. The results are easy to summarize. The CVSS content is correlated with the time delays (RQ2), but the correlations are spurious; the decreasing annual trend affecting the time delays (RQ1) also makes the effects of the CVSS content negiligle (RQ3). Three points are worthwhile to raise about the significance of these empirical findings.

First, the negative answers to RQ2 and RQ3 are positive findings in terms of practical applications using CVSS information. Whether the application context is governmental security intelligence sys- tems or commercial security assessment tools, there is currently no particular reason to worry that a NVD data feed would show significant delays for the CVSS information. Likewise, in 2017, there is no reason to suspect that information for severe vulnera- bilities would tend to arrive later (or earlier) than information for mundane vulnerabilities. However, this conclusion does not

NIST, NVD Data Feed and Product Integration, National Institute of Standards and Technology (NIST), Annually Archived CVE Vulnerability Feeds: Security Related Software Flaws, NVD/CVE XML Feed with CVSS and CPE Mappings (Version 2.0), 2017a. Retrieved in 23 September 2017 from: <https://nvd. nist.gov/download.cfm>.

