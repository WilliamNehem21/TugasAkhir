But, the FP growth algorithm has performance and scalability issues [3]. The FP-Tree, for very large data sets, will not fit in memory, making it difficult to process Big Data. A distributed and parallel implementation of the FP growth algorithm is needed to scale to large data sets. Hadoop stated some parallelization techniques based on shared memory SMP architectures for data mining algorithms. Using these techniques, they parallelized the FP growth algorithm and scaled it to process data- sets of few hundred megabytes (MBs). Similarly, [12] used two tech- niques: (i) a cache-conscious FP-array, which is a data reorganization of the FP tree and, and (ii) lock-free FP-tree construction, for parallelizing the FP growth algorithm on multi-core processors. These techniques aim to solve problems like poor data locality and insufficient parallelism of data mining algorithms on multi-core processors. [13] implemented a multithreaded solution on the multi-core CPU and GPU. Both imple- mentations were based on FP-arrays [12].

FiDoop-DP [21] balances the load on the reducer nodes by using a voronoi based partitioning technique. The partitioning technique almost creates clusters of transactions and mines the clusters in parallel in different reducers. FiDoop-DP not only aims to balance the load of the reducers, but also avoids duplicate transactions in the intermediate data. But FiDoop-DP needs a pre-processing step. Their experimental results do not account for this preprocessing step and hence the overhead of the pre-processing step is not known.

the fList by the number of items per group. Hence, complexity of grouping in MPFP is O(1). Though MPFP does not spend much time in grouping, it creates unbalanced groups. This penalizes some of the reduce tasks by slowing down the entire MapReduce job.

The FP-Growth algorithm is implemented as a MapReduce job. It is similar to the Parallel FP Growth step of the MPFP algorithm. The fList and the map containing the groups are copied to all map tasks and reduce tasks using the distributed cache feature of the MapReduce framework. Every map and reduce task reads them from the distributed cache. The group map is used in the map task to determine the group id for a given item, and is used in the reduce task to determine the items that belong to a given group id.

dataset, was built from about 1.7 million web documents. The html tags and the most common words (stop words) were filtered out of the web documents. Each transaction represents a web document and the items in the transactions represent the set of distinct terms that appear in that document. The resulting dataset was 1.48 GB in size, containing 1,692, 082 transactions with 5,267,656 distinct items. The maximal length of a transaction was 71,472.

For example, for the reduce memory size of 8 GB, both BPFP and HBPFP successfully complete the jobs for the minimum support threshold of 6.5%, and HBPFP runs faster than BPFP for all minimum support thresholds except the 10% and 10.5% thresholds. MPFP cannot handle large datasets for minimum support thresholds of 6.5%, 7% and 7.5%, even at the 7 or 8 GB reduce task memory size. At the reduce task memory size of 7 GB, BPFP could not even handle large datasets of minimum support thresholds of 7%.

At the low 3 GB reduce task memory size, both HBPFP and MPFP ran successfully at a minimum support threshold of 11%. However, HBPFP ran faster than both BPFP and MPFP. BPFP could handle large datasets for minimum support thresholds of 11% and 11.5%.

Vavilapalli VK, Murthy AC, Douglas C, Agarwal S, Konar M, Evans R, Graves T, Lowe J, Shah H, Seth S, Saha B. Apache hadoop yarn: yet another resource negotiator. In: Proceedings of the 4th annual symposium on cloud computing; 2013 Oct 1. p. 5.

