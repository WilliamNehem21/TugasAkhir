Large and Deep Convolutional Neural Networks achieve good results in image classification tasks, but they need methods to prevent overfitting. In this paper we compare performance of different regularization techniques on ImageNet Large Scale Visual Recognition Challenge 2013. We show empirically that Dropout works better than DropConnect on ImageNet dataset.

Visual object recognition is one of the most challenging problems in Computer Vision, especially in large scale and realistic settings, with high resolution images and thousands of object categories. Until recently neural networks were not widely used for this task, because they need a lot of labeled data and computational power to train. Now, with the advance of fast GPUs and big labeled image datasets, they can be used efficiently, and, moreover, they can beat other methods. Neural networks potentially have fairly large learning capacity, which can be controlled by the number and size of layers, so they can adapt to very big problems. Best results can be obtained with deep neural networks, because depth is essential for learning good internal representations of input data. Large neural networks suffer from the problem of overfitting, so there is a need for powerful regularization techniques like data augmentation (Krizhevsky et al., 2012), Dropout (Hinton et al., 2012) or recently introduced DropConnect (Wan et al., 2013). Another way to improve performance of neural networks is inserting some prior knowledge like awareness of 2D structure of input data. One type of such networks is Convolutional Neural Networks. Because of their structure they have fewer learnable parameters than standard fully-connected neural networks, so they are easier to train and less suffer from overfitting.

The specific contribution of this paper is comparing performance of DropConnect (which, to our knowledge, was evaluated only on small datasets) and Dropout and improved Data Augmentation in large scale settings - on ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013). We started with architecture close to proposed by (Krizhevsky et al., 2012), which is a winner of ILSVRC2012, and explored how it works with other regularization methods instead of Dropout. Also we proposed several ways to improve results.

Our results show that Dropout regularization works better than DropConnect for ImageNet classification task. Also we discovered that our network was too small, and for getting better results we need to use larger network with better regularization techniques. We think that results can be improved by using new methods like DropPart (Tomczak, 2013), standout (Ba and Frey, 2013), maxout (Goodfellow et al., 2013), Stochastic Pooling (Zeiler and Fergus, 2013b), DLSVM (Tang, 2013), Lp Units (Gulcehre et al., 2013) or channel-out (Wang and Jaja, 2013) and some data augmentation techniques.

