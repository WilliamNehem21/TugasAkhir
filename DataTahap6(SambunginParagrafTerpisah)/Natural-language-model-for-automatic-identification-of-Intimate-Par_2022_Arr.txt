class distribution in the annotated dataset was highly imbalanced, with only 668 posts (~11%) labeled as IPV- report. We then developed an effective natural language processing model to identify IPV-reporting tweets automatically. The developed model achieved classification F1-scores of 0.76 for the IPV-report class and 0.97 for the non-IPV-report class. We conducted post-classification analyses to determine the causes of system errors and to ensure that the system did not exhibit biases in its decision making, particularly with respect to race and gender. Our automatic model can be an essential component for a proactive social media-based intervention and support framework, while also aiding population-level surveillance and large-scale cohort studies.

depression, anxiety, PTSD, dissociation, and anger [6,7]. Physical health outcomes include injuries, sexually transmitted diseases, back and limb problems, memory loss, dizziness, and gastrointestinal conditions. Fe- male victims could experience undesirable reproductive outcomes, including miscarriages and gynecologic disorders. From an economic perspective, the approximate IPV lifetime cost is $103,767 per female victim and $23,414 per male victim; a population economic burden

Conventionally, IPV-related data have been collected through sur- veys and medical/police reports [17,18]. It has become challenging to obtain IPV-related data from these traditional sources during the COVID-19 pandemic because IPV victims, for example, were reluctant to see healthcare providers due to the fear of contracting the virus [17,18]. Social media websites (SMWs), such as Twitter and Reddit, can poten- tially get around the pandemic-induced obstacles by collecting data online. More than 4.5 billion individuals use SMWs globally, many of whom use it for extended periods [19]. SMWs have become a new communication tool for people to express their thoughts, emotions, opinions, and discuss daily problems, regardless of geographical local- ity. During the COVID-19 pandemic and lockdown, SMWs have become emotions, and details of daily lives with their friends and family mem- bers [20]. SMWs may have particular utility to IPV victims because they tend to share their sensitive information more with friends (64%) and family members (49%), but less with healthcare providers (26%), police (23%), and shelter advocates (20%) [21,22]. Also, SMWs have been shown in past research to be excellent sources for collecting live data precisely, at scale (i.e., a large number of observations), anonymously,

the population and individual levels [25]. Moreover, during the COVID-19 pandemic, the United Nations urged to increase investment in online technology and civil society organizations to build harmless means for women to find help and support without informing their abusers, and accordingly reduce domestic violence [12]. SMWs, where users can share anonymous postings, may serve as secure platforms compared to other means (e.g., phone call, email) for offering instru- mental, informational, and social support to IPV victims. These advan- tages of SMWs may complement (not substitute) the conventional resources (e.g., hotline, shelter) and strengthen the effort to prevent IPV and support IPV victims.

To the best of our knowledge, there has been no study employing social media analytics (e.g., natural language processing (NLP), machine learning) to identify IPV victims on SMWs for surveillance, prevention, and/or intervention. Therefore, this study aims to develop an auto- mated, social media-based system to detect and categorize streaming IPV-related big data (into IPV and non-IPV cases) during the COVID-19 pandemic on Twitter via NLP and machine learning.

An effective NLP pipeline, including an automatic machine learning classifier, will help develop a surveillance system for IPV self-reported on SMWs. Such a pipeline may be used to detect and proactively reach out to large numbers of IPV victims simultaneously to provide support instead of waiting for them to seek help. Also, the pipeline will lay a foundation to potentially deliver evidence-based, non-contact in- terventions to IPV victims on SMWs for IPV prevention, mental health treatment, empowerment, and support [26].

We conducted the annotation process iteratively over small datasets (~200 tweets). Guided by the definition of IPV and a domain-expert (SK), the annotators discussed disagreements in the early annotations until consistent coding rules were reached. With the finalized annotation guidelines, the annotators encoded the final dataset (n = 6,348) used for training and testing our NLP model. The gold-standard dataset was developed once reliable levels of agreement between the annotators

We investigated three different approaches for constructing an IPV classifier. We used three traditional machine learning algorithms: deci- sion tree [29], support vector machine (SVM) [30,31], and neural net- works (NN) [32]. We also experimented with more advanced algorithms for text classification, including a deep learning-based algorithm, namely, bi-directional long short-term memory (BiLSTM) [33,34] and two transformer-based models, namely, Bidirectional Encoder Repre- sentations from Transformers (BERT) [35,36] and Robustly Optimized BERT (RoBERTa) [36].

Deep learning model: We converted each word into a corresponding word vector then fed it into the BiLSTM classifier. For a word to vector conversion, we used the Twitter GloVe word embeddings trained on 2 billion tweets and 27 billion tokens, including a 1.2 million-word vo- cabulary. We used uncased GloVe embedding with 200-dimension vectors [38].

their personal experiences of handling violent acts, such as restraining orders (5.5%) and seeking help from police (3.3%). A comparison of male and female self-reports showed that the number of tweets mentioning ex-husbands is three times as many as those about ex-wives, although the distributions of their mentions are similar in the full data set.

The RoBERTa classifier achieved the best performance compared to other models, and the performance was almost comparable to human annotators (based on the IAA). This suggests that the developed systems can be practically employed for collecting and analyzing IPV data at a larger population level. Findings from the learning curve analysis illustrate the usefulness of the pretrained model (e.g., RoBERTa) in learning from a small dataset. The pretrained model continues improving in performance with additional training data but with a slow learning rate between 80% and 100%. It is likely that further manual annotation of data will improve performance, but the rate of improve- ment for the IPV-report class will be slow. The classification outcomes and word importance analyses show that our system is not biased to- ward certain gender or race groups. However, this result does not necessarily mean that our developed model is bias-free, which is difficult to claim without a large test data that contains more information. Rather, we intended to ensure that the model at this stage was not making classifications based on gender- or race-related words, as well as to find the best strategies to improve the quality and diversity of training datasets in the future.

Our model tends to misclassify tweets that contain implicit self- reports of IPV. Future work may include strategies to enrich the classi- fication with new training datasets that address this issue. Enriching the training data can be planned to mainly boost the number of tweets with similar characteristics to those misclassified in the first training round. The classifier also often misclassifies tweets due to words and symbols captured in the training data. Annotating a larger data set for training is also likely to resolve some of the performance issues related to these. It must be noted, however, that manual annotation is an extremely time consuming process and this acts as a limitation to intelligent system development.

Although not identified through content analysis, we suspect the insufficient contexts in the description of the events as possible reasons for misclassifications. Twitter as a micro-blogging platform allows only a limited number of characters in one tweet. Therefore, users often attempt to embed all their experiences in short sentences, forgoing detailed contexts that would be useful for accurate classification into either IPV-report or not. Users often overcome this limitation by posting a series of tweets (thread) explaining their IPV (or other) experiences.

The dataset used to pre-train the RoBERTa or the BERT model mainly contains a corpus of books (800 million words) and English Wikipedia. The way people express themselves in SMWs is different from the texts in books [37]. Efforts have been made to pre-train BERT-related models on social media. However, previous research showed that the RoBERTa model based on traditional text still performs better than these models in several social media NLP tasks [51,52]. A potential direction may be combining traditional data with social media data to have a hybrid model built with a large text corpus and social media language.

Although IPV victims often reach out for support and intervention through social media channels such as Twitter, there is little effort to use such platforms to address this public health problem. Posts about IPV are typically lost in the massive volume of data constantly posted on social media. Developing an effective, low-biased, and trustworthy model for classifying self-reported IPV in social media has significant practical applications. This study developed and evaluated an NLP pipeline to collect and classify posts from the Twitter platform to identify IPV- related tweets. Our NLP pipeline achieved comparable performance to humans and was shown to be particularly not biased to gender or race- related words. By identifying IPV victims on Twitter, our model will lay the groundwork to design and deliver evidence-based interventions, and support to IPV victims and potentially enable us to address the problem of IPV in close to real-time via social media.

MAA, SK and AS designed the experiments MAA, SK, YCY, YG, EW, SL and AS conducted the data collection, analysis, and evaluations. MAA, SK and AS conducted manuscript writing. MAA, SK, EW, and AS helped interpret relevant findings, discussed and analyzed the results.

