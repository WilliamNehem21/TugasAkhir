Nowadays, it is impossible to imagine advances in medicine without talking about artificial intelligence. The incredible amount of data from different sources, such as medical images, data from clinical examinations, sensors and many others, outperforms by far the human capacity to process and analyse them [1]. For example, an average radiologist technician analyses about 215,000 radiography in about 40 years, while an artificial intelligence method processes that amount in about an hour [2].

What became attractive from the MLP was its ability to be a universal classifier that adapts to different distributions, features and complexities of data [12]. This quality is highly desirable in the medical field, considering that medical data can have noise, be imbalanced in the distribution of the classes and can have errors of registration [13]. The MLP effectiveness depends on its learning process, which iden- tifies the weights and biases values that minimise the classification error of training samples. The Back Propagation Algorithm (BP) is the standard way to make the MLP learn. However, BP has several weaknesses that can lead to a divergence in the MLP learning process, such as, a tendency to get stuck in local optima or dependency on initial the optimisation process is applied in continuous search spaces [24,25]. Nevertheless, evolving large-dimensional solutions could be challeng- ing for genetic operators, causing the optimiser to diverge. To address this problem is necessary to design new genetic operators that, by using external information such as the stage of the evolutionary process, can speed up and enhance the convergence of the algorithm [26].

In this paper, the CGA is used as an alternative to finding the optimal weights and biases of the MLP for medical data classification. A new crossover operator, called Damped Crossover (DX), is proposed to improve the performance of the CGA at traversing the search space. The purpose is to obtain a reliable method for training the MLPs and improving classification quality. The main contributions of this work can be summarised as follows:

Learning is the process through which the ANN acquire knowledge. It is why they can perform and be effective in classification and regression tasks. In MLP, learning is reached by training the neural network, which is an iterative process for determining the optimal weights and biases to reduce the error between the obtained and the expected output.

Although conventional approaches have shown to be effective in most of the problems they were applied to, there were situations in which they stuck in the same error value of MLP during extended peri- ods or even stuck in local optima. Furthermore, their success strongly depends on the initial values of weights, the values of momentum and the learning rate, which can provoke divergence if they are not right defined [14]. Finally, conventional methods put aside biases, focusing just on the values of the weights [15,36].

The work by Kaveh et al. [38] uses the Biogeography-Based Op- timisation algorithm (BBO) to train an MLP that classifies sonar data into three different classes: noises, reverberation, and clutter. A novel mutation operator is introduced in this work to enhance the exploration capability of the BBO. Results have demonstrated that the proposal of new operators can positively impact the behaviour of the algorithm, increasing the resulting classification performance. Qiao et al. [39] also worked with sonar data, proposing a modified Whale Optimisation

Algorithm (WOA) to train an MLP that classifies sonar signals in real- time. This work introduces new ways to control the balance between exploration and exploitation during the evolutionary process using mathematical functions. This approach shows how the mathematical approach could help guide the seeking process of the metaheuristics in large search spaces. Results show that the proposal outperforms literature algorithms in terms of classification accuracy and speed of convergence.

Mansouri et al. [41] implement a GWO hybridised to an Evolu- tionary Strategy (ES) algorithm to train an ANN to detect unusual sensor networks behaviour. The GWO is used when accuracy is critical, and ES is used when it is preferred to perform quickly detections. Results demonstrated that both approaches perform as expected, being the ANN capable of accurately recognising anomalies in industrial sensor networks. It also shows that different metaheuristics can provide distinct behaviour tailored to the particular context of the problem.

In [43], a Butterfly Optimisation Algorithm (BOA) is used to train an MLP. Results showed that the BOA reached a performance similar to the existing approaches. Tests have just focused on the Parkinson and vertebral dataset, which suggests that different complexities and distributions of data need to be considered to confirm that the method can train the MLP effectively in complex situations. Furthermore, the author used the canonical version of the BOA, which suggests that bet- ter results would be achieved if a specific modification to the algorithm is proposed. Another bio-inspired algorithm in the literature is the pro- posal by Das et al. [44] in that a Velocity Enhanced Whale Optimisation Algorithm (VEWOA) trains an ANN to classify data related to breast cancer, cervical cancer, and lung cancer. The VEWOA raises that each whale has to have a velocity, calculated as in PSO, where positions of the best and the previous positions of particles are considered. Results were compared against different machine learning approaches and the canonical WOA, making it difficult to observe whether this approach enhances the performance of metaheuristics specifically designed for MLP training.

Bhattacharjee in [48] proposes five different hybridisations between GA and PSO to train an MLP that classifies human glioma from molec- ular brain neoplasia data. This paper provides an interesting point of view about how PSO and GA can be combined and which combination provides the best results. This work established that hybridisations between PSO and GA can report good results due to the synergetic effect generated.

After analysing all these contributions, it seems clear that very few approaches design and evaluate new operators to improve numerically the search along with the problem space. In this paper, a CGA approach is proposed to address the problem of optimising the weights and biases of the MLP. The idea is to take advantage of the properties of the CGA for better exploration and exploitation, such as the slow spread of the best solution and exploitation in neighbourhoods. In addition, a novel specially designed crossover operator is proposed. The aim is to make the evolutionary process more accurate as it runs and to consider the best solution in neighbourhoods that do not have it. To the best of our knowledge, no presented work has encompassed the components of this paper.

During the evolutionary process, where the genetic operators are applied, individuals can interact just with their neighbours. Neighbours are the closest individuals determined by a type of neighbourhood, considering the Manhattan distance. Through the use of neighbour- hoods, CGA is able to conduct a local search process within each one, which facilitates the discovery of better near solutions (exploitation of the search space). It is possible because genetic operators are applied to neighbourhoods as they were isolated from the whole population. In addition, neighbourhoods are superposed, which implies that an individual takes part in more than one neighbourhood. Thus, an in- dividual spreads the improvements in its genes throughout all of the neighbourhoods it belongs to. This quality provokes the slowly spread of better solutions through the population, enhancing the exploration process.

The DX operator is based on two premises. The first one is that the knowledge acquired by the best individual during the evolutionary process is essential. So, this information must be considered when parents are crossed. The second premise is that influence from parents and the best solution has to be more specific as the evolutionary process runs because it is supposed that the solutions are near-optimal at the latest iterations.

The training dataset aims to present the greatest amount of samples to the MLP to identify the optimal weights and biases values and obtain a reliable classification model. It is the only dataset involved in the training phase in traditional methods, so it is also used for the training process made by metaheuristics [14,37]. The test dataset is used when the metaheuristics have finished their training phase to corroborate the ultimate performance of classification reached by the MLP, observe its generalisation ability and confirm whether there was or not over-fitting.

Algorithms are executed in the Toko cluster1 with an AMD Opteron/ Epyc processor (64 cores and 128 GB of RAM). The operating sys- tem is Ubuntu 18.04 LTS. Metaheuristics are implemented using the jmetalpy [57] library, and for MLPs the neurolab2 library is utilised.

ones with the aim of another bird to breed them. If the host bird realises that an egg is not its own, the host can destroy the egg or leave the nest. The algorithm mimics this behaviour, considering that a cuckoo egg is laid in a nest if it is better than the egg in the nest.

Genetic Algorithm (GA) [51] is a metaheuristic based on the Darwinian theory of the evolution of species. In GA, a population evolves by iterations called generations. Three genetic operators are applied. The selection operator selects a set of individuals to go on to the next generation or be recombined by the crossover operator. The crossover operator exploits the shared space of two individuals. Finally, the mutation operator performs random changes to increase the diversity over the population.

Particle Swarm Optimisation (PSO) was proposed by Kennedy et al. [69]. PSO works imitating the behaviour of different organ- isms like bird flocking. It begins generating a swarm of particles distributed in the search space. At each iteration, the position and velocity of each particle are updated according to its previous position and the position of the best particle.

This section begins with an analysis of the numerical performance of metaheuristics. Then, a comparison of the time consumed in seconds to reach the stop criteria is presented. Tests are made by applying the algorithms to the five considered benchmarks datasets during the 30 independent runs.

(Breast and Diabetes) and has had the second-best mean accuracy in two other datasets (Parkinson and Vertebral). The GWO has emerged as the best solution with Liver, Parkinsons and Vertebral datasets. Because the algorithms that achieved the best fitness value did not reach the best accuracy with these datasets, it is evident that there is no relationship between fitness value and accuracy. As can be seen, all the metaheuristics have obtained very similar results. In particular, CGA with DX crossover has proven to achieve competitive accuracy results with all the datasets.

classifying positive and negative samples. Metaheuristics performed better at determining negative samples, suggesting they could learn patterns from a few samples (48 samples). But, the performance with positive samples was poor, which might have been related to noise in the data. In particular, BP appears to better tolerate noise in the Parkinson dataset.

a better classification. GA and GWO shared the best result of specificity. All the previous analyses indicate that CGA using the DX crossover offers better capabilities for exploring and exploiting solutions than the other approach, which enhances the classification ability of the MLP. It is necessary to remark that all the metaheuristics overcame the typical

with the UM mutation achieved better results than the other algorithms and reached minimal fitness values. Considering the times consumed by each algorithm, the DX variations were the best in four out of five datasets, overwhelming even to the CGA with other crossover operators. It is reliable proof that the DX operator performs its task efficiently, making the CGA work quicker.

the-art algorithms. These results suggest that the optimisation process depends on the fitness function definition, because minimal values of MSE do not necessarily imply better accuracy values. Despite that, these results confirm that DX variations can have a featured performance in optimising weights and biases of the MLP, being able to improve the classification.

Besides, metrics of classification quality showed that solutions CGA variations with DX crossover get competitive results of specificity and sensitivity, deriving in a level of learning and generalisation com- parable to other approaches. Specifically, DX variations highlight in the Breast dataset by reaching the second-best result. With the other datasets, the performance was very similar to the state-of-the-art algo- rithms.

Zeleznik R, Foldyna B, Eslami P, Weiss J, Alexander I, Taron J, Parmar C, Alvi RM, Banerji D, Uno M, Kikuchi Y, Karady J, Zhang L, Scholtz J-E, Mayrhofer T, Lyass A, Mahoney TF, Massaro JM, Vasan RS, Douglas PS, Hoffmann U, Lu MT, Aerts HJWL. Deep convolutional neural networks to predict cardiovascular risk from computed tomography. Nature Commun 2021;12(1). http://dx.doi.org/10.1038/s41467-021-20966-2.

