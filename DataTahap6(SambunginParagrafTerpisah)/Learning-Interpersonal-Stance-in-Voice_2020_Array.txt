It is not totally clear how people react to others attitudes. Neverthe- less, an influential theory that was initially proposed by Timothy Leary defines eight basic stances that can be performed by humans as well as the interpersonal dynamics associated with those eight basic stances [8]. Knowing how we react to each basic stance is useful to improve our re- lations in our social life and at work.

In [1], certain nonverbal signals, in particular, body posture, gestures and facial expressions, are inputs to measure interpersonal stances, with the aim of using this in the context of a police interrogation training game. The results show that professional actors are better at acting the different stances than non-actors. Furthermore, the authors found that, in general, it is very difficult to act a stance and that observers often disagree about which stance is acted. In linguistics, researchers have explored interpersonal stances in written and spoken interactions.

Ranganath et al. [11] developed a system to detect interpersonal stances in prosody using four classes: flirtatious, friendly, awkward, and assertive. They used prosodic, lexical and dialogue features of non-acted prosody fragments. Several analyses were applied, including gender performance, comparison of lexical and prosodic features and a

Other works focused on developing systems for training people to improve their skills in interpersonal stances. In Ref. [13], the authors built a simulator-training game to teach interpersonal stances by using text inputs. Unfortunately, the accuracy of the system is low, which limits the training effect of the game.

The simulation-based training game comprises two parts. The first part is the software interface in which the users can record voice samples. The software guides the user to record sentences. It indicates which sentences have to be spoken and gives some general hints on how to perform them.

Based on the recorded samples, unique voice models of each user are generated and used in the software interface. The model is built on a server through an automatic process that uses an SVM algorithm. See Ref. [4,5] for more details about the backend server, the algorithm, and how the model is generated.

were collected from 20 English speakers. Their average age was 28 years with a standard deviation of 8.69; 75% was male and 25% female. From this population, 30% were recruited in the university and 70% via the crowdsourcing platform Prolific. Four of the participants had already used the software several times before the experiment. These participants will be called experienced participants. Sixteen participants did not have any experience with the software before the experiment. These partici- pants will be called non-experienced participants.

In summary, with two precise attempts, one can reach the border of the circumplex and if it is the right category, one achieves the goal. The goal is positioning the computer status (green dot) into the big green circle. The user has eight attempts to do that. If the user performs well, in two attempts he/she achieves the goal. The left window shows some examples of sentences to be spoken and general hints to guide users on how to perform each stance.

After the button (STEP 2- Training) is pressed, the software records the users voice and sends the fragment to the server. The server is responsible for classifying the voices stance based on the personal model of that specific individual (which was recorded in part 1, and is identified by an exclusive hash code). A SVM algorithm running in the server classifies the submitted voice fragment and calculates a probability score for each category.

In accordance with previous works on inferring interpersonal stances in voice [4,5,11]and text [12,13], it is difficult to build models that are able to accurately classify the eight basic stances, at least with the current datasets. It is even more difficult to teach people using such models, as also shown in Refs. [9].

All those aspects of performing interpersonal stances should be considered in follow-up research to explore how people understand each of the categories. In future work, we are planning to extend the experi- ment to more participants to confirm the tendencies presented in this work and investigate the relation between the stances in more detail. More evidence needs to be found indicating that interpersonal stances in voice can be improved by using simulation-based training games and we need to better understand how people perceive and use interpersonal stances in practice. It is also important to determine to what extent people really recognise them as separate categories when they are interacting with others or whether they group them in clusters.

