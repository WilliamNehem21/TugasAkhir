Texture of an image/frame contain unique visual patterns. Texture features describes the object surface, these features are independent of object color Hu et al. (2011). The videos of flowers consist of large intra class variation such as variation in color of flowers. Therefore, to describe the flower region, texture features play a vital role. In this work, texture features namely, Gray Level Co-occurrence Matrix and Local Binary Pattern are used.

Gray level co-occurrence matrix (GLCM). GLCM describes the tex- ture of flower in terms of statistical information. In the current work, the system extracts 14 different gray level co-occurrence of statistical values (Haralick et al., 1973) are extracted from each FRoI. These features are represented as a feature vector.

LDA is a supervised dimensionality reduction method (Belhumeur et al., 1999). Ronald Fisher in 1936 proposed discriminant analysis, to find a new feature space from original feature space. LDA plays a vital role in order to maximize class separability and preserves the within

Support vector machine (SVM) is a computationally powerful tool for supervised learning (Kumar and Gopal, 2011, and Khan et al., 2012). Support vector machine is a vector-space-based classification method for both linear and non-linear data. The fundamental idea of SVM classifier is to find the optimal separating hyperplane between two classes. For more information please refer (Vapnik (1998) and Duda et al., 1997).

In (Jyothi et al., 2018), authors have proposed a flower video re- trieval system using deep leaning approach, here the similar videos for a given query video are retrieved using Multiclass Support Vector Ma- chine. For the extraction of features in (Jyothi et al., 2018), authors have proposed three different modalities; entire keyframe, segmented flower region of a keyframe, and the gradient of flower region are con- sidered for feature extraction using Deep Convolutional Neural Network

Explanation: In the present work, when a video consists of one class of flowers in some duration and then other classes in next duration. In such case, we manually split (cut) the video into shots. To overcome this drawback a video shot/class boundary detection is essential.

