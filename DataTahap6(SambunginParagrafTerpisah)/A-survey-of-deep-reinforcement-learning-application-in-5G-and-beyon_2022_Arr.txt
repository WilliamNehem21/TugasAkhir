a trial and error manner [35], learning from its actions to improve re- wards. That way, it resolves the need for a priori traffic profiles and addresses InP utility maximization challenges. However, it often per- forms poorly in large state-action spaces, referred to as the curse of dimensionality. When combined with DNNs, this challenge can be addressed. That way it addresses feature extraction and optimal policy search challenges expected in multi-domain and joint multi-dimensional

torical foundations of the RL domain and the core issues. The study in Ref. [30] investigates the role of the Bayesian method in the RL para- digm, the detailed Bayesian RL algorithm, and their theoretical and empirical properties. The study in Ref. [31] provides a comprehensive overview of the RL and Deep RL methods and their application in eco- nomics. Inverse Reinforcement Learning (IRL) is considered in Ref. [32]. IRL uses data generated during the execution of a task to build an autonomous agent that can model other agents without impacting the performance of the task. Preference-based reinforcement learning (PbRL), which learns from non-numerical rewards, is reviewed in Ref. [33].

As Deep RL continues to gain interest in the research community, there is an ever-growing number of publications on its applications in wireless networks. Recent research on cognitive, autonomous, intelli- gent 5G networks and beyond is summarized in Ref. [54]. The authors in Refs. [45,55] review AI and ML applications in the design and operation of 5G and beyond networks. The authors in Ref. [56] outline research publications from 2009 to 2018 can be found in Ref. [36]. The authors of [70,71], and [72] provide comprehensive tutorials on RL and Deep RL. The recently published tutorial in Ref. [63] provides a comprehensive overview of 5G network slicing and virtualization, its technical re- quirements, and information on 3GPP standardization efforts. A comprehensive summary of useful resources and resources specific to the RL and Deep RL frameworks is summarized in Section 3.6.

The existing review papers outlined above already addressed some of our research questions. However, our paper goes beyond these previous studies, with a particular focus on research associations between Deep RL and 5G network slicing. Our paper differs from existing studies in the following ways: (a) rather than focusing on Deep RL uses in the high- level 5G network slicing context as presented in Refs. [48,56], we focus on details surrounding its application in key network slicing functionalities, critical to InPs and slice tenants such as slice topology, resource allocation, admission control, forecasting and prediction.

(b) We review major RL and Deep RL platforms that can be used to develop and train RL and Deep RL agents, including environments that support testing agents that solve mobile network communication problems. (c) We review recent advances in Deep RL research and introduce readers to recent cutting-edge technologies that help re- searchers improve their RL/Deep RL agent training. (d) This paper fo- cuses on Deep RL in network slicing for 5G networks and beyond. However, relevant to this paper, we also discuss the potential use of Deep RL in wireless communication domains in general. (e) We outline useful tutorials, books, papers, and other research materials, as well opensource management and orchestration platforms that will enable readers gain meaningful insights in the intellectual foundations of RL/ Deep RL and at the same time quickly learn how to develop AI agents that operate freely in a 5G network slicing environment. The discussions will be summarized in tabular format. Lessons learned in each section will also be provided.

the updated state and reward to select the next action. This loop is used in episodes to learn how to get the most out of each one and repeated until the environment is terminated. The feedback can come directly from the environment, for example, a numerical counter in a visual environment, or as the result of a calculation or function. The goal of the RL agent is to learn the optimal behaviour or action policy at each state transition to maximize the reward. More detailed explanations on the foundations, principles, and concepts of RL can be found in Ref. [35].

or not exist. If the system model is available, dynamic programming methods such as policy evaluation can be used to calculate the value function for a policy and use value iteration and policy iteration to find the optimal policy [89]. An RL environment can be multi-armed bandit, an MDP, a partially observable MDP (POMDP), or a game.

In the areas of experience replay [106] and network cloning [108], the DNN has achieved significant development that make off-policy Q-learning attractive. The Deep Q-Learning (DQL) agent gathers expe- rience data (state-action-reward values) and trains its policy in the background. In addition, the learnt policy is saved in the neural network (NN) and may be easily transferred across instances. In the network slicing applications, the DQL agent may function effectively and make resource allocation choices in a timely manner based on its already learnt policy. That way, complex slice orchestration and resource allo- cation challenges in 5G and beyond network slicing might benefit from such a strategy. With its capabilities to handle large state-action spaces, [109] have emerged to improve performance and simulation results on Atari games have shown better performance when compared with the DQN. The authors in Ref. [115] attempted to understand the success of DQN and reproduced results with shallow RL. The authors in Ref. [116] proposed a hybrid combination of the policy gradient and Q-learning called PGQ method to improve the performance of the policy gradient. The PGQ performed better than actor-critic (A3C) and Q-learning on Atari Games. The authors in Ref. [118] designed a better exploration strategy to improve the DQN.

Recent breakthroughs in Deep RL have created a lot of euphoria in RL and Deep RL research. Several platforms are available to develop and train RL and Deep RL agents. An outline of some key platforms is pro- vided for interested readers and researchers.

operate in higher-order spaces, Deep RL enhances the robustness and effectiveness of 5G and beyond mobile networks in a variety of sce- narios. For example, in Ref. [61], Multiagent RL, which uses the DDQN approach, maximizes network utilities while preserving QoS for user devices on heterogeneous networks.

The objective of artificial intelligence is to develop intelligent and helpful agents that can solve real-world problems. An RL agent performs a series of activities and monitors states and rewards using the basic components of a value function, policy, and a model. A prediction, control, or planning problem can be written as an RL problem using model-based or model-free methods. A fundamental dilemma between

The history of wireless communication dates to Marconi, an Italian engineer, who first transmitted a wireless message over an electro- magnetic wave in 1898. Since then, wireless communication has trans- formed society and started a digital revolution that has impacted human civilization for decades. The first generation (1G) started the wireless evolution and as times went by the data rates, mobility, coverage, la- tency, and spectral efficiency improved till the fifth generation (5G) of wireless networks.

The 5G System architecture is intended to offer data connectivity and services while also allowing deployments to adopt network function virtualization (NFV) and software defined networking (SDN). These technologies separate the data plane from the signalling control plane, providing independent scalability and flexible deployments such as centralized or distributed deployments, paving the way for the imple- mentation of various 5G services. Network functions (NF) and service- based interfaces make up the architecture.

form of network sharing. Planetlab [245] pioneered slicing in the computer architectures in 2003. In their work slices were separated and tailored to a particular application. This laid the foundation for the current NGMN slicing concept. 2008 saw the acceleration of research

procedures. It supports the flexibility and programmability to simplify network management. NOX [166] is an example of the first SDN controller based on the OpenFlow protocol. Since then, the industry has developed several platforms, primarily for cellular networks, including the Open Networking Operating Systems (ONOS) [190], Central Office Re-architected as a Datacentre (CORD) [168], O-RAN [167], ONAP [195], Aether [169], and SD-RAN [170].

NFV supports scalable and flexible management and orchestration capabilities [240]. This is achieved by virtualizing network services and functions and separating them from their underlying physical infra- structure. Each feature or functionality is implemented in software format via a virtual network function (VNF) that runs on a virtual ma- chine (VM) instantiated on commercial off-the-shelf (COTS) hardware. Network functions (NFs) such as network address translation (NAT), firewalls, intrusion detection, domain name services, can all be imple- mented on COTS, reducing the total cost of ownership (TCO) for mobile operators. Network virtualization which began under virtual machines has continuously evolved through containerization and most recently to a cloud-native network architecture.

A hypervisor is a software layer that separates an operating system and application from the underlying physical infrastructure and allows the underlying host machine to support multiple guest VMs. The hypervisor may interconnect several SDN providers under a single abstraction, allowing applications to create E2E slices without having to observe the differences between SDN providers. The network slice constitutes a variety of virtual machines connected by (a) a virtual local area network (VLAN) if the VMs are hosted in the same data centre (DC) or (b) VLAN and Generic Routing Encapsulation if the VMs are in different DCs. The VM interacts with the underlying physical hardware using the hypervisor. Network slicing has been investigated in numerous studies related to network hypervisors, including OpenSlice [179], MobileVisor [180], RadioVisor [181], and HyperFlex [182]. Containers are based on the concept of virtualization at the operating system level and are an alternative to hypervisor-based virtual machines [183]. Containers contain the application, dependencies, and even the version of the operating system. This allows nomadic operations of applications on computer systems, or even the cloud. The orchestration and man- agement of different containers to all function as one system is critical to conventional system architectures. Several reference points such as Or-vnfm exist to interconnect the ETSI NFV MANO functional blocks. The MANO system is responsible for instantiating migrating and scaling VNF/NS instances, operating and managing network resources and networks slices based on predefined QoS policies.

In the context of ETSI NFV ISG guidelines in Ref. [192], MANO procedures plan and use physical and virtual network resources to create an NSI across multiple administrative domains. The authors in Ref. [117] provided more clarity on the multi-domain concept from multiple perspectives including (a) multi-technology which is stitching together heterogeneous technology domains such as RAN, CN to create and E2E SDN driven platform to accommodate various 5G services and interaction between several InPs or service providers to provide an E2E network slice. The authors in Ref. [150] agree with ETSI NFV ISG in Ref. [192] and argue that to ensure network security, different domain InPs follow their own internal management protocols without disclosing sensitive information to other players in the chain. Mechanisms that enable efficient resource allocation and utilization across multiple administrative domains are generating huge research interest.

Third-party consumers can access the system via customer-to-business (c2b) interface that is administered by the multi-domain resource orchestrator. Similarly, the 5G!Pagoda framework in Ref. [193], is influenced by the ETSI NFV architecture, with each technology domain having its own resource orchestrator. In addition, management domain resource orchestrators are considered to optimize resource usage and management across multiple administrative domains. The architecture in Ref. [111], also influenced by the ETSI NFV MANO, provides support for multi-domain multi tenancy slicing. Its authors argue for the creation of multi-domain slices as a chain of single domain slices, orchestrated by a single orchestrator to mitigate challenges with direct cross domain orchestration.

domains. When logical nodes are implemented, the SDN controller can be used to support logical link connections between logical nodes that can span multiple administrative domains and several physical nodes in an E2E slice network. The centralized and local SDN controllers could also benefit from Deep RL driven data analysis and decision making in that regard to simplify E2E network management. The network slicing

denser, heterogeneous installations, that output larger amounts of data. Traditional statistical model-based tools are also widely used in litera- ture, but given uncertain network conditions, the solutions tend to be suboptimal, resulting in inefficient use of resources and increased costs. In addition, the need for a priori traffic profiles impacts their perfor- mance in dynamic environments such as 5G and beyond networks. The versatility of AI/ML tools such as model-free Deep RL provides an alternative approach and solutions in addressing these challenges. Hybrid approaches using heuristics, metaheuristics, shallow ML, RL, and Deep RL have shown promising results [119].

Deep RL driven solutions in this subsection. The authors in Ref. [229] use Deep RL to address RAN and CN slicing. From their simulation re- sults, they conclude that Deep RL performed better than other conven- tional solutions. The authors of [228] propose an RL agent to control radio resource management (RRM). The authors in Ref. [230] propose an RL algorithm to enable 5G network slicing across multiple adminis- trative domains. The main constraints considered in their solution include delay and location.

solve the challenge of autonomous resource management for multiple slices, the authors in Ref. [213] successfully use Deep RL to adjust the slice allocated resources based actual usage and QoS feedback. Several studies above consider optimizing the procedures for allocating single resources such as radio resources. However, the study in Ref. [68] for- mulates a multidimensional resource (network, computing, and storage)

rameters) in a trial and error manner, receiving a reward in the process that is based on the action taken. Based on the value of the reward ob- tained for admitting or rejecting the slice, the agent can improve the next course of action. In Ref. [256], the authors design an admissibility region for slices. Using a priority based admission strategy, they use

results. In Ref. [264], the authors use RL to optimize priority slice admission control to maximize network utility. Services that are likely to generate high revenue with less KPI degradation are admitted based on the learned policy. When compared to two deterministic heuristics, the performance of RL-based admission policies performs better by 55%.

Forecasting, and predicting slice traffic improves slice admission control, resource allocation and usage, slice placement, and other areas of slice lifecycle management. Some studies use traditional linear sta- tistical models to forecast and predict traffic. However, the application of AI/ML in this area is gradually being accepted given the challenges and limitations of the traditional methods discussed earlier. In 5G is envisioned to support a plethora of Internet of Things (IoT) devices with most IoT sensors being held by the device owner rather than the mobile network operator. As a result, traditional Deep RL customization which works well for a single network entity may not be appropriate in a heterogeneous, dynamic 5G and beyond network environment with a multitude of players. Interactions between different owners (in this case agents) of IoT devices make managing network resources and services extremely difficult and significantly increase the state-action space in question. This condition inevitably slows down the learning algorithm and jeopardizes the performance of the learning policy. In this regard, more effort is needed to accelerate deep multi- agent RL research.

network slices with varying SLAs and objectives. There is a need to build algorithms to forecast and predict slice traffic so that the optimal re- sources are allocated based on actual needs and not SLAs or priority. Although Deep RL methods have been proposed for slice traffic forecast and prediction, there is a paucity of research in that space. Conventional statistical models tend to be dominant.

