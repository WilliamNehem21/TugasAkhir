(wind speed, etc.). This research has resulted in the development of two types of models, making it possible to have the movements of a given conductor in time and space for a given wind speed [1]. The first physical model, called the strip theory model (ST), is based on Computational fluid dynamics calculations (CFD), which are costly in calculation time and heavy in their handling (business software, control of outputs, storage, etc.). The second model, called the wake oscillator model (WO). This model was implemented in an ad hoc tool, is based on simplifications of the coupling terms, avoiding all of the

To achieve this, we propose a processing pipeline consisting of three stages: (i) Data collection, (ii) data modeling and (iii) data visualiza- tion. The first step is to collect the necessary data by running the WO model to build a Machine learning model. Here, we will use several optimization algorithms in order to find the best values of the WO parameters and build the training/test/validation sets. The HyperOpt optimization algorithm will be privileged here because of its adaptation to our multi-parameter and multidimensional problem. Then, we will propose a methodology allowing to build our ENS model. Finally, for comparison, a series of models will be tested in order to compare them to our model and choose the best.

The rest of this paper is organized as follows: Section 3 presents the methodology followed to calibrate this prediction model. Section 4 presents the results of this study. Section 5 presents a discussion of these results and finally we end with a conclusion and perspectives as well as some references.

In this study, our OPTI-ENS model operates directly on time-series WO data in order to predict the values of y/d. WO data can be obtained from the executions of the WO model, following the different values of its hyper parameters. Our dataset is composed of the following elements: 10 variables, including the different values of the hyperpa- rameters of the model, namely (md, U[m/s], d[m], m[kg/m], L[m], H[N], Nt, Dt[s], tf[s],ymax[m]) and the y/d values. The values of y/d are the values to be calculated using the model of WO

Now that the data is preprocessed, the next step discusses the modeling. The use of the learning framework as it is will cause a problem of flexibility despite the effectiveness of the method. Indeed, the learning pipeline will always feed on the data of the WO generated manually. In order to lighten and improve this approach, a method for optimizing the generation of WO data has been implemented. The study of different optimization methods already widely used in the state of the art, in particular for the Operational Research community, led us to explore one of them, namely HyperOpt.

The general operation is based on the suggestion algorithm which evaluates the search space in order to optimize the search towards sub-ranges of parameters which may be interesting to explore. These parameters are then supplied as input to the cost function, the output of which must be minimized. The algorithm keeps in memory the best result obtained.

Until now, the Hyperopt algorithm has used an RMSE cost function. We tried to use a cost function based on the R2 score. The R2 Score is a metric for evaluating the dispersion of results between 2 sets of data. A negative result indicating a less good regression than a straight line, a zero result indicating a clear difference and a result close to 1 indicating a resemblance between the 2 curves. Since Hyperopt only takes functions to minimize and not to maximize, we have chosen to define a cost function defined by the formula:

Our OPTi-ENS model is made up of two bricks: a Hyperopt type optimization algorithm followed by a learning algorithm. The training data were therefore generated thanks to the use of Hyperopt on the WO model. We are at this stage ready to train our ENS model with this training data. The technical architecture of our ENS model is composed of 10 XGboost type models (to form an XGboost drill) and a deep neural network (DNN). All the XGboost and DNN models take as inputs the data of the WO model, namely the values of y/d as well as the values of the hyper-parameters of the WO model which made it possible to obtain these y/d values. These models individually produce the predictions that serve as metadata to feed the Ridge Regression model. The final output of the OPTI-ENS model therefore takes the form of a weighted average of all the outputs of all the models (XGboost and DNN) taken individually.

itself and without any domain specific knowledge about the simulation time series data. By skipping the procedure of extracting features, the model could become more responsive. The training process is divided into two steps: pre-training and fine-tuning. Pretraining is unsuper- vised, and an initial network will be obtained using a greedy layer-wise training algorithm. Finetuning is supervised and the parameters of all the layers will be updated using back propagation algorithm. The following notations are used to denote the components of the network:

In this section, we will present the different results of the prediction of the values of y/d according to different machine learning models as well as our OPTI-ENS model. This benchmark mainly concerns RMSE type errors and the R2 Score, because these two metrics are the most significant, often widely used in the state of the art, especially in the validation of machine learning models whether in regression or in clas- sification. This section is divided into two sections: (i) The visualization of the data generated by the Hyperopt optimization algorithm which are the training data and (ii) The creation and training of our ENS model which is composed of several XGBoost models and an optimized DNN model by modifying its ReLu activation function. At the end we will also make available the prediction results, the RMSE errors, the differences in execution speed and finally some settings that we had adopted in the process of building our OPTI-ENS model.

better results than models used individually. The predictive capacity of the ENS model is mainly due to the characteristics of XGBoost models of producing results on not too high quantities of data as well as the predictive power of DNN type models on time series especially when learning is done on data. almost raw time series.

