Platforms using ASICs have high-performance and high-speed com- puting capabilities in addition to offering low power consumption. However, their architecture is fixed, which limits their use when adopt- ing different designs. This is particularly important in neural network implementations, which rely on various network and communication parameters.

FPU: Each tile shares a 32-bit FPU (floating-point unit) between four cores. Latency in IP core is the number of cycles it takes for an accumulation to propagate through the block from input to output. The FPU operates using Altera IP blocks that have 14 cycles latencies at 250MHz.

Real-world applications such as World Wide Web, medical infor- matics, social network, and machine learning libraries make use of graph-based algorithms [29]. POETS with the capability of fast and parallel processing at large scale uses graphs to map applications to the hardware. To perform graph processing, our framework provides high- level programming models for the users to easily create the graphs and edges without dealing with low-level FPGA coding. Users must think about how to break down their application components into vertices and edges. For instance, in the SNN case, the neurons in the spiking neural network are vertices and the edges are synaptic weights between the neurons. Currently, there are two different toolchains for mapping the simulations and applications to hardware, namely, Graph Schema and POLite [28].

POLite. Another high-level programming environment toolchain is PO- Lite [28], which is similar to the vertex-centric paradigm [29,30] but supports both synchronous and asynchronous messaging. POLite is a layer of abstraction that manage mapping arbitrary graphs onto the Tinsel overlay. POLite is a C++ light software layer on top of the Tinsel hardware designed to implement a graph-based event-driven abstraction, while it is able to hide architectural details from the user. Similar to Graph Schema, the vertices receive the events and if the state of the vertex is changed, it will send a message via the edge to the other connected vertices. The event messages are treated by the following event handlers:

Machine learning algorithms have been used to solve tasks in au- tomation, recognition, classification, and prediction without being ex- plicitly programmed [31]. In other words, they are capable of learning from data. Artificial neural networks are a class of machine learning models that are loosely inspired by their biological counterparts [32]. Conventional neural networks continuously transmit real-valued sig- nals between neurons that can be interpreted as firing rates. In contrast, spiking neural networks (SNNs) operate via the transmission of discrete events, referred to as spikes, mimicking action potential generation in the brain. SNNs offer huge energy savings due to their sparse event- driven nature, however, training and simulating such models remains challenging [33].

in larger networks, the number of messages surpasses that of clocked synchronization (CS). Nevertheless, removing global synchronization will accelerate computation, especially in non-fully connected networks running on distributed, federated or heterogeneous systems can facili- tate parallelization. Running this method on our platform, in addition to reviewed benefits of GALS, we predict more capability to speed up the computation and communication due to the lack of global clock connections.

Hardware idle detection (HID): In this research, we introduce HID for the neuromorphic event-driven system synchronization based on termination detection [28]. A hardware barrier synchronization will synchronize the event-based neurons in the HID method. It means a neuron does not wait for any other neurons to receive messages. The HID method is designed for globally-asynchronous applications, and it uses a signal to ensure there is no undelivered message in the system. HID identifies an event when there is no thread in the system with

POETS is designed to efficiently simulate highly scalable event- based models. We will focus on simulating spiking neural networks that implement parallel distributed processing at large scale. In the previous section, we presented a generic model that could be used for any neuron model. Here, we demonstrate how the Izhekevich model can be mapped on hardware. The balance of excitatory to inhibitory neurons ratio in the spiking network is 80% to 20% respectively.

Routing algorithms serve the purpose of directing data packets, while each algorithm being a software tasked with determining the most efficient path for transmitting a packet. Achieving effectiveness and efficiency in routing stands as a paramount aspect within NoC- based (Network on Chip) neuromorphic systems [38]. Message delivery in POETS system is guaranteed by the hardware provided that all threads eventually execute the messages available to them. Threads communicate with each other via mailboxes. Two different methods have been used for message communication between threads, namely unicast and multicast. In the unicast method, there is a point-to-point communication between two threads in which a single packet is sent to a single destination. The aim of multicasting is to send the same message to multiple destinations while minimizing messaging traffic in the system. Messages first will be delivered to the programmable routers, which automatically propagate messages to any number of destination threads distributed throughout the cluster. If a router sup- ports multicast routing, then neurons with a high fan-out can be communicate efficiently with minimizing inter-FPGA bandwidth while offloading work from the processing cores. Tinsel provides both unicast and multicast communication by having a programmable router on each FPGA board to support global multicasting.

Each box is hosted by an x-86 machine with 28 cores Intel(R) i9-7940X CPU@3.10 GHz. Although in previous work, we have used MNIST data set as an output due to establishing a learning algorithm on the hardware [19]. In this research, the input spiking data is generated randomly using a normal distribution for different networks to verify the network capability running different number of neurons. We implemented a random recurrent neural network with sizes from 100 to 8 million nodes and placed on one to 8 hardware boxes. The testing benchmark is similar to [13] but in more scalable sizes. The critical check points for the number of nodes are 100, 200, 500, 1k, 10k, 50k, 100k, 500k, 1000k, 2000k, 8000k. The number of synapses per neuron has been defined to verify a normal and extreme number of connections, 100 and 1000 synapses per neuron. Due to more robustness of results while using the random seeds inputs, we average value from a Gaussian (normal) distribution with a mean of zero and a standard deviation that corresponds to the chosen weight range. For connection weight modification, The system supports STDP (Spike Timing-Dependent Plasticity) and reward-based STDP which have been presented in our previous works running on the same platform [19,20]. In this work, we focus on neuron activities and message communication while STDP rule is used for weight modification without decoding any input data pattern.

The POETS ecosystem mapping strategy works efficiently depends on the number of vertices and edges in the mapped graph. If the vertices could be assigned parallelly into the threads of one physical board, it will not use two boards to address high-speed communication and energy saving challenges. The threads on neighboring mailboxes communicate faster compared to threads on further mailboxes, caus- ing these to spend more time and consume more bandwidth in the system. Therefore, it is important how neurons communicate with each other on the same thread, or different threads while still sharing 4.67 s using one box and eight POETS boxes respectively. We used the same networks with the same number of connections for Brian simulation. Results show that the hardware implementation on POETS is more than twenty times faster than the Brian simulator. Additionally, comparing the system with one 48-chip SpiNNaker node shows that POETS is at least 16 times faster. This speed comparison is made using optimal communication parameters such as HID synchronization, multicast routing, and considering homeostasis for POETS as discussed previously.

spikes across the network [44]. Neurogrid and BrainScaleS similarly use the temporal dynamics of memory elements to store the state of the network, with the capability of local learning. The second version of BrainScaleS is developed as BrainScaleS-2. This version is a multi- chip system building upon existing BrainScaleS wafer-scale system components. The architecture is implemented in a single-chip ASIC

Synchronization is one of the most important methods in designing neuromorphic event-driven systems in simulating SNNs. Three synchro- nization methods including clocked sync, GALS (Globally Asynchronous Locally Synchronous), and Hardware Idle Detection (HID) have been discussed and analyzed in this work that the HID is introduced in this work as a novel synchronization method for neuromorphic system. To implement these algorithms on hardware, we introduced POETS as a new large-scale neuromorphic system which is flexible using FPGA clusters, easily scalable by adding more FPGA boards, reliable with a guaranty of receiving messages, and fast due to the parallel processing of data and high-speed interconnection bandwidth. Running the SNN on POETS hardware, we demonstrated that HID is the best synchronization approach considering speed and spiking accuracy.

