fundamental mode is dominant and the energy information is ignored. However, that energy information depends strongly on the stratigraphy and therefore contains additional information (Socco and Strobbia, 2004). The Rayleigh wave velocity depends primarily on the shear wave velocity and layer thickness. The bulk density and P-wave velocity or

modal analysis it could happen that higher modes cross and overlap or are stronger than the fundamental mode. Ideally, we would not want to pick the dispersion curves at all but rather provide the whole f-k or f- Vph gather to the inversion or prediction algorithm. This is of course the problem that full waveform inversion (FWI) attempts to solve. However, FWI has its own challenges and is computationally very expensive, especially elastic FWI when applied to noisy land seismic data. We, therefore, set out to assess whether a deep neural network could bridge the gap between dispersion curve inversion and FWI and could learn

Deep learning methods excel at learning from large number of su- pervised examples, however, these methods do not typically generalize well for smaller datasets. For such cases, transfer learning helps through facilitating the transfer of knowledge from one task to another. This transfer of knowledge occurs in the form of initializing the weights of the network with values extracted from one previously learnt on a task that is usually more complex. A more complex dataset facilitates learning a more diverse set of discriminative features, thereby making the repre- sentation space richer. This generally helps in computer vision as kernels in the initial convolutional layers learn to generate high level features which are usually common across task (Yosinski et al., 2014) and reusing them helps to make optimization faster and better. For the geophysical problem studied in this paper, we use the pre-trained neural network weights and biases from the ImageNet task (Russakovsky et al., 2015) which used natural images divided into 1000 categories. Net- works trained on ImageNet are generally rich in terms of features and have been deployed for various applications (Razavian et al., 2014; Liu and Deng, 2015). Although seismic data and natural images differ significantly in terms of the distributions in the space, it is of interest to explore whether their latent spaces have any overlap and whether transfer learning from ImageNet can help for our cause.

Deep neural networks rely heavily on big data to avoid the issue of overfitting. Unfortunately, for most applications, including geophysics, labelled data is relatively scarce, and data augmentation serves as a data-space solution to overcome this issue. Referred as image augmen- tation for our problem, it is a technique to create variation in existing images to artificially increase the size of dataset. We apply affine transformations to images such that the ground truth is not affected. Augmentation techniques that we explore in this paper are random cropping, random translations, random dilations, random rotations and random masking. These help the deep learning model by improving generalization and thus in turn reduce over-fitting. We briefly discuss these augmentations below.

(c). This helps the network to not over _t on certain regions and use whole feature space effectively. Specifically, for this example it acts as an additional denoising approach, as only the high amplitude values following dispersion curves contain predictive information about the subsurface model (red colors). The green/yellow/blue colors are lower amplitude values that are less relevant or irrelevant for the velocity prediction.

This transformation involves scaling the original image inward or outward. For outward scaling, the final image size of the image is larger than the original image size. Following this, a sub-image of the size of the original image is cropped. Similarly, scaling inward reduces the size of the original. The empty pixels around the boundary in this case are filled with zero. Note that all the aforementioned augmentation methods are also applied to the coordinate channels, so the absolute and relative positions of the input data are preserved regardless of the rotation and dilation.

A very common problem with deep neural networks is their lack of generality on out-of-distribution datasets. In other words, a model trained on even very large training data can still fail easily, if the dis- tribution of the validation set is different from the training set. As stated earlier, a common assumption for the conventional deep learning pipeline is that the underlying distribution of the training and test sets are similar. From the practical point of view, it implies that the training set should be sufficiently rich, and should comprise samples from all possible scenarios. However, for geophysical problems, it is not realistic strictly the range of values for Vp and Vs for every input phase velocity spectrum. Rather the focus is only on learning the overall trend of the output curves. Second, the approximate ranges of Vp and Vs are explicitly provided to the network, and the network learns during the training process to fit the output curves within these ranges. With this modification, we hope that if the network has learned the trend of the output curves, it can map on out-of-distribution samples as well when the  additional  information  on  the  range  is  provided  to  it.

For this task as well, the backbone architecture is ResNet18. For adding the prior knowledge on Vp and Vs bounds, we have two meta architectures. Each of these comprises one linear layer, a batch- normalization layer and a ReLU activation module. For Vp as well as Vs, the input comprises 2 neurons each, expressing the upper and lower bounds, and the corresponding meta-architectures encode them into 50 neurons. The output 50 neurons from both architectures are then concatenated to the output of the CNN backbone and fed into the two linear heads to compute Vp and Vs.

improvement observed on the ID validation set. During inference, no augmentations are used. For the domain adaptation module, training and inference are performed in different ways. During training, we take the minimum and maximum values of Vp and Vs and perturb them with additive noise of up to 100 m/s. Further, a constant shift of up to 500 m/ s is added to Vp and 80% of the sampled value for Vs to induce the invariance in the network. During inference, the minimum and maximum values of Vp and Vs are directly used as bounds for input to the network.

velocities using the constrained network and the bottom row the pre- dictions using the unconstrained network. The uphole interval velocity derived from the pairs of uphole depth and times is also shown. Although there is not a perfect match, we see overall a better agreement between the predictions from the constrained network and the uphole velocities.

