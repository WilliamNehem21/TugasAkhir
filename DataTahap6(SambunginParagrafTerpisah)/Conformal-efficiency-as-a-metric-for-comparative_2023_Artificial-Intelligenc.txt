[34] is a data preparation package developed as part of the MELLODDY project. Its main features are: (i) SMILES standardization based on RD- Kit v2020.09.1.0 [35]; (ii) binary molecular fingerprint calculation (in this work extended-connectivity fingerprints; ECFP6, folded to 32k bits)

[36] through the GetMorganFingerprint function in RDKit; (iii) replicate aggregation according to assay type; (iv) filtering of tasks that do not meet the task size or fold distribution quora for inclusion in training and evaluation datasets; (v) splitting of the dataset in folds according to a se- lected folding scheme, in this case based on the scaffold network [37] as implemented in RDKit [38]. This approach enables different partners in the federated learning exercise to assign compounds to folds consistently without having to exchange any sensitive structural information [39]. Five folds were created, out of which three were used for training, one for validation, and one as test fold. To avoid possible bias in evaluation, acyclic and phenylic compounds were assigned to the training set; (vi) binarization of the activity values and addition of auxiliary tasks with shifted thresholds compared to the main task of interest, while still al- lowing for user-defined expert values. From any single assay, at most five tasks were derived by varying the threshold resulting in different class balances. To avoid any dominance of assays with a higher number of derived tasks during training, MELLODDY-TUNER produces a task weighting file for further machine learning which assigns 1/N weights to all tasks, with N being the number of derived tasks from that assay [40]. Full details on the data preparation are described elsewhere [20].

els was through grid search with different hyperparameters, such as a higher network size to accommodate the increase of data compared to the single-partner setting. For both the single-partner and multipartner models, the same datafiles outputted by MELLODDY-TUNER were used. The network architecture was a feedforward multilayer perceptron us- ing SparseChem v0.8.2 [41] and minimizing the binary cross entropy as loss function through stochastic gradient descent. The implementation on the federated platform is described elsewhere [21]. The test set was partitioned into two subsets of distinct scaffolds and comparable size, with one of the two selected as calibration set, and the other as evalu- ation set. Platt scaling [42] was performed in a post-processing step by fitting a logistic regression, as implemented in scikit-learn [43], to the logits and true labels. Predictive probabilities p(y|x) were transformed to predictive information entropies H by:

a size quorum of minimally 25 actives and inactives in the calibration set was imposed. For Phase 1 models this corresponded to the valida- tion fold, while for Phase 2 models this corresponded to approximately half of the test fold. In addition, tasks meeting this quorum but failing to achieve an AUC ROC of 0.6 or higher on the calibration set were dropped from the analysis. Only tasks of main interest were consid- ered. Any auxiliary tasks, including those resulting from thresholding

multipartner and single-partner model is associated with uncertainty reduction. Inductive Mondrian CP is applied, where the prediction for each class is estimated separately using an individual calibration set per class [26,30,44,45]. The code is based on an implementation from Toccaceli [46].

While there may be some degree of overlap with the internal MEL- LODDY datasets for these unlabeled datasets (e.g., publicly available compounds may be part of the internal library or may even belong to the training or validation set), this was not expected to contribute signif- icantly to the results due to the small overlap expected between public and proprietary datasets and the sparsity of the labeled activity ma- trix (far below 1%). Indeed, predictions on the unlabeled datasets are dense, i.e. across all tasks, while the predictions on the labeled datasets are sparse, since a prediction is only generated where there is an ac- tual label available. The degree of identical overlap of the unlabeled datasets with the internal MELLODDY datasets in the 32k bit descriptor space was investigated by three pharma partners, and the findings were in line with expectations: (i) the FDA-approved small-molecule drugs were clearly most highly overlapping with the internal library, suppos- edly due to their use as reference compounds; (ii) the virtual libraries (SCUBIDOO, DrugSpaceX) had very low overlap (but nonzero); and (iii) the other external datasets were in-between these two extremes.

both labeled and unlabeled space, reflecting higher uncertainty. It then dropped sharply with increasing layer sizes, indicating very low uncer- tainty. Regarding the training set size, larger training sets presumably extend the chemical space covered by the model, resulting in a lower uncertainty on a common test set. However, such an effect was not observed for the uncalibrated entropy. For the largest networks, only the smallest training set of 100 training samples displayed somewhat higher entropy values compared to the other training set sizes. Apply- ing calibration to the predictive probabilities through Platt scaling did effectively address this overconfidence. Also, the Platt-scaled entropy favorably did not display a strong dependency on the degree of regular- ization through the hidden layer size, and when more training data was involved, a lower uncertainty is observed, aligned with expectations.

First, the test set can be similar to the training and validation set due to limitations related to splitting an internal dataset into five folds. The underlying idea of the scaffold-based splitting approach is to separate distinct areas of chemical space and to assign them to different folds thus avoiding overconfident predictive performance estimates. While scaffold-network-based splitting achieves far better separation of similar molecules into different folds than random splitting, some structurally very similar compounds might still be assigned to different folds due to not sharing the same scaffold [39]. Switching to other splitting schemes such as sphere exclusion clustering was not expected to fundamentally alter the picture based on its limited performance gain over scaffold- network-based splitting [39]. While scaffold-based splitting challenges

Bosc N, Felix E, Arcila R, Mendez D, Saunders MR, Green DVS, Ochoada J, Shelat AA, Martin EJ, Iyer P, Engkvist O, Verras A, Duffy J, Burrows J, Gardner JMF, Leach AR. MAIP: a web service for predicting blood-stage malaria inhibitors. J Cheminform 2021;13:13. doi:10.1186/s13321-021-00487-2.

