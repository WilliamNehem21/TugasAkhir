The above criteria permit to drastically reduce the number of false positives with- out decreasing the detection rate. The proposed approach has been validated on three datasets composed of 180 samples including both 2D and depth images. The face position inside samples has been manually labeled for testing.

indicating the relative distance of that pixel from the sensor at the time of image capture. Depth information captured by Kinect is not useful to differentiate among different individuals at a distance, due to its very high inter-class similarity, but thanks to its low intra-class variation may be useful to improve the robustness of a face detector by reducing sensitivities to illumination, occlusions, changing of expression and pose. Kinect devices have been extremely popular recently, due to their low-cost and availability, and the first benchmark datasets have been collected for 3D face recognition (Tsalakanidou et al., 2003) or detection (Hg et al., 2012).

This work, similar to other approaches proposed in the literature (Anisetti et al., 2008), is aimed at using depth information to reduce the number of false positive detections and improve the percentage of correct detections. In (Anisetti et al., 2008) the authors use a 2D multi-step algorithm to obtain a coarse-to-fine classifi- cation, then refine the quality of the face location by a 3D tracking approach.

the third filtering rule is defined on the depth map to discard flat objects (e.g. candidate faces found in a wall) or uneven objects (e.g. candidate face found in the leaves of a tree). Combining color and depth data the candidate face region can be extracted from the background and measures of depth and reg- ularity are used for filtering out false positives.

Unfortunately, in the literature there are no freely available large datasets for face detection (with difficult images as complex background, the datasets used in (Shieh and Hsieh, 2013; Mattheij et al., 2012) are quite easy) that contains both the color and the depth map. There are several datasets for face recognition using the depth map, but the face detection step in those datasets is easy. Therefore, the proposed approach has been evaluated on a collected dataset that will be made freely available for further comparisons.

ing the faces in order to obtain a set of aligned color images and depth maps. For this purpose, the calibration data for the depth and color cameras of the Kinect are computed using the method proposed in Herrera et al. (2012). This approach com- putes both the intrinsic parameters of the depth and color cameras and the extrinsic parameters between the two cameras. The depth samples positions of the image in the 3D space are first computed by using the intrinsic parameters of the depth cam- era and the 3D samples are then reprojected in the 2D color image reference system by using both the color camera intrinsic parameters and the extrinsic ones. By the end of this procedure, a color and a depth value are associated to each sample.

Another significant information that can be obtained from the depth map is the flatness/unevenness of the candidate face regions. For this filter first a segmenta- tion procedure is applied, then from each face candidate region the standard devi- ation (std) of the pixels of the depth map that belongs to the larger segment is calculated. Regions having a std out of a fixed range [0.15, 4] are removed.

The segmentation of both color and depth map is performed according to the approach of Dal Mutto et al. (2012). This segmentation scheme is based on the normalized cuts spectral clustering algorithm (Shi and Malik, 2000) and jointly exploits the geometry and color information for optimal performances.

pi; i = 1; .. . ; N. After the joint calibration of the depth and color cameras it is pos- Each sample in the acquired depth map corresponds to a 3D point of the scene sible to compute the 3D coordinates x, y and z of pi and to associate to it a 3-D

vector containing the R, G, and B color components. Geometry and color then need to be unified in a meaningful way. Color values are converted to a perceptu- ally uniform space in order to give a perceptual significance to the distance between colors that will be used in the clustering algorithm. The CIELab space has been used for this purpose, i.e., the color information of each scene point is the 3-D vector:

where k is a parameter balancing the contribution of color and geometry. High values of k increase the relevance of geometry, while low values of k increase the relevance of color information. A complete discussion on the effect of this parameter and on how to automatically set it to the optimal value is presented in Dal Mutto et al. (2012).

versity campus in Padova. It includes both outdoor and indoor scenes, framed in different hours during the day, in order to account for the varying lighting conditions. The images capture one or several people performing various daily activities, e.g., working, studying, walking, chatting and so on. Note how most people are not directly looking into the camera, i.e., they did not pose for the frame acquisition but they were doing their activities without being aware of the camera shooting them. Some faces are also par- tially occluded by objects or other people. For these reasons, this dataset is more challenging than the previous ones.

The depth map allows to remove the false positives in many critical situations. In particular it allows to get the actual size of the candidate face allowing to remove objects too small or too large to be a face. It also aids the segmentation step in the proposed method that is a critical point to ensure proper processing in the remaining steps.

