This paper aims to solve the lack of generalization of existing semantic segmentation models in the crop and weed segmentation domain. We compare two training mechanisms, classical and adversarial, to understand which scheme works best for a particular encoder-decoder model. We use simple U-Net, SegNet, and DeepLabv3+ with ResNet-50 backbone as segmentation networks. The models are trained with cross-entropy loss for classical and PatchGAN loss for adversarial training. By adopting the Conditional Generative Adversarial Network (CGAN) hierarchical settings, we penalize different Generators (G) using PatchGAN Discriminator (D) and L1 loss to generate segmentation output. The generalization is to exhibit fewer failures and perform com- parably for growing plants with different data distributions. We utilize the images from four different stages of sugar beet. We divide the data so that the full-grown stage is used for training, whereas earlier stages are entirely dedicated to testing the model. We conclude that U-Net trained in adversarial settings is more robust to changes in the dataset. The adversarially trained U-Net reports 10% overall improvement in the results with mIOU scores of 0.34, 0.55, 0.75, and 0.85 for four different growth stages.

Weeds are undesirable plants that can be eliminated using herbi- cides. Traditionally, these herbicides are used uniformly over the whole field. This uniform spraying eliminates the weeds but affects the crop as well. It also consumes an unnecessary amount of herbicide, adversely affecting the environment and human health. The amount of herbicide usage can be decreased by selective spraying. The agricultural robots can perform weed and crop detection to perform precise spraying, which will help reduce herbicides' usage. However, these robots lack a universal weed detection mechanism making them unfit for commercial use. In addition, typical methods for detecting weeds do not translate to out-of-distribution data; therefore, each new weed type or field needs to be updated, which is time-consuming.

This paper proposes a solution for better model generalization. In this work, we perform experiments to achieve a more reliable model tuning that can be implemented on a robot to select weeds. Further- more, the resultant model performs better on the data on which it is trained and other growth stages without any additional training re- quirement. The dataset used for this research consists of images col- lected from multiple stages of sugar beet crop (Chebrolu et al., 2017). It has nineteen stages which are divided into four sections. The first three sections have five stages each, and the last section consists of the remaining four stages.

In semantic segmentation, given the limited access to labelled data and difficulty annotating at the pixel level, the generalization of the model parameters for the unknown stage of the plant becomes difficult. To articulate that, we explore the capability of different models to segment the crop and weeds from different stages when trained in an adversarial (using CGAN) and classical fashion. The main focus is to train different models to adapt to the unseen infor- mation given minimal access to data, i.e., training only for the last four stages. The target dataset has varying illumination, multiple weeds, and different soil texture. Using a fully grown stage for under- standing, we are searching for a mechanism to improve the model performance for different stages of the crop. In addition, we focus

The paper is divided into five sections. In Section 2, we present the related work where methods for weed detection are reviewed. The data analysis, experiments, and description of the model architectures are described in Section 3. Section 4 presents the results with a compar- ison of model performance. Finally, we conclude the paper in Section 5.

training (Haug et al., 2014). A similar approach is followed by Lottes et al., where Local Binary Patterns (LBP) are used to find statistical features over R, G, and NDVI channels. The LBP operator generates a binary pattern based on the centre pixel value within eight neigh- bours. Shape-based features are calculated for vegetation masks. These features are then used to train RFC to detect weeds and plants (Lottes et al., 2016). These methods depend on shape-based features, which vary for the same or growing stage of the plant.

Gated-shaped CNN (Takikawa et al., 2019) improves the semantic segmentation task by processing information in two streams. Instead of processing colour, shape, and texture information in a single tradi- tional network, a shape-based stream is proposed. The shaped-based stream focuses on gradient computation for the given mask. The in- termediate layers in the shape stream are connected using Gated Convolutional Layers (GCL). Finally, features from the regular and shape streams are fused using Atrous Spatial Pooling Pyramid (ASPP). They report an overall mean Intersection Over Union (mIOU) of 80.8% for the cityscape dataset.

Karacan et al. utilize semantic layout to synthesize realistic outdoor images. They use CGAN that learns target content to be drawn inside layout (Karacan et al., 2016). Rezaei et al. propose CGAN for segmenting brain tumours. Utilizing U-Net (Haug et al., 2014)e20 as generator and Markovian GAN (Radford et al., 2015) based discriminator, they report

In this work, we performed a comparison-based study to understand the effectiveness of model generalization under adversarial and classical training settings. Our focus is to find an architecture as well as a training scheme that could effectively detect weeds and crops at multiple growth stages. In the following subsection, we perform Exploratory Data Analysis (EDA) to understand the underlying pattern and chal- lenges in the data.

0 represent weed or no weed, respectively. The pixel value is fundamentally a one-hot encoded vector representing the pixel be- longing to a particular class based on its hot value. For example, the vector with values [1,0,0] means the pixel belongs to the first class.

Additionally, the difficulty of differentiating weeds and crops at an early stage may be the leading cause of poor performance. The soil var- iation can be addressed using artificial augmentation. Changing bright- ness and adding noise in the field images can tackle the soil variations effectively. However, due to the very close resemblance of weed and crop at this stage, it is hard to differentiate between crop and weed at the early stage.

