The dataset, acquired by Matek et al., contains 171,375 images from a cohort of 945 patients diagnosed with various hematological diseases at MLL Munich Leukemia Laboratory [52]. The minimum patient age was 18.1 years, and the maximum was 92.2 years. The average patient age, based off of the median, was 69.3 years. The mean age was 65.6 years.

In this study we performed data augmentation in order to rectify class imbalances. Data augmentation is a technique used to generate new data from a set of existing data. In the case of images, new data can be created by applying a variety of transformations to an image. Some such transformations are rotations, translations, zooming in or out, noise insertion, cropping, and flipping horizontally or vertically.

Data augmentation is especially effective when used to address un- even distribution of class data. In our case, some of the classes had as few as 8 images, while others had as many as 29,000. Augmenting the data helped address this issue, increasing the number of images we had for classes that were under-represented. This reduces bias towards over- represented classes.

Most morphological classifications were accurately predicted by our trained model. Because neural networks are data-driven learning algo- rithms, their classification performance improves as the number of train- ing sample data increases. Precision and recall, which are commonly used measurements of accuracy, precision, and recall, were utilized to evaluate our training method.

morphological categorization of diagnostically significant leukocytes. To the best of our knowledge, this picture database is the most compre- hensive one currently accessible in the literature in terms of the number of patients, diagnoses, and single-cell images that are included within it. We utilized the data set to train and evaluate a novel convolutional and attention network-based model for cytology morphological classi- fication, which was then compared with existing state-of-the-art CNN models.

L specifies the number of blocks, and D denotes the number of channels in the hidden dimension. We always utilize the kernel size 3 for all Conv and MBConv blocks, no matter what. Following [22], we increase the size of each attention head in all Transformer blocks to 32. The expansion rate for the inverted bottleneck is always 4, while the expansion (shrink) rate for the SE is always 0.25. The inverted bottleneck is also known as the inverted bottleneck.

For classes in which there are just a few training samples available, such as faggot cells or diseased eosinophils, the classifier performs less well, as would be anticipated for a data-driven strategy [60,61]. There would be a greater need for training data if the image-classification job was focused on the detection of these particular cell types. It is also pos- sible that training a binary classifier rather than a complete multiclass classifier will result in improved prediction performance. The false posi- tive and false negative cases depicts the similarity between each of these cells and this re-enforcing the importance as well as the complexity of the task of classification of cell morphology.

in this paper performs well in such an environment, which is quite promising. Our proposed convolution and attention network model (CoAtNet) outperformed the current state-of-the-art CNN models in clas- sifying cells and even took a huge leap in accurately classifying classes with low sample sizes. This shows how the attention network could be used in similar datasets in the future as well.

