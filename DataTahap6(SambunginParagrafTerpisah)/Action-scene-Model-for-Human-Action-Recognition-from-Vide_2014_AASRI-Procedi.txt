Human action recognition from realistic videos attracts more attention in many practical applications such as on-line video surveillance and content-based video management. Single action recognition always fails to distinguish similar action categories due to the complex background settings in realistic videos. In this paper, a novel action-scene model is explored to learn contextual relationship between actions and scenes in realistic videos. With little prior knowledge on scene categories, a generative probabilistic framework is used for action inference from background directly based on visual words. Experimental results on a realistic video dataset validate the effectiveness of the action-scene model for action recognition from background settings. Extensive experiments were conducted on different feature extracted methods, and the results show the learned model has good robustness when the features are noisy.

As human actions always occur under particular scenes where a rich source of contextual cues will present, the contextual relationships of background settings could be learned as a complement for action recognition. Although recognition based on scene detectors have become commonplace in the related literatures [3,4], a main problem of detector based methods is how to select general scene and action categories for all videos. Otherwise, training detectors is time-consuming and performances of recognition will be affected by the learned detectors.

In this paper, we intend to learn contextual cues using a generative framework with little prior knowledge on scene categories. The contextual relationship between actions and scenes could be used to infer actions from background settings. An action-scene model is proposed to automatically model the relationship between actions, scenes and background features, where the number of scenes only need to be given instead of the categories of scenes. Firstly, a video will be segmented into person regions and background regions. Then the relationship of a given action category and scenes will be modeled using an action-scene model. Finally a factor is computed based on the proposed model to measure the importance of learned context cue and the responding probability of action category is given from this model.

The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes the process of feature extraction and video representation. Action-scene model is proposed in Section 4 and experimental results are shown in section 5. Finally we present concluding remarks and future work in Section 6.

In this paper, we propose a novel approach different from the detector-based methods in learning contextual cues that has no use for action and scene categories. Inspired by the recent work of action recognition in static images [10] and unsupervised learning method in [2], we use a generative model to represent action and scene in videos. The underlying action-scene dependency is captured on the action-scene model directly from visual features.

The person body detection in videos is based on Felzenszwalb's object detector [11] and mean shift tracking [12]. We first use the object detector to find candidate person regions from each frame image. Since there are many false alarms and miss-detections, a sliding window based method is used to discard false alarms and track person area for missing detections. Two detections are viewed as similar if the bounding boxes overlap exceeds 40%. The window size is empirically set as 15 frames and the threshold is half of the windows size. Then, the miss detections are filled with tracking from previous frames using mean shift algorithm. The person detection provides the bonding boxes of person location at each frame.

Background feature extraction. The non-person region is seen as background region. In order to capture the color, shape and local features of the background, we extract color histograms for color features, Gist descriptors [15] for shape features and SIFT descriptors [16] from key frames for static features.

generative probabilistic model to model the contextual cues between them. In the action-scene model, each video can be seen as a mixture of action categories, where each action is a probability distribution over scenes and each scene is associated with a distribution over visual words. Each video clip is looked as a distribution of actions, and each action is a distribution of visual word. The action-scene model discovers not only which action happens in a video, but also which scene is associated with the action.

Suppose we have a collection of V video clips, each video v is represented as a set of visual words. Each word belongs to the visual codebook including W unique visual words. Assume we have K action categories, S scene categories. Now we can describe the process generating each video v as:

We evaluate our approach on the challenging YouTube dataset provided by Liu et al [7] which consists of 1168 videos collected by Liu et al. The dataset covers 11 action classes: basketball shooting, biking, diving, golf swing, horse riding, soccer juggling, swing, tennis swing, trampoline jumping, volleyball spiking, and walking with a dog. Each category of action is performed under several different scenes and divided into 25 subsets.

In this paper, we explore to recognize human actions from background settings of realistic videos. An action-scene model is proposed to model and learn the relationship between actions and scenes with little prior knowledge on scene categories. A generative learning method is used to infer actions from scenes according to a certain distribution. Experimental results validate that the addition of the context cues indeed improve the recognition precision. Besides, our proposed action-scene model has good robustness when applied in realistic video datasets.

