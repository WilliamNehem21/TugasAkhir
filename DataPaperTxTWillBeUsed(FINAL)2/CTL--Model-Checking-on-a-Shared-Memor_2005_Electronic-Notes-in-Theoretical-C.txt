Electronic Notes in Theoretical Computer Science 128 (2005) 107–123 
www.elsevier.com/locate/entcs


CTL* Model Checking on a Shared-Memory Architecture
Cornelia P. Inggs1 ,2 and Howard Barringer3
Department of Computer Science University of Manchester
UK

Abstract
In this paper we present a parallel algorithm for CTL* model checking on a virtual shared-memory high-performance parallel machine architecture. The algorithm is automata-driven and follows a games approach where one player wins if the automaton is empty while the other player wins if the automaton is nonempty. We show how this game can be played in parallel using a dynamic load balancing technique to divide the work across the processors. The practicality and effective speedup of the algorithm is illustrated by performance graphs.
Keywords: Model Checking, Shared-Memory, Parallelisation, Automata, Game Theory


Introduction
Model checking is an established technology for automated verification of de- signs, now adopted by industry to check correctness properties of many critical systems. The tremendous advances that have been made over the past decade in developing specialised state encodings and algorithms to reduce the burden of the state explosion problem, inherent with this style of verification, have been paramount to this industrial take-up. Even so, the size and complexity of

1 This work was fully supported under a Universities UK ORS award, a University of Manchester Department of Computer Science Scholarship, and a South African Harry Cross- ley Bursary.
2 inggscp@telkom.co.za
3 howard@cs.man.ac.uk


1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.10.022


systems that can be verified is still heavily constrained by time and available memory, and the development of techniques to alleviate the state explosion problem remains an active area of research. One technique that has gained significant interest recently is the parallelisation of model checking.
There were a few isolated publications on the parallelisation of model checkers in the 1980s and 1990s and then Stern and Dill’s seminal paper for parallel reachability analysis appeared in 1997 [17]. In the past three to four years parallel model checking has gained considerable interest.
Much of the extant research has focused on implementations over dis- tributed networks and the development of static partioning functions. Static partitioning functions depend on the state and not on the distribution of the workload. To the best of our knowledge the only algorithms that use a dynamic partitioning function are the symbolic algorithm of Heyman et al. [12] and the two algorithms based on Heyman et al.’s article [3,11]. In these algorithms the memory balance is maintained by repartitioning the state space whenever the memory becomes unbalanced. Initially only safety checking was parallelised, but in the last few years the developement of parallel algorithms for liveness checking increased and algorithms for checking LTL [2,6,16,15], CTL [7], and the µ-calculus [5] have been developed. See [13] for a full bibliography. The development of efficient parallel algorithms for liveness checking have been less successful than for safety checking and very few parallel algorithms for checking both liveness and safety properties achieve speedups.
We explored the parallelisation of model checking for shared-memory mul- tiprocessor computers to evaluate its feasibility and identify any inherent dif- ficulties or pitfalls when parallelising model checking for shared memory ar- chitectures. In particular, the parallelisation of explicit-state on-the-fly model checking was investigated for both safety and liveness properties and led to the development of a parallel model checker for CTL*. This research has shown the practicality and effective speedup of model checking using a shared- memory architecture. The performance of the parallel algorithm was evalu- ated via theoretical analysis and also via experimental analysis using a number of prototypical models, including correctness properties of the parallel model checker itself.
In our earlier paper, [14], we proposed a parallel algorithm for reachabil- ity analaysis on a shared-memory architecture. In this paper we present a parallel model checking algorithm for CTL* that uses the dynamic load bal- ancing technique of the parallel reachability analysis algorithm described in [14]. An overview of the parallel reachability analysis algorithm is given in the next section. This is followed by a description of the serial automata-driven game-theoretic algorithm for CTL* and its parallelisation in Sections 3 and 4.


The performance of the algorithm is analysed and discussed in Section 5 and conclusions are presented in Section 6.

Parallel Reachability Analysis
Our parallel reachability analysis algorithm is executed on N processes where each process (thread of control) runs on one physical processor. All N pro- cesses share one store for storing visited states and each process has its own unbounded private stack and bounded shared stack for storing unexpanded states. A process can add states only to its own private and shared stacks, but when its own private and shared stacks are empty it can steal a state, i.e., remove a state from the shared stack of another process.

Procedure ParallelReach(initState)
start parallel
if (procid = 0) then next := initState;
else next := EMPTY; endif;
do
if (next = EMPTY) then next := PopFromStack(); endif;
if (next /= EMPTYt)hen
ComputeSuccessors(next);
else
CheckForTermination(procid);
endif;
while (чterminate);
end parallel;
endprocedure

Fig. 1. Pseudocode for a parallel reachability analysis algorithm
The pseudocode for the parallel reachability analysis is given in Fig. 1. Each participating process executes a copy of the code between the start par- allel and end parallel directives. Termination is detected by the low overhead token-based algorithm of Dijkstra et al. and the pseudocode for Compute- Successors, PopFromStack and PushOnStack is given in the Appendix. Each shared stack has a lock to synchronise read and write access to it, but the store, a hash table, has no mutual exclusion locks to synchronise access. This can result in duplicate work when more than one process creates the same state, but with significantly more parallel computation available than natu- rally parallel tasks, performing redundant work is not a significant overhead. It was found, however, that very little work is duplicated. For example, when the model of the parallel model checking algorithm described in this paper


was checked for freedom of deadlock on eight processors, just over 7.8 million states were visited and only six states were duplicated.

AltMC: A Serial Model Checker based on Alternat- ing Automata and Game Theory
The model checking problem can be stated as follows: given a Kripke structure K and temporal logic formula φ determine if K |= φ. Formally, a Kripke structure is a four-tuple K = (S, R, s0, L) where S is a finite set of states, R ⊆ S × S is a transition relation that must be total (for every si ∈ S there exists at least one sj such that (si, sj) ∈ R), s0 is an initial state, and L : S '→ 2P maps each state to a set of atomic propositions true in that state. Several approaches have been developed to solve the model checking problem. The algorithm described in this article follows the automata-driven approach, which is based on the principles that a formula φ can be translated to an automaton Aφ that accepts the set of models for φ, and that the Kripke structure K can be seen as an automaton. The model checker then constructs the product automaton AK,φ = K × Aφ and if the language accepted by AK,φ is nonempty, φ holds for K, otherwise not [18,4]. In our context we use Hesitant Alternating Automata [4] to represent formulae specified in the branching time temporal logic CTL*, and formulate the construction of the product automaton Aφ and its nonemptiness check in terms of a game, called the nonemptiness game [19].
As a brief introduction, automata over infinite trees (tree automata) run over leafless Σ-labelled trees, where Σ is a finite alphabet. A run r of an alternating automaton A on a tree T is a tree where the root is labelled by (s0, q0) and every other node is labelled by an element of (N∗ × S) 4 . Each node of r corresponds to a node of T . A node (x, q) in r corresponds to the automaton in state q reading node x in T . Note that many nodes in r can correspond to the same node in T . The labels of a node and its successors have to satisfy the transition function.
A run r is accepting if all its infinite paths satisfy the acceptance condi- tion. Note that we can get finite branches in the tree representing the run when either true or false is read in the transition function. In an accepting run only true can be found at the end of a finite branch. Different types of alternating automata have different acceptance conditions. In Hesitant Alter- nating Automata (HAAs) the acceptance condition is a pair of states (G, B) of which the satisfaction depends on the following restriction on the transition

4 A tree, here, is modelled as a subset of N∗, where each sequence of N uniquely identifies a node of the tree by its pathname.


Table 1
Winning conditions for a play in the non emptiness game


structure of an HAA: the sets of an HAA can be partitioned into disjoint sets Si and there exists a partial order ≤ between the sets. The sets can further be classified as transient, existential, or universal, such that for each Si, and for all s ∈ Si,a ∈ Σ, and k ∈ N the following holds: (1) if Si is transient, then δ(s, a, k) (δ is the transition function of the HAA) contains no elements from Si; (2) if Si is existential, then δ(s, a, k) contains only disjunctively re- lated elements of Si; and (3) if Si is universal, then δ(s, a, k) contains only conjunctively related elements of Si. From this restricted structure of HAA it follows that every infinite path, π, will get trapped either in an existential or universal set, Si. The path then satisfies (G, B) if and only if either Si is existential and inf(π) ∩ G /=∅ or Si is universal and inf(π) ∩ B = ∅, (inf(π) is the set of states infinitely repeated on path π).
Checking the nonemptiness of the product of the HAA φ and Kripke struc- ture K can be defined as a two-player game in which player 1 (Brandy) tries to show that the alternating automaton is empty whilst player 2 (Port) tries to establish that it is nonempty [19]. A play of the game is a possibly infi- nite sequence of positions (s0, q0), (s1, q1),.. ., where each position is a node in the product (called an and–or tree in the sequel) of the Kripke structure and the alternating automaton for the formula. The structure of the and–or tree determines which player makes the next move. The winner of a play can be established when either a node that is labelled true (Port wins) or false (Brandy wins) is found in the play, or when a position in the current play is revisited, i.e., an infinite path is found. When an infinite path is found, the acceptance condition is then considered in order to determine the winner of the play. The cases are summarised in Table 1.
The serial implementation uses a depth first search (DFS) algorithm with a stack to store the current path. An infinite path is then given by all the


elements on the stack between the depth where a position is revisited and the current depth of the stack. For efficiency a store keeps track of results for states from which all moves have been made, so that when they are revisited the results can be reused, but then it may happen that an incorrect result is stored, since play is truncated whenever a position is revisited. To ensure that a result is correct when stored, a new game is played for a state once all moves from that state have been played. This new game uses a new results store and stack, so that any infinite play that might have been truncated during the previous play is now played to completion. New games can be played recursively, so to ensure termination, new games are not played from states for which new games are already being played. Once a new game for a state has been completed the new game’s stack and results store are deleted and the result is stored in the original results store.

PMC: A Parallel Model Checker based on Alternat- ing Automata and Game Theory
In the serial algorithm the nonemptiness game is played following the DFS path created by a serial reachability analysis algorithm. In the parallel algo- rithm the nonemptiness game is played using the parallel reachability analysis algorithm described in Section 2. This latter choice has two consequences. First, in a parallel analysis the and–or tree is no longer explored in a depth- first manner as in the serial case. Instead, an unexpanded position is removed from a stack and can therefore be a random position from anywhere in the and–or tree. Consequently, at any point, a set of paths each at a possibly different stage of generation is explored concurrently. Second, since there is no single stack, positions are added to the store as and when they are gen- erated, so that other processes can detect visited positions. This means that when a process revisits a position it should still determine whether revisiting the position closes a cycle or not. A further implication is that a process can reach an old position while another process is still busy computing the result for it. The algorithm can handle this scenario in two ways. When an old state sold is reached, the process can continue computing the result of the old state and thereby duplicate the work of the first process that reached sold, or sold can keep track of all the positions waiting for its result and forward the result to them once the winner has been established. We implemented the latter.
The other issue we need to consider is the influence of reusing results on the final winner of the game. Recall that the winner of a play is determined when either a true or false terminal state is reached or when a position on the current path is revisited (an infinite play is found). When a process reaches


a terminal true or false state it can immediately store the result and forward the result to the position’s predecessors. When a process revisits a position, and the result of the position is not known, the algorithm must first determine whether it is on an infinite play, and if so, whether Brandy or Port wins on the play. Both goals are achieved by playing a new game from the old state. To avoid duplicating work, a new game is only played from an old state if no other process is currently playing a new game from that state. When a process reaches an old position sold, while expanding a position spre, and another process is currently playing a new game from sold, spre is added to sold’s predecessors list. Then, once the new game is finished the result will be forwarded to spre. New games are played locally on one processor and therefore the serial nonemptiness game described in Section 3 can be played. Note that only results from completed local new games or a terminal true or false state are stored and reused. Since these results are correct, no further new games are needed.
In the parallel model checking algorithm, the game logic is embedded in the parallel reachability analysis algorithm that was described in Section 3. The parallel reachability analysis algorithm follows three steps: (1) idle – obtain a state, (2) idle – check termination, and (3) busy – compute succes- sors. In the parallel nonemptiness game these same steps are executed with two exceptions. The stacks now store work items and not states, and the action performed during step (3) now depends on the work item. There are four possible work items: Expand, Play, Result, and Play–Local–Game. The pseudocode for the main program logic of the parallel nonemptiness game is, except for line 8, identical to the parallel reachability analysis algorithm in Fig. 1. In the parallel nonemptiness game, Procedure ComputeSuccessors on line 8 is replaced by Procedure ProcessWorkItem. The pseudocode for these functions is given in the Appendix. The actions ProcessWorkItem per- forms for the different work items are: Expand Expand the expression in the alternating automaton table that corresponds to the current alternating au- tomaton state and the truth values of the propositions at the current Kripke state; Play Make a move in the product automaton by combining the new al- ternating automaton state with all the successors of the current Kripke state; Result Store the result of the current product state and then forward the result to all the states in the predecessor list waiting for the result of this state; Play–Local–Game Play a local new game from this state. After the new game has been played, a Result work item is created.

Correctness
The correctness of the parallel algorithm has been proved by first establish- ing that the serial and parallel algorithms will construct isomorphic and–or graphs. Then it was established that the serial nonemptiness game and the parallel nonemptiness game will determine the same winner from the initial position of an HAA for a given Kripke structure and CTL* formula by proving that the serial nonemptiness game and the parallel nonemptiness game will determine the same winner for a specific path, that the serial nonemptiness game and parallel nonemptiness game will determine the same winner for a specific position in the and–or tree, that the stored results can be reused, and that the forwarding of results in the parallel algorithm is correct [13]. Other properties about the parallel algorithm of PMC, such as freedom from deadlock have been verified by using both SPIN and PMC itself [13].

Performance
Ideally, an algorithm that runs in time TS on one processor will run in time TP = TS/P on P processors. Unfortunately TP is almost always greater than TS/P , because of extra overhead involved in executing the code on more than one processor. More accurately TP = TS/P + θP , where θP , the overhead term, is the difference between actual and ideal execution time [1,8].
There are several parallelisation costs that account for overhead. The first, θIP, is overhead because of insufficient parallelism. A parallel algorithm typically has a sequential component that cannot run in parallel, for example the combined startup and tidyup time of the algorithm. This overhead is often captured as Amdahl’s law [10], which states that if the sequential component of an algorithm accounts for 1/s of the program’s execution time, then s is the maximum possible speedup that can be achieved on a parallel computer. The second cost, θSched, is scheduling overhead; this is the time that each thread needs to execute scheduling code at the beginning of a parallel section. The third cost, θLI, is a result of load imbalance. The fourth cost, θSync, is the synchronisation overhead; it accounts, for example, for the time needed to acquire or release a lock. The final cost, θUnknown, is for extra overhead that has not been accounted for in the other terms, for example memory access time. The time that an algorithm runs on P processors can therefore be given as: TP = TS/P + θIP + θSched + θLI + θSync + θUnknown
The values for the different overheads can be measured by executing a reachability analysis on 1 process and using high resolution timers to measure the time it takes to execute different chunks of code. The value of TP can then be calculated for P = 1 to 16 to predict the performance of the algo-


rithm. This can then be compared against the measured values for TP during experiments. The performance of a parallel algorithm can be visualised by plotting 1/TP against the number of processes P . These performance graphs have the advantage that they provide a visual representation of the parallel algorithm’s scalability and at the same time the exact execution times can still be computed from the values presented in the graph.



Theoretical vs Experimental Performance
To model the theoretical performance of the parallel reachability analysis al- gorithm the overhead values for the parallel algorithm were measured and two different models were considered: Model A, which captures the best possible behaviour of the algorithm where there is no idle time and Model B, which captures a worst case scenario, where the stealing of states has to be synchro- nised with other accesses to the same shared stack and each time a process wants to steal a state it has to wait for all the other processes to gain and release the lock before it can access the shared stack. The performance graphs of Model A and Model B are shown in Fig. 2a.
The third performance graph in Fig. 2a shows the real performance of the algorithm; the performance graph was obtained from results of experiments that were run to time the reachability analysis on a 100 million state graph with a branching degree of 3. For each value of P , the reachability analysis was executed ten times and the performance graph labelled real, in Fig. 2a, shows the average over each set of ten runs. The vertical lines at regular intervals show the fastest and slowest result obtained during the ten runs on the corresponding process. The average over a number of runs is taken because there are many variants which influence the running time, e.g., the unpredictability of cache line usage inferred by cache contention, and the order in which processes gain access to the shared stack. Analysis of the results showed that the average has stabilised within its ten runs.
The performance of the parallel algorithm is very good and scales well with an increase in the number of processors. The implementation takes approxi- mately 4 minutes to complete the reachability analysis of a 100 million state graph on one process and approximately twenty seconds on sixteen processes. Model B shows similar behaviour to the experimental results of the imple- mented algorithm if its idle overhead is changed to model a scenario where a process that wants to steal a state has to wait for only one other process to complete a single access to the shared stack before it gains access to it.




0.07

0.06

0.05

0.04

0.03

0.02

0.01
MSMP: 100m states



1.2

1

0.8

0.6

0.4

0.2
Multiple Shared Multiple Private Stacks (63 423)


0
2	4	6	8	10	12	14	16
Number of Processors

0
2	4	6	8	10	12	14	16
Number of Processors


Fig. 2. (a) Theoretical vs real performance of exploring a 100 million state graph (b) Checking deadlock for an ESML model of the parallel reachability analysis algorithm

Experimental performance on ESML Models
To evaluate the performance of the model checker on real models, PMC was integrated with a state generator for ESML, a Promela-like modelling language [9]. Fig. 2b shows the performance of checking deadlock for an ESML model of the parallel reachability analysis algorithm described in Section 2 and Fig. 3a shows the performance graph of checking the liveness property EFAGp for a communications model. More graphs are given in Fig. 7 and Fig. 8.
For checking safety properties with the parallel model checker, effectively the same speedup resulted as for parallel reachability analysis. However, the properties are typically satisfied or violated before the entire state graph has been searched and can also be satisfied at different depths on different paths. The parallel algorithm also tends to find shorter paths than a model checker with a sequential DFS algorithm. For checking liveness properties the results varied, but when there is a speedup liveness checking also scaled well with an increase in the number of processors. Liveness checking requires more work (longer independent jobs) than safety checking, but requires extra synchroni- sation to store the information needed for checking cycles. However, the effect of longer independent jobs outweighs the effect of the extra synchronisation and generally better speedup is obtained than with safety checking.
It was further found that the variation in performance of liveness checking is usually higher than for safety checking, because the order in which states are visited also influences when and from which states local games are played. In some cases this variation is particularly high. As an example the performance of the model checker when checking a liveness property AGEFp for a Producer– Consumer model is depicted in Fig. 3b. On six processors, for example, the property was checked in 36 seconds after visiting 4000 states (excluding the states visited during local games) during the slowest run and in one second




0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0
Communications Model: Checking EFAGp0


2	4	6	8	10	12	14	16
Number of Processors



0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Producer Consumer Model: Checking AGEFp0


2	4	6	8	10	12	14	16
Number of Processors


Fig. 3. Checking (a) EFAGp for a Communications Model and (b) AGEFp for a Producer–Consumer Model
after visiting 250 states during the fastest run.
The number of local games that is played also has an influence on the per- formance of the model checker and, for a particular liveness property, depends on the model being checked. Often a large number of local games need to be played compared to the total number of states of the finite-state machine generated for the model. This has the effect that some processes are busy playing local games while others run out of work and then stay idle for long periods of time while local games are played; see Fig. 8d.

Conclusions
Our first objective was to evaluate the feasibility of parallel model checking on shared memory architectures and it is clear from the experimental results that model checking can be efficiently implemented on these architectures. The speedups are good and the dynamic load balancing algorithm works effectively; there is practically no idle time due to unbalanced load, except when the number of local games is high compared to the number of global states. The second objective was to identify any pitfalls when parallelising model checking for shared memory architectures. The main pitfalls that were identified are synchronisation overhead and false sharing (see Fig. 9), which occurs when two or more processors attempt to write to different words in the same cache line. It was found that, for efficiency, mutual exclusion locks should be used with care and memory should be allocated so that false sharing is avoided.
For future work we plan to implement an improved algorithm that re-uses all intermediate results of local games to avoid redundancy and provide work faster. More generally we also identified a need for benchmark models and a full investigation into the integration of serial state space reduction techniques into parallel algorithms.

Acknowledgement
In addition to our sponsors, we also wish to thank in particular the staff of the CNC (Centre for Novel Computing) within the Department of Computer Sci- ence for helpful discussion on the parallelisation approach and implementation and for dedicated access to their SGI Origin 3400 machine.

References
M. K. Bane and G. D. Riley. Extended overhead analysis for OpenMP. Lecture Notes in Computer Science, 2400:162–166, August 2002.
J. Barnat, L. Brim, and J. Stˇr´ıbrna´. Distributed LTL model-checking in SPIN. In Proceedings of the 8th International SPIN Workshop on Model Checking of Software, volume 2057 of Lecture Notes in Computer Science. Springer–Verlag, May 2001.
S. Ben-David, T. Heyman, O. Grumberg, and A. Schuster. Scalable distributed on-the-fly symbolic model checking. In Proceedings of the 3rd International Conference on Formal Methods in Computer-Aided Design (FMCAD’00), pages 390–404, Austin, Texas, November 2000.
O. Bernholtz. Model Checking for Branching Time Temporal Logics. PhD thesis, The Technion, Haifa, Israel, June 1995.
B. Bollig, M. Leucker, and M. Weber. Local parallel model checking for the alternation-free mu-calculus. In Proceedings of the 9th International SPIN Workshop on Model Checking of Software (SPIN 2002), Lecture Notes in Computer Science, Grenoble, France, April 2002. Springer–Verlag.
L. Brim, I. Cˇern´a, P. Krˇc´al, and R. Pel´anek. Distributed LTL model-checking based on negative cycle detection. Lecture Notes in Computer Science, 2245, 2001.
L. Brim, J. Crhov´a, and K. Yorav. Using assumptions to distribute CTL model checking. In Proceedings of the 1st International Workshop on Parallel and Distributed Model Checking (PDMC 2002), pages 80–95, Brno, Czech Republic, 2002.
J. M. Bull. A hierarchical classification of overheads in parallel programs. In I. Jelly, I. Gorton, and P. Croll, editors, Proceedings of the 1st IFIP TC10 International Workshop on Software Engineering for Parallel and Distributed Systems, pages 208–219. Chapman Hall, 1996.
P. J. A. de Villiers and W. Visser. ESML—a validation language for concurrent systems. In
Proceedings of the 7th Southern African Computer Symposium, pages 59–64, July 1992.
I. Foster. Designing and Building Parallel Programs. Addison-Wesley, 1995.
O. Grumberg, T. Heyman, and A. Schuster. Distributed symbolic model checking for µ- calculus. In G. Berry, H. Comon, and A. Finkel, editors, Proceedings of the 13th International Conference on Computer Aided Veriﬁcation (CAV 2001), volume 2102 of Lecture Notes in Computer Science, pages 350–363, Paris, France, July 2001. Springer–Verlag.
T. Heyman, D. Geist, O. Grumberg, and A. Schuster. Achieving scalability in parallel reachability analysis of very large circuits. In A. P. Sistla E. A. Emerson, editor, Proceedings of the 12th International Conference on Computer Aided Veriﬁcation (CAV 2000), volume 1855 of Lecture Notes in Computer Science, Chicago, Illinois, July 2000. Springer–Verlag.
C. P. Inggs. Parallel Model Checking on Shared-Memory Multiprocessors. PhD thesis, University of Manchester, Manchester, January 2004.


C. P. Inggs and H. Barringer. Effective state exploration for model checking on a shared memory architecture. In Workshop on Parallel and Distributed Model Checking (PDMC’02), volume 68 of Electronic notes in Theoretical Computer Science (ENTCS), Brno, Czech Republic, 2002.
P. Krˇca´l. Distributed explicit bounded LTL model checking. In L. Brim and O. Grumberg, editors, Proceedings of the 2nd International Workshop on Parallel and Distributed Model Checking (PDMC 2003), volume 89 of Electronic Notes in Theoretical Computer Science, Boulder, Colorado, July 2003. Elsevier.
A. L. Lafuente. Simplified distributed LTL model checking by localizing cycles. report 00176, Institut fu¨r Informatik, Universit¨at Freiburg, July 2002.
U. Stern and D. Dill. Parallelizing the Murφ verifier. In O. Grumberg, editor, Proceedings of the 9th International Conference on Computer Aided Veriﬁcation (CAV’97), volume 1254 of Lecture Notes in Computer Science, pages 256–278, Haifa, Israel, June 1997. Springer–Verlag.
M. Y. Vardi and P. Wolper. An automata theoretic approach to automatic program verification. In Proceedings of the 1st Symposium on Logic in Computer Science, pages 322–331, 1986.
W. Visser and H. Barringer.  Practical CTL model checking: Should SPIN be extended?
Software Tools for Technology Transfer, 2(4):350–365, March 2000.

Appendix


Procedure ParallelReach(initState)
start parallel
if (procid = 0) then next := initState;
else next := EMPTY; endif;
do
if (next = EMPTY) then next := PopFromStack(); endif;
if (next /= EMPTYt)hen
ComputeSuccessors(next);
else
CheckForTermination(procid);
endif;
while (чterminate);
end parallel;
endprocedure






Procedure ComputeSuccessors(s)
forall successors t of s do
if (t is new) then
AddToStore(t);
if (t is first successor) then s := t;
else PushOnStack(t); endif;
endif;
endforall;
endprocedure


Fig. 4. Procedure ParallelReach and a subfunction for ParallelReach


Procedure PopFromStack(s, id, privstack)
if (notempty(privstack)) then
s := Pop(privstack);
endif;
if (s = EMPTY && notempty(sharedstacks[id])) then
ompLock(stacklock[id]);
s := Pop(sharedstacks[id]);
ompUnlock(stacklock[id]);
endif;
while (s = EMPTY && more shared stacks to check) do
id := next processor;
if notempty(sharedstacks[id])) then
ompLock(stacklock[id]);
s := Pop(sharedstacks[id]);
ompUnlock(stacklock[id]);
endif;
endwhile;
endprocedure





Procedure PushOnStack(s, id, privstack)
if (notfull(sharedstack)) then
ompLock(stacklock[id]);
Push(sharedstack, s);
ompUnlock(stacklock[id]);
else
Push(privstack, s);
endif;
endprocedure


Procedure ParallelGame(initState)
start parallel
if (procid = 0) then next := initState;
else next := EMPTY; endif;
do
if (next = EMPTY) then next := PopFromStack(); endif;
if (next /= EMPTYt)hen
ProcessWorkItem(next);
else
CheckForTermination(procid);
endif;
while (чterminate);
end parallel;
endprocedure

Procedure ProcessWorkItem(next)
switch (next.type)
case PLAY:
cur := next;
deadlock := Move(cur, next);
case EXPAND:
next := ExpandFormula(next);
case RESULT:
cur := next;
next := EMPTY;
ProcessResult(cur, next);
case PLAY–LOCAL–GAME:
PlayLocalGame(next);
endswitch;
endprocedure

Fig. 5. Procedure ParallelGame and subfunctions for both ParallelReach and ParallelGame











next.result.andOr := AND
next.type := EMPTY;
deadlock := MoveInKripke(cur, next);
case OR−SUCC:
/∗ same as AND−SUCC case, but AND replaced with OR ∗/
endswitch;
endif;
return deadlock;
endfunction
if (Empty(next)) then next := cur;
else PushOnStack(cur); endif;
endwhile;
tmp.id++;
endif;
endif;
ompUnsetLock(tmp.lock);
if (forward) then AddSlotToFreeList(tmp) endif;
endprocedure


Fig. 6. Functions for the parallel nonemptiness game













8

7.5

7

6.5

6

5.5

5

4.5

4






2

Gneiss Communications Model (2 108 states)


2	4	6	8	10	12	14	16
Number of Processors
(c) Gneiss Global Model (55 144 states)




2


1.5


1


0.5


0





0.05

Sliding Window Model (46 840 states)

















2	4	6	8	10	12	14	16
Number of Processors
(d) Dining Philosophers Model (1 270 080 states)



1.5


1


0.5

0.04


0.03


0.02


0.01




0
2	4	6	8	10	12	14	16
Number of Processors

0
2	4	6	8	10	12	14	16
Number of Processors

Fig. 7. The performance of an exhaustive reachability analyses on each of four ESML models; see the caption for the size of each model.





0.3


0.25


0.2


0.15


0.1


0.05





2.5


2


1.5


1


0.5

(a) MSMP Model: Checking AG(not(p0))

2	4	6	8	10	12	14	16
Number of Processors
(c) Mutual Exclusion Model: Checking EFAGp0




0.14

0.12

0.1

0.08

0.06

0.04

0.02

0





0.00035

0.0003

0.00025

0.0002

0.00015

0.0001

5e-05

(b) Communications Model: Checking AGEFp0

















2	4	6	8	10	12	14	16
Number of Processors
(d) Sliding Window Protocol Model: Checking AGEFp0


0
2	4	6	8	10	12	14	16
Number of Processors

0
2	4	6	8	10	12	14	16
Number of Processors

Fig. 8. The performance of the model checking algorithm when checking temporal logic properties for different models. (a) Checking AG¬p for a model of the parallel algorithm described in this paper; more or less 63400 states are visited per run. (b) Checking AGEFp for a Communications Model; between 1400 and 3000 states are visited per run. (c) Checking EFAGp for a Mutual Exclusion Model; between 230 and 260 states are visited per run. (d) Checking AGEFp for a Sliding Window Model; between 57000 and 57900 states are visited per run.





False sharing due to poor memory allocation

0.06

0.05

0.04

0.03

0.02

0.01

0
2	4	6	8	10	12	14	16
Number of Processors
Fig. 9. The Effect of False Sharing on Performance. In the implementation each process has three private variables to store the current state, the current successor, and the next state to be expanded. The graph labelled false sharing shows the performance of the algorithm when the three private variables of all the processes are allocated memory in the same cache line. The graph labelled good memory allocation shows the performance of the algorithm when the three private variables of each process are allocated memory in such a way that they do not end up in the same cache line as the three private variables of any of the other processes. As the performance graphs show, the effects of false sharing can be quite severe.
