Electronic Notes in Theoretical Computer Science 44 No. 1 (2001)
URL: http://www.elsevier.nl/locate/entcs/volume44.html 40 pages


Deforestation, program transformation, and cut-elimination

Robin Cockett 1,2
Department of Computer Science University of Calgary
Calgary, Alberta, Canada



Abstract
The problem of proving that two programs, in any reasonable programming lan- guage, are equivalent is well-known to be undecidable. In a formal programming system, in which the rules for equivalence are finitely presented, the problem of provable equivalence is semi-decidable. Despite this improved situation there is a significant lack of generally accepted automated techniques for systematically searching for a proof (or disproof) of program equivalence. Techniques for search- ing for proofs of equivalence often stumble on the formulation of induction and, of course, coinduction (when it is present) which are often formulated in such a manner as to require inspired guesses.
There are, however, well-known program transformation techniques which do address these issues. Of particular interest to this paper are the deforestation techniques introduced by Phil Wadler and the fold/unfold program transformation techniques introduced by Burstall and Darlington. These techniques are shadows of an underlying cut-elimination procedure and, as such, should be more generally recognized as proof techniques.
In this paper we show that these techniques apply to languages which have both inductive and coinductive datatypes. The relationship between these program trans- formation techniques and cut-elimination requires a transformation from initial and final “algebra” proof rules into “circular” proof rules as introduced by Santocanale (and used implicitly in the model checking community). This transformation is only possible in certain proof systems. Here we show that it can be applied to cartesian closed categories with datatypes: closedness is an essential requirement. The cut- elimination theorems and attendant program transformation techniques presented here rely heavily on this alternate presentation of induction and coinduction.



1 Email: robin@cpsc.ucalgary.ca
2 Partially supported by NSERC, Canada.
◯c 2001 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.

Introduction
The problem of proving two programs equivalent in any reasonable program- ming language is well-known to be undecidable. In formal programming sys- tems, in which equivalence is determined by a finite set of rules, the situation is brought under better control as one can, at least, secure the semi-decidability of the problem. Despite this improvement, even in formal systems, there is still a significant lack of generally accepted automated techniques for system- atically searching for a proof (or disproof) of the equivalence of two programs. Techniques for searching for a proof that two programs are equivalent often flounder on the formulation of induction and, of course, on coinduction (when coinductive types are present). This is reinforced by two strongly entrenched traditions:
The first is the mathematical tradition of inductive proofs in which an in- spired “guess” to formulate an inductive step is often portrayed as being unavoidable. Not only is this view of developing proofs anathema to au- tomation but it has also fueled the perception that searching for inductive (and therefore coinductive proofs) is intrinsically difficult.
The second is the computer science habit of allowing general recursive pro- grams which need not terminate. This means that thrown into any (induc- tive) proof of equivalence there is a second problem of showing the equiva- lence of termination conditions. This not only complicates the proof system but it also creates a psychological “black hole” into which two traditional impossibilities 3 have been concentrated!

Program equivalence in formal systems
The objective of this paper is to provide a view of the proof theory of inductive and coinductive datatypes which justifies the discomfort with the traditional inductive proof techniques mentioned in the first point. In order to do this I will present a formal programming language and discuss its proof theory. I will dodge the “black hole” mentioned in the second point above by choosing a the formal system which has good termination properties. The precise meaning of “good termination properties” is a little technical. While this is not the main focus of the paper, in order to understand more precisely what program equivalence actually means in these formal programming systems it is useful to understand more precisely what the ability to evaluate programs provides and does not provide.

3 It is beyond the scope of this paper, but perhaps worth mentioning, that recent advances in the equational understanding of partiality (see [6,3]) show that it is possible to formulate simple equational programming logics which give a formal account of partiality. This means that even out of this “black hole” might yet be teased a surprising amount of light!

Evaluation in formal programming systems
The language I shall consider is a variant of charity [5]: it has both inductive and coinductive datatypes. This means that we cannot promise termination in its full force. Consider, for example, the program which produces the infinite list of primes (which can certainly be written in this language): clearly it is impossible to produce the whole list. Instead the language has the weaker ability to reduce in a terminating manner to a “head normal form.” This allows the primes, in our example, to be produced one-by-one on a demand basis with the guarantee that we can always produce the next prime in a terminating manner. For further discussion of these issues see [16].
A head normal form will often contain further unevaluated material which can be unlocked by the process of coinductive destruction. This gives rise to a “lazy” evaluation strategy for coinductive types which is not simply a side- effect of evaluation order: it is quite fundamental to the whole type system. Outside the coinductive records (which, recall, are predetermined by type) one can perform evaluation in any order. A coinductive record in effect freezes its arguments. This means that one can in fact mix (with potential efficiency advantages) by-value evaluation and lazy evaluation in this sort of language without changing the termination properties.
Now evaluation only applies to closed terms and thus, strictly speaking, cannot resolve the question of equivalence of programs – which will not usu- ally be closed. Of course, in a higher-order language we may internalize arbi- trary programs as closed terms of a higher type. However, these higher types are coinductive in nature so that evaluation simply produces the unevaluated program as a head normal form and, thus, no extra information is gained concerning equality.
However, before we dismiss evaluation it is worth recalling that it can be used to disprove equivalence. Essentially the idea is to find ground “values” (or “points”) on which the two programs perform differently. For an arbitrary program this is achieved by a combination of providing values, and destructing to ground types. These processes are both enumerable so can be undertaken in a methodical manner. Clearly this technique of distinguishing programs using their pointwise behavior should be an integral component of any automated proof system as trying to prove something which is obviously false can cause a great deal of wasted effort!

Indistinguishable and formally equivalent programs
We note that this discussion has revealed a fundamental (and possibly rather disturbing) feature of formal programming systems: two programs whose be- havior cannot be distinguished on points are not necessarily provably equal. If they were we would immediately have a decision procedure for equality - which we cannot have! Thus, the indistinguishable programs and the provably equivalent programs in such systems are never the same.
Thus, the fact that there are no points on which two given programs can be

distinguished does not imply that there is necessarily a proof that the two pro- grams are equivalent. This, in turn, raise the unhappy specter that the proof system may be so weak that one may routinely encounter programs which are clearly indistinguishable on “points” and yet cannot be proven equivalent!
In fact, in the system I shall be considering below this is not the case. It is known that equivalent and indistinguishable programs coincide for low complexity programs and therefore for these programs the equivalence problem is decidable. However, the precise manner in which program complexity relates to decidability – that is where indistinguishable and equivalence of programs coincides is an open question of some interest.

On formulations of induction and coinduction
We now turn more directly to the issue of program equivalence and, in particu- lar the question of how to tame inductive and coinductive program equivalence proofs.
The first small step away from standard mathematical induction based on the natural numbers is the step to, so called, structural induction. This allows a convenient expression of inductive arguments for arbitrary inductive (or initial) datatypes in much the same form as mathematical induction where the form of the inductive step is determined by the structure of the datatype (see Slind [13]).
However, as a principle which could encompass coinductive as well as in- ductive datatypes in a programming setting it has significant shortcomings. To start with its formulation relies on the presence of a calculus of subobjects (subsets): a structural inductive argument essentially works by showing that the subobject determined by a proposition must, in fact, be the complete ob- ject. Now it is arguable whether program logics a priori come equipped with a calculus of subobjects strong enough to support these sorts of inductive arguments.
Putting aside our concerns over the level of logic implied by these argu- ments, we are immediately forced to a further level of discomfort by what this approach implies for coinduction. For the analogous theory of structural coinduction would have to use a quotient structure. Again one may, this time with more force, argue that this is absent at the level of program logic. Fur- thermore, this structure compared to that for subobject – if indeed we allow that it is present – is considerably less well-behaved. In practice one has to simulate this structure through the subobject structure and this leads one ultimately to the theory of bisimulation.
Now, I do not want to suggest that the theory of bisimulation for coin- ductive types is without value: in fact, I believe it is an important tool in our understanding of coinduction. However, to reach this point one cannot fail to notice that one has had to employ a series of sophisticated contortions. In particular, these steps completely obscure the symmetry between inductive

and coinductive types as one has been forced to overlay the basic program logic with further structure which heavily emphasizes the asymmetry of the underlying setting.
I think, therefore, it becomes a reasonable question to ask whether there might not be more direct approaches than this to a proof system that encom- passes coinduction.
The categorical formulation
An obvious alternative starting point, in which the symmetry between in- duction and coinduction is given by duality , uses the categorical universal property associated with inductive (initial) and coinductive (final) datatypes. This is familiar to category theorists who would claim that this the funda- mental property expected of datatypes. The basic idea (see section 3) is that inductive (coinductive) datatypes are initial algebras (resp. final coalgebras) and as such have uniquely determined maps to all other algebras (resp. from all other coalgebras).
However, there is a major problem with this (categorical) abstract view: it is difficult to use as a proof principle. In particular one may need con- siderable inspiration to produce an algebra (or coalgebra) which secures a desired equivalence. Thus, from the standpoint of proof automation, this is discouraging and hardly an improvement on the original situation described for mathematical induction.
However, we should not loose sight of the fact that symmetry has been regained. Furthermore, these notions do seem to capture the fundamental properties of these structural types. Thus, any system we might choose to devise should, at a very minimum, satisfy the abstract categorical properties of initial and final datatypes.
Lessons from model checking
Initial and final algebras, under the guise of least and greatest fixed points, are also used extensively in model checking and, in particular, the modal-µ calculus. These developments have found a lucrative application in hardware and protocol verification. While these applications are concerned with pose- tal models the abstract proof theory involved applies to programs viewed as proofs.
Strikingly the more successful of these logics abandon the initial and final fixed point rules (which are the categorical initial and final “algebra” rules but in that community they are often called “the Kozen rules” [9]) in favor of more complex proof techniques. I will refer to these as “circular proof rules” following Santocanale [11] who introduced them to me in roughly the form I will use here. The advantage of this movement away from the simple fixed point rules is significant as these systems allow one to see quite easily that derivability, that is the ability to prove one thing from another, is decidable. While this is not the problem we wish to solve it is an indication of an

increased power that these circular proof systems provide. From a proof the- oretic point of view there is also a strong imperative to adopt this modified proof system: the categorical or Kozen system does not have a very satisfac- tory formulation of cut elimination.
At this point in time it may be rather hard to substantiate, from the model checking community itself, the claims that I am making. Model checkers do not generally take a proof theoretic view of the world. Indeed there is no direct published proof of cut-elimination for the modal-µ calculus even with the “circular proofs” (as far as I know). Instead there is a game theoretic argument which translates between proofs in the modal-µ calculus and winning strategies between games and then argues that winning strategies can be composed. A direct proof is, in fact, simpler and will appear, together with a suitable formulation of the modal-µ calculus, in Aldwinckle’s thesis [1].

The importance of cut-elimination
A basic property of a proof system is that one should be able to compose proofs: this is the import of the cut rule in logical systems. The ability to compose proofs in this manner is clearly absolutely fundamental to the view of programs as proofs which we wish to exploit here. A crucial observation of Gentzen [14] is that logical systems often allow one to eliminate the cut by a se- ries of proof rewritings. Basically this meant that if there was a proof from one proposition to another (which perhaps used some intermediate propositions) then there is a direct proof which does not use any intermediate propositions. Anyone familiar with Phil Wadler’s ideas on program deforestation [17] will see a parallel between the ideas there and the aim of cut-elimination. This is made more explicit in Simon Marlow’s thesis [10]. The idea of program defor- estation is precisely to remove from the program the building of intermediate datatypes. This removal often results in more efficient programs, as interme- diate structures are no longer built only to be destroyed, and so this has been
of great interest to the program transformation and compiler community.
It is tempting to think that cut elimination and deforestation are the same thing for programs. However, this is not the case: deforested terms can still have cuts present. Indeed, in some cases, there necessarily must be residual cuts. However, the cuts that do remain are “soft” in that they only rearrange structure - more precisely they are cuts essentially from the λ -calculus portion of the logic. The cuts that are removed are the “deforesting” cuts: these are the ones which cause the destruction of datatypes (including coproducts and products).
In a logical system the cut-elimination process has further significance. It is a constructive process and its steps, which are usually not confluent actually determine equalities which are needed between proofs. 4 This is why

4 It is not the case that all proof equivalences are necessarily forced by cut elimination: logical systems can also have representation rules (such as “a comma on the left is equivalent


cut-elimination is also relevant to the discussion of program equivalence: thus, one can in some sense determine the notion of program equivalence from a cut elimination process. Now the form of the equivalence rules which arise naturally from these considerations in a circular proof system bear a striking resemblance to the fold/unfold methodology of program transformation [2]: this may help to explain why those ideas, despite being only partially correct, remain the basis for most of the high-level optimizing techniques for programs. I should mention that there is another important approach to program transformation which does use in a fundamental manner the categorical initial and final algebra presentation of the proof system. These are the fold/build- fusion techniques introduced in [?] and further extended in [?]. These laws rely on the fact that a fold applied to a datatype produced by a “producer” program can be presented as replacing the constructors of the produced datatype by the function arguments of the fold. This means that if we can abstract the (right) constructors in the producer program one can replace the composite by a simple higher-order application thereby eliminating this composition (or cut). This technique generalizes to coinductive dataypes. The difficult with the technique is, of course, to determine how to abstract the right constructors
in the producer.
Transforming the proof system
The transformation of the proof system from one based on initial and final datatypes (which I call the “algebra based” system) to a “circular proof” system was a relatively easy conceptual step for the model checking community because their models were posetal. This relieved them of the necessity of maintaining the equality of the proofs. Unfortunately, when one regards proofs as programs, proof equality becomes the major concern.
Also if proofs are to become programs in a programming language the manner of correctly expressing circular proofs becomes another matter of some interest. This direction has already been developed by David Turner: in fact, in [15] he argues that this style of programming, which he calls “elementary strong functional programming,” is the future of functional programming. The discussion in this paper links his ideas to the charity system by showing that his programming system (in so far as it is the circular proof system - and I have not checked this in detail) have the same power. It is a tribute to David Turner’s great programming intuition that he discovered and realized the significance of this system
It turns out that a circular proof system implies the presence of initial and final datatypes but the reverse implication is dependent on the particulars of the proof system (if it is true at all). Given the nice properties of a circular proof system it is tempting just to use circular proofs directly rather than rules which might be implied by initial and final datatypes. However, circular

to a product”) which may produce further proof identifications.


proofs interact ultimately with all the rules of the system and if the whole package does not fit together the system simply will not be well-behaved. Therefore, I go to some trouble to establish that the programming logic, which I introduce below, has both a circular proof presentation and an algebra based presentation.

Contents
After this rather lengthy introduction it is appropriate to show how these relationships play out for a particular programming logic. To this end we present a term calculus for cartesian closed categories with positive datatypes. We describe how it may be viewed as a proof system and illustrate some simple programming examples in the system. In this case the “circular” programs, which are very much in the functional style, provide an equivalent formulation to the one implied by initial and final datatypes. Finally we discuss the implied program equivalences, a technique for deforesting, and the possibilities for automating equivalence proofs.

Program logic for inductive and coinductive datatypes
We will set up the programming logic as a type theory as we wish to exemplify the propositions as types and programs as proofs paradigm. The semantic basis of our programming language is a cartesian closed category with positive datatypes. We shall take the view that the coproducts are primitive inductive datatypes while products and exponentiation with a fixed object (A ⇒ ) are primitives of the coinductive structure.
To avoid the usual issues which arise from having contravariant functors (and to keep things simple) we shall only admit as primitive functors:
The constant functors K;
The product functor X × Y , or Πi∈IXi for I a finite set, with 1 being the empty product;
The coproduct functor X + Y , or  i∈I Yi for I a finite set, with 0 being the empty coproduct;
Exponentiation with respect to fixed objects A as in A ⇒ X.
We shall refer to the functors derived from these basic functors as the polyno- mial functors of the setting. These, of course, can involve multiple free type variables. These basic polynomials are the basis for forming the datatypes of the setting: these include the polynomials and those types which can be obtained from the basic functors and the following two forms of type binding:
An inductive binding µX.T giving the initial datatype or least fixed point of the type T with respect to the type variable X;
A coinductive binding νX.T giving the final datatype or greatest fixed



Fig. 1. Sum, product, and closure rules
point of the type T with respect to the type variable X.
These constitute the positive types of the programming setting we describe below.
The term annotated rules of the program logic can be described in the three tables which now follow. The first figure 1 gives the inference rules governing sums, products, and exponentiation:
Here are some things to note about the terms defined in figure 1 for this logic which we should view as programs:
The index sets I are all finite with element denoted by i1, ..., in.
The variables “declared” on the left of the sequents must always be dis- tinct but can in general be product patterns. Thus the following is a valid sequent:
(fst : x1, snd : x2): Xfst × Ysnd ▶ x1 : X.
This exemplifies the “indexed” product notation. When there is no dan- ger of confusion we will tend to omit the indexing letting the order of components carry this information.
We follow the standard programming convention of separating the indexes of all products and coproducts so that the names of the indexes uniquely determine the type.
Products have terms which are indexed records:
(fst : t1, snd : t2): Xfst × Ysnd.
the projection from this record to the first coordinate is just the index of





that coordinate, fst.
Fig. 2. Structural rules

Maps from coproducts are expressed through “case” terms:

 rgt x '→ t1  : X

+ Ylft → Z

lft y '→ t2
and the coproduct embedding are denoted by the index.
The logic uses λ abstraction with application given by the infix operator @ which, as all operators do, binds more tightly that ordinary composition.
The next figure 2 gives the structural rules and, in particular, the cut rule: Notice that the first rule introduces identity rules for atomic types. It is important to realize that the identity map for non-atomic types must be de- rived. This restriction is common in logics and greatly simplifies the proof theory. However, from a programming point of view this looks a bit crazy! While it is possible to formulate a logic without the necessity to have “ex- panded” identity maps it does make program equivalence checking slightly more complex as the identities need to be expanded anyway. Here we shall adopt the standard logical convention because our main interest is in program
equivalence rather than program optimization.
Notice that this logic has explicit substitution given by the syntax
[x '→ t]t'
which intuitively means that x should be substituted by t' in t. Written as a “let expression” this is
let x = t' in t.


Notice also that the only rule which uses explicit substitution is the cut rule. Thus, for example, I have written the contr(action) rule using a direct sub- stitution: this maintains the correspondence between the use of cut and the occurrences of explicit substitution in the programs.
The juxtaposition of [x '→ t] and t' should be regarded as the expression of ordinary composition. Thus, in [x '→ t]f @s, as operators bind more tightly than composition, the application is calculated before the composition. Those familiar with functional programming and the λ-calculus may find it curious that we distinguish between application for higher-order terms and application of functions. In this respect, as we want our terms to represent proofs in the logic, we are simply being blindly faithful to the logic which makes this distinction.
Clearly the expression, [x '→ t]f @s, should be formally equivalent to the substitution t[(f @s)/x] and, in fact, it is. As we shall discover many of the cut-elimination steps which will force the presence of certain proof equiva- lences, are actually concerned with turning explicit substitution into ordinary substitution and in this sense are rather mundane. However, recall that ex- plicit substitution does permit the “variable” which is to be substituted to be a pattern which gives the cut-elimination steps a little more content.
Now it should be clear that the semantics of the proof theory of this frag- ment lies in cartesian closed categories which have coproducts. That this fragment satisfies cut-elimination is well-known as it is a variant of intuition- istic logic. Furthermore this fragment has its proof equivalence decidable. In the pure logic (that is with no atomic types and no non-logical axioms) this is actually almost immediate as the initial model is just finite sets. In the general case, where arbitrary types and axioms are permitted, the problem is much harder; but even in this general case the calculus is decidable (these issues are discussed in Ghani’s thesis [7] and in recent work of Altenkirch, Dybjer, Hoffman, and Scott [12]).
Finally we come to the rules of the logic which are of primary interest to this discussion. They are the rules which determine the behavior of the inductive and coinductive datatypes. The rules for the term construction are given in figure 3.
These need some explaining: we shall start with the cons(truction) and dest(ruction) rules as they are somewhat simpler. The construction rule allows one to build an inductive datatype from its component: thus, for example, a list can be built from an element and a list, via µ cons. or (primitively) as a nil list, via µ nil. The application of a µ rule turns a coproduct term into a list term. This syntax may seem a little peculiar as usually one does not distinguish between a coproduct term and an inductive datatype term. The way we have set up this logic requires that we make the distinction. We will often ignore this requirement in order to make the terms look more familiar. The destruction rule is dual: it allows us to break up a coinductive datatype into its constituents. Notice that destruction must be substituted in the term.



Fig. 3. Circular inductive and coinductive proof rules
It might tempting to use an explicit substitution here but this should be resisted for, as explained above, we are reserving explicit substitution to record the occurrence of the cut rule.
The remaining two rules are the inductive and coinductive circular proof rules. The form of these rules needs some discussion as they are not perhaps as familiar: essentially they allow the recursive specification of functions but in a tightly controlled manner. Thus these rules are essentially Landin’s letrec construction, of course, we need two variants and some extra notation besides so that we can track the recursive arguments.
Please note I am following the charity tradition of using curly parentheses to denote inductive items and round parentheses to denote coinductive items. We shall name the datatypes: for example the inductive natural number and list datatypes are defined by:
Nat = µx.1zero + xsucc


: rev1(:=, y' = nil) =
x '→	nil()	'→ y'
 
 x
 

cons(v, vs) '→ rev1(vs, µ cons(v, y'))
Fig. 4. Fast reverse
: rev2 :=	=
x '→	nil() '→ µ nil()
 


 x.
 

cons(v, vs) '→ app(rev2(vs),µ cons(v, nil))
Fig. 5. Naive reverse
List(A)= µx.1nil + (Afst × xsnd)cons.
The append function in this system becomes:
: app  =	

x, y '→  (x' :=, y') '→
 nil()	'→ y'
 x'  (x, y)

 	 cons(v, vs) '→ µ cons(v, app(vs, y')) 	 
where notice I have simplified the product notation. Notice also that we indicate the regenerated variable with the := symbol. There is another way we shall write this in order to reduce the nesting of parentheses:

: app(:=, y' = y) =
x, y '→	nil()	'→ y'
 
 x
 

cons(v, vs) '→ µ cons(v, app(vs, y'))
This brings the notation more closely into line with that used by functional languages and the notation used by charity. Notice how the recursive variable is put outside while the non recursive parameter is assigned where the function is introduced.
Another example of a program which makes a non-trivial use of the the power of these circular definitions is the “fast” reverse function. This is dis- played in figure 4: compare this to the higher-order definition see figure 13. The circular (or recursive) part of this function actually reverses a first list onto a fixed second list.
The naive version, see figure 5, of the reverse function uses the append function repeatedly inside an outer recursion.
Notice, in particular, that, despite the fact that I have not indicated them, there are a number of cuts in these terms. For example a circular term like app can only have variables as arguments so these must be supplied by a cut.

: monus(:=, :=) =
x, y '→	(zero, )	'→ µ zero (m, zero)	'→ m
 



(x, y).
  

(succ(m), succ(n)) '→ µ succ monus(m, n)
Fig. 6. Monus by simultaneous recursion
The second “naive” version of reverse, as we shall discover in section 4.3.2, is not deforested and can be automatically improved.
In general, we shall not be punctilious about recording cuts as there is a significant notational overhead instead we will feel free to use the substituted (although often not derivable) term. However, we remark that the presence of a cut is often going to be an indication that something can be simplified. Notice also that we will feel free to use names of previously defined circular functions.
The circular proof rule for inductive datatypes also allow for simultaneous recursion. Thus, one can have more than one inductive datatype being “un- raveled” at the same time. However, notice that one cannot do this for the coinductive datatypes because there is a fundamental asymmetry in the logic which allows lists of propositions (types) on the left but only one proposition (type) on the right. The ability to do simultaneous recursion is of practical importance as it allows efficient implementations of certain functions. Here is a well-known example: without simultaneous recursion or higher-order func- tion terms it is impossible to provide the efficient implementation of monus shown in figure 6.
Notice that the positions of the recursive arguments in the term logic are indicated by the := type binder so that, in a circular function f which is to be recursively applied, we know precisely which arguments are active. In the proof theory the circular function involves the introduced types Zi whose scope is delimited by the box. These type variables Zi are regenerations of the underlying recursive types: this information is indicated by the assignment Zi := µx.Pi(x). The regeneration relation is, of course, transitive. A type variable Z' which is a regeneration of another type Zi can be treated exactly as if it were the type Zi. Crucially the converse is not true, thus the circular function cannot be applied to an earlier generation of its type variable.
There is another important way in which a regenerated type variable differs from the datatype from which it originated: we do not allowed the construction (or destruction) rules to be applied to a regenerated variable. This is because such an application would reverse the regeneration process. However, we do allow a regenerated variable to have the circular proof rule of the datatype applied to it. It is thus possible to have multiple regenerations, that is for a regenerated variable to be itself regenerated.
To illustrate the power of regeneration consider the problem of producing,

: odd(:=)  =
x '→  nil()	'→ µ nil	 
 	  

 cons(v, vs) '→ µ cons(v,  nil()	'→ µ nil)	   vs)  

 
cons( , L') '→ odd'(L'))
Fig. 7. The odd function
 



List(A) ▶ List(A)

Fig. 8. Circular proof of odd
from a list, the list of those elements which have odd indexes. The program is displayed in figure 7:
I have used another convention: namely that a circular proof term which never uses its circular function can be represented by simply omitting mention of the introduced function altogether. This almost reduces the syntax to that of the case combinator and, of course, this near confusion of notation is quite deliberate.
The purpose of this example was to illustrate the use of multiple regen- eration. While it is not hard to see that we could implement this without a multiple regeneration (see later in figure 14 ), I would claim that the resulting program is much less natural. In figure 8 we give the derivation of this term which shows the pattern of regeneration in the proof.
Notice that first the regenerated type variable Z is treated as if it were



: acc' : InfL(A), List(A) → List(A)= 
IL '→  (L, vs) '→  hd : vs
  (IL, µ nil).

tl : acc'(tl νL, µ cons(hd νL, vs))

Fig. 9. Simple accumulate











▶ List(A) nil
InfL(A), List(A) ▶ InfL(List(A))
cut

InfL(A) ▶ InfL(List(A))

Fig. 10. Proof of the simple accumulate function
List(A) by being the subject of a circular derivation. However, this inner circular derivation does not use it’s circular function. Instead the regenerated type variable Z' is treated as if it were Z and has the function odd' applied to it.
Now much the same considerations apply to the coinductive datatypes. We illustrate the coinductive side of this language with a function to “accumulate” an infinite list. This function given an infinite list InfL(A) produces an infinite list InfL(List(A)) in which the inner lists give the list of elements seen so far. Now there are two ways to accululate the values see so far: the first, see figure 9 collects the elements in reverse order, the second, see figure 11 collects the elements in order.
The infinite list datatype is defined by
InfL(A)= νx.Ahd × xtl.
Notice that this sort of infinite list always has a “next element” by fiat. It cannot be empty or decide to stop!
This is a simple illustration of how coinductive datatypes and inductive datatypes can be used in combination to produce programs. The derivation of this term is (approximately!) displayed in figure 10.
Notice that this proof does contain a number of cuts which, in the term, I have suppressed. However, these cuts are all soft cuts so that actually we shall regard this term as being already deforested. However, the version of the accumulate function in figure 11 which collects the lists in the right order



: acc' : InfL(A), List(A) → List(A)= 
IL '→  (L, vs) '→  hd : vs
  (IL, nil).

tl : acc(tl L, app(vs, cons(hd L, nil)))

Fig. 11. Accumulate in order
is not deforested we shall see how this may be deforested in section 4.3.3):
This then concludes our introduction to the formal programming language that we shall use in the sequel. At this stage the reader should be convinced – and it is most certainly the case – that one can write nontrivial programs in this language which is a basic “strong functional programming language” in the sense of David Turner.

The transformation from initial and final datatypes
These circular proof rules despite according with the recursive programming style do not seem at first sight to correspond to the initial algebra and final coalgebra interpretation of datatypes.
Recall that if F (A, X) is functor then µX.F (A, X) equipped with a map
cons : F (A, µX.F (A, X)) → µX.F (A, X)
is an initial algebra for F (A, ) in case for every F (A, )-algebra
g : F (A, Z) → Z
there is a unique map h : µX.F (A, X) → Z such that the following commutes:
F (A, µX.F (A, X))  cons  µX  .F (A, X)
F (A,h)	h
J	J 
F (A, Z)	g	 Z .

This is the universal property of the initial algebra. Dually the couniversal property of the final coalgebra is defined as follows: νX.F (A, X) equipped with a map
dest : νX.F (A, X) → F (A, νX.F (A, X))
is a final algebra for F (A, ) in case for every F (A, )-coalgebra
g' : Z → F (A, Z)



Fig. 12. The algebra based proof system

there is a unique map h' : Z → νX.F (A, X) such that the following commutes:

Z	g'	 F (A, Z)
h'	F (A,h')
J	J 

νX.F (A, X)
 F (A, νX.F (A, X)).


These conditions assert not only the existence of a comparison map but, importantly, the uniqueness that map. The uniqueness of the comparison map forces certain natural equalities on the programs of the system.
This algebra based approach to datatypes can also be translated into an- notated inference rules see figure 12. When one writes down these rules one quickly realizes that one needs the initial property “in context” as, to main- tain the style of the logic, one must allow other propositions (types) beside the initial one on the left-hand side of the sequent. This does not to relate directly to the property outlined above for the inductive datatypes, however, in the cartesian closed setting at least, the above diagrams do suffice as one can shunt the context onto the right-hand side. These matters are described in great categorical detail in [4] and in Bart Jacobs’s book on categorical logic [8].
This is essentially the charity syntax as described in [5] and that paper provides several examples of programs.
The main purpose of this section is to show that it is possible to translate

between these two styles of proof. 5 We shall show this through the sequent proofs but it is important to realize that this works must work for the proof terms too and that this does in fact gives a method of translating between the two styles of programming.
We shall start by showing that the algebra based proof system (which in the model checking community is sometimes referred to as the Kozen system [9]) can be simulated by the circular proof system. To this end we need first to show how the initial algebra derivation can be obtained in the circular proof system. The required derivation has the following form:

Γ1, µx.P (x), Γ2 ▶ X
The inference labeled “functor” has to be inductively established for all the possible functors P . It corresponds to a well-known categorical observation that all the functors involved are “strong” [4].
Next we need to show how the final coalgebra derivation can be simulated in the circular proof system. This is very similar to the above and again uses the strength of the functor P :

Γ1, Y, Γ2 ▶ νx.P (x)
This then shows that the circular proof system can simulate all the derivations of the algebra based proof system.
We now have to show that we can simulate all the proofs of the circular proof system in the algebra based proof system. This is a little more difficult as we also have to handle the possibility of multiple regenerations and simul- taneous recursion. We shall therefore approach this in three stages. First we

5 Please note the ideas behind this translation are not original. In particular, I am very much in debt to Santocanale for introducing me to his proof [11] of this for the finitely bicomplete poset case with initial and final fixed points: I am unashamedly borrowing his ideas.


will show that when the recursion is sequential and there are no multiple re- generations that we can easily translate the proof into the algebra based proof system. Next we will show that a circular proof with simultaneous recursion can be reduced (within the circular proof system) to one with only sequential recursion. Finally, we show how a proof with multiple regenerations can be reduced to one which has only single regenerations.
Consider a circular proof for a program from an inductive type which has no regenerations. Letting π stand for the derivation from the circular assumption this has the general form:

Γ, µx.P (x) ▶ X
Now there are actually many proofs into which we could transform this but we actually have to be somewhat careful. For example, the following might seem like a reasonable translation:
  X ▶ X	 weak
Γ, X, Γ ▶ X  π(X)
Γ,P (X) ▶ X
Γ, µx.P (x) ▶ X initial
where we heavily use the fact, which is not totally obvious, that the proof π(Z) is parametric in Z. The reason this works is because the only rule which can involve using the implicit type of Z is the circular induction rules for the type. However, a use of that rule would have meant that there was a multiple regeneration. Thus we can substitute an arbitrary X for Z in the proof π(Z) to obtain a proof involving X.
However, consider what this transformation does to the heart of the fast reverse introduced in figure 4: it becomes

(x, y) '→    

nil : ()	'→ y
  x

cons : ( , vs) '→ vs
which is equivalent to (x, y) '→ y. Not at all what we want!
The problem with this approach to the translation is that the non-recursive arguments are altered prior to being used by the rev1 function. By weakening we, in effect, throw away these modifications.
In order to obtain a correct translation it is necessary to make essential use of the higher-order aspects of the language. It is convenient to assume that

(x, y) '→  

nil : ()	'→ λx.x
  x)@y.

cons : (a, f ) '→ λx.f @(cons(a, x))
Fig. 13. Higher-order fast reverse

Γ is a singleton: this we can do without loss because if it is not then we may always form the product of the propositions to obtain an equivalent singleton. Here then is the desired translation:
 Γ, Γ ⇒ X ▶ X	
π(Γ ⇒ X)
Γ,P (Γ ⇒ X) ▶ X
P (Γ ⇒ X) ▶ Γ ⇒ X µx.P (x) ▶ Γ ⇒ X	initial

Γ, µx.P (x) ▶ X

which has the following effect (see figure 13) on the fast reverse introduced in figure 4: Notice that this does have the correct effect!
Similarly consider a circular proof for a program to a coinductive type which has no regenerations (where we assume Γ is a singleton):

Γ,Y ▶ νx.P (x)


this transforms to:

Γ ▶ Γ	Y ▶ Y

weak

 Γ, Y ▶ Γ × Y	
Γ,Y ▶ P (Γ × Y ) π(Y )

Γ,Y ▶ νx.P (x)
final
.

Notice that for the coinductive transformations we do not need the any higher- order constructs.
Next we will illustrate how to remove simultaneous recursion within the circular proof system. This transformation uses the higher-order aspects of the language non-trivially: the transformation is well-known and was pointed out to me by both Eric Meijer and Simon Thompson. I shall illustrate the technique by transforming a proof which has a simultaneous recursion on two arguments. Here is a typical such proof:






µx.P1(x), µx.P2(x), Γ ▶ C



This gets transformed to the following proof with this simultaneous recur- sion removed:


µx.P2(x), Γ ▶ µx.P1(x) ⇒ C

µx.P1(x), µx.P2(x), Γ ▶ C


Here notice that we use the fact that Z1 is a regenerated variable so may be treated as one of its earlier incarnations namely µx.P1(x) so that we can discharge the inner induction using the outer premise. Notice that in this proof the inner circular induction is essentially a case combinator as its circular function is never used.
It remains to remove multiple regenerations. Our strategy is, as for simul- taneous recursion, to show that multiple regenerations can be removed within the circular proof system itself. Suppose to start with that we have a multiple regeneration on an inductive type, we would then have the following form to

the proof:
Γ, µx.P (x) ▶ Y
where the proofs π1 and π2 could both have other regenerations of Z and (for
π2, Z').
In order to simplify the transformed proof it is convenient to assume that Γ and Γ' are singletons. Please note that the transformed proof has many implicit cuts (e.g. in π' , π' , π'', and π''). Also please note that again we have
1	2	1	2
made essential use of the fact that the language is higher-order.


Z := µx.P (x) | Z ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)


Z ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)
Z ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)
′′	Z ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)
π

Z ▶ Γ′ ⇒ Y	π2
Γ′,Z ▶ Y ′	2
Γ,Z ▶ Y	1

Γ′,Z ▶ Y ′ π1(Z) P (Z) ▶ Γ ⇒ Y
P (Z) ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)
Γ′,P (Z) ▶ Y ′	π2(Z)
P (Z) ▶ Γ′ ⇒ Y ′

µx.P (x) ▶ (Γ ⇒ Y ) × (Γ′ ⇒ Y ′)
π′

µx.P (x) ▶ Γ ⇒ Y	1
Γ, µx.P (x) ▶ Y
We remark that the proofs π1 and π2 are not parametric in the type variable Z as it is possible that these proofs contain another regeneration on these variables. However, what is certainly true is that we may substitute (any regenerated variable of) the same inductive type into these proofs.
To illustrate this transformation process consider the function odd in figure 7: it has multiple regenerations, thus it is reasonable to ask what the program looks like when it is transformed into the algebra based system. In figure 14 the translated version written in charity using a fold is dsiplayed. Notice the rather unexpected second component of the state of the fold: it is a higher-order function. Essentially this component lags one step behind the first component, so that it holds the even list waiting for the next element to



x	'→   

nil : ()	'→ (nil(), λa.[a]))
  x

cons : (v, (L, g)) '→ (g@v, (λa. cons(a, L)))
; (V, ) '→ V.
Fig. 14. Higher-order odd






be added. When the next element is added it becomes (by application to that element) the odd list again.
Of course this may not be a very efficient way to compute the list of odd elements; but this really is not the point. The question is only whether this is an equivalent expression of the program (which I claim is the case). Clearly the direct approach suggested by the earlier code for odd will be more efficient. To complete the transformation we need to also show that multiple regen- erations can be removed from coinductive circular proofs. It turns out that the proof of this is much easier: we do not even need to use the closedness of the setting. Here is the general form of a coinductive regeneration in which
the proofs π1 and π2 can themselves contain regenerations:








Γ ▶ νx.P (x)


Again to simplify the transformation it is convenient to assume that both Γ and Γ' are singletons. Under this assumption we can remove one level of regeneration by transforming this to the following:




Γ+ Γ' ▶ νx.P (x)

Γ ▶ νx.P (x)

We have now almost established the following:
Theorem 3.1 There is an isomorphism between the algebra based proof sys- tem and the circular proof based system for the program logic.
The respect in which we have not established this theorem is actually rather important: it concerns proof equivalence. We would like it to be the case that if we translate back and forth that the resulting proof (which is different if there is recursion) is equivalent to the starting proof. Furthermore, we would like that, for both the translations, that equivalent proofs are translated into equivalent proofs.
Of course, all we have really established (despite suggesting otherwise) is that the two systems have the same “derivability” power. Categorically speaking we are trying to establish an isomorphism between two different presentations of a category. So far we have shown that with respect to the posetal collapse the two systems are isomorphic. However, we really do want the stronger result and I do claim it is true.
For proofs with no regenerations establishing the equivalence of the double translated proof is quite easy. Proofs with multiple regenerations or simulta- neous recursion are translated internally to this special case, thus, so long as the internal translation is between equivalent proofs and the translations themselves maintain equivalence, the result will follow.
Here we do need to know that equivalence is maintained by both transla- tions and this is has lots of details. Furthermore, to establish this we really must begin to explore the notion of proof equivalence in the circular proof sys- tems. The algebra based proof system derives its notion of proof equivalence directly from its underlying categorical semantics: this is discussed at great length in [5]. On the other hand, it is not so clear where the natural notion of proof equivalence comes from in the circular system: the answer to this question, I claim, becomes clearer from examining how the cut-elimination procedure works for that system.

Program equivalence

A good rule of thumb when trying to understand proof equivalence in a logical system is to look at its cut-elimination procedure – if it has one. This will usually employ the more important identities in the system.
In this section we consider what cut-elimination may mean for the circular proof system. There is an obvious answer which is rather unsatisfactory as it involves infinite proof terms. However, it does suggest what the proof equivalences for the circular proof system should be.
The fact that we produce infinite term from the cut-elimination procedure is not really acceptable from a program transformation point of view. Thus, we turn to deforestation which is a process which aims at removing the cuts which remove structure. We informally introduce a technique for deforesting. This differs from that described in [10] in how it handles coinductive circular definitions. We give several examples of deforestation.
However, before, I do any of that I must first say an embarrassed word or two about the underlying proof system which corresponds to cartesian closed categories with coproducts.





Equivalence in intuitionistic logic

Despite the fact that the logic of (∧, ∨, ⇒) has been heavily studied since the turn of the century the question of providing a decision procedure for proof equivalence is still a thorny issue. Any approach which uses rewriting is considerably complicated by the presence of fairly complex “commuting conversions” or equations which makes providing a complete proof that the system works a daunting and highly technical task (see [7]). All the evidence, however, is that this fragment is highly decidable (see as well [12] for an alternate semantic approach). Unfortunately, there is still an outstanding technical problem which I must acknowledge: there is still no feasible and fully general decision procedure in the literature for this fragment of my formal programming system.
I am not going to try to correct this situation here as this part of the system is not of primary interest in this exposition. I will therefore restrict myself to a brief discussion of some of the proof equivalences which arise.
For this fragment we have the usual cut-elimination steps which give the following (representative) rewrites.


Γ ▶ C Xi, Γ ▶ C

weak 




i∈I


	Γ ▶ C	

Σi∈I Xi, Γ ▶ C	= Σi∈I Xi, Γ ▶ C
{i(xi) '→ t}i∈I s = t
Fig. 15. Idempotence
  {Xi, Yj, Γ ▶ C}j∈J	  {Xi, Yj, Γ ▶ C}i∈I 

Xi, Σj∈J Yj, Γ ▶ C


i∈I
Σi∈I Xi, Yj, Γ ▶ C


i∈I

Σi∈I Xi, Σj∈J Yj, Γ ▶ C  = Σi∈I Xi, Σj∈J Yj, Γ ▶ C
{i(xi) '→ {j(yj) '→ ti,j} t2} t1 = {j(yj) '→ {i(xi) '→ ti,j} t1} t2
Fig. 16. Transposition


,	,, X
πj
▶ Σ X	Γ,	X ,X ▶ Y
,,

Γ, Σ X ,X ▶ Y	j



Γ,X ,X 
▶ Y	cut

Γ, Σ Xi
▶ Y	contr =
Γ, Σ Xi ▶ Y

  i(xi) '→ ti(x) , x =  i(xi) '→ [x '→ ti(x)]i(xi) , x
Fig. 17. Repetition
[z '→ {i(xi) '→ ti}i∈I z]j(s) =⇒ [xj '→ tj]s
[(i1 : x1, ...) '→ t](i1 : s1, ...) =⇒ [x1 '→ ....[xn '→ t]sn...]s1
[z '→ t] {i(xi) '→ ti}i∈I y =⇒ {i(xi) '→ [z '→ t]ti}i∈I y
[z '→ t](i1 : s1, ..., in : sn) =⇒ (i1 : [z '→ t]s1, ..., in : [z '→ t]sn) [f '→ s1[f @s2/x]]λy.t =⇒ [x '→ s1]([y '→ t]s2)
[ '→ t1]t2 =⇒ t1
[x '→ x]t =⇒ t
All except the first two rule we regard as “soft” cuts. Notice that they both remove structure.
There are in this system a number of additional proof equalities which arise as natural proof identifications. Below are two simple examples which are illustrated in figures 15 and 16.
However, there are other more subtle identities which arises through con- traction, see for example figure 17. Notice that in this repetition identity a cut appears from nowhere. This is necessary if one wishes to push contractions up the proof tree. The cut reflects the fact that a choice in x has been resolved

and this is recorded by adding a cut.
I worked on this fragment with Guangwu Xu (who was my student at that time) as I had a long standing conjecture that these (and some identi- ties I have not mentioned) can be oriented to make a rewrite system modulo the two equations above. We almost completed (and may yet!) a proof of this for the first-order fragment: the termination of our system remained an outstanding issue. The details are quite daunting, however, to make signifi- cant progress in program transformation ultimately these problems must be completely resolved.
Circular proof equivalences
The rules governing the datatypes in the algebra based system are determined by the underlying categorical initial algebra and final algebra semantics: they were described in [5]. Therefore, we will now focus on the rules of the circular proof system.
The first and most obvious cuts involving the datatypes are those which are concerned with the construction and destruction. Both these cuts remove structure and so must be eliminated in any deforesting transformation. They are what we shall call deforesting cuts. A term with residual deforesting cuts will, by definition, not be deforested.
We will deal with the inductive case first:







Γ' ▶ P (µx.P (x))

'
π' cons


	 π'
Γ', µx.P (x) ▶ Y
π
'

Γ ▶ µx.P (x)
Γ, µx.P (x) ▶ Y
Γ, Γ' ▶ Y	⇒
Γ ▶ P (µx.P (x))
Γ ,P (µx.P (x)) ▶ Y
Γ, Γ' ▶ Y

in the term calculus this is the following rewrite:

 v '→ : f =

  (v, u) µ(s)	⇒	 z '→ t u/y, : f =
  /f  s

(z :=, y) '→ t	(z :=, y) '→ t

It may seem that no real progress has been made, but recall the the proof π has been hauled out and this will (most likely) allow the cut to move up. Notice also how the circular proof term is substituted into its body.
The cut elimination rule for the destruction of a coinductive type is similar:



Γ',P (νx.P (x)) ▶ C π

Γ ▶ νx.P (x)

Γ ▶ νx.P (x)


'	dest		 π
Γ ▶ P (νx.P (x))
Γ',P (νx.P (x)) ▶ C π

Γ', Γ ▶ C	cut	⇒
Γ', Γ ▶ C	cut


in the term calculus this is:
[z '→ t[ν(z)/z']](: f =	 u) ⇒ [z' '→ t]s[u/y, : f =	 /f ]
 y '→ s 	 y '→ s 

These rules cause the circular proofs to unroll – or to “unfold” in the terminology of Burstall and Darlington. Now, in fact, if we are to remove any cut from the conclusion of a circular proof then it is necessary to unroll the recursion to expose its inner structure. To effect this unrolling it is necessary to introduce a degenerate circular proof which does not use its circular function. We may then move the cut inside such a step.
There are three ways this can happen: two inductive cases and one coin- ductive. For the inductive cases the cut formula can be on the left or the right. The case where the cut formula actually is the inductive type itself was handled above.
Here is the cut elimination step when the cut formula is on the left:



Γ′	π′
Γ	( )

Γ′, Γ, µx.P (x) ▶ D	cut ⇒
Γ′, Γ, µx.P (x) ▶ D

where the open circular proof box indicates that the circular function is not used. Here is the corresponding term.


[x '→ : f =
(v, x)]s ⇒  y '→ x '→ t[x/y′, ,: f =	, /f ] s  v

( :=, y′) '→ t
, (y :=, y′) '→ t ,







Γ	( )
Γ′	π′

Γ, µx.P (x), Γ′ ▶ D	cut ⇒
Γ, µx.P (x), Γ′ ▶ D


In the coinductive case the cut formula must be on the left:






Γ′	π′


Γ	( )

Γ′, Γ ▶ νx.P (x)	cut ⇒
Γ′, Γ ▶ νx.P (x)

Notice that as the cut of a proof π with an identity proof is always equiv- alent to the original proof, and as we may precipitate unrolling with such a cut, we must conclude that the unrolled proof (to any depth) is equivalent to the original proof.
These are, in fact, the cut elimination steps. It is clear, however, that these steps do not terminate. In fact, in general they will precipitate an infinite unrolling of a circular proof.
In fact, it is reasonable to view cut-elimination in this system as producing infinite terms. There is an obvious “lazy” way in which one can regard this: for any demanded finite depth one can cut-eliminate to that depth. This idea was actually used, for example, by Simon Marlow [10] as the basis for a practical program transformation system. His first step was to lazily unroll the term performing cut elimination from the root upwards. As he wanted to produce a finite program this was followed up by a second stage he called knot tying, which was essentially Burstall and Darlington’s notion of folding. In this stage he searched for recurrences within the tree and tried to tie the lazy infinite program back into a finite program.
The difficulty with the idea of knot tying was to secure the termination of the search for knots. That is a guarantee that every path in the term will either be finite or go round a knot. While some special conditions are known it seems that in general the general recursive case there can be no guarantee that one can actually complete the tying of the knot.
We may explain the idea behind knot tying proof theoretically: the behav- ior of a circular proof is determined by the code it produces from its innards when it is unrolled. Now if another proof can simulate the production of the same innards when we unroll it then the two proofs must be equal. If one is transforming one ties the knot by replacing the second proof by the first which is more canonical.
This scheme for inductive proofs is shown in figure 18. Notice that, in order that the proof be applicable to a regenerated variable there can be no construction used on that type in Π. The proof Π need not start with a circular step. The term is shown in figure 19.
There is a similar inference for the coinductive datatypes whose equation



inferΓ, µx.P (x) ▶ Y	Π	=	Γ, µx.P (x) ▶ Y










	Π	 Γ, µx.P (x) ▶ Y =
Γ, µx.P (x) ▶ Y


Fig. 18. Inductive knot
t(u, w)= x '→ s[w/y, [(u, w) '→ t]/f ] , u


t(u, w)= ,: f  =	, (u, w)
(x :=, y) '→ s
Fig. 19. Inductive knot equation

t(u, w)=  (x, y) '→ s[[(u, w) '→ t]/f ]  (u, w)

t(u, w)= : f =	 (u, w)
(x, y) '→ s
Fig. 20. Coinductive knot equation

is given in figure 20.
These circular inferences are the source of equivalences in this proof sys- tem. Once one realizes that cutting with a constructor causes unrolling, it becomes clear that these are just a reformulation of the initial algebra prop- erties. The significance of the transformation lies in the fact that it is a much more convenient form to with which to work.
These remarks suggest in a little more detail why 3.1 is true.

Deforestation
In this section I shall demonstrate some program equivalences using a tech- nique to deforest the terms of this programming language. This technique of deforesting relies on determining where there is demand in the term. A term with no demand is deforested.
There are two ways that demand can occur in a term. The first source of demand is expressed at an argument of a term. This can either be at the active arguments of an inductive circular definition (including a coproduct) or at the argument of a destructor (including projection). This is one reason why we carefully labeled the active (often recursive) arguments of the induc- tive circular definitions using :=. However, these two expressions of demand only become real demand when they can potentially be put together with a supplier. For an active argument of an inductive circular definition a supplier is a constructor or something which could potentially produce a constructor. For a destructor a supplier is a coinductive circular definition or someone who can supply such a definition.
This demand will cause us do one of three things: to perform a deforesting cut if the constructor (resp. destructor) is present, to unroll the circular definition on which the demand has been placed, or, if demand is placed internally on an argument of a circular definition we will “pass the demand up”: this is illustrated below (see section 4.3.3). It is important to realize that as we are dealing with finite terms whether there is demand or not can be easily detected.
Below, through examples, I am describing an algorithm which can be im- plemented. If this algorithm terminates it will return a deforested term. Of course, the problem concerns the termination (see also [10]); I conjecture that this procedure does terminate and so will always return a deforested (finite) term, however, at the time of writing I do not have a proof.
Associativity of append
We start with a classic example to illustrate the strategy of deforesting and the way it manages to discover useful identities. Recall the definition of append:

: app(:=, y' = y) =
x, y '→	nil()	'→ y'
, 
,, x
 ,

cons(v, vs) '→ µ cons(v, app(vs, y'))
Notice in this definition there is no demand: or more specifically the only demand is on a free variable which we cannot satisfy until it is instantiated in some way. This means that app is already deforested.
We shall indicate the demand we are working on by a dot. This will usually be the demand nearest the root. However, the knot tying process may actually oblige us to calculate certain demands in order to secure the match it wants.

The first step below responds to the demand by unrolling the inner append: app(· app(x, y), z)

= app(· , nil()	'→ y
 , x, z)

,	, cons(v, vs) '→ µ cons(v, app(vs, y)) , ,

=  nil()	'→ app(y, z)
  x



=  nil()	'→ app(y, z)
  x

cons(v, vs) '→ µ cons(v, app(· app(vs, y), z))
There is now an opportunity to tie a knot: the term app(app(x, y), z) occurs with vs substituted for the variable x. It is important to note that vs is a “regenerate” variable (that is of regenerated type) as this tells us where the recursion lies. The trick now is to tie a tight knot! The only structure which is of relevance lies between the occurrence where the recurrence template stands and the actual recurrence. All other arguments may be treated as context as they are never altered by the recusions. Thus we may infer:


: F (:=,u = app(y, z)) =
app(app(x, y), z)=  nil	'→ u
, , x

, cons(w, ws) '→ µ cons(w, F (ws, u)) ,
Clearly F = app and so we have just deforested
app(app(x, y), z)	⇒	app(x, app(y, z)).

In this example there is also a slight efficiency improvement: the first argument is traversed twice in the first expression but only once in the second.

Improving naive reverse
Deforestation can make large efficiency improvements. Here is a classic exam- ple of an O(n2) program which can be improved to an O(n) program. This transformation is interesting as it derives precisely what one might hope would be derived. Consider the following definition of reverse:


: rev2(:=) =
x	nil()	'→ µ nil()
, 
,, x
 ,

,, cons(v, vs) '→ app(rev2(vs),µ cons(v, µ nil)) ,,


This definition already has internal demand: the app function creates de- mand on rev2(vs) which causes us to unroll. We let z = cons(v, nil)) in this calculation and we will use our previous calculation in the last step (which, of course, would be recalculated automatically by a system) with a deforesting constructor cut with append:
app(· rev2(vs), z)

= app(· , nil()	'→ µ nil()
 , vs, z)

cons(v', vs') '→ app(rev2(vs'),µ cons(v',µ nil))
= ,  nil()	'→ app(·µ nil(), z)
 , vs

= ,  nil()	'→ z
 , vs

cons(v', vs') '→ app(· rev2(vs'),µ cons(v', z))
We can now tie the knot to obtain:
: rev1(:=, z' = z) =
app(rev (vs), z)=	'


, , vs.

, cons(v', vs') '→ rev1(vs',µ cons(v', z)) ,

This has no residual demand and so is deforested. This is the fast defini- tion of reverse which accumulates the reversed list on its second argument. Substituting this back into the original definition gives:

x '→ , nil()	'→ nil()
 , x

cons(v, vs) '→ rev1(vs, cons(v, nil))
which has no residual demand and so is a deforested version of our original algorithm.
Improving the accumulate on an infinite list
We use the example of collecting the list of values so far in an infinite list:

: acc(L', xs)= 
L '→  (L', xs) '→  hd : x
  L

tl : acc(tl νL', app(·xs, µ cons(hd νL',µ nil)))
This function has internal demand: the demand is generated at the first ar- gument of the app function — the destructors do not generate demand as

there is none passed down through the arguments of the inductive circular definition.
To remove this internal demand we must, this time, pass up the demand: this involves creating another argument through which we express that de- mand (in as tight a fashion as possible). This demand then becomes a new parameter of the coinductive circular definition. Now when we create this extra argument, rather like a time anomaly, we must reflect the change in the future circular call. In this case we abstract the append on its non-recursive argument and this gives the following equalities:
acc(L, xs)
= acc'(L, xs, λz. app(xs, z))

 hd : xs
  

tl : acc' (tl νL'
, app(·xs, µ cons(hd νL',µ nil)))
 

    

 hd : xs
 
, λz. app(app(·xs, µ cons(hd νL',µ nil))), z)) 
  

tl : acc' (tl L'
, app(·xs, cons(hd L', nil)))
 

    

 hd : xs
 
, λz. app(xs, cons(hd L', z)))) 

  

tl : acc' (tl νL'
, app(·xs, µ cons(hd νL',µ nil)))
 

    

	, λz'.(λz. app(xs, z))@(µ cons(hd νL', z'))) 
We can now tie the knot to obtain: acc'(L, xs, F )
: G =	

 	 hd : y
 

=
 (L, y, F ) '→ 
tl : G (tl νL
(L, xs, F ).
 

		, app(y, µ cons(hd νL ,µ nil))  

	
, λv.F @(µ cons(hd νL, v)))
 

This is now deforested as all the demand has gone. However, we may make a further simplification: we can note that second argument can be obtained from the first at every call by xs = F @nil. This means we do not need the middle argument at all it can be replaced by the calculation. While this is a

nice observation it is not necessary in order to complete the deforestation and so we shall not use it here. We now have:
acc(L, µ nil) = acc'(L, µ nil, λz. app(µ nil, z)) = acc'(L, µ nil, λz.z) which expresses the original function in deforested form.
Getting values from infinite structures
One way to think of a computation, such as calculating the nth even number, is to create an infinite table containing the values and then use this to look up the value. Clearly this is horrendously inefficient, but, one might ask: if we allow for deforestation can this be transformed to an efficient program?
This example of a program transformation illustrates the effect of having a coinductive type “inside” an inductive type. In particular it shows how demand can be generated through a containing inductive circular definition.
We start by considering the program which gets the nth tail of an infinite list.

: gtt(:=, L' = L) =
(L, n) '→	zero	'→ L
, 
,, n
 ,

succ(n') '→ tl ν gtt(n', L)
This is deforested as the demand generated by tl is never supplied. However, if we substitute a coinductive circular definition in for L this will create a demand on inner occurrence of gtt. We shall resolve this as follows without reference to what caused the demand:
tl ν · gtt(n, L)

= tl ν · , zero	'→ L
 , n

= ,  zero	'→ tl ν · L
 , n

succ(n') '→ tl ν tl ν · gtt(n', L)
This allows us to tie the knot (tight):

: H(:=, L' = tl L) =
tl · gtt(n, L)=  zero	'→ L'



, , n

,, succ(n') '→ tl νH(n', L') ,,

Now it is clear that H is gtt so we can replace tl gtt(n', L) by gtt(n', tl L) to



get a new version of gtt (I shall not change the name):

: gtt(:=, L' = L) =
(L, n) '→	zero	'→ L
, 

,, n
 ,

,, succ(n') '→ gtt(n', tl νL) , ,

Now let us consider the program which gets the nth even number. first we introduce the notion of the infinite list of every other number starting at m:
: evn=	

m '→
 hd : m'	  m.

 m' '→ 
tl : µ succ µ succ m'
 


Now we get the nth element of the infinite list of evn starting at zero by applying hd to the the process of getting the nth tail.
This means we want to deforest the following term:

n '→ hd ν · gtt(n, evn(µ zero))

Where this has a demand expressed at the argument of hd and supplied at the second argument of gtt. We shall resolve this demand as follows:
hd · gtt(n, L)

= hd ν · , zero	'→ L
 , n

= ,  zero	'→ hd ν · L
 , n

succ(n') '→ hd ν · gtt(n', tl νL)
This allows us to tie another little knot:

: gtt'(:=, L' = L) =
hd · gtt(n, L)=  zero	'→ hd L'



, , n

,,, succ(n') '→ gtt'(n', tl L') ,,,

Now consider the original problem again, or rather a subproblem thereof:



gtt'(n, evn(m))
= ,  zero	'→ hd · evn(m)
 , n

succ(n') '→ gtt'(n', tl · evn(m))
= ,  zero	'→ m
 , n

succ(n') '→ gtt'(n', evn(succ succ m))
Notice that abstracting the zero out of the calculation helps us to spot the knot. We may now tie this knot to obtain:

: E(:=, m' = m) =
gtt'(n, evn(m)) =  zero	'→ m'
, , n.



This is now deforested.
,,, succ(n') '→ E(n',µ succ µ succ m') , ,,

Now it is worth remarking that E(n,µ zero) may not have been quite the expected form for calculating even numbers. We may have expected:

: E'(:=, m) =
hd · gtt(n, evn(m)) =	zero	'→ m
, 
,, n.
 ,

succ(n') '→ µ succ µ succ E'(n', m)
Both of these functions are deforested and, furthermore, they are equal. We shall take this as a small reminder that deforesting does not solve the equiva- lence problem!
Remarks on program equivalence
As we have seen deforestation does not provide the solution to the program equivalence problem. However, it is very clear that we should in determining equivalence use the fact that one can deforest. If one does this then one of the remaining problems is to establish that the two programs can be expressed in such a manner as to have the same recursion pattern.
Now, in principle, this is easy to achieve. The idea is this: one unrolls the two programs in parallel. The demand to unroll arrives now from two sources. The first is because there are still deforesting cuts as usual. The second is because one wants to keep the programs unrolling in parallel. Thus, if there is demand on the one program this must be transmitted to the second. Finally knot tying must be done in parallel.
There is an advantage to this process as unrolling is equivalent to looking at distinguishability. Thus, if the two programs unroll in different ways one can actually reconstruct a pattern of destruction and application to values

which will distinguish the programs. Thus, in principle, one can either prove the programs equivalent or provide a counter-example – or never terminate, of course. If this works, it seems that this should be a standard tool should be provided with any programming language which is worth considering!
There are, unfortunately, a few outstanding problems with this idea! For example, this presumes that one has tamed the program equivalence problem for the proof theory of intuitionistic logic! It also presumes that deforestation always terminates and that such a proof technique is complete. There is, therefore, considerable work that is required before we can provide the sort of program verification tool which I feel should be standard. However, it is possible that we now have most of the technology in hand to provide such a tool.

Conclusions
I hope that these discussions have underlined that arriving at an appropri- ate semantic and proof theoretic formulation for inductive and coinductive datatypes is an extremely important for the field of program transformation and optimization. Furthermore, that a satisfactory semantic formulation (as given by mathematical induction or categorical initial and final datatypes) does not necessarily translate into a good manipulative system. In the case of inductive and coinductive datatypes, I would argue, the circular proof system provides a crucial insight and link between formal settings and the various techniques which have proved to be most useful in practice.
This paper has provided a walk through some of the ideas which underpin the development of good transformation and proof tools for the basic program- ming system provide by a cartesian closed category with datatypes. I have not attempted to provide detailed proofs and, indeed, I have introduced tech- niques, such as the deforestation algorithm, in a very informal way. I would be the first to admit that there remains alot of work to be done. Therefore, what I have described should be regarded as a program for future work. I hope, however, that I have provided some indications that this program might be fruitful.

References
Aldwinckle, J., On the proof theory of model checking (to appear).
Burstall, R. and J. Darlington, A transformation system for developing recursive programs, Journal of the Association for Computing Machinery 24(1) (1997),
pp. 44–67.
Cockett, R. and S. Lack, Restriction categories II: Partial map classification, Theoretical Computer Science (to appear).


Cockett, R. and D. Spencer, Strong categorical datatypes I, in: R. A. G. Seely, editor, International Meeting on Category Theory 1991, Canadian Mathematical Society Proceedings (1992), pp. 141 – 169.
Cockett, R. and D. Spencer, Strong categorical datatypes II: A term logic for categorical programming, Theoretical Computer Science 139 (1995), pp. 69–113.
Fuhrmann, A. B. C. and A. Simpson, Equational lifting monads, Theoretical Computer Science (to appear).
Ghani, N., “Adjoint Rewriting,” Ph.D. thesis, University of Edinburgh (1995).
Jacobs, B., “Categorical Logic and Type Theory,” Elsevier, Amsterdam, 1999.
Kozen, D., Results on the propositional mu-calculus, Theoretical Computer Science 27 (1983), pp. 113–118.
Marlow, S., “Deforestation for Higher-Order Functional Programs,” Ph.D. thesis, University of Glasgow (1996).
Santocanale, L., “Sur les µ-treillis libre,” Ph.D. thesis, Univerite de Quebec a Montreal (1999).
Scott, T. A. P. D. M. H. P., Normalization by evaluation for typed lambda calculus with coproducts (2001), extended abstract.
URL http://www.cs.nott.ac.uk/~txa/drafts/coprod.ps
Slind, K., “Reasoning about Terminating Functional Programs,” Ph.D. thesis, TU Munich (1999).
URL http://www.cl.cam.ac.uk/users/kxs
Szabo, M., editor, “The Collected Papers of Gerhard Gentzen,” Studies in Logic and the Foundations of Mathematics, North-Holland, Amsterdam, 1969.
Telford, A. J. and D. A. Turner, Ensuring termination in esfp, in: 15th British Colloquium in Theoretical Computer Science, 1999.
Turner, D., Elementary strong functional programming, in: R.Plasmeijer and P.Hartel, editors, First International Symposium on Functional Programming Languages in Education, number 1022 in LNCS (1996).
Wadler, P., Deforestation: transforming programs to eliminate trees, Theoretical Computer Science 73 (1990), pp. 231–248, (Special issue of selected papers from 2’nd ESOP.).
