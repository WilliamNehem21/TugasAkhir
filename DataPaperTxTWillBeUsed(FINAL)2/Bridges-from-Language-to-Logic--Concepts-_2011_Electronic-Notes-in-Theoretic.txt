

Electronic Notes in Theoretical Computer Science 269 (2011) 83–94
www.elsevier.com/locate/entcs

Bridges from Language to Logic: Concepts, Contexts and Ontologies
Valeria de Paiva1,2
Cuil, Inc.
Menlo Park, CA, USA

Abstract
This paper gives a brief overview of the work on translating natural language sentences into logic done at PARC and distills a few simple-minded lessons. Then we turn our attention to formal logics applicable to systems of contexts and describe some open questions on these systems.
Keywords: logics for natural language, contexts, ontologies, constructive modalities


Language to Logic: the problem
The problem I want to discuss in this note, one that I have been working on for the last ten years or so, is simple to state. I want a computer system that reads sentences in English and constructs a logical representation of these sentences in an automatic and efficient way. I do not want very complicated sentences, no need for several layers of meaning, metaphors or poetry. Simple, factual language that your average twelve-year old would understand is the goal here.
But when I discuss this problem with friends I usually meet with opposing and uninformed reactions. Too many people grew up watching science fiction films and believe that the super efficient female ‘Computer’ of “Star Trek” or the menacing ‘HAL’ of “2001: An Space Odissey” are technology, if not totally developed, just around the corner. And they think that the problem is with the mechanics of sounds, not with the meaning of the sentences. On the other extreme, some people will say that language is clearly too complicated, too full of nuances of meaning, too human for computers to understand. That logic is not able to cope and that the

1 Thanks to all my ex-colleagues at PARC for insights and discussions. Thanks to Rodrigo de Salvo Braz for inviting me to give a talk at SRI, that became this note. Special thanks to Dick Crouch and Ann Copestake, who tried to teach me this, a long time ago.
2 Email: valeria.depaiva@gmail.com.

1571-0661 © 2011 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2011.03.007

problem is really impossible. Myself, I have been guilty of both misapprehensions at different times. Hence the goal of this paper is to describe a (collection of) possible approaches to the problem of making computers construct logical representations of sentences in a way that makes clear that the problem is indeed very hard, but not hopeless so. That we do not need (another) sixty years to crack it, but that indeed more resources, of different kinds, need to brought to bear to solve the problem completely. That with more groups and individual researchers working on it presently and with Moore’s law on our side, we are well situated to get it done. Soon.
I call any proposed system that makes this translation from usual language sentences into logical representations, a Bridge system, as it is connecting the land of text to the land of logic. This was the name of a system of this kind develop by the group I worked with at PARC from 2000 till 2008, which I will discuss below. But the name Bridge is used here in a generic way, for any system of this kind. This paper tries to summarize the more abstract logical content of my research into this problem. Most of the results of this research were described in the group’s papers (in the references), so this note reads somewhat like an annotated bibliography. It is a very personal view and in no way reflects the opinions of PARC or of my ex-colleagues. It is written to make sure that I pay attention to the lessons learned and that I commit myself to doing some more future work.

Why a bridge?
Why do we want this bridge from language to logic? We want our computer to understand us and to help us in our daily tasks. We want to to be able to ask questions, as we ask other humans. We want to search for content on the web, exploiting the large amounts of textual data explicitly there. But we also want the information that is only implicitly present, the conclusions you can draw from what is stated or known. We want large-scale intelligent information access, the ability to process huge amounts of data, with the precision that real understanding allows. We want to summarize logically and effectively and we would also like automatic, high quality, grammatical translation into other languages, inter alia. While there are systems around for all these tasks, it is clear that having “full semantic interpretation” would make these systems much better, more useful and more reliable.
The usual rejoinder is that full semantic interpretation is too expensive, too brittle and only attainable for very restricted domains. Hence we settle for ‘ap- proximations’ of interpretation in many domains and the results can be surprisingly good. But in the best of all possible worlds what we really would like is the full semantic interpretation. And we should strive to build systems that get closer and closer to this ideal situation.
We want a translation from English sentences into a logical system (to be deter- mined) and this translation must satisfy a few constraints. Let us briefly examine some of these constraints. The first constraint is that the translation must be com-

positional. We must be able to describe the translation of components and be able to compose the translation of components, until the whole sentence is dealt with. The translation must be principled. And of course it has to be meaning preserving to some extent. It must at least preserve truth-values, but we will not mind some simplification of meaning. It is clear that if I say ‘John managed to close the win- dow’ that there was something that made the process of closing the window hard for John in that situation. We will not worry about that component of the meaning, but simply about the fact that if ‘John managed to close the window’ holds then ‘John closed the window’ also holds.
Another constraint on the translation is that we want to cover a reasonable fragment of all possible English sentences. If you force all your sentences to use at most eight words in English, then the translation is easy, but useless. Similarly we want to be able to deal with generic texts, and not simply with pre-specified domains. A fourth and most important constraint is that the ‘logical forms’ that we obtain from our translation must be useful for automatic reasoning. It is the usual trade-off: the translation has to deal with complicated phenomena, but given that we want to use it for reasoning, we want its image, as a function, as simple as possible.
Some of these issues are discussed at length in Condoravdi et al “Entailment, Intensionality and Text Understanding”[5]. Several important questions arise out of this simple set up, for example: which kind of logic should be the target of this translation function? how do we know when we are done? how do we measure quality of the results of the translation?

A historical digression
Before we can address these questions, there is need to dispel the myth that the problem of providing automatic logical translations of sentences of natural language is impossible.
The skepticism in logical circles about the possibility of describing natural lan- guage with logical methods is pervasive. Despite Russell’s initial optimism, most logicians are basically influenced by Carnap, Tarski and Quine’s views that natural language is misleading because it is not sharply defined and because it is not sys- tematic enough, as discussed in [19]. The prevalent opinion among logicians is that it is problematic to use logical methods for natural language as any such semantics assumes a precisely formulated syntax, and that it is out of the question that such could be found for natural languages [19].
Despite logicians disbelief, the early seventies saw seminal work of Montague, Davidson, Chomsky and many others establishing the field of Semantics of Natural Language, an intersection area, between philosophical logic and linguistics. Some forty-odd years on of solid work on natural language semantics by linguists and com- putational linguists, the received wisdom is somewhat different. While Montague’s quote
I reject the contention that an important theoretical difference exists between

formal and natural languages - English as a formal language, 1970[20]
is by now somewhat accepted, the use of logical methods for language is still not accepted as useful. The hugely successful use of statistical and learning methods for all tasks initially associated with symbolic processing of language meant that purely symbolic processing is now considered somewhat obsolete. But it should be clear that the symbolic methods should not be tossed aside.
The architecture of bridges
Knowledge-based (deep or logic-based) representations of meaning allow high pre- cision and recall, but typically on very restricted domains. It is hard for users to read/interpret the output of the system, but much more importantly, it is very hard for systems to build up knowledge. By contrast shallow, key-word-based open- domain representations, have broad-coverage and fail “gracefully. They do have lower precision and recall and are too sensitive to the form of sentences, but they work surprisingly well in the existing applications. Up to a point.
Existing applications have been multiplying, and the more data one has, the better these methods perform. For example, machine-learned statistical translation is nowadays much better than it was only a few years back and for some tasks it is good enough. Some believe that all the problems of computational linguistics will be solved by throwing more data at systems, but it seems clear to me that we need both machine learning and symbolic methods in tandem. In particular data-driven methods seem to not perform well in the presence of negation and antinomy. For example it is clear that the sentences below are contradictory, if we are referring to the same incident:
No civilians were killed in the Najaf suicide bombing. Two civilians died in the Najaf suicide bombing.
but negations are not very prominent in texts and more data is unlikely to make it easier for a key-word based system to note the contradiction. Our approach will be firmly in the symbolic side of the divide, but hoping to use as much data provided by the machine-learned methods as possible.
The traditional architecture of a symbolic system for language understanding consists of four components: a preprocessing module, a parsing module (based on some grammar), a semantics module and a post-processor. Traditionally many different kinds of grammar (based on different linguistic theories) are available, as are many different kinds of semantics. While for grammaticality there is at least some degree of consensus and there are huge numbers of annotated sentences with judgements, the situation for semantics is very different. There is almost no consensus and almost no annotated data.
The PARC approach
The system Bridge developed at PARC and described in[14] parses sentences in English using the industrial-strength parser XLE and a hand-crafted lexical

functional (LFG) grammar. The system’s long pipeline is shown in Figure 1. (This variation on the main pipeline of the Bridge system comes from “Con- structive Hybrid Logics and Contexts”, talk presented by de Paiva at Hylo2006.)

Parsed sentences are mapped to f-structures and f-structures are then mapped to linguistic semantic structures. These are mapped to abstract Knowledge Repre- sentation (KR) structures using a robust rewriting system [21]. The mapping from syntactic structures to (linguistic) semantics and then abstract knowledge repre- sentations runs on the XLE platform and is described in Crouch and King[30] and Crouch [21]. The linguistic, semantic rationale for the use of concepts in AKR was originally described in Condoravdi et al. ([1,5]). Components of the system have been described in Crouch and King [30], Gurevich et al.[15], and Nairn et al.[27]. An earlier application to a collection of copier repair tips written by Xerox technicians is described in Crouch et al.[3]. The more recent application to question-answering in the framework of the PASCAL-organized competition Recognizing Textual En- tailment (RTE) is described in Bobrow et al. [13].
This layered approach to producing logic from text is useful and natural, but very much out of fashion. A quantified study of how much out of fashion this approach is, can be seen in Hall, Jurafsky and Manning’s paper[32]. On the other hand, some of the new work in this tradition is described in Bos and Delmonte’s book[17].
The logic system associated with the system Bridge has been synthetized from the actual descriptions produced by the system. Each sentence produces a represen- tation, consisting of concepts and contexts. The logic of the representations system thus summarized, called TIL for (textual inference logic) was described in Bobrow et al. [7] and de Paiva et al. [12]. ( The second paper discusses how the Bridge system went from a system based on the common sense ontology Cyc to one based on the PARC Unified Lexicon ontology and how this change was not problematic.)

Here we present a third version of TIL this time trying to be independent from the grammar formalism (LFG), from the parsing mechanism (XLE) and from the rewriting linguistic semantics (Transfer) previously used.

Towards Textual Inference Logic TIL
Consider a given a corpus C of English sentences, appropriately tokenized and parsed. The sentences in C will be denoted s1, s2,.   Assume we have a perfectly
extended version of a combination of WordNet and VerbNet which produces lexical information about verbs, nouns, adjectives and adverbs in our corpus and that is accurate and complete. Assume that we have a named entity recognizer (NER) at our disposal as well as a parser producing a well-ranked collection of parse trees for each sentence.
Then for each sentence si in C a bridge system will produce a representation repi of the semantical content of si. This representation consists of a collection of assertions, relating the concepts and contexts mentioned in the sentence si. Any sentence has one or more contexts. Simple sentences like The boy arrived will have a single context, which we write as context(t). This context represents the world from the viewpoint of the author of the sentence.
For example for a sentence s1 like John Smith discovered that three men died, its representation rep1 will have two contexts, the top context context(t) and the context of what was discovered, context(ctx(die: 5)) and four concepts: Smith: 1, discover: 2, man: 4 and die: 5. These concepts are subconcepts of a general ontology, for concreteness sake, subconcepts of the WordNet synsets picked by the interpreta- tion and they are inter-connected by roles. For example the “agent” of the discover: 2 concept is the concept Smith: 1. The full representation rep1 is as below: Conceptual Structure:
subconcept(discover:2, [detect-1,.	, identify-5])
role(Theme, discover:2, ctx(die:5)) role(Agent, discover:2, Smith:1) subconcept(Smith:1, [male-2]) alias(Smith:1, [John, Smith, John Smith]) role(cardinality restriction, Smith:1, sg) subconcept(die:5, [die-1, die-2,.	, die-11])
role(Theme, die:5, man:4) subconcept(man:4, [man-1,.	, world-8])
role(cardinality restriction, man:4, 3) Contextual Structure:
context(t) context(ctx(die:5)) top context(t)
context lifting relation(veridical, t, ctx(die:5))
context relation(t, ctx(die:5), crel(Theme, discover:2)) instantiable(Smith:1, t)
instantiable(discover:2, t)

instantiable(die:5, ctx(die:5)) instantiable(man:4, ctx(die:5)) Temporal Structure:
temporalRel(startsAfterEndingOf, Now, discover:2) temporalRel(startsAfterEndingOf, Now, die:5)
A discussion on why using concepts, as subconcepts of given ones in a Word- Net/VerbNet ontology is a useful representation tool for natural language semantics, as it allows for natural inference of monotonicity entailments, is provided in [1,12]. Contexts as used in our representations come from ideas of McCarthy and Guha of partitioning knowledge into so-called microtheories which one can enter and exit, using lifting axioms – see [23,6]. In the example above, information from the inter- nal context (the context of what was discovered by John, the context ctx(die : 5), can be lifted to the top context, via the lift axiom that says that information in contexts introduced by ‘discover’ percolated up.
Were it not for the contexts, the logic produced by the bridge system would be a simplified version of (a translation of) a description logic, with only conjunctions and existential roles. But the intermingling of contexts and roles, represented above by the assertion role(Theme, discover:2, ctx(die:5)) makes this system more interesting and more complicated.
Given that we were trying to formalize an evolving system, it seemed a good strategy, at least as a first cut, to discuss possible logics of contexts over a proposi- tional basis in an abstract and modular fashion and to try to match these possible systems with the output coming up from the implemented system. This was the approach pursued in [6], [8] and [10]. We recap some of this approach, so that we set up some open problems later on.

Constructive logics of contexts
A survey of the extant logics of context for AI in the literature, as well as some adaptation of these ideas to the constructive setting was presented in [6]. That paper discusses how a very weak multi-modal logic, where modalities simply introduce boundaries within which different kinds of reasoning can happen seems a promising way to deal with the contexts of the bridge systems.
The idea following McCarthy is that apart from basic propositional variables, we have a collection of contexts Contexts and that propositional formulas are tagged with the contexts in which they hold. We write Ci(φ) meaning that proposition φ holds in context Ci. Buvac and Mason formalized this idea in [28]. The work in
[6] changes the basis of the logic of contexts into an intuitionistic (or constructive) system and also removes one extra axiom, called Δ by Buvac and Mason. The slogan summary of [6] is “Contexts as Constructive Modalities”.
The system proposed in [6], called CKn (for constructive modal Kn) is a multiple operators K-style multi-modality modal logic. The modalities are independent of each other, so that CiCj(φ) is not the same as CjCi(φ) in principle. Also the only property we have for each modality Ci is the K axiom, or that modus ponem is pre-

served as Ci(φ → ψ) → Ci(φ) → Ci(ψ). The system CK was originally introduced with a Natural Deduction proof theory and a categorical semantics in [18], Mendler and de Paiva produced Kripke-style semantics for the system CKn in [8]. Recently Mendler and Scheele have done some work on the correspondence theory for CK (personal communication) and also on its use as a basis for constructive description logic [22]. The main characteristics of CKn are that it has a very weak notion of necessity, not definable in terms of possibility, as traditional in constructive versions of modal logic. Bt unlike most constructive modal logics, the possibility modality 3 does not satisfy distribution over disjunction:
3(φ ∨ ψ) → 3(φ) ∨ 3(ψ)
3(⊥) →⊥ 
But even if one accepts that contexts should be modelled as constructive modal- ities, there are several possible kinds of constructive modalities. The system CKn can be contrasted with the system proposed by Simpson in his doctoral thesis[31], systematizing previous work on intuitionistic modal logics. This builds up a whole framework of intuitionistic modal logics, based on the system IK, which does satisfy the two propositions above. Simpson’s systems give us a generic framework with many of the desired properties of intuitonistic modal logics, smoothly integrated in a single framework. Its proof theory makes use of an extra kind of assertion that builds in the Kripke semantics as part of the deductive apparatus of the log- ics concerned. Other viewpoints on constructive modal logics exist, in particular, work starting from the premise that intuitionistic logic should be thought of as an S4-modality. This has produced different systems of constructive modal logics. In principle any of these competing systems of modal logic could be used in our mod- elling of contexts. One of the reasons for using CKn is that it is the weaker of these systems and extra axioms can be added, as necessary, to describe phenomena more precisely.

Constructive Description Logic
Given the empirical evidence from the linguists, which suggest strongly that we want a logical system with both contexts and concepts, it makes sense to investigate variations on the kinds of description logics available and also how notions of context could be incorporated with the concepts. In particular we would like to describe a traditional description logic, say ALC, with contexts. Our philosophy has been, from the beginning, to produce constructive logics, where we have a better chance of exploiting the Curry-Howard isomorphism to extract programs from specifications. So it make sense to think about a simple “constructive description logic” to begin with. Some of the issues involved here were spelled out in the paper “Constructive Description Logics: what, why and how”[10], from 2006. This line of work has been followed by M. Mendler and S. Scheele, who developed a fully functioning system, applicable to electronic auditing in [22]. Also Bozzato et al [26] have produced a full system KACL, part of a larger programme that includes the system BCDL for “Basic Constructive Description Logic”, given Kripke semantics and a tableau construction

for both systems. These systems also aim for a Curry-Howard isomorphism applied to description logic, but insist on a version of Kuroda’s axiom
∀R.¬¬A → ¬¬∀R.A
which they believe is necessary for a finite model property of the Kripke semantics. Recent work of de Paiva, Hausler and Rademaker[11] produces still another system of constructive description logic, this time following the lead of Simpson’s framework for constructive modal logic.

Constructive Hybrid Logic
However, the case could be made that instead of constructive description logics, we should investigate the notion of constructive hybrid logics. Hybrid logics, like Simpson’s constructive modal logics, take the notion of Kripke model and make it part of the description of the logic itself. But while Simpson’s approach simply adds to usual Natural Deduction formulas, labels to denote in which world x that formula φ holds, as well as assertions of the form xRy to explain how world x ‘sees’ world y via the accessibililty relation R, hybrid logics goes go a bit further and have the names of the worlds (‘nominals’) as a second kind of variable in the logic system, which can freely mix with the usual propositional variables. It is unclear to me whether this extra functionality might be useful (or not) for the application to natural language. But it is worth noticing that the rediscovery of hybrid logics, first conceived by Prior, was spear-headed by work of Seligman [29] and others on variants of hybrid logics for natural language descriptions.

Logic Open Problems
Several options of logical systems, some partially described, some not investigated at all, were mentioned in the previous subsections. It is my belief that these sys- tems should be, at least minimally, investigated for their potential uses as logics of context. To help start this chartering of the land, I present the following diagram, explaining some of the connections.
In the figure below (extracted from slides presented by the author in a semi- nar in Nottingham, June 2010), we have in the middle row, the classical systems of Hybrid Logic, Multimodal K and the canonical Description Logic, ALC. On the first row we have the corresponding systems, when constructivized in the Simpson way and in the bottom row, the systems constructivized in the de Paiva/Mendler way. The system at the bottom left does not exist, yet, it would be a “hy- brid logic” version of constructive Kn. The middle system in the bottom is the system CKn, as described by Mendler and de Paiva, while the right most sys- tem is the constructive description logic of Mendler and Scheele. Systems not in the diagram, but that should be considered are systems adding contexts, us- ing description logics endowed with modalities. As it can be seen there many opportunities for completing the diagrams and showing these logic systems useful.




Conclusions
We discussed a class of systems that take natural language sentences as input and produce logical formulas corresponding to these sentences. We called these systems bridges and we looked at possible targets for these bridges from language to logic. Then we surveyed some of purely logical systems. Much remains to be done. This enterprise is necessarily an empirical one and one of the main success criteria will be the coverage that any such system obtains. The pipelines of processing those possible bridges are long and there are opportunities for mistakes at every junction, so robust regression systems have to be in place.

References
C. Condoravdi, D. Crouch, J. Everett, R. Stolle, D. Bobrow, M. van den Berg and V. e Paiva, “Preventing Existence”. In Proceedings of Formal Ontology in Information Systems, FOIS’01, October 2001.

Everett, J., Bobrow, D., Condoravdi, C., Crouch, D., van der Berg, M., Polanyi, L., and V. de Paiva, “Making Ontologies Work for Resolving Redundancies Across documents”, Communications of the ACM, 2002.

Crouch, R., Condoravdi, C., Stolle, R., King, T. H., Everett, J. , Bobrow, D. and V. de Paiva“Scalability of redundancy detection in focused document collections”, ScaNaLU, Heidelberg, 2002


Stolle, R, Bobrow, D., Condoravdi, C., Crouch, R. and de Paiva, V. ”Knowledge Tracking: Answering Implicit Questions”. Proceedings AAAI Spring Symposium on New Directions in Question Answering, Stanford, California, March 2003.
Condoravdi, C., Crouch, R., Stolle, R., Bobrow, D. de Paiva, V. “Entailment, Intensionality and Text Understanding ”, Proceedings Human Language Technology Conference (HLT-NAACL-2003), Workshop on Text Meaning, Edmonton, Canada, May 2003.
de Paiva, V.“Natural Deduction and Context as (Constructive) Modality”, In Modeling and Using Context, Proceedings of the 4th International and Interdisciplinary Conference CONTEXT 2003, Stanford, CA, USA, Springer Lecture Notes in Artificial Intelligence, vol 2680, June 2003.
Bobrow, D, Condoravdi, C., Crouch, R., Kaplan, R., Karttunen, L., King, T. H., de Paiva, V. and Zaenen, A.,“A Basic Logic for Textual Inference”. In Procs. of the AAAI Workshop on Inference for Textual Question Answering, Pittsburgh PA, July 2005.
Mendler, M. and de Paiva, V. “Constructive CK for Contexts”, In Proceedings of the Worskhop on Context Representation and Reasoning, Paris, France, July 2005.
Brau¨ner, T. and de Paiva, V.“Towards Constructive Hybrid Logic”, Presented at Methods for Modalities 3, LORIA, Nancy, France, September 22-23, 2003. Journal version, “Intuitionistic Hybrid Logic” Journal of Applied Logic(JAL), 4(2006), 231– 255, 2006.
de Paiva, V. “Constructive Description Logics: what, why and how” (extended draft) Presented at Context Representation and Reasoning, Riva del Garda, August 2006.
de Paiva, V., Hausler E.H. and Rademaker, A. “Constructive Description Logics Hybrid-Style”, Manuscript submitted to HYLO10, Edinburgh 2010.
de Paiva, V., Bobrow, D., Condoravdi, C., Crouch, R., Karttunen, L, King, T. H., Nairn, R., and Zaenen, A.“Textual Inference Logic: Take Two”, Proceedings of the Workshop on Contexts and Ontologies, Representation and Reasoning, CONTEXT 2007.
Bobrow, D., Condoravdi, C., Karttunen, L., King, T. H.,de Paiva, V., Price, C., Nairn, R., Zaenen, A.“Precision-focused Textual Inference”, in Proceedings of ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 16-21, 2007.
Bobrow, D., Cheslow, R., Condoravdi, C., Karttunen, L., King, T.H, Nairn, R., de Paiva, V., Price,
C. and Zaenen, A.“PARC’s Bridge and Question Answering System”. Proceedings of Grammar Engineering Across Frameworks, pp 26–45, 2007.
Gurevich, O., Crouch, R., King, T. H. and de Paiva, V. “Deverbal Nouns in Knowledge Representation,
J. Log. Comput. 18(3): 385-404 (2008). 2008
Price, C., de Paiva, V and King, T. H.. “Context Inducing Nouns, in Knowledge and Reasoning for Answering Questions (KRAQ’08) workshop associated with COLING’08, Manchester, UK, 23 August 2008, preliminary version., 2008
Bos, J. and Delmonte, R. “Semantics in Text Processing”, College Publications, 2008.
Bellin, G., de Paiva, V. and Ritter, E.“Extended Curry-Howard Correspondence for a Basic Constructive Modal Logic”, In Methods for Modalities, 2001.
Gamut, L. T. F., “Logic, Language and Meaning, Volume 1: Introduction to Logic”, CSLI Publications, 1990.
Montague, R. “Universal grammar”. Theoria 36 (1970), 373398. (reprinted in Thomason, 1974).
Crouch, R. “Packed Rewriting for Mapping Semantics to KR.” In Proceedings Sixth International Workshop on Computational Semantics, 2005.
Mendler, M. and Scheele, S. “Towards Constructive DL for Abstraction and Refinement”. Description Logics, 2008.
McCarthy, J. “Notes on Formalizing Context” in Proceedings of IJCAI - 1993.
Copestake, A.“Slacker Semantics: Why Superficiality, Dependency and Avoidance of Commitment can be the Right Way to Go”, Invited talk at EACL 2009, available from http://www.cl.cam.ac.uk/~aac10/ research.html.
MacCartney, W. and Manning, C.“An extended model of natural logic.” The Eighth International Conference on Computational Semantics (IWCS-8), Tilburg, Netherlands. 2009.
Bozzato, L., Ferrari, M., Villa, P. “A note on constructive semantics for description logics.” Accepted at CILC09 - 24 Convegno Italiano di Logica Computazionale. 2009.

Nairn, R., Condoravdi, C. and Karttunen, L.“ Computing Relative Polarity for Textual Inference.” In Proceedings of ICoS-5 (Inference in Computational Semantics), 2006.
Buvac, S., Buvac, V. and Mason, I. “Metamathematics of contexts”, Fundamenta Informaticae, 23(3), 1995.
Seligman, J. “The Logic of Correct Description” In M. de Rijke, et al (ed.) Advances in Intensional Logic. Dordrecht, Kluwer Academic Publishers, 107-136, 1997.
Crouch, R. and King, T. H. “Unifying Lexical Resources” Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes, Saarbruecken, Germany. 2005.
Simpson, A. “The Proof Theory and Semantics of Intuitionistic Modal Logic”.PhD thesis, University of Edinburgh, 1994.
Hall, D., D. Jurafsky and C. D. Manning. 2008. “Studying the History of Ideas Using Topic Models”. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 363-371.
http://nlp.stanford.edu/~manning/papers/D08-1038.pdf
