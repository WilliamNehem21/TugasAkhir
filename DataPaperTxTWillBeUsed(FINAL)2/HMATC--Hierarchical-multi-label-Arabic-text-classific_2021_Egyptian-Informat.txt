Egyptian Informatics Journal 22 (2021) 225–237











HMATC: Hierarchical multi-label Arabic text classification model using machine learning
Nawal Aljedani ⇑, Reem Alotaibi, Mounira Taileb
Department of Information Technology, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia



a r t i c l e  i n f o 


Article history:
Received 21 May 2020
Revised 4 August 2020
Accepted 29 August 2020
Available online 22 September 2020


Keywords:
Text classification
Multi-label classification Hierarchical classification Machine learning
Arabic natural language processing
a b s t r a c t 

Multi-label classification assigns multiple labels to each document concurrently. Many real-world classi- fication problems tend to employ high-dimensional label spaces, which can be naturally structured in a hierarchy. In this type of problem, each instance may belong to multiple labels and labels are organized in a hierarchical structure. It presents a more complex problem than flat classification, given that the clas- sification algorithm has to take into account hierarchical relationships between labels and be able to pre- dict multiple labels for the same instance. Few studies have investigated multi-label text classification for the Arabic language. Most of these studies have focused mainly on flat classification and have neglected the hierarchical structure. Therefore, this paper explores the hierarchical multi-label classification in the context of the Arabic language. It proposes a hierarchical multi-label Arabic text classification (HMATC) model with a machine learning approach. The impact of feature selection methods and feature set dimensions on classification performance are also investigated. In addition, the Hierarchy Of Multilabel ClassifiER (HOMER) algorithm is optimized via examination of different sets of multi-label classifiers, clus- tering algorithms and different numbers of clusters to improve the hierarchical classification. Moreover, this study contributes to existing research by introducing a hierarchical multi-label Arabic dataset in an appropriate format for hierarchical classification and making it publicly available. The results reveal that the proposed model outperforms all models considered in the experiments in terms of the computational cost, which consumed less cost (2 h) compared with other evaluated models. In addition, it shows a sig- nificant improvement compared with the state-of-the-art model (Fatwa model) in terms of Hamming loss (0.004), hierarchical loss (1.723), multi-label accuracy (0.758), subset accuracy (0.292), micro-averaged
precision (0.879), micro-averaged recall (0.828), and micro-averaged F-measure (0.853).
© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intel- ligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

The rapid growth of web pages, online storage providers, and social media networks has led to a significant increase in the num- ber of available electronic text documents. According to the Inter- national Data Corporation, ‘‘the digital data on the internet will grow to 40,000 exabytes in 2020 from 130 exabytes in 2005” [1]. Effective management and classification of such data requires an accurate automatic text classification model. Thus, text classifica- tion or categorization (TC) remains an important field of research which attracts a significant attention. In general, TC can be for-

* Corresponding author.
E-mail addresses: naljedani0026@stu.kau.edu.sa (N. Aljedani), ralotibi@kau.edu. sa (R. Alotaibi), mtaileb@kau.edu.sa (M. Taileb).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
mally defined as a supervised machine learning technique that automatically assigns a given instance to predefined labels based on its content [2]. A classification model is trained using training data, which includes a collection of instances and their correspond- ing labels (categories) [3].
Prior research has used two approaches for classification. On one hand, single-label classification, is the traditional classification, which assigns only one predefined label to each instance. Single- label classification can be either binary classification or multi- class classification. On the other hand, multi-label classification (MLC) assigns a set of predefined labels for each instance simulta- neously [4]. Usually, it is inadequate to classify each instance under just one single label, because several labels could describe its con- tent concurrently [5]. For example, a news article that is assigned to an ‘education’ label could also be assigned to several other labels simultaneously, such as ‘social’ and ‘technology’.


https://doi.org/10.1016/j.eij.2020.08.004
1110-8665/© 2020 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



MLC is divided into two classification types: flat and hierarchi- cal. In flat classification, a set of predefined labels are classified without considering the hierarchy of the relationship between the labels [6]. In contrast, in hierarchical multi-label classification (HMC) a single instance may have multiple labels concurrently, and these labels are structured in a hierarchy [7]. This type of clas- sification, presents a more complex classification problem than flat classification, given that the classification algorithm has to take into account hierarchical relationships between labels and be able to predict multiple labels for the same instance.
HMC tasks have become important in many application, such as web pages classification, digital libraries, electronic books, patents, and newspaper articles. The use of machine learning to conduct HMC is a good way to facilitate document classification and retrieve, search, and request information easily and efficiently [8]. Several studies have conducted MLC for the English language.
However, few studies have been conducted for the Arabic lan- guage, and research in this field has tended to focus on flat multi-label classification and has not considered HMC in depth. Arabic is the native language of 380 million people [9] and is one of the six official languages used by the United Nations [10]. It con- tains twenty-eight alphabet letters, with twenty-five consonants and three long vowels. It also has a vast vocabulary and complex morphology [11]. The significant use of Arabic language on elec- tronic websites and social media networks, especially in recent years, has led to the requirement to develop an automatic text clas- sification technique that can organize and categorize a large amount of electronic Arabic text documents efficiently.
Therefore, this paper addresses the problem of HMC in the con- text of the Arabic language by proposing a hierarchical multi-label Arabic text classification (HMATC) model that can deal with the hierarchical structure of Arabic text. The proposed model opti- mized the Hierarchy Of Multilabel classifiER (HOMER) algorithm to improve the hierarchical classification task.
In particular, the primary contributions of this paper can be summarized as follows:

Proposes the HMATC model based on HOMER algorithm. Optimizes the essential parameters of the HOMER algorithm using different sets of multi-label classifiers, clustering algo- rithms and different numbers of clusters.
Provides an insight into the impact of feature selection methods and feature set dimensions on the proposed model.
Introduces multi-label Arabic dataset in an appropriate format for a hierarchical classification and making it publicly available online.
For reproducibility purposes, the source code used in the HMATC implementation is accessible online on the GitHub page.1

The rest of this paper is organized as follows: Section 2 summa- rizes different MLC methods and discusses the relevant research studies that have been conducted on MLC for Arabic text. Section 3 introduces the HMATC model and describes the steps required for its development. Then, Section 4 discusses the experiment by describing the dataset statistics, the evaluated methods, and the experimental setting. Section 5 discusses the results. Finally, Sec- tion 6 presents the conclusion and future research directions.


Related work

An extensive review of multi-label text classification is pre- sented in the following sections to give insight into the existing
MLC techniques and the relevant research studies that have focused on Arabic text.

Multi-label classification methods

MLC can be divided into flat and hierarchical classification. Flat classification can be conducted using a problem transformation (PT) or algorithm adaptation technique. A PT technique simply aims to transform MLC problem into single-label problems, after which a traditional single-label classification algorithm is used to perform the classification task. An algorithm adaptation technique is concerned with adapting single-label classification algorithms to deal with the MLC problem directly. Examples of such techniques include a multi-label lazy learning (ML-kNN) algorithm [12] and a multi-label decision tree (ML-DT) algorithm [13].
The PT technique includes two general methods. The first of which involves transforming the multi-label problem into a set of binary classification problems. Examples of this method include binary relevance (BR) [14], classifier chains (CC) [15], and ranking by pairwise comparison (RPC) [16]. The second method converts the multi-label problem into a multi-class classification problem. Examples of this method include label powersets (LP) [17], pruned sets (PS) [18], and random k-labelsets (RAkEL) [19].
On the other hand, HMC is considered an extension or variant of MLC in which a hierarchical structure is considered on the multi- labels. The output of the classification algorithm for a given multi-label problem is a set of labels structured in a hierarchy [7]. The hierarchical multi-label problems are classified typically as a tree-hierarchy or a directed acyclic graph (DAG) [8]. Generally, the classification algorithms proposed for HMC are more capable of dealing with large sets of labels than those proposed for flat classification.
Several algorithms have been proposed for HMC, such as: hier- archical decision trees [20], the hierarchical k-nearest neighbors algorithm [6], incremental algorithms for hierarchical classification [21], hierarchical support vector machines [22], HMC using fully associative ensemble learning [23], the HOMER algorithm [24], basic categorization algorithm (BCA), and percentage difference categorization (PDC) algorithm [25].

Multi-label classification for Arabic text

Generally, MLC methods applied for English can be applied to Arabic language as well, but the difference basically is in the pre- processing phase. The following sections present a review of exist- ing research studies that have been conducted regarding MLC in Arabic texts.
Ahmed et al. [26] conducted a study on binary and multi-class classification transformation methods using several multi-label classifiers. They transformed the MLC of Arabic data into single- label classification using MEKA2 tool to implement LP, BR, and rank- ing and threshold-based (RT) approaches. The standard single-label machine learning algorithms applied as base classifiers were: a sup- port vector machine (SVM), a k-nearest neighbor (k-NN) algorithm, a Naive Bayes (NB) method, and a decision tree (DT). The collected dataset on which the evaluation was performed consisted of 10,000 news articles classified into five labels (Sports, Arts, Economy, Politics, and Science). The results of the evaluation showed that using SVM as a base classifier with the LP method achieved the best ML-accuracy with 71%.
Taha and Tiun [5] developed a new MLC model using a BR method. The main objective of the study was to solve the MLC problem for Arabic datasets by employing a BR method based on


	
1 https://github.com/NawalJed/HMATC.	2 http://waikato.github.io/meka/.



different sets of single-label machine learning classifiers including SVM, NB, and k-NN. The evaluative experiments were performed using the same dataset collected in [26]. The experiment results showed that using a BR method consisting of various sets of single-label classifiers (SVM, NB, and k-NN) achieved the best results.
Shehab et al. [27] conducted a study that focused on Arabic news articles. Three multi-label classifiers were adapted to deal with MLC problems: random forest (RF), DT, and k-NN with k = 5 (5-NN). The researchers conducted experiments on a collected dataset of 10,997 news articles classified with multiple labels, such as Economics, Sports, World, Middle East, Science & Technology, and Miscellaneous. The evaluation results showed that the DT clas- sifier achieved a better performance than the RF and 5-NN classifiers.
Hmeidi et al. [28] proposed a lexicon-based multi-label Arabic text classification model. They collected 4720 Arabic articles with thirty-five labels from the BBC news website. To classify the multi-label Arabic dataset, they employed a combination of lexi- cons of each label in a multi-label problem. Label prediction was achieved by matching the terms of each label stored in the lexicons with the term vector of a given instance and classifying them according to term frequency. Then the first five labels with the greatest count values were predicted. Finally, several experiments were conducted, and the results indicated that the performance of the lexicon-based model outperformed the corpus-based approach in terms of multi-label accuracy (ML-accuracy).
Al-Salemi et al. [1] conducted a study that sought to investigate MLC problems by conducting an in-depth comparison of the most common MLC algorithms used in the PT method, such as BR, CC, LP, and calibrated ranking by pairwise comparison (CRPC) [29]. These methods were trained using three base classifiers (SVM, kNN, and RF). Four algorithm adaptation techniques were also evaluated. These were: ML-kNN, RFBoost [30], binary relevance kNN (BRkNN) [31], and instance-based learning by multi-label logistic regression (IBLRML) [32]. The algorithms were evaluated using the RTAnews dataset3 which is a multi-label Arabic dataset of 23,837 Arabic news articles distributed over forty categories. A comparison was per- formed to investigate the effectiveness of the introduced dataset (RTAnews) in MLC tasks. The experiment results showed that both RFBoost and LP with SVM outperformed the other MLC algorithms. Moreover, the algorithm adaptation methods performed faster than the other PT algorithms except for the LP method.
Elnagar et al. [33] conducted a study that introduces two new Arabic datasets for both single-label and multi-label text classifica- tion tasks. The datasets called SANAD (Single-label Arabic News Articles Dataset) and NADiA (multi-label News Articles Dataset in Arabic). Both datasets were collected from news sources and avail- able online on Mendely.4 Further, they conducted an extensive com- parison of several deep learning models, to investigate the effectiveness of the introduced datasets on the Arabic text classifica- tion tasks. The results showed that all models achieved good results when evaluated using SANAD dataset, CGRU (convolutional gated recurrent unit) achieved the lowest accuracy of 91.18%, whereas HANGRU (hierarchical attention network-gated recurrent unit) achieved the highest performance of 96.94%. Concerning NADiA dataset, HANGRU has the best overall accuracy of 88.68%.
Zayed et al. [34] constructed a hierarchical multi-label classifi- cation model to address HMC problems in the Arabic language which used the HOMER algorithm to classify received Islamic requests (Fatwa) into the most appropriate hierarchical categories. They trained the HOMER algorithm using the default classifiers (BR

3 https://data.mendeley.com/datasets/322pzsdxwy/1.
4 https://doi.org/10.17632/57zpx667y9.1;https://doi.org/10.17632/hhrb7phdyx.1.
and NB classifiers) and conducted experiments on the collected hierarchical multi-label Arabic dataset. The dataset before pre- processing contains about 100,000 text instances labelled with 830 labels. It was processed with standard pre-processing meth- ods, including text cleaning, stop-word removal, and stemming. The words were stemmed using a Light10 stemmer, and the fea- tures were selected using a BR and Chi-square feature selection method. After processing the dataset, removing the empty instances (instances with no features or labels), and removing the labels that have less than 20 instances, the final version of the dataset included about 15,539 text instances assigned to 310 multiple labels organized as a tree-structured hierarchy. The authors focused on comparing the HOMER classifier with a flat BR-NB multi-label classifier. The results of the study showed that using the HOMER and its variations in the hierarchical classifica- tion of Fatwa requests achieved more effective predictive perfor- mance compared to the BR-NB classifier, which simply classified each label independently.
Table 1 presents a summary of the relevant research studies that have applied MLC methods to Arabic text using machine learning. It shows the type of MLC methods that have been inves- tigated in each study. It also identifies the dataset size and the dataset source for each study.
The main challenges identified by the authors of the previous studies, which caused the lack of research into MLC in the Arabic context were: the dearth of large and publicly-available multi- label Arabic datasets and the vast vocabulary and complex mor- phology of the Arabic language.
Notably, most previous research has focused mainly on flat MLC methods, and to the best of our knowledge, only one study has investigated dealing with HMC problems in Arabic texts using machine learning. This study [34] employed a HOMER algorithm with its default classifiers (BR and NB classifiers). The study did not investigate empirically the impact of feature selection methods and feature set dimensions on the model. In addition, the dataset used in that research has not been published online.
Therefore, in our work, we focus on addressing HMC problems in the context of the Arabic language by proposing the HMATC model. We optimize the essential parameters of HOMER algorithm using different sets of multi-label classifiers, clustering algorithms and different numbers of clusters to improve the hierarchical clas- sification. We further, provide an insight into the impact of feature selection methods and feature set dimensions on the proposed model. Moreover, we introduce a hierarchical multi-label Arabic dataset in an appropriate format for hierarchical classification and making it available online to the research community. The model proposed in [34], which is called the ‘Fatwa model’, is included in the evaluation comparison conducted in this paper as it was applied in the same domain.


Hierarchical Multi-label Arabic text classification (HMATC) model

The overall architecture of the proposed HMATC model is pre- sented in Fig. 1, which illustrates the phases of the model’s development.
The main purpose of this model is to classify Arabic text (Isla- mic Fatwa requests) automatically into multiple labels organized in an appropriate hierarchical structure. The model considers label dependencies by exploiting label correlations found in the training data. It incorporates the pre-processing technique and feature selection method and optimizes the essential parameters of the HOMER algorithm using different multi-label classifiers and clus- tering algorithms in order to obtain a competitive hierarchical multi-label classification model for the Arabic language.

Summary of the relevant studies that applied MLC methods on the Arabic text using machine learning.

Reference	Year	MLC	Dataset size	Dataset source

Flat	HMC
p



[1]	2019	p	23,837	Obtained from ‘‘Russia Today Arabic news portal” website.



Fig. 1. Architecture of HMATC model.

The HMATC model was trained in this study using a hierarchical multi-label Arabic dataset. Then it was evaluated using evaluation metrics dedicated to multi-label classification. The following sub- sections describe the main phases of the model’s development.

Dataset preparation phase

Since the objective of this study is to address HMC problems in the Arabic language, the experiments were conducted on a multi- label Arabic dataset, in which the labels are organized into a tree- structured hierarchy.
There is a lack of publicly-available multi-labelled Arabic data- sets, especially those with hierarchical multi-labels. For this rea- son, the dataset used in this study was a raw hierarchical multi- label Arabic dataset taken from a study performed by Zayed et al. [34], which was applied in the same domain.
The dataset was a raw dataset related to the Islamic field and written in Modern Standard Arabic (MSA). It was stored in three main database tables. The first one was the Fatwa table, which con- tained about 100,000 text instances (Islamic Fatwa requests), as presented in Table 2. The second table was the Categories table, which contained 830 labels associated with their parent-ID that defined their hierarchical structure, as presented in Table 3. The third table was the Fatwa-Categories table, which contained a set of instances Ides’ and labels Ides’ and defined the associated labels for each instance. The third database table was used to assign each instance to its set of labels and the second table was used to trace the parent-ID of each label to define the labels’ hierarchical structure.
One of the main challenges faced in this study was preparing the dataset by assigning each instance its own set of labels based on the third aforementioned database table (Fatwa-Categories table), and ensuring that the labels were in an appropriate format for hierarchical classification (they satisfied the hierarchical con- straints). After removing the empty instances (instances with no features or labels), and assigning each text instance to its hierarchi- cal multi-labels, the dataset was reduced to 26,484 text instances as presented in Table 4. The table shows that e.g., instance number one was labelled with two sets of hierarchical multi-labels. These were similar in terms of parent labels (General Category, Jurispru-
dence, Worship, Zakat –	) but dif-
fered in terms of leaf(ves) labels (Zakat Conditions – ) and (The Rule of Zakat – ).
After the labelling process, the hierarchical multi-label prob- lems were identified based on their occurrences using a Boolean bag of words ({0,1} representation). To satisfy the hierarchical con- straints, whenever the instance labelled by a label at a particular node, all parent labels of this node were represented by {1}, includ- ing the root node, otherwise, they were represented by {0}. The total number of labels identified was 830, which was a large num- ber that would have increased computational cost. As a result, the label dimensionality was reduced by removing the labels that were rarely used in the dataset (e.g., labels associated with two instances or less), such that when the rare labels were the only labels related to the text instances, we deleted the associated text instances with these labels. In other cases, when there are other labels associated with text instances than these rare labels, we kept the text instances. The removed rare labels usually were the leaf(ves) labels, leaving 578 remaining labels that were used in the experi- ments. Fig. 2 presents an example of the tree-structured hierarchy of the labels in the Islamic Fatwa requests dataset.


Text pre-processing phase

To classify any text document using a machine learning algo- rithm, a raw text should be pre-processed. Pre-processing is a chal- lenging task in the Arabic language but has the greatest impact on the classification performance of the model used due to the rich- ness of the Arabic language, which contains more complex mor- phology than other languages [35–38]. In this phase, the raw text was prepared and transformed into a representation suitable for the application of a classification algorithm [11]. The vector space model is the most well-known approach used for document repre- sentation. According to this approach, each text instance is repre- sented by a vector x that contains a list of distinct features (words) [39]. A term frequency-inverse document frequency (tf- idf) weighting scheme [40,41] was applied in this study to identify the features of the model, whereby each instance was represented using a vector of the terms’ weights.
The following procedures were applied to the dataset in this study in the pre-processing phase [42,43]:


Table 2
Examples of Fatwa requests (Questions) from database table of the Islamic Fatwa requests dataset, where Instance # represents number of instance and ID represents the primary key of this table in the database.




Table 3
Example of categories (Labels) database table of the Islamic Fatwa requests dataset, where Label # represents number of label and ID represents the primary key of this table in the database.







Table 4
Examples of text instances from the Islamic Fatwa requests dataset, labelled with their hierarchical multi-labels.






Fig. 2. Example of a hierarchical structure found in the labels of the Islamic Fatwa requests dataset.



Tokenization. Also called text segmentation, tokenization aims to split a text into a set of features, namely tokens, based on a specific delimiter such as commas, white space, periods, etc., Stop-word removal. Stop-word-removal aims to remove insignificant words. Stop-words are words which are frequently encountered in the text and carry no useful information and therefore can affect classification performance. These include prepositions, conjunctions, pronouns, and others.
Stemming. Stemming aims to stem or return the derived fea- tures to their roots or stems. Transforming semantically similar features to their root form reduces the feature space, decreases the morphological variance of words, and improves the classifi- cation performance of the model [44,45].

In the current study, the researchers pre-processed the dataset and cleaned the raw text instances by removing numbers, non- Arabic letters, punctuation, and special characters ($, %, @, &). After
text cleaning, the three steps described above were applied. First, tokenization was applied using space as the text delimiter. Then, a stop-words list was prepared and the identified words were removed from all instances. These included preposition (from, to,
about, in –	) and conjunctions (but, or, even, then –
), although stop-words with a significant meaning
in the Islamic context, such as Ramadan, during, before, and after were retained. In addition, Arabic human names (such as Khaled, Ahmed, Sara, Amal – ) were also removed, since
they were repeated frequently in the dataset and carried no useful
information, which may have affected classification performance. Finally, stemming was carried out using a Snowball stemmer. Snowball is a small text processing language used to create the stemming algorithm, it supports many languages including the Arabic language. It aims to return words to their stems by stripping off common prefixes (e.g.,	) and suf-
fixes (e.g.,	)
The number of features that remained after the pre-processing phase was 11,000 features. After completion of the dataset prepa- ration, implementation of the pre-processing phase, and removal of unlabeled instances and rare labels, the final version of the data- set contained 26,470 text instances labelled with 578 hierarchical multi-labels in total.
Table 5 presents the dataset as a vector space model, whereby each instance is represented as a vector x containing a list of dis- tinct features represented using a tf-idf weighting scheme.
The dataset was imbalanced, which means that the number of labels associated with the instances was not equally distributed. In general, the high-level labels in the root and parent nodes were usually associated with a high number of instances, whereas the low-level labels in the leaf(ves) nodes were associated with a low number of instances.

Feature selection phase

The pre-processing phase produced a set of stemmed features. However, the total number of features and the number of unneces- sary features remained high, which would have affected classifica-
Table 5
An example of a vector space model of the processed dataset.
tion performance by increasing computational complexity. Therefore, an additional feature selection method was needed to reduce the features number by selecting a subset of the most rele- vant and highest-ranking features. Using such a method effectively reduces the dataset dimensionality by removing irrelevant or redundant features without decreasing the classification perfor- mance. Hence, it can improve the learning process and reduce computational complexity [43].
However, unlike single-label data, which can directly use tradi- tional feature ranking methods (such as Chi-square (v2) [46], infor- mation gain (IG) [47], gain ratio (GR), and relieF (RF)) to score features according to their relevance to the labels and then select
the most relevant features, the standard method of feature selec- tion in multi-label data, as illustrated by Spolaor et al. [48], is based on applying a PT classifier (e.g., LP, BR, CC) to transform MLC prob- lems into single-label problems, after which a traditional feature ranking method can be employed to perform the feature selection. One of the most common methods of multi-label text classifica- tion is using a BR method to determine the discriminative power of each feature concerning each label independently of the rest of the labels. After this, the computed scores are aggregated to obtain
overall features’ ranking [49].
In the MLC context, each combination of PT classifier and fea- ture ranking method is considered a separate feature selection method. Thus, this study used an abbreviation of the PT classifier followed by a feature ranking method abbreviation separated by a hyphen symbol to denote the feature selection method in the multi-label classification context. For example; LP-IG referred to an independent feature selection method consisting of an LP clas- sifier and IG feature ranking method, respectively.
The feature selection phase was examined by investigating dif- ferent sets of feature selection methods and examining different sets of the high-ranking features, to obtain the best method and feature set dimensions for optimizing the model performance. The details of this phase are presented in the results and discussion section (see Section 5.1 and Section 5.3).

Classification phase

This paper investigates applying the HOMER algorithm in a domain with a large set of labels such as the Islamic context. The HOMER algorithm is an effective hierarchical multi-label classifier that uses a divide-and-conquer approach [24]. It can efficiently handle MLC problems with a large number of labels by construct- ing a tree-shaped hierarchy of simpler MLC problems. First, HOMER automatically organizes the large set of labels into a tree-shaped hierarchy. This is accomplished by applying a cluster- ing algorithm which repetitively partitions the labelset into a num- ber of nodes (clusters). Then, it employs a multi-label classifier (e.g., BR) for each node, where each classifier can handle a small number of labels instead of handling a large labelset.
There are three variants of the HOMER algorithm based on the clustering algorithm applied in the label distribution task. The first variant is HOMER-K, which uses a k-means clustering algorithm to distribute the labels into k number of clusters based on label sim- ilarity, without any constraint on the size of the cluster. The moti- vation here is investigating the benefits of even label distribution








on top of the clustering. However, the authors of [24] proposed a new clustering algorithm known as a balanced k-means clustering algorithm, where the labels are evenly distributed into k balanced clusters. This defines the second and the default variant of HOMER called HOMER-B, which includes the additional constraint of con- structing clusters with equal size. The third HOMER variant known as HOMER-R evenly but randomly clusters the labels into k clus- ters. The motivation of HOMER-R is investigating the benefits of clustering on top of the even label distribution.
Fig. 3 illustrates the tree hierarchy in the HOMER algorithm used for the classification of a simple MLC problem with nine labels (General Category, Jurisprudence, Worship, Zakat, Fast, Fasting Validity,  Fasting,  Zakat  Conditions,  The  Rule  of  Zakat  –
-
). These labels represent a small part of the
hierarchical multi-label of the Islamic Fatwa requests dataset, as highlighted in Fig. 2. Each internal node includes the union of the meta-labels l of its children, and the root node includes the labels
of all nodes in the tree. A multi-label classifier S was employed for each node to predict the meta-labels of its children. The default structure of the HOMER algorithm was implemented by employing a BR-NB multi-label classifier and balanced k-means clustering algorithm. Where the abbreviation BR-NB refers to the BR multi- label classifier implemented using NB base classifier.
To predict the labels of an unseen instance e (let’s suppose it is related to Zakat – ), HOMER starts from the root node and then follows a recursive approach to forward e to the most relevant multi-label classifier. In the example illustrated in Fig. 3, the multi-label classifier S1 at the root node will forward the instance
e to the multi-label classifier S2 only if l2 is among the predicted
labels of the S1 classifier. By following a recursive process, the final prediction will be a union of all predicted labels just higher than the corresponding leaf(ves) nodes. Regarding the instance e, the predicted labels of this instance will be General Category, Jurispru- dence, Worship, Zakat, Zakat Conditions, The Rule of Zakat –
. This means
that when the instance e is assigned to the labels at a particular node	(e.g.,	Zakat	Conditions,	the	Rule	of	Zakat	–
), it should satisfy the hierarchical constraint
and should therefore be assigned to all parent labels of these nodes including the root node (e.g., General Category, Jurisprudence, Worship, Zakat – ). If no label is pre-
dicted, the algorithm will return an empty set of labels.
hierarchical classification outcomes (see Section  5.2 and Section 5.4).

Evaluation metrics

MLC models are evaluated using evaluation metrics that are commonly used in the MLC field [49]. Several multi-label evalua- tion metrics were proposed which divided into two main approaches: example-based (instance-based) and label-based met- rics [3]. The first approach is measured for each test instance and then averaged over all test instances. Whereas the second approach is measured for each label and then averaged over all labels.

Example-based metrics
The most common example-based metrics that are used to eval- uate MLC models are described in the following. Suppose that: m refers to the total number of instances in the test dataset, i indi- cates an instance in the test dataset (where 1 6 i 6 m), L = kj : j = 1 ... q is the set of labels, where q is the total number of labels, Zi and Yi refer to the predicted and actual labels, respectively.

Hamming loss. It calculates the average number of errors found in the instance-label pairs, averaged over all instances as shown
in Eq. (1). Where the factor  1  is used to get the normalized
(q)
value in [0,1] and M defines the symmetric difference between predicted and actual labels.
1 m 1
Hamming loss = m	q |ZiMYi|	(1)
Hierarchical loss. It  i=1	the hierarchical structure of labels
into consideration [21]. It follows a top-down approach to examine the predicted labels based on the existing label hierar- chy. Thus, whenever the label is predicted wrong, the subtree rooted at that node is not considered in the loss calculation
[49] as shown in Eq. (2). Where anc(k) refers to the all ancestor nodes (subtree) of label k.
H	Loss	 1	Y  Z anc	Y  Z	2
m i=1
ML-accuracy. It is known as multi-label accuracy or a Jaccard index. It computes the ratio of the labels predicted correctly over the total number of labels, as shown in Eq. (3).
m

Tsoumakas et al. [24] evaluated the HOMER algorithm with its variations by employing a BR-NB multi-label classifier. The results
ML	accuracy	 1	|Zi ∩ Yi|
m i=1 |Zi ∪ Yi|
(3)

of the study showed that the HOMER which relied on similarity- based distribution and employed a BR-NB classifier reduced com- putational complexity and improved predictive performance.
The benefit of similarity-based distribution is that it keeps the labels belonging to each node as similar as possible. This means that only relevant meta-labels are predicted, and the rest of the sub-tree is inactivated during the testing phase, which reduces the total computational cost. Another advantage of even label dis- tribution is that it avoids the class-imbalance problem by attempt- ing to distribute the labels into a set of balanced clusters; thus,
each multi-label classifier deals with a more balanced distribution
Subset accuracy. It also called exact match ratio or classifica- tion accuracy [50]. It is a very strict metric used to measure the ratio of predicted labels which exactly match their corre- sponding actual set of labels, as shown in Eq. (4). Where I (true) = 1 and I(false) = 0.
m
Subset accuracy	I Zi	Yi	4
m i=1
Precision. This metric giving us the ratio of labels correctly clas- sified of the predicted labels as shown in Eq. (5).
m

of positive instances at each node.
Precision =  1 X |Zi ∩ Yi |
(5)

the optimization of the essential parameters of the HOMER algo- rithm (multi-label classifier, clustering algorithm and numbers of
Recall. This metric is computed as shown in Eq. (6), computes the ratio of correctly predicted labels of the actual labels.
m

clusters) using a different set of multi-label classifiers, and cluster-

Recall =  1 X |Zi ∩ Yi |
(6)




Fig. 3. Example of a classification task of MLC problem with nine labels of the used dataset using HOMER algorithm.



F-Measure. It represents the harmonic mean between precision
Card =  1 X|Y |	(10)

and recall. It is computed as shown in Eq. (7).
m
m i=1

F	measure	 1	2|Zi ∩ Yi |
m i=1 |Zi| + |Yi|
(7)
Dividing the label cardinality by the number of labels (q) obtains the normalized version of label cardinality known as den- sity [49], and it is computed as shown in Eq. (11).

All example-based metrics described in this section indicate
1 m |Yi|

that the metric with the highest value has better performance, except Hamming loss and H-loss metrics, the lower value of these metrics indicates the better performance.

Label-based metrics
Dens = m
i=1


4.2. Methods
q	(11)

The binary evaluation metrics (e.g., recall, precision, and F- measure) can be calculated for all labels based on two approaches of computing the average; either macro or micro averaged approaches. These metrics are widely used to measure the average for recall, precision, and F-measure. Let B(tp, tn, fp, fn) a binary evaluation measure of the label i computed based on the number of true positives (tp), true negatives (tn), false positives (fp), and false negatives (fn). The expressions of macro-averaged and micro-averaged metrics for B(tp, tn, fp, fn) are illustrated in Eq.
(8) and Eq. (9), respectively.
1
i	i	i
i=1
The primary aim of this work was to incorporate the pre- processing techniques, feature selection methods, and HOMER algorithm to obtain a competitive HMC model for the Arabic lan- guage. Thus, several experiments were conducted, which primarily focused on investigating the impact of the feature selection method and the dimension of the selected feature set on the clas- sification performance of the model. In addition, the effect of the HOMER parameters (multi-label classifier, clustering algorithm, and the number of clusters) on the model performance were also examined.
Finally, after the HMATC model was constructed using the methods that obtained the best performance in each phase, it was then compared against four other models, two of which are
baseline models and the two others are the state-of-the-art models

q
Bmicro = B
i=1

tpi ;
Xi=1

fpi ;
Xi=1

tni;
Xi=1
fni !

(9)
— the BR, LP, CC, and Fatwa model [34], respectively. The results of these experiments are illustrated and discussed in Section 5.

4.3. Experimental setting

Experiment set-up

The following sub-sections describe the dataset statistics, meth- ods, and experimental setting used in this study.

Dataset statistics

The size of the final version of the dataset was 26,470 labelled text instances, which included 11,000 features. The total number of labels was 578, and the total number of labelsets was 6107. Labelsets are the active set of labels associated with each instance and represented by {1} in a vector space model. There are other basic measures of multi-label data, which are label cardinality (Card) and density (Dens). Label cardinality represents the average number of labelsets (active labels) per instance, and it is computed as shown in Eq. (10). Where i is any instance in the dataset

Different experiments were performed on the large dataset, which was associated with a large set of labels. Thus, the experi- ments required a large memory and greater computational resources. As a result, all experiments were carried out using the ‘Aziz’ High Performance Computing Center (HPCC)).5 Aziz supports parallel processing and comprises of 496 computing nodes. There are 4 login servers in addition to several servers that offer different sys- tem services. The compute nodes are grouped into four types of queues depending on the physical characteristics of these computing nodes. All nodes run Unix operating system (CentOS 6.4). The exper- iments were conducted on the ‘‘Fat Queue” which contains 112 com- pute nodes, each node has 256 GB memory and an Intel Xeon processor with 2.40 GHz. These are vital properties for parallelizing experiments and reduce the total computational cost of the evalua- tion process.5-fold cross-validation sets were used to evaluate the

(1 6 i 6 m), q and Y refer to the number of labels and labelsets,		

respectively [49].
5 https://www.hpcc-kau.com/aziz-super-computer.



predictive performance of the models. All the MLC methods were implemented using the MULAN multi-label learning tool [51], which is an open-source Java library that supports multi-label learning.
The dataset was prepared in ARFF file format, along with the XML file which are compatible with the MULAN tool. As the pur- pose of this study is to address the lack of available multi-label Arabic datasets, the processed version of the dataset used in the experiments has been made publicly available online.6 It consists of 26,470 text instances distributed over 578 hierarchical multi- labels. It also contains different sets of high ranking features
(1000,.. .,8000) in the ARFF file format, along with the XML file that
defines the hierarchical structure of the labels.


Results and discussion

All the conducted experiments were evaluated according to the four example-based metrics (Hamming loss, H-loss, ML-accuracy, subset accuracy) and three label-based metrics (micro-averaged recall, micro-averaged precision, micro-averaged F-measure) dedi- cated to multi-label learning described in Section 3.5. The evalu- ated methods were ranked based on the seven metrics used, and the average ranks [52] were calculated in order to compare the effectiveness of the methods, whereby the method that obtained the best result was ranked with a 1. Several experiments were per- formed to explore the model’s predictive performance from differ- ent perspectives; thus, they are discussed separately in the following sub-sections.


Feature selection methods

The first experiment investigated the impact of feature selec- tion method on the predictive performance of the model. The stan- dard multi-label feature selection approach described previously in Section 3.3 was applied. Two of the most common PT methods —
LP and BR — were used with four feature ranking methods (v2,
GR, RF, IG); thus, the methods evaluated were: BR-v2 , BR-GR, BR- IG, BR-RF, LP-v2 , LP-GR, LP-IG, and LP-RF.
Each feature selection method was used to select a subset of high-ranking features from the total number of features (11,000). Accordingly, the classification model was evaluated using the 2000 features selected by each feature selection method. Also, the HOMER was implemented using its default multi-label classi- fier (BR-NB) and balanced k-means algorithm with four clusters.
Table 6 presents the results of all evaluated feature selection methods. It also reports the average ranks for each method across all evaluation metrics. As the table shows, the BR-v2 method obtained the best results for Hamming loss, H-loss, ML-accuracy, micro-averaged precision, micro-averaged recall, and micro- averaged F-measure. Meanwhile, BR-GR performed better regard- ing subset accuracy (0.0394). In terms of average ranks, BR-v2 per- formed best followed by BR-IG, (1.29 and 2.29, respectively). This indicates that using a BR method, which transforms MLC problem into several single-label problems to determine the discriminative features for each label independently of other labels, obtains better
results than LP method, which transforms MLC problem into a multi-class classification problem. The following are examples of the  top  ten  high-ranking  features  over  all  labels
(	– mistake,
divorced, blood money, share, heir, homicide, divorce, prayer, per- missible, taraweeh).

6 https://data.mendeley.com/datasets/rxhpvwwmbz/1.
Multi-label classifiers

The second experiment sought to optimize the HOMER algo- rithm by investigating different sets of multi-label classifiers in order to examine their effect on the predictive performance of the model. Three PT methods were employed as multi-label classi- fiers since they are among the most widely-used methods of the PT technique. These were: BR, CC, and LP classifiers. The PT methods were employed with three base classifiers (NB, SVM and J48). Accordingly, HOMER was run with each different multi-label clas- sifier set (BR-NB, BR-J48, BR-SVM, CC-NB, CC-J48, CC-SVM, LP-NB, LP-J48 and LP-SVM).
The experiment was conducted using the BR-v2 feature selec-
tion method since it obtained the best results in the first experi- ment. It was employed to select 2000 high-ranking features. HOMER was implemented using balanced k-means clustering algo- rithm with four clusters. The results of all evaluated multi-label classifiers are presented in Table 7. As the table shows, the best results were obtained when HOMER was run using the LP-SVM multi-label classifier across all evaluation metrics with an average rank of 1, followed by BR-SVM and CC-SVM, which achieved aver- age ranks of 2 and 3, respectively. It was observed that the SVM base classifier used with the PT methods (LP, CC, BR), achieved a better predictive performance than the other base classifiers (NB and J48). This indicates that SVM is a more effective classification algorithm and may improve classification performance [24].

Feature set dimensions

The third experiment was conducted using the feature selection method and the multi-label classifier that yielded the best results in the first two experiments. The HOMER algorithm was run using the LP-SVM multi-label classifier and balanced k-means clustering algorithm with four clusters. This experiment mainly sought to investigate the effect of the dimensions of the feature set on model
performance. The BR-v2 feature selection method was used to
select different sets of high-ranking features involves 1000, 2000,.. ., 8000 features, from the total number of features (11,000). The results of the experiment are presented in Table 8, which
shows that outcomes obtained for feature sets ranging from 1000 to 8000 features were approximately the same, with small differ- ences in favor of the 4000-feature set, which obtained the best results in terms of Hamming loss, H-loss, ML-accuracy and micro-averaged F-measure. In the average ranks, the 4000 and 5000 features obtained the highest value, which was 2.29. Conse- quently, the 4000-feature set was employed with the evaluated models in order to avoid model overfitting and improve predictive performance.

Clustering algorithms

The fourth experiment sought to investigate the effect of three applied clustering algorithms on the model. These were: balanced k-means, k-means, and random clustering, which represent the three HOMER variations (HOMER-B, HOMER-K and HOMER-R), respectively. The model was evaluated using the methods and the feature set dimensions that achieved the best results in the
previous experiments. The BR-v2 feature selection method with
4000 selected features was employed in the feature selection phase, and HOMER was implemented with the LP-SVM multi- label classifier in the classification phase.
Table 9 presents the results of the label distribution performed using the three clustering algorithms with different numbers of clusters k = 2, 4, 6, and 8. It was observed that when the number of clusters increased, model performance improved for all three

Effect of feature selection methods on the predictive performance of the model.




Table 7
Effect of the multi-label classifiers on the predictive performance of the model.




Table 8
Effect of the dimensions of the features set on the predictive performance of the model.





Table 9
Effect of clustering algorithm along with a set of number of clusters k = {2, 4, 6, 8} on the predictive performance of the model.



HOMER variations. Then, the three clustering algorithms were com- pared with a high number of clusters (k = 8 clusters). Label distribu- tion performed using the balanced k-means algorithm obtained the best results for Hamming loss (0.0045) and ML-accuracy (0.7581), random clustering performed best regarding Hamming loss (0.0045), H-loss (1.7149), micro-averaged precision (0.8832), and micro-averaged F-measure (0.8536), and the k-means algorithm obtained the best result for subset accuracy (0.302).
In the average ranks, the balanced k-means algorithm scored the highest (2.57), followed by random clustering and the k- means algorithm with 2.71 and 3.57, respectively. This indicates that performing even label distribution using a balancing cluster- ing algorithm (balanced k-means) is more effective in this domain than performing label distribution using a simple clustering algo- rithm (k-means). As stated in [24], this may be because balanced clustering is more useful in domains with a large number of labels,


Table 10
Compare HMATC model against state-of-the-art (Fatwa, CC) and baseline (BR, LP) models.



and it also helps to avoid class imbalance. Hence, it is possible to conclude that implementing HOMER using a balanced k-means clustering algorithm with a large number of clusters will obtain the best results.

Model comparisons

Finally, the HMATC was compared with the state-of-the-art models (Fatwa, CC) and the baseline models (LP, BR). This experi- ment was conducted using the BR-v2 feature selection method and 4000 selected features. The HMATC model was implemented using the HOMER algorithm with LP-SVM multi-label classifier
and balanced k-means clustering algorithm with eight clusters. In addition, the BR, CC, and LP models were implemented using SVM as a base classifier. Furthermore, the Fatwa model was imple- mented, in accordance with [34], by running the HOMER algorithm using the BR-NB multi-label classifier.
As presented in Table 10, BR obtained the best results in terms of Hamming loss, micro-averaged precision, and micro-averaged F- measure, whereas, CC yielded the best result for ML-accuracy and micro-averaged recall, and LP outperformed the other models in terms of H-loss and subset accuracy.
The average ranks presented in Table 10 show that the BR and CC models performed better than other models (2.14), followed by the HMATC and LP models with 2.85, and finally the Fatwa model with 5. A Friedman test was conducted based on the average ranks for all evaluated models [53]. This test ranked each metric separately over the five evaluated models; thereby, the best model obtained the rank of 1, the second best the rank of 2, and so on. After this, it calculated the test statistic on the average ranks to analyze the significant differences in the models’ performance.
By performing the Friedman test and obtaining the distribution with a k 1 degree of freedom, the p-values were measured at a significance level of 5%. Since the Friedman test gave a significant difference of 5% for all metrics, the null-hypothesis supposing that all the models would be equivalent was rejected. A Wilcoxon signed-rank test [53] at a 5% significance level was performed. The p-value of the comparison between the HMATC model and the Fatwa model = 0.016. Thus, the null-hypothesis supposing that both models would be equally effective was rejected and the alter- native accepted, indicating that the HMATC model outperformed the Fatwa model since it achieved a higher average score. In con- trast, the HMATC model was compared with the CC, BR, and LP models, which obtained the p-values 0.160, 0.160, and 1.000, respectively. Thus, the null hypothesis indicating that both models would be equally effective was also accepted.
Overall, it is possible to conclude that the HMATC, CC, BR, and LP models are equally effective. Although BR, CC, and LP outper- formed the HMATC model with a small difference in terms of some evaluation metrics, they suffered from some drawbacks such as a lack of scalable efficiency with a large number of labels. Due to the large number of labels in the dataset, the models implemented using BR, CC, LP classifiers suffered from class imbalance, high computational cost, and the inefficiency of applications requiring fast response times. For instance, BR needed to train a set of base
classifiers equal to the number of labels, so it employed 578 base classifiers to equal the number of labels in the dataset.
Moreover, CC suffered from random order chain. The results also indicated that the chaining property in CC may cause error propagation at classification time. Furthermore, LP experienced an exponential increase in computational complexity in relation to the number of labels. Additionally, it could only predict labelsets observed in the training data and it also suffered from model overfitting.
In contrast, the HMATC, which was built based on the HOMER algorithm, efficiently dealt with a large number of labels by con- structing a tree-shaped hierarchy of simpler MLC problems, in which the root node was constructed to contain all labels in the tree. Then, the balanced k-means algorithm was employed to distribute large labelsets into balanced clusters that represented new nodes. For each cluster, the LP-SVM multi-label classifier was employed (since it achieved the best results) to handle labels in that cluster only. If the predicted label was in the meta-labels of the child node, only the multi-label classifier of that node was activated.
One of the advantages of the balanced clustering algorithm was that the related labels belonged to the same cluster, which meant that only the multi-label classifier of that node will be invoked and the computational cost was reduced. Also, since each node dealt with fewer training instances, predictive performance was improved.
On the other hand, the Fatwa model was also built based on the HOMER algorithm, but the HOMER algorithm in the Fatwa model was implemented using the BR-NB classifier, which led to lower predictive performance compared to the other evaluated models.

Computational cost

Since computational cost is a vital factor in performance evaluation, it was essential to measure the computational cost consumed by all evaluated models (BR, LP, CC, Fatwa, HMATC). Fig. 4 presents the computational cost of each model (in hours). The figure shows that the evaluated models can be ranked based on the lowest cost achieved in the following order: HMATC, Fatwa, BR, CC, and LP model. It is important to note that all models were


Fig. 4. Computational cost required for each evaluated models.



evaluated using the same experimental setting, as described previ- ously in Section 4.3. Overall, the HMATC model consumed less computational cost (2 h) than the other evaluated models. This may be because the HMATC model was implemented using the HOMER algorithm, which is an effective and computationally effi- cient hierarchical multi-label algorithm that requires a linear train- ing and logarithmic testing complexities with respect to the number of labels.
Furthermore, the effect of each different set of methods was investigated in each phase. Then, the HMATC model was imple- mented using the methods which obtained the best performance. This led to improved model performance and reduced total compu- tational cost.

Conclusion

Hierarchical classification is an important research field and it has been increasingly required by many applications in various domains, including text classification. This paper proposed a model for hierarchical multi-label text classification of the Arabic language. The HMATC model was implemented based on the HOMER algo- rithm, which was optimized by employing LP-SVM classifier and balanced k-means algorithm with eight clusters in order to improve hierarchical classification outcomes. The researchers prepared a hierarchical multi-label Arabic dataset in an appropriate format for hierarchical multi-label classification. This dataset was pub- lished online to address the lack of publicly available multi- labelled Arabic datasets, especially those which are well annotated with hierarchical multi-labels, and also to encourage future researchers to investigate hierarchical multi-label classification methods for Arabic text.
Furthermore, the impact of feature selection method and fea- ture set dimensions on the predictive performance of the model was investigated. The results showed that the HMATC model out- performed all the evaluated models in terms of computational cost. In addition, it was equally effective with the BR, CC, and LP models and performed significantly better than the state-of-the-art model (Fatwa model) over a wide range of evaluation metrics.
The researchers intend to extend this work in the future by investigating the HMATC model with other multi-label classifiers, such as RAkEL, RPC, and PS. They also aim to apply different struc- tured methods for selecting the number of clusters in the cluster- ing algorithm (k). Regarding the feature selection phase, the authors intend to examine a principal component analysis (PCA) method instead of transforming the multi-label problem using a PT method. Finally, they plan to give more attention to the pre- processing phase by employing word embedding techniques like word2vec and FastText as text representation methods, applying different stemming algorithms, and preparing a list of Arabic human names so they can be easily removed during the stop word removal phase.

Acknowledgment

This work was supported by the Deanship of Scientific Research (DSR), King Abdulaziz University, Jeddah, Saudi Arabia, under Grant No. (DG 29-612-1440). The authors, therefore, gratefully acknowledge the DSR technical and financial support.

References

Al-Salemi B, Ayob M, Kendall G, Noah SAM. Multi-label arabic text categorization: a benchmark and baseline comparison of multi-label learning algorithms. Inf Process Manage 2019;56(1):212–27.
Al-Salemi B, Noah SAM, Ab Aziz MJ. Rfboost: an improved multi-label boosting algorithm and its application to text categorisation. Knowl-Based Syst 2016;103:104–17.
Gibaja E, Ventura S. A tutorial on multi tutorial on multilabel learningilabel learning, ACM Comput Surv 47(3):2015; 52:1–52:38. [Online]. Available: http://doi.acm.org/10.1145/2716262..
Tsoumakas G, Katakis I. Multi-label classification: an overview. Int J Data Warehousing Min (IJDWM) 2007;3(3):1–13.
Taha AY, Tiun S. Binary relevance (br) method classifier of multi-label classification for arabic text. J Theor Appl Inf Technol 84(3):2016..
Duwairi RM, Al-Zubaidi R. A hierarchical k-NN classifier for textual data. Int Arab J Inf Technol 2011;8(3):251–9.
Brucker F, Benites F, Sapozhnikova E. Multi-label classification and extracting predicted class hierarchies. Pattern Recogn 2011;44(3):724–38.
Silla CN, Freitas AA. A survey of hierarchical classification across different application domains. Data Min Knowl Discovery 2011;22(1–2):31–72.
Mubarak H, Darwish K. Using twitter to collect a multi-dialectal corpus of arabic. In: Proceedings of the EMNLP 2014 workshop on arabic natural language processing (ANLP). p. 1–7.
Eldos T. Arabic text data mining: a root-based hierarchical indexing model. Int J Model Simul 2003;23(3):158–66.
Ahmed Y, Xiang J, Zhao D, Al-qaness MAA, Elsayed abd el aziz M, Abdelghani D. A study of the effects of stemming strategies on arabic document classification. IEEE Access PP:2019;1–1..
Zhang M-L, Zhou Z-H. Ml-knn: a lazy learning approach to multi-label learning. Pattern Recogn 2007;40(7):2038–48.
Clare A, King RD. Knowledge discovery in multi-label phenotype data. In: European conference on principles of data mining and knowledge discovery. Springer; 2001. p. 42–53..
Boutell MR, Luo J, Shen X, Brown CM. Learning multi-label scene classification. Pattern Recogn 2004;37(9):1757–71.
Read J, Pfahringer B, Holmes G, Frank E. Classifier chains for multi-label classification. Mach Learn 2011;85(3):333.
Hüllermeier E, Fürnkranz J, Cheng W, Brinker K. Label ranking by learning pairwise preferences. Artif Intell 2008;172(16–17):1897–916.
Tsoumakas G, Vlahavas I. Random k-labelsets: an ensemble method for multilabel classification. In: European conference on machine learning. Springer; 2007. p. 406–417..
Read J, Pfahringer B, Holmes G. Multi-label classification using ensembles of pruned sets. In: Data mining, 2008. ICDM’08. Eighth IEEE international conference on. IEEE; 2008. p. 995–1000..
Tsoumakas G, Katakis I, Vlahavas I. Random k-labelsets for multilabel classification. IEEE Trans Knowl Data Eng 2011;23(7):1079–89.
Vens C, Struyf J, Schietgat L, Dzˇeroski S, Blockeel H. Decision trees for hierarchical multi-label classification. Mach Learn 2008;73(2):185.
Cesa-Bianchi N, Gentile C, Zaniboni L. Incremental algorithms for hierarchical classification. J Mach Learn Res 7(Jan);2006:31–54..
Chen Y, Crawford MM, Ghosh J. Integrating support vector machines in a hierarchical output space decomposition framework. In: Geoscience and remote sensing symposium, 2004. IGARSS’04. Proceedings. 2004 IEEE International, vol. 2. IEEE; 2004. p. 949–952..
Zhang L, Shah S, Kakadiaris I. Hierarchical multi-label classification using fully associative ensemble learning. Pattern Recogn 2017;70:89–103.
Tsoumakas G, Katakis I, Vlahavas I. Effective and efficient multilabel classification in domains with large number of labels. In: Proc. ECML/PKDD 2008 workshop on mining multidimensional data (MMD’08), vol. 21. sn, 2008.
pp. 53–59..
Yahya A, Salhi A. Arabic text categorization based on arabic wikipedia. ACM Trans Asian Lang Inf Process (TALIP) 2014;13(1):4.
Ahmed NA, Shehab MA, Al-Ayyoub M, Hmeidi I. Scalable multi-label arabic text classification. In: Information and communication systems (ICICS), 2015 6th international conference on. IEEE; 2015. p. 212–217..
Shehab MA, Badarneh O, Al-Ayyoub M, Jararweh Y. A supervised approach for multi-label classification of arabic news articles. In: Computer science and information technology (CSIT), 2016 7th international conference on. IEEE; 2016. p. 1–6..
Hmeidi I, Al-Ayyoub M, Mahyoub NA, Shehab MA. A lexicon based approach for classifying arabic multi-labeled text. Int J Web Inf Syst 2016;12(4):504–32.
Fürnkranz J, Hüllermeier E, Mencía EL, Brinker K. Multilabel classification via calibrated label ranking. Mach Learn 2008;73(2):133–53.
Schapire RE, Singer Y. Improved boosting algorithms using confidence-rated predictions. Mach Learn 37(3);1999:297–336. [Online]. Available: doi: 10.1023/A:1007614523901..
Spyromitros E, Tsoumakas G, Vlahavas I. An empirical study of lazy multilabel classification algorithms. In: Darzentas, J., Vouros, G.A., Vosinakis, S., Arnellos,
A. (Eds.), Artificial intelligence: theories, models and applications, Berlin, Heidelberg: Springer, Berlin Heidelberg; 2008. p. 401–406..
Cheng, W, Hüllermeier, E. Combining instance-based learning and logistic regression for multilabel classification. In: Buntine, W., Grobelnik, M., Mladenic´, D., Shawe-Taylor, J. (Eds.), Machine learning and knowledge discovery in databases. Berlin, Heidelberg: Springer, Berlin Heidelberg; 2009. p. 6–6..
Elnagar A, Al-Debsi R, Einea O. Arabic text classification using deep learning models. Inf Process Manage 2020;57(1). 102121.
Zayed, RA, Hady MFA, Hefny H. Islamic fatwa request routing via hierarchical multi-label arabic text categorization. In: Arabic computational linguistics (ACLing), 2015 first international conference on. IEEE; 2015. p. 145–151..
Ababneh J, Almomani O, Hadi W, El-Omari NKT, Al-Ibrahim A. Vector space models to classify arabic text. Int J Comput Trends Technol (IJCTT) 2014;7 (4):219–23.



Mustafa SH. Word stemming for arabic information retrieval: the case for simple light stemming. Abhath Al-Yarmouk Sci Eng Ser 2012;21(1):2012.
Froud H, Lachkar A, Ouatik SA. A comparative study of root-based and stem- based approaches for measuring the similarity between arabic words for arabic text mining applications, arXiv preprint arXiv:1212.3634, 2012..
Habib MB. An intelligent system for automated arabic text categorization, Master’s thesis, University of Twente; 2008..
Joachims T. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. Carnegie-mellon univ pittsburgh pa dept of computer science, Tech. Rep.; 1996..
Karisani P, Rahgozar M, Oroumchian F. A query term re-weighting approach using document similarity. Inf Process Manage 2016;52(3):478–89.
Wu H, Gu X, Gu Y. Balancing between over-weighting and under-weighting in supervised term weighting. Inf Process Manage 2017;53(2):547–57.
Uysal AK, Gunal S. The impact of preprocessing on text classification. Inf Process Manage 2014;50(1):104–12.
Syiam M, Fayed ZT, Habib MB. An intelligent system for arabic text categorization. Int J Intell Comput Inf Sci 2006;6(1):1–19.
Ayedh A, Tan G. Building and benchmarking novel arabic stemmer for document classification. J Comput Theor Nanosci 2016;13(3):1527–35.
Larkey LS, Ballesteros L, Connell ME. Improving stemming for arabic information retrieval: light stemming and co-occurrence analysis. In: Proceedings of the 25th annual international ACM SIGIR conference on research and development in information retrieval. p. 275–82.
Chen Y-T, Chen MC. Using chi-square statistics to measure similarities for text categorization. Expert Syst Appl 2011;38(4):3085–90.
Lee C, Lee GG. Information gain and divergence-based feature selection for machine learning-based text categorization. Inf Process Manage 2006;42 (1):155–65.
SpolaôR N, Cherman EA, Monard MC, Lee HD. A comparison of multi-label feature selection methods using the problem transformation approach. Electron Notes Theor Comput Sci 2013;292:135–51.
Tsoumakas G, Katakis I, Vlahavas I. Mining multi-label data. In: Data mining and knowledge discovery handbook. Springer; 2009. p. 667–685..
Zhu S, Ji X, Xu W, Gong Y. Multi-labelled classification using maximum entropy method. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM; 2005. p. 274–281.
Tsoumakas G, Spyromitros-Xioufis E, Vilcek J, Vlahavas I. Mulan: a java library for multi-label learning. J Mach Learn Res 2011;12:2411–4.
Brazdil PB, Soares C. A comparison of ranking methods for classification algorithm selection. In: European conference on machine learning. Springer; 2000. p. 63–75..
Demšar J. Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 7(Jan);2006:1–30.

Nawal Aljedani received the B.S. and M.S. degrees in information technology from King Abdulaziz University, Jeddah, Saudi Arabia. She is currently a Collaborative Teaching Assistant with the Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia. Her research interests include machine learning, data mining, natural language processing, and multi-label clas- sification.

Reem Alotaibi received the Ph.D. degree in computer science from the University of Bristol, Bristol, U.K., in 2017. She is currently an Assistant Professor with the Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia. Her research interests include machine learning, data mining, and multi-label classification.

Mounira Taileb received the Ph.D. degree in computer science from the University of Paris-Sud, Paris, France, in 2008. She is currently an Associate Professor at the Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia. Her research interests include image retrieval, image anno- tation, machine learning, data mining, and text classification.
