Available online at www.sciencedirect.com



Electronic Notes in Theoretical Computer Science 281 (2011) 143–157
www.elsevier.com/locate/entcs

Robust Range Finder Through a Laser Pointer and a Webcam
Christian E. Portugal-Zambrano 1
Sociedad Peruana de Computaci´on C´atedra Concytec en Tecnolog´ıas de la Informaci´on
Universidad Nacional de San Agust´ın – Arequipa, Peru´
Jesu´s P. Mena-Chalco 2
Instituto de Matema´tica e Estat´ıstica Universidade de S˜ao Paulo – S˜ao Paulo, Brasil

Abstract
Currently, there is a great interest in the study of three-dimensional reconstruction from digital images. The applications of photogrammetric algorithms has allowed us significant improvements in procedures of camera calibration, movement of objects in scenes, shape from shading and range images. The procedure of the distance estimation from simple images is important to find the depth measurements in any procedure of three-dimensional reconstruction of scenes. In this work, we describe an adaptation of a scanner prototype based on a laser pointer and a webcam. It was applied to the robust estimation of absolute distance on images obtained from real time video sequences. Experimental tests were performed in order to demonstrate the effectiveness of the distance calculation in real time through a geometric model and a simple system of linear regression. From a wide data set of tests with different scanning parameter, good results on range finding were obtained.
Keywords: photogrammetry, camera calibration, computer vision, range finder

Introduction
Vision is one of the main senses used by humans to move around the world. The amount of information received through the eyes is incomparable with other senses. The goal of the artificial vision, in the field of computer vision, is to construct a computational prototype capable of understand a digital image [28]. For descrip- tions of scenes, computer vision provides techniques such as pattern recognition, statistical learning, projective geometry, image processing. One technique which is becoming important in the computer vision field is the image range finding,

1 Email:christ.pz.cs@gmail.com
2 Email:jmena@vision.ime.usp.br

1571-0661 © 2011 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2011.11.031

which is defined as the distance between the object into the scene and the image captured by the sensor [8,10,11,13]. This kind of technique allows us obtain three- dimensional information of scenes such as background, shape and depth measures of objects. Many applications has been created using depth information, e.g. colli- sion detection on mobile robots [2,10] and engineering applications with range optic sensors. Several applications works with applications of quality control and storage planning. The latter is very important for companies which need to obtain three- dimensional measurements of objects for manipulation [8]. This work is focused in the robust estimation of the absolute distance [6,7,8,21,24], where are considered low-cost components such as a webcam and a laser pointer, both used to estimate distances obtained from the real world.
To make the design of the scanner, both the webcam and the laser pointer were setup on parallel position to simplify projection calculation. The main advantage of this configuration (hereby named scanner), is the use of low-cost components and the low computational cost. Additionally, a camera calibration technique was carried out and implemented as improvement to perform a correction of distance calculation. As a complementary part, the robust range finder was evaluated on real time. This work has been motivated by the need to provide a manual technique that meets the requirements of researchers in computer vision. In this way, it is possible to build a three-dimensional imaging system which satisfies the requirements of their applications. Currently, several applications which are emerging are based on efficiency and low computational cost [14].
This work is organized as follows: In Section 2 previous works related to distance estimation are briefly described. In Section 3 is shown the process used to camera calibration. The scanner and the geometry of the model are detailed in Section 4. Finally, in Section 5 the implementation and analysis are presented. Conclusions and possible future directions of the results are described in Section 6.

Previous works
In the literature has been published several works related to range finding with devices using a webcam and a laser pointer. Follow we briefly describe the more representative previous works:
Close-range camera calibration (1971) [4]. This work establishes the importance of photogrammetry close-range finding to obtain object measures, this led to more refined techniques of photogrammetry which were applied to obtain measurements of parabolic antennas.
3D range acquisition through differential light absorption (2002) [18]. This work describes a 3-D camera based on the differential absorption of light by a colored liquid that was illuminated by a circular light source to obtain 3-D information. The illumination is a function of range in addition to other parameters such as the orientation of the surface, the position of the light sources, the spectral characteristics of the liquid and the position of the observer.

A low cost dynamic range-ﬁnding device based on amplitude-modulated continu- ous ultrasonic wave (2002) [13]. This work describes a practical low-cost acous- tic range-finding method, which provides a high speed and high-accuracy. The method is based on the linear phase delay of the received signal, with respect to the transmitted signal caused by the distance between the transmitter and the receiver.
A 120x110 position sensor with the capability of sensitive and selective light de- tection in wide dynamic range for robust active range ﬁnding (2004) [22]. In this work, a 120x110 sensitive and light selective position sensor array with dynamic range is presented. The sensor can detect the position of a low-intensity light projected on non-uniform background illumination for a robust range finding sys- tem. At this work the authors have been achieved 3-D reconstruction of objects with scanners projections.
A projective method to the measurement of box dimensions in real time (2006) [8]. This work describes a method to the computation of box dimensions in real time, focusing on projective geometry which uses the information of the box edges and the projection of two dot lasers on one side of the box, leading to recover the box dimensions in real time. The effectiveness of this work was validated through a scanner prototype implemented by the authors.
Extrinsic self calibration of a camera and a 3D laser range ﬁnder from natural scenes (2007) [27]. This work describe a new approach for the extrinsic calibration of a camera with a 3D laser range finder, that can be done on the fly, this approach does not require any calibration object, only few point correspondences are used, which are manually selected by the user from a scene viewed by the two sensors. The extrinsic calibration is done using a nonlinear refinement process.
Range ﬁnder with a laser pointer and a camera (2007) [21]. This work describes a laser-scanner system, composed by a camera and a laser pointer. The scanner project a horizontal line which track the object and determines if a object is near or far to the scanner. The general idea is based on the estimation of distances to the object focusing in the difference of the location of the beam projected in the scene. This work described the fact in which, to nearest distances of the beam its location tends to the bottom of the image and a farthest distances tends to center of the image.
Webcam telemeter (2007) [6]. T. Danko explains the importance of the design of low-weight devices, which can be used in applications where the weight is an important issue. This work describes how to configure a low-precision laser with a low-cost webcam in order to construct a vision machine with range information.
There are several aspects to consider when we look results among different tech- niques. The precision is the most important aspect, in that sense, we adopted the procedure of camera calibration proposed by Abdel-Azis and Karara [1] who were the first in developing the Direct Linear Transformation (DLT). Karara, in 1979, improved the method that take into account the lens distortions. The simplicity of the model and the robust results obtained have led to extend the application of this

technique in the scientific community.
In our work, we used the advances of [8], modifying the procedure of distance estimation based on the idea proposed by [6,21]. Looking both proposals, with a previous camera calibration procedure, we form a model of range finding which provides 3-D information (depth information) in real time 3 .
Camera calibration and correction of images
The procedure of Camera Calibration is a photogrammetric component mainly used in range finding with high precision [25]. In order to obtain robust 3-D metric information from images, as a prelude to a three-dimensional reconstruction and shape recovering [23], is necessary to perform a procedure of orientation and an accurate camera calibration [3]. For our purposes, the concept of camera calibration is expressed as the direct relationship between the image’s coordinates and object’s coordinates relative to the center of the camera, however the position of the camera in the space is not necessarily known.
Much effort was performed in the procedure of camera calibration. Some authors proposed the use of genetic algorithms [16,26] and the pre- and post-processing of data. From a practical point of view, some authors suggest a method based on 2-D patterns [5] as a model simply to use which obtains good results.
The camera orientation includes the calculation of external parameters to define its state and its axis in a coordinates system of high order, generally called World Coordinate System. We use three translational parameters and three rotational parameters for the camera 4 . See in [24] a description of the realized procedure in the camera calibration. In the implementation of our approach has been used OpenCV [9] and the obtained results from the camera calibration were validated with the MatLab Camera Calibration Toolbox [3].
Distance calculation
This section details the scanner used to calculate the distance to an object of plane surface. Figure 1 shows the arrangement of the webcam and laser pointer. This configuration allow us to maintain in parallel position both of components, thereby facilitating a quick calculation of real distances from a simple model.
Geometry of the model
In Figure 2 the scheme used for the calculation of distance is showed. The distance between the webcam and laser pointer is defined by H (in centimeters). The distance between the scanner and the object is defined by D (in centimeters).
Also note that the representation of the scene in the image formed by the webcam lens is the size A′ B′ , where AB represents the same scene in real size. On the

3 A previous version of this work was presented in the II Concurso de Trabajos de Tesis de pregrado en Computacion Grafica y Procesamiento de Im´agenes [24].
4 In our system we are considering the use of a single household webcam.


	

Fig. 1. Model of scanner used in our work (based on the model proposed by [6]). See in [24] a implementation and validation of the proposed model [6].


SURFACE
Fig. 2. Scheme of the arrangement of the camera-laser. The distance D is obtained from the projection of the laser dot onto the surface of the object. (figure adapted from [6]).


other hand, the distance between the center of the image to the location of the beam projected in the images can be defined as pfc (pixels from center), where θ represents the angle of vision (in radians) formed by projection of the laser beam projected onto the image.

With this scheme we can establish the following relation:


H
D =
tan θ
(1)

We consider that the relationship can be raised to obtain the total of pixels per degree of incoming light, here called rpc (radian per pixel pitch), thus converting each pixel in the image to its corresponding value in centimeters. The following equation shows rpc in relation with θ:
θ = pfc ∗ rpc	(2)
Due to the existence of lens deformations on the webcam, in Equation 2 we consider a parameter which allows us the correction of alignment, here called ro (radian offset ):
θ = pfc ∗ rpc + ro	(3)
To obtain the values of rpc and ro we use a model of linear regression to find the relation between a dependent variable Y with k explanatory variables xk, which generate a hyperplane of parameters β. Usually this linear model is defined as:
Y = Σ βkXk + ε	(4)
where ε is a random variable that deals with those uncontrollable factors. The model satisfies the following general equation:
Y = mX + b	(5)

with
n Σ xy − Σ x Σ y
m =  n Σ x2 − (Σ x)2 , b =
Σ y − m Σ x, n

where n, is the total of samples used; m, is the slope of the regression line (rpc); b, is the value originated by uncontrollable factors (ro) and Y represents θ.
In order to obtain the distance to the object, we introduce the Equation 3 into the Equation 1, each of one in centimeters:
H
D = tan(pfc ∗ rpc + ro)	(6)
We called this formulation as the Equation of distance calculation. In Figure 3 we present a summary of the data flow diagram of the system. Note that, in order to estimate distances, it is necessary to calibrate the scanner only once. Note also that, in the camera calibration process, the values of rpc and ro are obtained.
Localization of the laser beam into the image
We define laser beam as the light project by the laser pointer onto the object and captured by a webcam. For our work, we use a laser pointet that emits red light

S

corners


Image sequences on real time (frame
Distance Camera – Laser (H) a
set of Real Distances
Scanner – Object (








New images sequences on real time
(frames)
Distance calculation

Parameters Calibrated of the Equation of Distance Calculation


pfc value for each image





Distance Scanner - Object



Fig. 3. Data flow diagram of the system. Each box represents a module and the arrows represents the data flow between each modules.

and its localization onto the images is considered as a form of introduce the depth information into the image. The process of localization is influenced by (i) the presence of illumination into the environment; (ii) the texture of the object pointed by the laser beam, and (iii) the size of the laser beam projected onto the image (the size of the projected beam depends of the proximity between the scanner and the object).
Previous works, as see in [8], have used two laser pointers Class II with a wave- length of 650nm to make a projection. In [21], J. Y. Montiel et al., used a laser pointer of Class II of 633-670nm in order to calculate distance to objects applying a line tracking. For our work we used a modification of a laser pointer Class II with red light color and 630-650nm of wavelength.
In [6,24] the localization of the projected laser beam was realized with the red color of the RGB scale. This procedure is influenced by the presence of objects with the same color as well as the presence of high incidence of light. An efficient solution was proposed in [8], where the author used the luminance information present into the image considering a Y’CbCr scale of colors. In [24] was observed that the laser beam projected in the image shows a vertical displacement in relation to the scanner-object distance. To a farthest distances the localization of laser beam tends to be in the middle of the image and a nearest distances it tends to be in bottom side of the image. Given this characteristic, the localization of laser beam into the image (Y’CbCr representation) can be realized just for a region determined



(a)
(b)
Fig. 4. GUI used in the calculation of absolute distances: (a) Dialog windows for the input of data, (b) Data set used in the equation of distance estimation (Calculation of rpc and ro).
into the image which is located at middle bottom side of the image, thus avoiding unnecessary searches of laser beam into the totally whole image.
Experimental results
The design of the system was implemented using Eclipse Galileo as IDE, Mingw as compiler on Windows Systems and GCC4 for Linux Systems, Qt4[20] for the design of the GUI and the OpenCV 2.1 libraries [9] for the image and video processing. The tests were performed in a laptop: AMD Turionx2x64, 2,4Ghz, and 4Gb of RAM. The webcam has a image resolution between 1.3 and 5.2 Mega pixels with a frame rate of 30fps, and a focal range of 30mm. See in Figure 4 two screenshots of GUI created.

The laser beam projected onto the image is used to obtain the distance measure. The model of the system has been tested considering alternatively a calibrated and uncalibrated camera.
In the experiments were used different measures of H (distance between webcam and laser pointer). Different image resolutions and different data samples were used in the regression model. In order to obtain the data samples, used in the regression model, we consider the parameter S that defines the difference between each sample obtained from real world (step).
Table 1 shows the parameters used in the experiments. All samples used in the regression model were obtained considering S = 5cm.
For each configuration used in Table 1 has been calculated the difference between the real distance and the distance obtained by the scanner, considering every sample of the dataset (mean absolute error) then for every equation the average error was found.

Table 1
Dataset used in the experiments. Example: In the set 7 was used a distance H of 25cm with a resolution of 352x288 using a calibrated camera, considering 25 samples, linearly separated by a step of 5cm. The samples was collected between 73cm to 193cm.

Table 1 shows that when the distance measurement is performed with a cali- brated camera, the approximation error is less than the obtained when an uncali- brated camera is used. This is due to the correction of the image throughout the internal parameter obtained from the camera calibration. Thus, the values of pfc, correctly located, allow us a more robust distance estimation.
It is important to note that the evaluation of the robustness in the calculation of distances is related to the amount of samples used for the regression model. The amount of samples is influenced by the parameter S (to lower values of S greater amount of data and vice-versa). In Table 2 the tests were performed with different values of S.
An analysis of the samples in Table 2 shows that values of proximity errors are greater than the values obtained in the Table 1. This shows empirically that, to get a robust model of distance calculation we must use small values of S. In the





400	400



350	350



300	300



250	250




200	200



150	150



100	100



50
0	10	20	30	40	50	60
Sample

50
0	10	20	30	40	50	60
Sample

S =5	S = 10

400	400



350	350



300	300



250	250




200	200



150	150



100	100



50
0	10	20	30	40	50	60
Sample

50
0	10	20	30	40	50	60
Sample

S = 20	S = 40

400



350



300



250




200



150



100




50
0	10	20	30	40	50	60
Sample
S = 50
Fig. 5. Comparison chart between the real distances and the distances obtained from the samples for each configuration describes in Table 2.

Table 2
Set of variables samples of regression obtained with different values of S. It was used a calibrated camera and a video resolution of 320x240 pixels.

Figure 5 are presented five comparative graphics between the real distance and the distance obtained for each of values S indicated in Table 2.
Note that while the scanner-object distance increase, the error of distance also

makes it, this is due to the laser beam projected is located in the center of the image and its size is lower, these leads to a process of distance calculation less accurate.

Fig. 6. Scheme of a spiral and its relation to the object (target), used in evaluating distance estimation. Each mark (red dot) into the spiral represents a real distance to the object.

The robustness of the proposed system was evaluated considering distances cal- culations following a movement defined by a spiral. This was achieved making a model and defining random real values of distances along the model. Figure 6 shows a scheme of the design of the spiral and it relation with the object. For the calibra- tion of the scanner has been used the parameters described in row No.1 showed in the Table 2.


Fig. 7. Comparison between the data of real distance and its corresponding distance obtained by the scanner with a calibrated camera (CC) and uncalibrated camera (UC).

We allocated the scanner in every position with a red dot in the spiral, then the distance calculated by the scanner for every position was calculated.
This set of distances calculated in every position of the spiral was compared with their corresponding real values. The Figure 7 shows the distances calculations, confronted with its real values. The mean average error (MAE) of approximation

was 0.5392cm and the MAPE was 0.82% with a uncalibrated camera, and its MAE was 0.4295cm and the MAPE was 0.53% with a calibrated camera.

Fig. 8. Scheme of a scene where the scanner locates on a fixed axis and calculates different distances to several objects.

Another way to proof the robustness of the system was performing a scanning of a scene as described in Figure 8, where it were selected a series of random distances to objects from a fixed axis. Each line represents a distance calculation to an object and for each of them was obtained a distance calculated for the scanner. Note that, the distance calculation is performed for a position determined in the object without consider the position of this object to the scanner. The comparative results between the real distances and the distances obtained by the scanner with a calibrated camera and an uncalibrated camera are shown in Figure 9.
From this last experiment we can note the high level of approximation of the distances obtained by the scanner. The mean absolute error (MAE) of approxi- mation was 0.8613cm and the MAPE was 0.824% for an uncalibrated camera and present a MAE of 0.6492cm and 0.557% of MAPE for a calibrated camera.

Restrictions
If still the results are promising and the time of calculation is almost instant (mil- liseconds) the system present some restrictions:
The localization of laser beam in the image generates a minimum distance (laser beam located at bottom at the image) and a maximum distance (center of the image) this is proportional to H (Camera-Laser distance) therefore to modify the range of distance to reach we must modify the distance H in the scanner. Note that, the actual arrangement of the scanner is designed to resolve this restriction.




Fig. 9. Comparison between the real distance and the distance obtained by the scanner considering the scheme shown in Figure 8.
The system has been tested with objects of plane surface, smooth and opaques. We believe that there exist a variation in the accuracy of calculation when being considered non planar rough surfaces, due to the fact that the projection of the laser beam onto these surfaces is affected.
Conclusions and future directions
The effectiveness and functionality of the scanner was proved through the implemen- tation of the mathematical model and the software necessary to achieve this, in contrast to our previous work conducted in [24]. In this work we improved the design of the scanner and we tested the scanner with different configuration of parameters. It was proved that we can adapt the configuration of the scanner to coverage larger distances.
The localization of the laser beam into the images is a important step to the process of distance calculation. In this work was used a Y’CbCr color model, in contrast to the RGB color model used in [24]. The experiments results suggest that this color model enable us a more accurate localization of the laser beam in an uncontrolled environments. We believe that the use of a webcam with infrared will enable us to obtain better results in the localization of the laser beam process.
While does not have a direct relation with the process of distance calculation, this work has proved that the correction alignment applied to an image, based on the pa- rameters obtained from the camera calibration, allows us a better distance calcula- tion, decreasing the error present in the obtained measures. The camera calibration process was implemented based on homographies, obtaining linear relationships. In future works, a better study of calibration techniques will be performed [16,25,26]. Additionally, we could consider the works [17,19] with the purpose of building a

model of self-calibrated scanner applied to this work. It is worth to note that the combined use of the camera calibration procedure and the distance calculation allow us to obtain necessary and effective information for a future work of 3-D superficial reconstruction of objects [8,12,15,23]. In particular, we can configure the scanner using a second laser pointer, while both are separated a distance, and a second webcam that enabled us 3-D reconstructions of scenes containing real information of depth and height, e.g. 3-D scenes with real information about dimensions of the objects present in the scene. Finally, it can be noted that this methodology can be used in mobile robotic applications and in the determination of real measures of objects contained on digital images.
Acknowledgement
Research partially funded by C´atedra Concytec en TICs of the National University of San Agust´ın - Peru and the CNPq - Brazil.

References
Y.I. Abdel-Aziz and H.M. Karara. Direct linear transformation from comparator coordinates into object space coordinates in close-range photogrammetry. Proceedings of the Symposium on Close- Range Photogrammetry, 1:1–18, 1971.
H. Baltzakis, A. A. Argyros, and P. Trahanias. Fusion of laser and visual data for robot motion planning and collision avoidance. Machine Vision and Applications, 15(2):92–100, 2003.
J. Y. Bouguet. Matlab camera calibration toolbox. In Technical Report, 2000.
D.C. Brown. Close-range camera calibration. Photogrammetric Engineering, 37(8):855–866, 1971.
P. Campos D. Aracena and C. Tozzi. Comparaci´on de t´ecnicas de calibraci´on de c´amaras digitales.
Universidad de Tarapac´a, 13(1):57–68, 2005.
T. Danko. Webcam based DIY laser rangefinder. http://sites.google.com/site/todddanko/home/ webcam_laser_ranger, 2007.
A. Derhgawen. Obstacle detector using webcam and laser pointer. http://ashishrd.blogspot.com/ 2006/11/obstacle-detector-using-webcam-and.html, 2011.
L. A. Frata Fernandes. Um m´etodo projetivo para c´alculo de dimens˜oes de caixas em tempo real. Master’s thesis, Universidade Federal Do Rio Grande Do Soul, 2006.
B. Gary and K. Adrian. Learning OpenCV: Computer Vision with the OpenCV Library. O’Reilly, first edition, 2008.
N. Gonzales. Sistema de visi´on por computadora para la medici´on de distancia e inclinaci´on de obst´aculos para robots m´oviles. Pontificia Universidad Javeriana: Facultad deIngenieria, 9(2):125– 134, 2005.
Yann Goyat, Thierry Chateau, and Laurent Trassoudaine. Tracking of vehicle trajectory by combining a camera and a laser rangefinder. Mach. Vis. Appl, 21(3):275–286, 2010.
R.I. Hartley and A. Zisserman. Multiple View Geometry in computer Vision. Cambridge University Press, second edition, 2004.
H. Hua, Y. Wang, and D. Yan. A low-cost dynamic range-finding device based on amplitude-modulated continuous ultrasonic wave. Instrumentation and Measurement, IEEE Transactions on, 51(2):362–367, 2002.
K.F. Hulme, E.M. Kasprzak, K.E. Lewis, D. Moore-Russo, P. Singla, and D.P. Fuglewicz. Game-based experiential learning in dynamics education using motion simulation. In The Interservice/Industry Training, Simulation & Education Conference (I/ITSEC), volume 2010. NTSA, 2010.


B. Jahne, H. HauBecker, and P. Geibler. HandBook of Computer Vision and Applications, volume third. Academic Press, first edition, 1999.
Q. A. Ji and Y. M. Zhang. Camera calibration with genetic algorithms. IEEE Trans. Systems, Man and Cybernetics, 31(2):120–130, 2001.
M. Kurisu, H. Muroi, and Y. Yokokohji. Calibration of laser range finder with a genetic algorithm. In Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on, pages 346–351. IEEE, 2007.
D. Laurendeau, R. Houde, M. Samson, and D. Poussart. 3d range acquisition through differential light absorption. Instrumentation and Measurement, IEEE Transactions on, 41(5):622–628, 2002.
D. D. Lichti. Self-calibration of a 3D range camera. In ISPRS Congress, page B5: 927 ff, 2008.
D. Molkentin. The Book of Qt 4. No Starch Press, first edition, 2007.
J.Y. Montiel, J.L. L´opez, and R. Hern´andez. Estimacio´n de distancias con un l´aser de linea y una c´amara. Cient´ıfica, 11(3):129–134, 2007.
Y. Oike, M. Ikeda, and K. Asada. A 120x110 position sensor with the capability of sensitive and selective light detection in wide dynamic range for robust active range finding. Solid-State Circuits, IEEE Journal of, 39(1):246–251, 2004.
N. Paragios, Y. Chen, and O. Faugeras. Handbook Of Mathematical Models In Computer Vision. Springer-Verlag, first edition, 2005.
C. Portugal-Zambrano and J. P. Mena-Chalco. Estimaci´on de distancias absolutas utilizando un puntero l´aser y una c´amara web. In II Concurso de trabajos de Pregrado en CGI - III Simposio Peruano de Computaci´on Gr´afica y Procesamiento de Imagenes, SCGI-2009, Dec. 28–29 2009.
F. Remondino and C.S. Fraser. Digital camera calibration methods: Considerations and comparisons. In IAPRS, editor, ISPRS Commission V Symposium Image Engineering and Vision Metrology, volume 36, pages 266–272, 2006.
G. G. SAVII. Camera calibration using compound genetic-simplex algorithm. Journal of optoelectronics and advanced materials, 6(4):1255–1261, 2004.
D. Scaramuzza, A. Harati, and R. Siegwart. Extrinsic self calibration of a camera and a 3d laser range finder from natural scenes. In Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on, pages 4164–4169. IEEE, 2007.
L. G. Shapiro and G. C. Stockman. Computer Vision. Prentice Hall, 2001.
