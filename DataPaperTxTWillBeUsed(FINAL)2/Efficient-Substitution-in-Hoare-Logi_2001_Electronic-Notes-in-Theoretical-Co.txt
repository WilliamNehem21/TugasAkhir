Electronic Notes in Theoretical Computer Science 41 No. 3 (2000)
URL: http://www.elsevier.nl/locate/entcs/volume41.html pages 35–49


Efficient Substitution in Hoare Logic Expressions

Andrew W. Appel	Kedar N. Swadi Roberto Virga
Department of Computer Science Princeton University Princeton, NJ, U.S.A.
Email: {appel,kswadi,rvirga}@cs.princeton.edu



Abstract
Substitution plays an important role in Hoare Logic, as it is used in interpreting assignments. When writing a computer-based realization of Hoare Logic, it is there- fore important to choose a good implementation for it. In this paper we compare different definitions and implementations of substitution in a logical framework, in an effort to maximize efficiency.
We start by defining substitution as a logical formula. In a conventional ap- proach, this is done by specifying the syntactic changes substitution performs on expressions. We choose instead a semantic definition that describes the behavioral relation between the original expression and its substituted counterpart.
Next, we use this semantic definition as an abstract specification, and compare two of its concrete implementations. The first we consider is the usual one, that operates recursively over the structure of the term. This requires a number of inference steps proportional to the size of the expression, which is unacceptable for many practical applications. We therefore propose a different method, that makes better use of the primitives provided by the logical framework, and manages to reduce the complexity to a quantity proportional to the number of free variables.
We conclude the paper outlining a refinement technique that, by taking advantage of some simple program analysis information, holds promise of improving the results presented even further.




1 Thanks to everyone who should be thanked
◯c 2000 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.

Introduction
Hoare Logic [5] is a well-established formalism for proving properties of pro- grams. Expressions in Hoare Logic are triples
P {S} Q,
where P and Q are logical formulas, and S is a sequence of program in- structions. The meaning of this statement is that for all states for which the precondition P is true, executing the sequence S leads us (when it terminates) to a state satisfying the postcondition Q.
In real-life programs, the length and complexity of the sequences of in- structions S we need to consider makes use of Hoare Logic by hand infeasible. Hence, we often make use of Logical Frameworks [9], such as HOL [4] or Isabelle [8]. These software packages offer us automated or semiautomated proving facilities, so that we can concentrate on the difficult theorems while the trivial ones are automatically carried out by the system. Logical frame- works typically provide a meta-logic to encode mathematical formalisms (in our specific case, Hoare Logic) as object-logics.
The rule in Hoare Logic that deals with the assignment of a value v to a variable i can be formulated as follows:
[v/i]Q {i := v} Q
where the notation [v/i]Q represents the substitution of all the (free) occur- rences of i by v in Q. Here, the semantic change of state is reflected syntac- tically by the notion of substitution. When implementing Hoare Logic in a logical framework, we will therefore be interested in implementing substitution efficiently.
Most frameworks offer built-in primitives for substitution of meta-logic variables. Therefore any encoding where object-logic variables coincide with (or are closely related to) their meta-logic counterparts is bound to be efficient. However there might be cases when this implementation choice may conflict with other requirements that we impose on our encoding of Hoare Logic. Therefore we want an efficient substitution method that can be adapted to whatever representation we choose. The techniques we present in this paper come very close to fulfilling this goal, as they rely on the sole assumption that the logical formulas P and Q above are written in a higher-order calculus.
We use Hoare Logic for Proof Carrying Code (PCC) [7,1] applications, to reason about safety of machine language instructions encoded in logical framework as outlined in [2]. In PCC, mobile code is endowed with a (Hoare Logic) proof that it conforms to a specified safety policy. In this context we will not be merely interested in performing substitutions, but we will also require a proof that these substitutions have been performed correctly. More- over, we will need these correctness proofs to be small, so that they can be


easily transmitted, together with the program code, over a network. This has certainly influenced our definition of efficiency of substitution, making it more restrictive: judging the efficiency of a method, we will take in account proof size too.
Throughout this paper our focus will be on Hoare triples representing assignments:
[v/i]Q {i := v} Q
and, more specifically, we will concern ourselves with how to obtain the pre- condition [v/i]Q by substituting the value v for the variable i in Q.
In order to make the discussion easier to understand, we will fix the logic in which the formula Q is written. We adopt the one we used in our PCC applications, which is listed in the appendix. Although this is a higher-order calculus based on a polymorphically typed lambda calculus, it is important to stress that we never make use of type polymorphism in our presentation, and that the same techniques and results hold if applied to a different calculus, possibly even an untyped one.
We will rely on the availability of primitives for functional abstraction (‘λ’) and application (‘·’), and hence on rules for β-conversion, defined as the smallest equivalence relation containing the reduction
(λx. F ) Z →β [Z/x]F and closed with respect to the term structure.
Other primitives of our logic will be implication (‘⇒’) and universal quan-
tification (‘∀’). Commonly used logical connectives and quantifiers, such as negation (‘¬’), conjunction (‘∧’), disjunction (‘∨’), and existential quantifica- tion (‘∃’) can be defined in terms of these primitives. The logical constant for falsehood (‘⊥’), as well as the equality predicate (‘=’), can be similarly introduced as a defined symbols.
To reason about assignments, we will need to introduce some additional notion, and specifically:
a type for variable names, together with constants I, J , K for all the variable names used in our language;
a type for values, to which expressions like “2 ∗ j” will belong;
a syntactical representative for the semantic notion of state.
For (c), we use environments σ, defined as functions from variable names to values.
Since both precondition and postcondition formulas will describe state properties, we will view them as functions from environments to truth values. Hence the expression Q will take the shape of an abstraction term (λσ. Q'), where Q' is a formula containing the bound environment variable σ. We will furthermore require that preconditions and postconditions are allowed to refer to the environment function in a point-wise fashion. This means that


all the occurrences of σ in Q' will appear inside sub-expressions of the form σI, where I is one of the variable name constants mentioned in (a). Similar conventions apply to the term v: since a value can reference the content of program variables (e.g. v = 2 ∗ j references the content of the variable j), it will be also viewed as a function, mapping environments to values.
Under these conventions, the Hoare statement
[2 ∗ j/i]Q {i := 2 ∗ j} Q


is translated into


(λσ. [(2 ∗ σJ )/σI]Q') {I := 2 ∗ J} (λσ.Q'),

where the precondition is obtained from the postcondition by substituting all the free occurrences of σI in Q' by the term (2 ∗ σJ ). By abuse of notation, we will use the term “variable” to also describe expressions of the form σI.
Since the expression σI is an application term(the environment σ is applied to the variable name I), it is not possible for us use the β-reduction of the framewor. Even if we were to make this possible (by switching to a different representation of the problem, for example), it might still not be advisable to do so: in [6] Mason shows the pitfalls of using LF beta conversion to model Hoare Logic substitution. This justifies the decision to model substitution as a set of inference rules. An added bonus to this approach is that every time we perform a substitution we also obtain a proof that this operation perfomed correctly. The problem that we attack in this paper is therefore to choose an inference system that makes best use of the primitives offered by the framework, so that substitution proofs are found quickly. If we want to retain the these proofs, we will also want to keep their size to a minimum.

Substitution
At this point of the discussion, we have not still specified our definition of substitution. As it turns out, we have great freedom of choice in this matter. But we have to choose wisely: the “right” specification can be of great help towards an efficient implementation; on the other hand, we do not want a definition geared too much towards one specific algorithm, since we want to use it to compare different realizations.
Substitution will be modeled as a predicate “subst” of four arguments: the name I of the variable we want to substitute, the value V we want to substi- tute it with, and the expressions F and F ' before and after the substitution, respectively
subst I V F F '.
We will require this notion to be adequate, i.e the statement
subst I V (λσ. Q') (λσ. [(V σ)/σI]Q')



Fig. 1. “Inductive” definition of substitution
should be provable within our logical calculus.
Example 2.1 For the Hoare statement in the previous section, (λσ. [(2 ∗ σJ )/σI]Q') {I := 2 ∗ J} (λσ.Q'),
adequacy translates in the natural requirement that
subst I (λσ. 2 ∗ σJ ) (λσ. Q') (λσ. [(2 ∗ σJ )/σI]Q')
holds.
A first, fairly intuitive definition of substitution proceeds by induction on the structure of the formula. Since our logic does not provide primitives for structural induction, we use the induction-less definition substI shown in Fig- ure 1. Roughly, we describe substitution as the smallest relation r satisfying all the inductive hypotheses.
Lemma 2.2 The definition of substI is adequate.
Proof. Using the definition in Figure 1, we can construct inductive rules for deriving substI statements, e.g.

∀g. ∀g'.
substI I V (λσ. g) (λσ. g')
⇒ (substI I V (λσ. ¬g) (λσ. ¬g'))



Fig. 2. Semantic definition of substitution
Using these rules, we prove the statement arguing on the structure of Q'.	✷
There are two problems with this approach. First, inductive specifications usually force recursive implementations, and we are striving for a notion which is as implementation-agnostic as possible. The second problem is that the definition of substI depends heavily on the set of primitive symbols we use in our logic. If we add a new primitive, we need to modify the formula defining substI, and consequently all the proofs that rely on it.
We propose a different approach, more semantical in nature, as it appeals to the concept of environment. Informally, we define substS I V F F ' in term of the behavior of F and F ' when evaluated over some environments σ and σ' agreeing on all variable names except I. The precise definition can be found in Figure 2.
Lemma 2.3 The definition of substS is adequate.
Proof. Similarly to the proof of Lemma 2.2, we give rules to derive substS.
For example, let us show
substS I V (λσ. σI) V.
Unfolding the definitions, we have to show

∀σ. ∀σ'. (∀j. (j /= I ⇒ σj = σ'j)) ∧ (σ'I = (V σ))
⇒ ((λσ. σI) σ') = (V σ)

where the conclusion follows directly from the second hypothesis by a single
β-reduction step.	✷
We may ask ourselves how the inductive and semantic definitions are re- lated. It turns out that the latter is in general a weaker notion than the former. The two, however, can be proven to coincide if the calculus has the following additional properties:
Progress
An assignment instruction is always executable. In other words, given any environment σ, variable name I, and value V , we can always produce an environment σ' obtained from σ by updating the content of the variable


name I to (V σ). In symbols:

∀σ. ∃σ'. (∀j. j /= I ⇒ σ'j = σj) ∧ (σ'I = (V σ))
Extensionality
If two functions are equal point-wise, then they are equal:

∀f. ∀g. (∀x. f x = g x) ⇒ f = g
Lemma 2.4 For all I, V , F, and F ', the following holds:
substI I V F F ' ⇒ substS I V F F '.

The reverse implication is true, provide progress of assignments holds and the calculus is extensional.
Proof. Assume substI I V F F ', instantiating the variable r in the definition with substS, and using the same inductive rules we constructed in the proof of Lemma 2.3, we show substS I V F F '.
Conversely, assume substS I V F F ' and let σ be any environment. By Progress, there is σ' such that F σ' = F ' σ.
If we abstract all the occurrences of σI in F ,
F = (λσ. (λx. Fˆ) σI)


we have


F ' σ = F σ' = (λσ. (λx. Fˆ) σI) σ' = (λσ. (λx. Fˆ) (V σ)) σ


where the last equality follows from the fact that σ' was obtained from σ by updating the content of I to (V σ).
Using Extensionality, we conclude
F ' = (λσ. (λx. Fˆ) (V σ))

Applying Lemma 2.2,
substI I V (λσ. (λx. Fˆ) σI) (λσ. (λx. Fˆ) (V σ)),
and hence, replacing equals by equals, we conclude substI I V F F ', which is what we intended to prove.	✷
We adopt this latter, semantic version of substitution throughout the rest of this paper. For convenience, we will drop the subscript and write “subst” for “substS”.



Fig. 3. Rules for recursive traversal
Substitution Procedures
The definition of subst given in the previous section can be seen as a logical specification: it states the properties that we want substitution to satisfy, but does not tell us how to compute it. In this section we will present two different implementations that can be proved to satisfy this specification.
A first, straightforward one operates recursively on the structure of the formula. Unfortunately, a recursive implementation is not very good, both with respect to execution time and size of the proofs generated. To explain why, let us consider the expression F with the following tree representation:

λσ
,,,,,⇒r,~¸~ ,¸¸¸¸

sr,Λ~¸~ ,¸,,,
¸r,>~¸~ ,
,

r,~¸~ ,ssss
ccc	,
¸¸
r,>~¸~ ,
ccc	,
ııı	,
3	σJ

σI	σK	σI	2
and assume we want to substitute the numerical constant 2 for the variable
σJ . The rules for the recursive replacement method are listed in Figure 3;


they are the usual collection, introduced in the proof of Lemma 2.3, together with two new ones that deal with numbers and inequality, respectively.
The proof object generated using this inference system will be of the form:
subst⇒¸¸

    
   
  
substΛ¸¸¸¸

subst/= subst/=
¸¸¸




¸¸¸¸
=

Note that the structure of the proof term reflects that of the term F . In general, both time and proof size for this method will be proportional to the size of the expression. For large expressions, and/or for long sequences of substitutions, this is unacceptable.
Observe that σJ does not appear free in the left branch of F . If this information was available to us, we could avoid traversing it, saving in both time and proof size. Unfortunately it is not clear that information about the free variables of a subexpression can be computed without recursively travers- ing that subexpression. Also, since the set of free variables of an expression changes as a result of a substitution, it appears that such information would have to be recomputed after each substitution.
Searching for a better method, the key idea is to find a way to make use of the β-reduction and substitution primitives the logical framework puts at our disposal. Since substitution of meta-variables is implemented natively by the framework, it will be fast. Also, because β-reduction is one of the “trusted” operations, a method that is based on it will have to do less work in trying to convince us of its correctness, and hence will potentially generate shorter proofs.
Unfortunately, as we observed before, the substitution primitives of the framework substitute variables, while we are interested in replacing application terms σI. A first step in devising a new method is therefore to make all these application terms into (framework) variables. Consider a formula F ≡ (λσ. Fˆ), where Fˆ contains one or more occurrences of the variable σI. Using β-expansion, we can abstract out all these occurrences and write Fˆ as
Fˆ = (λx. F1) σI.
If we repeat this operation for all the variables I1,..., In appearing in G, we have
F = λσ. Fˆ = λσ. (λx1. . . . λxn. Fn) σI1 ... σIn,
where, by what we stipulated at the beginning (σ is used only point-wise), the term Fn will not contain any occurrence of σ.


Let let0 and let1 be defined as follows:
let1 I G ≡ (λσ. G (σI) σ) let0 I G ≡ (λσ. G).
With these new definitions, the formula F above can be written as
F = let1 I1 (λx1. let1 I2 (λx2. . . . let1 In (λxn. let0 Fn) ... ))
We will use the compact notation
[[Ix1 ,..., Ixn ]] Fn
1	n
for the above let-formula.
The let constructs are easily shown to satisfy the following basic properties:
Permutation
The position in which variables appear in a let-sequence can be permuted: [[..., Ix,Jy,.. .]] F = [[...,J y, Ix,.. .]] F
Absorption
Multiple occurrences of the same variable name can be compacted into one:


Elimination
[[... Ix, Iy,.. .]] F = [[..., Ix,.. .]] [x/y]F

If a let-bound variable no longer appears in an expression, the binding can be removed from the let-sequence:
[[..., Ix,.. .]] F = [[..... .]] F	(x ∈/ FV(F ))
Using these simple facts, it is easy to see that let-sequences can be identified with the set of free variables of an expression: if a variable σI does not appear in F , the corresponding entry Ix can be removed from the let (Elimination); moreover, let-sequences are order-less (Permutation) and without duplicates (Absorption).
Assuming both F and V are written as let-expressions, subst I V F F ' can now be computed with a better method, illustrated by the inference system of Figure 4.
Example 3.1 Let
F ≡ λσ. σI > σK ⇒ 3 > σJ
and assume we want to replace 2 for σJ . Transforming everything into let notation, we want to find G0 such that
subst J ([[]] 2) ( [Ix,Jy, Kz]] x > z ⇒ 3 > y) G0.















Fig. 4. Substitution rules for let-formulas
We use the inference system of Figure 4 in a backward-chaining fashion, using the inference rules to reduce our initial statement to progressively smaller goals.
We notice that, letting G0 ≡ (let1 I (λx. G1)), our thesis matches the conclusion of rule subst/=. Hence we are left to prove
subst J ([[]] 2) ( [Jy, Kz]] x > z ⇒ 3 > y) G1


for all x. Using rule subst=
, we further reduce this to

subst J ([[]] 2) ( [Kz]] x > z ⇒ 3 > 2) G1.
This matches again with the conclusion of rule subst/=, so that we are left to
prove
subst J ( [ ] 2) ([[]] x > z ⇒ 3 > 2) G2,
where G1 ≡ (let1 K (λz. G2)). Finally, we can solve this goal using substlet0 with G2 ≡ ([[]] x > z ⇒ 3 > 2). Putting all together, we conclude
G0 ≡ ([[Ix, Zz]] x > z ⇒ 3 > 2).
Using this approach both time and proof size become proportional to the number of variables in the formula F . Even if we account for the time re- quired to convert a generic formula to the let representation (this will be proportional to the size of the formula), in scenarios where we need to apply (sequentially) several substitutions the new method presents clear advantages over the recursive one presented before.

Empirical Results
To compare the practical performance of the two methods presented in the previous section, we implemented in the Twelf logical framework [10] an au- tomated prover that constructs substitution proofs using the rules of either


system (Figures 3 and 4). We ran this prover on nine test cases, varying in both size and number of variables. Figure 5 compares the sizes of the proofs generated by the two methods. The data we collected verified our claim that the method based on the let-representation produces proofs which are pro- portional in size to the number of variables of the expression; the exact ratio between these two quantities turned out to be 10.







1500
1000
500


100
50


10




0	400	800	1200
Formula size (tokens)

Fig. 5. Proof size

The method based on let also offered substantial savings with respect to execution time. Again, this turned out to be proportional to the number of variables with a ratio of approximately 1/20 (Figure 6).







10





1








0	300	600	900	1200
Formula size (tokens)

Fig. 6. Proof generation time

Asymptotically, we expect the time gains of the let formulation to become less significant than what it might appear looking at Figure 6. Our method just converts object-logic substitutions to meta-logical ones; while in all examples we considered the time required by the latter was negligible, as we increase the expression size these are bound to impact performance.



Fig. 7. Rules for nondependence

A Refinement of the Method
Substitution using let can be further improved by making use of the additional structure that our domain of application (program analysis) has.
We observe that, although the programs we wish to consider are quite large, they can be usually split into blocks. Assignments in a specific block B will read from and write to a subset ∆B of the entire set of program variables Γ. The substitution techniques presented so far do not make use of this information, and require the traversal of the entire let-sequence, which will have length |Γ|.
Let us define a “nondependence” predicate nFree as nFree S F ≡ 6i. 6v. i ∈ S ⇒ subst i v F F
Intuitively, this says that the variables in S are not free in F ; that is, for all I ∈ S, any substitution for σI does not affect F . A set of rules to derive “nFree” is given in Figure 7.
A refinement of our method requires that, for each block B
we move the variables ∆B to the beginning of the let-sequence (using Per- mutation):
[[Γ ] Fˆ = [[∆B, ΓB]] Fˆ;

we construct a proof φB of nFree ∆B ([[ΓB]]
Fˆ).

Although these two operations might be expensive (both in terms of time and proof size), they might offer a big payoff, since now each substitution of a variable σI inside the block B can be done by
traversing the initial ∆B-segment of the let-sequence;
testing for membership of I to ∆B and using φB to conclude that the remaining ΓB-sequence is not changed by the substitution.
For (a), we know that the number of steps required is proportional to the length of the subsequence, in this case |∆B|. For (b), we can use an encoding of sets that minimizes the time of a test for membership; assuming the set ∆ is represented as a balanced binary tree, membership can be established in at most log2 |∆B| steps.
An implementation of this refinement of the original method is currently under development.

Conclusions
Encoding Hoare Logic in a logical framework has been a very instructive ex- perience, in at least two distinct aspects.
First, it opened us to the idea of giving semantical definitions for syntac- tical concepts like substitution. The same principle can be applied to other notions: for example, we can define the set of free variables of a formula as
freevars S F ≡ 6σ. 6σ'. (6i. i ∈ S ⇒ σi = σ'i) ⇒ (F σ) = (F σ') Secondly, it has shown to us that the myth that the feature set of a frame-
work automatically forces most implementation choices is not universally true.
Our experiments with substitution have convinced us that often a bit of inge- nuity goes a long way in broadening the selection of possible choices. Thus, we more likely to find an implementation that is optimal with respect to our goals, which in this case were execution time and proof size efficiency.

References
A. W. Appel and A. P. Felty. A semantic model of types and machine instructions for proof-carrying code. In Proceedings of the 27th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL ’00), pages 243–253, January 2000.
A. W. Appel and N. G. Michael. Machine instruction syntax and semantics in higher order logic. In Proceedings of the 17th International Conference on Automated Deduction (CADE-17). Springer-Verlag, June 2000.
R. Burstall and F. Honsell. Operational semantics in a natural deduction setting. In G. Huet and G. Plotkin, editors, Logical Frameworks, pages 185–214. Cambridge University Press, 1992.
M. J. C. Gordon. Introduction to HOL: A Theorem Proving Environment. Cambridge University Press, 1993.
C.  A.  R.  Hoare.	An  axiomatic  basis  for  computer  programming.
Communications of the ACM, 12(10):576–580, October 1969.
Ian A. Mason. Hoare’s logic in the LF. Technical Report ECS-LFCS-87-32, Laboratory for Foundations of Computer Science, University of Edinburgh, June 1987.
G. C. Necula and P. Lee. The design and implementation of a certifying compiler. In Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages 333–344, 1998.
L. C. Paulson. Introduction to Isabelle. Technical Report 280, University of Cambridge, Computer Laboratory, 1993.


F. Pfenning. Logical frameworks. In Alan Robinson and Andrei Voronkov, editors, Handbook of Automated Reasoning. Elsevier Science Publishers, 1999.
F. Pfenning and C. Schu¨rmann. System description: Twelf — A meta-logical framework for deductive systems. In Harald Ganzinger, editor, Proceedings of the 16th International Conference on Automated Deduction (CADE-16), pages 202–206, Berlin, July 1999.

Appendix
Our calculus is a Twelf encoding of the system presented in [1]. The core symbols and rules are introduced as follows:
% Types
tp	: type.
form : tp.
arrow: tp -> tp -> tp.	%infix right 14 arrow. allt : (tp -> tp) -> tp.
% Formulas
tm	: tp -> type.
lamt : ({T:tp} tm (F T)) -> tm (allt F).
@@	: tm (allt F) -> {T:tp} tm (F T).	%infix left 20 @@. lam	: (tm T1 -> tm T2) -> tm (T1 arrow T2).
@	: tm (T1 arrow T2) -> tm T1 -> tm T2.	%infix left 20 @. imp	: tm form -> tm form -> tm form.	%infix right 10 imp. forall: (tm T -> tm form) -> tm form.
% Rules
pf	: tm form -> type.
betat_e : {F}{P} pf (P (lamt F @@ T)) -> pf (P (F T)).
betat_i : {F}{P} pf (P (F T)) -> pf (P (lamt F @@ T)).
beta_e : {P} pf (P (lam F @ X)) -> pf (P (F X)).
beta_i : {P} pf (P (F X)) -> pf (P (lam F @ X)). imp_i	: (pf A -> pf B) -> pf (A imp B).
imp_e	: pf (A imp B) -> pf A -> pf B. forall_i: ({X} pf (A X)) -> pf (forall A). forall_e: pf(forall A) -> {X} pf (A X).
Other useful logical symbols can be defined in terms of the above:
eq	: tm T -> tm T -> tm form =
[A][B] forall [P] P @ B imp P @ A. and	: tm form -> tm form -> tm form =
[A][B] forall [C] (A imp B imp C) imp C. or	: tm form -> tm form -> tm form =
[A][B] forall [C] (A imp C) imp (B imp C) imp C. exists : (tm T -> tm form) -> tm form =
[F] forall [B] (forall [X] F X imp B) imp B. false: tm form = forall [A] A.
not : tm form -> tm form = [A] A imp false.
