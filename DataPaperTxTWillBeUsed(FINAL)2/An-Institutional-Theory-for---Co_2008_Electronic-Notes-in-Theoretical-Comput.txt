	Electronic Notes in Theoretical Computer Science 195 (2008) 113–132	
www.elsevier.com/locate/entcs

An Institutional Theory for #-Components
Francisco Heron de Carvalho-Junior1 ,2
Departamento de Computac¸˜ao Universidade Federal do Ceara´ Fortaleza, Brazil
Rafael Dueire Lins3
Departamento de Eletroˆnica e Sistemas Universidade Federal de Pernambuco Recife, Brazil

Abstract
The # (hash) component model has been proposed to bring the advantages of a component-based per- spective of software for the development of high performance computing applications, targeting computer architectures enabled for grid, cluster and capability computing. In simple terms, it is a component model for general purpose parallel programming targeting distributed architectures. This paper presents an insti- tutional theory for #-components, which has originated the idea of introducing parameterized and recursive abstract component types in # programming systems, making possible a general notion of skeletal program- ming.
Keywords: Theory of institutions, category theory, components, parallel programming.


Introduction
Advances in technologies for developing software for high performance computing (HPC) applications, commonly originated from computational sciences and engi- neering, have been influenced by current trends in software integration, distribution, and parallelization [8]. The component technology, which has been successfully ap- plied to business applications, has been considered a promising approach to meet those requirements, yielding the birth of several component models, architectures, and frameworks for HPC, including CCA and its compliant frameworks [3], P-COM [33], Fractal/Proactive [5], and many others [44]. Components deal with require- ments of integration and distribution in a natural way, but parallel programming

1 Thanks to CNPq for the financial support (grant 475826/2006-0).
2 Email: heron@lia.ufc.br
3 Email: rdl@ufpe.br

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.08.029

based on the peer-to-peer pattern of components interaction is still not suitable for high performance software, mainly when there are non-trivial patterns of parallel synchronization among a collection of processes in architectures with deep hierar- chies of parallelism and memory, potentially enabled for grid, cluster, and capability computing [14,1,17]. Indeed, outside the context of components technology, paral- lel programming artifacts that can exploit the performance of these architectures, such as message passing libraries [20,23], still provide poor abstraction, requiring a fair amount of knowledge on architectural details and strategies of parallelism that go far beyond the reach of users in general [28]. Higher level approaches, such as functional programming languages [43] and parallel scientific computing libraries
[18] do not merge efficiency with generality. Skeletal programming has been con- sidered a promising alternative, but has reached low dissemination [16]. Parallel programming paradigms that reconcile portability and efficiency with generality and abstraction are still looked for [7,40].
The current trend of parallelism support in components infrastructures has been to encapsulate parallel synchronization inside components, sometimes introducing a minimal set of orthogonal extensions at the coordination level to enable parallel execution. Thus, parallelism is not fully treated as a coordination level concern, as expected. Such approach is influenced by the common trend of taking processes as units of software decomposition in the same dimension of concerns, thus making software engineering disciplines too hard to be applied to parallel programming. Processes and concerns must be placed at orthogonal dimensions of software de- composition [13]. The # component model takes the hypothesis of orthogonality as a premise, proposing an alternative for component based parallel programming, inspired in the coordination model of Haskell# [11], a parallel extension to the functional language Haskell [41]. Most possibly, any component model and parallel programming artifact may be interpreted in terms of the # component model.
This paper introduces an institutional theory for #-components, which has pro- vided valuable insights about abstraction mechanisms that could be supported by # compliant programming systems. In fact, it has suggested that #-components may be categorized by component classes represented by recursive component types, with support to polymorphism based on universal and existential bounded quantifi- cation [37]. A generalized use of skeletal programming [16], a promising approach for promoting abstraction in parallel programming, has been made possible from such perspective. A basic background in Category Theory and Petri nets is recom- mended for readers of this paper.
Section 2 introduces the basic principles behind the # component model. Sec- tion 3 includes the formalization of the # component model using Theory of In- stitutions. Section 4 concludes this paper, describing ongoing and lines for further works regarding formalization, specification, and verification of #-components.


  

r = (A x X) . (B x Y)

redistribute


P0 || P1
P2 || P3
P0 || P1 || P2 || P3

Fig. 1. Slicing a Simple Parallel Program by Concerns
The # Component Model: Principles and Intuitions
Motivated by the inadequacy of usual forms of peer-to-peer component interactions for parallel programming, extensions have been proposed to current component arti- facts. In general, they free component frameworks from concerns about parallelism at coordination level, leaving synchronization encapsulated inside components, tan- gling functional code. Despite covering a wide spectrum of parallel programs, they do not reach the generality of message-passing. Indeed, it is common to find papers on HPC components that include “support for richer forms of parallelism” in the list of lines for further investigation. The # component model comes from a deductive generalization of channel-based parallel programming for supporting a general and efficient notion of parallel component. Its origins lie in Haskell# [11], a parallel ex- tension to the functional language Haskell. The following paragraphs introduce the fundamental principles behind the # component model: the separation of concerns through process slicing ; and orthogonality between processes and concerns as units of software decomposition. Some familiarity with parallel programming is needed to understand the # component model from the intuition behind its basic principles. Figures 1 and 2 sketch a simple parallel program that illustrates the idea of slicing
processes by concerns 4 . Let A and B be n×n matrices and X and Y be vectors. It computes (A × XT )•(B × Y T ).
In fact, we have searched for the fundamental reasons that make software engi-
neering disciplines too hard to be applied to parallel programming, and concluded that they reside on the trend to mix processes and concerns in the same dimension of software decomposition, due to the traditional process-centric perspective of parallel programming practice. Software engineering disciplines assume concerns as basic units of software decomposition [34]. We advocate that processes and concerns are

4 The use of the term slicing is not accidental. The problem of slicing of programs according to some criterion was proposed originally in the nineteen seventies and has motivated several research directions [42].

orthogonal concepts. Without any loss of generality, aiming at clarifying the intu- itions behind the enunciated orthogonality hypothesis, let P be an arbitrary parallel program formed by a set of processes that synchronize through message-passing. Each process may be split into a set of slices, each one related to a concern. In
Figure 1, four processes are sliced into its constituent concerns. Examples of typical concerns are:
a piece of code that represents some meaningful calculation, for example, a local matrix-vector multiplication;
a collective synchronization operation, which may be represented by a sequence of send/recv operations;
a set of non-contiguous pieces of code including debugging code of the process;
the identity of the processing unit where the process executes;
the location of a process in a given process topology.
The reader may be convinced that there is a hierarchical dependency between pro- cess slices. For instance:
the slice representing collective synchronization operation is formed by a set of slices representing send/recv point-to-point operations;
a local matrix-vector multiplication slice may include a slice that represents the local calculation performed by the process and another one representing the col- lective synchronization operation that follows it.
If one takes all the processes into consideration, it is easy to see the existence of concerns that cross-cut processes. For example:
the concern of parallel matrix-vector multiplication includes all slices, for each involved process, that defines the (local) role of the process in the overall opera- tion;
the concern of process-to-processor allocation is formed by the set of slices that defines the identities of processors where each process executes.
From the perspective of the parallel program, most of slices inside individual pro- cesses do not make sense when observed in isolation. Individually, they are not concerns in the overall parallel program.
The # component model moves parallel programming to a concern-oriented perspective. A #-component realizes an application concern, functional or non- functional one. Focused on concerns, programmers may build #-components by combining other #-components through overlapped composition. The units of a #- component C correspond to the slices of processes of the intended parallel program that define the role of each process in the cooperation to realize the concern of
C. The reader must be convinced that the concrete interpretation of a unit is abstract at the perspective of the # component model, since it may correspond to slices of any nature, covering functional and some kinds of non-functional concerns. The units of overlapped #-components are combined to form the units of the #- component being composed. Let u be a unit. The units that are combined to

















Fig. 2. Forming a Unit by Combining Units of Overlapped #-Components

form u are called slices of u. Slices have a hierarchical structure, like process slices. Sharing between components comes from fusion of slices when combining units. For example, two units of #-components whose concerns are linear algebra operations may fuse slices that represent vector or matrix operands, specifying that they are applied to the same instance of data structure. Sharing of data structures is a fundamental issue in components-based HPC. It is also supported by Fractal [9]. Rooted directed acyclic graphs have captured the hierarchical structure of slices in units, describing their signature. The protocol of a unit is specified by a labelled Petri net, whose corresponding formal language says in which order processes may execute their functional slices (labels of the Petri net), which may be interpreted as blocks of code or procedure calls. Petri nets allow the analysis of formal properties and performance evaluation of # programs. An interface is defined as a set of units that complies to the same signature and protocol. In fact, it is the type of such set of units. The component type of a #-component is defined by the set of interfaces of its units. In more intuitive terms, a #-component differs from usual components by its ability to be deployed in a distributed environment, with each unit being its representant in an individual computer, and by its ability to deal with a rich set of non-functional concerns. A #-program is an executable #-component.
The # component model goes far beyond the idea of raising connectors to the status of first-class citizens [36,39], by promoting them to components, leading to uniformity of concepts. For example, a communication channel could be imple- mented as a #-component Channel. Fractal also supports connectors as compo- nents through composite bindings, but primitive bindings are not yet components. The # connectors are exogenous [30], like in P-COM, while they are endogenous in CCA and Fractal.

The Institutional Theory of #-Components
A precise and rigorous mathematical characterization of the # component model was developed, based on the Theory of Institutions and Category Theory. Institu-



Fig. 3. Institutions and Their Satisfaction Condition
tions were proposed by J. A. Goguen and R. M. Burstall [27]. Their main purpose have been to formalize the idea that satisfaction in logical systems is invariant un- der change of notation. An interesting variant of institutions are π-institutions, proposed by J. L. Fiadeiro [22]. Institutions have been applied in the semantics of algebraic specification systems [27], type systems theory [24,25], and logical systems [26].
Definition 3.1 An institution K is a quadruple ⟨Sig, modK, senK, |=K⟩, where:
Sig is a category of signatures;
modK : Sig → Catop is a functor that map signatures to the category of models
with that signature.
senK : Sig → Cat is a functor that maps each signature to the category of
sentences over that signature.

Let Σ be a Sig-object, the relation |=K
associates Σ-models with Σ-sentences.

The satisfaction condition showed in the commutative diagram in Figure 3, where
institutions are defined in terms of the category of twisted relations (TRel), must be satisfied.
The Theory of Institutions was adopted in this work because it concisely cap- tures the notions of compatibility between protocols, which gives rise to interface morphisms, and the idea to interpret #-components as models of component types, for supporting skeletal programming. This work is deeply inspired by the idea of “types as theories” [25]. Institutions are a sophisticated formal machinery. Any attempt to present further details about them in this paper would be incomplete. For those readers interested in the use of institutions in specification languages and type systems theory, we refer to [27] and [24]. The book by J. L. Fiadeiro is another alternative [22]. Connections of Institutions with logical systems are addressed in reference [26].
A Formal Characterization for Concerns
First, it is necessary to provide an abstract characterization for software concerns. Unfortunately, it is not possible to provide a rigorous and general definition of


	
Interface Signature S	rDAG of S
Fig. 4. Signatures and rDAGs
concerns separated from a particular language. But it is possible to characterize certain classes of concerns with special interest in software development. For ex- ample, functional concerns may be defined by computable functions, specified using some specification language. Richer software specification languages may support specific kinds of non-functional concerns, such as security and demands for com- puting resources. Concerns may be abstracted from implementation details, at several levels. The functional concern of solving a linear system, for example, may be implemented using several techniques, on top of many programming artifacts. #-components intend to be software “materializations” for concerns. The # com- ponent model adopts an abstract characterization of concerns, viewed as elements
of a collection named Concerns. The metaphor of colors will be used to characterize
concerns. Indeed, it is said that a unit has the color of the concern it represents.
The Institution of Interfaces (I)
Definition 3.2 [Interface Signatures]
An interface signature is a pair ⟨G, γ⟩, where G=⟨N, A, ∂0, ∂1⟩ is a Rooted Di- rected Acyclic Graph (rDAG), and γ:N →Concerns is a function to color nodes with concerns. A rDAG is: (1) an empty graph; or (2) a non-empty directed acyclic graph
with a distinguished root node r ∈ N .
Interface signatures are rDAGs whose nodes have a color that denotes their concerns. Let S be an interface signature and r be a slice of S. An interface slice r of S is the branch of S, also an rDAG, that includes all nodes of S that are reachable from r. The hierarchy of slices in processes is captured by rDAGs. Also, notice that nodes with the same color may have sets of sons with distinct sets of colors, respectively, denoting different implementations for a given concern. Figure 4 illustrates interface signatures. In particular, it shows how they capture fusion of slices, by allowing interface slices to be shared. For example, an interface slice representing a sparse matrix may be shared by all interface slices that represent computations over that matrix.
Definition 3.3 [Interface Signatures Morphisms]
Let Gr be the category of graphs. Let S1=⟨G1, γ1⟩ and S2=⟨G2, γ2⟩ be interface signatures. An interface signature morphism σ:S1→S2 exists if and only if there is
a morphism σ∗:G∗→G∗ in Gr and γ1 = γ2 ◦ σ∗ (color preservation), where G∗ is
1	2	N


























S and S' are interface signatures (ISig-objects), while σ:S→S' is a ISig-morphism. The protocol P1 is an example of senI (S)-object, while P2 and P3 are examples of senI (S')-objects. The reader may be attempted to the hierarchical structures of the underlying Petri nets of P1 , P2 , and P3 . The picture emphasizes that P2 is the image of P1 in senI (S') through the morphism senI (σ), which maps protocols in senI (S) to its corresponding protocols in senI (S'). Also, there is a general Petri net morphism from P2 to P3 . All morphisms inside senI (S) and senI (S') are general Petri net morphisms.
Fig. 5. Interface Signatures and Protocols

the freely generated graph of G.
Theorem 3.4 Interface signatures and their morphisms forms a category, named
SigI.
Figure 4 also depicts an example of a morphism between interface signatures (σ) that emphasize that they do not need to preserve roots. The contravariant functor modI:SigI→Catop maps an interface signature S onto the category of units with interface signature S, called S-units. Intuitively, a unit is a process slice that
implements the concern specified by an interface signature. The # programming systems are responsible to define the real nature of what is called a process slice and what is meant by a process slice that is a model of an interface signature, but it must be ensured that if an interface signature S with root s says that s has slices s1,... , sn as its children, a S-unit U must implement the concern that colors s by combination of units Ui, for 1 ≤ i ≤ n, each one complying to the the interface
slice si. The units Ui are the unit slices of U . A unit has a behavior that may be
viewed as a control flow that defines valid orders for activating a subset of its unit slices. Thus, the behavior of a unit will be described by a set of traces of the form t1t2.. .tk that are valid with respect to its control flow, where each ti represents the activation of a unit slice. The behavior of a unit will be treated as a terminal Petri net formal language. It is important to emphasize that the meaning of the activation of a unit, when viewed as a unit slice, may vary according to the kind of concern that it addresses. For instance, if a unit slice denotes a computation or synchronization operation, its activation may correspond to a call to a subroutine. Notice that unit slices that denote several kinds of non-functional concerns, such as allocation of processes to processors, do not require an activation semantics. Unit slices with an activation semantics will be called executable slices.
Let S be an interface signature. Tracing sentences over S, or S-traces, are defined by protocols. A protocol is a hierarchical Petri net, whose transitions are

labelled with interface slices (nodes of the rDAG) that denotes activable concerns. The hierarchical structure of Petri nets of protocols obeys the hierarchy of slices of the interface signature. Therefore, if a transition is labelled with a node s whose ancestor node is s', then:
s' does not label any transition in the protocol; or
s' is the label of some substitution transition and s belongs to the Petri net page that refines s'.
It is still necessary to give an interpretation for tracing sentences over behavior of units. Let U be a S-unit and Lt be the formal language that define the behavior of U (set of traces). T is true for Lt iif Lt ⊆ l(T ), where l(T ) denotes the terminal
Petri net language of the protocol T . Let T and T ' be tracing sentences over S. T ' is
deducible from T , written T ▶ T ', iif l(T ) ⊆ l(T '). As usual, the transitive relation
▶∗ is inductively defined over ▶. The functor senI : SigI → Cat maps an interface signature S to the category of tracing sentences over S, and an interface signature morphism σ : S→S' to the corresponding inclusion morphism. The morphisms in the category senI(S), for some interface signature S, are rigid morphisms of Petri nets [6], in order to ensure existence of products of labelled Petri nets. Let σ : S → S' be an interface signature morphism and T be a S-trace. senI(σ)(T ) may be obtained from T by simple application of homomorphism σN over transition labels of T . It can be demonstrated that senI(σ)(T ) is isomorphic to T in categories of Petri nets, since they only differ by the identification of labels. The relation
|=I : modI(S) × senI(S) associates S-units with S-traces. Indeed, U |=I T iif T is
S	I	S	I
true for the behavior of U . The union of all |=S , for some S, is denoted by |= .
Theorem 3.5 (The Institution I) The	quadruple	⟨SigI, senI, modI, |=I⟩
forms the institution of interfaces, named I.
Proof. Let σ : S → S' be an interface signature morphism, T ∈ senI(S), U ' ∈
modI(S'), U ≡ modI(σ)(U '), and T ' ≡ senI(σ)(T ).  It must be proved that
U |=I T iif U ' |=I T ', or the commutative of the diagram in Figure 3. (⇒) First,
S	S I
suppose that U |=S T . It follows, from the interpretation of units as slices of
processes, that if t ∈ l(T ), then t is a trace in the behavior of U . Let t = s1.. .sk. Since the Petri net senI(σ)(T ), or T ', is obtained from T by homomorphism of
labels according to σ, then t' = s' .. .s' ∈ l(T '), where s' is the image of si with
1	k	i	I	'
respect to σN . Thus, since σ preserve concerns and, by mod (σ), traces of U
are mapped to traces of U by inverse application of the homomorphism σN , where each symbol in a trace of U ' that is not in the image of σN is discarded in the corresponding trace of U , t' is a trace in the behavior of U '. By consequence,

U ' |=I T '. (⇐) Conversely, suppose that U ' |=I
T '. Let t' = s' .. .s' ∈ l(T ') be a

S	'	'	I
S	1	k	I

trace of U . Since T ≡ sen
(σ)(T ), by isomorphism between T and sen
(σ)(T ),

T may be obtained from T ' by replacing s for each label s' in the transitions of T ', such that σN (s)= s'. From this fact, it follows directly that each t ∈ l(T ) may be obtained from t' ∈ l(T ') by application of the homomorphism σN . By modI(σ), t is a trace of U . Thus, U |=I T .	 

Let S be an interface signature. An interface I with signature S is defined by a S-presentation ⟨S, Θ⟩ over the institution I, where Θ is a set of tracing sentences over S. Since Petri net languages are closed under intersection, Θ may refer to the labelled Petri net whose language is the intersection of the Petri net languages of
the protocols in Θ. Let I = ⟨S, Θ⟩ be an interface. A S-unit U has interface I (U satisfies I) iff U |=S Θ. The set of all S-units that satisfy the tracing sentence Θ is
the denotation of Θ, defined as Θ∗ = {U | U |=I Θ}. Let Ω be a set of units. Then,
∗	I	∗∗	S	•
Ω = {T | ∃U ∈ Ω. U |=S T }. Θ	may be written as Θ . The S-theory presented
by the interface I is ⟨S, Θ•⟩. The category of interfaces is denoted by Theo(I), the category of theories over I. Under such interpretation, interfaces may be used to classify units, as intended. From the institutional theory, an interface morphism is a presentation morphism Ψ : ⟨S, Θ⟩ → ⟨S', Θ'⟩ induced from the interface signature morphism ΨS : S → S', such that ΨΘ(Θ) ⊆ Θ'•. The denotation of Ψ is defined by the forgetful functor Ψ∗ : Θ'∗ → Θ∗.
The institution I is liberal. Given an interface morphism Ψ : I1 → I2, where
I1 = ⟨S1, Θ1⟩ and I2 = ⟨S2, Θ2⟩, let Ψ∗ : Θ∗ → Θ∗ be its denotation, ΨS : S1 → S2
2	1	∗
be the corresponding interface signature morphism, and U1 be a model in Θ1. Then,
there is a model U2 in Θ∗ that is said to be free over U1 with respect to Ψ∗. Formally,
2	∗	∗	'
there is some morphism i : U1 → Ψ (U2) such that given any j : U1 → Ψ (U2),
there is a unique morphism h : U2 → U ' such that Ψ∗(h) ◦ i = j. Informally, U2
∗	2
is the best model in Θ2 that “extends” U1, in the sense that any other extension
of U1, represented by U ' , may be defined in terms of U2. In fact, informally, we can take U2 as the S2−unit obtained from U1 whose traces are the interleaving of the formal languages defined by the traces U1 and by the closure of the alphabet formed by the symbols representing the slices of U2 that are not in U1. In such case, Ψ∗(U2)= U1. Thus, since i = idU , h is unique.

3.2.1  A Subtype Relation for Interfaces
The notion of subtype interface comes from the notion of subpresentation in in- stitutional theory. Let Ψ : ⟨S, Θ⟩ → ⟨S', Θ'⟩ be an interface morphism. Ψ is a subpresentation if S ⊆ S' and Θ ⊆ Θ', but such formulation still depends on a suitable notion of subsignature (⊆). The symbol <:, denoting subtype relation, will represent the inverse relation of ⊆. For instance, let S and S' be interface signa- tures, and σ : S → S' be a interface signature morphism. Then, S' <: S iif σ is a monomorphism that preserve roots.
Theorem 3.6 Let Ψa : Ix → Ia and Ψb : Ix → Ib be Theo(I)-morphisms, such that Ψa is a subtype morphism, i. e., Ia <: Ix. The categorical pushout of Ψa and Ψb, denoted by Ψa ⊕ Ψb, exists.
Proof. Let Ix = ⟨Sx, Θx⟩, Ia = ⟨Sa, Θa⟩, and Ib = ⟨Sb, Θb⟩. The use of the in- stitutional framework makes necessary only to show that the pushout between the corresponding signature morphisms σa : Sx → Sa and σb : Sx → Sb exists. For instance, let Sx=⟨Gx, γx⟩, Sa=⟨Ga, γa⟩, and Sb=⟨Gb, γb⟩.  The pushout between
σa and σb is defined by two arrows σ' :Sa→Sa+S Sb and σ' :Sb→Sa+S Sb, where
a	x	b	x

Sa+Sx Sb=⟨Ga+Gx Gb, γa+γx γb⟩. Ga+Gx Gb denotes the vertex of the pushout be-
tween σ∗:G∗ →G∗ and σ∗:G∗ →G∗ in Gr by forgetting transitive relation. Ga+G Gb
a	x	a	b	x	b	x
is also a rDAG, since morphisms under consideration preserve roots and direction
of arcs are preserved by graph morphisms. γa+γx γb is a pushout of morphisms in Set, which involves their source and target objects. Since Gr and Set has all pushouts, we can conclude that it is always possible to build the pushout of two ISig-morphisms.	 

The Institution of Component Types (C)
An institution of component types C may be built over I. The category SigC, for signatures of component types, has finite sets of interface signatures as objects and total functions between sets of interface signatures as morphisms, whose mappings are induced from the interface signatures involved. More formally, Let C1 and C2 be SigC-objects and Φ:C1→C2 be a SigC-morphism. If Φ(S1)=S2, then S1 ∈ C1, S2 ∈ C2, and there is a SigI-morphism σ:S1→S2.
Let C=⟨S1, S2,.. ., Sk⟩ be a signature of component type. The category of mod- els of C, or #-components with component type signature C, and the category of sentences over C are respectively defined as:
k	k
modC(C)= Σ modI(Si)	and	senC(C)=  senI(Si)
i=1	i=1
where  and Σ denote categorical k-ary product and coproduct, respectively. Since modC is a contravariant functor, a model of C is M = ⟨U1, U2,... , Uk⟩, where each Ui is a Si-unit. A sentence over a component type signature is represented by
Θ = ⟨T1, T2,... , Tk⟩, where each Ti is a Si-trace. The relation |=C : modC(C) ×

senC
C
(C), for every component type signature C, associates #-components with

signature C with sentences over C. It is induced from the relation |=I , for interface signatures. For instance, let C = ⟨S1, S2,... , Sk⟩ be a signature of component type, Θ = ⟨T1, T2,... , Tk⟩ be a sentence over C, and M = ⟨U1, U2,... , Uk⟩ ∈ modC(C)
be a #-component of signature C. M |=C Θ iif Ui |=I Ti, for 1 ≤ i ≤ k.
Let Φ : C  → C' be a SigC-morphism.	Let C=⟨S1,... , Sn⟩ and C'  =
⟨σ1(S1),... , σn(Sn), S'	,... , S'	⟩, where σi, for 1≤i≤n, are SigI-morphisms
that define the mappings of the total function Φ. Let Θ = ⟨T1,... , Tn⟩ be a trace
over C. Then, senC(Φ)(Θ) = ⟨senI(σ1)(T1),... , senI(σn)(Tn), ∅n+1,... , ∅n+m⟩, where ∅i denotes the empty protocol (an empty Petri net), is the corresponding trace over C'. Since ∅i is the initial object of the category of protocols over the in- terface signature Si, the intuition behind such definition says that SigC-morphisms
do not impose any protocol restriction in the implementation of #-components with respect to units of interface signatures that are not in their image.
Theorem 3.7 (The Institution C) The   quadruple   ⟨SigC, senC, modC, |=C⟩
forms the institution of components types, named C.
Proof. Let Φ  :  C  →  C'  be a SigC-morphism, Θ  ∈  senC(C),  M '  ∈
modC(C'),  M  ≡  modC(Φ)(M '),  and Θ'  ≡  senC(Φ)(Θ).   It must be










’1
’2 ...



Fig. 6. Diagram for the Proof of Theorem 3.8
proved that M	|=C	Θ iif M '	|=C	Θ'.	Let C	=  ⟨S1,... , Sn⟩, M '	=
'	'	C	C	'
⟨U1,... , Um⟩, and Θ = ⟨T1,... , Tn⟩.  By Φ = {σ1,... , σn}, we can take C  =
⟨σ1(S1),... , σn(Sn), S'	,... , S'	⟩, M = ⟨modI(σ1)(U ' ),... , modI(σn)(U ' )⟩
n+1	'	n+m	1	n
(for simplicity, we take ⟨U ,... ,U ' ⟩, for n ≤ m, as the units in the image of
C	'	I	1	n	I

mod (Φ)), Θ = ⟨sen (σ1)(T1),... , sen
(σn)(Tn), ∅n+1,... , ∅n+m⟩. (⇒) Suppose

that M |=C Θ. Thus modI(σi)(U ') |=I  Ti, for 1 ≤ i ≤ n. From I, we can
C	i	Si

conclude that U ' |=I senI(σi)(Ti), for 1 ≤ i ≤ n. Since U |=I
∅, for all S-unit

i	Si	S

U , we can conclude that M ' |=C Θ'. (⇐) Conversely, suppose that M ' |=C
Θ'.

'	'	C	I	C	I	C
Thus, ⟨U1,... , Um⟩ |=C ⟨sen (σ1)(T1),... , sen (σn)(Tn), ∅n+1,... , ∅n+m⟩. From I
and Ui |=I senI(σi)(Ti), for 1 ≤ i ≤ n, modI(σi)(U ') |=I Ti, for 1 ≤ i ≤ n. Thus,
Si	i	Si
M |=C Θ.	 
Presentations over the institution C denote component types. In fact, a com- ponent type C is denoted by a set of interfaces {⟨Si, Θi⟩ | i = 1, 2,... , n}. A #-component N is a model of the component type C iif it is a model over its

underlying presentation C, meaning that N  |=C
Θ, for all Θ ∈ ΘC, written

C	C
N |=C  ΘC.  The category of all component types is denoted by Theo(C), the
category of theories over C. A component type morphism is a presentation mor-
phism Ψ : ⟨SC , ΘC⟩ → ⟨S' , Θ' ⟩ induced from the interface signature morphism
'	C	C	'•
Φ : SC → SC , such that: Θ ∈ ΘC ⇒ Ψ(Θ) ∈ ΘC . A component class repre-
sented by the component type C is defined by the set of models that are free with
respect to the component type (theory) morphism Ψ : ∅ → C, where ∅ denotes the empty component type. In fact, the component class includes the #-components of component type C that are considered initial.
3.3.1	The Subtype Relation for Component Types
Let C1 and C2 component types. The relation C2<:C1 (C1 is a subtype of C2) is defined by an injective function Φ:C1→C2 between its sets of interfaces. Also,

if Φ(I1) = I2, for I1 ∈ C1 and I2 ∈ C2, then I2<:I1. The following theorem is important for the formalization of parameterized and recursive component types.

Theorem 3.8 Let Φa : Cx → Ca and Φb : Cx → Cb be Theo(C)-morphisms, such that Φb is a subtype morphism, i. e., Ca <: Cx. The pushout Φa ⊕ Φb exists.

Proof. Let Cx = ⟨Sx, Θx⟩, Ca = ⟨Sa, Θa⟩, and Cb = ⟨Sb, Θb⟩. It is only necessary to prove the existence of the pushout Φa ⊕ Φb, where Φa:Sx→Sa and Φb:Sx→Sb are the CSig-morphisms underlying Φa and Φb. The pushout Φa ⊕ Φb is a pair
of arrows Φa':Sa→Sa⊕S Sb and Φb':Sb→Sa⊕S Sb, where Sa⊕S Sb is induced from
pushout in Set and ISig, since component type signatures are sets (of interface
signatures). Thus, interfaces in Cx, through Φa and Φb, define equivalence classes of interfaces in the disjoint union of the sets Ca and Cb. Interfaces that belong to the same equivalence class are fused to form a unique interface of Sa⊕Sx Sb, from the computation of pushouts of arrows in Φa and Φb.  It is showed how
such interfaces are calculated in a general case of equivalence class. For instance, let {I1,... , In} be interfaces in Cx mapped by Φa to the same interface J in Ca, i. e., Φa(Ii) = J , for 1 ≤ i ≤ n. Since Φb is a subtype morphism, it is an injective function. Thus, for the same set of interfaces, Φb(Ii) = Ji, where Ji ∈ Cb, for 1 ≤ i ≤ n. The corresponding Theo(I)-morphisms are αi : Ii → J and βi : Ii → Ji, for 1 ≤ i ≤ n. This is the most general case where a set of
interfaces, represented by J, J1,... , Jn, will be fused to form a new interface J¯

in Sa+Sx Sb.  In fact,
J¯ is the vertex of the colimit of the diagram representing

the arrows α1,... , αn, β1,... , βn and their respective source and target objects.
We want to demonstrate that such colimit always exist. For that, we show how to compute it from the computation of pushouts in Theo(I), where one of the arrows is a subtype morphism. Theorem 3.6 has shown the existence of these pushouts. In what follows, we proceed by induction on n. The reader may look
at the diagram in Figure 6 for better visualization of the induction. For n=1 (base), the pushout of the arrows α1:I1→J and β1:I1→J1 exists in Theo(I), since β1 is a subtype morphism in Theo(I) because Φa is a subtype morphism in
Theo(C). It is represented by the triple ⟨J¯(1), α' , β' ⟩. For n=k (hypothesis),
1	1
suppose that the colimit of the diagram with arrows α1,... , αk, β1,... , βk is rep-
resented	by	⟨J¯(k), α' ◦ α' ◦ ... ◦ α' , β' ◦ α2 ◦ ... ◦ αk,... , β' ◦ α3 ◦ ... ◦ αk, β' ⟩.
1	2	k	1	2	k
For	n=k+1	(induction	step),	the	colimit	of	the	dia-
gram	with	arrows	α1,... , αk+1, β1,... , βk+1,	represented	by
⟨J¯(k+1), α' ◦ α' ◦ ... ◦ α'	, β' ◦ α2 ◦ ... ◦ αk+1,... , β' ◦ α3 ◦ ... ◦ αk+1, β'	⟩,
1	2	k+1	1	2	k+1
can be computed from the colimit of the diagram with arrows α1,... , αk, β1,... , βk
(dotted lines), whose existence is ensured by the induction hypothesis, again by
application of a pushout of two Theo(I)-morphisms, where one of them is a sub-

type morphism. For instance, the arrows β'
:Jk+1→J¯(k+1) and α'
:J¯(k)→J¯(k+1)

'	k+1
k+1

'
comes from the pushout βk+1 ⊕ (α1 ◦ ... ◦ α ◦ αk+1), as depicted in Figure 6, where
βk+1 is known to be a subtype morphism.	 



LSSolve[Sparse,
GaussSeidel[a <: GSApproach], e <: Environment]
Component


LSSolve[Sparse,
Multigrid,
e <: Environment]
LSSolve[Sparse,





LSSolve[Dense,
Jacobi,



component type
(initial object)

Class
LSSolve[Sparse,
GaussSeidel[Wavefront],

SOR,
e <: Environment]
e <: Environment]
LSSolve[Dense, LSSolve[Dense, Jacobi,

LSSolve[Sparse,  e <: Environment]
LSSolve[Dense,
Jacobi, MPI[Cluster]]

GaussSeidel[RedBlack], e <: Environment]
LSSolve[Dense,
Jacobi, Globus]

Fig. 7. Component Class for LSSolver
Jacobi, MPI[Grid]]
OpenMP]


Skeletal Programming and Parameterized Component Types
The abstraction of skeletons has been widely studied in the context of parallel pro- gramming research since the beginning of 1990’s, after the seminal work of Murray Cole [15]. Skeletons attempt to capture common patterns of parallel computation, whose implementation may be tuned to specific architectures. In [16], a retrospect of the research on skeletal programming is presented, focusing on the analysis of the reasons that have made difficult the dissemination of skeletal programming in widespread programming environments. This paper adopts a general characteriza- tion of skeletons, by relating them to parameterized abstract component types, or simply parameterized component types.
The simpler form of abstraction in # programming resembles routine calls from linear algebra libraries [19]. For example, a user of the #-component LSSolver,a linear system solver comprising N homogenous units, does not need to be aware of synchronization operations performed when one of its units are activated. Thus, a version of LSSolver whose implementation takes advantage of peculiar features of the target parallel architecture may be chosen dynamically, at startup time. More- over, implementations for LSSolver may be adapted to specific processor topolo- gies, and density properties of the characteristic matrix. Such kind of abstraction will be approached in # programming systems by means of abstract component types, with existential quantification semantics [37]. Thus, instead to deal directly with overlapping composition of #-components, programmers may form applica- tions by combining abstract component types to form new abstract component types. In addition, primitive #-components may be developed and declared to in- habit a given abstract component type for a given execution context. At the startup of a deployed abstract component type, a #-component is formed using the tuned versions for the current execution environment and context. Figure 7 illustrates a component class for LSSolver.
# programming also supports skeletal programming through parametrization of abstract component types that denote common parallel interaction patterns, such as the skeletons proposed by Cole [15] and many others proposed in several sub- sequent works [10,38]. Such skeletal component types are inhabited by skeleton implementations (#-components) tuned for each parallel architecture. The compo-

nent perspective of skeletons in # programming is inspired by partial topological skeletons of Haskell# [12], a previous work of the authors that proposed skeletons inspired in collective communication operations of MPI that could be combined by nesting and overlapping. Such idea is very similar to skeletons of eSkel [16], which do not allow skeleton composition. More recently, HOC’s (Higher-Order Components)[2] have also been proposed to meet skeletons and components, firstly implemented in the Proactive/Fractal framework for Web Services programming [21]. The parameterized component types have been formalized using categorical constructions over the institution C, as following.
Definition 3.9 [Parameterized Component Type]
A  parameterized  component  type  is  defined  by  a  cocone  C[X]  =
⟨C, α : X → ΔC⟩, where X : I → Theo(C) is the basis diagram of the cocone and ΔC : I → Theo(C) is the constant diagram over C.
Let C be a component type. The diagram X, with shape I, denotes a subset of the component types that form C that are set to be parameters of C, denoted by C[X]. The parameters are denoted as Xi, for i ∈ I. Informally, a parameter Xi may be instantiated by any component Ci, that belongs to another diagram C with
the same shape I, for which there is a Theo(C)-morphism Φi : Xi → Ci, such that Ci <: Xi (Ci is subtype of Xi). The choice of a suitable notion of subtyping relation is one of the current research topics with component types in # programming. This paper abstracts from a particular subtype notion.
Definition 3.10 [Parameter Instantiation]
Let C[X]= ⟨C, α : X → ΔC⟩ be an arrow component type with shape I. Let P : I → Theo(C) be a diagram, and v : X → P be a diagram morphism (application). The morphism v specifies the substitutions for the component type variables in X by component types in P . The application of C[X] through v is the component type C[v], the pushout in the diagram of Figure 9(a).
The nomenclature adopted emphasizes that C[v] is uniquely determined from the parameterized component type C[X] through the instantiation v. The existence of pushout C[v] is ensured by the finite cocompleteness of the institution C.
Recursive Component Types
Recursive component types are useful to describe self-similar process topologies that are common in parallel programs. Figure 8 presents the recursive parameterized component type TreeFilterDivide, whose signature comprises three interfaces: ISendTree, denoting a unit (process slice) that successively splits input data of type D1 in a pair of data chunks of the same type, transmitting each final chunk to a unit of interface ICompute; ICompute, which denotes units that takes a chunk of data of type D1 as input and maps it to a chunk of data of type D2 as output; and IRecvTree, which denotes units that receive a set of chunks of data of type D2 and join them in a single chunk of the same type, by successive application of a joining function over pairs of data chunks. TreeFilterDivide has five formal parameters: D1 and



(a)

(b)

(c)
Fig. 8. The Recursive Topology of the TreeFilter Component Type

D2 specifies the input and output data types; S specifies the splitting function over data chunks of type D1; J specifies the joining function over data chunks of type D2; P specifies a function for transforming a chunk of data D1 to a chunk of type D2. In fact, the resultant recursive parameterized component type is a skeleton for a well-known divide-and-conquer pattern of parallelism. Figure 8(a) illustrates the recursive configuration of TreeFilterDivide. The component type TreeFilter is configured to be the recursion basis. This is possible because TreeFilterDivide and TreeFilterConquer are both subtypes of TreeFilter, making possible to apply TreeFilterDivide recursively over TreeFilter until the end of the recur- sion is reached, when TreeFilterConquer is finally applied over TreeFilter.
Figure 8(b) presents a recursive unfolding of TreeFilterDivide, with depth 3.
In Figure 8(c), it is presented the resultant hierarchy for interface signatures of ISendTree, from an recursive unfolding with depth 4, and ICompute. The protocols of ISendTree and ICompute (labelled Petri nets) are illustrated in the figure. In the signature of ISendTree, the leave nodes (black circles) are units slices that represent data structures of type D1 and D2. The reader may be asking about how recursion basis is reached when unfolding a configuration of a recursive component type. In fact, this is a concern of # programming systems and could be resolved dynamically, at startup or execution time.


	
(a)	(b)
Fig. 9. Diagrams for Parameterized and Recursive Component Types
Let Theo(C)<: be the wide subcategory of Theo(C) restricted to subtype mor- phisms. For formalizing recursive component types, some usual categorical ma- chinery is needed. For instance, let Rec(C) = Δ4 ↑ (Δ ↑ (id ↑ id)), where ↑ denotes the comma category, Δ denotes the diagonal functor over Theo(C)<:, id is the identity functor over Theo(C), and Δ4 denotes the four-dimensional diag- onal functor over Theo(C)<:. For convenience, we represent a Rec(C)-object by O = ⟨f0:K→C0, f1:K→C, f2:K→C, f :K→D⟩. Also, it is defined the endofunctor F over Rec(C), such that F(⟨f0, f1, f2,f ⟩) = ⟨f0, f1, f2, Θ(⟨f0, f1, f2,f ⟩)⟩, where
Θ(⟨f0, f1, f2,f ⟩) = f2 ◦ (f1 ⊕ f ) + f0 and (f ⊕ f ') denotes the left arrow of the
'	r	r

pushout between f and f
subtyping relation.
in Theo(C), whose existence may be ensured by a suitable

Definition 3.11 [Recursive Component Type] The fixed point of F, denoted by
μ(F), denotes the category of recursive component types over Theo(C).
Figure 9(b) illustrates the construction of a recursive component type from the arrows f0 : K → C0, f1 : K → C, and f2 : K → C. It is important to notice that f0 and f2 are Theo(C)<:-morphisms, denoting subtyping relationships. C0 denotes the basis component type of the recursion, while C is the component type where the recursion is applied. In the proposed example, TreeFilterConquer is C0, while TreeFilterDivide is C. K denotes a component type that is in the structure of C, through f1, but which is also a subtype of C0 and C, through f0
and f2, respectively. In the example, TreeFilter is K. Thus, C may be applied recursively over K in the structure of C until the recursion basis is reached, when C0 is finally applied over K.
The Relation of # Programming with Aspects, Hyperspaces, and Features
Since the # component model is mainly focused on separation of concerns that cross cut the processes of a parallel program, the reader may consider it helpful to understand the relation between modularity in # programming and modularity in some recently proposed artifacts for separation of cross-cutting concerns in software, such as aspects [29], hyperspaces [35], features [4], and so on. For instance, aspects may be encapsulated in #-components. In this perspective, joinpoints are the ex- ecution points before, after, and around activation of slices in protocols. Advices correspond to the interleaving of activations of slices from #-aspects (aspect slices) with other slices in protocols. Weaving is related to the process of composition of a parallel program from the overlapping composition of a set of #-components that

represent components, in the usual sense, and aspects. However, modularity in # programming is closer to mechanisms of multi-dimensional separation of concerns, where hyperspaces and features are included. In fact, a hyperslice that cross cuts a set of classes is like a #-component that cross cuts a set of processes, in such a way that a unit of a hyperslice corresponds to a unit of a #-component. The combination of hyperslices to form hypermodules is captured by overlapping com- position of #-components, where hyperslices and hypermodules are both viewed as #-components. The relation of features with aspects and hyperspaces have been detailed studied in [32]. Features are closer to hyperspaces, by making refinements correspondent to hyperslices. Transitively, it is easy to see the relation between features and #-components. Also, the idea of synthesizing efficient programs that meet some specification by looking for the best implementation of features has a clear relation to the idea of skeletal programming through existential polymorphism with component types in # programming.

4	Conclusions and Lines for Further Works
This paper presented an institutional theory for #-components. Besides to provide important insights on the nature of #-components and to allow proving important properties about them, such approach has also the advantage of introducing abstrac- tion mechanisms from algebraic specification and type theory into # programming systems. For instance, this leads to the idea of supporting skeletal programming using recursive polymorphic component types.
Work on progress address some pragmatic issues regarding the implementation of type systems for #-components, such as decidability and generality issues that always rise when talking about type systems supporting bounded quantification [37]. Besides that, it is intended to extend the institutional framework presented in this paper to deal with specific kinds of concerns. As discussed along the paper, the current approach abstracts from the nature of concerns addressed by #-components. For example, a simple approach to support functional concerns could be to enrich the specification of units of functional component types with pre-conditions, post- conditions, and invariants, trying to apply specification matching techniques [45] for verification of combination of units in the overlapped composition of #-components. Such approach suggests the application of notions of behavioral subtyping [31] onto component type systems.

References
Allan, B. A., R. C. Armstrong, A. P. Wolfe, J. Ray, D. E. Bernholdt and J. A. Kohl, The CCA Core Specification in a Distributed Memory SPMD Framework, Concurrency and Computation: Practice and Experience 14 (2002), pp. 323–345.
Alt, M., J. Du¨nnweber, J. Mu¨ller and S. Gorlatch, HOCs: Higher-Order Components for Grids, in:
Workshop on Component Models and Systems for Grid Applications (in ICS’2004) (2004).
Armstrong, R., D. Gannon, A. Geist, K. Keahey, S. Kohn, L. McInnes, S. Parker and B. Smolinski, Towards a Common Component Architecture for High-Performance Scientific Computing, in: The 8th IEEE International Symposium on High Performance Distributed Computing (1999).


Batory, D., J. N. Sarvela and A. Rauschmayer, Scaling Step-Wise Refinement, in: Proceedings of the International Conference on Software Engineering, 2003.
Baude, F., D. Caromel and M. Morel, From Distributed Objects to Hierarchical Grid Components, in:
International Symposium on Distributed Objects and Applications (2003).
Bednarczyk, M. A. and A. Borzyszkowski, General Morphisms of Petri Nets, Lecture Notes in Compute Science 1113 (1999), pp. 190–199.
Bernholdt D. E., J. Nieplocha and P. Sadayappan, Raising Level of Programming Abstraction in Scalable Programming Models, in: IEEE International Conference on High Performance Computer Architecture (HPCA), Workshop on Productivity and Performance in High-End Computing (P-PHEC), Madrid, Spain (2004), pp. 76–84.
Bramley, R., R. Armstrong, L. McInnes and M. Sottile, High-Performance Component Software Systems, SIAM 49 (2005), p. .
Bruneton, E., T. Coupaye and J. B. Stefani, Recursive and Dynamic Software Composition with Sharing, in: European Conference on Object Oriented Programming (ECOOP’2002) (2002).
Campbell, D. K. G., Towards the Classification of Algorithmic Skeletons, Technical Report YCS 276, Department of Computer Science, University of York (1996).
URL citeseer.nj.nec.com/campbell96towards.html

Carvalho Junior, F. H. and R. D. Lins, Haskell#: Parallel Programming Made Simple and Efficient, Journal of Universal Computer Science 9 (2003), pp. 776–794.
Carvalho Junior, F. H. and R. D. Lins, Topological Skeletons in Haskell#, in: International Parallel and Distributed Processing Symposium (IPDPS) (2003), 8 pages.
Carvalho Junior, F. H. and R. D. Lins, Separation of Concerns for Improving Practice of Parallel Programming, INFORMATION, An International Journal 8 (2005).
Chiu, K., “An Architecture for Concurrent, Peer-to-Peer Components,” Ph.D. thesis, Department of Computer Science, Indiana University (2001).
Cole, M., “Algorithm Skeletons: Structured Management of Paralell Computation,” Pitman, 1989.
Cole, M., Bringing Skeletons out of the Closet: A Pragmatic Manifesto for Skeletal Parallel Programming, Parallel Computing 30 (2004), pp. 389–406.
Dongarra, J., Trends in High Performance Computing, The Computer Journal 47 (2004), pp. 399–403.
Dongarra, J., I. Foster, G. Fox, W. Gropp, K. Kennedy, L. Torczon and A. White, “Sourcebook of Parallel Computing,” Morgan Kauffman Publishers, 2003 .
Dongarra, J., I. Foster, G. Fox, W. Gropp, K. Kennedy, L. Torczon and A. White, “Sourcebook of Parallel Computing,” Morgan Kauffman Publishers, 2003.
Dongarra, J., S. W. Otto, M. Snir and D. Walker, A Message Passing Standard for MPP and Workstation, Communications of ACM 39 (1996), pp. 84–90.
Du¨nnweber, J., Gorlatch S., F. Baude, L. Legrand and N. Parlavantzas, Towards Automatic Creation of Web Services for Grid Component Composition, in: CoreGRID Workshop on Grid Systems, Tools and Environments, 2005.
Fiadeiro, J. L., “Categories for Software Engineering,” Springer, 2005.
Geist, G., A. Beguelin, J. Dongarra, W. Jiang, R. Manchek and V. S. Sunderam, PVM: Parallel Virtual Machine - A User’s Guide and Tutorial for Networked Parallel Computing, MIT Press, Cambridge (1994).
Goguen, J., Higher-Order Functions Considered Unnecessary for Higher-Order Programming, in: D. A. Turner, editor, Research Topics in Functional Programming, Addison-Welsey, Reading, MA, 1990 pp. 309–351.
URL citeseer.ist.psu.edu/goguen90higher.html

Goguen, J., Types as Theories, in: G. M. Reed, A. W. Roscoe and R. F. Wachter, editors, Topology and Category Theory in Computer Science, Oxford, 1991 pp. 357–390.
URL citeseer.ist.psu.edu/503432.html
Goguen, J., “Logica Universalis - Towards a General Theory of Logic,” Springer, 2005 pp. 113–133.

Goguen, J. and R. Burnstal, Institutions: Abstract Model Theory for Specification and Programming, Journal of ACM 39 (1992), pp. 95–146.
Gorlatch, S., Send-Recv Considered Harmful? Myths and Truths about Parallel Programming, ACM Transactions in Programming Languages and Systems 26 (2004), pp. 47–56.
Kiczales, G., J. Lamping, Menhdhekar A., Maeda C., C. Lopes, J. Loingtier and J. Irwin, Aspect- Oriented Programming, , 1241 (1997), pp. 220–242.
Lau, K., P. V. Elizondo and Z. Wang, Exogenous Connectors for Software Components, Lecture Notes in Computer Science (Proceedings of 2005 International SIGSOFT Symposium on Component-Based Software Engineering - CBSE’2005) 3489 (2005), pp. 90–108.
Leavens, G. T., Concepts of Behaviral Subtyping and a Sketch of Their Extension to Component-Based Systems, in: Foundations of Component Based Systems (2000), pp. 113–135.
Lopez Herrejon, R. E., D. Batory and W. Cook, valuating Support for Features in Advanced Modularization Technologies, Lecture Notes in Computer Science (Proceedings of ECOOP’2005) 3586 (2005), pp. 169–194.
Mahmood, N., G. Deng and J. C. Browne, Compositional development of parallel programs., in:
L. Rauchwerger, editor, LCPC, Lecture Notes in Computer Science 2958 (2003), pp. 109–126.
Milli, H., A. Elkharraz and H. Mcheick, Understanding Separation of Concerns, in: Workshop on Early Aspects - Aspect Oriented Software Development (AOSD’04), 2004, pp. 411–428.
Ossher, H. and P. Tarr, Multi-Dimensional Separation of Concerns and the Hyperspace Approach, in: Proceedings of the Symposium on Software Architectures and Component Technology: The State of the Art in Software Development (2000), University of Twente, Enschede, The Netherlands.
Perry, D. E., The Inscape Environment, in: The Proceedings of the 11th International Conference on Software Engineering (1989).
Pierce, B., “Types and Programming Languages,” The MIT Press, 2002.
Rabhi, F. A. and S. Gorlatch, “Patterns and Skeletons for Parallel and Distributed Computing,” Springer, 2002.
Shaw, M., Procedure Calls are the Assembly Language of Software Interconnection: Connectors Deserve First-Class Status, in: International Workshop on Studies of Software Design, Lecture Notes in Computer Science (1994).
Skjellum, A., P. Bangalore, J. Gray and Bryant B., Reinventing Explicit Parallel Programming for Improved Engineering of High Performance Computing Software, in: International Workshop on Software Engineering for High Performance Computing System Applications (2004), pp. 59–63, Edinburgh.
Thompson, S., “Haskell, The Craft of Functional Programming,” Addison-Wesley Publishers Ltd., 1996.
Tip, F., A Survey of Program Slicing Techniques, Journal of Programming Languages 3 (1995), pp. 121– 189.
Trinder, P. W., H.-W. Loidl and R. F. Pointon, Parallel and Distributed Haskells, Journal of Functional Programming 12 (2002), pp. 469–510.
van der Steen, A. J., Issues in Computational Frameworks, Concurrency and Computation: Practice and Experience 18 (2006), pp. 141–150.
Zaremsky, A. M. and J. M. Wing, Specification Matching of Software Components, in: 3rd ACM SIGSOFT Symposium on the Foundations of Software Engineering, 1995.
