
FULL-LENGTH ARTICLE
Classification of Clustered Microcalcifications using MLFFBP-ANN and SVM

Baljit Singh Khehra a,*, Amar Partap Singh Pharwaha b,1

a Department of Computer Science & Engineering, Baba Banda Singh Bahadur Engineering College, Fatehgarh Sahib 140407,
Punjab, India
b Department of Electronics and Communication Engineering, Sant Longowal Institute of Engineering and Technology, Longowal 148106, Sangrur, Punjab, India

Received 26 March 2015; revised 26 July 2015; accepted 16 August 2015
Available online 14 September 2015

Abstract The classifier is the last phase of Computer-Aided Diagnosis (CAD) system that is aimed at classifying Clustered Microcalcifications (MCCs). Classifier classifies MCCs into two classes. One class is benign and other is malignant. This classification is done based on some meaningful features that are extracted from enhanced mammogram. A number of classifiers have been proposed for CAD system to classify MCCs as benign or malignant. Recently, researchers have used Artificial Neu- ral Networks (ANNs) as classifiers for many applications. Multilayer Feed-Forward Backpropaga- tion (MLFFB) is the most important ANN that has been successfully used by researchers to solve various problems. Similarly, Support Vector Machines (SVMs) belong to another category of classi- fiers that researchers have recently given considerable attention. So, to explore MLFFB and SVM clas- sifiers for MCCs classification problem, in this paper, Levenberg-Marquardt Multilayer Feed- Forward Backpropagation ANN (LM-MLFFB-ANN) and Sequential Minimal Optimization (SMO) based SVM (SMO-SVM) are used for the classification of MCCs. Thus, a comparative evalu- ation of the relative performance of LM-MLFFBP-ANN and SMO-SVM is investigated to classify MCCs as benign or malignant. For this comparative evaluation, first suitable features are extracted from mammogram images of DDSM database. After this, suitable features are selected using Particle Swarm Optimization (PSO). At the end, MCCs are classified using LM-MLFFBP-ANN and SMO- SVM classifiers based on the selected features. Confusion matrix and ROC analysis are used to




* Corresponding author. Tel.: +91 9463446505.
E-mail addresses: baljitkhehra@rediffmail.com (B.S. Khehra), amarpartapsingh@yahoo.com (A.P.S. Pharwaha).
1 Tel.: +91 9463122255.
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
http://dx.doi.org/10.1016/j.eij.2015.08.001
1110-8665 © 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).


measure the performance of LM-MLFFBP-ANN and SMO-SVM classifiers. Experimental results indicate that the performance of SMO-SVM is better than that of LM-MLFFBP-ANN.
© 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).



Introduction

Breast cancer that occurs among women in both developed and developing countries is one of the most dangerous dis- eases. It is difficult to prevent it but early detection is the key for reducing the mortality rate. Mammography is one of the most effective imaging techniques for early detection of breast cancer [1]. Clusters of Microcalcifications (MCCs), mass lesions, distortion in breast architecture and asymmetry between breasts are various types of breast abnormalities that are partially detected from mammograms. Clusters of Micro- calcifications (MCCs) are the most frequent symptoms of Duc- tal Carcinoma in Situ (DCIS). DCIS is one of various types of breast cancers [2]. Although mammography is frequently used in both developed and developing countries for breast cancer detection, but un-correct reading of mammogram is a prob- lem. This type of problem is occurred due to human error. Un-correct readings of mammogram are called false positive and false negative readings of mammogram. Due to false pos- itive detection, the need of unnecessary biopsy occurs while due to false negative detection, an actual tumor remains unde- tected. Thus, false positive and false negative readings of mam- mogram are main causes of unnecessary biopsy and missing the best treatment time. In fact, the need of the hour is to develop Computer-Aided Diagnosis (CAD) system for improving diagnosis accuracy of early breast carcinoma which would prevent unnecessary biopsy and not miss the best treat- ment time. The classifier is the last phase of CAD system that is aimed to classify MCCs as benign or malignant. For this, first mammogram images are enhanced. After this, features are extracted from enhanced mammogram. Then, a suitable set of features is searched from extracted features. At the end, classifiers classify MCCs based on suitable set of features. Recently, various researchers have applied a variety of classi- fiers for CAD system to classify MCCs as benign or malignant.


Experiments are performed on mammogram images of DDSM database [17].

Artificial Neural Network

An ANN is a computational model that is commonly used in situations when the knowledge is not properly defined and there is a need to solve non-linear complex problems. Multi- layer Feed-Forward Neural Network trained with Backpropa- gation algorithm is widely used for non-linear classification problems [18]. Multilayer Feed-Forward Backpropagation Artificial Neural Network (MLFFBP-ANN) is treated as a nested sigmoid scheme. Therefore, the following equation is used to represent the output function of ANN [19]:
F(xi)= FN(WN * (FN—1(... F2(W2 * F1(W1 * x1 + B1)
+ B2) .. .)+ BN—1)+ BN)	(1)
where N is total layers of Artificial Neural Network; B1, B2,
.. ., BN are the bias vectors; W1, W2, .. ., WN are the weight vectors and F1, F2, .. ., FN are activation transfer functions of layers.
In fact, studies on ANNs [20,21] have highlighted that the most often used ANN architecture is feed-forward network designed around multilayer topologies with Backpropagation learning algorithm. In the feed-forward ANN, a neuron trans- fers data to the neuron of the next layer through a function called activation function that may be the sigmoid or linear one [22]. When sigmoid activation function is used, the output of the jth neuron is calculated as
1
zj = 1 + e—r(z inj )	(2)
where z inj is the input of the jth neuron received from the neu- rons of the previous layer calculated as
Xn



tures to classify MCs in digitized mammograms using K-
Nearest Neighbor (KNN) classifier. Bruce and Adhami [4] classified mammographic masses into stellate, nodular and round using Linear Discriminant Analysis (LDA) classifier. Bottema and Slavotinek [5] classified lobular and DCIS (small cell) MCs in digital mammograms using decision trees. In 2007, Bayesian network classifiers are used by Nicandro et al. [6] for the diagnosis of breast cancer. Fuzzy rough sets hybrid scheme is used by Hassanien [1] for breast cancer detec- tion. For classification of MCs as benign or malignant, Artifi- cial Neural Networks (ANNs) have been widely used [7–9]. Multilayer Feed-Forward Backpropagation (MLFFB) is the most important ANN that has been applied successfully to solve many problems [10–12]. Support Vector Machines (SVMs) have also been recently used to solve many problems [2,13–16]. SVMs are based on statistical learning theory. In the proposed research work, LM-MLFFBP-ANN and SMO- SVM are explored to classify MCCs as benign or malignant.
where xi is the output of ith neuron of the previous layer; n is the total number of neurons in the previous layer; wij is the connection weight of the jth neuron with ith neuron of the pre- vious layer; bj is the jth neuron bias and r defines the steepness of the sigmoid activation function.
In the learning phase, wij and bj values are updated using the following equations [23]:
wij(new)= wij(old)+ Dwij	(4)
bj(new)= bj(old)+ Dbj	(5)
Dwij = adjxi	(6)
Dbj = adj	(7)
where a is the learning rate and d is correction factor. The per- formance of ANN is evaluated by Mean Square Error (MSE) that is defined as


MSE =
1 XL Xm

(tk — yk)2

(8)
voj(new)= voj(old)+ Dvoj	(13)

L k=1 j=1
where L is the number of training pairs; m is the number of neurons in the output layer; yk and tk are the actual and target
Dvoj = adj	(14)
wjk(new)= wjk(old)+ Dwjk	(15)

j	j

outputs at jth neuron for kth training pair.

2.1. MLFFBP-ANN for classifying MCCs

Classification of MCCs as benign or malignant classes is a two- class pattern classification problem. Let Xj ∈ [x1; x2; x3; ... ; xi;
... ; xn] be a set of features extracted from mammograms that acts as input vector for MLFFBP-ANN and Yj ∈ [0; 1] be out- put vector of MLFFBP-ANN. ‘0’ represents benign MCCs
j
Dwjk = adkzj	(16)
wok(new)= wok(old)+ Dwok	(17)
Dwok = adk	(18)
where dk is factor that is used to update weights wjk and dj is a factor that is used to update weights vij. Using Eq. (8), MSE for the proposed model is calculated as
L

and ‘1’ represents malignant MCCs. Let M be one training


MSE =  1 X[Y — F	(X )]2	(19)




vector and number of hidden neurons (p) is chosen experimen- tally. Number of neurons in output depends upon output vec- tor. Features are used as inputs of the neurons of input layer. Thus, n is number of features extracted from MCCs. Classifi- cation of MCCs is a two-class pattern classification problem. So, output layer has only one neuron to represent binary out- put. Neurons are interconnected with each other and a weight is assigned to each link for representing the link-strength between the neurons. In order to classify MCCs as benign or malignant, an attempt is made here to implement a MLFFBP-ANN based classifier to classify MCCs as benign or malignant. An algorithm is written using MATLAB pro- gramming. The implementation neural model comprises of a hidden layer of sigmoidal neurons that receives numeric values of features and broadcasts output values to a layer of linear neurons, which finally computes the network output. Using Eq. (2), the output of jth neuron of hidden layer for the pro- posed model is computed by
During training, a set of numeric values of features of MCCs corresponding to the MCCs category is used to update the weights and biases of the neurons to minimize the output neuron error. However, the best ANN structure is not known in advance [18]. The best ANN structure depends upon the number of hid- den layers, number of neurons in each hidden layer, activation function, learning algorithm and training parameters. To train MLFFBP-ANN, various training algorithms are available [24]. In the proposed research work, Levenberg-Marquardt training algorithm is considered to train MLFFBP-ANN for characterization of MCCs as benign or malignant.

Support Vector Machine

SVM is a two-class classifier developed by Vapnik [25]. Learn- ing of SVM is supervising learning that is based on statistical learning theory. Basic principal of SVM is structural risk min-


zj = tan sig voj +

n


i=1
xivij!

(9)
imization. Structural risk minimization means to get a low error rate on unseen data set (outside training data set). For non-linear classification problems, kernel function based

where n is the total number of neurons in input layer; voj is the bias of jth neuron of hidden layer; vij is the weight between ith neuron of input layer and jth neuron of hidden layer and tan sig(s_) is a hyper-tangent sigmoid activation transfer function. The output of the proposed MLFFBP-ANN model is com- puted as
SVM is used. Kernel function converts non-linear classifica- tion problem to linear classification problem through mapping the input feature space to higher dimensional feature. After this, an optimal separating hyperplane is used to separate the two classes of the two-class pattern classification problem.
For classification of MCCs as benign or malignant, a set of L
training data samples is considered. Such set is denoted as


FANN(Xj)= purelin  wok +
Xj=1
zjwjk!

(10)
{(Xj; Yj); j = 1; 2; ... ; L}, where Xj is input data sample that belongs to class Yj ∈ {+1; —1}. Input data sample is represented
by a vector X ∈ {x ; i = 1; 2; ... ; n} in which {x ; i = 1; 2; ... ; n}

i	i

where p is the total number of neurons in hidden layer; wok is the bias of kth neuron of the output layer (in the proposed model, k = 1 because output layer has only one neuron); wjk is the weight between jth neuron of hidden layer and kth neu- ron of the output layer; and purelin(s_) is linear activation trans- fer function.
The proposed MLFFBP-ANN model updates weights and biases values by means of an adaptive process which minimizes the output neurons errors using Eqs. (4)–(7) as follows:
vij(new)= vij(old)+ Dvij	(11)
Dvij = adjxi	(12)
is a set of n features of the cluster of MCs and output is repre- sented as Y ∈ {+1; —1}, where ‘+1’ is for malignant cluster of MCs and ‘—1’ is for benign cluster of MCs. For separating the positive and negative classes, a separating hyperplane is used. Separating hyperplane should be optimal for correct classifica- tion of positive and negative classes. From the above discussion, the formulation of optimization problem to find optimal sepa- rating hyperplane can be stated as
wTw
Target : Minimize
2
Constraints : Yj(wTXj + b) P 1  for 6j


where w is the norm to the hyperplane;  |b|  is the perpendicular distance from the origin to hyperplane to the origin.
Thus, the main objective is to find w and b for minimization
where non-zero Lagrange multipliers, a*; j = 1; 2; ... ; Ls, indi- cate their corresponding support vectors Sj ∈ (Xj; Yj). Thus, the following equation is used to classify a test example Z as

of wTw along with the satisfaction of constraints. The optimal values w* and b* are used to classify a test example Z as

class(Z)= sign

Ls
a*YjXTZ + b*

(27)

follows:
class(Z)= sign(w*TZ + b*)	(20)
The above defined problem belongs to Quadratic Program- ming (QP) optimization problem with linear constraints. Lagrangian formulation of the above said problem [26] is required to solve it. In Lagrangian formulation, objective func- tion is defined as
j	j
j=1


Karush–Kuhn–Tucker condition

Lagrangian formulation of the problem (LP) is a convex min- imum QP optimization problem. Property of convex minimiza- tion problem is as follows: if a local minimum exists, then it is a global minimum [27]. Karush–Kuhn–Tucker condition [28]
is sufficient when objective function is convex and solution

LP =
1 wTw —
2
L


j=1
ajYj(wTXj + b)+ 
L


j=1
aj	(21)
space is also convex. According to this condition, gradient of the objective function of Lagrangian problem w.r.t w and b

Target: Minimize LP w.r.t w, b
Constraints:
vanishes and multiplication of each Lagrangian multiplier with corresponding constraint is also zero for all Lagrangian multi- pliers greater than or equal to zero [29,30].

Derivatives of LP
with respect to all aj vanish
@L	L
= 0 ⇒ w =	a Y X
(28)

aj P 0	@w
Dual formulation of the above primal problem is written as
j  j  i
j=1

L

Target: Maximize L



@LP = 0 ⇒ Xa Y = 0	(29)




Gradient of LP w.r.t w and b vanishes
aj P 0

When gradient of LP w.r.t w and b is vanished, then the fol- lowing conditions are occured:
XL
aj(Yj(wTXj + b)— 1)= 0  6j	(30)

aj P 0  6j	(31)
The above equations are used to obtain optimal values of
w* and b*.

w =
j=1

L
ajYjXj	(22)
Non-linear SVM classifier

For solving non-linear classification problems, non-linear

ajYj = 0	(23)
j=1

From Eqs. (21)–(23), the following equation is obtained:
SVM classifiers are used through kernel functions. Kernel function maps training input data of input space Rd onto a higher dimensional feature space H using transformation oper- ator /(·). This is done to separate training data points into two

L
LD =
aj —
1 XL XL
ajaiYjYiXTXi	(24)
classes by a hyperplane [29].
d

j=1
j=1 i=1
/ : R → H	(32)

Thus, new formulation of the problem is obtained as Target: Maximize LD
Constraints:
Relation between kernel function K (Xj, Xi) and mapping operator /(·) [31] is shown as
K(X ; X )= /(X )T/(X )  6X ; X ∈ Rd	(33)

(a) PL
ajY j = 0
Thus, dual form of the problem can be formulated as

(b) aj P 0
Thus, the main objective is to find a1, a2, .. ., aL for maxi- For non-zero a*; j = 1; 2; ... ; Ls, the optimal values of w*
and b* are obtained as follows:
follows:
Find  a ,  a ,  .. .,  a	such  that  L
PL a a Y Y K(X ; X ) is maximized and
PL  ajY j = 0
= PL
a — 1 PL


w* =

XLs

a*YjXj	(25)
0 6 aj 6 C for 6aj

For non-zero a*; j = 1; 2; ... ; L , w* is calculated from Eq.


j=1
j


XLs

	


j	s
(25) and b* is obtained as follows:
XLs






Thus, a test example Z is classified as
Measures for classifier accuracy


class(Z)= sign
Ls j=1
a*YjK(Xj; Z)+ b*!

(35)

Confusion matrix [35] and ROC analysis [36] are two measures that are commonly used to find the accuracy of classifier to classify MCCs as benign and malignant. A confusion matrix

A most commonly used kernel function in SVM [32,33] is linear that is defined as follows:
K(x; y)= xTy	(36)
3.3. Sequential Minimal Optimization for SVM

Sequential Minimal Optimization (SMO) [34] decomposes SVM-QP problem into QP sub-problems. At each step, the smallest possible optimization problem is selected for solving. For this, at each step, SMO selects two Lagrange multipliers. This is done to find the optimal values for Lagrange multipliers and SVM is updated to reflect the new optimal values. First, a Lagrange multiplier (a1) is selected that violates the Karush– Kuhn–Tucker condition [28] for the optimization problem. After this, second Lagrange multiplier (a2) is selected and opti- mizes the pair (a2, a2). This process is repeated to achieve the convergence. The main advantage of SMO is that two Lagrange multipliers are solved analytically instead of entirely numerical QP optimization. In addition, no extra matrix is required for storage at all.
evaluates the accuracy of classifier based on the actual and pre- dicted classifications done by a classifier. For a classifier and an instance, possible outcomes are four: true positive, true neg- ative, false positive and false negative. True positive is a cor- rect judgment of classifier about a malignant cluster of MCs while true negative is a correct judgment of classifier about a benign cluster of MCs. Similarly, false positive is a wrong judg- ment of classifier about a benign cluster of MCs while false negative is a wrong judgment of classifier about a malignant cluster of MCs. These possible outcomes of a classifier are shown in Table 1. Such table is called confusion matrix.
ROC analysis is another measure that is used to find the accuracy of classifier related to medical decision. In ROC anal- ysis, ROC curve is plotted to measure the accuracy of classifier. To plot ROC curve, 1-Specificity is taken along x-axis while Sensitivity is taken along y-axis and at various threshold set- tings, the curve is generated by plotting the Sensitivity against the 1-Specificity. The meaning of 1-Specificity is False Positive Rate while Sensitivity is True Positive Rate.
In case of confusion matrix, accuracy of classifier is found by using the following equation:
True Positive Rate and False Positive Rate are defined as

TPs
True Positive Rate (Sensitivity)= 
TPs + FNs
FPs

(37)

False Positive Rate (1-Specificity)= TNs + FPs	(38)
where TPs are number of true positive decisions taken by classifier; TNs are number of true negative decisions taken


16	B.S. Khehra, A.P.S. Pharwaha




















Figure 1	ROC curves of 1st and 10th random experimental trials of LM-MLFFBP classifier for classifying MCCs as benign and malignant.



























Figure 2	Confusion matrices of 1st and 10th random experimental trials of LM-MLFFBP classifier for classifying MCCs as benign and malignant.


by classifier; FNs are number of false negative decisions taken by classifier and FPs are number of false positive decisions taken by classifier.
In case of confusion matrix, accuracy of classifier is found by using the following equation:
According to Hosmer and Lemeshow [38], classifiers are divided into the following four categories based on the Accuracy:
If 0.5 6 Accuracy < 0.6, then classifier is called fail classifier

Accuracy =	TPs + TNs
TPs + FPs + TNs + FNs
(39)
If 0.6 6 Accuracy < 0.7, then classifier is called poor classifier
If 0.7 6 Accuracy < 0.8, then classifier is called fair

In case of ROC analysis, the area under the ROC curve
(AZ) is used to measure the accuracy of classifier [37]. AZ is in the range between 0.0 and 1.0. So, AZ lies between 0.0 and 1.0. For 100% accuracy, AZ should be 1.0. Trapezoidal rule or Simpson’s rule can be used to compute AZ.
classifier
If 0.8 6 Accuracy < 0.9, then classifier is called good classifier
If 0.9 6 Accuracy 6 1.0, then classifier is called excellent classifier



Experimental results and discussion

In order to explore LM-MLFFBP-ANN and SMO-SVM to classify MCCs as benign or malignant, experiments are per- formed on data extracted from mammogram images of DDSM database [17]. For comparative evaluation, confusion matrix


















Figure 3  Common ROC curve of 10 random experimental trials of LM-MLFFBP classifier for classifying MCCs as benign and malignant.
and ROC analysis are used. MATLAB 7.7 software is used for simulation.

Results of LM-MLFFBP-ANN

In order to find the performance of LM-MLFFBP for classify- ing MCCs as benign or malignant, different types of mammo- gram images are taken from standard benchmark digital database for screening mammography (DDSM) [17]. From mammogram images of DDSM database, a total of 380 suspi- cious regions are selected. From these samples, malignant sam- ples are 235 and benign samples are 145. A set of 50 features is extracted from suspicious regions [39]. After this, Particle Swarm Optimization (PSO) is used to select an optimal subset of 23 most suitable features from 50 extracted. Such optimal subset of features is used in LM-MLFFBP. In the architecture of LM-MLFFBP, one hidden layer is taken with 15 hidden units. The activation function between input layer and hidden layer is sigmoid while that between hidden layer and output layer is linear. Default values of different parameters are set according to MATLAB 7.7 environment. For training pur- pose, 191 samples are selected from 380 samples and the remaining samples (189) are used for testing purpose. For find- ing the performance of the classifier to classify MCCs as benign or malignant, 10 random experiment trials are per- formed. Confusion matrix and ROC analysis are used to mea- sure the performance of the trained classifier for classifying Clusters of MCs. Tabular results of 10 random experimental trials of LM-MLFFBP for classifying MCCs as benign or malignant in the form of accuracy calculated from confusion matrix and ROC analysis are shown in Table 2. Fig. 1 illus- trates ROC curves of first and last random experimental trials while Fig. 2 illustrates confusion matrices of first and last ran- dom experimental trials.






From confusion matrices, it is observed that the average accuracy is 0.8296 while from ROC analysis (average of all areas under ROC curves) the average accuracy is 0.8738. In ROC anal- ysis, Simpson’s rule is used to find area under ROC curve. Log- arithmic function is used to plot a common ROC curve of 10 random experimental trials. Common ROC curve is shown in Fig. 3. Accuracy from common ROC curve is 0.8918. The over- all accuracy of LM-MLFFBP is calculated through the average of the three accuracy measures (average accuracy from confu- sion matrices, average accuracy from ROC curve and accuracy from common ROC curve). Thus, the overall accuracy of LM- MLFFBP is 0.8651 that is shown in Table 3.
Results of SMO-SVM

Secondly, in order to explore the performance of SMO-SVM for classifying MCCs as benign or malignant, the same sam-
ples that have been used for LM-MLFFBP are considered. The same 23 features are used that have been selected from 50 features by PSO for LM-MLFFBP. In this study, linear ker- nel function is considered. In the same way as used in LM- MLFFBP, the same 191 samples as used in LM-MLFFBP are used for training and the same 189 samples are used for testing purpose. Similarly, as in LM-MLFFBP, 10 random experimental trials are performed. Results of 10 random exper- imental trials in the form of confusion matrix and area under ROC curve are shown in Table 4. Fig. 4 is used to show ROC curves of first and last random experimental trials while Fig. 5 illustrates confusion matrices of first and last experimental tri- als. From confusion matrix, the average accuracy of SMO- SVM classifier for classifying Clusters of MCs is 0.8788 while average accuracy of SMO-SVM classifier for classifying Clus- ters of MCs from ROC curves is 0.8752. Fig. 6 illustrates com- mon ROC curve obtained from 10 random experimental trials.




















Figure 4	ROC curves of 1st and 10th random experimental trials of SMO-SVM with linear kernel function classifier for classifying MCCs as benign and malignant.


























Figure 5	Confusion matrices of 1st and 10th random experimental trials of SMO-SVM with linear kernel function classifier for classifying MCCs as benign and malignant.

Classification of Clustered Microcalcifications	19

for radiologists to characterize clusters of MCs in mammogram.
The results of the mentioned classifiers are encouraging and show good accuracy within experimental errors. But, in future to achieve above 91% overall accuracy, metaheuristic approaches can also be used to find the optimal hyperplane along with different kernel functions in SVM.

References











Figure 6  Common ROC curve of 10 random experimental trials of SMO-SVM with linear kernel function for classifying MCCs as benign and malignant.




Accuracy of SMO-SVM from ROC curve in terms of area under ROC curve is 0.9509. Thus, the overall accuracy of SVM with linear kernel function and SMO hyperplane finding method for classifying MCCs as benign or malignant is 0.9016 that is shown in Table 5.

Conclusion and future work

In this paper, an attempt is made to compare MLFFB-ANN and SVM classifiers to classify MCCs as benign or malignant. For this purpose, Levenberg-Marquardt training algorithm for MLFFB-ANN and Sequential Minimal Optimization hyper- plane finding method with linear kernel function for SVM are investigated. For this investigation, 10 random experiment trials are performed for LM-MLFFBP-ANN and SMO-SVM classifiers to classify MCCs as benign or malignant. From these experimental results, it is observed that LM-MLFFBP- ANN classifier belongs to good classifier category according to Hosmer and Lemeshow’s rule, while linear kernel function with SMO method based SVM classifier belongs to the excel- lent classifier category. Results of this study are quite promis- ing for selecting a suitable classifier to classify MCCs as benign or malignant. Based on the results of simulation studies and experiments performed in this study, it is concluded that linear kernel function with SMO method based SVM classifier can be used as a classifier to classify MCCs as benign or malignant for achieving highest accuracy. This research work is very useful
Hassanien AE. Fuzzy rough sets hybrid scheme for breast cancer detection. Image Vis Comput 2007;25(2):172–83.
Fu JC, Lee SK, Wong ST, Yeh JY, Wang AH, Wu HK. Image segmentation features selection and pattern classification for mammographic microcalcifications. Comput Med Imag Graph 2005;29(6):419–29.
Kramer D, Aghdasi F. Classifications of microcalcifications in digitized mammograms using multiscale statistical texture analy- sis. In: Proc. South African symposium on communications and signal processing (COMSIG-98), Rondebosch, South African, 7–8 Sep.; 1998. p. 121–6.
Bruce LM, Adhami RR. Classifying mammographic mass shapes using the wavelet transform modulus-maxima method. IEEE Trans Med Imag 1999;18(12):1170–7.
Bottema MJ, Slavotinek JP. Detection and classification of lobular and DCIS (small cell) microcalcifications in digital mammograms. Pattern Recogn Lett 2000;21(13–14):1209–14.
Nicandro C-R, Hector GA-M, Humberto C-C, Luis AN-F, Rocio EB-M. Diagnosis of breast cancer using Bayesian networks: a case study. Comput Biol Med 2007;37(11):1553–64.
Christoyianni I, Koutras A, Dermatas E, Kokkinakis G. Com- puter aided diagnosis of breast cancer in digitized mammograms. Comput Med Imag Graph 2002;26(5):309–19.
Halkiotis S, Botsis T, Rangoussi M. Automatic detection of clustered microcalcifications in digital mammograms using math- ematical morphology and neural networks. Signal Process 2007;87 ():1559–68.
Mazurowski MA, Habas PA, Zurada JM, Lo JY, Baker JA, Tourassi GD. Training neural network classifiers for medical decision making: the effect of imbalanced datasets on classifica- tion performance. Neural Netw 2008;21(2–3):427–36.
Delen D, Walker D, Kadam A. Predicting breast cancer surviv- ability: a comparison of three data mining methods. Artif Intell Med 2005;34(2):113–27.
Singh AP, Kamal TS, Kumar S. Virtual curve tracer for estimation of static response characteristics of transducers. Measurement 2005;38(2):166–75.
Varela C, Tahoces PG, Mendez AJ, Souto M, Vidal JJ. Computerized detection of breast masses in digitized mammo- grams. Comput Biol Med 2007;37(2):214–26.
Arodz T, Kurdziel M, Sevre EOD, Yuen DA. Pattern recognition techniques for automatic detection of suspicious-looking anoma- lies in mammograms. Comput Methods Programs Biomed 2005;79(2):135–49.
El-Naqa I, Yang Y, Wernick MN, Galatsanos NP, Nishikawa RM. A support vector machine approach for detection of microcalcifications. IEEE Trans Med Imag 2002;21(12):1552–63.
Mavroforakis ME, Georgiou HE, Dimitropoulos N, Cavouras D, Theodoridis S. Mammographic masses characterization based on localized texture and dataset fractal analysis using linear, neural and support vector machine classifiers. Artif Intell Med 2006;37 ():145–62.
Wei L, Yang Y, Nishikawa RM. Microcalcification classification assisted by content-based image retrieval for breast cancer diagnosis. Pattern Recogn 2009;42(6):1126–32.
http://marathon.csee.usf.edu/Mammography/Database.html.



Widrow B, Lehr MA. 30 Years of adaptive neural networks: perceptron, machine and backpropagation. Proc IEEE 1990;78 ():1415–42.
Haykin S. Neural networks: a comprehensive foundation. 2nd ed. Delhi, India: Pearson Education, Inc.; 2004, p. 202.
Bhowmick B, Pal NR, Pal S, Patel SK, Das J. Detection of microcalcification with neural networks. In: Proc. IEEE interna- tional conference on engineering of intelligent systems (ICEIS-06), Islamabad, Pakistan, 22–23 Apr.; 2006. p. 281–6.
Cheng HD, Cui M. Mass lesion detection with a fuzzy neural network. Pattern Recogn 2004;37(6):1189–200.
Haykin S. Neural networks and learning machines. 3rd ed. New Delhi: PHI; 2010, p. 691.
Fausett L. Fundamentals of neural networks: architectures, algorithms, and applications. Englewood Cliffs (NJ, USA): Prentice-Hall, Inc.; 1994, p. 289.
Hagan MT, Demuth HB, Beale M. Neural network design. USA: Thomson Learning; 1996, p. 2–11.
Vapnik VN. The nature of statistical learning theory. 2nd ed. NY (USA): Springer-Verlag; 2000, p. 131.
Rardin RL. Optimization in operation research. 2nd ed. Delhi, India: Pearson Education, Inc.; 2003, p. 810.
Deb K. Optimization for engineering design: algorithms and examples. New Delhi, India: PHI; 2003, p. 77.
Taha HA. Operation research: an introduction. 7th ed. New Delhi, India: Pearson Education, Inc.; 2006, p. 765.
Burges CJC. A tutorial on support vector machines for pattern recognition. Data Min Knowl Disc 1998;2(2):121–67.
Fletcher R. Practical methods of optimization. 2nd ed. NY (USA): John Wiley & Sons, Inc.; 1987, p. 219.
Ayat NE, Cheriet M, Suen CY. Automatic model selection for the optimization of SVM kernels. Pattern Recogn2005;38(10):1733–45.
Karatzoglou A, Meyer D, Hornik K. Support vector machines in
R. J Stat Softw 2006;15(9):1–28.
Malon C, Uchida S, Suzuki M. Mathematical symbol recognition with support vector machines. Pattern Recogn Lett 2008;29 ():1326–32.
Platt JC. Fast training of support vector machines using sequen- tial minimal optimization. In: Scholkopf Bernhard, Burges Christopher JC, Smola Alexander J, editors. Advances in Kernel methods: support vector learning. Cambridge (MA, USA): MIT Press; 1999. p. 185–208.
Kohavi R, Provost F. Glossary of terms. J Mach Learn-Spec Issue Appl Mach Learn Knowl Discov Process 1998;30(2–3):271–4.
Fawcett T. An introduction to ROC analysis. Pattern Recogn Lett
2006;27(8):861–74.
Karnan M, Thangavel K. Automatic detection of the breast border and nipple position on digital mammograms using genetic algorithm for asymmetry approach to detection of microcalcifi- cations. Comput Methods Programs Biomed 2007;87(1):12–20.
Hosmer DW, Lemeshow S. Applied logistic regression. 2nd ed. NJ (USA): John Wiley & Sons, Inc.; 2000, p. 164.
Khehra BS, Pharwaha APS. Least-squares support vector machine for characterization of clusters of microcalcifications. World Acad Sci, Eng Technol Int J Comput, Inform Sci Eng 2013;7(12):932–41.
