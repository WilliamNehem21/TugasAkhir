Electronic Notes in Theoretical Computer Science 130 (2005) 23–37  
www.elsevier.com/locate/entcs


Memoryless Strategies for Stochastic Games via Domain Theory

Carroll Morgan1 ,2
Dept. of Computer Science and Engineering University of New South Wales
Sydney 2052 Australia

Annabelle McIver1 ,3
Department of Computer Science Macquarie University Sydney 2109 Australia


Abstract
In Formal Methods we use mathematical structures to model real systems we want to build, or to reason about; in this paper we are concerned principally with game-based models.
In an earlier work [16] we presented an approach to stochastic minimising games based on our logic for probabilistic programs [17,11]. The contribution here is to extend our logical approach to maximising games. That maximising and minimising are not (simply) duals is due to the use of least fixed-points in both cases, as is normal for iterating programs. (For duality, we would have to use greatest fixed-points in the maximising case.)
The significance of that extension is that it makes a further link between the program-logical approach and the more general methods for finding control strategies that “solve” games of this kind, in the sense of giving the player a “recipe” which, if followed, will deliver the theoretically optimal result.
Keywords: Probabilistic semantics, stochastic games, Markov decision processes.



1 We are grateful for the support of the Australian Research Council (ARC) under its Discovery Grant DP0345457.
2 Email: carrollm@cse.unsw.edu.au
3 Email: anabel@ics.mq.edu.au



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2005.03.003

Introduction
Techniques such as Z [19] and B [1] model a real-world system by postulat- ing a mathematical model of its state, and formulating abstracted operations that change that state in a way that is supposed to correspond to the actual operations that can be observed or carried out on the real system.
In this work we are concerned with more general systems than the above, those that suggest (to their observers) the process of playing a game, possibly involving demonic nondeterminism (the environment’s trying to frustrate the user), angelic nondeterminism (the user’s attempts to succeed in spite of that) and probabilistic nondeterminism (out of control of either “player”, but at least quantifiable).
A recent trend, especially for systems like these, is to characterise correct- ness (and refinement) not as simply “achieving a postcondition”, the tradi- tional view, but as maximising a reward (of which the traditional view is a special case: get $1 if you achieve the postcondition; get $0 if you don’t).

The “classical” approach to such problems is via linear-programming tech- niques and Markov Decision Processes (MDP’s) [5] rather than programming logic; but we have found that the approach can be enhanced by using our
experience with program logic in other areas. Similarly, we find that the clas- sical work done already can inform our design of the programming logics and even in some cases suggest techniques for proving properties of them [4, for example].
In particular, we have shown how two-player turn-based demonic /angelic
/probabilistic games correspond to expressions in our quantitative µ-calculus qMµ [14,15,10], which is itself a superset of a quantitative temporal logic qTL.
Thus we are following a more general approach than is typical of e.g. Z or
B, where we observe a system, describe its behaviour as a state-with-operations system (necessarily an informal step), codify that description as a mathemat-
ical model, and then use mathematical tools (theorems) to manipulate the formal text (refinement etc.) Finally, we translate our results back into the real world, as predictions about the system’s behaviour or as code that will
implement it.
Here, we observe a system, describe its behaviour as a game (necessarily an informal step), codify that description as a mathematical model (a game tree), and then use mathematical tools (theorems) to manipulate the formal text (via quantitative logic corresponding to the tree). And finally, again, we translate our results back into the real world, as predictions about the system’s behaviour or as code that will implement it.


The specific technical result of this paper is to show how the program- logic approach can predict that the result of “playing the game” has a certain optimal bound, and can prove that there is a memoryless player’s strategy that will realise it.

Existing theory, and our contribution
In the mathematical style, Markov Decision Processes address realistic prob- lems related to resource contention within a probabilistic context. A (non- competitive) MDP, in its most abstract form, describes the actions of a solitary gambler and the consequences of his selected strategy in placing bets.
The use of MDP ’s brings with it a long-established body of algorithmic techniques, based on linear programming, for quantifying certain well-defined properties; however, even simple arguments can be intricate and difficult to
follow — largely because of the need to include all the details of each MDP.
There is no built-in facility of abstraction.
Our approach to this specification and analysis problem is to abstract the properties of a probabilistic system using a quantitative logic, in which the expressions include real values that reflect the probabilistic transitions in the underlying system. As with program logic generally, it leads to simpler specifi-
cation and analysis techniques; and the basis for the logic is a domain-theoretic model of the underlying system. The abstraction is included naturally.
Abstraction is of course the essence of logic; and in this paper we con- centrate on the domain-theoretic models over which such quantitative logics [14,15,11] are built. That domain-theoretic “spin” allows simpler proofs of some of the key existence theorems.

In the Computer-Science style, early treatments containing both probabil- ity and nondeterminism sought to introduce probabilistic models of distributed systems, which went a long way towards clarifying the many issues related to
how probability and nondeterminism should interact. 4 A major contribution of the work was to provide specification languages and verification techniques for probabilistic properties in general [18] and temporal properties in particu- lar [20,2,6]. Examples of such properties include “a given set of system states will be reached with probability one” and “a given property is invariant with such-and-such a probability”. Later work extended the application areas to include average expected times between events and shortest-path problems [3].

4 When we write “nondeterminism” on its own, we mean demonic rather than probabilistic.


A key component of those treatments was to provide actual algorithms for computing the properties specified, and many model-checking systems are based on the ideas put forward in that early work.

In this paper we treat mathematical-style problems but with Computer- Science-style tools, in particular domain theory: we address the problem of showing that a memoryless strategy exists for a maximising player within an iterating game whose value for infinite behaviour is zero (thus a least fixed- point).

We use left-associating dot “.” for functional application, and write quan- tifications in the order bound variable then constraint then term, with scoping always explicit, given either by parentheses (·· ·) or set-comprehension braces
{·· ·}: thus the set of positive cubes is {i: Z | i > 0 · i3}.

The classical perspective: Markov Decision Processes
The basics
In this section we set out the basic theory of MDP ’s, following closely the approach taken by Filar and Vrieze [5], although we do not use their notation exactly.
Let S be a ﬁnite state space, let L be a finite set of labels and let S be the set of discrete probability distributions over S. 5
Definition 3.1 A finite-state Markov Decision Process over S is a pair (ρ, ω), where ρ is a transition system of type L × S → S and ω is a reward function of type L × S → R. The transition system ρ maps every label l in L and state s in S to a probability distribution ρ.l.s over final states, while ω specifies an immediate reward ω.l.s following the selection of label l in state s.
The following metaphor, which complements the above definition, provides a useful intuition based on a gambler’s playing a game of chance:
From any position s he may choose from a selection of #S-sided dice each with the elements of S written on it; each die has some label l and is biased according to the distribution ρ.l.s.
Each choice of die incurs an immediate cost or reward of ω.l.s.



5 Later we will extend S to include sub-distributions, those that sum to no more than one (rather than exactly to one).

His total expected winnings are then determined with respect to a payoff function φ: S → R by his immediate reward and the final state s' he reaches after rolling his chosen die: in total, he wins φ.s' plus the reward.
Thus if in the MDP (ρ, ω) the gambler plays from initial state s, and chooses to roll the die labelled l, then his total expected one-step winnings is given by
M.l.s.φ	:=	ω.l.s + ∫	φ	,
ρ.l.s
where we are thus letting M stand for the function of l, s and φ that (ρ, ω) represents as above. In general we write ∆ φ for the expected value of random
variable φ over distribution ∆. In the finite case, as here, the integral in (1)
is simply Σs':S (ρ.l.s.s × φ.s ), since the distribution ∆ produced by the MDP
'	'
is just ρ.l.s and finite expected values are just weighted sums.
A gambler’s greatest expected one-step winnings (1) from initial state s is the best he can do over all choices of l (i.e. by selecting the die most biased in his favour, given ω and the payoff function φ); it is given by the function
M.H, defined as follows: 6


Definition 3.2 For MDP M = (ρ, ω) we define
M.H.φ.s	:=	(H l: L · M.l.φ.s)	=	(H l: L · ω.l.s + ∫
ρ.l.s


φ	) .

We illustrate the above definitions with an example due to Filar, repro- duced here in Fig. 1 (but slightly modified). Let the MDP in Fig. 1 be M , and consider a payoff function φ given by
φ.s1 := 10,	φ.s2 := 20,	φ.s3 := 30 .
Then the gambler’s greatest expected winnings from (initial) s2 are:
M.H.φ.s2
=	(10 + 1×20) H (5 + 0.8×10 + 0.2×20)
=	30 H 17
=	30 .


6 Strictly speaking the symbols “M.H” are the name of the function we are defining, based on M , rather than indicating some label “H” passed to M as its first argument. This unorthodox but suggestive notation will be useful later.



Label l1


Label l2


State s1	State s2	State s3
In this representation a box portrays an action in a state and its reward/transition (upper- left/lower-right) consequences. For example, in the above the choice of Label l1 results in a reward of ω.l1.s1 = 0 from State s1 and transition probabilities ρ.l1 .s1.s1 = 0.9, ρ.l1.s1 .s2 = 0 and ρ.l1.s1 .s3 = 0.1.
Where there are differing numbers of alternatives (e.g. only one label in state s3 ), we regard the missing labels as giving “copies” of the effects of labels already there for that state. In that way in the theory we can assume a fixed number of labels throughout.

Fig. 1. A summable Markov Decision Process [5, Example 2.1.1 p10]

Discounted-cost games
Suppose now that the gambler plays repeatedly, accumulating his rewards as he goes, to give an overall “multi-step” winnings. At each state he chooses a label determining his next move; let his strategy be σ: S → L representing “in
advance” the choice of label he would make in state s. The gambler’s problem now is to evaluate the effectiveness of his strategy, were he to follow it, in terms of his overall winnings. A strategy σ, transition system ρ and an initial state s together determine a well-defined probability distribution over sequences of states in S that will be observed by the gambler as he plays the game. From that distribution the expected overall winnings can be computed, although
for some games it could be undefined. One kind of game that guarantees definedness is the discounted-cost game, which we now describe.
Definition 3.3 A discounted MDP is a tuple (ρ, ω, β) where (ρ, ω) is an (undiscounted) MDP and the extra component β with 0 ≤ β < 1 is a discount factor. Play is according to some strategy σ as above, but the rewards are “discounted” according to when they occur: a reward after n steps is worth
only βn of its nominal value as given by ω.
The discount has the effect that the later a reward is received the less it is worth: thus its value declines at a fixed percentage. Suppose the gambler starts playing the MDP given by (ρ, ω, β) from initial state s0 at Step 0: the discount is ineffective for his first move (it is β0), so that his first reward is


ω.(σ.s0).s0, as in the single-step case.
For his second move, from some state s1 chosen probabilistically from the distribution ρ.(σ.s0).s0, he will receive (only) the discounted value β × ω.(σ.s1).s1; therefore his expected overall winnings when he has reached State
s1 is
ω.(σ.s0).s0	+	β × ∫	ω.(σ.s1).s1 ds1 .	7
ρ.(σ.s0 ).s0
Similar —but increasingly complex— expressions give the expected total re- ward for later steps. Note that here we are not using a final payoff function φ, since the game is unending.
In general for discounted M = (ρ, ω, β) we write M ∗.σ.n.s for the overall expected winnings up to Step n, with strategy σ. 8
The gambler’s overall expected winnings, were he to play according to strategy σ, is thus the limit of the above as n increases, that is limn→∞ M ∗.σ.n.s, which accordingly we write M ∗.σ.∞.s.
It turns out that for discounted-cost games, the σ-like strategies are suffi- cient to realise the the maximum possible overall expected winnings; with the next theorem we sketch the traditional proof. (That is, we do not need more powerful strategies of the kind sketched in Sec. 3.3 below.)
Theorem 3.4 A gambler playing the discounted game set out in Def. 3.3 can achieve the greatest possible expected winnings by following a σ-like strategy.
Proof. The standard proof uses linear-programming techniques; for details refer to Filar [5].	 
In the sections to come, we look at a domain-theoretic proof of that same result, and then we extend it to show that the result applies even when the game is not discounted.

On more general strategies
The “σ-like” strategies we considered above are called pure, because they are not themselves probabilistic, stationary because although they depend on the state they do not depend on the “time” at which that state is reached and

7 In the usual way, we read “ds1” as indicating a functional abstraction of the expression ω.(σ.s1).s1 over its parameter s1. Thus the argument of the integral sign is a function over S, as it should be.
8 Continuing our abuse of notation, we use the superscript in M ∗ to remind us that in
this case we are iterating the single-step M . The σ in place of the label indicates that the labels are chosen according to that strategy; and we write (integer) n in the place formerly occupied by payoff φ, since both are to do with the stopping of the MDP.

memoryless because they depend only on the current state and not on any previous states the iterated process M ∗ may have passed through.
There are more general kinds of strategies, however; and the more infor- mation a strategy can take into account, the more “powerful” it is in the sense of being able to force the MDP to display certain behaviours. The most interesting behaviour, for us, is the one which realises the greatest expected
payoff M ∗.H.∞. We give a domain-theoretic style proof below that in fact the σ-like strategies —pure, stationary and memoryless— are sufficient for that, provided we restrict ourselves to non-negative rewards.
The further significance of such strategies is that they place the overall problem back in the context of finite state spaces, since for a finite state space S, and finitely many labels, there are only finitely many strategies of that type. (On the other hand, even generalising slightly, to “impure” or probabilistic strategies, makes the number of possible strategies infinite.) And it means that gamblers who can achieve their optimal reward by playing such strategies can evaluate their optimal expected payoffs using linear-programming techniques.

The Computer-Science perspective: domain theory
We are studying “infinite event-horizon games”, where there is no definite time horizon which will assure convergence: the more general framework pro- vided by a domain-theoretic perspective clarifies the conditions under which convergence is guaranteed.
We begin by recalling some elementary facts of domain theory.

A domain of quantitative functions
A partially ordered set is a pair (C, ±) where C is a set, and ± is a reflexive, anti-symmetric and transitive order on C. The order is said to be complete if any chain in C has a least upper bound in C, where a chain is a totally- ordered subset. For us here, a domain is a complete partially-ordered set with
a least element.
Given a finite state space S we construct S S, the set of R∞-valued functions over S, where R∞ is the set of non-negative reals completed with a top-element
∞; they are called payoffs. The only use of ∞ we make here is to ensure (R∞, ≤) is complete (thus a domain), i.e. the only property of ∞ that we use is that it dominates all proper reals. Next we lift ≤ on R∞ to S S pointwise,
defining for φ, φ': S S that
φ ± φ'	iff	(6s : S · φ.s ≤ φ'.s) .
We use a different symbol (“±” rather than “≤”) to remind us that the lifted


order is only partial.
For real k we write k for the constant-valued payoff with value k for all states. The least element of S S is thus 0, and the greatest is ∞, and we note the following:
Lemma 4.1 The space (S S, ±) is a domain.
We will see that the optimal value of an MDP can be found as a “fixed- point” in the domain of payoffs, though in the standard theory the existence of those fixed-points is proved by appealing to a “contraction-mapping theorem” which requires the single-step game to define a contraction mapping over a suitable metric.
In the domain-theoretic setting however the existence of fixed-points is more straightforward. In general, functions over a domain are guaranteed to have fixed-points if they are “monotone”, where (in general) a function F : C → C is said to be monotone if c ± c' implies F.c ± F.c'. We write µ.F for the least ﬁxed-point of F in C — it is the ±-smallest element c such that
F.c = c.
Happily, discounted accumulation games, like the ones M ∗.H.∞ we studied above, have monotone single-step functions M.H over the order ± on S S, and thus we have immediately that their least fixed-points exist. Of course in some cases that least fixed-point may be infinite (as could be the case in accumulation games if no discount were applied).
Taking the least fixed-point may seem an arbitrary decision, but in fact it agrees with the operational specification that a player receives only the rewards explicitly determined by his appeal to ω as he “passes through”, and a payoff of zero is waiting for him “at infinity”. (Note also that ∞ is the greatest fixed-point of any M , making that an unlikely choice.) Indeed the properties of least fixed-points justify precisely the constraints used in the linear-programming techniques that are commonly employed to compute the
greatest overall value. Translated into the MDP accumulation-game notation
of Sec. 3.1, the lfp-induction property says that the the least fixed-point of
M.H is the least payoff φ that satisfies
M.H.φ	±	φ , 
which inequation, being a comparison of functions, can be interpreted as a set of inequalities, one for each element of the state space.
The connection with accumulation games is given by the following lemma:

Lemma 4.2 Let M be an MDP with non-negative reward function ω. Then the greatest expected overall winnings of the iterated game M ∗.H.∞ is the least


ﬁxed-point of the payoff-to-payoff function M.H. Similarly, for strategy σ, the expected winnings of the iterated game M ∗.σ.∞ is the least ﬁxed-point of the function M.σ.
Proof. The value of either form of game — either M ∗.H.∞ or M ∗.H.∞ — is given as the limit of an infinite sum; exactly the same sum occurs in the iterated calculation of the corresponding fixed point.	 
The value of the game is the basis on which a player determines how he should play — if he plays a bad strategy then his ultimate payoff will be far from the value of the game. On the other hand if he uses a good strategy then his expected payoff will be close to the value of the game.
Linear programming techniques [5] can be used to compute his optimal strategy; unfortunately for many sizeable systems the enormous state space makes linear-programming solutions infeasible, so that alternative computa- tion strategies need to be employed. Here the domain-theoretic approach will
suggest using iteration, and this is indeed the approach taken in practice (for example PRISM [8,7]).
Determining the optimal strategy remains a problem, however, and we turn our attention to that topic next.

Memoryless strategies for discounted games
We begin first by defining a discounted game in domain-theoretic terms, so that we can study its properties. First we define the “norm”  f  of any function f from the state space into the reals (including negatives).
Definition 4.3 The norm of a function f : S → R is the largest absolute value in its codomain: we have
 φ 	:=	(Hs: S · |X.s|) ,
where |·| denotes absolute value.
Clearly  f  is always finite for finite S. We now define “contraction mapping”.
Definition 4.4 Function F from S S to S S is a contraction mapping with
factor β if 0 ≤ β < 1 and
 F.φ — F.φ' 	≤	β × φ — φ' 
for all payoffs φ, φ'.
We now show how we can regard a discounted MDP as a contraction mapping. Given an undiscounted (ρ, ω), and a discount factor β, form the


(again undiscounted) “β-scaled” MDP (β × ρ, ω) by defining (β × ρ).l.s.s' := β ×(ρ.l.s.s'), thus making the “distributions” (β × ρ).l.s sum to β rather than to 1. Since β < 1 we call them sub-distributions; regarded as a matrix (β ×ρ).l for any l would be called sub-stochastic. When M is (ρ, ω) we write M×β for (β × ρ, ω); note that only the transition probabilities, and not the reward, are
scaled by β.
Note that we have the following property immediately.
Lemma 4.5 The β-scaled M×β formed from an undiscounted MDP M is dominated by M itself: for all payoffs φ we have
M×β. H .φ	±	M.H.φ .
For this we can write M×β. H ± M.H .
We observe now that M×β.H is a contraction mapping with factor β.
Lemma 4.6 For any MDP M, payoffs φ, φ' and real 0 ≤ β we have that
 M×β. H .φ — M×β. H .φ' 	≤	β × φ — φ'  .
Thus whenever β < 1 we have that M×β is a contraction mapping with respect to the metric  · .
Proof (sketch) Suppose the contrary, for a contradiction; let φ — φ'  be some real δ. Then for some state s we must have wlog
M×β. H .φ.s + β × δ	<	M×β. H .φ'.s .
Thus for some labels l, l' we have M×β.l.φ.s + β × δ	<	M×β.l'.φ'.s and, furthermore, that M×β.l.φ.s ≥ M×β.l'.φ.s. Thus
M×β.l'.φ.s + β × δ	<	M×β.l'.φ'.s ,
and the result now follows by arithmetic since Σ(β × ρ).l' ≤ β.	 
Putting together Lemmas 4.2 and 4.6 gives us the following.

Lemma 4.7 For any MDP M with scaling factor β we have that M ∗
.H.∞

is the unique ﬁxed-point of M×β.H .
Proof (sketch) We have that M×β.H is a contraction mapping over S S; it is analytically continuous by construction (recall that L is finite); and all continuous contraction mappings over an analytically closed set, which S S is, have a unique fixed point.	 
Finally, we observe that when we have unique fixed points the strategies are particularly easy to calculate.
Lemma 4.8 Suppose that some MDP M is such that all its ﬁxed-strategy in- stantiations M.σ have unique ﬁxed points. (This would be the case for example

if M were of the form M '	for some M ', since then all instantiations M ' .σ
would be contraction mappings.)
Deﬁne φ := µ(M.H) and construct a strategy σ such that M.σ.φ = φ, easily done since we are considering a strategy for just one step of M with respect to a known payoff function φ. 9
Then we have that σ realises the optimal value of the iterated MDP, that is it achieves φ: we have
M ∗.H.∞	=	µ.(M.H)	=	φ	=	µ.(M.σ)	=	M ∗.σ.∞ .
Proof. By construction of σ we know that φ is a fixed-point of M.σ, and by assumption M.σ has only one fixed-point: thus in fact φ must be its least fixed point.	 
That is, Lem. 4.8 tells us immediately how to compute the optimal strategy provided that the value of the game is known: it can be found by computing the strategy σ that solves the equation
M.σ.φ  =  φ , 
where φ is µ.(M.H) and, as we noted above, can be computed by iteration beforehand (i.e. without knowing σ).

Memoryless strategies for non-discounted games
We can prove the existence of stationary strategies for non-discounted games by approximating them with discounted games, as we now show.
For a general non-discounted MDP given by M = (ρ, ω) we can form a discounted MDP given by M×β as before, and we noted then in Lem. 4.5 that M×β. H ± M.H. In fact we have the following stronger property.
Lemma 4.9 For any (non-discounted) MDP M we have
M.H	=	(Hβ | β < 1 · M×β.H) .
Proof. We calculate directly, as follows: for any payoff φ,
(Hβ | β < 1 · M×β.H).φ
= (Hβ | β < 1 · M×β. H .φ)
= (Hβ | β < 1 · M.H.(β × φ))
= M.H.(Hβ | β < 1 · β × φ)
= M.H.φ ,

9 Actually φ is known “in principle”, since µ.(M.H) itself must first be calculated.


where in the second-last step we rely on the continuity of M.H .	 
We can now deduce quite easily the existence of memoryless strategies for non-discounted maximising MDP ’s.
Lemma 4.10 Let M be a (non-discounted) MDP; then M ∗.H has an optimal memoryless strategy.
Proof. For any β < 1 we have from Lem. 4.6 that M×β.H is a contraction

mapping, and thus by Lem. 4.8 that M ∗
.H has a memoryless optimal strategy

σβ, say, that is one that satisfies M ∗
. H .∞ = M ∗
.σβ.∞.  We observe the

relationships
M×β.σβ	±	M.σβ	±	M.H ,
where the first inequality relies on Lem. 4.5 (or Lem. 4.9), and the second holds because in fact M.σ ± M.H for any σ.
By monotonicity of the least-fixed-point operator, we can immediately deduce a similar relationship between the corresponding least fixed-points, giving us
µ.(M×β.σβ)	±	µ.(M.σβ)	±	µ.(M.H) .
Because of the way we chose σβ, however, that immediately simplifies to
µ.(M×β.H)	±	µ.(M.σβ)	±	µ.(M.H) .
The result now follows from the following observations. Let β approach 1
in (2); we know from Lem. 4.9 that M×β.H approaches M.H; by continuity of µ itself for this kind of limit, 10 we therefore know that µ.(M×β.H) approaches µ.(M.H), which is the right-hand side; thus the middle term µ.(M.σβ), squashed
between left and right, must approach µ.(M.H) in the limit as well. But there are only ﬁnitely many σ’s in total, and so one of the σβ’s must actually attain the limit: let it be σ^.
Thus µ.(M.σ^) = µ.(M.H), that is M ∗.σ^.∞ = M ∗.H.∞, and we are done.

Conclusions
The direct connection between Markov processes and probabilistic-program se- mantics is only to be expected: both model a stochastic activity evolving in a series of discrete steps. Going “forwards”, the Markov approach multiplies dis- tributions by matrices, while the program-semantics approach (Kleisli-) com- poses state-to-distribution functions; going “backwards”, the former multiplies payoffs by matrices, and the latter (functionally-) composes payoff transform- ers.

10 This continuity of µ requires a separate proof.


In fact these four approaches correspond so well that there might not be much to choose for simplicity between them. When nondeterminism is introduced, however, the last — payoff transformers — is singled out because
it does not need to be extended.
Markov processes must be extended to Markov decision processes, with their labels and strategies; and state-to-distribution functions become state- to-distribution relations. But payoff transformers retain their “shape”, merely broadening the class transformers that are considered well-formed; 11 thus many of the simpler notions applicable to the deterministic case can be carried over unchanged.
An example of that — but not our topic in this report — is stationarity: it is well know that a long-term “stationary” distribution exists for any Markov process whose transition matrix satisfies certain conditions; but if those con- ditions are not met, the stationary distribution does not exist. McIver shows
[9] that when recast as an payoff transformer the process has a stationary dis- tribution in all cases, and considerable simplification results: the traditional conditions merely ensure that the distribution is “deterministic”.
Our concern here has been the more general simplification that might result from treating the nondeterminism of MDP ’s in the transformer style. In particular, the whole apparatus of state sequences, strategies and labels
is “sucked up” into the provisions that transformers make automatically for nondeterminism. Rather than seek extremal strategies — which of necessity requires that strategies themselves be modelled explicitly — we look instead at extremal fixed-points of functions. The price we pay for that is the explicit modelling of the program lattice.

References
Abrial, J.-R., “The B Book: Assigning Programs to Meanings,” Cambridge University Press, 1996.
Aziz, A., V. Singhal, F. B. R. Brayton and A. Sangiovanni-Vincentelli, It usually works: The temporal logic of stochastic systems, in: Computer-Aided Veriﬁcation, 7th Intl. Workshop, LNCS 939 (1995), pp. 155–65.
de Alfaro, L., Computing minimum and maximum reachability times in probabilistic systems, in: Proceedings of CONCUR ’99, LNCS 1664 (1999), pp. 66–81.
Everett, H., Recursive games, in: Contributions to the Theory of Games III, Ann. Math. Stud.
39, Princeton University Press, 1957 pp. 47–78.
Filar, J. and O. Vrieze, “Competitive Markov Decision Processes: Theory, Algorithms, and Applications,” Springer Verlag, 1996.

11 In standard programming the transformers drop their “disjunctivity” when nondetermin- ism is introduced, retaining conjunctivity; and deterministic payoff transformers are linear, becoming (only) sublinear when nondeterminism is introduced.


Hansson, H. and B. Jonsson, A logic for reasoning about time and reliability, Formal Aspects of Computing 6 (1994), pp. 512–35.
Kwiatkowska, M., Model checking for probability and time: from theory to practice, in: P. G. Kolaitis, editor, Proc. Eighteenth Annual IEEE Symp. on Logic in Computer Science, LICS 2003 (2003), pp. 351–360.
Kwiatkowska, M., G. Norman and D. Parker, Probabilistic symbolic model checking with PRISM: A hybrid approach, International Journal on Software Tools for Technology Transfer (STTT) 6 (2004), pp. 128–142.
McIver, A., A generalisation of stationary distributions, and probabilistic program algebra, Electronic Notes in Theo. Comp. Sci. 45, Elsevier, 2001 .
McIver, A. and C. Morgan, Games, probability and the quantitative µ-calculus qMu, in:
Proc. LPAR, LNAI 2514 (2002), pp. 292–310, revised and expanded at [12].
McIver, A. and C. Morgan, “Abstraction, Refinement and Proof for Probabilistic Systems,” Technical Monographs in Computer Science, Springer Verlag, New York, 2004.
McIver, A. and C. Morgan, Results on the quantitative µ-calculus qMµ (2004), to appear in
ACM TOCL.
McIver, A., C. Morgan, J. Sanders and K. Seidel, Probabilistic Systems Group: Collected reports,
web.comlab.ox.ac.uk/oucl/research/areas/probs.
Morgan, C. and A. McIver, A probabilistic temporal calculus based on expectations, in: L. Groves and S. Reeves, editors, Proceedings, Discrete Mathematics and Computer Science (1997), pp. 4–22, available at [13, key PTL96].
Morgan, C. and A. McIver, An expectation-based model for probabilistic temporal logic, Logic Journal of the IGPL 7 (1999), pp. 779–804, available at [13, key MM97].
Morgan, C. and A. McIver, Cost analysis of games using program logic, in: Proc. of the 8th Asia-Paciﬁc Software Engineering Conference (APSEC 2001), 2001, abstract only: full text available at [13, key MDP01].
Morgan, C., A. McIver and K. Seidel, Probabilistic predicate transformers, ACM Transactions on Programming Languages and Systems 18 (1996), pp. 325–53, doi.acm.org/10.1145/229542.229547.
Segala, R., “Modeling and Verification of Randomized Distributed Real-Time Systems,” Ph.D. thesis, MIT (1995).
Spivey, J., “Understanding Z: a Specification Language and its Formal Semantics,” Cambridge University Press, 1988.
Vardi, M., A temporal ﬁxpoint calculus, in: Proc. 15th Ann. ACM Symp. on Principles of Programming Languages (1988), pp. 250–9, extended abstract.
