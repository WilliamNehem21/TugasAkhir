Egyptian Informatics Journal 24 (2023) 100417










Multi-fuzzy clustering validity index ensemble: A Dempster-Shafer theory-based parallel and series fusion
Hong-Yu Wang, Jie-Sheng Wang *, Guan Wang
School of Electronic and Information Engineering, University of Science & Technology Liaoning, Anshan 114044, China



A R T I C L E I N F O 

Keywords:
Fuzzy clustering validity index Information ensemble
Fuzzy clustering algorithms D-S evidence theory Parallel and serial ensemble
A B S T R A C T 

Clustering validity evaluation is a key part in clustering process. To adapt the complex data structure, the traditional fuzzy clustering validity index (FCVI) is designed more complex. The weighted combined validity evaluation method (WCVEM) is simple in structure but difficult in weight selection. Therefore, this paper pro- posed an ensemble method based on multi-fuzzy clustering algorithms and multi-FCVI. Firstly, multi-FCVI are calculated by using the multiple sets of cluster centers and membership degrees that obtained by multi-fuzzy clustering algorithms. This can improve the robustness of the multi-FCVI. Secondly, multi-FCVI are ensembled by Dempster-Shafer (DS) theory. The validity index basic probability assignment function can be obtained by calculating the credibility of each validity index with different clusters number. Finally, the decision module is used to output the optimal clusters number. This paper ensembles multi-fuzzy clustering algorithms, multi-FCVI, and the DS theory by using series and parallel structure to verify performance of the proposed model and the degree of information retention of the FCVI. The proposed method is simple in structure and does not need to be select weighted. 6 artificial datasets and 12 UCI datasets were selected to simulate and verify the method. When facing different data, the simulation results show that the parallel structure has the highest accuracy, and the series structure is even worse than the weighted method in some datasets. In addition, the paper changes the value of fuzzy weighted, and experimental results show that the ensemble method has better stability than other methods in the face of different fuzzy weighted strategy.





Introduction

With the continuous popularization and application of smart devices, a large amount of data is accumulated. How to get useful information from those data has become a major problem facing mankind, and data mining technology has come into being [1]. At present, Data mining has been applied to medical, education, finance and other fields. As one means of data mining, cluster analysis has been widely concerned and studied by scholars [2,3]. Clustering is unsupervised learning, it is applicable to data sets without prior knowledge, such as the inherent structure cognition of biological populations, commercial location based on user location information, traffic recommendation of search engines, and other more fields [4,5].
The core of clustering is similar samples are clustered into one cluster, and dissimilar samples are divided into different clusters [6]. The K-means clustering algorithm [7] is the first membership degree division algorithm, it divides samples into different clusters, that is, either 0 or 1. To overcome this shortcoming and make samples in
different classes but have similarities have a better division method [8], Bezdek extending hard clustering like K-means to fuzzy clustering by introduced fuzzy sets [9] and proposed the Fuzzy c-means clustering algorithm (FCM) [10]. The FCM got widely study by scholars because its simple process and low computational complexity [11].
For the same dataset, various clustering algorithms will obtain different results. To improve the adaptability of clustering algorithms, the clustering ensemble method was first proposed by Strehal in 2002
[12] and has since been widely used by scholars to improve traditional clustering algorithms and enhance the ability to process datasets of clustering algorithms. A selective clustering ensemble means based on repeated clustering algorithm proposed by Hong [13] in 2009. Berikov proposed a probability clustering ensemble means based on clustering combination of the fuzzy clustering algorithm and weighted consensus matrix [14]. Bagherinia proposed an ensemble framework that not require the characteristics of samples weighted by fuzzy clustering [15]. Mojarad proposed a clustering ensemble based on iterative Fusion of base clusters [16]. Clustering ensemble technology is to integrate the



* Corresponding author.
E-mail addresses: wanghongyuww@126.com (H.-Y. Wang), wang_jiesheng@126.com (J.-S. Wang), 480433838@qq.com (G. Wang).

https://doi.org/10.1016/j.eij.2023.100417
Received 15 April 2023; Received in revised form 9 July 2023; Accepted 3 November 2023
Available online 11 November 2023
1110-8665/© 2023 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



basic clustering algorithms by some information fusion method and obtains better clustering results than the basic clustering algorithm. The scheme of clustering ensemble mainly includes the ensemble by changing different parameters, the ensemble between different algo- rithms, and the ensemble of dataset feature levels [17,18].
At present, fuzzy clustering needs to face four problems, including dataset preprocessing, clustering analysis, clustering validity evalua- tion, and post clustering processing [19]. Their relationship is shown in Fig. 1. It is unknown how many clusters these data should be divided into, so clustering is used to classify the data set. Cluster analysis can only give the corresponding cluster situation of different clusters. In order to select the best clustering result, it is necessary to use the validity evaluation to get the value of the best cluster number. Therefore, the validity evaluation directly determines the quality of the clustering re- sults in the whole clustering process.
multi-FCVI, and DS theory. From the perspective of parallel and serial, three ensemble model frameworks (DSMFCE-I, DSMFCE-II and DSMFCE-III) are proposed. Finally, this paper uses manual datasets and UCI datasets to simulate and verify the proposed DSMFCE. The simu- lations show that DSMFCE has good stability and accuracy in clustering validity evaluation.

Various fuzzy clustering algorithms

FCM clustering algorithm (FCM)

The FCM is the earliest and the most frequently used algorithms in fuzzy partition algorithm. The basic principles of FCM is classifying n samples x1, x2, ⋯, xn in dataset X into c classes, and calculates the min-
imum value of loss function shown in Eq. (1).

Research on the fuzzy clustering validity evaluation methods have	c	n
two directions: fuzzy clustering validity index (FCVI) and weighted	JFCM (U, V) = ∑ ∑ uij )m ‖xj — vi ‖2	(1)

based on the following 2 aspects: (1) FCVI only based on membership. Such as PC index [20], Modified Partition Coefficient [21] and P index proposed by Chen [22]. (2) FCVI based with membership and data ge- ometry. Such as Xie-Beni index [23], Wang proposed the VHY validity index [24], Naderipour proposed a fuzzy cluster-validity index based on the topology structure [25], and Tang proposed a fuzzy clustering val- idity index induced by triple center relation [26]. Study on the WCVEM

As shown in the above function, c is the number of clusters. m∊(1, ∞)
is the fuzziness index, which is the measure the fuzziness of the generic
degree in each class of X. xj is the sample, vi is the cluster center calculation as Eq. (2) and ‖xj —vi‖ is the Euclidean distance between xj and vi.
∑n  uijm • xi

only based on the way of weighted combination. In 2005, Sheng given the weighted sum validity function (WSVF) from the perspective of hard
i	n
j=1
uijm

clustering [27]. Based on Shen’s idea, Dong introduced fuzzy clustering
into WSVF and proposed the fuzzy weighted sum clustering function (FWSCF) [28]. Wang and Liu proposed component-wise design method of FCVI with different construction method [29,30].
In order to cope with the increase in the structural complexity and
number of samples of datasets, the structural design of FCVI is becoming
uij(0≤uij ≤ 1) is the membership of xj to vi that satisfy Eq. (3). uij ∈ U,
where U is a c × n matrix made up with uij. V = {v1, v2, ⋯, vc} is a set of
vi . JFCM(U, V) represents the sum of square error of the same xj in
different c, the optimal c can be obtained by the least error value
minJFCM(U, V).

more and more complex, and the calculation amount is also increasing.
However, the ability of FCVI to adapt to datasets has not been funda- mentally solved, and some FCVI will fail due to the large gap between

uij =
c k=1
2/ m 1	1
  j	i 
‖xj — vk‖
(3)

the intra-class compactness and the inter-class separation degree.
Initialize membership U  and cluster center V .	c
i=1
uij = 1,

WCVEM is simpler and more stable than FCVI. However, the weighting
0	n
j=1
uij < n, where i = 1, 2, ⋯, c, j = 1, 2, ⋯, n.

method of WCVEM is difficult to determine and the calculation is very large.
Therefore, this paper proposed a clustering validity ensemble model (DSMFCE) based on the combination of multi-fuzzy clustering algo- rithms, multi-FCVI and DS theory. Clustering algorithm ensemble uses the membership and clustering center of multi-fuzzy clustering algo- rithms to improve the generalization ability of the DSMFCE. Then inputs the updated cluster centers and membership degrees into multi-FCVI which can improve the adaptability of DSMFCE. The values of multi- FCVI are ensemble by D-S evidence theory to obtain each validity index basic probability assignment (BPA). Finally, the decision module is used to output the optimal number of the clustering. The paper also discusses the ensemble framework of multi-fuzzy clustering algorithms,
The procedure of the FCM is described as follows:
Step 1: Fix clustering parameter c, fuzzy weighted m and conver- gence threshold ε;
Step 2: Initialize the fuzzy partition membership matrix U0 and cluster center V0;
Step 3: Update the U = uij c×n according to Eq. (3);
Step 4: Update the V = {v1, v2, ⋯, vc} according to Eq. (2);
Step 5: Calculate e=‖Ut+1 —Ut‖. If e ≤ ε (ε is a threshold that usually from 0.001 to 0.01), the FCM stops and outputs optimal c. Else Ut = Ut+1 turn to Step 2 and repeat the flow.




Fig. 1. Clustering process.


Kernel-based FCM clustering algorithm (KFCM)

Chen proposed an novel the FCM clustering algorithm (KFCM) [31].
Different with FCM, KFCM adopts the Gauss kernel function to measure the distance between xj and vi. KFCM uses the kernel function to map the

feature of the i — th cluster, wik∊[0, 1],	l  wik = 1, 1 ≤ i ≤ c. β > 0,
m > 0. V = {v1, v2, ⋯, vc}, each cluster center vi also has l attributes vi =
{vi1, vi2, ⋯, vil}. According to the KFCM algorithm, Gaussian kernel basis function is selected to get JWKFCM, which is shown in Eq. (11).

x of the X to the feature space Φ x ), and then analyzes and calculates to	J
U V	∑ ∑
∑ w βu m 
K x
) )

x2, ⋯, xn to feature space F: Φ : X→Φ xj ∊F. Then the adopted kernel function is defined as Eq. (4).
K xi, xj) = 〈Φ(xi) • Φ xj)〉	(4)
Under the constraints of U and V, the objective function JWKFCM(U, V)
is minimized. The Lagrange extremum condition was used to deduce U
shown in Eq. (12) and V shown in Eq. (13).

)	∑ ∑l
β 1 — K x , v ) )m—1

where,
〈•〉 is the Euclidean inner product of Φ(xi) and Φ xj . The dis-
uij =
k=1 ik
∑
jk  ik
—1
) ) —1
(12)


c	n	vik = 0, if wik = 0

JKFCM(U, V) = ∑ ∑ uijm‖Φ xj) — Φ(vi)‖2	(5)
⎧⎪⎨
∑n  u mK x
)x

⎪⎩ vik = ∑n u m K x , v ) , if wik ∕= 0

)	2	)
)	)	j=1 ij
jk  ik

K(vi, vi) and K xj, vi) select the Gaussian radial basis function (RBF) as
∑  ∑n
u m 1 — K x , v ) )β—1

the operation formula, which is shown in Eq. (6).
wik =	∑=1
) )—1
(14)

( ‖a — b‖2 )
t=1
n j=1
uijm 1 — K xjt , vit


β—1

K(a, b) = exp  —	τ2
(6)
The procedure of the WKFCM is shown as follows: Step 1: Input the τ, c, m, ε and weighted wik = 1/l;

where, τ>0. The value of K xj, xj and K(vi, vi) is 1, so the JKFCM(U, V) can be simply expressed as Eq. (7).
Step 2: Initialize U0 and V0 with FCM;
Step 3: Update the U = uij)c×n according to Eq. (12);

c	n	Step 4: Update the V = {v1, v2, ⋯, vc} for vi = {vi1, vi2, ⋯, vil} ac-

JKFCM(U, V) = 2	uijm 1 — K xj, vi	(7)
i=1  j=1
cording to Eq. (13);
Step 5: Update the weighted coefficient wik according to Eq. (14);
Step 6: Calculate e=‖Ut+1 —Ut‖. When e ≤ ε, or there exists

Initialize UandV, find the minimum value of loss function JKFCM(U, V). The Lagrange extreme condition was used to deduce U as Eq. (8) and V
as Eq. (9):
 1 — K x , v ) )m—1
i(1 ≤ i ≤ c) such that	c uij = 0, the WKFCM stops and outputs optimal c. Else Ut = Ut+1 turn to Step 2 and repeat the process.

uij = ∑c

j  i
K x
—1
v ) )m—1
(8)
2.4. KFCM based on genetic algorithm (GAKFCM)

∑n  uijm K xj, vi)xj
netic Algorithm [34,35] to optimize the V
and then uses the KFCM

i =	n
j=1
uijm K xj, vi)
algorithm to clustering. Genetic Algorithm is introduced to enhance the search speed and accelerate of GAKFCM, will not fall into the local

The procedure of the KFCM is shown as follows:
Step 1: Input the parameters of Gaussian radial function τ, c, m and ε; Step 2: Initialize U0 and V0 with the FCM;
Step 3: Update the U = uij c×n according to Eq. (8);
Step 4: Update the V = {v1, v2, ⋯, vc} according to Eq. (9);
Step 5: Calculate e=‖Ut+1 —Ut‖. When e ≤ ε, or there exists
gradient. V = [v1, v2, ⋯, vc] is the clustering center matrix. Randomly selected several samples from X = x1, x2, ⋯, xn as cluster, and their mean
values are calculated as the initial cluster centers as c cluster centers are obtained as an individual of the initial population by c calculation. Then, the same method is used to generate n individuals as a population P =
{V1, V2, ⋯, Vn}.

i(1 ≤ i ≤ c) such that	c  uij = 0, the KFCM stops and outputs optimal
c. Else Ut = Ut+1 turn to Step 2 and repeat the process.

Weighted Kernel-based fuzzy clustering algorithm (WKFCM)

The FCM assumes that all the attributes of data are equally related.
The loss function is used to judge the quality of individuals in the population and determine the search direction. In the KFCM, the loss function JKFCM(U, V) of individuals in the population is derived from Eq.
(7). According to the JKFCM(U, V), when take the minimum value, the
optimal c is obtained. Therefore, the GAKFCM loss function is defined
with the influence of JKFCM(U, V), as shown in Eq. (15):

But in most states, some attributes are equally related and some attri- butes are not so important. Therefore, sheng proposes WKFCM [32] based on weighted according to the importance of sample features.
1
1 + JKFCM(U, V)	1 + 2  c


n j=1
1
uijm 1 — K xj, vi) )

Given a dataset X = {x1, x2, ⋯, xn}, for each sample xj has l attributes, xj
can be expressed as xj	xj1, xj2, ⋯, xjl . Define the mapping from
dataset X to feature space F: Φ : X→Φ xjl ∊Fl. The purpose of WKFCM is to calculate the minimum of loss function as shown in Eq. (10).
The procedure of the GAKFCM is shown as follows:
Step 1: Input the τ, c, m and ε. Genetic initial crossover probability pc0 (generally 0.4–0.99), initial probability of variation pm0 (generally 0.0001–0.1), genetic evolution algebra T, genetic stop threshold δ;
Step 2: Randomly generate n cluster center individuals as a popula-

c	n	l	；

J	(U, V) = ∑ ∑ ∑ w βu m‖Φ x ) — Φ(v )‖2	(10)
tion P = {V1, V2, ⋯, Vn}

WKFCM

i=1
j=1
k=1
ik  ij	jk	ik
Step 3: Calculate f U, V of individuals in the population, relative
fitness f(j) = q(1 — q)j—1, crossover probability fc(t) = pc0 1 — t ) and

where uij∊[0, 1], ∑c
uij = 1, 1 ≤ j ≤ n. wij is the weighted of the k — th
variation probability fm(t) = pm0 1 — t ), where q is the parameter of



(0,1),j is the sequence number, and t is the current iteration times; Step 4: The t +1 generation population was formed by selection,
crossover and mutation of the t generation population;
Step 5: Calculate f(U, V) = 1∑n  f(k). if |f(t) —f(t + 1) | > δ, then
3.2. Combined validity evaluation method

The study on the WCVEM are not deep at present, mainly using a weighted method for composition. The WCFCVE are shown in Table 3,

t = t + 1, turn to Step 3, else turn to Step 6;
Step 6: Output the V, and use the KFCM to get the final clustering
r i=1
wi =
1, r
= 6, where r means then number of validity functions

number.

Fuzzy clustering validity evaluation methods

Fuzzy clustering validity indexes

The FCVI can be divided into two types. The first is FCVI only based with membership as shown in Table1. Bezdek first proposed the parti- tion coefficient (VPC) and partition entropy (VPE) [36], the structure is simple and the calculation is small, but it cannot overcome the mono- tonicity trend. Silva proposed VMPC to overcome the monotony of VPE. Chen proposed the P index (VP) by introducing the overlap degree, which can better adapt to the overlap samples.
The other is FCVI based with membership and dataset geometry as shown in Table2. Xie and Beni proposed the XB index (VXB) by the ratio of inter-class separation degree to intra-class compactness. Fukuyam and
Sugeno replaced the ‖vi —vj‖ with the ‖vi—v‖ to calculate the degree of
separation between classes, and proposed the FS index (VFS) [37], which gives a better grasp of the overall degree of separation between classes. Know combines VXB and VFS to propose the VK validity function (VK) [38]. Wu changed the scale operation mode and proposed the new validity function (VPCAES) [39] by using the exponential operation method. VPCAES will normalize the distribution coefficient and index and can cope with noisy data well. However, VPCAES relies too much on the clustering center and falls into local optimization. Pakhiar and Ban- dyopadhyay proposed the PBMF clustering validity function (VPBMF) in
the form of product [40]. Wu introducted median vi	vj 2 to improve
i/=j
VXB and proposed the WL validity function (VWL) [41] so as to better deal with noisy data. Haouas improved VWL by introducing a penalty term and proposed HF index (VHF) [42] to prevent VWL from failing due to too small denominator value. Zhu also introducing a penalty term to sup- press the values of compactness and separation, and proposed VZ index
formed. fi(x) is constructed by 1/VDB,  VSIL, VD, V33,  VCH, and
VPBM[44–49]. The WSVF takes an averaged-weighted approach.
Dong proposed the FWSVF by extending the idea of weighted com- binations to fuzzy clustering through WSVF, where r = 4, fi(x) is 1/VXB, 1/VPE, VPC and VPBMF. And wi needs to change according to the dataset.
The FWSVF takes an averaged-weighted approach. Li proposed the WSCVI [50], the weighting method is consistent with the FWSVF, where r represents the number of validity functions, where r = 10,  r wi = 1,
0 ≤ w ≤ 1, CVIi is constituted by 1/VPC, 1/VNPC, VPE, VNPE, VXB, VK, 1/
VPBMF, VFS, VT, and VSC.
Wu proposed the DWSVF [51] based on dynamic weighted, where
wi = sumfi —fi sumfi , which can be changed because of the different validity index, and fi(x) is 1/VMPC, VXB and VPBMF. Wang proposed HWCVF [24] based with a hybrid weighted. whybrid = θwobject + (1 —θ)wsubject,wobject is experts weighted, wsubject is information entropy weighted, θ is the equilibrium factor,Fi(x) is the VMPC, VXB, VK, VP, VPBMF, VWL, VZ, VHY after standardization.

DS theory and FCVI’s BPA matrix
DS theory

The embryonic DS theory was proposed by Dempster in 1967 [52]. Shafer introduce the trust functions into the embryonic DS theory pro- posed an intact uncertainty reasoning theory, that is the Dempster- Shafer (DS) theory [53]. The DS theory is often used to model multi- object information fusion, because it can model uncertainty flexibly and effectively without prior probabilities [54,55].
In the DS theory, Θ is the recognition frame consisted with a finite set
of hypotheses, and all hypothesis are mutually exclusive. z is the number of hypotheses, and the subset A of Θ is called proposition. The power set 2Θ of Θ is composed of all A, can be expressed as Eq. (16):

(VZ) [43]. Wang proposed the new index (VHY) by combining
compactness, separation and overlap to better deal with overlapping data.
2Θ = {A|A ⊆ Θ}
Θ = {θ1, θ2, ⋯, θz}
(16)

In Table 1 and Table 2, where uij is the fuzzy membership matrix, xj is the sample n is the number of xj, c is the clustering number, m is the fuzziness index, vi and vk are two different cluster center, ‖vi —xj‖ is the
v	∑n
	

If there is a function m whose basic probability distribution is a one-
to-one mapping from 2Θ to [0,1], and satisfy the conditions of Eq. (17),
m is the BPA function of Θ:
{ ∑m(A) = 1

1∑c


‖vi—v‖ , and median‖vi — vk‖ is the median distance between vi
m(∅) = 0

and vk. In the absence of any special explanation in this paper, all the operations are matrix operations.



Table 1
FCVI only based on membership.


Proposer	Validity Index Description	Optimal
c


Bezdek and	V  = 1∑c  ∑n  u 2	Max
where, m(A) is the confidence degree of A. When m(A) > 0, A is named focus element, and especially m(∅) = 0 means that A does not belong to the recognition frame Θ. m(A) represents the confidence that only A belongs to Θ. The uncertainty of event A can be described as [Bel(A), Pl(A) ], Bel and Pl are the confidence function and likelihood function of event A, respectively, defined as Eqs. (18) and (19):
Bel(A) = ∑m(B)	(18)

Pl(A) = ∑ m(B)	(19)

Simovici
V  = —1∑c ∑n
[u log u ) ]	Min

[36]
PE	n
i=1
j=1  ij
a  ij
[Bel(A), Pl(A) ] are called confidence intervals, which are the lower

Silva [21]	VMPC = 1 — c
Max
( — PC )
and upper bound of the confidence degree of A, respectively. For

Chen and Linkens

VP =
1∑n




max u


1∑c—1 ∑c

[1∑n


min u , u ) ]
Max
different sources of evidence that are independent of each other, there are different BPA functions. The DS theory synthesis formula uses an






Table 2
FCVI based on geometric structure and membership.
Proposer	Validity Index Description	Optimal c

Xie and Beni [23]



XB =
1∑c



n u m‖v — x ‖2
min‖v — v ‖
Min

i	k
i/=k

Fukuyama and Sugeno [37]	VFS = ∑c
∑n um ‖xj — vi ‖2 —‖vi —ν‖2)	Min

Kwons [38]
∑c ∑n


u 2‖x — v ‖2 + 1∑c
‖v —v‖
Min

min‖v — v ‖2


Wu and Yang [39]
i	k
i/=k
c	n uij 2	c

(—min‖vi — vk‖2 ⎞

Max

VPCAES = ∑i=1 ∑j=1 umj —∑i=1 exp
βT	⎠

Pakhira and Bandyopadhyay [40]
n j=1
uij xj  vi	max  vi  vk  2	Max
i,k=1,⋯,k

VPBMF =
k × ∑k
∑n u ‖x — v ‖	⎠

Wu and Li [41]
∑  (∑n
uij 2 ‖xj — vi ‖2)
Min

i=1	∑n  uij

VWL
	j=1	
min‖v — v ‖2 + median‖v — v ‖2

i	k	i	k
i=/k

Haouas [42]
1∑c  ∑n  u ‖x — v ‖2 +	1
∑c  ∑c
‖v — v ‖
Min

n
VHF =
i=1
j=1 ij  j	i
1


c(c — 1)
i=1
k=1,k/=i i	k

(min‖vi — vk‖2 + median‖vi — vk‖2 )

Zhu [43]
1  minuij
n	i
Min

j=1 ∑c
‖xj — vi ‖

c	c	c(c — 1)
∑k=1 ∑i=1;i=/k ‖vi — v‖/  2

Wang [24]

∑c ∑n



u2 x
v 2  1∑c—1 ∑c	[1∑n
min u , u ) ]
Min

VHY =

min ∑n u2 + min‖v — v ‖2






Table 3
Clustering validity evaluation methods based on weighted combination.
However, there are some disadvantage the DS theory, such as evidential must remain independent, potentially explosive, lack of solid theoretical foundation for evidence synthesis theory, and the controversy of ratio-

Proposer	Definition	Weighted method
Composed validity index
nality and validity [57].

Sheng
maxWSVF =
Mean
1/VDB , VSIL , VD , V33 , VCH ,

[27]
r i=1
wifi (x)
weighted
VPBM
BPA matrix of FCVI

Dong
maxFWSVF =
Mean
1/VXB , 1/VPE , VPC , VPBMF

[38]
r i=1
wifi (x)
weighted
By ensemble the FCVI, r types of results can be generated by judging

Li [50]	minWSCVI =


Mean




1/VPC , 1/VNPC , VPE , VNPE , VXB ,



the optimal c in the same dataset by r FCVE. The scope of c is 2 √̅n̅̅m̅̅̅̅a̅̅x̅̅̅ ,




Wang
r i=1
minHWCVF =
weighted
Hybrid

VMPC , VXB , VK , VP , VPBMF ,
defined as Mc×r
, as shown in Eq. (22):

[24]
∑r whybrid Fi (x)
weighted

VWL , VZ , VHY
⎡ M1 ⎤
⎡ m11  m12  ⋯  m1r ⎤

	i=1	

  	  
M2	m21  m22  ⋯	⋮
=	=	⎦

	


(m ⊕ m ⊕ ⋯ ⊕ mn)(A) =	∑	m (A )⋅m (A )…mn(An) (20)
where m  is the first FCVI belief degree corresponding to c = 2, r = 1,


where m , i = 1, ⋯, n are the BPA Function, k is the conflict coefficient
c = √̅n̅̅m̅̅̅̅a̅̅x̅̅̅. c should be an integer. When c takes all values, the cu-

i
defined as Eq. (21). The closer k is to 1, the more serious the conflict
between the evidence sources. The closer k is to 0, the more consistent
mulative confidence of FCVI should be 1, that is,	c
1, 2, ⋯, r.
mij=1, where j =

the evidence sources.
k = (m1 ⊕ m2 ⊕ ⋯ ⊕ mn)(∅) =
A1 ∩A2 ∩⋯∩An =∅

m1(A1) • m2(A2) • ⋯ • mn(An)
(21)
Different FCVI have different changes when judging the optimal c, and there is also a gap in the dimension. Therefore, the dimension of the FCVE is normalized first, so that the resulting value range falls within [0,1] as shown in Eq. (23):
Fi — Fmin (x)

The DS theory can fusion human subjectivity with the objectivity of
fi = Fmax
i
min
(23)

function. Its advantage is that the description of uncertain information uses “interval estimation” replace “point estimation”. The uncertain aspects, as well as the accurate reflection evidence collection, show
great flexibility, so it has received extensive attention and application.
i  (x) — Fi (x)
where, Fi is the i-th FCVI, Fmin(x) is the minimum of the Fi, and Fmax(x) is
the maximum of the Fi, fi is the value of the Fi after standard normali- zation. However, inputting fi into the confidence matrix directly cannot


satisfy the condition of the	n mij = 1, because the some FCVI selects the maximum as the optimal c, and some FCVI select the minimum value
as the optimal c. Therefore, it is discussed in two cases, when the FCVI takes the maximum as the optimal c, the BPA function is mij = fi/sum(fi). When the FCVI takes the minimum as the optimal c, the BPA function is m	 1—fi  . This can solve the problem of the maximum
1—sum(fi )
and minimum value above.
Multiplying the i-th row to rank and the j-th row in M can obtained
Am×m as shown in Eq. (24):
⎡ mi1 × mj1	mi1 × mj2  ⋯ mi1 × mjr ⎤

calculated by FCM are input into multi-FCVI respectively to obtain the fuzzy clustering validity ensemble based on DS theory (DS-FCVE) [58], as shown in Fig. 2. The flow of the ensemble model is as follows.
The process of this ensemble model is described as follows:
Step 1: Use the FCM to obtain U, c, V and other parameters and then input them into Fi, and obtain the value of different types of the FCVI; Step 2: Normalize the value calculated in Step 1 according to Eq.
(23);
Step 3: Construct the BPA according to the normalized result;
Step 4: Input the value calculated by the BPA function into the in- formation fusion model of the DS theory to compute the belief degree

A = Mi × Mj = ⎢⎣
   
⋮
mir × mj1

 
⋮
mir × mj2

 
⋮	⋮
⋯	mir × mjr
⎥⎦	(24)

Based on the FCM, with the ensemble of its various multi-FCVI, DS- FCVE improves its adaptability to different datasets and can better judge the optimal c.

where, the main diagonal element is the i-th and j-th FCVI, and the cu- mulative confidence of the optimal c target recognition is Z = mip × mjq(p = q). The sum of the residual diagonals is the conflict coefficient k, as shown in Eq. (25).
k =  mip × mjqp, q = (1, 2, 3, ⋯, m)	(25)
p=/q
The design of decision module is described as follows. Set Ai(i = 1, 2,
⋯, n) is the size of , Aω is the value of FCVI. Then calculating the con- fidence mij(θ) of Ai under the evidence framework Θ. The decision module should satisfy this condition: m(Aω) = max(m(Ai) ), and the max- belief degree related to the optimal c. m(Aω) —m(Ai) > δ, (δ > 0), δ is threshold, and the credibility gap between the optimal c and others c
should lager than δ.

Ensemble method based on multi-fuzzy clustering algorithms and Multi-FCVI

Ensemble based on FCM clustering algorithm and Multi-FCVI

It can be learned in Section 4.1 that all events should be independent, and the multi-FCVI are also mutually exclusive. The results of vi and uij

Ensemble based on multi-fuzzy clustering algorithms and Multi-FCVI

By ensemble single fuzzy clustering algorithm with multi-FCVI, such as DS-FCVE, the ability to adapt to the dataset is still poor due to the different ways of updating U0 and V0 of the initial clustering algorithm. In this section, multi-fuzzy clustering algorithms are introduced into DS- FFCVM, and an ensemble model based on multi-fuzzy clustering algo- rithms and multi-FCVI was proposed (DSMFCE). The stability and ac- curacy are improved by changing the structure of DS-FFCVM. DSMFCE is makeup with five parts. The first part, multi-fuzzy clustering algo- rithms and their corresponding updated membership degrees and clus- tering centers are used. The second part is calculating the value of multi- FCVI. The third part is to construct the validity index BPA function by regarding multi-FCVI as a hypothese. The fourth part is to fuse the in- formation of multi-FCVI through the DS theory. The fifth part is to design the decision module then output the optimal c. Therefore, this section integrates the five parts of DSMFCE by using series and parallel ensemble methods, and finally proposes three clustering ensemble models, DSMFCE-I, DSMFCE-I and DSMFCE-III.




Fig. 2. DS-FCVE based on FCM and multi-FCVI.



Fig. 3. Structure diagram of DSMFCE-I.



Fig. 4. Structure diagram of DSMFCE-II.


Parallel clustering ensemble model (DSMFCE-I and DSMFCE-II)
Fig. 3 shows the parallel ensemble model DSMFCE-I. First updates the Uij and Vi by the different clustering algorithms (FCM, KFCM, WKFCM, GAKFCM). Then all of Uij and Vi are inputted separately inputted to the corresponding FCVI to calculate Fi and the value of Fi is normalized to get fi. Third, according fi to construct the BPA. The values
of all BPA functions are input into DS theory to obtained the belief de- gree values of different c. Finally, decision module judges the optimal c corresponding to the maximum confidence. From the structure diagram, it can be found that the multi-FCM, FCVI and BPA function are inde- pendent and parallel before D-S fusion, and they do not interfere with each other. Finally, the overall results are inputted into D-S evidence


theory for realizing the clustering ensemble. More details about DSMFCE-I are shown in the Algorithm 1 pseudocode.

Algorithm 1: DSMFCE-I’s Pseudocode

Input: FCM’s parameters c, ε, and KFCM’s parameters τ, and WKFCM’s parameters
wik = 1/l, and GAKFCM’s pc0, pm0, T, δ, P;
Output: optimal c;
Initialize U0, V0;
For iter = 1: the maximum number of iterations do For i = 2:c do
For j = 1 : n do
Update the FCM’s Uij , Vi according to Eq. (2) and Eq. (3);
Update the KFCM’s Uij , Vi according to Eq. (8) and Eq. (9);
Update the WKFCM’s Uij , Vi according to Eq. (12) and Eq. (13);
Update the GAKFCM’s the Uij , Vi according to Eq. (14) and Genetic Algorithm;
Calculate the e=‖Ut —Ut—1‖;
While e ≥ ε, do
Ut = Ut—1
Else
Return all the values of Uij , Vi ;
End For; End For; End For;
Return all the values of Uij , Vi ;
Calculate the different types of the Fi according to all the values of Uij , Vi ;
Calculate the fi according to Eq. (23);
Calculate the BPA according to the Eq. (24);
Calculate the belief degree corresponding to different c according to the BPA;
Calculate the optimal c according the decision-making module;
Return optimal c;


Fig. 4 shows the parallel ensemble model of DSMFCE-II. DSMFCE-II is similar to DSMFCE-I in structure, which is also based on different values of c, first updated the membership matrix Ui and cluster center matrix Vi by different clustering algorithms input to the corresponding FCVI Fi, then the Fi is normalized to get fi. According to fi frames the BPA function. The values of all BPA functions corresponding to different al- gorithms are inputted into D-S evidence theory. What needs to be focused on is that the values of BPA are input into D-S fusion respec- tively. Finally, the confidence values of cluster number c corresponding to different algorithms are weighted and combined, and the max- confidence corresponding to c is the optimal through decision module. More details about DSMFCE- II are shown in the Algorithm 2 pseudocode.
It can be found from the structure chart that the cluster validity functions corresponding to different algorithms, BPA function and D-S fusion are independent and parallel before re-weighting ensemble, and they do not interfere with each other. Finally, the c corresponding to the maxi-confidence is calculated by weighted combination of individual results, and the value of c is the optimal.

Algorithm 2: DSMFCE- II’s Pseudocode

Input: FCM’s parameters c, ε, and KFCM’s parameters τ, and WKFCM’s parameters
wik = 1/l, and GAKFCM’s pc0, pm0, T, δ, P;
Output: optimal c;
Initialize U, V;
For iter = 1: the maximum number of iterations do For i = 2:c do
For j = 1 : n do
Update the FCM’s Uij , Vi according to Eq. (2) and Eq. (3);
Update the KFCM’s Uij , Vi according to Eq. (8) and Eq. (9);
Update the WKFCM’s Uij , Vi according to Eq. (12) and Eq. (13);
Update the GAKFCM’s the Uij , Vi according to Eq. (14) and Genetic Algorithm;
Calculate the e=‖Ut —Ut—1‖;
While e ≥ ε, do
Ut = Ut—1
Else
Return all the values of Uij , Vi ;
End For; End For; End For;
(continued on next column)

(continued )

Algorithm 2: DSMFCE- II’s Pseudocode

Return all the values of Uij , Vi ;
Calculate the different types of the Fi according to all the values of Uij , Vi ;
Calculate the fi according to Eq. (23);
Calculate the BPA according to the Eq. (24);
Calculate the belief degree corresponding to different c according to the BPA;
Calculate the weighted combination of confidence values of different clustering number c;
Calculate the optimal c according the DS decision-making module;
Return optimal c


Series clustering ensemble model (DSMFCE-III)
Fig. 5 shows the series ensemble model of DSMFCE-III. DSMFCE-III is different from DSMFCE-II and DSMFCE-I that is based on different values of c, updated the membership matrix Ui and cluster center matrix Vi by different clustering algorithms, according to the consensus func- tion (this paper selects weighted average fusion) fusion to get Unew and Vnew, and then Unew and Vnew is input to the FCVI Fi, then the value of Fi is normalized to get fi. Then according to fi, the BPA was calculated. The values of all BPA functions are inputted into D-S fusion, and the confi- dence values of different c are obtained. Finally, the decision module judges that the c corresponding to the maxi-confidence is the optimal. As shown in Fig. 5, it displays that different algorithms are combined with consensus function, and then the membership matrix and cluster center matrix are fused. The ensemble of FCVI and the BPA function are all in serial structuret until the final decision output. Different algorithms restrict each other, and finally the maxi-confidence of the corresponding c is the optimal clusters. More details about DSMFCE- III are shown in the Algorithm 3 pseudocode.

Algorithm 3: DSMFCE- III’s Pseudocode

Input: FCM’s parameters c, ε, and KFCM’s parameters τ, and WKFCM’s parameters
wik = 1/l, and GAKFCM’s pc0, pm0, T, δ, P;
Output: optimal c;
Initialize U, V;
For iter = 1: the maximum number of iterations do For i = 2:c do
For j = 1 : n do
Update the FCM’s Uij , Vi according to Eq. (2) and Eq. (3);
Update the KFCM’s Uij , Vi according to Eq. (8) and Eq. (9);
Update the WKFCM’s Uij , Vi according to Eq. (12) and Eq. (13);
Update the GAKFCM’s the Uij , Vi according to Eq. (14) and Genetic Algorithm;
Calculate the e=‖Unew —Ulast ‖; Calculate the e=‖Ut —Ut—1‖; While e ≥ ε, do
Ut = Ut—1
Else
Return all the values of Uij , Vi ;
End For; End For; End For;
Return all the values of Uij , Vi ;
Calculate the Unew , Vnew according to the all the values of Uij , Vi ; Calculate the different types of the FCVI Fi according to Unew , Vnew Calculate the fi according to Eq. (23);
Calculate the BPA according to Eq. (24);
Calculate the belief degree corresponding to different c according to the BPA;
Calculate the optimal c according the DS decision-making module;
Return optimal c;



Simulation experiments and results analysis

Hyper parameters and experimental datasets

For FCM clustering algorithm, the clustering number c is 2 ≤ c ≤ 14, and the fuzzy weighted m = 2. Decision threshold ε = 0.1, the param- eter of Gaussian radial basis function τ = 150, the initial crossover probability of genetic algorithm pc0 = 0.6, initial mutation probability pm0 = 0.01, genetic evolution algebra T = 100, genetic stop threshold δ = 0.001. The entire algorithm flow iterations times t = 100. The




Fig. 5. Structure diagram of DSMFCE-III.


experiment selected 6 manual datasets and 12 UCI datasets. The manual data given in Fig. 6(a) is 2-dim and 3-class dataset, Fig. 6(b) is 2-dim and 3-class with overlapping samples dataset, Fig. 6(c) is 2-dim and 3-class with noise samples dataset, Fig. 6(d) is 2-dim and 5-class dataset, Fig. 6(e) is 3-dim and 3-class dataset, and Fig. 6(f) is 3-dim and 6-class dataset. All samples of manual datasets shown in Fig. 5 obey normal distribution.
The detailed information of UCI datasets is given in Table 4. The column 1 is the datasets name. The column 2 is the datasets samples number. The column 3 represents datasets attributes, the column 4 represents the datasets classes, the column 5 represents the datasets creator, and the last column represents source of the dataset.
More supplement detailed information about the dataset is as fol- lows. The iris dataset collects 3 types of irises, namely Setosa, Versicolor and Virginica and the iris dataset includes 4 attributes, namely the length and width of calyx, the length and the width of petal. The seeds dataset stores different varieties of wheat seeds such as grain size, habitat and more. The wine dataset records the chemical composition analysis of three varieties of wine in the same region of an Italian region. The heart dataset is relative to heart disease contains a total of 4 data- bases on heart disease diagnosis. The cryotherapy dataset provides in- formation related to patient characteristics, that is, whether the treatment result indicates the cancer level, that is, the malignancy is 0 and the benign is 1. The zoo dataset is a statistical dataset about zoo species information, including attributes of 7 animals such as whether they have tails, legs, fins and other features. The wpbc dataset is following the addition of information to the Wisconsin Breast Cancer Database (wdbc). The balance dataset is designed to simulate psycho- logical experiment results, and the attributes are left weight and dis- tance, right weight and distance. The bupa dataset is on whether adult men will cause liver disease due to alcohol intake. The ibeacon dataset includes 5 attributes of blood tests. The breast dataset is a set of breast cancer datasets. The Led7 dataset indicates that the led display contains 7 light-emitting diodes, and therefore has 7 properties. All the above data sets can be downloaded from UCI website.
The data sets selected in the experiment are different from the cat-
egories, features, and the number of samples, so it can well reflect the adaptability of each validity evaluation method in the face of different types of data sets. VMPC, VXB, VK, VP, VWL, VZ and VHY are selected to
integrate the model, mainly because the clustering analysis algorithms in the paper are fuzzy clustering algorithms, and the fuzzy membership degree and fuzzy index in the validity function of fuzzy clustering can be better integrated with cluster analysis.


Comparative experiments among DSMFCE-I, DSMFCE-II, and DSMFCE-III

The simulation experiments on DSMFCE-I, DSMFCE-II, and DSMFCE- III were carried out by using manual datasets, and results are reflected in
Figs. 7–9. The DSMFCE-I simulation is reflected in Fig. 7 (a)-(b). DSMFCE-I cannot identify the optimal c for data_3_6. The DSMFCE-II
simulation is shown in Fig. 8 (a)-(b). DSMFCE-II can identify the optimal c for all manual datasets. The DSMFCE-III simulation is shown in Fig. 9 (a)-(b). DSMFCE-III could not identify the optimal c for data_3_3, data_2_5, data_3_6. The judgment results of DSMFCE-I, DSMFCE-II, and DSMFCE-III on manual datasets are listed in Table 5.
The simulation experiments on DSMFCE-I, DSMFCE-II, and DSMFCE- III were carried out by using UCI datasets, which results are reflected in Figs. 10–12. The simulation of DSMFCE-I is given in Fig. 10 (a)-(c).
DSMFCE-I is unable to find the optimal c for wine, heart, cryotherapy,
zoo, wpbc, breast, bupa and ibeacon datasets, and even fails to make judgments for bupa and ibeacon data sets, which is because of the evi- dence conflicts. The simulation of DSMFCE-II is shown in Fig. 11 (a)-(c). DSMFCE-II unable to find the optimal c for ibeacon datasets. DSMFCE-III is shown in Fig. 12 (a)-(c). DSMFCE-III can find the optimal c for iris, seeds, heart and balance datasets. The judgment results of DSMFCE-I, DSMFCE-II, and DSMFCE-III under UCI datasets are listed in Table 6.
According to Table 5 and 6, it can be found that DSMFCE-II has the highest accuracy. Compared with DSMFCE-III, DSMFCE-II has a large amount of computation, but it can greatly retain the information after the combination of multi-FCM clustering algorithms and multi-FCVI. DSMFCE-III will lose some information when it fuses membership ma- trix and clustering center matrix. Moreover, compared with DSMFCE-I, DSMFCE-II not only reduces the calculation steps, but also avoids the problem of proposition conflict. In the process of simulation experi- ments, it can be found that DSMFCE-I not only has a low accuracy, but even produces failure results, which directly fails to provide the final evaluation results of clustering validity. For this reason, only DSMFCE-II


 





Fig. 6. Manual datasets.


was selected for comparison in subsequent comparative experiments.


Comparison between DS-FCVE and DSMFCE-II

DS-FCVE is multi-FCVI ensemble based on single Fuzzy clustering algorithm, DSMFCE-II is the ensemble result of multi-fuzzy clustering algorithms and multi-FCVI and then weighted combination to obtain the optimal number of clustering. For this reason, this section compares DSMFCE-II with the DS-FCVE based on GAKFCM, KFCM, WKFCM, and FCM separately. To observe whether the performance of DSMFCE-II improves in the selection of the final cluster number. Validity
functions ensemble based on FCM (DS-FVF), validity functions ensemble based on KFCM (DS-KFVF), validity functions ensemble based on WKFCM (DS-WKFVF), validity functions ensemble based on GAKFCM (DS-GAKFVF) are selected to compared with DSMFCE-II and the simu- lation are given in Fig. 13.
Comparison and simulation of FCVI ensemble based on single fuzzy clustering algorithm and DSMFCE-II is reflected in Fig. 13 (a)-(l). In Fig. 13 (a), only DS-WKFVF cannot get the optimal c for iris dataset. In Fig. 13 (b), the optimal c for seeds dataset can be found by above- mentioned five methods. Fig. 13 (c) shows that DSMFCE-II and DS- GAKFVF can find the correct optimal c for wine dataset and the other



Table 4
UCI datasets.
Data Name	Samples	Features	Classes	Creator	Source
Fig. 13 (f) shows that DSMFCE-II and DS-FVF can get the optimal c for zoo dataset. In Fig. 13 (g), that above-mentioned five methods can find the optimal c for wpbc dataset. Fig. 13 (h) shows that only DS-GAKFVF
cannot get the optimal get the optimal c in balance dataset. Fig. 13 (i)

cryotherapy	90	7	2	Fahime
K.


zoo	101	17	7	Richard
S. F.

iris	150	4	3	Fisher R. A.

wine	178	13	3	Forina
M.
https://archive.ics. uci.edu/ml/ datasets/ cryotherapy https://archive.ics. uci.edu/datase t/111/zoo https://archive.ics. uci.edu/ml/datase ts/iris https://archive.ics. uci.edu/dataset
/109/wine
shows that only DS-KFVF cannot get the optimal get the optimal c for breast dataset. Fig. 13 (j) shows that DSMFCE-II and DS-GAKFVF can calculate the optimal c for bupa dataset. In Fig. 13 (k), all five methods cannot calculate the optimal c for ibeacon dataset. Fig. 13 (l) shows that DS-KFVF and DS-GAKFVF wrongly judge the optimal c for led7 dataset. The results of DS-FVF, DS-KFVF, DS-WKFVF, DS-GAKFVF, and DSMFCE-
II in calculating the optimal c of the UCI datasets are given in Table 7. It can be found from Table 7, under the condition of different data sets, the effectiveness of different clustering algorithms in the same function ensemble results are different, that is because the way of

wpbc	198	34	2	Olvi M.	https://archive.ics.
uci.edu/ml/mach ine-learning- databases
different clustering algorithm update membership degree and the clus- tering center is different. Hence DSMFCE-II in combination multi-fuzzy clustering algorithm information when combines the structure charac-
teristics of multi-FCVI. In this way, even if some algorithms fail, the

seeds	210	7	3	MaA O.
C.
https://archive.ics.
uci.edu/ml/datas ets/seeds
correct c of the datasets can still be get under the weighted combination of the remaining algorithms. Therefore, DSMFCE-II is more stable than

heart	270	13	2	CDC	https://archive.ics.
uci.edu/ml/dataset s/Statlog?% 28Heart%29
breast	286	9	2	Matjaz Z.	https://archive.ics.
uci.edu/ml/dataset s/Breast?Cancer
DS-FVF, DS-KFVF, DS-WKFVF, and DS-GAKFVF.


Comparison experiment between DSMFCE-II and FCVI

DSMFCE-II is the multi-FCVI information ensemble under the sup-

bupa	345	6	2	BUPA
Ltd.

led7	500	7	11	Breiman
L.


balance	625	4	3	Siegler R.
S.


ibeacon	6611	15	13	Apple
lcn.
https://archive.ics. uci.edu/ml/datas ets/Liver?Disorders https://archive.ics. uci.edu/ml/datas ets/LED?Display? Domain https://archive.ics. uci.edu/ml/su pport/balance?sca le https://archive.ics. uci.edu/ml/machi ne-learni
ng-databases
/00435/
port of multi-fuzzy clustering algorithms. For this reason, DSMFCE-II is designed to be contrasted with the common FCVI respectively to observe whether DSMFCE-II has better adaptive capacity than single FCVI in judging the optimal c. In the experiment, VMPC, VXB, VK, VP, VWL, VZ and VHY were selected and compared with DSMFCE-II under the condition of FCM clustering algorithm. The results are given in Table 8. As the FCVI corresponding to each dataset has amount number of values to judge the optimal c, most of which have no comparative significance and have a weak impact on the experimental results.
The simulation results on the UCI datasets are shown in Table 8. The optimal c can be found for iris and balance datasets by VMPC, VHY and DSMFCE-II. The optimal c can be got for seeds dataset by VMPC, VP, VHY and DSMFCE-II. For cryotherapy and bupa datasets only VMPC cannot get the optimal c. For the breast dataset, VMPC, VP and VWL cannot get the

3 methods make mistakes. Fig. 13 (d) shows that DS-GAKFVF could not accurately determine the optimal c found in heart dataset. In Fig. 13 (e), DS-FVF and DS-KFVF cannot get the optimal c for cryotherapy dataset.
optimal c. For led7 dataset, only DSMFCE-II can get the optimal c. For wine and zoo datasets, only DSMFCE-II can get the optimal c. Only VMPC cannot get the optimal c for heart and wpbc datasets. The optimal c cannot be found for ibeacon. The optimal clustering number judgment





Fig. 7. Simulation results of DSMFCE-I under artificial datasets.





Fig. 8. Simulation results of DSMFCE-II under manual datasets.



Fig. 9. Simulation results of DSMFCE-III under manual datasets.



Table 5
Judgment of optimal c by DSMFCE-I, II, and III under artificial datasets.


results of VMPC, VXB, VK, VP, VWL, VZ, VHY and DSMFCE-II on 12 UCI
datasets are shown in Table 10.
Therefore, DSMFCE-II has a good improvement in judging the optimal c compared with the single FCVI. The effectiveness judgment of the single FCVI under the standard FCM clustering algorithm will lead to errors in the final result due to the shortcomings of FCM clustering al- gorithm. DSMFCE-II performs information fusion on the validity func- tion under the condition of multiple FCM clustering algorithms to make it more stable.
Comparative experiment between DSMFCE-II and WCVEM

Weighted combination validity evaluation method (WCVEM) is the ensemble of FCVI research of the important direction, but the selection of weighted way tend to be more difficult. Its core is to put single FCVI weighted combined to form a novel ensemble method, then the effec- tiveness evaluation on the data set. In this paper, four WCVEM methods (HWCVF, DWSVF, WSCVI and FWSCF) were selected to compare with DSMFCE-II to proof whether the ensemble strategy of DSMFCE-II improved stability and accuracy compared with the weighted integra- tion strategy. FCM clustering algorithm was selected to update the val- idity evaluation method of weighted combination. The simulation results are given in Table 9.
In Table 9, 12 groups of UCI datasets were chosen for compare verification. Only FWSCF could not get the optimal c for iris and seeds datasets. Simulation of wine dataset, only DSMFCE-II can get the optimal c. For heart dataset, DWSVF cannot get the optimal c, and WSCVI was not accurate in its judgment. The optimal c cannot be found for cryotherapy data set only with FWSCF. Only DSMFCE-II can get the optimal c for zoo dataset. DWSVF and WSCVI cannot get the optimal c




Fig. 10. Simulation on DSMFCE-I under UCI datasets.



Fig. 11. Simulation on DSMFCE-II under UCI datasets.



Fig. 12. Simulation on DSMFCE-III in under UCI datasets.



Table 6
Judgment of optimal c by DSMFCE-I, II, and III under UCI datasets.


for wpbc dataset. Just FWSCF cannot get the optimal c for balance dataset. DWSVF, WSCVI cannot get the optimal c for breast and bupa datasets. None of the schemes could find optimal c for ibeacon dataset. DSMFCE-II can find the optimal c for led7 dataset.
Table 9 listed the simulation value of 4 WCVEM and DSMFCE-II in judging the best c for 12 UCI datasets. It can be found from the
experimental results that different weighted ways have a large influence to the final simulation results of the WCVEM.

Influence of fuzzy weight on DSMFCVE-II

Fuzzy weight m is often the most widely discussed in fuzzy clustering algorithms. When m = 1, fuzzy clustering degenerates into hard clus- tering. At present, most of the selection of m is between 2.0 and 2.5, and
m = 2.0 is used in the above experimental processes. In order to verify the stability of the proposed model in the face of different m, m=1.5, m=2.0, m=2.5, m=3 and m = 5 were selected in this paper to observe the accuracy of DSMFCVE-II under different m. The selected datasets
include all the manual and UCI datasets used in the appeal experiment. The accuracy of each validity evaluation method under different m is observed.
Table 10 and Table 11 show the accuracy of FCVI, combination modes and three ensemble modes (DSMFCE-I, DSMFCE-II, DSMFCE-III) under all datasets selected in the experiment, as well as the average and standard deviation of accuracy under different value conditions of m. The experimental results show that the average accuracy of DSMFCE-II is the highest, while the average accuracy of FCVI and WCVEM is almost the same that lower then DSMFCE-II. From the perspective of standard







Fig. 13. Comparison between ensemble based on single FCM algorithm and DSMFCE-II.







Fig. 13. (continued).


deviation, DSMFCE-II is not the lowest. The standard deviation of VWL, VZ, VP, DWSVF, FWSCF are all lower than DSMFCE-II. Therefore, the m has a relatively large influence on DSMFCE-II, mainly because DSMFCE-
II is composed of multiple FCVI. Each FCVI is affected by m, so the ambiguity is amplified after ensemble. Moreover, it can be found that the accuracy of DSMFCE-II shows a cliff drop when m = 5.0.





Fig. 13. (continued).



Table 7
Judging results of optimal c by various DS-FCVE and DSMFCE-II under UCI datasets.


Figs. 14 and 15 show the boxplot of the accuracy of FCVI, WCVEM and DSMFCE-I, DSMFCE-II, DSMFCE-III under all datasets selected in the experiment under different m value conditions. The boxplot can visually represent the mean value and the degree of dispersion of the data. It can be seen that DSMFCVE-II has a high fitness value on most datasets, and the result distribution is concentrated with fewer outliers, which proves the robustness of the proposed algorithm. In summary, all the above results show that DSMFCE-II has excellent performance in most cases, significantly outperforming the original algorithm and other
comparison algorithms.

7. Conclusion

As a means to improve the quality of clustering, fuzzy clustering algorithm integration has been widely studied. In this paper, the inte- gration of fuzzy clustering algorithm is extended to the integration of validity index, and the DSMFCE algorithm is proposed. DSMFCE com- bines multi-fuzzy clustering algorithm with multi-FCVI algorithm by DS theory. In the module combination of clustering algorithm, validity


Table 9
Judgment of optimal c by four WCVEM and DSMFCE-II under UCI datasets.



Table 8
Judgment of optimal c by VMPC, VXB, VK, VP, VWL, VZ, VHY and DSMFCE-II under UCI datasets.


Table 10
Average and standard deviation of ensemble model and WCVEM accuracy under different m.
-	DWSVF	FWSCF	WSCVI	HWCVF	DSMFCE-I	DSMFCE-II	DSMFCE-III
m = 1.5	0.19	0.35	0.47	0.62	0.73	0.83	0.32
m = 2.0	0.25	0.42	0.50	0.67	0.83	1.00	0.50
m = 2.5	0.27	0.42	0.49	0.60	0.70	0.92	0.43
m = 3.0	0.15	0.40	0.37	0.50	0.54	0.90	0.30
m = 5.0	0.067	0.27	0.15	0.3	0.33	0.70	0.11
AVG	0.19	0.37	0.40	0.54	0.63	0.87	0.33
STD	0.07	0.06	0.13	0.12	0.17	0.10	0.13




Table 11
Average and standard deviation of DSMFCE-II and FCVI accuracy under different..m

-	VMPC	VXB	VK	VP	VWL	VZ	VHY	DSMFCE-II


m = 1.5	0.50	0.50	0.67	0.50	0.30	0.37	0.60	0.83
m = 2.0	0.33	0.42	0.42	0.42	0.25	0.42	0.58	1.00
m = 2.5	0.60	0.50	0.50	0.50	0.42	0.57	0.55	0.92
m = 3.0	0.46	0.43	0.44	0.42	0.25	0.44	0.47	0.90
m = 5.0	0.15	0.20	0.20	0.30	0.15	0.30	0.32	0.70
AVG	0.41	0.41	0.45	0.43	0.27	0.42	0.50	0.87
STD	0.16	0.11	0.15	0.07	0.09	0.09	0.10	0.10




function and DS theory, three kinds of clustering integration frameworks with serial and parallel structure are constructed. The existing fuzzy clustering effectiveness evaluation methods can be roughly divided into

FCVI and WSCVE. FCVI has its own characteristics, for example, VHY has good effect on overlapping data, VWL and VPCAE have good effect on noisy data, and VPBMF is suitable for processing high-dimensional data. Although the method of WSCVE combines several validity functions, its weight method is difficult to determine. By constructing a probability distribution matrix, DSMFCE proposed in this paper regards FCVI as an event and determines the final optimal number of clusters through the
probabilities of multiple events. Compared with FCVI, DSMFCE’s generalization ability is improved, and no new validity function is
required. DSMFCE does not need to set weights and can avoid the in- fluence of human subjective factors.
In this paper, artificial data sets and UCI data sets are selected to compare DSMFCE with existing fuzzy clustering effectiveness evaluation methods. The experimental results show that the parallel DSMFCE-II has the highest accuracy, because DSMFCE-I inputs all BPA results into DS



Fig. 14. DSMFCE-II and FCVI accuracy box plot Under different m.


Fig. 15. Ensemble model and WSCVE accuracy box plot Under different m.



evidence theory integration at one time, while DSMFCE-II conducts integration using DS evidence theory separately, which is not prone to evidence conflict, but requires a large amount of calculation. While DSMFCE-III adopts weighted average method when updating cluster center and membership matrix, it loses a lot of information, but its computation is the least. When comparing with FCVI and WSCVE, the results show that DSMFCE is more effective than traditional fuzzy val- idity evaluation methods. However, the computation of DSMFCE is much larger than FCVI and WSCVE, and there may be problems of ev- idence conflict. In future work, we plan to propose a clustering algo- rithm ensemble to optimize DSMFCE-III and reduce the efficiency of DSMFCE. In addition, only fuzzy clustering validity evaluation method is used in this paper. If it can be combined with hard clustering validity evaluation method or external validity function, the generalization ability of DSMFCE may be further improved. Moreover, there are more hyperparameters in DSMFCE, which is what we need to further discuss in our future work.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgement

This work was supported by the Basic Scientific Research Project of Institution of Higher Learning of Liaoning Province (Grant No. LJKZ0293), and the Postgraduate Education Reform Project of Liaoning Province (Grant No. LNYJG2022137).

References

Xu, Haoxiang, Research on clustering algorithms in data mining. 2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE). IEEE, 2022.
D’Orazio P. Mapping the emergence and diffusion of climate-related financial policies: Evidence from a cluster analysis on G20 countries. Int Econ 2022;169:
135–47.
Bai L, Liang J, Cao F. A multiple k-means clustering ensemble algorithm to find nonlinearly separable clusters. Inf Fusion 2020;61:36–47.
Kenny G, McCann K, O’Brien C, Savinelli S, Tinago W, Yousif O, et al. Identification
of distinct long COVID clinical phenotypes through cluster analysis of self-reported symptoms. Open Forum Infect Dis 2022;9(4).
Jin J, et al. An agent-based traffic recommendation system: revisiting and revising
urban traffic management strategies. IEEE Trans Syst, Man, Cybernetics: Syst 2022; 52(11):7289–730.
Monshizadeh M, Khatri V, Kantola R, Yan Z. A deep density based and self-
determining clustering approach to label unknown traffic. J Netw Comput Appl 2022;207:103513.
Hartigan JA, Wong MA. Algorithm AS 136: a K-means clustering algorithm. Appl Stat 1979;28(1):100.
Zhang L, Zhang H, Cai G. The multiclass fault diagnosis of wind turbine bearing
based on multisource signal fusion and deep learning generative model. IEEE Trans Instrum Meas 2022;71:1–12.
Zadeh, Lotfi A. Fuzzy sets. In: Fuzzy sets, fuzzy logic, and fuzzy systems: selected
papers by Lotfi A Zadeh. 1996, 394-432.
Bezdek JC, Ehrlich R, Full W. FCM: The fuzzy c-means clustering algorithm. Comput Geosci 1984;10(2-3):191–203.
Chen H, Li C, Mafarja M, Heidari AA, Chen Y, Cai Z. Slime mould algorithm: a
comprehensive review of recent variants and applications. Int J Syst Sci 2023;54 (1):204–35.
Strehl A, Ghosh J. Cluster ensembles–-a knowledge reuse framework for combining
multiple partitions. J Mach Learn Res 2002;3:583–617.
Hong Y, Kwong S, Wang H, Ren Q. Resampling-based selective clustering ensembles. Pattern Recogn Lett 2009;30(3):298–305.
Berikov VB. A probabilistic model of fuzzy clustering ensemble. Pattern Recognit
Image Anal 2018;28(1):1–10.
Bagherinia A, Minaei-Bidgoli B, Hosseinzadeh M, Parvin H. Reliability-based fuzzy clustering ensemble. Fuzzy Set Syst 2021;413:1–28.
Mojarad M, Nejatian S, Parvin H, Mohammadpoor M. A fuzzy clustering ensemble
based on cluster clustering and iterative Fusion of base clusters. Appl Intell 2019; 49(7):2567–81.
Dong X, Yu Z, Cao W, Shi Y, Ma Q. A survey on ensemble learning. Front Computer
Sci 2020;14(2):241–58.
Z. Yao, et al. State evaluation of sight control box based on rough set fusion
improved FCM clustering algorithm. In: 2020 Prognostics and Health Management Conference (PHM-Besançon). IEEE, 2020. p. 207–213.
Wang H-Y, Wang J-S, Wang G. A survey of fuzzy clustering validity evaluation
methods. Inf Sci 2022;618:270–97.
Bezdek JC, Pal NR. Some new indexes of cluster validity. IEEE Trans. Syst., Man Cybernetics 1998;28(3):301–15.
Silva L, Moura R, Canuto AMP, Santiago RHN, Bedregal B. An interval-based
framework for fuzzy clustering applications. IEEE Trans Syst 2015;23(6):2174–87.
Chen M-Y, Linkens DA. Rule-base self-generation and simplification for data-driven fuzzy models. Fuzzy Set Syst 2004;142(2):243–65.
Xie, Lisa X, Gerardo B. A validity measure for fuzzy clustering. IEEE Trans Pattern
Anal Mach Intell 1991;13(8):841–7.
Wang H-Y, Wang J-S, Wang G. Combination evaluation method of fuzzy c-mean clustering validity based on hybrid weighted strategy. IEEE Access 2021;9:
27239–61.
Naderipour M, Fazel Zarandi MH, Bastani S. A fuzzy cluster-validity index based on the topology structure and node attribute in complex networks. Expert Syst Appl 2022;187:115913.
Tang Y, Huang J, Pedrycz W, Li B, Ren F. A fuzzy clustering validity index induced by triple center relation. IEEE Trans. Cybernetics 2023;53(8):5024–36.
Sheng W, Swift S, Zhang L, Liu X. A weighted sum validity function for clustering
with a hybrid niching genetic algorithm. IEEE Trans. Syst., Man, Cybernetics, Part B (cybernetics) 2005;35(6):1156–67.
Dong H, Hou W, Yin G. An evolutionary clustering algorithm based on adaptive
fuzzy weighted sum validity function. In: 2010 Third International Joint Conference on Computational Science and Optimization; 2010. p. 357–61.
Wang G, Wang J-S, Wang H-Y, Liu J-X. Ratio component-wise design method of
fuzzy c-means clustering validity function. J Intell Fuzzy Syst 2022;43(4): 4691–707.
Liu JX, Wang JS, Wang G, et al. Exponent and logarithm component-wise
construction method of FCM clustering validity function based on subjective and objective weighting. Int J Fuzzy Syst 2023;25:647–69.
Girolami M. Mercer kernel-based clustering in feature space. IEEE Trans Neural
Netw 2002;13(3):780–4.
Shen H, Yang J, Wang S, Liu X. Attribute weighted mercer kernel based fuzzy clustering algorithm for general non-spherical datasets. Soft Comput 2006;10(11):
1061–73.
Ding Y, Fu X. Kernel-based fuzzy c-means clustering algorithm based on genetic algorithm. Neurocomputing 2016;188:233–8.
Goldberg DE. Genetic algorithms in search, optimization, and machine learning.
New York: Addison-Wesley; 2012.
Jennings PC, Lysgaard S, Hummelshøj JS, Vegge T, Bligaard T. Genetic algorithms for computational materials discovery accelerated by machine learning. npj
Comput Mater 2019;5(1).
Simovici DA, Jaroszewicz S. An axiomatization of partition entropy. IEEE Trans Inf Theory 2002;48(7):2138–42.
Fukuyama Y, Sugeno M. A new method of choosing the number of clusters for the
fuzzy c-means method. 5th Fuzzy Systems Symposium, 1 989: 247–250.
Kwon SH. Cluster validity index for fuzzy clustering. Electron Lett 1998;34(22): 2176.
Wu K-L, Yang M-S. A cluster validity index for fuzzy clustering. Pattern Recogn Lett 2005;26(9):1275–91.
Pakhira MK, Bandyopadhyay S, Maulik U, Ujjwal M. Validity index for crisp and
fuzzy clusters. Pattern Recogn 2004;37(3):487–501.
Wu C-H, Ouyang C-S, Chen L-W, Lu L-W. A new fuzzy clustering validity index with
a median factor for centroid-based clustering. IEEE Trans Fuzzy Syst 2015;23(3): 701–18.
F. Haouas, A new efficient fuzzy cluster validity index: Application to images
clustering, In: 2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE). IEEE, 2017, pp. 1–6.
Zhu LF, Wang JS, Wang HY. A novel clustering validity function of FCM clustering
algorithm. IEEE Access 2019;7:152289–315.
Vogel MA, Wong AKC. PFS clustering method. IEEE Trans Pattern Anal Mach Intell 1979;PAMI-1(3):237–45.
Hadi AS. Finding groups in data: an introduction to chster analysis. Technometrics
1992;34(1):111–2.
Jain AK, Dubes RC. Algorithms for clustering data. Prentice-Hall Inc; 1988.
Bezdek JC, Pal NR. Some new indexes of cluster validity. IEEE Trans Syst, Man, and Cybernetics, Part B (cybernetics) 1998;28(3):301–15.
Calin´ski T, Jerzy H. A dendrite method for cluster analysis. Commun Statistics- Theory and Methods 1974;3(1):1–27.
Pakhira MK, Bandyopadhay S, Maulik U. Validity index for crisp and fuzzy clusters. Pattern Recogn 2004;37(3):487–501.
Zhou K, Ding S, Fu C, Yang S. Comparison and weighted summation type of fuzzy
cluster validity indices. Int. J. Computers Commun. Control 2014;9(3):370.
Wu Z-F, Huang H-K. A dynamic weighted sum validity function for fuzzy clustering with an adaptive differential evolution algorithm. In: 2010 Third International
Joint Conference on Computational Science and Optimization; 2010. p. 362–6.
Dempster AP. Upper and lower probabilities induced by a multivalued mapping. Ann Math Stat 1967;38(2):325–39.
Yager RR. On the dempster-shafer framework and new combination rules. Inf Sci
1987;41(2):93–137.
Li P, Wei C. An emergency decision-making method based on DS evidence theory for probabilistic linguistic term sets. Int J Disaster Risk Reduct 2019;37:101178.



Lu S, Li P, Li M. An improved multi-modal data decision fusion method based on DS evidence theory. In 2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC) Vol. 1, pp. 1684-1690.2020.June. IEEE.
Yi-Bo L. Based on DS evidence theory of information fusion improved method. In 2010 International Conference on Computer Application and System Modeling (ICCASM 2010) (Vol. 1, pp. V1-416). 2010.october.
Li F, Qian Y, Wang J, Liang J. Multigranulation information fusion: a Dempster-
Shafer evidence theory-based clustering ensemble method. Inf Sci 2017;378: 389–409.
Wang HY, Wang JS, Wang G. Clustering validity function fusion method of FCM
Clustering algorithm based on Dempster-Shafer evidence theory. Int J Fuzzy Syst 2022;24:650–75.
