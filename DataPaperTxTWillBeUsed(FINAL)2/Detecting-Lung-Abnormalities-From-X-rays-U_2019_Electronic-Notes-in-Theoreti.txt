Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 343 (2019) 19–33
www.elsevier.com/locate/entcs

Detecting Lung Abnormalities From X-rays Using an
Improved SSL Algorithm
Ioannis Livierisa,1,2 Andreas Kanavosb,3 Panagiotis Pintelasa,4
a Department of Mathematics, University of Patras,
Patras, Greece.
b Department of Computer Engineering & Informatics University of Patras,
Patras, Greece.

Abstract
A significant component in computer-aided medical diagnosis is the automatic detection of lung abnormal- ities from digital chest X-rays; thus it constitutes a vital first step in radiologic image analysis. During the last decades, the rapid advances of digital technology and chest radiography have ultimately led to the development of large repositories with labeled and unlabeled images. Semi-supervised learning algorithms have become a hot topic of research, exploiting the explicit classification information of labeled images with the knowledge hidden in the unlabeled images. In the present work, we propose a new semi-supervised learning algorithm for the classification of lung abnormalities from X-rays based on an ensemble philosophy. The efficacy of the presented algorithm is demonstrated by numerical experiments, illustrating that reli- able prediction models could be developed by incorporating ensemble methodologies in the semi-supervised framework.
Keywords: Semi-supervised learning; self-labeled algorithms; ensemble learning; majority voting, image classification.


Introduction
Despite the advances in medicine, as well as the development of efficient treatments, the diseases caused by lung abnormalities are considered to be of the greatest lethal diseases worldwide. According to the World Health Organization (WHO), pneumo- nia kills about 1.5 million children under 5 years old every year and only in 2013, it

1 Corresponding author
2 Email:livieris@teiwest.gr
3 Email:kanavos@ceid.upatras.gr
4 Email:ppintelas@gmail.com

https://doi.org/10.1016/j.entcs.2019.04.008
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

was estimated that 1.5 million people died of tuberculosis and 9 million new cases occurred [26].
A typical method for the detection of lung abnormalities consists of a posterior- anterior Chest X-Ray (CXR) in order to search the lung region for any abnormal- ities that could be present. Due to its easy accessibility and relatively low cost, CXR imaging is widely used for health diagnosis and monitoring. In medical cen- ters, the image interpretation has been mostly performed by human experts and it is considered a long and complicated process. Nevertheless, distinguishing the various chest pathologies is a difficult and challenging task, even to the expert hu- man observer. As a result, the area of diagnostic medicine has massively changed; from a rather qualitative science that was based on observations of whole organ- isms to a more quantitative science, which is also based on knowledge extraction from databases [24]. More specifically, research focused on developing intelligent computer-aided diagnosis systems for the automatic recognition of abnormalities from CXRs in order to assist radiologists in identifying and integrating all useful information available in a chest image. These systems incorporate machine learning and data mining techniques in order to exploit vast amount of information provided by patients’ records and laboratory data (see [9, 11, 17, 27, 33, 34, 38] and the ref- erences therein). Along this line, several methodologies and techniques have been proposed, aiming at:
classifying and/or detecting the presence of an abnormality (image classification);
identifying the boundaries of lung for extracting quantitative information and segmenting images into normal and abnormal (medical image segmentation).
Mansoor et al. [25] presented an extended review and explained the capabilities and performance of currently available approaches for segmenting lungs with patho- logic conditions on chest tomography images. Furthermore, they divided the lung field segmentation methods into five broad categories, with an overview of relative advantages and disadvantages of the methods belonging to each group.
Candermir et al. [6] proposed a robust lung segmentation method which de- tects lung boundaries utilizing image retrieval-based patient specific adaptive lung models. Their proposed methodology incorporates non-grid registration with CXR databases of pre-segmented lung regions to develop an anatomical atlas as a guide combined with graph cuts based on image region refinement. They presented a series of experiments utilizing 585 chest radiographs from three different datasets, which demonstrated the efficacy and robustness of the proposed approach. Rajaraman et al. [30] proposed a decision support system which is based on a convolutional neural network to expedite accurate diagnosis of the pathology. Their proposed system de- tects pneumonia in pediatric CXRs and further differentiates between bacterial and viral types to facilitate swift referrals which require urgent medical intervention.
In more recent works, Santosh and Antani [32] developed a novel concept which takes into account right and left lung region changes in terms of symmetry for detect- ing the evidence of tuberculosis. Their method utilizes common pulmonary abnor- malities exhibited in CXR images including cavitations, consolidations, infiltrates,

blunted costophrenic angles, opacities, pleural effusion. Unlike other the state-of- the art techniques, they have proved that the way the features are represented is the appropriate for chest X-ray screening to detect pulmonary abnormalities. Alam et al. [2] developed an efficient lung cancer detection and prediction algorithm using multi-class support vector machine classifier. In every stage of classification, the im- age enhancement and the image segmentation have been done separately. Moreover, image scaling, color space transformation and contrast enhancement have been used for image enhancement while threshold and marker-controlled watershed based seg- mentation has been used for segmentation. Subsequently, a set of textural features extracted from the separated regions of interest is classified and the algorithm can efficiently detect whether the input image contains a tumor or not.
Nevertheless, despite all these efforts, there is still no widely-utilized method; mostly due to fact that the progress in the field has been hampered by the lack of available labeled images for efficiently training an accurate supervised classifier. With the rapid advances in digital chest radiography, the vigorous development of the Internet and the widespread adoption of electronic medical records, research centers have accumulated large repositories of classified (labeled) images and mostly of unclassified (unlabeled) images from human experts. By leveraging these images researchers and medical staff have a significant potential to transform biomedical research and the delivery of healthcare. However, the process of correctly labeling new unlabeled CXRs frequently requires the efforts of specialized personnel and expert physicians, which incurs high time and monetary costs.
To address this problem, Semi-Supervised Learning (SSL) algorithms constitute an appropriate machine learning methodology for extracting useful knowledge from both labeled and unlabeled data. The algorithms comprise characteristics of both supervised and unsupervised learning algorithms in order to efficiently combine the explicit classification information of labeled data with the hidden information in the unlabeled data in order to build efficient classifiers [7, 36]. Self-labeled algorithms are probably considered the most popular class of SSL algorithms, exploiting the unlabeled data via a self-learning process based on supervised prediction models. They perform an iterative procedure, aiming to obtain an enlarged labeled dataset, in which they accept that their own predictions tend to be correct. Recently, Zem- mal et al. [41] implemented a computer assisted detection system for the diagnosis of breast cancer from mammographic images which is based on a semi-supervised support vector machine classifier. Along this line, Livieris et al. [24] proposed a semi-supervised learning algorithm for the classification of chest X-rays of tuber- culosis. Their proposed algorithm exploits the individual predictions of three of the most efficient and frequently used self-labeled algorithms i.e., co-training, self- training and tri-training, using a voting methodology. Their numerical experiments presented the efficacy of the proposed SSL algorithm and its classification accuracy, therefore illustrating that reliable prediction models could be developed utilizing a few labeled and many unlabeled data.
Motivated by their work, we propose a new semi-supervised self-labeled algo- rithm, which is based on an ensemble philosophy. The proposed algorithm ex-

ploits the individual predictions of self-labeled algorithms, using a majority voting methodology. Our preliminary numerical experiments present the efficiency and the classification accuracy of the proposed algorithm, illustrating that reliable pre- diction models could be developed by incorporating ensemble methodologies in the semi-supervised framework.
The remainder of this paper is organized as follows: Section 2 presents a brief description of the semi-supervised self-labeled algorithms and Section 3 presents a detailed description of the proposed algorithm. Section 4 presents a series of exper- iments carried out in order to examine and evaluate the accuracy of the proposed algorithm against the most popular self-labeled classification algorithms. Finally, Section 5 discusses the conclusions and some research topics for future work.

On Semi-supervised Self-labeled Classification Algo- rithms
In this section, we present a formal definition of the semi-supervised classification problem. Let (x, y) be an example, where x belongs to a class y and a D-dimensional space in which xi is the i-th attribute of the instance. Suppose that the training set L ∪ U consists of a labeled set L of NL instances where y is known and of an unlabeled set U of NU instances where y is unknown with NL  NU . Furthermore, there exists a test set T of NT unseen instances where y is unknown, which has not been utilized in the training stage. It is worth noticing that the basic aim of the semi-supervised classification is to obtain an accurate learning hypothesis utilizing the training set, especially when the number of labeled instances is low.
Self-labeled methods constitute prominent SSL methods which address the shortage of labeled data via a self-learning process based on supervised prediction models. This class of algorithms is characterized by their simplicity of implemen- tation as well as their wrapper-based philosophy. From a theoretical point of view, Triguero et al. [36] proposed an in-depth taxonomy based on the main character- istics presented in them and conducted an exhaustive study of their classification efficacy on several datasets. Next, we briefly describe the most relevant self-labeled approaches proposed in the literature which are divided into two main groups: Self- training and Co-training.
Self-training algorithm [40] is considered as one of the most simple and efficient algorithm to leverage unlabeled data. This algorithm wraps around a base learner and utilizes its own predictions to assign labels to unlabeled data. More analytically, a supervised classifier is initially trained on labeled examples and at each iteration the training is augmented gradually with classified unlabeled instances that have achieved a probability value over a defined threshold c. Nevertheless, in case noisy examples are characterized as confident, they can later be incorporated into the labeled training set; hence this technique can lead to erroneous predictions and low classification accuracy [44]. On the other hand, in standard Co-training algo- rithm [5], the attributes of data are split into two conditionally independent views. Subsequently, two classifiers are trained independently in each view and at each

iteration they teach each other the most confidently predicted examples. Nigam and Ghani performed an extensive experimental analysis and concluded that the Co-training algorithm outperforms other self-labeled algorithms when a natural ex- istence of two distinct and independent views exists. Unfortunately, the assumption about the existence of sufficient and redundant views is a luxury hardly met in most real-case scenarios. In general, the self-labeled algorithms proposed in the litera- ture are based on the philosophy of these algorithms while most of them exploit on ensemble ideas and techniques.
The Tri-training algorithm [43] is probably the most representative approach, which is based on the ensemble philosophy, and constitutes an improved single-view extension of the Co-training algorithm. Generally, this algorithm attempts to deter- mine the most reliable unlabeled data as the agreement of three classifiers and it can be considered as a bagging ensemble of three classifiers which are trained on data subsets generated through bootstrap sampling from the original labeled training set [13]. Specifically, in each Tri-training iteration, the labeled set of each classifier is augmented with a unlabeled instance, labeled from the other two classifiers in case it disagrees.
Democratic Co-learning algorithm [42] is based on the idea of ensemble learning and majority voting and follows the multi-view theory but from another aspect. More specifically, instead of demanding for multiple views of the corresponding data, it utilizes multiple algorithms for producing the necessary information and endorses a voted majority process for the final decision. Based on the previous works, Li and Zhou [18] proposed Co-Forest algorithm, which is based on the efficient training a number of Random trees on bootstrap data from the dataset. The basic idea of this algorithm is that the assignment of a few unlabeled examples to each Random tree during the training process. Eventually, the final decision is composed by a simple majority voting. It is worth noticing that the efficacy of Co-Forest is based on the utilization of Random trees, although the number of the available labeled examples is reduced. A rather similar approach was proposed by Hady and Schwenker [13] in which they proposed the Co-Bagging algorithm where confidence is estimated from the local accuracy of committee members. It creates several base classifiers using the same learning algorithm on a bootstrap sample created by random resampling with replacement from the original training set. Each bootstrap sample contains about 2/3 of the original training set, where each example can appear multiple times.
In more recent works, Livieris et al. [24] and Livieris [20] proposed some ensem- ble self-labeled algorithms based on voting schemes. These algorithms combine the individual predictions of three self-labeled algorithms i.e. Self-training, Co-training and Tri-training utilizing a difference combination of voting mechanisms. Motivated by the previous works, in [23] the authors proposed a new semi-supervised learning algorithm, called AAST, which dynamically selects the most promising learner for a classification problem from a pool of classifiers based on a self-training philoso- phy. AAST initially uses several independent base learners and during the training process dynamically selects the most promising base learner relative to a strategy

based on the number of the most confident predictions of unlabeled data.


A new Ensemble Self-labeled Algorithm
In this section, we present a detailed description of the proposed self-labeled algo- rithm, which is based on an ensemble philosophy, entitled EnSL (Ensemble Self- Labeled) algorithm.
Generally, the generation of an ensemble of classifiers considers mainly two steps: Selection and Combination. The selection of the component classifiers is considered essential for the efficiency of the ensemble while the key point for its efficacy is based on their diversity and their accuracy; while the combination of the individual classifiers’ predictions takes place through several techniques with different philos- ophy [10, 31].
By taking these into consideration, the proposed algorithm is based on the idea of generating a set C = (C1, C2,..., CN ) of N self-labeled classifiers by applying different algorithms (with heterogeneous model representations) to a single dataset and the combination of their individual predictions takes place through a majority voting methodology.
A high-level description of the proposed algorithm, entitled EnSL, is presented in Algorithm 1 which consists of two phases: Training and Voting-Fusion phase. In the Training phase, the self-labeled algorithms, which constitute the ensemble are trained utilizing the same labeled L and unlabeled U datasets (Steps 1 − 3). Next, in the Voting-Fusion phase, the final hypothesis on each unlabeled example x of the test set combines the individual predictions of self-labeled algorithms utilizing a majority voting methodology (Steps 4 − 9). An overview of the proposed EnSL is depicted in Figure 1.


Experimental Methodology
In this section, we present a series of experiments in order to evaluate the perfor- mance of the proposed EnSL for X-ray classification against the most efficient and frequently utilized self-labeled algorithms. The implementation codes were written in Java, using the WEKA 3.9 Machine Learning Toolkit [14].
Our experimental results were obtained by conducting a two phase procedure: in the first phase, we evaluate the performance of the proposed algorithm EnSL against the most popular self-labeled algorithms, i.e. Self-training, Co-training, Tri- training, Co-Bagging, CST-Voting, Co-Forest and Democratic-Co learning, while in the second phase, we performed a statistical comparison between all compared semi- supervised self-labeled algorithms.



Algorithm 1 EnSL
Input:	L − Set of labeled instances.
U − Set of unlabeled instances.
C = (C1, C2,..., CN ) − Set of self-labeled classifiers which constitute the ensemble.


/* Phase I: Training */
1: for each Ci ∈ C do
2:	Train Ci using the labeled L and the unlabeled dataset U .
3: end for
/* Phase II: Voting-Fusion */
4: for each x ∈ T do
5:	for each Ci ∈ C do
6:	Apply classifier Ci on instance x.
7:	end for
8:	Use majority vote to predict the label y∗ of x.
9: end for

Output: The labels of instances in the testing set.


Fig. 1. EnSL algorithm

The performance of the classification algorithms is evaluated using the following four performance metrics: Sensitivity (Sen), Specificity (Spe), F -measure (F1) and Accuracy (Acc) which are respectively defined by


  TP		  TN	
Sen =	,	Spe =	,	F1
	2TP	
=
	TP + TN	
Acc =	,

TP + FN
TN + FP
2TP + FN + FP
TP + TN + FP + FN

where TP stands for the number of instances which have been correctly classified as positive, TN stands for the number of instances which have been correctly classified as negative, FP (type I error) stands for the number of instances which have been wrongly classified as positive, FN (type II error) stands for the number of instances which have been wrongly classified as negative.
It is worth mentioning that Sensitivity of classification is the proportion of actual positives that are predicted as positive; Specificity represents the proportion of actual negatives that are predicted as negative, F1 consists of a harmonic mean of precision and recall while Accuracy is the ratio of correct predictions of a classifier.

Datasets
The compared semi-supervised learning classification algorithms were evaluated uti- lizing three different datasets: the chest X-ray (Pneumonia) as well as the CT Medical images dataset.
Chest X-ray (Pneumonia) dataset : This dataset contains 5830 chest X-ray im- ages (anterior-posterior) which were selected from retrospective cohorts of pedi- atric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. For the analysis of chest X-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two ex- pert physicians before being cleared for training the artificial intelligence system. In order to account for any grading errors, the evaluation set was also checked by a third expert. Moreover, the dataset was partitioned into two sets (train- ing/testing), where the training set consists of 5216 examples (1341 normal, 3875 pneumonia) and the testing set of 624 examples (234 normal, 390 pneumonia) as in [16].
CT Medical images dataset : This data collection 1 contains 100 images [3] which constitute part of a much larger effort, focused on connecting cancer phenotypes to genotypes by providing clinical images matched to subjects from the cancer genome Atlas [8]. The images consist of the middle slice of all Computed To- mography (CT) images taken where valid age, modality and contrast tags could be found which results in 475 series from 69 different patients. Furthermore, this dataset is designed in order to allow different methods to be evaluated for examining the trends in CT image data associated with using contrast and pa- tient age. The basic idea is to identify image textures, statistical patterns and

1  https://www.kaggle.com/kmader/siim-medical-images/home

features correlating strongly with these traits and possibly build simple tools for automatically classifying these images when they have been misclassified (or finding outliers which could be suspicious cases, bad measurements, or poorly calibrated machines). Notice that all compared algorithms were evaluated using the stratified 10-fold cross-validation on this dataset.
In order to study the influence of the amount of labeled data, four different ratios
(R) of the training data were used, i.e., 10%, 20%, 30% and 40%.
Performance Evaluation of Semi-supervised Self-labeled Algorithms
In the sequel, we focus our interest on the experimental analysis for evaluating the classification performance of EnSL algorithm against its component self-labeled methods, i.e. Self-training, Co-training, Tri-training, Co-Bagging, CST-Voting, Co-Forest, Democratic-Co learning. Notice that the first five self-labeled methods were evaluated by deploying as base learners the Sequential Minimum Optimization (SMO) [28], the C4.5 decision tree algorithm [29] and the kNN algorithm [1]. These supervised classifiers probably constitute the most effective and popular machine learning algorithms for classification problems [39]. Moreover, similar to Blum and Mitchell [5], a limit to the number of iterations of all self-labeled algorithms is established which has also been adopted by many researchers [19–24, 35–37].
The self-labeled algorithms which constitute the ensemble of EnSL are: Self- training, Tri-training utilizing C4.5 as base learner, Co-training using (SMO) as base learner, Co-Forest and Democratic-Co learning. The motivation for this se- lection is based upon the fact that these algorithms have been reported as the most efficient self-labeled algorithms [36]. We recall that these methods are self- labeled ones, which exploit the hidden information in unlabeled data using different methodologies. More specifically, apart from the number of classifiers used by each method, the key concern is whether they are composed of the same (single) or dif- ferent (multiple) learning algorithms. Self-training, Co-training, Tri-training and Co-Forest are single learning methods while Democratic-Co learning is a multiple learning method.
All self-labeled algorithms utilized the configuration parameter settings as in [4, 24,36]. Furthermore, similar to [20–24] all base learners were used with their default parameter settings included in the WEKA 3.9 library [14] in order to minimize the effect of any expert bias, instead of attempting to tune any of the algorithms to the specific dataset.
Tables 1 and 2 present the performance of all self-labeled methods on Pneumo- nia dataset, using labeled ratio equal to 10% − 20% and 30% − 40%, respectively. Notice that the highest classification performance for each labeled ratio and perfor- mance metric is highlighted in bold. The aggregated results showed that EnSL was the most efficient and robust method, independent of the utilized ratio of labeled instances in the training set. Moreover, it is worth noticing that EnSL exhibited the highest results for Acc and F1 performance metrics.








Table 1
Performance of all self-labeled algorithms on Pneumonia dataset for ratio R = 10% and R = 20%






Table 2
Performance of all self-labeled algorithms on Pneumonia dataset for ratio R = 30% and R = 40%

Tables 3 and 4 present the performance of all self-labeled methods on CT Medical dataset, relative to all performance metrics, using labeled ratio equal to 10% − 20% and 30% − 40%, respectively. As mentioned above, the accuracy measure of the

best-performing self-labeled algorithm is highlighted in bold. Similar observations can be made as well with the previous benchmark. Firstly, it is worth mentioning that the proposed algorithm EnSL demonstrated the best performance. Regarding the F1 and Acc metrics, EnSL exhibited the highest accuracy reporting the top performance in all cases, followed by CST-Voting. Finally, the results clearly show that EnSL increased its classification performance as the labeled ratio increased.





Table 3
Performance of all self-labeled algorithms on CT Medical dataset for ratio R = 10% and R = 20%


Statistical and Post-Hoc Analysis
In machine learning, the statistical comparison of multiple algorithms over multiple datasets is fundamental, and usually it is carried out by means of a statistical test [22–24]. Since our motivation stems from the fact that we are interested in evaluating the rejection of the hypothesis that all the algorithms perform equally well for a given level based on their classification accuracy and highlighting the existence of significant differences between our proposed algorithm and the classical self-labeled algorithms, we utilized the non-parametric Friedman Aligned Ranking (FAR) [15] test. Furthermore, the Finner test [12] is applied as a post-hoc procedure in order to find out which algorithms present significant differences.
Table 5 presents the information of the statistical analysis performed by nonpara- metric multiple comparison procedures. The best (e.g. lowest) ranking obtained in each FAR test determines the control algorithm for the post-hoc test. Furthermore, the adjusted p-value with Finner’s test (pF ) was presented based on the correspond- ing control algorithm, at the α = 0.05 level of significance. It is worth mentioning that the test rejects the hypothesis of equality when the value of pF is less than the








Table 4
Performance of all self-labeled algorithms on CT Medical dataset for ratio R = 30% and R = 40%

value of a. Notice that, Self-training, Co-training, Tri-Training, Co-Bagging and CST-Voting utilized C4.5 as base learner, since they exhibited the best reported performance.
Clearly, EnSL demonstrates the best overall performance, as it outperforms the rest self-labeled algorithms. This is due to the fact that it reports the highest probability-based ranking by statistically presenting better results, relative to all labeled ratio.

Algorithm	FAR	Finner Post-Hoc Test
Table 5
Friedman Aligned Ranking (FAR) test and Finner Post-Hoc test


Conclusions
In this work, we proposed a new ensemble self-labeled algorithm for the detection of lung abnormalities from X-rays, entitled EnSL. The proposed algorithm combines

the individual predictions of efficient self-labeled algorithms utilizing a majority vot- ing methodology. For testing purposes, the algorithm was extensively evaluated on the chest X-rays (Pneumonia) dataset and the CT Medical images dataset utilizing Self-training, Co-training, Tri-training, Co-Bagging and Democratic-Co learning to constitute the ensemble. Our numerical experiments indicated the efficiency and the classification accuracy of the proposed algorithm EnSL, as statistically confirmed by the Friedman Aligned Ranks nonparametric test as well as the Finner post-hoc test. Therefore, we conclude that reliable and robust classification models could be developed by the adaptation of ensemble methodologies in the semi-supervised learning framework.
In our future work, we intend to pursue extensive empirical experiments for comparing the proposed EnSL with other algorithms, belonging to different SSL classes such as generative mixture models, transductive SVMs, as well as graph- based methods. Furthermore, since the experiments’ results are quite encouraging, as a next step, we could consider the evaluation of the EnSL in several biomedi- cal datasets for image classification as well as in specific scientific fields applying real-world datasets, such as educational, financial, healthcare, etc. and explore its performance on imbalanced datasets.

References
D. Aha. Lazy learning. Dordrecht: Kluwer academic publishers, 1997.
J. Alam, S. Alam, and A. Hossan. Multi-stage lung cancer detection and prediction using multi-class svm classifier. In 2018 International Conference on Computer, Communication, Chemical, Material and Electronic Engineering, pages 1–4. IEEE, 2018.
B. Albertina, M. Watson, C. Holback, R. Jarosz, S. Kirk, Y. Lee, and J. Lemmerman. Radiology data from the cancer genome atlas lung adenocarcinoma [TCGA-LUAD] collection. The Cancer Imaging Archive, 2016.
J. Alcal´a-Fdez, A. Fern´andez, J. Luengo, J. Derrac, S. Garc´ıa, L. S´anchez, and F. Herrera. Keel data-mining software tool: data set repository, integration of algorithms and experimental analysis framework. Journal of Multiple-Valued Logic & Soft Computing, 17, 2011.
A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In 11th annual conference on computational learning theory, pages 92–100, 1998.
S. Candemir, S. Jaeger, K. Palaniappan J.P. Musco, R.K. Singh, Z. Xue, A. Karargyris, S. Antani,
G. Thoma, and C.J. McDonald. Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Transactions on Medical Imaging, 33:577–590, 2014.
O. Chapelle, B. Sch¨olkopf, and A. Zien. Semi-supervised learning. MIT Press, Cambridge, MA, 2006.
K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips, D. Maffitt, and M. Pringle. The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository. Journal of Digital Imaging, 26(6):1045–1057, 2013.
Mari Antonius Cornelis Dekker and Sandro Etalle. Audit-based access control for electronic health records. Electronic Notes in Theoretical Computer Science, 168:221–236, 2007.
T.G. Dietterich. Ensemble methods in machine learning. In J. Kittler and F. Roli, editors, Multiple Classifier Systems, volume 1857, pages 1–15. Springer Berlin Heidelberg, 2001.
S. Dua, U.R. Acharya, and P. Dua. Machine learning in healthcare informatics, volume 56. Springer, 2014.
H. Finner. On a monotonicity problem in step-down multiple test procedures. Journal of the American statistical association, 88(423):920–923, 1993.

M.F.A. Hady and F. Schwenker. Combining committee-based semi-supervised learning and active learning. Journal of Computer Science and Technology, 25(4):681–698, 2010.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. Witten. The WEKA data mining software: An update. SIGKDD explorations newsletters, 11:10–18, 2009.
J.L. Hodges and E.L. Lehmann. Rank methods for combination of independent experiments in analysis of variance. The annals of mathematical statistics, 33(2):482–497, 1962.
D.S. Kermany, M. Goldbaum, W. Cai, C.C.S. Valentim, H. Liang, S.L. Baxter, A. McKeown, G. Yang,
X. Wu, and F. Yan. Identifying medical diagnoses and treatable diseases by image-based deep learning.
Cell, 172(5):1122–1131, 2018.
Evelina Lamma, L Maestrami, Paola Mello, Fabrizio Riguzzi, and Sergio Storari. Rule-based programming for building expert systems: A comparison in the microbiological data validation and surveillance domain. Electronic Notes in Theoretical Computer Science, 59(4):397–411, 2001.
M. Li and Z.H. Zhou. Improve computer-aided diagnosis with machine learning techniques using undiagnosed samples. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 37(6):1088–1098, 2007.
C. Liu and P.C. Yuen. A boosted co-training algorithm for human action recognition. IEEE Transactions on Circuits and Systems for Video Technology, 21(9):1203–1213, 2011.
I.E. Livieris. A new ensemble self-labeled semi-supervised algorithm. Informatica, (accepted for publication), 2018.
I.E. Livieris, K. Drakopoulou, V. Tampakas, T. Mikropoulos, and P. Pintelas. An ensemble-based semi-supervised approach for predicting students’ performance. In Research on e-Learning and ICT in education. Elsevier, 2018.
I.E. Livieris, K. Drakopoulou, V. Tampakas, T. Mikropoulos, and P. Pintelas. Predicting secondary school students’ performance utilizing a semi-supervised learning approach. Journal of educational computing research, 2018.
I.E. Livieris, A. Kanavos, V. Tampakas, and P. Pintelas. An auto-adjustable semi-supervised self- training algorithm. Algorithm, 11(9), 2018.
I.E. Livieris, A. Kanavos, V. Tampakas, and P. Pintelas. An ensemble SSL algorithm for efficient chest X-ray image classification. Journal of Imaging, 4(7), 2018.
A. Mansoor, U. Bagci, B. Foster, Z. Xu, G.Z. Papadakis, L.R. Folio, J.K. Udupa, and D.J. Mollura. Segmentation and image analysis of abnormal lungs at CT: current approaches, challenges, and future trends. RadioGraphics, 35(4):1056–1076, 2015.
World Health Organization. Global tuberculosis report 2013. World Health Organization, 2013.
Jos´e Ram´on Pasillas-D´ıaz and Sylvie Ratt´e. An unsupervised approach for combining scores of outlier detection techniques, based on similarity measures. Electronic Notes in Theoretical Computer Science, 329:61–77, 2016.
J.C. Platt. Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge, Massachusetts, 1998.
J.R. Quinlan. C4.5: Programs for machine learning. Morgan Kaufmann, San Francisco, 1993.
S. Rajaraman, S. Candemir, I. Kim, G. Thoma, and S. Antani. Visualization and interpretation of convolutional neural network predictions in detecting pneumonia in pediatric chest radiographs. Applied Sciences, 8(10):1715, 2018.
L. Rokach. Pattern classification using ensemble methods. World Scientific Publishing Company, 2010.
K.C. Santosh and S. Antani. Automated chest X-ray screening: Can lung region symmetry help detect pulmonary abnormalities? IEEE Transactions on Medical Imaging, 37(5):1168–1177, 2018.
M. Sonka, V. Hlavac, and R. Boyle. Image processing, analysis, and machine vision. Cengage Learning, 2014.
K. Suzuki. Machine learning in computer-aided diagnosis: Medical imaging intelligence and analysis. IGI Global, 2012.
J. Tanha, M. van Someren, and H. Afsarmanesh. Semi-supervised self-training for decision tree classifiers. International journal of machine learning cybernetics, 8:355–370, 2015.

I. Triguero, S. Garc´ıa, and F. Herrera. Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and information systems, 42(2):245–284, 2015.
I. Triguero, J.A. S´aez, J. Luengo, S. Garc´ıa, and F. Herrera. On the characterization of noise filters for self-training semi-supervised in nearest neighbor classification. Neurocomputing, 132:30–41, 2014.
Ton van Deursen, Paul Koster, and Milan Petkovi´c. Hedaquin: A reputation-based health data quality indicator. Electronic Notes in Theoretical Computer Science, 197(2):159–167, 2008.
X. Wu, V. Kumar, J.R. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G.J. McLachlan, A.F.M. Ng, B. Liu,
P.S. Yu, Z.H. Zhou, M. Steinbach, D.J. Hand, and D. Steinberg. Top 10 algorithms in data mining.
Knowledge and information systems, 14(1):1–37, 2008.
D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting of the association for computational linguistics, pages 189–196, 1995.
N. Zemmal, N. Azizi, N. Dey, and M. Sellami. Adaptive semi supervised support vector machine semi supervised learning with features cooperation for breast cancer classification. Journal of Medical Imaging and Health Informatics, 6(1):53–62, 2016.
Y. Zhou and S. Goldman. Democratic co-learning. In 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), pages 594–602. IEEE, 2004.
Z.H. Zhou and M. Li. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on Knowledge and Data Engineering, 17(11):1529–1541, 2005.
X. Zhu and A.B. Goldberg. Introduction to semi-supervised learning. Synthesis lectures on artificial intelligence and machine learning, 3(1):1–130, 2009.
