	Electronic Notes in Theoretical Computer Science 204 (2008) 163–179	
www.elsevier.com/locate/entcs
Minimality in a Linear Calculus with Iteration
Sandra Alvesa M´ario Floridoa Ian Mackieb Fran¸cois-R´egis Sinota
a Universidade do Porto, DCC/LIACC, Rua do Campo Alegre 1021–1051, Porto, Portugal
b LIX, CNRS UMR 7161, E´cole Polytechnique, 91128 Palaiseau, France

Abstract
System L is a linear version of G¨odel’s System T , where the λ-calculus is replaced with a linear calculus; or alternatively a linear λ-calculus enriched with some constructs including an iterator. There is thus at the same time in this system a lot of freedom in reduction and a lot of information about resources, which makes it an ideal framework to start a fresh attempt at studying reduction strategies in λ-calculi. In particular, we show that call-by-need, the standard strategy of functional languages, can be defined directly and effectively in System L, and can be shown minimal among weak strategies.
Keywords: linearity, iteration, System T, strategies, efficiency, minimality, optimality

Introduction
G¨odel’s System T is an extremely powerful calculus: essentially anything that we want to compute can be expressed [14]. A linear variant of this well-known calculus, called System L, was introduced in [1], and shown to be every bit as expressive as System T . The novelty of System L is that it is based on the linear λ-calculus, and all duplication and erasing can be done through an encoding using the iterator.
There are many well-known, and well-understood, strategies for reduction in the (pure) λ-calculus. When investigating deeper into the structure of terms, we get a deeper understanding of reduction (and vice versa). For instance, calculi with explicit resource management or explicit substitution enjoy a more fine-grained reduction. In a similar way, System L splits the usual λ in two different constructs: a binder, able to generate a substitution, and an iterator able to erase or copy its argument. This entails a finer control of these fundamentally different issues, which are intertwined in the λ-calculus. Having a calculus which offers at the same time a lot of freedom in reduction and a lot of information about resources makes it an ideal framework to start a fresh attempt at studying reduction strategies in λ-calculi.

1571-0661© 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.03.060

This paper is a first step towards a thorough study of reduction strategies for System L. The main contributions of this paper are:
we present, and compare, different ways of writing the reduction rules associ- ated to iterators;
we define a weak reduction relation for System L (we call this new system weak System L) similar to weak reduction used in the implementation of functional programming languages, where reduction is forbidden inside abstractions;
we present reduction strategies for the weak reduction relation: call-by-name, call-by-value, and call-by-need (emphasising this last one), proving that they are indeed strategies in a technical sense. Since neededness is usually undecid- able, extra features (like sharing graphs, environments, explicit substitutions) are generally added to actually implement call-by-need. In contrast, for System L, we can define call-by-need within the calculus in an effective way.
we give a proof of minimality of the call-by-need strategy. It is well-known that there exists no computable minimal strategy for the λ-calculus [5]. One of the main contributions in this paper is a computable (and effective) minimal strategy for weak System L.
The rest of this paper is structured as follows. In the next section we present some related work. In Section 3, we recall some background on rewriting and System
L. In Section 4 we discuss the issues about strategies and choices. In Section 5 we define weak reduction in System L, and study different weak strategies in Section 6. Finally, Section 7 concludes the paper.

Related Work
In [4] (see also [5, Chapter 13]), a notion of L-1-optimal (or minimal 1 , in this paper) strategy for the λ-calculus is defined as a normalising strategy, minimal with respect to the length of paths in the terms reduction graph. It was shown that there exists no computable optimal L-1-strategy for the λ-calculus. One of the main contributions in this paper is a set of computable minimal strategies for weak System L (a version of System L where reduction is forbidden inside abstractions). Weak System L is very much inspired in weak λ-calculi [20,21,3,6], weak reduc- tions for functional programming languages [23] and lazy evaluation models [18,27]. In all these works reduction is forbidden inside abstractions and lazy evaluation is achieved by enlarging the calculus with extra syntax (graph reductions [27], explicit let bindings [3,20] or explicit heap [18]) to express sharing of subterm evaluation. In this paper we present a set of minimal strategies for weak System L with the same features as lazy evaluation: there is no loss of sharing except inside abstractions, and we only reduce terms that are actually used, but we insist that our definition is effective, within the calculus. This is possible due to the finer control of linear sub- stitution and copying using the iterator, as opposed to the λ-calculus where these

1 In [15], the notion of minimality is different.

different issues are mixed up.
The notion of reduction in System L, called closed reduction, is already weak in the sense that it imposes strong constraints on the application of reduction rules (see [11]). In this paper we define a weaker form of reduction: closed reduction with- out reduction inside abstractions. The main motivation for this further constraint is to define a simple computable minimal strategy for weak System L.
Some previous works studied the relation between recursion and iteration in System T [22,8,24] showing that, in many cases, recursion is more efficient than iteration. We choose to use iteration in System L because it avoids the duplication of a variable, and it is then more suitable within a linear setting, such as System
L. Thus, in this paper, efficiency should be understood in this setting: a linear calculus with iteration. Another reason why we insist on using a linear discipline (and thus an iterator instead of a recursor) is that efficiency in a linear calculus, such as System L, can be measured by the number of steps to normalise terms, because each reduction either decreases the size of the term (by linear β-reduction) or increases it by adding the size of the iterated function (applying the iteration reduction rule). This is no longer true in a non-linear setting, such as System T , where the number of reduction steps cannot be used as a measure of efficiency (see [9] for a detailed discussion about the problems of naively using reduction steps as a measure of efficiency in a non-linear calculus).
In general, call-by-need (and even minimal strategies) may copy expressions in some situations (for example inside abstractions). Sharing of subterms across different instantiations of bound variables is addressed by optimal reduction strate- gies [19,17,12,21,28]. Although this line of research applied to System L is a promis- ing one, optimal reduction in this sense is not an issue in this paper: here we follow the weak reduction approach, as is standard in the implementation of functional languages [23].

Background
Rewriting
We briefly recall some definitions, and refer the reader to [25] for more details.
Definition 3.1 An abstract reduction system (ARS) is a directed graph (A, →). We write t → u if there is an edge in → from t to u. The reflexive transitive closure of → is →∗, and ← is the inverse relation of →. A normal form is a t ∈ A such that there exists no u such that t → u. We also write t →n u if t `→ ·˛·¸· →x u.
n
Definition 3.2 → is said to have the diamond property 2 if, whenever u1 ← t → u2
with u1 /= u2, there exists a v such that u1 → v ← u2. → is said to be confluent if
→∗ has the diamond property. → is said to be strongly normalising if there is no object admitting an infinite →-reduction path.

2 This property is called CR1 in [25, Ex. 1.3.18], where “diamond property” means something else.

Definition 3.3 A strategy for an ARS (A, →) is a sub-ARS (A, d) of (A, →) (i.e. such that d ⊆ →) with the same normal forms.
Note that this definition is more liberal than others (e.g. [5]), in the sense that a strategy is not required to be deterministic.
Definition 3.4 A →-strategy d is normalising if all d-reduction paths starting from an object, which admits a finite →-reduction path to normal form, are finite. It is minimal 3 if the length of any d-reduction from an object a to a normal form b is minimal among all possible →-reductions from a to b.
We do not want to recall too much about higher-order rewriting. The systems defined in this paper will fit the framework of context-sensitive conditional expres- sion reduction systems (CERS) [16]. In particular, the notion of residuals make sense in these systems [7].
Definition 3.5 A redex is needed if some residual of it must be fired in any reduc- tion to normal form.
In [26], van Oostrom gives a method to reduce the global problem of proving that a strategy is minimal (or maximal), to a verification of certain properties of local reduction diagrams. To avoid recalling all that work here, we combine some parts of Theorems 1 and 2 of [26], as the following theorem, which will be used to show the minimality of call-by-need among weak strategies (Theorem 6.8).
Theorem 3.6 Let d be a →-strategy. If, whenever s D t → u, either u admits an inﬁnite d-reduction or there exists an r such that s →n r Dm u with n ≤ m, then d is normalising and minimal.
System L
In this section we recall the syntax and reduction rules of System L [1]. Table 1 gives the syntax of System L. The set of linear λ-terms is built from: variables x, y, . . .; linear abstraction λx.t, where x ∈ fv(t); and linear application t u, where fv(t) ∩ fv(u) = ∅. Here fv(t) denotes the set of free variables of t. These conditions ensure that terms are syntactically linear (variables occur exactly once in each term).
Since we are in a linear calculus, we cannot have the usual notion of pairs and projections; instead, we have pairs and splitters which use both projections, as shown in Table 1. A simple example is the swapping function (see below).
Finally, we have booleans true and false, with a linear conditional; and numbers (built from 0 and S), with a linear iterator. Sn0 denotes n applications of S to 0.
The dynamics of the system is given by the set of conditional reduction rules in Table 2. The system fits in the framework of context-sensitive conditional expression reduction systems (CERS) [16]. The conditions on the rewrite rules ensure that Beta only applies to redexes where the argument is a closed term (which implies that α- conversion is not needed to implement substitution), and only closed functions are

3 Minimality is called L-1-optimality in [5] and simply optimality in [28].

iterated. Table 2 gives the reduction rules for System L, substitution is a meta- operation defined as usual. Reductions can take place in any context where the conditions are satisfied.


Table 1 Terms

Table 2 Closed reduction

We give some examples to illustrate the system:
Swapping: swap = λx.let ⟨y, z⟩ = x in ⟨z, y⟩.
Erasing numbers 4 : although we are in a linear system, we can erase (more pre- cisely: consume) numbers by using them in iterators.
fst = λx.let ⟨t, u⟩ = x in iter u t (λz.z)
snd = λx.let ⟨t, u⟩ = x in iter t u (λz.z)
Copying numbers: C = λx.iter x ⟨0, 0⟩ (λx.let ⟨a, b⟩ = x in ⟨S a, S b⟩) takes a number n and returns a pair ⟨n, n⟩.
Addition: add = λmn.iter m n (λx.S x)
Multiplication: λmn.iter m 0 (add n)
Predecessor: λn.fst(iter n ⟨0, 0⟩ (λx.let ⟨t, u⟩ = C(snd x) in ⟨t, S u⟩))

4 Some terms t can be erased with iter 0 t u, but only those such that the construction is well-typed.

Ackermann: ack(m, n) = (iter m (λx.S x) (λgu.iter (S u) (S 0) g)) n
System L is essentially a typed calculus (this further restriction still fits in the framework of CERSs), and most of the properties stated in the remainder of this paper rely on this in a crucial way, although some properties are also valid in the untyped calculus (this will always be stated explicitly). We write Γ ▶L t : A if the term t has type A in the environment Γ, where A is a linear type: A, B ::= Nat | Bool | A −◦ B | A ⊗ B where Nat and Bool are the types of numbers and booleans. The full details of the type system are not essential for the remainder of this paper and are thus omitted. The type system, and further details, including a type reconstruction algorithm, can be found in [2].
Lemma 3.7 System L is an orthogonal [16] CERS.
Proof. Apart from easy syntactic verifications, we have to notice that all descen- dants of a redex are redexes. This comes from the preservation of linearity con- straints, subject reduction [1], and the fact that a closed term cannot become open during reduction.	 
As a corollary, this reproves confluence of System L [1]. We also recall from [1] that typable terms are strongly normalising and:
Theorem 3.8 (Adequacy) If t is closed and typable, then one of the following holds:
▶L t : Nat and t →∗ Sn 0 for some integer n;
▶L t : Bool and either t →∗ true or t →∗ false;
▶L t : A −◦ B and t →∗ λx.u for some term u;
▶L t : A ⊗ B and t →∗ ⟨u, w⟩ for some terms u, w.
(For a proof, we refer the reader to the proof of Theorem 4 in [1].)

Intuitions and Choices
Here we emphasise what the exact choices are when defining reduction strategies in System L, in particular, from an efficiency point of view.

Efficiency.
Semantically, we have: iter (Sn0) u w = wn(u) (i.e. n copies of w applied to u). However as shown in the given rewrite rules, we actually make use of n + 1 occur- rences of w, and then throw one away. To circumvent this defect we change the definition in order to stop at S0 rather than 0:
iter 0 u w	→ u	fv(w) = ø
iter (S 0) u w	→ wu	fv(w) = ø
iter (S(S t)) u w → w(iter (S t) u w)	fv(t) = fv(w) = ø

There is no strong motivation behind the condition on the second rule, except to ensure the conservativity of the new rules with respect to System L. It is clear that the last two rules split the previous one, and because there are only two cases to consider in the pattern matching (S and 0) then this will not have any consequences on any of the results of System L, which can be stated as follows:


Proposition 4.1 Let us call old
the reduction relation deﬁned in Section 3.2 and

−−→ the reduction relation with the modiﬁed rules for iter above. Then:

if
new
old n


if
t −−→ u, then t −−→
old
u with n = 1 or n = 2;
old ∗ new

old ∗

t −−→ u, then there exists a w such that t −−→ −−→ w and u −−→ w;
v is a new -normal form if and only if v is a old -normal form;

new
−−→
is strongly normalising;
−−→

if v is a normal form (for old
−−→), then
new
t −−→
v if and only if
old
t −−→ v

new
Proof.
is confluent.

Straightforward.
The problem in this case is that a old -redex is not necessarily a new -redex:

if we have iter (St) u w
old
−−→ w
(iter t u w), we know that t is closed, and

by adequacy (Theorem 3.8), t −−→	S 0 for some n ≥ 0, so that, in the

case n ≥ 1, iter (St)
u w −−→  iter (S	0)
new
u w −−→ w
(iter (Sn 0) u w) and

w(iter t u w) old ∗
w(iter (Sn 0) u w), and similarly in the case n = 0.

Consequence of Points i and ii.
Consequence of Point i and strong normalisation: suppose we have an infinite
new	old
−−→-reduction, then we obtain an infinite −−→-reduction.
The “only if” part is a consequence of Point i.  For the “if” part, assume
old	new
t −−→ v with v a normal form. Using Point iv, consider w such that	∗ w
and w is a normal form. By the “only if” part of this point, we know that
old ∗
t −−→ w, thus v = w by the unicity of normal forms in System L (consequence
of the confluence of System L).

Assume
new
t −−→
u1 and
new
t −−→
u2. By Point i, we also have
old
t −−→
u1 and

old	old	old
t −−→  u2. By confluence, there is a w such that	1	∗ w and	2	∗ w.
Using strong normalisation, let v be the old -normal form of w. Then, using

Point v,
new ∗
1
and
new ∗
2

u −−→ v	u −−→ v


Of course, if we are considering an untyped calculus, where non-terminating

computations can be represented, then old
new
−−→
are not equivalent as we now

require to force more evaluation to complete the pattern matching: let Δ be the term (λx.iter (S20) (λx1x2.x1x2) (λz.zx)) and Ω be the non-terminating (untyped) term ΔΔ. Then iter (S Ω) I (λx.iter 0 I x) will terminate with the old system but

not with the new. In other words, iter is now more strict in its first argument. We use this version, because efficiency is now an issue. From now on, → means new .

Alternative iteration.
There are two ways of writing the rules for an iterator. The one given above (both
old	new
−−→ and −−→) which we shall call outer-iter (and denote →out) and also this one, which we shall call inner-iter :
iter 0 u w	→in u	fv(w) = ø
iter (S 0) u w	→in wu	fv(w) = ø
iter (S(S t)) u w →in iter (S t) (w u) w	fv(t) = fv(w) = ø
We remark the relation with fold right and fold left for lists in functional program- ming. These operators encapsulate recursion patterns on lists, in the same way as an iterator on numbers encapsulates recursion patterns on numbers. The difference between foldl and foldr is simply the order in which the elements of the lists are accessed: left-to-right, or right-to-left. A left-to-right approach can start working on elements of lists, even infinite lists, whereas the right-to-left approach works well in the finite case (i.e. it is strict in the list). The same reasoning applies to our iterator. Of course, the origins of these operators on lists are indeed iterators on numbers (primitive recursive schemes).
With the inner-iter reduction policy, iter is strict in its first argument. For example, in an untyped calculus, if the number is not terminating, then neither is the iter (irrespectively of the evaluation order). This will not be a problem in System L because it is a strongly normalising calculus.
Now we have a whole collection of strategies to look at: leftmost and outermost with each of the alternatives gives different strategies. For instance, if we use inner- iter with leftmost reduction, then we get iter evaluated first. If we have outermost with outer-iter, then we compute the applications first, etc. And of course, we are interested in finding the “best” combination.
The next results show that extending System L with this new form of iteration does not change the calculus itself (although it gives one more way to reduce itera- tors), and that both ways of reducing iterators essentially use the same number of steps when the number of iterations is known.
Lemma 4.2 For any number n ≥ 1, any term u and any closed term w, we have:

iter (Sn 0)
out
u w −−→
wn(u) n in
iter (Sn 0) u w.

Proof. Straightforward by induction on n.	 
Theorem 4.3 If we add the rules corresponding to inner-iteration (→in) to reduc- tion rules of System L (→out), we get a new system (→i+o=→out ∪ →in) with the following properties:
subject reduction;
strong normalisation;

confluence;
the normal form of a term is the same using →out, →in or →i+o.
Proof.
Subject reduction: Straightforward.
Strong normalisation:  Adapt the proof for System T based on reducibil- ity [14]. Let ν(t) bound the length of every normalisation sequence beginning with t, and let l(t) be the maximal number of symbols in all reachable nor- mal forms of t (there are finitely many thanks to Koenig’s lemma). We prove that if t, u and w are reducible, then iter t u w is reducible, by induction on ν(t)+ ν(wn(u)) + ν(w)+ l(t), where Sn(0) is the normal form of t.
Confluence: Let us first note that, because of inner-iter, we lose confluence of the untyped calculus. For example


w(iter true u w) out
iter S(true)
in
u w −→
iter true (w u) w.


Now for typed terms (System L), let us consider the only critical pair:


w(iter t u w) out
iter S(t)
in
u w −→
iter t (w u) v

Since t is closed and typable, then t →∗ (Sn0), therefore
w(iter t u w)	iter t (w u) w

∗ v
w(iter (Sn0) u w)
∗ v
w(wn(u))
∗ v
iter (Sn0) (w u) w
∗ v
=	wn(w u)

The result follows using Newman’s Lemma.
Normal forms: →out and →in can be seen as strategies of →i+o, i.e. the notion of normal form is the same for the three reductions. For instance, consider a term t, v a →i+o-normal form of t and w a →in-normal form of t (both exist because →i+o is strongly normalising and →in ⊂ →i+o). But w is also a →i+o- normal form of t, hence v = w since →i+o is confluent (unicity of normal forms).


Iteration vs. β-reduction.
In the linear λ-calculus, where each bound variable occurs exactly once, it is known that all computation is useful and is used exactly once. In System L, this is true at the level of abstraction, but we have the power of copying and erasing at the level of the iterators. We therefore claim that the choice at the level of β-reduction is inessential; what only matters is the choice in the iterator.

Here we present several reduction strategies for iterators, following their coun- terpart definitions for β-reductions in the λ-calculus and functional programming languages. These reduction strategies are defined for System L (where every term is linear), thus the only problematic reductions are in the iterator case.
Basically, we have the choice to reduce as much as possible inside iterators before firing them, or not. But we also have the choice to give the preference to outer-iter or to inner-iter. In fact, since the inner-iter reduction policy makes iter strict, it
makes a lot more sense to use either call-by-name and outer-iter together, or call- by-value and inner-iter. Below, we only show the rules for the iterator, assuming that it is properly lifted to any context.

Outer iteration by name.
Iteration by name reduces the leftmost outermost iterator first. It is closely related to Engelfriet and Schmidt’s outside-in derivation for context-free grammars or first- order recursion equations [10].
iter 0 u w	→ u	fv(w) = ø
iter (S 0) u w	→ wu	fv(w) = ø
iter (S(S t)) u w → w(iter (S t) u w)	fv(t) = fv(w) = ø

There is no syntactical constraint on w, so that outermost reduction is possible.

Inner iteration by value.
Iteration by value reduces leftmost innermost iterators first. It is closely related to Engelfriet and Schmidt’s inside-out derivations.
iter 0 u v	→ u	fv(v) = ø
iter (S 0) u v	→ vu	fv(v) = ø
iter (S(S t)) u v → iter (S t) (v u) v	fv(t) = fv(v) = ø

where v is some notion of normal form (in the sequel, it will be that of value).

The Weak System L
Weakness of System L reduction
Although reduction in System L is allowed in any context, in particular under λ-abstractions, it is already somehow weaker than usual strong reduction for the λ-calculus, due to the use of closed reduction (free variable conditions on the rules). In particular, normal forms may still contain iterators. Note that we can however always compute the weak head normal forms of closed terms (see [1]).
We note the following:
Normal forms of closed terms of functional type may contain iterators. For in- stance, T = λx.iter (S20) I x is a normal form.

We also remark that T could be an argument to a function, and thus values are not the normal forms we could think of, even if we allow reduction under an abstraction. In other words, there is no strategy that will always allow us to avoid copying an iterator. For instance, in iter (S20) (λx.x) (λx.iter (S20) I x), the argument λx.iter (S20) I x is a normal form, so it will be copied by the other iterator no matter which strategy we are using.


Weak System L
In the λ-calculus, two views of the notion of function coexist. One of them is that functions are ordinary syntactic objects, on which we can compute. The other sees functions as abstract objects inside which it is not sensible to compute; as pieces of programs which have to wait for their argument before executing. This opposition can be seen in the following rule:


t → v λx.t → λx.v

(ξ)

This rule is part of the λ-calculus, but a strategy of the λ-calculus is free to contain it or not: in the first case, the strategy is said to be strong, in the second, it is weak. In general, weak strategies cannot reduce beyond weak head normal form, thus they are not strategies of the λ-calculus in the sense of Definition 3.3.
Weak strategies are those used in functional programming languages [23,13]. In fact, it is more convenient to see weak strategies of the λ-calculus as strategies of a weak λ-calculus, along the lines presented in [21].
Here we present a weak version of System L (with outer-iter, so with the rules in page 6), which we call weak System L with the same restriction as in ordinary weak reduction: do not reduce under abstractions, i.e. we remove the (ξ) rule. Similarly, reduction in the second argument of let constructs should also be prohibited. We also forbid reduction inside pairs, so as to avoid computations in the first argument of let constructs that are not needed in order to reach a pair. We do allow reduction
under a S, though, as well as under cond and iter. The new calculus is defined as:

Definition 5.1 The weak System L is the calculus with reduction →w, defined by allowing System L reduction → in any weak evaluation context W , defined as follows:

W ::= [ ] | W t | tW | S W | let ⟨x, y⟩ = W in t
| cond W u w | cond t W w | cond t u W 
| iter W u w | iter t W w | iter t u W 


There is still a lot of freedom to define strategies, in particular in the iter case.

Confluence
In the λ-calculus, it is well-known that removing the (ξ) rule leads to a non-confluent calculus, as evidenced by the following diverging pair, where I = λx.x (see e.g. [21]):
λy.y (I I) ← (λxy.y x) (I I) → (λxy.y x) I → λy.y I

This has led to the introduction of frameworks such as supercombinators or explicit substitutions [21], which is not completely satisfactory either, because these systems are usually more complicated. We have the same kind of restriction here, hence non- confluence of the weak calculus is expected (as opposed to “stronger” weak calculi like [6]). However, like in other weak λ-calculi, weak System L is confluent for programs: closed terms of base type.
Definition 5.2 A program is a closed System L term of type Nat or Bool.
Definition 5.3 We call values the closed normal forms for →w.
Proposition 5.4 Values are the closed terms of this form:
v ::= Sn 0 | true | false | ⟨t, u⟩ | λx.t

Proof. By adapting the proof of adequacy (again, this result is not valid in the untyped calculus).	 
In the following, a term denoted by v will always be assumed to be a value.
Proposition 5.5 →w is confluent on programs.

Proof. Assume t →∗
u1 and t →∗
u2, where t is of base type. Consider v1 and v2

the →w-normal forms of u1 and u2 respectively. v1 and v2 are values of base types,
hence are also normal forms for → (using Proposition 5.4). But → is confluent, thus has the property of unicity of normal forms. We conclude v1 = v2.	 

Weak Strategies
We are now in a position to define reduction strategies for the weak System L similar to known strategies for the weak λ-calculus. In this section, all strategies are weak: they perform no reduction under abstraction, and, consistently, they are defined only on closed terms. We essentially just mention call-by-name and call-by-value, while we will give more details on call-by-need, which can interestingly be defined directly in the calculus, in an operational way.
Call-by-name and call-by-value
Definition 6.1 Call-by-name reduction is leftmost outermost weak reduction. In particular, iteration is by name. Call-by-value differs from call-by-name by reducing the argument of an application before contracting the redex and by using iteration by value instead of iteration by name.

Proposition 6.2 Call-by-name and call-by-value are strategies of weak System L. Moreover, call-by-name is normalising (in the untyped weak System L).
Proof. They are clearly strategies. Normalisation is as in Proposition 6.6.	 
Remark 6.3 Call-by-value is not normalising in the untyped calculus: recall Ω, a (untypable) term without weak head normal form. Then iter 0 (λx.x) Ω starts an infinite reduction although the term has normal form λx.x.

Call-by-need
Under call-by-need (or lazy evaluation), an iterated term, not in normal form, is evaluated at most once, regardless of how many times the term is iterated. Thus such an iterated term may not be duplicated (by another iterator) before it has been reduced and may be reduced only if actually used.
The standard, non operational, definition of call-by-need is: reduce the argu- ment first (i.e. use call-by-value) if it will be needed, do not reduce it otherwise (i.e. use call-by-name). In general, it is difficult to decide if an argument will be needed or not in the syntax of the λ-calculus, and extra features are added to actually im- plement call-by-need (sharing graphs, environments, explicit substitutions). Here, the interesting point is that we can characterise call-by-need within the calculus.
Definition 6.4 Call-by-need is defined by the weak strategy (still with the liberal meaning) →l. See Table 3.
Lazy evaluation contexts:
L ::= [ ] | Lt | S L | let ⟨x, y⟩ = L in t
| cond L u w | cond true L w | cond false u L 
| iter L u w | iter 0 L w | iter (S t) u L 

Base cases:
(λx.t) u →l t[u/x]	fv(u) = ø
let ⟨x, y⟩ = ⟨t, t'⟩ in u →l u[t/x][t'/y]	fv(t) = fv(t') = ø
cond true u w →l u
cond false u w →l w
iter 0 u w →l u	fv(w) = ø
iter (S 0) u w →l w u	fv(w) = ø
iter (S(S t)) u v →l v (iter (S t) u v)	fv(t) = fv(v) = ø,v is a value
t →l v

Context rule:


L[t] →l L[v]


Table 3 Call-by-need

Proposition 6.5 →l is a strategy for →w.
Proof. It is clear that →l ⊂ →w. Moreover, the normal forms for →w are values in the sense of Proposition 5.4, as we can replace →w by →l in the proof of Adequacy (Theorem 3.8).	 
Proposition 6.6 →l reduces only needed redexes. Hence →l is normalising and it has the same normal forms as →w (see Proposition 5.4).
Proof. Say that a position is needed if it is the position of a needed redex or if it is above a needed position. By induction, it is easy to see that L only defines contexts where the hole [ ] is in a needed position. In an orthogonal and fully extended CERS, reducing only needed redexes terminates [15]. System L is not fully extended because a non-redex can become a redex (if a term becomes closed). But it is a sub-system of a suitable CERS (where we forget the conditions), which is enough to get the result.	 
Proposition 6.7 →l has the diamond property.
Proof. In this proof, we simply write → for →l, and we assume that there are diverging reductions t1 ←p t →q t2 at positions p and q respectively. If p = q, the same rule is used in both reductions, hence t1 = t2 and the reductions are not diverging. If p and q are disjoint, the pair is joined in one step on each side by applying the other rule at the corresponding position. Otherwise, one of the positions is the outermost, let’s say p and write q = p · q'. We look at all possible cases for the subterm t' at position p.
t' = u w: by definition of L, u →q' u' so u /= λx.s (by Proposition 5.4), and no rule is applicable at the root of t'; this case thus does not happen.
Similar argument for t' = let ⟨x, y⟩ = w in u.
t' = cond true u w: u ← t' →q' cond true u' w, then u→u', cond true u' f →u'.
Similar argument for t' = cond false u w and t' = iter 0 u w.
t' = iter (S 0) u w: straightforward.
t' = iter (S(S s)) u w: reduction at the root is allowed only when w is a value, thus the only possible innermost reduction is in s, and it is straightforward to conclude.


Minimality
Efficiency is a very pragmatic notion. In many cases, there is no better argument to demonstrate the efficiency of a strategy than a benchmark. On the contrary, here, System L gives us enough grip to actually give a proof of the efficiency of call-by-need. To measure efficiency, we just count the number of reduction steps; hence minimality (Definition 3.4) corresponds to the most efficient strategy. This is a more realistic notion here than in the λ-calculus, because implicit substitution

is always linear and all issues of duplication and erasure are explicit, hence taken into account when counting the number of rewrite steps.
Theorem 6.8 (Minimality) →l is minimal, i.e. if t is a closed term, t →m v and
t →n v where v is a value, then n ≤ m.
Proof. We use Theorem 3.6 (see [26] for more details on these techniques). Through- out this proof, we write d instead of →l and → instead of →w to improve read- ability. Assume t1 Dp t →q t2. We want to show that there exists t3 such that t1 →m t3 Dn t2 with m ≤ n. If the reductions are disjoint, this is easy because redexes are preserved by disjoint reductions.
If q is above p, then the →q step is also a dq step (definition of L and d) and we may use the diamond property for d, except in the case iter (S(S t)) u w' D iter (S(S t)) u w → w (iter (S t) u w) where t and w are closed and w is not a value. This case requires some more work. First iter (S(S t)) u w' → w' (iter (S t) u w') and w (iter (S t) u w) d w' (iter (S t) u w). Using Propositions 6.6 and 5.4, and the fact that w' is closed, we have w' dk λx.w'', hence w' (iter (S t) u w') →k+1 w''[iter (S t) u w'/x] and w' (iter (S t) u w) dk+1 w''[iter (S t) u w/x].  Again, w''[iter (S t) u w/x] dn v, where v is a value. If w is not at a needed position in w''[iter (S t) u w/x], then the same reduction can be mimicked on w''[iter (S t) u w'/x].
Otherwise, for some multi-hole context C, this reduction can be decomposed as w''[iter (S t) u w/x] dn1 C[w, w,..., w] d C[w', w,..., w] dn2 v with n = n1 +n2 + 1, and we can mimic this reduction on w''[iter (S t) u w'/x], omitting at least one step (because at least one residual of a needed redex is needed): w''[iter (S t) u w'/x] dn1

C[w', w',..., w'] dn'
v with n'
≤ n2
(technically, this dn'
reduction is the projec-

tion of the dn2 reduction after the reduction w d w'). In both cases, we indeed have w''[iter (S t) u w'/x] →m v Dn w''[iter (S t) u w/x] with m ≤ n. This concludes this case.
If q is below p and the →q step is not also a dq step (otherwise, use Proposi- tion 6.7), we look at all possible cases. There are three (by looking at the definition of L and d). For instance, u D cond true u w → cond true u w' d u. The cases for cond false u w and iter 0 u w are similar. The important point is that w (iter t u v) D iter (S t) u v → iter (S t) u v' is not a case to consider (v is a normal form for →w).	 
Hence, thanks to Proposition 6.7, any sub-strategy (in particular any deter- ministic one) of →l will also be minimal. It is already known that call-by-need is optimal in a large class of rewrite systems, including weak λ-calculi [21]. However, our present statement is much stronger because the notion of optimality in [21] takes into account parallel reduction of family of redexes. In other words, it is assumed that there is some adequate sharing mechanism that will allow all redexes of the same family to be reduced at the same time. We should also mention that this proof is a nice illustration of using the techniques of [26]. The call-by-need strategy presented here is an effective approximation of the internal needed strategy (whose minimality for orthogonal TRSs is reproved in [26]), which retains minimality (in our system; there is no hope of a similar result in general for orthogonal TRSs).

Each iterated term is evaluated at most once and it is reduced only if actu- ally used. It is remarkable that call-by-need is easily implementable without any syntactic extension to the calculus. Note that this does not happen with standard call-by-need, which is not expressible within the syntax of the λ-calculus: one has to extend it with some explicit binding syntax (Wadsworth graph reductions, explicit let bindings or explicit heap) to express sharing of subterm evaluation.
Conclusion
System L is a calculus that isolates the linear and non-linear components of a computation. We have used this calculus to make a study of evaluation strategies in this context, where it is precisely the non-linear aspects of the computation that we need to control. This leads to a simple description of strategies and to a definition of minimal strategies within the calculus. Moreover, We anticipate that we can make heavy use of these results in current implementation work based around System L.
Acknowledgement
We are sincerely grateful to the anonymous referees for their numerous and judicious comments (some of which could unfortunately not be followed because of space and time constraints).

References
S. Alves, M. Fern´andez, M. Florido, and I. Mackie. The power of linear functions. In Z. E´sik, editor, Proceedings of the 15th EACSL Conference on Computer Science Logic (CSL’06), volume 4207 of Lecture Notes in Computer Science, pages 119–134. Springer-Verlag, 2006.
S. Alves, M. Fern´andez, M. Florido, and I. Mackie. Iterator types. In Proceedings of Foundations of Software Science and Computation Structures, (FOSSACS’07), volume 4423 of LNCS, pages 17–31. Springer-Verlag, 2007.
Z. M. Ariola and M. Felleisen. The call-by-need lambda calculus. Journal of Functional Programming, 7(3):265–301, May 1997.
H. Barendregt, J. A. Bergstra, J. W. Klop, and H. Volken. Degrees, reductions and representability in the lambda-calculus. Technical report, University of Utrecht, Department of Mathematics, 1976.
H. P. Barendregt. The Lambda Calculus: Its Syntax and Semantics, volume 103 of Studies in Logic and the Foundations of Mathematics. North-Holland Publishing Company, second, revised edition, 1984.
T. Blanc, J.-J. L´evy, and L. Maranget. Sharing in the weak lambda-calculus. In Processes, Terms and Cycles, volume 3838 of Lecture Notes in Computer Science, pages 70–87. Springer, 2005.
H. J. S. Bruggink. Residuals in higher-order rewriting. In R. Nieuwenhuis, editor, Proceedings of Rewriting Techniques and Applications (RTA’03), volume 2706 of Lecture Notes in Computer Science, pages 123–137. Springer, June 2003.
L. Colson and D. Fredholm. System T, call-by-value and the minimum problem. Theor. Comput. Sci., 206(1-2):301–315, 1998.
U. Dal Lago and S. Martini. An invariant cost model for the lambda calculus. In A. Beckmann,
U. Berger, B. L¨owe, and J. V. Tucker, editors, Logical Approaches to Computational Barriers, Second Conference on Computability in Europe, CiE 2006, Proceedings, volume 3988 of Lecture Notes in Computer Science, pages 105–114. Springer, 2006.
J. Engelfriet and E. Schmidt. IO and OI. Journal of Computer and Systems Sciences, 15:328–353, 1997.

M. Fern´andez, I. Mackie, and F.-R. Sinot. Closed reduction: explicit substitutions without alpha conversion. Mathematical Structures in Computer Science, 15(2):343–381, 2005.
J. Field. On laziness and optimality in lambda interpreters: Tools for specification and analysis. In Conference Record of the 17th Annual ACM Symposium on Principles of Programming Languages (POPL ’90), pages 1–15, San Francisco, CA, USA, Jan. 1990. ACM Press.
P. Fradet. Compilation of head and strong reduction. In ESOP, volume 788 of Lecture Notes in Computer Science, pages 211–224, 1994.
J.-Y. Girard, Y. Lafont, and P. Taylor. Proofs and Types, volume 7 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1989.
J. R. W. Glauert, R. Kennaway, and Z. Khasidashvili. Stable results and relative normalization. Journal of Logic and Computation, 10(3):323–348, 2000.
Z. Khasidashvili and V. van Oostrom. Context-sensitive conditional expression reduction systems.
Electronic Notes in Theoretical Computer Science, 2:167–176, 1995.
J. Lamping. An algorithm for optimal lambda calculus reduction. In Proceedings of the 17th ACM Symposium on Principles of Programming Languages (POPL’90), pages 16–30. ACM Press, Jan. 1990.
J. Launchbury. A natural semantics for lazy evaluation. In Conference Record of the Twentieth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 144– 154, Charleston, South Carolina, Jan. 1993.
J.-J. L´evy. Optimal reductions in the lambda-calculus. In J. P. Seldin and J. R. Hindley, editors, To H.
B. Curry: Essays on Combinatory Logic, Lambda-Calculus, and Formalism, pages 159–191. Academic Press, Inc., New York, NY, 1980.
J. Maraist, M. Odersky, and P. Wadler. The call-by-need lambda calculus. Journal of Functional Programming, 8(3):275–317, May 1998.
L. Maranget. Optimal derivations in orthogonal term rerwiting systems and in weak lambda calculi. In Proc. of the 1991 conference on Principles of Programming Languages. ACM Press, 1991.
M. Parigot. On the representation of data in lambda-calculus. In CSL ’89: Proceedings of the third workshop on Computer science logic, pages 309–321, New York, NY, USA, 1989. Springer-Verlag New York, Inc.
S. L. Peyton Jones. The Implementation of Functional Programming Languages. Prentice Hall International, 1987.
Z. Spawski and P. Urzyczyn. Type fixpoints: iteration vs. recursion. In ICFP ’99: Proceedings of the fourth ACM SIGPLAN international conference on Functional programming, pages 102–113, New York, NY, USA, 1999. ACM Press.
Terese. Term Rewriting Systems, volume 55 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 2003.
V. van Oostrom. Random descent. In Proceedings of 18th Rewriting Techniques and Applications, (RTA’07), volume 4533 of Lecture Notes in Computer Science, pages 314–328. Springer-Verlag, 2007.
C. P. Wadsworth. Semantics and Pragmatics of the Lambda-Calculus. PhD thesis, Oxford University, 1971.
N. Yoshida. Optimal reduction in weak lambda-calculus with shared environments. Journal of Computer Software, 11(6):3–18, Nov. 1994.
