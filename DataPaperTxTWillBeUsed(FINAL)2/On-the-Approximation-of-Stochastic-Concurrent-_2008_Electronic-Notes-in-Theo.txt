

Electronic Notes in Theoretical Computer Science 220 (2008) 163–180
www.elsevier.com/locate/entcs

On the Approximation of Stochastic Concurrent Constraint Programming by Master Equation
Luca Bortolussi1
Department of Mathematics and Computer Science, University of Trieste, Italia Center for Biomolecular Medicine, Area Science Park, Trieste

Abstract
We explore the relation between the stochastic semantic associated to stochastic Concurrent Constrain Programming (sCCP) and its fluid-flow approximation. Writing the master equation for a sCCP model, we can show that the fluid flow equation is a first-order approximation of the true equation for the aver- age. Moreover, we introduce a second-order correction and first-order equations for the variance and the covariance.
Keywords: Stochastic Concurrent Constraint Programming, ordinary differential equations, biological systems, fluid-flow approximation.

Introduction
Process algebras offer an elegant framework to describe a wide range of systems, from computer networks to biological systems. When additional information re- garding (stochastic) speed of actions is added, we end up in the realm of stochastic process algebras (SPA), a widespread modeling technique in performance analy- sis [15] and in systems biology [20]. SPA are usually given a semantics in terms of continuous-time Markov Chains (CTMC [19]) - in fact, they can be seen as an high-level description language for CTMC. Therefore, all the analysis techniques for CTMC can be applied also to SPA, like steady-state or transient analysis. Moreover, many features of CTMC (like block decomposability) can be lifted to the level of SPA, greatly simplifying the analysis [15]. However, analyzing a stochastic system is a computationally expensive activity, and all previously mentioned techniques suffer severely from state space explosion. This phenomenon gets even worse when com- ponents are present in many copies, like molecules involved in chemical reactions.

1 luca@dmi.units.it

1571-0661© 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.11.025

In order to handle more effectively these cases, in [16] a fluid-flow approximation method for PEPA [15] has been presented. The central idea in [16] was to approxi- mate the number of each component’s type with a continuous variable, describing its dynamical evolution with a set of ODEs derived analyzing statically the SPA model. The same method has been developed also for other process algebras, like stochastic π-calculus [8] and stochastic Concurrent Constraint Programming [4]. The problem of continuously approximating SPA has, since then, received a lot of attention and it is considered an important topic for the applications in the biological field [7].
The main problem with these approximation methods is that their relation with the standard CTMC associated to a SPA model is not very clear. Practice showed that in many cases the approximation was satisfactory [7], although in other ex- amples the method did not work [3]. Clearly, precise theoretical foundations are required, and research is moving quickly in this direction [14,11,12]. In [14,11] the authors prove that for a particular class of PEPA models, namely those not co-
operating, 2 the equations obtained —linear, in this case— describe exactly the
evolution of the average of the system. Moreover, in [14] authors recognize that in the non-linear case the equations are only an approximation, as the differential equation for the average depends on all higher-order momenta, through a non-closed system of equations. Moreover, in [14] the authors suggest that, for some PEPA models, hybrid schemes of approximation should work better. This idea appeared also in [5], where authors use hybrid automata as a target formalism for approxi- mating stochastic Concurrent Constraint Programming.
The question of relating stochastic and differential models is not a new one, and in fact it has been extensively studied in mathematical, physical, and chemical literature in order to compare stochastic and deterministic description of physical systems, from gases to chemical reactions [18]. Most of the methods used start from a (partial) differential equation for the time evolution of the probability mass dis- tribution, known as Master Equation (ME). This equation is essentially equivalent to the Chapman-Kolmogorov forward equation [19], although it is more convenient to manipulate when the states of the system are vectors of (integer or real) vari- ables whose evolution is determined by a fixed set of interactions. The ME is an exact equation, in the sense that it gives a precise picture of the evolution of the stochastic system. Unfortunately, it is almost impossible to solve, both analytically and numerically, hence approximate methods have been developed to deal with it. In this paper we apply approximation methods for the ME to the fluid-flow ap- proximation of SPA. The first step is, therefore, the definition of a master equation for stochastic Concurrent Constraint Programming (sCCP [1], a stochastic exten- sion of CCP [21]). It turns out that defining the ME is rather simple, once some preliminary rewriting of sCCP programs in a more convenient graphical form is done. Besides, these first steps coincide with those needed to associate a set of ODEs to an sCCP program [4]. A related work in this direction is [9], where the author derives a master equation for a subset of stochastic π-calculus, in order to

2 More precisely, in [14] binary cooperation is possible only when on one side of the operator there is exactly one passive component.




Table 1
Syntax of restricted sCCP.

prove equivalence between the canonical stochastic model of a set chemical reactions and their description in stochastic π-calculus.
Instead, we will focus on the application of the master equation to justify the current derivations of differential equations from SPA model and to refine these methods. First of all, we will deduce the exact equation for the average of the stochastic process. Remarkably, when the rate functions describing interactions are linear, this equation coincide with the ODEs derived from fluid-flow approximation (thus giving a different proof of some results presented in [11,14]). In general, we will show that these equations are just a first-order approximation of the exact equation for the average. Using a similar method, we will then deduce first-order equations for variance and covariance, introducing also a second-order correction for the average depending explicitly from variance and covariance.
The paper is organized as follows. Section 2 introduced briefly a subset of sCCP that will be used in the following. In particular, in Section 2.1 we overview the method to associate ODEs to an sCCP program. Section 3 contains the derivation of the master equation for an sCCP program. Section 4, instead, relates the ME with the average of the system, while in Section 5 we define the equations for the variance and the covariance. In Sections 5.1, 5.2, 5.3 and 5.4 we give four examples, showing how the method works in practice. Finally, in Section 6 we draw conclusions and we present research directions to investigate further on.

Stochastic Concurrent Constraint Programming
Concurrent Constraint Programming (CCP [21]) is a process algebra in which agents interact exchanging information through a shared memory (the constraint store). Informational units are the constraints, i.e. interpreted first-order logical formulae stating relationships among variables. Agents interact asynchronously by adding new constraints (tell) and by checking if certain relations are entailed by the current configuration of the constraint store (ask).
The stochastic version of CCP (sCCP [1,2,6]) is obtained by adding an ex- ponentially distributed stochastic duration to all instructions interacting with the constraint store. The rates are given by a function λ : C → R+ associating a positive real number to each configuration of the store. The underlying semantic model of the language (defined by structural operational semantics, cf. [1,2]) is a CTMC.
In the following, we will use a restricted version of sCCP:

Definition 2.1 A restricted sCCP-program (or sCCP-network ) satisfies the fol- lowing conditions:
its syntax is given by the grammar in Table 1;
the variables of the store are X = (X1,..., Xn), with domain Z; they are all stream variables, i.e. growing lists representing time-varying quantities (cf. [1]). Moreover, their scope is always global ;
constraints that can be added to the store are updates of stream variables of the form X' = X + k, with k ∈ Z constant, meaning that the value of X is replaced by X + k;
constraints that can be checked in ask instructions are positive boolean for- mulae containing inequalities involving stream variables only;
the initial conﬁguration of the constraint store always consists in a conjunction of constraints of the form X = x0, one for each variable in X. x0 is the initial value of variable X.
The syntax defined in Table 1 imposes that all agents definable are sequential,
i.e. they cannot contain any occurrence of the parallel operator, whose usage is restricted to the top level of the global network (denoted with N in Table 1). This implies that processes cannot be forked at run-time, hence their number remains
constant during an execution. In addition, admitting only global variables we can
avoid any parameter passing to called procedures.
In [2] and [6], we argued that sCCP can be conveniently used for modeling a wide range of biological systems, like biochemical reactions, genetic regulatory networks, the formation of protein complexes, and the process of folding of a protein. Actually, the restricted fragment of sCCP presented here suffices to deal with the first two classes of systems.

Reduced Transition Systems, Interaction Matrix and ODEs
Each sequential sCCP agent can be conveniently represented as a graph, with ver- tices corresponding to different stochastic choices and edges corresponding to tran- sitions. The edges are labeled by the guard, the update, and the rate function of the corresponding transition.
Such graphs can be constructed from the syntactic tree of the sequential agent, simply merging all instantaneous transitions with the preceding stochastic one (up- dating consistently the label of the edges) and replacing a node corresponding to the call of a procedure p with the syntactic tree of p. This needs to be done at most once for each procedure p; all other nodes corresponding to calls to p are removed and their incoming edge redirected to the root of the unique copy of the syntactic tree of p.
The resulting graph for each component is called in [4] reduced transition system
(RTS). Given a sequential agent A, we denote its RTS by RTS(A), the set of its ver- tices (RTS-states) by S(A) = {s1,..., sk} and the set of its edges (RTS-transitions or simply transition) by T (A) = {t1,..., th}. Moreover, the guard of a transition

tj is denoted by g(tj) = gj (with gj(X) we denote also its indicator function), its update by u(tj) = uj, and its rate function by λ(tj) = λj(X).
As an example, below is a sCCP process composed by one single component A, together with its RTS (∗ is shorthand for true):

A :- tellλ1 (X' = X + 1).B
B :- askλ2 (X > 0).tell∞(X' = X − 1).A

In order to simplify the analysis of an sCCP model, we can define a fluid-flow approximation of the system, by treating variables as continuous and describing their time-evolution by means of ODEs [4].
Starting from an sCCP network N , with initial configuration N = A1  ...  An,
we build the RTS for each sequential component Ai. Then, letting S(N ) = S(A1) ∪
... ∪ S(An) and T (N ) = T (A1) ∪ ... ∪ T (An), 3 we associate a continuous variable to each RTS-state of S(N ). The variables Y of the differential equations will thus comprehend the stream variables X of the store, approximated as continuous, and all RTS-state variables.
Next, we build what we will call the interaction matrix ν of our sCCP-network. The interaction matrix has as many rows as system’s variables Y and as many columns as the transitions in T (N ). In this matrix we store the updates (constant by Definition 2.1) that each transition induces on stream variables: for instance, if transition tj increases variable X by 2, we put 2 in position νX,j. Moreover, we put also a +/−1 in correspondence to the enter/exit RTS-state of the transition.
To write the ODEs, we simply need a final vector φ storing the (functional) rates of each transition in T (N ). 4 In addition, we multiply such rates by the indicator function of the guards of the RTS-edges (depending only on stream variables X, cf. Definition 2.1) and by the variable corresponding to the exit RTS-state of the transition, denoted by e(tj) = ej. In summary,
φ(tj) = φj(Y) = λj(X)ejgj(X).
Then, letting νj represent the j-th column vector of matrix ν, the differential equa- tions are given by Φ1:
Φ1 = ν · φ =	νjφj.
j
The initial values are determined by the initial configuration of the store (cf. Defini- tion 2.1) and by the initial state of each sequential agent. For the previous example, we have (⟨·⟩ returns the logical value of a formula):

(X) 0 1 −1 1


λ · A
!	8>< X˙ = Φ1 = λ1 · A − λ2 · B · ⟨X > 0⟩

I = (A) B@ −1 1
CA	φ =	1
A˙ = Φ1 = −λ1 · A + λ2 · B · ⟨X > 0⟩

(B)
1 −1
λ2 · B · ⟨X > 0⟩
>: B˙ = Φ1 = λ1
A − λ2
B · ⟨X > 0⟩

3 We consider states of different components as distinct.
4 Here and in the following, we suppose to have fixed an ordering of RTS-states, stream variables and transitions.

Master Equation for sCCP
The master equation for an sCCP network N = A1  ...  An describes the time- variation of the probability mass function of the CTMC. First of all, we need to describe states of the CTMC by a set of variables. The choice is rather simple, given the method of the previous section. In fact, a state of the network is described by the values of the stream variables X and by the variables associated to RTS-states S(N ), i.e. by a valuation of the variables Y. Note that in this case we are not assuming these variables to be continuous, like in the ODE construction; in fact, they will generally be integer-valued.
Consider now the probability of being in state Y at time t, denoted by P(Y, t). Each transition entering in state Y will increase this probability, while all transitions leaving Y will decrease it. Consider now a transition tj ∈ T (N ). Its effect on Y is described by the j-th column vector νj of the interaction matrix ν. In fact, the happening of tj in Y will bring us in state Y + νj (if Y + νj does not belong to the domain of Y, then transition tj cannot happen — in this case, we assume its rate φj(Y) to be equal to zero). In addition, the probability that a transition tj fires in the infinitesimal time dt, given that we are in state Y, is φj(Y)dt. The conditioning can be removed multiplying by P(Y, t).
In summary, transition tj increases P(Y, t) in the infinitesimal time dt by

P(Y − νj, t)φj(Y − νj)dt,

and decrease it by
P(Y, t)φj(Y)dt.
The first term describes the probability of tj leading into state Y, while the second term describes the probability of tj leading out of state Y. 5
Summing over all transitions and dividing for dt, we get the master equation for the sCCP network N :
∂P(Y, t) = Σ (φ (Y − ν )P(Y − ν , t) − φ (Y)P(Y, t)) .

The initial conditions P(Y, 0) for P are given by the initial configuration of the store (cf. Definition 2.1): at time 0 the system is in the state Y0 with probability 1, where the stream variables are determined by the constraints X = x0, while the RTS-state variables are fixed by the initial state of each agent.
An important issue for the discussion to follow regards the continuity proper- ties of rate functions φj. Recalling their definition given in equation 1, φj(Y) = λj(X)ejgj(X), we note that φj is the product of three functions: ej, λj, and gj. ej is a simple linear function (actually, a single variable), hence it is analytical. Gener- ally, we can also expect λj to be an analytical function (this is the case, for instance, for biochemical reactions). On the contrary, gj is an indicator function, hence dis- continuous whenever the guard gj is non trivial (i.e., different from true). Guards

5 If Y − νj does not belong to the domain of Y, then we let φj (Y − νj )= 0.

are generally used to synchronize agents; in this sense, discontinuity seems the price to pay for synchronization. In some cases, however, the discontinuous nature of gj is absorbed by the function λj. For instance, suppose that gj(X) = I(Xi > 0) and that λj vanishes for Xi = 0; for all non-negative reals it then holds gjλj = λj, and so φj is continuous whenever λj is. This situation is not so uncommon, especially when guards are used to force upper and lower bounds on variables. An example are biochemical reactions, cf, below.

3.1	Relation with the Chemical Master Equation
Biochemical networks are generally expressed as a list of chemical reactions, encod- ing all possible actions the system can undergo. A general biochemical reaction has the form
R1 + ... + Rn →f(R;k) P1 + ... + Pm,
where R1,..., Rn are the reactants and P1,..., Pm are the products. The real-valued kinetic function of the reaction is f (R; k), depending on the reactants R and on some parameters k. This function, generally analytical, can be one of the many used in biochemistry (cf. [10]), although usually it follows the mass action law (i.e., it is proportional to the concentration of the reactants). Moreover, it is required to satisfy the following boundary condition: it must be zero whenever one reactant is less than its amount consumed by the reaction. For instance, if the reaction consumes two molecules of R, then f must be zero for R = 0, 1 6 .
In sCCP, each reaction is associated to the agent
f -reaction(R, P, k) :-

tellf (R;k)( n
(Ri − 1) ∧  m
(Pj + 1)).

f -reaction(R, P, k).
This agent is a simple recursive loop, modifying the value of reactants’ and products’ variables at a speed given by the kinetic law. Note that the boundary conditions for the rate function f imply that no stream variable will ever become negative, as all reactions that may produce this effect have rate zero 7 . Consequently, we do not have to check domain constraints explicitly by guards, as their introduction would be redundant in the sense of the discussion at the end of previous section.
The RTS of this agent has just one state. In this case, the variable associated to it can be removed, as its equation would be P˙ = 0, P (0) = 1, and we can set P ≡ 1. Hence, the rate function φ associated to the agent f -reaction is φ = f (R; k). Now, the chemical master equation is defined as equation 3 with f in place of φ, hence the master equation of the sCCP model of a set of biochemical reactions coincides with the chemical master equation. The same result has been proved in [9] for chemical π-calculus, for mass action reactions with at most 2 reactants.

6 In case of mass action kinetics, this condition means that the rate for R + R → P must be kR(R − 1) and not kR2. This is, however, consistent with the definition of the mass action principle in the stochastic setting.
7 Boundary conditions for f may be relaxed by checking explicitly with ask instructions that variables stay within their domain. For instance, for the reaction R + R → P , we can precede tell by ask(R > 1) . This
allows us to use the more common kR2 as rate function, at the price of introducing discontinuity in the rates.

First-order approximation
From the Master Equation 3 we can deduce an exact differential equation for the average of each system variable Yi of Y. At time t, the average value ⟨Yi⟩t equals 8
⟨Yi⟩t =	YiP(Y, t),
Y
where the sum ranges over all possible states of the system. As the only time- dependent quantity within the sum is P, deriving both sides w.r.t. time t and substituting equation 3, we obtain
d ⟨Yi⟩t = Σ Σ Y φ (Y − ν )P(Y − ν , t) − Σ Y φ (Y)P(Y, t) .

In the first summation in the right-hand side, we can substitute Y with Wj + νj, breaking the sum in two pieces (one piece for Wij and one piece for νij). The first part cancels out with the second summation in 4, hence we are left with

d ⟨Yi⟩t = Σ ν

Σ φ (W )P(W , t).


Applying the definition of the average we obtain	Wj φj(Wj)P(Wj, t) = ⟨φj⟩t; recalling equation 2, we get
d ⟨Yi⟩t = Φ1(Y) 
Thus, the average value of Y in the stochastic system evolves as the average of the function Φ1(Y), which is the vector of functions of the ODE system associated to the sCCP-network N by the fluid-flow approximation reviewed in Section 2.1.
Remark 4.1 If all rate functions φj are linear, then so is Φ1(Y), and thus, thanks to the linearity of expectation ⟨·⟩t, the equation 5 reduces to
d ⟨Yi⟩t = Φ1(⟨Y⟩ ),
dt	i	t
which is equation 2. Therefore, if all rate functions φj are linear, the method of Section 2.1 provides the exact equation for the average. This is essentially the same result proved in [11,14].
Of course, if the linearity condition on φj does not hold, then the previous remark is no more valid. In this case, the equation 2 is no more the correct equation for the average of the system. However, it is a first order approximation, hence reasonable whenever fluctuations are small. To see this, consider the time-dependent Taylor expansion of function φj(Y) around the average value ⟨Y⟩t; for simplicity we truncate it at second order:

8 We adopt the conventions used among physicists to denote average and covariance. Namely, ⟨X⟩ indicates the average of X, while ⟨⟨XY ⟩⟩ denotes the covariance of X and Y . ˙˙X2¸¸ indicates the variance of X.

|Y|
φj(Y) ≈ φj(⟨Y⟩t) +	∂kφj(⟨Y⟩t)(Yk − ⟨Yk⟩t)
k=1
|Y|

+ 1 Σ
∂2 φ (⟨Y⟩ )(Y
— ⟨Y
⟩ )(Y
— ⟨Y ⟩ ),



where ∂k denotes the partial derivative w.r.t. Yk. Taking the average of both sides, the linear term cancels out, as ⟨(Yk − ⟨Yk⟩t)⟩t = 0; moreover the covariance of Yh and Yk at time t, ⟨⟨YhYk⟩⟩t = ⟨(Yh − ⟨Yh⟩t)(Yk − ⟨Yk⟩t)⟩t, appears in the second-

order term. Therefore, as Φ1(Y) second order:
= Σj νj ⟨φj(Y)⟩, we obtain for Φ1(Y)  at
|Y|

​
 Φ1(Y) 
≈ Φ1(⟨Y⟩ )+ 1
Σ ∂2

Φ1(⟨Y⟩ ) ⟨⟨Y
Y ⟩⟩

If we truncate the previous approximation at first order, then we obtain Φ1(Y)	≈
Φ1(⟨Y⟩t), and so
d ⟨Yi⟩t = Φ1(⟨Y⟩ ).
dt	i	t
The previous method works only if the functions φj can be expanded in Taylor series (at least up to second order, hence φj must have continuous second order derivatives). Preferably, φj should be analytic. In the presence of guards, this may not be true, even if all rates λj are analytic. In this case, a different treatment is needed, cf. Section 6 for further comments. However, for the rest of the paper, we suppose φj to be analytic.

Second-order approximation
Equation 6 shows that, at second order, the average depends also on the variance and covariance of system variables. By the way, by taking the expansion to higher orders, we end up having an equation potentially depending on all higher order momenta. We will comment more on this later. In any case, if we can get equations for the variance and covariance, we may use the equation 6 to improve the approximate equation 2 for the average, taking into account fluctuations. To get equations for the variance and covariance, we can proceed similarly to the average: we deduce an exact equation from the master equation and then we linearize it.
First of all, we derive the differential equation for ⟨YiYk⟩t:

d ⟨YiYk⟩t = Σ Y Y

∂P(Y, t)
.


By substituting equation 3 to ∂P(Y,t) , letting W = Y − νj, as in Section 4, and

setting
Φ2 (Y) = Σ νijνkjφj(Y),

j

we obtain the following
d ⟨YiYk⟩t = Φ2 (Y)  + Y Φ1(Y)  + Y Φ1(Y)  .

The exact equation for the covariance ⟨⟨YiYk⟩⟩t can now be obtained easily from:


d ⟨⟨YiYk⟩⟩t = d (⟨Y Y ⟩
— ⟨Y ⟩
⟨Y ⟩ ) .


	
dt	dt
i k t
i t	k t

After simple manipulations, we obtain

​
d ⟨⟨YiYk⟩⟩t =  Φ2 (Y) 
+ (Y
— ⟨Y ⟩ )Φ1(Y) 
+ (Y
— ⟨Y ⟩ )Φ1(Y) 

This equation involves the average of functions over the state space, and the right hand side cannot in general even be computed. However, if we do a first-order expansion of functions Φ1 and Φ2, as in the previous section, we can obtain the following approximate equation:

|Y|
d ⟨⟨YiYk⟩⟩t = Φ2 (⟨Y⟩ ) +	∂

Φ1(⟨Y⟩ ) ⟨⟨Y Y ⟩⟩



dt	ik
t	h k
h=1
|Y|
t	i h  t

(8)	+ Σ ∂hΦ1(⟨Y⟩ ) ⟨⟨YkYh⟩⟩ .

First of all, note that in case the functions φj are linear, then the previous equation is exact. If this is not the case, then expansion of Φ1 up to first order is essential, otherwise higher order momenta appear in the equation. In fact, covari- ance appears in the RHS because the term (Yi − ⟨Yi⟩t) gets multiplied by the factor
(Yh − ⟨Yh⟩t) of the first order term in the Taylor expansion of Φ1. Therefore, the
factor (Yh −⟨Yl⟩t)(Yh −⟨Yl⟩t) of the second order term in this expansion would bring in the third order momenta. This remark shows that, when φj are not linear, then
the exact system of ODEs for the average is not closed, in the sense that equation for momenta of order k involve momenta of higher order, leading to an infinite set of equations (this fact has been observed also in [14]). The only way to overcome this infinity is to linearize equations for the highest order momentum we are taking into account. However, even if in theory we can introduce momenta of order as high as desired, we are concretely limited by the combinatorial explosion of the number of variables needed: for k-th order momenta, we need Θ(nk) variables.
Equation 8 is a first order ODE for the variance and covariance. Note that it can be derived from an sCCP network using the purely syntactic method presented
in Section 2.1: the functions Φ2 can be computed easily, given the knowledge of
the rate vector φj and of the interaction matrix ν, while the derivatives of Φ1 can
be computed by symbolic derivation. Such equations can give a picture of the fluctuations of the system, even if in an approximate way. They are not exact, but they indeed are more informative than the equations for the average alone. We can see this in the following simple examples.

Once we have the equations for the variance and covariance of system variables, we can use equation 6 to add a second order correction also for the average, obtaining
|T (N )|

(9)
d ⟨Yi⟩t = Φ1(⟨Y⟩ )+ 1
Σ ∂2

Φ1(⟨Y⟩ ) ⟨⟨Y
Y ⟩⟩ .


Example: Random Walk
We consider a random walk in one variable X, expressed by the following sCCP process
RWX :- tellk(X = X + 1).RWX + tellk(X = X − 1).RWX ,
where the rate of both tell actions is constant and equal to k. Since the RTS of this agent has a single state, with two edges t1 and t2 looping on it, we may forget about the state variable (it will have equation A˙ = 0, A0 = 1, so it can be eliminated by setting A ≡ 1). The two rate functions will then be φ1(X) = k and φ2(X) = k, while the interaction matrix ν will be equal to ν = (ν1, ν2) = (1, −1). Therefore, the Φ functions are
Φ1(X) = 0,	Φ2(X) = 2k,
giving the following two equations for the average ⟨X⟩ and the variance  X2  of
⎧⎨ ⟨X˙ ⟩ = Φ1(⟨X⟩)+ 1  X2  ∂2	Φ1(⟨X⟩) = 0 
⟨⟨X˙2⟩⟩ = Φ2(⟨X⟩)+2	X2	∂X Φ1(⟨X⟩) = 2k
These very simple equations can be readily solved, yielding ⟨X⟩t = X0 and

  X2 
= 2kt +  X2  .  These equations are exact, as the rate functions are

linear, giving the well known result that a random walk has constant average but linear growing variance. Note that the equation for the average alone, despite being exact, is not very informative, as it does not capture the fact that the variable X has unbounded variance (hence it will eventually reach any number in Z with probability 1).

Example: Dimerization
As another example, we consider the dimerization of a protein P . This is a bio- chemical system composed by two molecule’s types, P and its dimer P2, and two reactions, P + P →k1 P2 and its inverse P2 →k2 P + P . The rates associated to these to reactions are φ1 = k1X(X−1)/2 and φ2 = k2Y , with X giving the quantity
of P molecules and Y giving the quantity of P2. Due to the results in Section 3.1,
these are also the rate functions of the sCCP network describing the dimerization system. Notice that this is a closed system, as the number of P molecules is con- stant ; formally X + 2Y = X0 + 2Y0 = 2C. This observation allows to remove one variable from the system, setting X = 2C − 2Y . This one-dimensional sys- tem is a birth-death process, and so it can be solved exactly. For instance, setting
k1 = 0.00166, k2 = 0.2, C = 150, and Y0 = 0, we obtain that the average value of Y

converges to the stationary value of ⟨Y ⟩∞ = 80.3512, while the variance converges

to  Y 2 
= 24.3005.

Using the method just presented, we can write the first order and the second order approximation for ⟨Y ⟩. For the first order, we have:

⟨Y˙ ⟩ = k1(C − ⟨Y ⟩)(2C − 2 ⟨Y ⟩− 1) − k2 ⟨Y ⟩ .


The second order correction, instead, gives rise to the following set of equations for
⟨Y ⟩ and its variance	Y 2	:


⟨Y˙ ⟩ = k (C − ⟨Y ⟩)(2C − 2 ⟨Y ⟩− 1) − k ⟨Y ⟩ + 2k	Y 2
⟨⟨Y˙2⟩⟩ = k1(C − ⟨Y ⟩)(2C − 2 ⟨Y ⟩− 1) + k2 ⟨Y ⟩ + ˙˙Y 2¸¸ (4k1 ⟨Y ⟩− 4k1C + k1 − k2)


These equations can be solved numerically, and they both predict for ⟨Y ⟩ con- vergence to an asymptotic value. For first order approximation, this value equals
⟨Y ⟩∞ = 80.2291, while at second order we have ⟨Y ⟩∞ = 80.3509, closer to the true

mean. Moreover, variance converges asymptotically to	Y 2
good agreement with the true variance.



Example: Effects of Variance
= 24.258, also in


Consider the following biochemical system, described by a set of mass action reac- tions:

∅ →k X;	∅ →k Y ;	X →α1 ∅
Y →α2 ∅;	X + Y →ka ∅

Its sCCP code can be obtained easily according to the prescriptions of Sec- tion 3.1. In Figure 1 we show the result of a stochastic simulation via Gillespie Algorithm [13]. As readily seen, the effect of fluctuations is predominant, and the standard deviation is of the same order of the average. Therefore, equations for the average, even if accurate, may fail to give a detailed picture of this system: the knowledge of the variance is essential.
Using the method of this paper, we can obtain approximate differential equations for the average and the variance. There is a total of 5 variables: two for the average (⟨X⟩ and ⟨Y ⟩), two for the variance (  X2  , and  Y 2  ) and one for the covariance
(⟨⟨XY ⟩⟩). The resulting equations are


⟨X˙ ⟩ = k − α1 ⟨X⟩− ka ⟨X⟩ ⟨Y ⟩− ka ⟨⟨XY ⟩⟩
⟨Y˙ ⟩ = k − α2 ⟨Y ⟩− ka ⟨X⟩ ⟨Y ⟩− ka ⟨⟨XY ⟩⟩
⟨⟨X˙2⟩⟩ = k + α1 ⟨X⟩ + ka ⟨X⟩ ⟨Y ⟩− 2α1	X2
−2ka ⟨Y ⟩	X2	− 2ka ⟨X⟩ ⟨⟨XY ⟩⟩
⟨⟨X˙Y ⟩⟩ = ka ⟨X⟩ ⟨Y ⟩− ka ⟨Y ⟩	X2	−
(α1 + α2 + ka ⟨X⟩ + ka ⟨Y ⟩) ⟨⟨XY ⟩⟩ − ka ⟨X⟩	Y 2
⟨⟨Y˙2⟩⟩ = k + α2 ⟨Y ⟩ + ka ⟨X⟩ ⟨Y ⟩− 2α2	Y 2
−2ka ⟨X⟩	Y 2	− 2ka ⟨Y ⟩ ⟨⟨XY ⟩⟩
Their solution is shown in Figure 2. As we can see, the variance is of the order of 106 at equilibrium, suggesting a standard deviation of order 103, in agreement with the simulation results.

Example: The Circadian Clock
In this section we provide a more complex and biologically driven example. The system is schematically shown in Figure 3. It is a simplified model of the machinery involved in the circadian rhythm of living beings. In fact, this simple network is present in a wide range of species, from bacteria to humans. The circadian rhythm is a typical mechanism responding to environmental stimuli, in this case the periodic change between light and dark during a day. Basically, it is a clock, expressing a protein periodically with a stable period. This periodic behaviour, to be of some use, must be stable and resistant to both external and internal noise. Here with internal noise we refer to the stochastic fluctuations observable in the concentrations of proteins. The model presented here is taken from [22], a paper focused on the

Average trajectory	(b) Average and standard deviation

Fig. 1. (1(a)) Average trajectory from 250 runs of the system of Section 5.3. Parameters were set equal to
k = 1000, α1 = 10−4, α2 = ka = 10−3. (1(b)) In this plot also the standard deviation is shown. As can be seen, the fluctuations are predominant: the sole average thus does not give an accurate picture. Note that standard deviation seems to be of the same order of the average.


	
(a) Average	(b) Variance

Fig. 2. (2(a)) Numerical solution for second-order equations for the average. Parameters are like those in caption of Figure 1. (2(b)) Numerical solution of equations for the variance and covariance.
study of the resistance to noise of this system. Interestingly, they showed that the stochastic fluctuations make the oscillatory behaviour even more resistant.

Fig. 3. Biochemical network for the circadian rhythm regulatory system. The figure is taken from [22], like

numerical values of rates. Rates are set as follows: αA = 50, α'
= 500, αR = 0.01, α'
= 50, βA = 50,

βR = 5, δME = 10, δMR = 0.5, δA = 1, δR = 0.2, γA = 1, γR = 1, γC = 2, θA = 50, θR = 100.

The system is composed by two genes, one expressing an activator protein A, the other producing a repressor protein R. The generation of a protein proceeds in two phases: the transcription of DNA to mRNA and the translation of mRNA to the protein. Protein A is an enhancer for both genes, meaning that it regulates positively their expression. Repressor R, instead, can capture protein A, forming the complex AR and making A inactive. Proteins A and R are degraded at a specific rate (see the caption of Figure 3 for more details about the numerical values), but R can be degraded only if it is not in the complexed form, while A can be degraded in any form. Notice that the regulation activity of A is modeled by an explicit binding to the gene, which remains stimulated until A unbinds. The sCCP code for this system is not reported here, but can be found in [6]. The description of the system involves 9 variables, hence the 54 differential equations for average and variance have been omitted.
In Figure 4(a), we show the result of the stochastic simulation of the model of the circadian clock. We can see clearly neat and regular oscillations. However, if we















(a) Single run	(b) Average out of 1000 runs

Fig. 4. (4(a)) Single run of the stochastic simulation with Gillespie algorithm [13] of the circadian system of Figure 3. (4(b)) Average trajectory of the circadian system, computed from 1000 runs.

First-order average	(b) Second-order average

Fig. 5. (5(a)) Numerical solution of first order ODEs for the average of the circadian clock described in Figure 3. (5(b)) Numerical solution to second-order ODEs for the average.

compute the average trajectory out of an ensemble of 1000 runs, we have a surprise: the resulting trajectory has dampening oscillations and it converges towards a stable state (Figure 4(b)). Probably, this is the effect of small fluctuations of the period, which bring different trajectories out of phase, so the oscillations cancel out. This warns against the use of the average to describe a stochastic system, at least in the case in which single traces exhibit neat oscillations.
In Figure 5(a) we show the numerical integration of first-order ODEs for the average. They show oscillations similar to the one exhibited by a single stochastic trajectory! This is a strange behavior: the solution is far away from the average (hence it is a bad approximation), though behaviorally they look better than the average. Introducing a second order correction, we should expect to have a result closer to the true average. In this case, however, we still obtain an oscillatory pattern, though the oscillations are dirtier (Figure 5(b)).
The solution of first-order ODEs possesses a limit circle [22], which however disappears changing some parameters. For instance, increasing the value βR of production of protein R form 5 to 50 we obtain a convergent trajectory (Figure 7(a)). The corresponding stochastic system has single trajectories still showing oscillations, even if the life-period of protein A is greatly reduced (Figure 6(a)). The average of















Single run	(b) Average out of 1000 runs

Fig. 6. (6(a)) Single run of the stochastic simulation with Gillespie algorithm [13] of the circadian system of Figure 3, with βR = 50. (6(b)) Average trajectory of the circadian clock with βR = 50, computed from 1000 runs.

(a) First-order average	(b) Second-order average

Fig. 7. (7(a)) Numerical solution of first order ODEs for the average of the circadian clock described in Figure 3, with βR = 50. (7(b)) Numerical solution to second-order ODEs for the average.

the stochastic system has dampening oscillations, like the case for βR = 5, although now the dampening takes less time (Figure 6(b)). Looking at the second-order trajectory for the average (Figure 7(b)), we observe a solution similar to the first- order solution, but showing some dampening in accordance with the true average.

Conclusions and Future Work
In this paper we defined a differential equation for the time-evolution of the prob- ability mass distribution, the so called master equation, for a model written in stochastic Concurrent Constraint Programming. From this equation, we derived exact differential equation for the average and the variance/covariance of the model. These equations contain averages on the right hand side, which can be eliminated by linearization or Taylor expansion up to second (or higher) order. As a collateral result, we obtained the “standard” fluid-flow equations as first-order approxima- tions of the exact equation for the average. The most important fact is, however, the derivation of an approximate set of equations for the variance and covariance, which can be obtained by a simple modification of the method for building the fluid-flow approximation of a generic sCCP model. These equations can be used

to draw a more detailed picture of the system’s evolution, giving information on the average magnitude of fluctuations and, consequently, on the goodness of the fluid-flow approximation.
Starting from the ME, there are other approximation methods that can be ap- plied to sCCP. For instance, if one can introduce a meaningful notion of system size Ω for the sCCP model 9 , then a Taylor expansion around Ω of the master equation can lead to the so-called linear noise approximation [18]: the equation for the aver-
age, following the first-order approximation, is coupled with a linear Fokker-Planck stochastic differential equation governing the offset from the solution of the average equation.
However, sCCP (and SPA in general) has characteristic features that present some challenges for the application of these approximation techniques. Specifically, in presence of synchronization, sCCP models have discontinuous rate functions, which cannot be expanded in Taylor series over their whole domain of definition. In this case, a different treatment is required. Usually, the functions φj will be piecewise analytic, and their analytic regions will be separated by discontinuity boundaries of measure zero. Consequently, we may use an hybrid approximation scheme, expanding in series functions φj only within their analytic regions, while moving discretely from one region to another when a boundary is hit. However, near discontinuity boundaries, approximations of stochastic processes with ODEs can lead to big errors: for instance, the continuous trajectory can converge asymp- totically to the border, while the stochastic trajectory can cross it in finite time, entering in a different dynamical regime. In these case, we can keep the stochastic ingredient, using stochastic differential equations instead of ODEs and maintain- ing the transitions discrete near the boundaries, de facto approximating the sCCP model with a (stochastic) hybrid automaton [17].
As a matter of fact, the second order correction of fluid-flow approximation pre- sented here is still too crude for sCCP models in which the stochasticity or the discreteness play a dominant role in the dynamical evolution (see [5] for an exam- ple). In these cases, we again expect that an approximation based on (stochastic) hybrid automata will outperform the one based solely on ODE’s. We are currently investigating this direction [5].


Acknowledgement
I wish to thank Jane Hillston and Alberto Policriti for the suggestions and the precious discussions. This work was partly supported by projects FIRB-LIBi and PRIN-BISCA.



9 For instance, in [11] the authors suggest the notion of density, i.e. the ratio between the current number of agents of a given type and the maximum number of agents of that type, which may work for closed systems.

References
L. Bortolussi. Stochastic concurrent constraint programming. In Proceedings of 4th International Workshop on Quantitative Aspects of Programming Languages (QAPL’06), volume 164 of ENTCS, pages 65–80, 2006.
L. Bortolussi. Constraint-based approaches to stochastic dynamics of biological systems. PhD thesis, PhD in Computer Science, University of Udine, 2007. Available at http://www.dmi.units.it/
~bortolu/files/reps/Bortolussi-PhDThesis.pdf.
L. Bortolussi and A. Policriti. Dynamical systems and stochastic programming I - ordinary differential equations. Submitted to Transactions of Computational Systems Biology, 2007.
L. Bortolussi and A. Policriti. Stochastic concurrent constraint programming and differential equations. In Proceedings of Fifth Workshop on Quantitative Aspects of Programming Languages, QAPL’07, volume 167 of ENTCS, 2007.
L. Bortolussi and A. Policriti. Hybrid approximation of stochastic concurrent constraint programming. In To be presented at IFAC’08., 2008.
L. Bortolussi and A. Policriti. Modeling biological systems in concurrent constraint programming.
Constraints, 13(1), 2008.
M. Calder, S. Gilmore, and J. Hillston. Modelling the influence of RKIP on the erk signalling pathway using the stochastic process algebra PEPA. Transactions on Computational Systems Biology, 4230:1– 23, 2006.
L. Cardelli. From processes to ODEs by chemistry. downloadable from http: // lucacardelli. name/ , 2006.
L. Cardelli. A process algebra master equation. In Proceedings of QEST’07, 2007.
A. Cornish-Bowden. Fundamentals of Chemical Kinetics. Portland Press, 3rd edition, 2004.
J. Ding and J. Hillston. On odes from pepa models. In Proceedings of PASTA’07, 2007.
N. Geisweiller, J. Hillston, and M. Stenico. Relating continuous and discrete pepa models of signalling pathways. Theoretical Computer Science, 2008. in print.
D.T. Gillespie. Exact stochastic simulation of coupled chemical reactions. J. of Physical Chemistry, 81(25), 1977.
R. A. Hayden and J. T. Bradley. Fluid-flow solutions in PEPA to the state space explosion problem. In Proceedings of PASTA’07, 2007.
J. Hillston. A Compositional Approach to Performance Modelling. Cambridge University Press, 1996.
J. Hillston. Fluid flow approximation of PEPA models. In Proceedings of the Second International Conference on the Quantitative Evaluation of Systems (QEST’05), 2005.
J. Hu, J. Lygeros, and S. Sastry. Towards a theory of stochastic hybrid systems. In Proceeding of Hybrid Systems: Computation and Control, HSCC 2000, volume 1790 of LNCS, 2000.
N. G. Van Kampen. Stochastic Processes in Physics and Chemistry. Elsevier, 1992.
J. R. Norris. Markov Chains. Cambridge University Press, 1997.
A. Regev and E. Shapiro. Cellular abstractions: Cells as computation. Nature, 419, 2002.
V. A. Saraswat. Concurrent Constraint Programming. MIT press, 1993.
J. M. G. Vilar, H. Yuan Kueh, N. Barkai, and S. Leibler. Mechanisms of noise resistance in genetic oscillators. PNAS, 99(9):5991, 2002.
