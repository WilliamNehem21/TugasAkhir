Available online at www.sciencedirect.com
ScienceDirect


CAAI Transactions on Intelligence Technology 1 (2016) 14e29
http://www.journals.elsevier.com/caai-transactions-on-intelligence-technology/

Salient pairwise spatio-temporal interest points for real-time activity recognition
Mengyuan Liu a, Hong Liu a,b,*, Qianru Sun a, Tianwei Zhang c, Runwei Ding a
a Engineering Lab on Intelligent Perception for Internet of Things (ELIP), Peking University, Shenzhen Graduate School, 518055, China
b Key Laboratory of Machine Perception, Peking University, 100871, China
c Nakamura-Takano Lab, Department of Mechanoinformatics, The University of Tokyo, 113-8685, Japan
Available online 8 March 2016

Abstract
Real-time Human action classification in complex scenes has applications in various domains such as visual surveillance, video retrieval and human robot interaction. While, the task is challenging due to computation efficiency, cluttered backgrounds and intro-variability among same type of actions. Spatio-temporal interest point (STIP) based methods have shown promising results to tackle human action classification in complex scenes efficiently. However, the state-of-the-art works typically utilize bag-of-visual words (BoVW) model which only focuses on the word distribution of STIPs and ignore the distinctive character of word structure. In this paper, the distribution of STIPs is organized into a salient directed graph, which reflects salient motions and can be divided into a time salient directed graph and a space salient directed graph, aiming at adding spatio-temporal discriminant to BoVW. Generally speaking, both salient directed graphs are constructed by labeled STIPs in pairs. In detail, the “directional co-occurrence” property of different labeled pairwise STIPs in same frame is utilized to represent the time saliency, and the space saliency is reflected by the “geometric relationships” between same labeled pairwise STIPs across different frames. Then, new statistical features namely the Time Salient Pairwise feature (TSP) and the Space Salient Pairwise feature (SSP) are designed to describe two salient directed graphs, respectively. Experiments are carried out with a homogeneous kernel SVM classifier, on four challenging datasets
TSP  +  SSP  +  BoVW  can  properly  describe  human  actions  with  large  intro-variability  in  real-time. KTH, ADL and UT-Interaction. Final results confirm the complementary of TSP and SSP, and our multi-cue representation
Copyright © 2016, Chongqing University of Technology. Production and hosting by Elsevier B.V. This is an open access article under the CC
BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Keywords: Spatio-temporal interest point; Bag-of-visual words; Co-occurrence



Introduction
Recently, human action classification from video sequences plays a significant role in humanecomputer interaction, content-based video analysis and intelligent surveillance, however it is still challenging due to cluttered backgrounds, occlusion and other common difficulties in video analysis.

* Corresponding author. G102-105, School of Computer & Information Engineering Peking University, Shenzhen University Town, Xili, Nanshan
District, Shenzhen, Guangdong Province, China. Tel.: +86 (0755)2603 5553.
E-mail addresses: liumengyuan@pku.edu.cn (M. Liu), hongliu@pku.edu.
cn (H. Liu), qianrusun@sz.pku.edu.cn (Q. Sun), zhangtianwei5@gmail.com (T. Zhang), dingrunwei@pkusz.edu.cn (R. Ding).
Peer review under responsibility of Chongqing University of Technology.
What's worse, intro-variability among the same type of actions also brings serious ambiguities. To tackle these problems, many human action classification methods based on holistic
and local features have been proposed [1,2]. Holistic features have been employed in Refs. [3e5], where actions were treated as spaceetime pattern templates by Blank et al. [3] and the task of human action classification was reduced to 3D object recognition. Prest et al. [4] focused on the actions of humaneobject interactions, and explicitly represented an ac- tion as the tracking trajectories of both the object and the person. Recently, traditional convolutional neural networks (CNNs) which are limited to handle 2D inputs were extended, and a novel 3D CNN model was developed to act directly on raw videos [5].


http://dx.doi.org/10.1016/j.trit.2016.03.001
2468-2322/Copyright © 2016, Chongqing University of Technology. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC- ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



Comparing with holistic features, local features are robust to shelters which need no pre-processing such as segmentation or tracking. Laptev [6] designed a detector which defines spaceetime interest points (STIPs) as local structures where the illumination values show big variations in both space and time. Four later local feature detectors namely Harris3D de- tector, Cuboid detector, Hessian detector and Dense sampling were evaluated in Ref. [7]. Recently, dense trajectories sug- gested by Wang et al. [8] and motion interchange patterns proposed by Kliper-Gross et al. [9] have shown great improvement to describe motions than traditional descriptors though both need extra computing costs. Besides using content of local features, researches only using geometrical distribu- tion of local features also achieve impressive results for action classification. Bregonzio et al. [10] described action using clouds of SpaceeTime Interest Points, and extracted holistic features from the extracted cloud. Ta et al. [11] concatenated 3D positions of pairwise codewords which are adjacent in space and in time for clustering. A bag of 3D points was employed by Li et al. [12] to characterize a set of salient postures on depth maps. Yuan et al. [13] extended R transform to an extended 3D discrete Radon transform to capture dis- tribution of 3D points. These methods assume that each local feature equals to a 3D point, and all local features have the only difference of location.
Bag-of-visual words (BoVW) introduced from text recog- nition by Schuldt et al. [14] and Dollar et al. [15] is a common framework to extract action representation from local features. STIPs are firstly extract from training videos and clustered into visual words using clustering methods. BoVW is then adopted to represent original action by a histogram of words distribu- tion, and to train classifiers for classification. Despite its great success, BoVW ignores the spatio-temporal structure infor- mation among words and thus leads to misclassification for actions sharing similar words distribution. To make up for above problem of BoVW, the spatio-temporal distribution of words is explored. Words are treated in groups to encode spatio-temporal information in Refs. [16e18]. Latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model are utilized by Niebles et al. [16] to learn the probability distributions of words. Cao et al. [17] applied PCA to STIPs, and then model them with Gaussian Mixture Models (GMMs). A novel spatio-temporal layout of actions, which assigns a weight to each word by its spatio-temporal proba- bility, was brought in Ref. [18]. Considering words in pairs is an effective alternative to describe the distribution of words. From one point of view, pairwise words which are adjacent in space and in time were explored by Refs. [11,19,20]. Local pairwise co-occurrence statistics of codewords were captured by Banerjee et al. [19], and such relations were reduced using Conditional Random Field (CRF) classifier. Savarese et al.
[20] utilized spatial-temporal correlograms to capture the co- occurrences of pairwise words in local spatio-temporal re- gions. To represent spatio-temporal relationships, Matikainen et al. [21] formulated this problem in a Nave Bayes manner, and augmented quantized local features with relative spatial- temporal relationships  between  pairs  of features. From
another point of view, both local and global relationships of pairwise words were explored in Refs. [22,23]. A spatio- temporal relationship matching method was proposed by Ryoo et al. [22] which explored temporal relationships (e.g. before and during) as well as spatial relationships (e.g. near and far) among pairwise words. In Ref. [23], co-occurrence relationships of pairwise words were encoded in correlo- grams, which relied on the computation of normalized google- like distances.
In this work, the directional relationships of pairwise fea- tures are explored to make up the problems of BoVW. It is observed that human actions make huge senses in the direc- tional movement of body parts. From one aspect, the spatial relationships among different parts, which are moving at the same time, are directional. Besides, one part keeps direction- ally moves from one place to another. Here, a “push” action in
denote local features. As shown in Frame t + 1, the pusher's Fig. 1 is used to illustrate observations, where green points hands and the receiver's head are moving at the same; mean-
while, the vertical location of hands is lower than the head. The relationship between this type of pairwise motions, which is according to the first observation, is called directional co-
occurrence. Crossing from Frame t — 1 to Frame t, the
pusher's hands keep moving forward. This type of pairwise
motions are also directional and reflect the second observation.
The observations both indicate the importance of directional information for action representation. Hence the attribute of mutual directions are assigned to pairwise STIPs to encode structural information from directional pairwise motions, generating new features called Time Salient Pairwise feature (TSP) and Space Salient Pairwise feature (SSP).

Time Salient Pairwise feature

Time Salient Pairwise feature (TSP) is formed from a pair of STIPs which shows “directional co-occurrence” property. In our previous work, [24] and [25] have already employed this property for action recognition. The TSP mentioned in this paper is a refined and expanded version from the conference proceedings paper [24]. TSP is compared with traditional

Fig. 1. A “push” action performed by a “pusher” and a “receiver”.



BoVW and “Co-occur” based methods in Fig. 2, where action 1, action 2 are simplified as labeled points a, b and ti
(i ¼ 1,…,12) means time stamps. Here, “Co-occur” adopted
by Sun et al. [23] means only using co-occurrence feature of
pairwise words. BoVW fails in the second and third rows when two actions share the same histogram of words. “Co-
Table 1
Illustrating the meanings of symbols.


Symbol	Meaning


F0	Number of frames for video V0
N	Number of training videos
It	The tth frame of a video

occur” can distinguish actions in the second row but also fails
V 0 ¼ fItgF0
A video containing an action

when two actions share the same co-occurrence features. TSP adds extra directional information to co-occurrence features, thereby avoiding two failing cases of both BoVW and “Co-
occur”. Comparing with [22], our novelty lies in the use of
M	Number of STIPs for V 0
S 0	STIPs from V 0
des	A descriptor for one STIP
D	Dictionary for feature quantization
pti ¼ (xi,yi,ti,labeli)	One STIP with label labeli

Sg0 ¼ fpti ¼ ðxi; yi; ti; labeli)gM
Labeled STIPs from V 0

occurrence. TSP also differs from Refs. [20] and [23] in the
use of both number and direction of pairwise words.

Space Salient Pairwise feature

Note that TSP only captures the directional information between different labeled pairwise words and ignores the re- lationships among same labeled words. To encode this rela- tionship, geometrical distribution of local features need to be involved. In this work, any pair of words sharing same labels are linked into a vector, and all vectors are as input instead of local descriptors like Histogram of Gradient (HoG) [26] or Histogram of Flow (HoF) [27] for traditionally BoVW model. This new feature is named Space Salient Pairwise feature (SSP) which is different from Ref. [11] in capturing global distribution of pairwise points. As shown in the fourth row of Fig. 2, SSP provides spatial location information for TSP to classify two actions with same co-occurrence properties.

Modeling human actions as directed graphs
In graph theory, a directed graph refers to a set of nodes connected by edges, where edges have directions associated
E s ¼ < E ss; E st >	Salient edges
G st ¼ < P ; A st >	A time salient directed graph
G ss ¼ < P ; A ss >	A space salient directed graph
K	Clusters for BoVW
Tx, Ty, T	Threshold value for TSP
N	Distribution map for TSP
K2	Clustering centers for SSP
V E 0	All possible SSP in S~0
HTSP, HSSP	TSP feature and SSP feature
H	Representation for V 0




with them. In this paper, directed graphs are employed to represent the human action in a video V 0, and the main work lies in the determination of nodes, the choice of edges and the assignment of directions between nodes. Some symbols used in following sections are listed in Table 1 with their meanings. An action sequence can be denoted by a cloud of Spatio- temporal interest points (STIPs) in the field of action anal- ysis using local features. By referring to a dictionary D , STIPs are clustered into different labels and each label stands for a kind of movement. Here, all labeled STIPs are defined as nodes of the directed graphs. To construct dictionary D , a set


Fig. 2. Comparing representations of similar actions by four methods, namely Bag of Visual Words (BoVW), Co-occurrence Feature (Co-occur), Time Salient Pairwise feature (TSP) and Space Salient Pairwise feature (SSP).



Fig. 3. Representing a human action as a directed graph with salient edges.


of training videos {V
Fn  N

n	t t=1 n=1
are needed, where V
generating in two directional edge sets A ss and A st (Fig. 3). Then, the undirected graph G s is changed to time salient

S n = {(x; y; t; des)|(x; y)2It; t2(1; …; Fn)}  are  detected is  the  nth  video  with  Fn  frames.  STIPs from video V n, where x,y refer to horizontal and vertical
coordinates, t is the index of frame, des2ℝN denotes the N- dimensional feature vector of the STIP. Then, all des from
S = {S 1; …; S n; …; S N} are clustered into K clusters D = {des1; …; desk; …; desK} using algorithms like k-means. To label STIPs S 0 = {(x; y; t; des)|(x; y)2It; t2(1; …; F)} from the video V 0 = {It}F0 , each des in S 0 is labeled by finding the nearest center in dictionary D . If the nearest
cluster is desk, then des is labeled k. Till now, the video V 0 is represented	by	M	labeled	points
Sg0 = {pti = (xi; yi; ti; labeli)|(xi; yi)2It; ti2(1; …; F0); labe
directed graph G st = < P ; A st > and space salient directed graph G ss = < P ; A ss > .
Time salient directed graph
It is observed that pairwise different movements appearing at the same time are a good feature to distinguish an action. For example, an action “Blow Dry Hair” from UCF101 dataset
[28] usually refers one person moves his hand and hair simultaneously. When an action is denoted as a cloud of labeled STIPs, this observation can be represented by the co- occurrence of different labeled pairs, which is captured by time salient graph G st. To describe G st, directions are


To describe the spatio-temporal distribution of S 0, points
necting any pair of points from S 0, an undirected graph G = are considered in pairs for simplicity and efficiency. By con-
< P ; E > is defined to model video V 0, where P = {pti}M
this part, a simple direction assignment criteria is established
to convert G st to G st. Then, a new descriptor called Time Salient Pairwise feature (TSP) is introduced, involving not only nodes but also the directional edges in G st. Finally, the

and E =
edge(pt ; pt ) (ci; j21; …; M)∧(is
i=1
d
statistics of TSP is utilized to represent G st.

that edge(pti; ptj) is the edge between pti and ptj. Since
undirected graph G s = < P ; E s > with less edges is defined directly using G to represent V 0 is not time efficient, a new by splitting E into salient edges E s and non-salient edges E u.
Moreover, salient edges is split into time salient edges E st and space salient edges E ss. The time saliency refers to two different labeled nodes appearing at the same time, which is also called co-occurrence, and the space saliency denotes two
liency of an edge edge(pti; ptj)2E s is formulated as follows, same labeled nodes appearing cross different frames. The sa- edge pti; ptj 2E st⇔ti = tj∧labelislabelj
ss	( )
i	j	i	j	i	j
An example of E s is shown in Fig. 3, where gray edges belongs to E ss and black edges pertain to E st. In order to give edges in E s quantitative descriptions, different direction assignment methods are respectively applied on E ss and E st ,
Time Salient Pairwise feature

The criteria of direction assignment between STIPs are introduced before defining TSP. Suppose STIPs of a given sequence are clustered into K words. Sketch in Fig. 4 shows how to assign direction for word A and word B. Although the vector formed by A and B provides exact spatial information, it considers little about the noise tolerance. Instead, whether the direction is from A to B or B to A is a more robust feature. Vertical or horizontal relationship is utilized to figure out the direction between A and B with two reference directions defined from up to down and left to right respectively. It is noted that human actions like waving right hand and waving left hand are usually symmetric. Their directions are opposite in horizontal direction but same in vertical direction. Thus, we consider the vertical relationship priority to the horizontal one



Fig. 4. Direction assignment criterion for pairwise STIPs in the same frame.


to eliminate the ambiguities of symmetric actions. Let Dx and Dy represent projector distances and Tx, Ty stand for threshold values (in Fig. 4). If A and B are far in vertical direction
(Dx ≥ Tx), the reference direction is set from up to down. In
contrast (Dx < Tx), the relationship in the vertical direction is
not stable and thus discarded. The horizontal relationship is
checked in the same way. As for A and B in Fig. 4, since


established, which records both labels and the direction in- formation between two labels.

Time salient directed graph

stored	in	Sg0 = {pti = (xi; yi; ti; labeli)|(xi; yi)2It; ti2(1; For a given video V 0, M labeled STIPs are detected and
		


selected and the direction is assigned from B to A, which is in
accordance with the reference direction. This criteria ignores same labeled pairs like E and F in Fig. 4, and also discards any
sent a word labeled i appearing on frame tpti . Horizontal and
vertical coordinates are xpti and ypti . Then, the time salient
directed	graph	G st = < P ; A st > ,	where

pair of points like C and D that are too close to each other.
A st =
{TSP
labeli;labelj
 i; j2
d
(1; …; M)}.

Summarily speaking, the criteria to assign direction for points
pti = (xi,yi,ti,labeli) and ptj = (xj,yj,tj,labelj) are as follows,
To describe G st, 4( pti,ptj) is firstly used to record whether
there exists TSPlabeli ;labelj between pti and ptj,

4 pti; ptj =  z pti; ptj ; if  Dx ≥ Tx∧xi < xj ∨ Dx < Tx∧ Dy ≥ Ty∧yi < yj ;
(3)




if ti = tj∧labelis labelj cpti; ptj2P 
where Dx = xi — xj, Dy = yi — xj, threshold Tx, Ty are

if xi < xj then i/j else j/i
elseif abs y — y  ≥ T
(2)
Formula (3) is equal to that of Formula (2). In Formula (3),

if yi < yj then i/j else j/i
where i / j indicates the direction.

z pt ; pt  =  1;  if labelislabelj∧ti = tj
(4)

After direction assignment, the reserved directions are discriminative to represent directional co-occurrent move- ments. Each direction with two linked nodes construct a new descriptor called Time Salient Pairwise feature (TSP). Taking A and B in Fig. 4 as an example, two assumptions are made. a) A and B satisfy the direction assignment criteria in Formula
(2);  b)  the  direction  is  from  B  to  A.  Then  a  TSP
TSPlabelB ;labelA = (labelB; labelA; labelB/labelA)	is
i	j	0;	otherwise

Co-occurrence literally means happening on the same frame. While, in an action sequence, movements constituting the whole action last several sequential frames. To encode this temporal relationship, we treat adjacent several frames as a whole to extract co-occurrence features. Thus, z( pti,ptj) is reformulated as,

z pt ; pt  =  1;  if labelislabelj∧ ti — tj < Tt
(5)
tail endpoints adjacent to a node is its out-degree. In Formula




If z( pti,ptj) in Formula (5) equals one, a co-occurrence
feature is defined between pti and ptj. Threshold Tt is an empirical value determining the number of adjacent frames.


m
and C(m) is the number of m.
PK  N (m; n)

labels as start point or end point. Matrix N in Formula (6)
records the number distribution of all types of TSP in G st,
n=1
Similarly, P(TSPin N ; C) in Formula (8) represents the
probability of m being the end point.

N (m; n)= 
~ m
pti; ptj
~ n	K

cpti 2S 0 ;cptj 2S 0
s.t. m; n2(1; …; K)
(6)
P TSPin N ; C = P
n=1 N (n; m)
{C(m)$C(n)}
(8)

The distribution map N is most related to the co-occurrent
map [23] which records the number of co-occurrence between STIPs labeled m and n for location (m,n). In order to intui- tively show the difference, a simple action “eating a banana” is used. Two result maps namely distribution map and co- occurrent map are shown in Fig. 5. It is shown that element


n=1
to construct video representation HTSP with K × 2 dimension. Above two probability values are combined in Formula (9) Using HTSP instead of histogram N, the video representation is
compressed at a ratio of K/2.
H	= n P TSPout N ; C  K  ; P TSPin N ; C  K  o	(9)

while different in the distribution map, and element value in (m,n) from co-occurrent map equals the average value between element values in (m,n) and (n,m) from distribution map. Therefore, distribution map encodes more information than co-occurrent map.
Till now, the directed graph G st is reduced to a distribution
element N (m; n) in N is related to the number of m and n. map N with K$K dimension which is still high. What's worse, Directly using N  as video representation should be at slow
speed and is sensitive to the effected by number of STIPs. Therefore a dimension reduction method which also handles the number of STIPs is needed. As shown in Fig. 6, G st is convert to a new directed graph T st by merging same labeled nodes. The in-degree and out-degree are introduced as statis- tics for each node in T st. In mathematics, and more specif- ically in graph theory, the number of head endpoints adjacent to a node is called the in-degree of the node and the number of
In this section, we focus on pairwise features and extracting
directional information from them to reflect the natural structure of human actions that our motion parts are direc- tional. Time Salient Pairwise feature (TSP) is proposed to describe the relationships between pairwise STIPs on the same frame, and only the pairs with different labels are considered. Obviously, TSP ignores the relationships between pairwise STIPs with same labels in G ss, and brings ambiguous to distinguish actions with similar G st. Thus, this paper proposes another descriptor called Space Salient Pairwise feature (SSP) to describe G ss.

Space salient directed graph
To describe an action sequence, a cloud of STIPs are extracted	and	organized	in	a	directional	graph
G s = {G ts; G ss}. A feature called TSP is proposed to

d	d	d


Fig. 5. Distribution map of TSP and co-occurrent map are respectively shown in (a) (b). To facilitate observation, STIPs are extracted and clustered to 30 labels.




















Fig. 6. Extracting statistics from distribution map N of TSP.


captures directional information in G ts. As for G ss, another feature called Space Salient Pairwise feature (SSP) is intro- duced to encode the relationships between pairwise STIPs
A ss = {SSPpt ;pt i; j2(1; …; M)}. Let V E 0  involve all possible SSP in S~0, which is defined as follows,

sharing same labels. And the histogram of quantized SSP is
V E 0 =	∪
∪	SSPpti ;ptj	(11)

simply utilized as the representation of G ss. For an action
m2(1;…;k) cpt ;pt 2S~ m

i  j	0
d

constructed by some main movements, labeled STIPs are
dominated by a minor group of labels. Therefore, relationships
And  V E 0  is  clustered  into  K2	centers, namely
{V E 1; …; V E K2 }. Then, G ss is represented by HSSP, which

0	0	d

among same labeled STIPs are important to describe this kind
of actions. Take action “boxing” from KTH dataset [14] as an example, which means stretch out a hand and then withdraw it rapidly and periodicity. This action is dominated by the “clenched fist” which appears repeatedly. Obviously, the dis- tribution of the “clenched fist” encoded by SSP is vital to represent “boxing”.

Space Salient Pairwise feature

For same labeled STIPs appearing on different frames, Space Salient Pairwise feature is defined. Given two labeled
STIPs  pti = (xpt ; ypt ; tpt ) and  ptj = (xpt ; ypt ; tpt ),  a  SSP
simply tallies K2 clusters of V E 0.
Using HSSP to describe G ss is inspired by traditional BoVW model, which utilizes the number histogram of STIPs and has achieved markable results in human action recognition. Spe- cifically, this method refers to obey the BoVW model and to use pairwise features instead of traditional HOG-HOF features for clustering and quantization. Detailed steps for computing HSSP are illustrated in Fig. 7. STIPs are firstly extracted from an input action sequence and assigned labels. All STIPs are divided into different channels by their labels. In each channel, a vector is formed between any pair of STIPs from different frames. Then vectors are collected from all channels to construct a vectors
bank, which refers to the edges of G ss. Finally, vectors in the

i	i	i
j	j	j	d

SSPpti;ptj
= (xi — xj; yi — yj; ti — tj)$d(ti — tj) is established if
bank are clustered and a histogram is formed to represent G ss.

tistj, where	A human action video V 0 is described using salient directed
graph in Algorithm 1. {V	= {I }Fn }N	are N videos con-

  1;	if t < t ;
n	t t=1 n=1

—1; if ti > tj
Intuitively speaking, SSPpti ;ptj indicates the vector with the
point which appears earlier to be a start point.

Space salient directed graph

For a given video V 0, M labeled STIPs are detected and
g
K, K2 are pre-defined for k-means clustering method. STIPs are extracted and clustered into labels from line 1 to line 7. A vector set V E n is also formed for video V n in line 8. To extract
representation HTSP from video V 0 = {It}F0 , the procedure is
detailed in Algorithm 1 from line 10 to line 23. Symbol ptm in line
13 denotes any point labeled m. Function z( pti,ptj) is shown in Formula (5), which is a part of Formula (6) in line 17.
P(TSPout N ; C) in line 20 means the probability of label m

…; F ); label 2(1; …; K)}M .	Note	that
appearing as a start point. P(TSPin N ; C) in line 21 means the

S~ 0 = {S~ 0; …; S~ 0; …; S~ 0 }, and S~ 0 stores all STIPs
noted that P(TSPout N ; C) plus P(TSPin N ; C) is no more than



Fig. 7. Flowchart of extracting SSP feature from pairwise points.


taking word pair (C, D) in the sketch of Fig. 4 as an example. If relationships between points labeled m and all other points are
considered, the value P(TSPout N ; C) plus P(TSPin N ; C)
should equal one. Using space salient pairwise feature to extract
action representation named HSSP from testing video V 0, the procedure is illustrated in Algorithm 1 from line 24 to line 26.


TSP and SSP are naturally combined for their ability of capturing structural relationships of different kinds of STIPs. On one hand, TSP only focus on different labeled pairwise STIPs, while it ignores the spatial temporal constraints which are brought in by same labeled pairs. Additionally, SSP pro-
compatible with TSP. Let H = {HTSP,HSSP} stand for the vides extra relationships among same labeled pairs, and thus is combination form of both methods. Moreover, The combina-
tion form of H and traditional BoVW, which provides general statistical information of STIPs, is also constructed.
For a given video V 0, let M denote the number of STIPs extracted from V 0 with F0 frames, and these STIPs are clustered into K clusters. Suppose that there are equal number of STIPs in each cluster, and that the number of STIPs are equal for each frame. In this case, the number of pairwise
feature  for  calculating  TSP  and  SSP  are  respectively


C2 $ 

M K$F0
2
$F0 and C2 $
2
M K$F0

$K. The time complexity

for	calculating	final	representation	H	is


O C2 $

M K$F0
2
$F0
+ O(K)+ O 

C2 $
0
2
M K$F0

O(M 2), where O(K ) denotes the time complexity of the
dimension reduction method for TSP. Since the main time cost
is to calculate TSP and SSP, reducing the number of pairwise feature will improve the efficiency of Algorithm 1. To this end, feature selection methods like [29,30] can be applied.
To improve the speed of calculating TSP and SSP, we convert main calculation into several matrix operations which is suitable for MATLAB in the experiments. The main computation shared by TSP and SSP is to compute all pairwise

distances among a set of points {xi}M
, where xi denotes the

coordinate of point i. Let X1;M equals [x1,…,xM], which



matrix ZM,M = AM,1X1,M, where all elements in AM,1 equals denotes a matrix with one row and M columns. We form a
'
one. Then the distance matrix equals Z—Z , whose element in
ith row and in jth column records the distance between point xi
and xj. Comparing with Algorithm 1 which directly compares
and “dial a phone” are alike in ADL dataset. Besides the similarity between action “kick” and “punch” in UT- Interaction dataset, the complex filming scenes in UT- Interaction scene-2 also brings difficulty for classification. In following, KTH, ADL and UT datasets are utilized to evaluate

any pair of points and thus cost C2
times of computation, only
our method against inter-similarity among different types of

matrix     by     AX     —    (AX   )'. three matrix operations are needed here to obtain the distance
Experiments and discussions
The proposed descriptors are evaluated on four challenging datasets: KTH dataset in Ref. [14], ADL dataset in Ref. [31] and UT-Interaction dataset in Ref. [22]. KTH dataset con- tains 600 videos of 25 persons performing 6 actions: “wal- king”,“jogging”,“running”,“boxing”, “hand waving” and “hand clapping”. Each action is repeated 4 times with homo- geneous indoor/outdoor backgrounds. ADL dataset contains 150 videos of five actors performing ten actions: “answer a phone”, “chop a banana”, “dial a phone”, “drink water”, “eat a banana”, “eat snacks”, “look up a phone number in a phone book”, “peel a banana”, “eat food with silverware” and “write on a white board”. Each action is repeated three times in the same scenario. Segmented version of UT-Interaction is utilized which contains six categories: “hug”, “kick”, “point”, “punch”, “push” and “shake-hands”. “Point” is performed by single actor and other actions are performed by actors in pairs. All actions are repeated ten times in two scenes resulting in 120 videos. Scene-1 is taken in a parking lot with little camera jitter and slightly zoom rates. In scene-2, the backgrounds are cluttered with moving trees, camera jitters and passers-by.
Several action snaps from above datasets are shown in Fig. 8, where inter-similarity among different types of actions are observed. Actions like “walking”,“jogging” and “running” are similar in KTH dataset, and actions like “answer a phone”
actions, and to evaluate the efficiency of proposed algorithm.
“UT” involves both scenes in UT-Interaction dataset.
This work applies Laptev's detector in Ref. [14] obeying original parameter setting to detect STIPs and uses HOG-HOF in Ref. [32] to generate 162 dimension descriptors (90
dimension for HOG and 72 dimension for HOF). After extracting 800 points from each video, k-means clustering is applied to generate visual vocabularies. In order to obtain maximum average recognition rates, the number of clusters for DPF, BPF and BoVW on different datasets are set in Table 2. Recognition was conducted using a non-linear SVM with a homogeneous kernel in Ref. [33]. In order to keep the reported results consistent with other works, we obey the same cross- validation method with [14,31] and [22]. Since random initialization is involved in clustering method, all confusion matrices are average values over 10 times running results.

TSP evaluation

Different parameters Tt and T for TSP are tested on KTH,
other parameter in default values: Tt = 0, T = 0. Parameter Tt ADL and UT datasets, with one parameter changing and the is the number of adjacent frames. In other words, each frame
with its adjacent Tt frames are considered as a whole to extract TSP for current frame. In Formula (3), Tx and Ty are both set to T, which is the threshold value both for the horizontal and vertical directions.
As shown in Fig. 10, Tt ranges from 0 to 4 at 2 intervals, and T ranges from 0 to 10 at 5 intervals. Taking UT dataset




Fig. 8. Human action snaps from four datasets: KTH, ADL and UT-Interaction.



avgRate = 0.9383
avgRate = 0.9283
avgRate = 0.9450






ans. chop. dial. drink. eatB. eatS. lookup. peel. use. write.
avgRate = 0.9133
avgRate = 0.7400
avgRate = 0.9267



avgRate = 0.8417
avgRate = 0.7083
avgRate = 0.8500

	
Fig. 9. Comparing three methods namely BoVW (a1, b1, c1), TSP (a2,b2,c2) and BoVW + TSP (a3, b3, c3) on different datasets.

which contains clustered backgrounds and moving disruptors
as an example, the recognition rate slightly improves when Tt grows, and keeps quite still when T changes. This phenome- non shows that the performance of TSP is not sensitive to the
following experiments are conducted with Tt = 0, T = 0. changes of parameters Tt, T in a large range. In this work, all
Representation TSP and BoVW are separately compared on
KTH dataset (Fig. 9(a)), ADL dataset (Fig. 9(b)) and UT dataset (Fig. 9(c)) using confusion matrices. Generally speaking, TSP achieves less average recognition rates than
BoVW. Meanwhile, TSP + BoVW works better than both TSP
Table 2
Number of clusters for different datasets.
to traditional BoVW. The method of TSP + BoVW shows and BoVW, which shows the complementary property of TSP 0.67% higher than BoVW on KTH dataset, 1.34% higher on
ADL dataset and 0.83% higher on UT dataset.
In Fig. 9(a3), TSP improves the discrimination between “jogging” and “running” in KTH dataset. TSP also reduced the errors among “answer a phone” and “dial a phone” in Rochester since extra spatial information is encoded. In UT dataset, most errors happens between “kick” and “punch” in Fig. 9(c1). These two actions appear similar to BoVW which focus on describing local features, since they share similar basic movement “stretch out one part of body (hand or leg) quickly towards others”. Seeing from human's view, “punch”
refers to leg and “kick” refers to hand. Thus, their spatial
distribution of movements, which are captured by spatial


	
Fig. 10. Classification precisions using TSP with different parameter settings.


As can be seen in Fig. 9(c3), the recognition rate of “punch” drops when compared with BoVW. The reason lies in that TSP brings some ambiguities to BoVW to distinguish “punch” and “shake-hands”. To solve this problem, SSP is utilized to make up the limitations of TSP. The effect of SSP to improve the recognition precisions of “punch” and “shake- hands” are detailed in next section.

SSP evaluation

Obeying procedures in Algorithm 1, we firstly set cluster number K the same as Section 5.1 to cluster STIPs into labels. After obtaining vectors from all channels, these vectors are then clustered into K2 clusters. The value of K2 with best recognition rates are shown in Table 2.
Representation SSP and BoVW are separately compared on KTH dataset (Fig. 11(a)), ADL dataset (Fig. 11(b)) and UT dataset (Fig. 11(c)) using colored histograms. Generally speaking, SSP achieves less average recognition rates than
BoVW. Meanwhile, SSP + BoVW works better than both SSP
to traditional BoVW. The method of SSP + BoVW shows and BoVW, which shows the complementary property of SSP 1.84% higher than BoVW on KTH dataset, 3.34% higher on
ADL dataset and 5.00% higher on UT dataset.
As shown in the UT dataset of Fig. 11, the recognition precisions of “punch” and “shake-hands” are improved when comparing with traditional BoVW. The reason lies in that SSP encodes the movements of same types of movements, which are neglected by BoVW. In next section, SSP is combined with TSP and BoVW, and the final representation outperforms SSP, TSP and BoVW.

Comparison with related works

Tables 3e5 compares the performances of proposed method with state-of-the-arts and cluster number K is marked with classification rate. Since parameters like the number K of k-means clustering method differs in different algorithms, the accuracy refers the classification rate with optimal parameters.
KTH dataset is originally utilized by Ref. [14], and the cited paper is marked in italic in Table 3. Our results on KTH dataset are most directly comparable to the method in Refs.
[14] and [27], which both utilize the laptev's local feature detector and the BoVW framework. Our BoVW shows much higher than [14] since Laptev's HOG/HOF descriptor and a
adopted. TSP + BoVW, SSP + BoVW achieves average ac- non-linear SVM with a homogeneous kernel in Ref. [33] are curacies of 94.50% and 95.67%. Improvements of 2.70% and
3.87% are respectively achieved over [27], which can be attributed to our addition of spatial temporal distribution in-
formation. TSP + SSP + BoVW achieves average accuracy of
95.83%, which is respectively 1.03% and 0.83% higher than
state-of-the-art works [34] and [35].
ADL dataset is originally utilized by Ref. [31], which main focus on people's interaction with objects in the kitchen. In the dataset, actions like “answer a phone” and “dial a phone”
looks similar in motions, which leads to an average accuracy of only 67.00% using “Velocity Histories” feature in Ref. [31]. It is noted that the background in ADL keeps still, and an “Augmented Velocity Histories” is proposed in Ref. [31] which achieves an average accuracy of 89.00%. Without using structural information from the still background, our
What's more, TSP + SSP + BoVW achieves average accuracy methods all performs better than [31], shown in Table 4. of 95.33%, which is 3.33% higher than state-of-the-art work
[36]. Comparing with our previous work [25], additional 4.00% accuracy is gained, which shows the importance of SSP to TSP and BoVW.
UT dataset is originally utilized by Ref. [22], which main focus on people's interaction with others. Since moving trees and not related persons are also included in the scenes, this dataset can be used to evaluate method's robustness to clut- tered backgrounds. As shown in Table 5, our best result ach-
ieves 92.50% accuracy, which is 4.9% higher than recent work [38]. Since [39] mainly focus on the speed of the algorithm, the local feature detector and clustering steps are implemented using more fast method like V-FAST interest point detector and semantic texton forests. To ensure a fair comparison with
























Fig. 11. Comparing BoVW, SSP and BoVW + SSP on different datasets.

our method, we compare the time cost of extracting features with [39] in next section.
Recently, dense trajectory [8] are widely used in off-line human action recognition, and achieves better accuracy than
HOG/HOF features. However, methods in Ref. [8] requires longer time to extract dense trajectories and to form the BoVW features, which are not suitable for real-time applica- tions. Thus, we detect the sparse Harris3D points and extract



Table 3
Comparing with related works on KTH.


Methods	Accuracy (%)	Details

LF + SVM [14]	71.70	Schuldt et al. (2004) LF + SP + non-linear SVM [27]  91.80	Laptev et al. (2008) MBH + STP [8]	95.30	Wang et al. (2013)
RMD + Mode Finding [36]	92.10	Oshin et al. (2014) RMD + Outlier Detection [36]	94.00	Oshin et al. (2014) Multi-ch. Gabor + SOD [34]	94.80	Zhang et al. (2014)
STLPC [35]	95.00	Shao et al. (2014)
BoVW	93.83	K = 900
BoVW + TSP	94.50	K = 900,200
BoVW + SSP	95.67	K = 900,100
BoVW + TSP + SSP	95.83	K = 900,200,100

Table 4
Comparing with related works on ADL.




Table 5
Comparing with related works on UT.

HOG/HOF features using Laptev's detector and descriptor instead of using dense trajectory. The computation efficiency of proposed features TSP and SSP are evaluated in next part.
Final recognition rates using multi-cue representation are shown in Fig. 12, and there still exists ambiguities among similar actions. In ADL dataset, “answer a phone” and “dial a phone” are similar naturally since they contains same move- ments like picking up a phone and bring it to the ear. “Peel a banana” and turning pages in “look up a phone a number” also look similar in having same hand motions.
In Fig. 13(a), the detected STIPs are too sparse for same actions, which also response for imperfect results. In Fig. 13(b,c), cluster backgrounds and passers-by bring in extra STIPs, which result in more ambiguities for representation and classification. Despite these difficulties, our method obtains remarkable results by adding extra spatial structural informa- tion to traditionally BoVW method, e.g., better discriminative results between “answer a phone” and “dial a phone” are shown in Fig. 12(b).

Computation efficiency and potential applications

The efficiency of calculating TSP and SSP on different datasets are evaluated in Fig. 14, where parameter K is in default for both SSP and TSP. Meanwhile, TSP is evaluated with different parameters F and T. The computation time was estimated with MATLAB R2011a (The MathWorks, Natick, MA) on a PC laptop with a 3.00 GHz Intel Core i5-2320 CPU and 4 GB of RAM. Two indicators namely Td and Tf are uti- lized for evaluation, which mean the time cost of extracting feature TSP or SSP for whole dataset and for each frame.
Since the values of Td and Tf are related to the number of STIPs, the more STIPs cost the longer time. On KTH dataset, Td nearly equals 12 s for extracting TSP and 60 s for calcu- lating SSP. Since KTH contains more number of STIPs for whole dataset, Td on KTH is bigger than ADL and UT, which is shown in Fig. 14(a1, b1). On UT dataset, Tf nearly equals
0.3 ms for extracting TSP and 1.8 ms for calculating SSP. As the complex background of UT brings more STIPs for each frame, Tf on UT is larger than KTH and ADL, which is illustrated in Fig. 14(a2, b2).
The TSP and SSP can be generated efficiently, thus expands the usage of proposed algorithm in many applications like


   	

Fig. 12. Recognition result on KTH (a), ADL (b), UT (c) combining three methods BoVW, TSP and SSP.



Fig. 13. Key frames of three actions from ADL and UT are illustrated to show misclassification.





 	 

	
Fig. 14. Comparing computation efficiency of TSP and SSP with different parameters.


realtime human action classification and video retrieval, ac- tivity prediction and human robot interaction:
The pipeline of performing real-time human action clas- sification is as follows. Given a video containing an action,
STIPs are extracted quickly using Laptev's detector in Ref.
[14]. Then BoVW, TSP and SSP features are calculated in real-time using offline trained models. Finally, non-linear SVM with homogeneous kernel generates the type of ac- tion efficiently. Since the proposed algorithm are not limited to human actions, it can be utilized to improve the performance of content based video retrieval.



Fig. 15. Applying human action recognition method to interact with robot named “Pengpeng” in a noisy environment.


Recently, many researches focus on the prediction of ongoing activities [40e42], whose objective is to predict
potential actions and alarm person to prevent dangers like “fighting” from happening. Treating an ongoing activity as small segments of videos, our algorithm can be applied to intelligent systems to predict some activities by trans- forming the task of prediction to classify early video segments. For example, when an early action named “one person stretch out his fist quickly towards another person” is observed, it's likely to be a later action named “fighting”
afterwards.
A mobile robot designed by our lab with a camera and a humanemachine interface are shown in Fig. 15. We adopt
the PHILIPS SPC900NC/97 camera and place it on the head of the robot with a height of 1.8 m. Additionally, a curve mirror is utilized to change the camera into a 360 degree panoramic camera. The mobile robot works in a
hall, semi-door environment, with a size of 8 m × 8 m. We
defined three types of actions namely “Waving”, “Clap-
ping” and “Boxing”, which refer to three orders “moving forward”, “circling” and “moving backward”. As shown in the pipeline of Fig. 15, human actions are captured as input for our real-time human action recognition system after preprocessing. Action models are trained based on the KTH dataset [14], and also as input for the system. The output of the action type “Waving” serves as a command “Moving forward” for the robot. Especially in noisy en- vironments, our proposed action recognition method can clearly deliver orders in real-time than using sounds or traditional BoVW method.

Conclusions and future work
In this work, a video of human action is referred to a cloud of STIPs, which are modeled by a saliet directed graph. To
describe the salient directed graph, a Time Salient Pairwise feature (TSP) and a Space Salient Pairwise feature (SSP) are proposed. Different from BoVW and related works in capturing structural information, TSP involves the words' co-
occurrence statistic as well as their directional information.
Since richer information of spatial-temporal distribution is involved, TSP outperforms baseline BoVW. Additionally, a Space Salient Pairwise feature (SSP) is designed to describe geometric distribution of STIPs which is ignored by TSP. The SSP achieves compatible results with BoVW model on different datasets which proves the effect of spatio-temporal distribution for action classification without lying on content of  STIPs.  Finally,  a  multi-cue  representation  called
“TSP + SSP + BoVW” is evaluated. This united form out-
performs the state-of-the-arts proving the inherent comple-
mentary nature of these three methods. Experimental results on four challenging datasets show that salient motions are robustness against distracted motions and efficient to distin- guish similar actions. Future work focus on how to model geometric distribution of STIPs more accurately. As only STIPs are involved in current work, high level models and features like explicit models of human-object [4] and dense tracklets in Ref. [43] can be considered. Additionally, more real-time applications will be designed to apply our algorithm.

Acknowledgment
This work is supported by the National Natural Science Foundation of China (NSFC, nos. 61340046), the National High Technology Research and Development Programme of China (863 Programme, no. 2006AA04Z247), the Scientific and Technical Innovation Commission of Shenzhen Munici- pality (nos. JCYJ20130331144631730), and the Specialized Research Fund for the Doctoral Programme of Higher Edu- cation (SRFDP, no. 20130001110011).



References
A.A. Efros, A.C. Berg, G. Mori, Recognizing action at a distance, in: ICCV, 2003, pp. 726e733.
J. Aggarwal, M.S. Ryoo, ACM Comput. Surv. (CSUR) 43 (3) (2011) 16.
M. Blank, L. Gorelick, E. Shechtman, M. Irani, R. Basri, Actions as space-time shapes, in: ICCV, 2005, pp. 1395e1402.
A. Prest, V. Ferrari, C. Schmid, PAMI 35 (4) (2013) 835e848.
S. Ji, W. Xu, M. Yang, K. Yu, PAMI 35 (1) (2013) 221e231.
I. Laptev, IJCV 64 (2e3) (2005) 107e123.
H. Wang, M.M. Ullah, A. Klaser, I. Laptev, C. Schmid, Evaluation of local spatio-temporal features for action recognition, in: BMVC, 2009,
pp. 124.1e124.11.
H. Wang, A. Klaser, C. Schmid, C.L. Liu, IJCV 103 (1) (2013) 60e79.
O. Kliper-Gross, Y. Gurovich, T. Hassner, L. Wolf, Motion interchange patterns for action recognition in unconstrained videos, in: ECCV, 2012,
pp. 256e269.
M. Bregonzio, S. Gong, T. Xiang, Recognising action as clouds of space- time interest points, in: CVPR, 2009, pp. 1948e1955.
A.P. Ta, C. Wolf, G. Lavoue, A. Baskurt, J. Jolion, Pairwise features for human action recognition, in: ICPR, 2010, pp. 3224e3227.
W. Li, Z. Zhang, Z. Liu, Action recognition based on a bag of 3d points, in: CVPRW, 2010, pp. 9e14.
C. Yuan, X. Li, W. Hu, H. Ling, S. Maybank, 3D R transform on spatio- temporal interest points for action recognition, in: CVPR, 2013, pp. 724e730.
C. Schuldt, I. Laptev, B. Caputo, Recognizing human actions: a local svm approach, in: ICPR, 2004, pp. 32e36.
P. Doll´ar, V. Rabaud, G. Cottrell, S. Belongie, Behavior recognition via sparse spatio-temporal features, in: VS-PETS, 2005, pp. 65e72.
J.C. Niebles, H. Wang, L. Fei-Fei, IJCV 79 (3) (2008) 299e318.
L. Cao, Z. Liu, T.S. Huang, Cross-dataset action detection, in: CVPR, 2010, pp. 1998e2005.
G.J. Burghouts, K. Schutte, PRL 34 (15) (2013) 1861e1869.
P. Banerjee, R. Nevatia, Learning neighborhood cooccurrence statistics of sparse features for human activity recognition, in: AVSS, 2011, pp. 212e217.
S. Savarese, A. DelPozo, J.C. Niebles, L. Fei-Fei, Spatial-temporal correlatons for unsupervised action classification, in: WMVC, 2008, pp. 1e8.
P. Matikainen, M. Hebert, R. Sukthankar, Representing pairwise spatial and temporal relations for action recognition, in: ECCV, 2010, pp. 508e521.
M.S. Ryoo, J.K. Aggarwal, Spatio-temporal relationship match: video structure comparison for recognition of complex human activities, in: ICCV, 2009, pp. 1593e1600.
Q. Sun, H. Liu, Action disambiguation analysis using normalized google- like distance correlogram, in: ACCV, 2012, pp. 425e437.
H. Liu, M. Liu, Q. Sun, Learning directional co-occurrence for human action classification, in: ICASSP, 2014, pp. 1235e1239.
M. Liu, H. Liu, Q. Sun, Action classification by exploring directional co- occurrence of weighted STIPS, in: ICIP, 2014.
N. Dalal, B. Triggs, Histograms of oriented gradients for human detec- tion, in: CVPR, 2005, pp. 886e893.
I. Laptev, M. Marszalek, C. Schmid, B. Rozenfeld, Learning realistic human actions from movies, in: CVPR, IEEE, 2008, pp. 1e8.
K. Soomro, A.R. Zamir, M. Shah, Ucf101: A Dataset of 101 Human Actions Classes from Videos in the Wild, 2012 arXiv preprint arXiv: 1212.0402.
A. Gilbert, J. Illingworth, R. Bowden, Fast realistic multi-action recog- nition using mined dense spatio-temporal features, in: ICCV, 2009, pp. 925e931.
J. Liu, J. Luo, M. Shah, Recognizing realistic actions from videos in the wild, in: CVPR, 2009, pp. 1996e2003.
R. Messing, C. Pal, H. Kautz, Activity recognition using the velocity histories of tracked keypoints, in: ICCV, 2009, pp. 104e111.
P.F. Felzenszwalb, R.B. Girshick, D. McAllester, PAMI 32 (9) (2010) 1627e1645.
A. Vedaldi, A. Zisserman, PAMI 34 (3) (2012) 480e492.
H. Zhang, W. Zhou, C. Reardon, L.E. Parker, Simplex-based 3d spatio- temporal feature description for action recognition, in: CVPR, IEEE, 2014, pp. 2067e2074.
L. Shao, X. Zhen, D. Tao, X. Li, Cybern. IEEE Trans. 44 (6) (2014) 817e827.
O. Oshin, A. Gilbert, R. Bowden, CVIU 125 (2014) 155e171.
P. Banerjee, R. Nevatia, Pose filter based hidden-crf models for activity detection, in: ECCV, Springer, 2014, pp. 711e726.
V. Kantorov, I. Laptev, Efficient feature extraction, encoding, and clas- sification for action recognition, in: CVPR, IEEE, 2014, pp. 2593e2600.
T.-H. Yu, T.-K. Kim, R. Cipolla, Real-time action recognition by spatiotemporal semantic and structural forests, in: BMVC vol. 2, 2010, p. 6.
M.S. Ryoo, Human activity prediction: early recognition of ongoing activities from streaming videos, in: ICCV, 2011, pp. 1036e1043.
Q. Sun, H. Liu, Inferring ongoing human activities based on recurrent self-organizing map trajectory, in: BMVC, 2013, pp. 11.1e11.11.
K. Li, Y. Fu, PAMI 36 (8) (2014) 1644e1657.
P. Bilinski, E. Corvee, S. Bak, F. Bremond, Relative dense tracklets for human action recognition, in: FG, 2013, pp. 1e7.
