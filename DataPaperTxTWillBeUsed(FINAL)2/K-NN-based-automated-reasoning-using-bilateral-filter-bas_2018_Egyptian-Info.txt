Egyptian Informatics Journal 19 (2018) 133–144








Full length article
K-NN based automated reasoning using bilateral filter based texture descriptor for computing texture classification
Sonali Dash a, Manas Ranjan Senapati b,⇑, Uma Ranjan Jena a
a Department of Electronics and Tele-communication Engineering, Veer Surendra Sai University of Technology (VSSUT), Burla 768018, Odisha, India
b Department of Information Technology, Veer Surendra Sai University of Technology (VSSUT), Burla 768018, Odisha, India



a r t i c l e  i n f o 

Article history:
Received 23 March 2017
Revised 27 November 2017
Accepted 8 January 2018
Available online 10 January 2018

Keywords:
Bilateral filter
Laws mask descriptor Feature extraction Texture classification
a b s t r a c t 

Regions in the visual field can be characterized by differences in texture, brightness, colour, or other attributes. Bilateral filter is an efficient way to smooth any digital image while preserving the fine information. In bilateral filter, it has been observed that by selecting carefully, the bilateral filter range parameter and bilateral filter domain parameter the ability to smooth any arbitrary digital image while preserving the edges can be improved. This trait of bilateral filter helps to adapt it to application specific requirements. In this study, a new feature extraction method is recommended by integrating the conven- tional Laws’ mask method with bilateral filter, which results in the improvement of classification accuracy. The texture features are extracted by using different values of range parameter and domain parameter and are fed as input to k-Nearest Neighbor (k-NN) classifier for classification. The new fusion model is tested with Brodatz, VisTex, STex and ALOT databases. The results of the proposed method are also compared with the conventional Laws’ mask descriptor for all the aforementioned four datasets. The experimental results show that bilateral filter based Laws’ mask feature extraction technique provides better classification accuracy for all the four databases for various combinations of bilateral filter range and domain parameters.
© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

Texture is a property of image region. Texture analysis is one of the significant functions in pattern recognition and computer vision. In spite of its significance, texture analysis still is a great challenge. In 1970s the first work on texture analysis by Haralick is best known for co-occurrence matrix [1]. Random Markov Fields, a contemporary approach is used mainly for segmentation pur- poses [2]. Laws has recommended computing the energy of the image by using filters at the end of same decade [3]. Fractal geom- etry appeared as a powerful theory to obtain texture feature during

* Corresponding author.
E-mail addresses: sonali.isan@gmail.com (S. Dash), manassena@gmail.com (M.R. Senapati), urjena@rediffmail.com (U.R. Jena).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.


the 1980s [4,5]. Later contribution to texture feature extraction is made by LBP (Local Binary Patterns) [6]. All the methods proposed are without considering the complementary information that cannot be expressed by the intensities of pixels, but only through particular operations over those pixels. Some authors have made an effort to achieve this gap, and proposed the extraction of features from other domains, like wavelets [7], discrete cosine transform [8], Hough transform [9], and others.
Recently some hybrid models have also been developed by inte- grating the traditional texture descriptors to obtain better classifi- cation accuracy. Yadav et al. have proposed hybrid models of DWT (Discrete Wavelet Transform) with different LBP variants and named as Multi-resolution local binary pattern variants. They have obtained best classification accuracy of 97.87% for DWTCLBP u2 (325 features) at the third level of DWT decomposition for hard- wood species [10]. Yadav et al. have also proposed hybrid models of Gaussian pyramid (GP) with LBP, LCP (Local Configuration Pattern) and LPQ (Local Phase Quantization) for hardwood species database and texture surface database. For texture database (UIUC) they have achieved classification accuracy of 98% for GPLCPriu2 hybrid model with SVM as classifier [11]. Qi et al. have proposed globally rotation invariant multi-scale co-occurrence local binary


https://doi.org/10.1016/j.eij.2018.01.003
1110-8665/© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



pattern and investigated the proposed technique on Outex data- base for texture classification [12]. Zhe et al. have proposed a local binary pattern based texture descriptors for classification of tea leaves [13].
Filtering methods also outstretched interests in texture analysis due to their simulation of human vision. Various filtering methods like Gabor filter, spatial filter, etc. are used for texture analysis [14– 16]. Bilateral filter is a nonlinear filter developed in 1998 by Tomasi and Manduchi. The idea underlying bilateral filtering is to do in the range of an image what traditional filters do in its domain. Two pixels can be close to one another, that is, occupy nearby spatial location, or they can be similar to one another, that is, have nearby values, possibly in a perceptually meaningful fashion [17]. It has been observed that by selecting carefully, the bilateral filter range parameter and bilateral filter domain parameter the ability to smooth any arbitrary digital image while preserving the edges can be improved. Out of several applications, one application of the bilateral filter is texture and illumination separation. Based on a large-scale/small-scale decomposition of images, these appli- cations edit texture and manipulate the tonal distribution of an image to match the capacities of a given display. Wang et al. have used a bilateral filter decomposition in which high-dynamic-range image is generated from a single low-dynamic-range image [18]. They seek to reconstruct data in over and under exposed areas of the image. They have used bilateral filter to create decomposition into texture and illumination motivated by Oh et al.’s work [19]. This allows them to apply user-guided texture synthesis to the detail (texture) layer, after bilateral filtering removed the large- scale illumination variations. Lin et al. have proposed switching bilateral filter as texture/noise detector for universal noise removal [20]. Zhang and Allebach have proposed adaptive bilateral filter for sharpness enhancement and noise removal. They have mentioned that, because bilateral filter is a combination of both range filter and domain filter, it ensures that averaging is done mostly along the edge and is greatly reduced in the gradient direction. Due to this reason, the bilateral filter can smooth the noise while preserv- ing edge structures [21]. Cho et al. have proposed a novel structure of preserving image decomposition operator called bilateral tex- ture filter where they have successfully removed texture while preserving main image structures [22]. Hinnawi and Daear have investigated the performance of the bilateral filter based on his- togram moments and co-occurrence matrix for four low dose images of CT plastic phantom [23].
The energy filters are designed to enhance some textural properties of the images by Laws [3]. This method is based on the application of convolutions of the original image, Y, using different filters f1, f2.. . fN, therefore obtaining N new images Fn = Y ⁄ fn (n = 1,2,.. .N). However, in the medical image analysis Laws’ mask descriptor for texture feature extraction has received wide acceptance. Rachidi et al. have used Laws’ masks for bone texture analysis and found the TEME5R5 and TEME5W5 give very good result in terms of reproducibility [24]. Heba has used Laws’ masks statistical texture features for cancer and water lung detection [25]. Setiawan et al. have used Laws’ texture energy measure for mammogram classification. For Laws’ descriptor, they have found accuracy of 93.90% for normal-abnormal and 83.30% for benign-malignant classification [26]. Legesse et al. have utilized laws’ mask descriptor for automated detection of skin cancer and found classification results as 85% for sensitivity and 88% for specificity with all texture parameters [27]. In a comparative study for classification of Remote sensing data, Ruiz et al. have derived different classification accuracies for variety texture descriptors with different forest datasets. For combined descriptor GLCM (Gray Level Co-occurrence Matrix) + WV (Wave- Let) + Gabor + Energy they have achieved highest classification
accuracy of 88.41% on Urban dataset. By combining only GLCM and Energy (GLCM + Enegy) the maximum classification accuracy of 85.82% is obtained on Urban dataset. For only Energy filters the highest classification accuracy of 70.70% is achieved on FOR- EST 3 dataset [28]. Ertugrul has proposed adaptive texture energy measure. The classification accuracies of 0.08, 0.3292 and 0.3343 are obtained by TEM (Texture Energy Measure) on butterfly, flower seed and Brodatz datasets respectively. The adaptive TEM has delivered the correct classification rates of 0.0053, 0.2417 and 0.3153 respectively on the same three data- bases [29]. Sharma and Singh have shown maximum classifica- tion accuracy of 83.30% by using Laws’ mask method for Meastex database with k-NN as classifier [30]. The same authors Singh and Sharma have done another experiment with Meastex and VisTex benchmarks with different texture descriptors. They have shown that by utilizing Laws’ mask descriptor the classifi- cation accuracies for Meastex database are of 82.80% with linear classifier, 75.10% with k-NN classifier on original data and 69.30% with k-NN classifier on PCA (Principal Component Analy- sis) data. The classification accuracies for VisTex database are 68.80% with linear classifier, 56.10% with k-NN classifier on orig- inal data and 53.20% with k-NN classifier on PCA data [31]. Ojala et al. have compared a range of texture features extraction tech- niques by using nearest neighbour classifiers on Brodatz data- base. They have achieved best performance for the gray level difference method and poorest performance for Laws’ mask method [32]. Ojala et al. have done two different types of exper- iments with grain mixture images. Firstly, the performances of Laws’ texture energy measures are determined using k-NN clas- sifier based on feature vectors. Secondly, distribution based clas- sification experiments are performed. For Laws’ energy measure, classification accuracies are found as 51.70% with sample size
128 × 128, and 39.93% with sample size 256 × 256. For his-
togram equalization (EQ) classification accuracies of 61.93% and 52.84% are obtained with sample size 128 × 128 and 256 × 256 respectively [33]. Harwood et al. have done texture classification by center-symmetric auto-correlation by using Kullback discrim-
ination by distributions on Brodatz textures. They have also included experiments with 24 different Laws’ 5 × 5 masks in order to see the influence of size of the masks. They have achieved error rates of 25.9% and 39.20% for 64 × 64 and 32 × 32 samples, respectively, for the best performing mask S5S5 [34].
Detailed assessment shows that for texture classification by using only conventional Laws’ mask descriptor has not achieved attractive classification accuracy. Hence, with an objective to improve the performance of conventional Laws’ mask descriptor a new method is proposed by integrating bilateral filter with Laws’ mask descriptor. In bilateral filter selection of range parameter (rr)
and domain parameter (rd) is very important. If their values are
set too high, the filter will act as a smoothing filter. If their values are set too low noise cannot be removed. So very carefully, we have selected various combinations of bilateral filter range parameter (rr) and bilateral domain parameter (rd) and applied to the tex- ture images. The generated filtered images for different values of rr and rd are convolved with Laws’ mask descriptor. Once more, the convolved images are passed through three types of energy measurement filters for extracting the texture features. Further, k-NN classifier is employed to examine the effectiveness of the proposed technique.
The arrangement of the paper is as follows: The comprehensive review of bilateral filter and Law’s mask descriptor is presented in Section 2. Section 3 discusses in detail the texture features extraction, classification, and validation. Section 4 conveys critical discussion of results of the proposed technique and comparison with conventional method. Section 5 concludes the work.



Review of bilateral filter and laws’ mask descriptor

Bilateral filter

The coefficient weights of bilateral filter are determined accord- ing to both the variation in pixels intensities and the location of pixels in a certain neighbourhood. Each pixel is replaced by a weighted average of the intensities in the window. The weighting function gives high weighting to those pixels that are both near the central pixel and similar to the central pixel. Let xi;j be the current
pixel, and let xi+s;j+t be the pixels in a (2N + 1)× (2N + 1) window
that surrounds xi;j; (i; j) and (i + s; j + t) are the location of xi;j and
xi+s;j+t. The output of bilateral filter yi;j is defined as follows [20]:
Ripple → R5 = (1; —4; 6; —4; 1)= S3 * S3
Edge → E5 = (—1; —2; 0; 2; 1)= L3 * E3
where ⁄ represents the convolution operation. The level vector L demonstrates the center weighted local average, edge vector E resembles the gradient operator and responds to the column and row stepped edges in an image, spot vector S represents the spot extraction, Ripple vector R detects ripple from the image and wave vector W responds to any pixel changes in image. In the similar way, the one-dimensional convolution masks of length five are con- volved in turn with transposes of each other to give a set of 25 dif- ferent two-dimensional convolution masks. As an example, the L5E5 mask is obtained by convolving a vertical L5 mask with hori-

PN	PN
WG(s; t)WR(s; t)xi s j t
zontal E5 mask. Among all the 25 two-dimensional convolution

i;j


where
N s=—N
N t=—N
WG(s; t)WR(s; t)
masks are of zero sums. All the 25 numbers of 5 × 5 two- dimensional masks are given below.


WG(s; t)= exp —
(i — s)2
+ (j — t)2

(2)
L5L5; E5L5; S5L5; W5L5; R5L5;



and  WR(s; t)= exp —
2r2

(xi;j — xi s;j t )2
2r2


(3)
L5E5; E5E5; S5E5; W5E5; R5E5; L5S5; E5S5; S5S5; W5S5; R5S5

The Gaussian filter and range filter are defined in (2) and (3), respectively. In the Gaussian filter, the Euclidean distance between xi;j and xi+s;j+t is calculated and the difference of luminance is com-
puted in the range filter. Hence, the bilateral filter has three main parameters. These are the bilateral filter window size, the bilateral filter range parameter and the bilateral filter domain parameter.

Laws’ mask descriptor

The Laws’ masks method of feature extraction was given by Kenneth Ivan Laws in 1980, where the main idea behind it is the filtering of images with specific masks produced from the combi- nation of one-dimensional Kernel vector in order to assess the tex- ture properties. The Laws’ masks match the pixel neighbourhood to the set of standard masks to compute the texture properties of images. Laws’ properties, which he called ‘‘texture energy mea- sures”, are extracted from three simple vectors. The origin of Laws’ filters of three vectors is as given below:
L3 = [1; 2; 1];  E3 = [—1; 0; 1];  and  S3 = [—1; 2; —1]
where L represents the one dimensional operations of center- weighted local averaging, E represents symmetric first differencing, and S represents second differencing. One-dimensional 1 × 3 masks
are convolved in turn with transposes of each other to generate 9 different two-dimensional 3 × 3 masks. As an example, The L3S3 mask is obtained by convolving vertical L3 mask with horizontal S3 mask. Correspondingly all the 9 numbers of 3 × 3 masks obtained are listed below.
L3L3; L3E3; L3S3 E3L3;  E3E3;  E3S3
L5W5; E5W5; S5W5; W5W5; R5W5; L5R5; E5R5; S5R5; W5R5; R5R5.
Proposed methodology

The principal motive of this work is to improve the Laws’ mask descriptor performance by using bilateral filter for texture classifi- cation. This section outlines the feature extraction technique, clas- sification, and validation as follows. The procedural steps are shown in block diagram Fig. 1.

Feature extraction

The feature extraction steps are summarized as follows:

Step1: It starts with creating the database by splitting the orig- inal texture images into sub images.
Step 2: This step involves in generation of bilateral filtered images. Bilateral filter formulation is simple: each pixel is replaced by its average of neighbors. It depends only on two parameters bilateral filter domain parameter (rd) and bilateral filter range parameter
(rr) that preserves the features. Window size of bilateral
filter is an additional parameter. Considering these three aspects of bilateral filter, the images are generated for a fixed window size of 5 × 5 with various combinations of rd and rr.
Step 3: The resulting different filtered images are convolved with Laws’ mask descriptor. Then texture features are extracted using Laws’ mask technique. The convolved outputs are passed through three energy measurement filters [25], which are described as follows:


S3L3;  S3E3;  S3S3
Mean = PNNeighbouringpixels
(4)

Convolving one dimensional vectors in turn transposes with them- selves or each other generates 5 × 1 vectors with a mnemonics Level, Edge, Spot, Wave, and Ripple as follows.
Level → L5 = (1; 4; 6; 4; 1)= L3 * L3
Absolute mean = PNabs(Neighbouringpixels)
sPﬃﬃﬃﬃﬃﬃﬃﬃ(ﬃﬃNﬃﬃﬃeﬃﬃﬃiﬃgﬃﬃhﬃﬃﬃbﬃﬃﬃoﬃﬃuﬃﬃﬃrﬃﬃiﬃnﬃﬃﬃgﬃﬃpﬃﬃﬃiﬃxﬃﬃﬃeﬃﬃlﬃsﬃﬃﬃ—ﬃﬃﬃﬃﬃmﬃﬃﬃﬃeﬃﬃﬃaﬃﬃnﬃﬃﬃﬃﬃ2ﬃﬃ

(5)


Spot → S5 = (—1; 0; 2; 0; —1)= —E3 * E3 = L3 * S3
Standard deviation =
  N	) 
N
(6)

where N represents the window size.




Fig. 1. Block diagram of the proposed method.


Step 4: Texture features are normalized using max min normal- ization method because a wide range of numerical values are present in the texture features. For each normalized feature vector three most important statistics are evaluated. Those are absolute mean (ABSM), mean square or energy (MS) and entropy, which are represented as textural parameters.
Classification

The features extracted using the proposed method are then classified. This paper implements k-NN classifier in the task of clas- sification. Even though, it is simple but effective in the texture clas- sification. The k-NN is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. The k-NN rule classifies x by assigning it the label most frequently rep- resented among the k nearest samples; this means that, a decision is made by examining the labels on the k-nearest neighbors and taking a vote. Then assign it to the class to which the majority of these k-nearest neighbors belong. The data are represented in a vector space. In summary, for a given set of x with n points and with given distance function, k-nearest neighbor search to find
the k closest points in x to a query point or set of points y. Classi- fication is done by comparing feature vectors of the different points. Learning in this algorithm consists of storing the presented training data. When a new query instance is encountered, a set of similar related instances is retrieved from memory and used to classify the new query instance. Therefore, it is also named as memory-based reasoning or case-based reasoning. Selection of parameter k is an important task. If k = 1, select the nearest neigh- bour and if k > 1, select the most frequent neighbour for classifica- tion. The value of k must be chosen that error rate is minimum. In this experiment the value is chosen as k = 5. There are many dis- tance measures such as Euclidean distance, City block, and Cosine distance. We have used Euclidean distance as distance metric.

Experiments and validation

With the following experiments, we validate the theoretical basis and demonstrate the efficiency of the proposed technique. In this section, two experiments are carried out for each database to demonstrate our proposed feature extraction method for texture




Fig. 2. 24 classes of Brodatz.


Fig. 3. 25 classes of VisTex.


classification. In our experiments, we have used four benchmark databases Brodatz, VisTex, STex and ALOT to validate the efficiency of the proposed method. Initially all the original databases are divided into sub images with different sample size to produce train and test sets. Experiments are conducted for the extraction of tex- ture features utilizing the training and testing sets individually. Subsequently, the classification is performed based on train and test validation by following the k-NN rule for each dataset. The classification results of the proposed method are compared with the traditional Laws’ mask method. The results are compared in terms of correctness rates, which is the ratio (percentage) of images whose classes are correctly predicted by the classifier.
In the first experiment, we use all the five masks Level (L), Edge (E), Spot (S), Ripple (R) and Wave (W) by which twenty- five different masks are created by using these five masks. Both training and testing images are convolved with twenty-five num- bers of masks separately. The convolved outputs are passed through three energy measurement filters known as Mean, Abso-
lute mean and Standard deviation of window size 15×15, which
are normalized by min–max normalization method and then the statistical features Absolute mean, mean square or energy and entropy are calculated, which are used as texture feature parameters. For twenty-five number of masks 75 nos of features are extracted from each image.
Here, the second experiment is our proposed technique of inte- grating of Laws’ mask descriptor with bilateral filter with different combinations of filter domain and filter range parameters. The bilateral filter is applied on the texture images for each dataset. The filter domain parameters (rd) are altered with the following values: 0.5, 1, 2, 3, 4, 5, 6, 7 and 8. Throughout the experiment, the values of filter range parameters (rr) are changed with following values: 0.05, 0.1, 0.2, 0.3, 0.4, 0.5 and 1. For a certain value of rr with different values of rd as mentioned above are applied to texture images. The window size of bilateral filter is set as 5 × 5. The generated different bilateral filtered images are then convolved with Laws’ mask descriptor for 25 numbers of
masks. Further, these convolved outputs are proceeded through three types of energy measurement filter named as Mean, Absolute mean and Standard deviation filter. After normalization three tex- ture parameters entropy, absolute mean and mean square are extracted.

Results discussion and comparison

The generation of the datasets and the classification results obtained through the above-mentioned experiments on each data- set are discussed separately as below.
Results of Brodatz database

The Brodatz database, in which the images suffer a very high signal to noise ratio and formed under different imaging conditions [35]. Here 24 texture images are selected randomly from Brodatz database [36]. The database used in this experiment is shown in
Fig. 2. The dataset is created from the original images, which are of size 640 × 640, by subdividing each into non-overlapping sam- ples of size 128 × 128. Thus from 600 (=24 ⁄ 25) resulting sub images, we have taken 312 sub images for testing and 288 for
training.
The traditional Laws’ mask descriptor gives classification accu- racy of 95.83% for mean filter, 91.35% for absolute mean filter and 91.60% for standard deviation filter. For integrated model among all the results, it is observed that the highest accuracy of 97.12% is achieved by keeping rr value at 1 and rd value at 0.5 for the mean energy filter. The second best accuracy of 96.79% is obtained
from two combinations one is with rr value at 0.2 and rd value at
1 and second is with rr value at 0.4 and rd value at 1 for the mean energy filter. The third best classification accuracy of 96.47% is achieved at three different values of rang and domain parameters. First is with rr = 0.2 and rd = 0.5, second is rr = 0.3 and rd = 1 and
the third is with rr = 0.4 and rd = 0.5. Furthermore, the proposed
techniques also have delivered higher classification rates of 96.15% at some other values of rr and rd. The results discussed above are only for the mean energy filter.
For this database, also it has been observed that classification accuracies are increased for absolute mean filter and standard deviation filter. For absolute mean filter, the highest classification accuracy of 94.87% is obtained with rr = 0.4 and rd = 0.5. The sec- ond best accuracy of 93.59% is obtained for the absolute mean filter with combination of rr = 0.1 and rd = 0.5. For many other combi-
nations of rr and rd better classification accuracies are achieved
for this energy filter which can be observed from the tables. For standard deviation filter first best classification accuracy of 92.63% is obtained with combination of rr = 0.05 and rd = 2 and
second best accuracy of 92.53% is obtained combination of rr =
0.4, 0.5 and rd = 1. For standard deviation filter better classifica- tion accuracies are also achieved at some other values of rr and rd.

Results of VisTex database

The Vision Texture (VisTex) dataset is prepared by the Mas- sachusetts Institute of Technology (MIT). VisTex database contains color texture images [37]. The challenges of VisTex database are
different view points and illumination orientations. The second dataset consists of 25 texture images of size 512 × 512 as shown




Fig. 4. 25 classes of STex.



Fig. 5. 10 classes of ALOT.



Table 1
Classification results for conventional Laws’ masks descriptor (5 × 5).

25 number of masks


VisTex dataset
25 number of masks

75	59.00	52.50	55.00


STex dataset
25 number of masks

75	72.50	67.00	63.50


ALOT dataset
25 number of masks

75	94.00	92.50	92.00





in Fig. 3 from VisTex database. For second database each image is divided into sixteen 128 × 128 non overlapping samples. Thus there are total 400 samples are available out of which 200 samples
are used for training and 200 samples are used for testing. VisTex dataset is converted to grayscale before feature extraction.
For VisTex dataset the traditonal Laws’ mask descriptor gives classification accuracy of 59.00% for mean filter, 52.50% for abso- lute mean filter and 55.00% for standard deviation filter. From the results it has been observed that for the integrated model higher classification accuracies are only obtained for the absolute mean filter. The best classification accuracy of 57.00% is achieved from absolute mean filter with combination of rr = 1 and rd = 7. The second best accuracy of 56.00% is obtained with combination
of rr = 1 and rd = 0.5. The third best accuracy of 54.50% is obtained with combination of rr = 1 and rd = 8. Also better accuracy of 53.50% is obtained with a combination of rr = 0.1 and rd = 0.5,
which is higher than the accuracy of traditional Laws’ mask descriptor for the absolute mean filter.

Results of STex database

The Salzburg Texture Image Database (STex) is a large collection of 476 numbers of color texture image that have been captured around Salzburg, Austria [38]. The images are used in texture retrieval experiments and are homogeneous. The web site for this database did not describe the acquisition conditions. Inside the STex dataset, the great majority of texture images are stationary and have more chromatic richness than VisTex. The third dataset
consists of 25 texture images of size 512 × 512 as shown in
Fig. 4 from STex database. For this database each image is divided into sixteen 128 × 128 non overlapping samples. Thus there are total 400 samples are available out of which 200 samples are used
for training and 200 samples are used for testing. STex dataset is converted to grayscale before feature extraction.
The traditonal Laws’ mask descriptor gives classification accu- racy of 72.50% for mean filter, 67.00% for absolute mean filter and 63.50% for standard deviation filter for this dataset. For STex database our proposed model delivers best classification accuracies for both mean filter and standard deviation filter. The best classifi- cation accuracy of 74.00% is achieved from mean filter with combi- nation of rr = 0.1 and rd = 1. The second best accuracy of 73.50% is
obtained with combination of rr = 0.05 and rd = 3 for the mean fil-
ter. The third best accuracy for the mean filter is of 73.00% is obtained with combination of rr = 0.05 and rd = 1. For standard deviation filter, our proposed model gives much higher classifica- tion accuracies than the traditional Laws’ mask standard deviation filter. The highest classification accuracy for standard deviation fil- ter is of 66.50% and is obtained with combination of rr = 0.05 and
rd = 2. The second highest classification accuracy of 66.00% is
obtained for the standard deviation filter with combination of rr
= 0.4 and rd = 0.5. The third highest classification accuracy for standard deviation filter is of 65.50% and is obtained with combi- nation of rr = 0.05 and rd = 0.5. For some other combinations of rr and rd higher classification accuracies are also achieved for the standard deviation filter.

Results of ALOT database

ALOT is an impressive colour image collection of 250 distinct rough textures, acquired by 4 different colour camera (c = 1,.. ., 4) [39]. For each image and camera, six illuminations (I = (1, 2, 3, 4,
5, 8)) and 4 rotations (r = 00, 600, 1200, 1800) are presented. In the database, images are available in three sizes, full resolution (1536 × 1024 pixel), half resolution (768 × 512 pixel) and quarter
resolution (384 × 256 pixel). In our fourth database as shown in
Fig. 5, we have selected 10 classes from ALOT database of size 384 × 256. Under each class, there are 20 images of different angle, rotation and illumination. Each image is divided into six 128 × 128



100
90
Table 3
Performance evaluation of Laws’ mask descriptor by means of bilateral filter with rr
= 0.1.

80
70

Different databases

No. of features for each image

Classification accuracy (%)


Mean  Absolute mean  Standard deviation

60
50
40
30
20
10
0
Brodatz	VisTex	STex	ALOT
conventional Laws mask descriptor

Fig. 6. Classification rates of traditional Laws’ mask.


Brodatz dataset
rd = 0.5	75	95.19  93.59	92.63
rd = 1	75	95.51  88.46	91.35
rd = 2	75	95.51  91.03	91.35
rd = 3	75	96.15  89.10	91.35
rd = 4	75	95.51  91.35	92.31
rd = 5	75	95.51  91.03	92.31
rd = 6	75	95.51  88.78	91.03
rd = 7	75	96.15  90.71	91.35
rd = 8	75	95.19  90.38	92.31
VisTex dataset


ALOT dataset
rd = 0.5	75	90.17	89.00	87.50























rd = 1	75	89.50	90.00	90.00
rd = 2	75	90.17	88.67	89.17
rd = 3	75	91.33  86.83	88.50
rd = 4	75	91.33  90.33	89.67
rd = 5	75	90.83	88.50	89.50
rd = 6	75	89.33  88.50	90.33
rd = 7	75	90.83	86.83	89.33
rd = 8	75	90.33	89.33	90.33
non overlapping samples. Thus there are total 1200 samples are available out of which 600 samples are used for training and 600 samples are used for testing.
For ALOT database the traditonal Laws’ mask descriptor gives classification accuracy of 94.00% for mean filter, 92.50% for
absolute mean filter and 92.00% for standard deviation filter. For this database our proposed model provides better classification accuracy for the mean filter only. The best classification accuracy of 95.33% is achieved from a combination of rr = 0.1 and rd = 5 for mean filter. The second best accuracy of 94.83% is obtained with combination of rr = 0.1 and rd = 3 for the mean filter.

Summary of the results for all the four databases

Hence, the proposed technique has achieved better classifica- tion accuracy with Brodatz database for all the three energy mea- surement filters for various combinations of bilateral filter domain and bilateral filter range parameters. Among the three energy mea- surement filters the absolute mean filter has achieved 94.87% that is 3% more than the traditional Laws’ mask descriptor. For the mean energy filter, the highest classification accuracy of 97.12% is achieved that is 1.29% more than the traditional Laws’ mask descriptor. For the standard deviation filter, the highest classifica- tion accuracy of 92.63% is achieved that is 1% more than traditional Laws’ mask descriptor.
For VisTex database, the proposed technique has achieved bet- ter classification accuracy only for absolute mean filter for various combinations of bilateral filter domain and bilateral filter range. Among the three energy measurement filters, only the absolute



Table 4
Performance evaluation of Laws’ mask descriptor by means of bilateral filter with rr
= 0.2.
Table 5
Performance evaluation of Laws’ mask descriptor by means of bilateral filter with rr
= 0.3.


Different

No. of features for

Classification accuracy (%)

Different

No. of features for

Classification accuracy (%)




mean filter has achieved highest classification rates of 57.00% that is 4.5% more than the traditional Laws’ mask descriptor.
For STex database, the proposed technique has achieved better classification accuracy for mean and standard deviation filter for various combinations of bilateral filter domain and bilateral filter range. Among the three energy measurement filters the standard deviation filter has achieved 66.50% that is 3% more than the tradi- tional Laws’ mask descriptor. For the mean energy filter the best classification accuracy of 74% is obtained that is 1.5% more than traditional Laws’ mask descriptor.
For ALOT database, the proposed technique has achieved better classification accuracy only for mean filter for various combina- tions of bilateral filter domain and bilateral filter range. Among the three energy measurement filters, only the mean filter has achieved classification accuracy of 95.33% that is 1.33% more than the traditional Laws’ mask descriptor.
The classification results for the traditional Laws’ mask method for all the four databases are presented in Table 1 and the corre- sponding graphical presentation is shown in Fig. 6. The classifica- tion results of the proposed technique are presented from Tables 2–8. Figs. 7–13 plot the curves of classification success rates of the proposed method on Brodatz, STex, and VisTex databases. To avoid overlapping with the results of Brodatz database the plots of the ALOT database are presented separately from Figs. 14–20.


Conclusion

Improvement in the texture classification using Laws’ mask descriptor by means of bilateral filter with k-NN reasoning is pro- posed in this paper. The texture features are obtained by using var- ious combinations of bilateral filter range and domain parameters integrated with conventional Laws’ mask descriptor. The proposed fusion model of bilateral filter with Laws’ mask technique exhibits encouraging results compared to the traditional Laws’ mask descriptor for all the four databases. Our proposed method has achieved best classification accuracies of 97.12% for mean filter, 94.87% for absolute mean filter and 92.63% for standard deviation filter by utilizing Brodatz database. The proposed method has achieved classification accuracy of 57.00% for absolute mean filter by utilizing VisTex database. For STex database, classification accu- racy of 74.00% for mean filter and 66.50% for standard deviation fil- ter and 95.33% for mean filter for ALOT database are achieved by our proposed feature extraction technique. The results in this paper support the potential of integrating the bilateral filter with Laws’ mask for texture classification. By observing the results, it indicates that the best setup to use bilateral filter is to set a small value of rr and different rd to obtain better classification.



Table 6
Performance evaluation of Laws’ mask descriptor by means of bilateral filter with rr

Table 7 (continued)




= 0.4.

Different databases


No. of features for each image


Classification accuracy (%)


Mean  Absolute mean  Standard deviation
Different databases

STex dataset
No. of features for each image
Classification accuracy (%)


Mean  Absolute mean  Standard deviation


Brodatz dataset
rd = 0.5	75	96.47  94.87	92.31
rd = 1	75	96.79  89.74	92.95
rd = 2	75	95.83  89.10	91.03
rd = 3	75	93.91  87.82	91.35
rd = 4	75	94.55  88.14	88.14
rd = 5	75	95.51  89.74	89.10
rd = 6	75	95.83  90.38	90.38
rd = 7	75	96.15  88.78	90.06
rd = 8	75	95.83  88.78	90.38
VisTex dataset
rd = 0.5	75	49.50	52.00	54.00
rd = 1	75	52.00	47.50	42.50
rd = 2	75	50.00	51.50	43.00
rd = 3	75	51.00	49.00	49.50
rd = 4	75	53.50	48.00	47.00
rd = 5	75	49.00	47.50	45.00
rd = 6	75	50.00	46.50	47.00
rd = 7	75	51.00	47.00	46.00
rd = 8	75	51.00	45.50	44.50
STex dataset
rd = 0.5	75	66.00	63.50	66.00
rd = 1	75	68.00	63.00	54.50
rd = 2	75	65.50	60.00	55.00
rd = 3	75	66.50	59.00	51.50
rd = 0.5	75	70.00	65.00	62.00
rd = 1	75	69.00	61.00	55.50
rd = 2	75	66.50  62.50	46.00
rd = 3	75	66.50  58.50	47.00
rd = 4	75	65.50  58.50	50.50
rd = 5	75	67.00	60.00	50.50
rd = 6	75	66.00	57.50	52.00
rd = 7	75	64.00	61.00	52.50
rd = 8	75	63.50  60.50	48.50
ALOT dataset
rd = 0.5	75	88.00	90.67	88.00
rd = 1	75	91.50  88.17	88.50
rd = 2	75	92.00	88.33	87.00
rd = 3	75	88.33  87.67	84.00
rd = 4	75	89.17  86.17	83.67
rd = 5	75	88.17  86.17	82.00
rd = 6	75	88.50  84.00	82.17
rd = 7	75	88.83  86.50	81.50
rd = 8	75	90.00	85.83	83.00
Table 8
Performance evaluation of Laws’ mask descriptor by means of bilateral filter with rr
= 1.

Different
No. of features for
Classification accuracy (%)

databases
Brodatz dataset
each image
Mean  Absolute
mean
Standard deviation

rd = 0.5	75	97.12  92.95	92.31
rd = 1	75	94.55  91.03	90.71
rd = 2	75	93.59  91.03	89.42
rd = 3	75	93.59  88.78	89.10
rd = 4	75	94.23  89.10	89.42
rd = 5	75	95.19  89.10	87.82
rd = 6	75	94.55  91.35	89.42
rd = 7	75	94.55  91.03	88.46
rd = 8	75	94.87  91.35	88.78
VisTex dataset









Different








No. of features








Classification accuracy (%)







rd = 6	75	56.00  50.50	49.00
rd = 7	75	55.00  57.00	49.00






100
Range parameter=.05 with different domain parameters

100
Range parameter=0.3 with different domain parameters



90


80


70


60


50



40 0	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz

Fig. 7. Successful classification rates at rr = 0.05.
40
0	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz

Fig. 10. Successful classification rates at rr = 0.3.




100
Range parameter=0.1 with different domain parameters


100

Range parameter=0.4 with different domain parameters



Brodatz
90



80



70


STex

60


50



40
0	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz

Fig. 8. Successful classification rates at rr = 0.1.



VisTex



40 0	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz

Fig. 11. Successful classification rates at rr = 0.4.




100



90



80
Range parameter=0.2 with different domain parameters


100

Range parameter=0.5 with different domain parameters




70



60



50


40
0	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz
400	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz


Fig. 9. Successful classification rates at rr = 0.2.	Fig. 12. Successful classification rates at rr = 0.5.




100


90


80


70


60


50
Range parameter=1 with different domain parameters







Brodatz














STex





VisTex

100

98

96

94

92

90

88

86

84
Range parameter=0.2 with different domain parameters


400	1	2	3	4	5	6	7	8
bilateral Laws masks on VisTex,STex ,Br odatz

Fig. 13. Successful classification rates at rr = 1.


Range parameter=.05 with different domain parameters

82

800	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

Fig. 16. Success rates on ALOT at rr = 0.2.

100

98

96

94

92

90

88

86

84

82

80








100

98

96

94

92

90


0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

Fig. 14. Success rates on ALOT at rr = 0.05.


Range parameter=0.1 with different domain parameters


100

98

96

94

92

90

88

86

84

82

80








100

98

96

94

92

90

Range parameter=0.3 with different domain parameters

0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

Fig. 17. Success rates on ALOT at rr = 0.3.

Range parameter=0.4 with different domain parameters


88	88

86	86

84	84


82

80
0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

82

80
0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT


Fig. 15. Success rates on ALOT at rr = 0.1.	Fig. 18. Success rates on ALOT at rr = 0.4.





100

98

96

94

92

90

88

86

84

82

80
Range parameter=0.5 with different domain parameters


0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

Fig. 19. Success rates on ALOT at rr = 0.5.



Range parameter=1 with different domain parameters
Wang YM, Feng HR, Zhou YF. Machined surfaces texture analysis based on hough transform and run length statistics. Appl Mech Materi 2012;217– 219:1292–6.
Yadav AR, Anand RS, Dewal MI, Gupta S. Multiresolution local binary pattern variants based texture feature extraction technique for efficient classification of microscopic images of hard wood species. App Soft Comp 2015;32:101–12.
Yadav AR, Anand RS, Dewal MI, Gupta S. Gaussian image pyramid based texture features for classification of microscopic images of hardwood species. Optik 2015;126:5570–8.
Qi X, Shen L, Zhao G, Li Q, Pietikainen M. Globally rotation invariant multi- scale co-occurrence local binary pattern. Img and Vis Comp 2015;43:16–26.
Tang Z, Su Y, Joo Er M, Qi F, Li Z, Zhou J. A local binary pattern based texture descriptors for classification of tea leaves. Neurocomputing 2015;168:1011–23.
Randen T, Husy JH. Filtering for texture classification: a comparative study. IEEE Trans Patt Ana Mach Int 1999;21(4):291–310.
Jain AK, Farrokhnia F. Unsupervised texture segmentation using Gabor filters. Patt Recog 1991;24(12):1167–86.
Coggins JM, Jain AK. A spatial filtering approach to texture analysis. Patt Recog Lett 1985;3(3):195–203.
Tomasi C, Manduchi R. Bilateral filtering for gray and color images. In: Proceedings of the IEEE Int Conf on Computer Vision; 1998. p. 839–46.
Wang L, Wei L-Y, Zhou K, Guo B, Shum H-Y. High dynamic range image hallucination. In: Proceedings of the 18th Eurographics conf on rendering techniques; 2007. p. 321–326.
Oh BM, Chen M, Drosey J, Durand F. Image-based modelling and photo editing. In: Proceedings of the 28th Annual Conf on Computer graphics and interactive techniques; 2001. p. 433–42.
Lin C-H, Tsai J-S, Chiu C-T. Switching bilateral filter with a texture/noise detector for universal noise removal. IEEE Trans Img Procs 2010;19 (9):2307–20.
Zhang B, Allebach JP. Adaptive bilateral filter for sharpness enhancement and

100

98

96

94

92

90

88

86

84

82

80
0	1	2	3	4	5	6	7	8
bilateral Laws masks on ALOT

Fig. 20. Success rates on ALOT at rr = 1.


References

Haralick RM. Statistical and structural approaches to texture. Proc IEEE 1979;67(5):786–804.
Derin H, Elliot H. Modelling and segmentation of noisy and textured images using Gibbs random fields. IEEE Trans Pat Ana Mach Int 1987;9(1):39–55.
Laws KI. Texture energy measures. In: Image Understanding Workshop; 1979.
Mandelbrot BB. The fractal geometry of nature. New York: Freeman; 1968.
Pentland A. Fractal-based description of natural scenes. IEEE Trans Pat Ana Mach Int 1984;6(6):661–74.
Pietikainen M, Hadid A, Zhao G, Ahonen T. Computer vision using local binary patterns, vol. 40. Springer; 2011.
Roux SG, Clausel M, Vedel B, Jaffard S, Abry P. Self similar anisotropic texture analysis: the hyperbolic wavelet transform contribution. IEEE Trans Im Procs 2013;22(11):4353–63.
Rebhi A, Abid S. Texture defect detection with combined local homogeneity analysis and discrete cosine transform.In: Int Conf on Elec Engg and Soft App (ICEESA); 2013. p. 13–7.
noise removal. IEEE Trans Img Procs 2008;17(5):664–78.
Cho H, Lee H, Kang H, Lee S. Bilateral texture filtering. ACM Trans Grap 2014;33(4):1–8.
AL-Hinnawi AR, Daear M. Image texture descriptors to quantify bilateral filter on low dose computerized tomography. Int J Sig Pros Img Procs Patt Recog 2012;5(3):123–36.
Rachidi M, Marchadier A, Gadois C, Lespessailles E, Chappard C, Benhamou CL. Laws’ Masks descriptors applied to bone texture analysis: an innovative and discriminant tool in osteoporosis. Skeletal Radio 2008;37(6):541–8.
Heba AE. Statistical analysis of law’s masks texture features for cancer and water lung detection. Int J Comp Sci Issues 2013;10(6):196–202.
Setiawan AS, Elysia Wesley J, Purnama Y. Mammogram classification using law’s energy measure and neural network. Procedia Comput Sci 2015;59:92–7.
Legesse FB, Medyukhina A, Heuke S, Popp J. Texture analysis and classification in coherent anti-strokes Raman scattering (CARS) microscopy images for automated detection of skin cancer. Comp Med Img Grap 2015;43:36–43.
Ruiz LA, Fdez-Sarria A, Recio JA. Texture feature extraction for classification of remote sensing data using wavelet decomposition: a comparative study. In: 20th ISPRS Congress 2004;35(Part B):1109–1114.
Ertugrul OF. Adaptive texture energy measure method. Int J Intell Inform Syst 2014;3(2):13–8.
Sharma M, Singh S. Evaluation of texture methods for image analysis. In: Int Inf Syst Conf, The Seventh Australian and New Zealand; 2001. p. 117–21.
Singh S, Sharma M. Texture analysis experiments with meastex and vistex benchmarks. In: Advances in Pattern Recognition (ICAPR). Springer; 2001. p. 419–26.
Ojala T, Pietikainen M, Harwood D. A comparative study of texture measures with classification based on featured distributions. Patt Recog 1996;29 (l):51–9.
Ojala T, Pietikainen M, Nisula J. Determining composition of grain mixtures by texture classification based on feature distributions. Int J Patt Recog Artif Int 1996;10(1):73–82.
Harwood D, Ojala T, Pietikanen M, Kelman S, Davis L. Classification by centre- symmetric auto-correlation using Kullback discrimination of distributions. Patt Recog Lett 1995;16(1):1–10.
Bigun J. Frequency and orientation sensitive texture measures using linear symmetry. Sig Procs 1992;29(1):1–16.
Brodatz, <http://www.ux.uis.no/tranden/brodatz.html>.
VisTex, Texture dataset. <http://vismod.media.mit.edu/vismod/imagery/ VisionTexture/vistex.html>.
Department of Computer Sciences, U.S., Salzburg texture image database STex
<http://www.wavelab.at/sources/STex>.
Amsterdam library of textures (ALOT) <http://www.science.uvanl/~mark/ ALOT>.
