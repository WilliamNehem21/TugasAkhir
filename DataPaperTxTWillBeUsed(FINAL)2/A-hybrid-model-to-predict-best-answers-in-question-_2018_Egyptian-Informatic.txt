Egyptian Informatics Journal 19 (2018) 21–31








Full length article
A hybrid model to predict best answers in question answering communities
Dalia Elalfy ⇑, Walaa Gad, Rasha Ismail
Information Systems Department, Faculty of Computer & Information Sciences, Ain Shams University, Abbasia, Cairo, Egypt



a r t i c l e  i n f o 

Article history:
Received 22 September 2016
Revised 13 March 2017
Accepted 22 June 2017
Available online 29 June 2017

Keywords:
Question answering communities Knowledge exchange
Expert
Best answer Feature extraction Content feature
Non-content feature Hybrid classifier
a b s t r a c t 

Question answering communities (QAC) are nowadays becoming widely used due to the huge facilities and flow of information that it provides. These communities target is to share and exchange knowledge between users. Through asking and answering questions under large number of categories.
Unfortunately there are a lot of issues existing that made knowledge process difficult. One of those issues is that not every asker has the knowledge and ability to select the best answer for his question, or even selecting the best answer based on subjective matters. Our analysis in this paper is conducted on stack overflow community. We proposed a hybrid model for predicting the best answer. The proposed model is consisting of two modules. The first module is the content feature which consists of three types of features; question-answer features, answer content features, and answer-answer features. In the sec- ond module we examine the use of non-content feature in predicting best answers by using novel rep- utation score function. Then we merge both of content and non-content features and use them in prediction. We conducted experiments to train three different classifiers using our new added features. The prediction accuracy is very promising.
© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

There are many types of social networks that you can use through internet. The types are according to the functionality each network can provides [1]. One of these social networks is collabo- rative social network. Stack Overflow (http://stackoverflow.com), Quora, Yahoo!-answers and Never are examples of this type. In stack Overflow community the question answering process is con- ducting as follows. The user can choose a category to post a ques- tion to. After posting the question the asker waits specific amount of time in order to receive the answers from the expert users. Expert users are the users whose have a great knowledge in this category or sub category of the question field. If the question does not receive any answer, the asker can set a bounty to it. A bounty is

* Corresponding author.
E-mail address: dallol_elalfy47@hotmail.com (D. Elalfy).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.


a special reputation award given to answers. This feature was designed to motivate answerers, and help questions get the answers they deserve. Bounty awards are funded by the personal reputation of the users who offer them. Reputation is a rough mea- surement of how much the community trusts the answer author; it is earned if the answer author convinced other users that his answer is the best and this answerer knows what he is talking about. While bounty maker do not need to be the owner of a ques- tion to start a bounty on that question, only one bounty can be active on a question at once, and each user can only have up to three active bounties at once. Users must have specific reputation score to start a bounty, and at least as much reputation as the bounty amount. The bounty award will be subtracted from your reputation when the bounty is started, not when it is awarded.
If the question receives many answers, the users can give an up and down votes to both the question and answers in its answer thread. The most reputation points score is gained when the answer is up voted, it received a bounty, or it is selected as best answer .Also users can add comments to question and answers. Moreover users can set post as a favorite post. There are other activities but to be able to use them it depends on a user privilege under the community. User privilege depends on user reputation score. As an example of these privilege is to mark question or an answer as an offensive post.


http://dx.doi.org/10.1016/j.eij.2017.06.002
1110-8665/© 2018 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



Question’s author can then select the most preferable or satis- fied answer. This answer is called the best or the accepted answer. Unfortunately, this mechanism in question answering portals may lead to a lot of issues. Such as some askers cannot be able to choose the accepted answer.
For their question. The consequences are a lot of questions left as ‘‘not-answered” [2]. Even if the question is answered, there exist some probability that the answerer is not an expert in such cate- gory. And just answer it because he faced with that question when he/she opens the question answering portal. As a result the com- munity lose one of its targets which is sharing knowledge because of the low quality answers that can be exists.
Moreover, the answerer that gives the low quality answer is capable of give a high quality one if that user was faced by the right question that he/she is expert in.
In this paper, we focus on the problem of exchanging and shar- ing of the knowledge in stack overflow community. And how to ease knowledge exchange by saving asker wasted time and effort that user exerts to find a satisfied answer. By predicting the best answer to the user, which considered the main problem in ques- tion answering portals.
The work in this paper is organized as follows. In Section 2, related work and a depth analysis to different knowledge exchange approaches will be introduced. Section 3, a study for the model in Ref. [3] which is used to give a local reputation score to answers and our proposed hybrid model for predicting best answer. Sec- tion 4, presents the experiment results and discussion. In Section 5 the conclusion is presented.

Related work

In order to improve the mechanism in which question answer portals work under, we need to focus on enhancing the question answer routing approaches and finding best answer techniques... There are a lot of efforts done by researchers in this field to over-
come these problems; we are categorizing these studies to four categories: recommend right experts to a specific question, pre- dicting the best answer, finding group of collaborative experts, and direct questions to an expert. All of these are solutions in order to improve the user satisfaction rate by giving high quality answers to him. Also to minimize the loss of time that is as a result of wait- ing for the right expert to answer the question.

Recommend experts to the current question

In Ref. [4] they tend to find the right expert to answer specific question under certain category. They proposed a hybrid model to find experts using user reputation, user authority, and user sub- ject relevance. In evaluating their model they used Yahoo! Answer platform in Taiwan. Also called Yahoo! Knowledge plus. They assign different priority to terms according to their place like if the word is in answer post, question post, or in question title. One of the main issues in their technique that they do not consider the quality of posts posted by the expert. Since HITS take only the number of posts as an indication to the authority of that user.
In Ref. [5] their aim is to recommend an appropriate users to answer a specific type of. They split questions into two types an authority and affinity questions. They also recommend
A social network that is suitable to answer that type of question. Authors created a website that would allow user’s from different social networks to ask questions in order to gain knowledge or social interaction. Social networks used are Facebook, Myspace, and Twitter. The authority question is the question that seeks information, the affinity question is the question that seeks social interaction or opinion. They build Expertise Estimation Algorithm
to determine the objectivity levels of questioners and responders. This objectivity level is used later in determine the affinity and authority users. They does not consider the relationship between answer and question so that the answer might be off-topic.
In Ref. [6] Enterprise Social Network (ESN) service can help employees to collaborate and communicate effectively with col- leagues, with customers and with suppliers. In this paper authors propose a model to better support question answering process in ESN, by using a graph analysis approach. Based on the questioner’s initial input list of potential answerers, it can extract a shared- interest group of people, whose interest is similar to the initial list of potential answerers, and sort the group of people according to a score of interest distance, and then recommend them to the ques- tioner. To evaluate its applicability, the method is implemented in KDWeibo the most popular ESN platform in China. The algorithms include three key concepts: Interest Distance, Aggregate Special- ization Graph (ASG) and Specialization Sub Graph (SSG). One of the drawbacks exist in their model is that the method will not work if the user is not providing any initial list of potential answer- ers or the quality of the initial list is too low.
The Authors in Ref. [7] introduced a probabilistic framework to predict best answerers for questions. By tracking answerers’ his- tory, interests of answerers are modeled with the mixture of the language model and the Latent Dirichlet Allocation Model LDA. They also used both user authority and activity in predication. They utilizes two models to define user’s interests. They calculated the likelihood probability based on user profile in order to predict the expert. Also they model the prior information of user which is made up of two parts the authority and user activity. They con- ducted their experiment using Isak CAQ in china. As a feedback to their work they need to further investigate their work in large scale dataset. And to use more accurate and different user activity and authority models.
In Ref. [8] Authors use a mechanism to filter online social streams received as well as enable them to interact with most sim- ilar users by personalizing the process of information distribution. Their framework identifies the most appropriate users to receive specific post by calculating the similarity between posts of the tar- get user and the others. The platform used in research is stack overflow. Similarity is calculated based on user’s social activity. User’s social activity is an integration of both user interest in posts published and social activities of that user. Each user is represented by two vectors in vector space model. First vector is social pattern vector which contains influence attributes and user’s distribution. The second vector contains bag of words as a post content’s vector. Term frequency and inverse document frequency is used to weight terms of each vector. Then an aggregated linear model is applied to combine the calculated cosine similarity in two vectors.

Finding the best answer

In Ref. [9] they focus on finding best answer in massive online open courses in which users enroll in courses and to further under- stand it they can ask and answer question in the course forum. The experiment is conducted on openHPI MOOC platform. The users used machine learning through train four classifiers. They are bag- ging, naive Bayes, MultiPerceptron, and Random Forest using user features, thread features and content features. They used as a his- torical data the questions that has at least two answers. The train- ing is performed on the answers of 416 questions.
Ref. [10] is a survey that the researchers found that there is a high correlation between posting a high quality question and get- ting a high quality answer. So they studied the features that most important in question to be found in order to get high quality answers. These question related features are tags and terms, length of the question, presence of an example that may help users to



more understand the question. Asker related feature such as an asker reputation is also mentioned as a good indicator to the qual- ity of the question post.
In Ref. [11] they predict best answer using Yahoo! Answers platform historical data. The experiment is conducted on small number of questions with at least five answers they put 13 criteria and asked five Amazon Mechanical Turk workers to rate the answers based on those features. To be sure that they use high cor- related features, they match the rate given by worker with the actual assessment of the asker. The MTurk rating is from 1 to 5. They furthered their investigation by applying logistic regression classifier to their thirteen features. They found that their features are highly correlated. So they decided to use different features, the new features are extracted from answer and question post. Their data set consists from 116 questions and 575 answers.
In Ref. [12] the main aim of their research is to get the factors that affect selecting the best answer. They perform their investiga- tion on 250 stack Exchange community which considered a large scale analysis. They found that users do not evaluate the answers by any criteria to select it as a best answer. They simply depend on the cognitive heuristic such as the space of this answer or the order of it in the answer thread. As the number of answers increases the users rely more in voting on the cognitive heuristic and that lead to less reliable evaluation by users.
In Ref. [13] they investigate the problem of low quality answers in question answering communities. They found that most of the work nowadays in this area focus on answer features to predict the best answer, they ignore the question type as a feature that decides the attributes that should be exist to determine the best answer to an existing question. They divided their work into two parts, first part is analysis to question type to find the most suitable answer features to train the model. Second part is that they train quality classifier based on question type and aggregate the overall answer quality score. Moreover they propose novel quality fea- tures to predict best answer. Their experiment is conducted on Yahoo! Answers platform with large data set approximately 50 thousand question answer pairs. They found that their hierarchy classifier can predict low quality answers more than high quality ones.
In Ref. [14] they propose question answering model to collabo- rative learning. Their system is for Indonesian high school as a part of E-learning process. They used Wikipedia as a free, web-based, multilingual encyclopedia. After posting a question and receiving answers and votes by the students in the system, the answers is evaluated by comparing it with Wikipedia database. And similarity percentage is provided also a link as a source of more information about that answer. Then all previous question and answers are entered into knowledge base to be a reference to new students or students that may will face the same problem in studying.
In Ref. [15] this paper the researchers trying to predict if the answer will be selected as the best answer by the asker or not. They conduct their research on stack Overflow site. They designed features to train a supervised classifier. Then they investigate the most influential features in prediction results. They used only answer’s content features. The features are divided into contextual features which are the features that address the relation between the answers under the same thread, content features which are the statistical features of the answer itself, and finally the question-Answer relationship features such as the time lag between the posting time of question and the time of the response. They found that contextual features are the most important and influential ones in prediction results. They used large dataset con- sists of 196,145 answers. The classification process is accomplished using random forest classifier with two fold cross validation.
In Ref. [1] they conduct large experiment of Yahoo! Answers platform. In order to recommending top high quality answers
based on reputation score function. Also they used answer content features and combine both content based method and reputation score method. They collected approximately 130 thousands data- set from four different categories in Yahoo! Answer platform. They found that the proposed non content method beats the benchmark link analysis method like HITS method in finding the user authority in this network. They performed an analysis and found that the dis- tribution of the network’s activities follows the power law distri- bution. Moreover they found that reputation non content method outperformed the content one.

Finding collaborative experts

In Ref. [16] this paper authors trying to find group of experts to collaborate to give an answer to a specific question. The idea of col- laboration is to give the question thread a high lasting value. They used stack Overflow platform to conduct their experiments. They finds that question answer process is a collaborative effort that require input from different users. They introduce a user to user compatibility concept. And then present a mechanism to model this concept in question answer communities. The model is build using users compatibility, topical expertise since users must be expert in the same topic or category of question, and availability of users in specific time to collaborate to come up with only one high quality answer.

Route questions to a specific expert

Ref. [17], this paper the first reporting large scale analysis of answerer behavior on session level. The purpose of this study is to route new questions to experts on the topic of the question. And to reuse large scale data to satisfy asker needs. They answered many questions one of those is when the user tend to answer the question. And how the answerer choose the question to answer for. The answers to these questions help in developing a more reliable question recommendation model. Yahoo! Answer platform is used in.
This large scale analysis. They found that user participate in maximum two communities or categories. And that users chose to answer the questions that face them when entering the plat- form. They applied the analysis on all users, they do not consider super active users.

The proposed hybrid best answer prediction model

Our proposed model is consisting of two modules the first mod- ule is to predict the best answers using content features. And the second one is predicting the best answer using non-content fea- tures. After that we combine them in one hybrid model to get the best prediction result.

Content model

The model is an enhancement to the work in Ref. [15]. The orig- inal model is considering answer content features only. The model in Ref. [15] consisting from three parts: the features that deals with the content of the answer alone which are answer content features. Features that considering the relation between a question and its answers which are question-Answer features. And features that measure the degree of similarity between the answer and others answers under the same question which called context features. Our previous model in Ref. [3] added novel features under answer content features. The novel features are added as an enhancement and they are chosen based on the results of the original model in Ref. [15] (see Fig. 1).


The proposed system described in Fig. 2. It consists of three modules. The first module is the preprocessing module, in this stage the question and its answers are preprocessed to extract the tokens that are more effective in feature extraction phase. The second module is feature extraction, in this phase the novel features are added. The added features are added under answer content features only. The last phase is the classification, in which we chose to train more than one supervised classifier to get the most accurate results.

Content model experiment
As in our work in Ref. [3] we choose stack overflow as a plat- form to conduct our research on. We collected the ground truth data under an Academic category, which is a stack overflow cate- gory. Our data is classified to two classes’ best answer and not- best answer class. Our data size is 18,000 answers. We have eval- uated the previous model using three different classifiers, Random forest RF, logistic regression, and Naïve Bayes. We trained Random forest classifier with 10 fold cross validation. We applied and used 22 answer content features only. The features described in Table 1.


Non-Content model

The model is an enhancement to the work in Ref. [1]. The main difference is that the work in Ref. [1] is conducted on Yahoo- answer platform. Part of their work is to predict the best answer using non content feature which is reputation score of the user that








Table 1




Fig. 2. Phases of content based model and new added features.

answered the question. We applied the reputation score function here in stack overflow portal and notice it’s affection in the best answer prediction accuracy.

Reputation based non-content model
Reputation based non-content model is calculated by the mul- tiplication of two scores the first one is user confidence level score and the second one is user expertise level score. Fig. 3 shows the structure of the non-content model as in Ref. [1].

User’s participation level
This is the function that computes the participation level for answerers and to measure their activeness. The participation func-
Features designed under each type and their description.

Type  Symbol
fA	ave_comment, var_comment comment_num URL_tag, pic, code ans_len readability
fQ-A	QA-sim timeSlot
fA-A	ave_ans_sim min_ans_sim max_ans_sim competitor_num ans_index
f (u )=	1













(1)

tion f(uij) is a sigmoid function which provides a high reward to the users with high number of answers and less reward to the users with low number of answers. All answers are under certain cate- gory. Moreover the reward is increased lesly when the number of answers exceeds a certain threshold.
ij	1	e —xij —l 



where xij is the total number of answers that user uij provides in category Ci and m is the threshold value that determined based on the answer distribution pattern of the users. m is used to reward




Fig. 1. (Hybrid model 1) Content and non-content model and their features description.

R(uij)= con(uij)× expe(uij)	(6)
The value of R(uij) should be between 0 and 1. Zero means that the user uij is not an expert and one means that the user is an expert with a high reputation.

Experiments

Dataset description





Fig. 3. User’s reputation score.

most active users. Some highly active users provide more answers than other highly active users. To further reward those people the parameter r is used which is the variation in the number of answers. It is calculated as
s(ﬃﬃxﬃﬃﬃiﬃjﬃﬃ—ﬃﬃﬃﬃﬃx¯ﬃﬃ)ﬃﬃ2ﬃﬃ


In this research we used the same data set in the previous model in Ref. [3]. The ground truth data is collected from stack overflow portal and academic category. We separate our data into
4 windows, each window has different size. Window sizes are 8000, 11,000, 13,000 and 18,000 answer posts. The data is labeled with not-best answer and best answer class. We extracted the answerer information such as number of best answers answered by user in that category.




where x¯ is the average number of answers in the category i. and t is the total number of unique answer authors in the category i.

User’s best answer level
This function is responsible for measuring the degree of exper- tise of the user under certain category. As in Ref. [1] they found that most of users have little number of answers selected as best answer and the minority have large number of best answers. So the best answer score g(uij) can be driven by applying the sigmoid function as in the participation function.

In this paper, we have dealt with the problem of prediction if the given answer is going to be selected as the best answer or not based on the non-content features of this answer. We con- ducted our proposed model based on the work presented in Ref. [1].
We divided our work into three parts:

Content model as in our previous work in Ref. [3].
Non-content model which is a combination from user exper- tise level score and user confidence level score.

	1		3- Merging content and non-content models with each other’s.
)= 

( ij
yij —lb	( )
1 + e — rb

Results and discussion

where yij is the total number of best answers that user uij provides in category Ci and mb is the threshold value that determined based on the best answer distribution pattern of the users. The parameter rb is used which the variation in the number of best answers is given by users in the category I. It is calculated as in Eq. (2).

User’s expertise level
User’s expertise level is influenced by the user’s activeness and

We used precision, recall and accuracy to measure the perfor- mance of our experiments. Precision measurement which is the proportion of the true positives tps versus all the positive results (true positive tps and false positive fps). True positive means the classifier correctly classified the case as best answer class. False positive means the classifier incorrectly classified the case as best answer.

degree of participation in answering the questions. It’s the average of both scores.
Precision =   tps	
tps + f ps
(7)

expe(uij)= f (uij)+ g(uij)
2

Confidence level
(4)
Recall measure which is the fraction of the true positives to all pos- itive classes (true positives tps and false negative fns). False negative means the classifier incorrectly classified the cases to be in not best answer class.

Confidence level con(uij) it’s the ratio between the number of best answers yij to the total number of answers xij in category ci by user uij.
Recall =   tps	
tps + f ns
(8)

( yij/xij, if |yij > 0
While accuracy measure is the proportion of true results (both true

con(uij)= 
0.0001,  if |yij = 0
(5)
positives and true negatives) among the total number of cases examined.

The confidence level function ensures that users who have high number of best answers have high score than others. To differenti- ate between new users who have small numbers of answers but most of them are best answers and the users who have large num- ber of answers and the minority of them are best answers.
Accuracy = 	tps + f ns	
tps + f ps + f ns + tns


Results of using content features only
(9)

Reputation score
The reputation score of the user in a specific category is deter- mined by both the confidence level con(uij) and expertise level expe(uij).
We chose to train Random Forest classifier, Logistic Regression and Naive Bayes using only content features. Random forest classi- fier with 200 trees and 10 fold cross validation while considering 5 random features. The random features are chosen by Weka framework.



In Table 2 we divided our ground truth data into parts each part called window size. We train the classifiers in stages each stage contains different window size. In order to inspect the impact of increasing the data set to the classifier accuracy. In window size 8000 answer set. The best prediction accuracy is 76.87% using logistic regression classifier. Then random forest which is 75.98% and naïve Bayes is the latest one by 73.98%.
In second window size which is 11,000 answers. As you can see the prediction accuracy decreases in both logistic regression and naïve Bayes. While it is increases by 5% in Random forest classifier which is a good indicator that random forest classifier is fit for our data set. Because it is a powerful and efficient classifier when deal- ing with large data set.
In window size 13,000, it is obvious that by increasing the win- dow size, the behavior of the classifier does not change. So both logistic regression and naïve Bayes still decreases in the accuracy. They are 74.05% and 66.13% respectively. And random forest increases by 4% to become 84.12%.
The same behavior by classifiers in window size 18,000 except logistic regression is increased but slightly. And still random forest classifier beats the other. Its accuracy increased to be 88.36%. The most accurate prediction accuracy when using random forest clas- sifier. In general in random forest as increasing window size the prediction accuracy increases.
Fig. 4 shows the comparison between the classifiers behavior with respect to different window sizes in x-axis and prediction accuracy in y-axis. It’s clearly stated that random forest classifier beats others.
Results of using local reputation score only
The same analysis was applied to the data set using non- content feature only which is our proposed reputation score model RNC. Here the classifiers are trained using only the reputation score of the answer author and the predicition class. Table 3 shows the prediction accuracy results using different window sizes.
As in Table 3 you can see that the behavior of the three classi- fiers is almost the same random forest, naïve Bayes, and logistic regression prediction accuracy is decreased when the window size is increased. In window size 8000 answer set. The best prediction accuracy is 74.148% using random forest classifier. Then both logis- tic regression and naïve Bayes with 73.635%.
In second window size which is 11,000 answers. You can observe that the prediction accuracy decreased in all classifiers.
In window size 13,000, it is obvious that by increasing the win- dow size, the behavior of the classifier does not change, it still suf- fering from decreasing in the prediction accuracy.
In window size 18,000 all classifiers results started to increase but still report results less that the first ones.
Fig. 5 shows the comparison between our local reputation clas- sifiers behavior with respect to different window sizes in x-axis and prediction accuracy in y-axis. The classifiers report less accu- racy as we increase the window sizes.

Hybrid model 1(using content and non-content features)
In this subsection, we investigated the affection of merging both previous content features and non-content features (local reputa- tion). The prediction accuracy results are listed in Table 4.





Table 2
Best answer prediction accuracy in content classifier.




Fig. 4. Content classifiers results.



We found that adding non content feature to the content classi- fier does not affect the prediction accuracy positively. And does not decrease it a lot, the decrease in percentage is about 0.05 which is a non-significant change.
Fig. 6 shows the comparison between classifiers using both content and local reputation non content features under title hybrid model 1. The comparison with respect to different win- dow sizes in x-axis and prediction accuracy in y-axis. Classifiers report almost the same behavior and accuracy as in using con- tent features only. Still random forest classifier is the best and its accuracy is 88.34% occurred using window size 18,000 (see Figs. 7–10).
Results of using stack over flow reputation score only (SR)
We decided to use and apply the stack overflow reputation score instead of our local reputation model’s score in order to investigate their effect in prediction accuracy. The following table shows the prediction accuracy of the three classifiers using only the reputation score given by stack overflow community (see Table 5).
Surprisingly, we found that using stack overflow reputation score in prediction does not make the high improvement in accu- racy. And our reputation model and stack overflow reputation model give almost the same accuracy in prediction. Moreover, our reputation score was slightly higher than the stack overflow one.





Table 3
Best answer prediction accuracy in RNC Model (local reputation score only).




Fig. 5. Local reputation classifiers results.



Table 4
Best answer prediction accuracy after adding content and non-content features in classifier (hybrid model 1).





Fig. 6. Hybrid model 1 results.



Fig. 7. Stack overflow reputation score results.



Fig. 8. Hybrid model 2 results.




Fig. 9. Separated non-content results.




Fig. 10. Hybrid model 3 results.


Hybrid model 2
Table 6 captures the prediction accuracy of the classifiers using both stack overflow non content feature and content features.
We found that as we increase the window size the prediction accuracy increased and to compare hybrid model 1 HCR results with hybrid model 2 HSR, we found that in the first and the second window sizes the hybrid model 2 results is higher by 0.13%. And in the third and the fourth window sizes the opposite occurs. In gen- eral hybrid model 1 and hybrid model 2 are the same in prediction accuracy. And both of them is lower than the accuracy when we train the classifier using only the content features. The higher accu- racy we got is when using the content features only it is 88.36%.
Using both content and stack overflow non-content reputation score in the classification process together is better than using stack overflow reputation score only. But still hybrid model 1 accu- racy in prediction is higher than hybrid model 2.


Results of separated Non-content model
We expanded our analysis to investigate non content features separated. The non-content features used is number of participa- tions to the answerer in this category of question and number of best answers of this user in this category. We reported the predic- tion results of classifiers in Table 7.




Table 5
Best answer prediction accuracy using non-content feature in classifier SR (Stack overflow reputation model only).






Table 6
Best answer prediction accuracy after adding content and stack overflow non-content feature in classifier (hybrid model 2 HSR).



Table 7
Best answer prediction accuracy using non-content separated features in classifier.





Table 8
Best answer prediction accuracy using both content and non-content separated features in classifier.






Table 9
Comparison between the three non-content models in prediction accuracy.



As you can see in Fig. 9, the three classifiers results is mostly the same. And the behavior is the same. The higher prediction result is when using smallest window size in all classifiers. And still random forest beats the others.

Hybrid model 3
Then we added both content with non-content separated fea- tures into the classification model to inspect the effect of the add- ing and the results are shown in Table 8. Non content separated features are number of participated answers by the user y in cate- gory x and number of best answers by user y in category x, respectively.
In Table 8, Hybrid model 3 is introduced. Contains both answer content features and non-content separated featues which is both number of participations to this answerer in the category and number of best answers to this answer in the same category of question. We found that as usual random forest classifier beats
other classifiers in prediction accuracy. Finally, We got a promising results by hybrid model 3, since its prediction accuracy using 18,000 window size as well as random forest classifier is 88.65% (see Fig. 10).

Comparing results
In Table 9 we Compares the three non-content models in pre- diction accuracy.
In Table 9, we compared all non-content models in Tables 3, 5, and 7 consecutively with respect to the largest window size. We concluded that if you want to train a model to predict best answers based on non-content features only, you can use random forest classifier with our local reputation score feature instead of using either stack overflow reputation score or separated non-content features to get higher prediction accuracy.
In Table 10 we compared the three introduced hybrid models in Tables 4, 6, and 8, we found that the same trend is in all classifiers.


Table 10
Comparing three hybrid models in prediction accuracy.



Where as the window size increased so the prediction accuracy. And that the higher prediction accuracy is got using random forest which is a reasonably result, since random forest is a reliable and robust classifier that works better as the number of dataset increases. In hybrid model 1 in Table 2 which is the combination between content features and our local non- content reputation features. We found that random forest classifier accuracy using window size 18,000 is 88.34%. In hybrid model 2 in Table 4 which is the combination between content features and stack overflow non-content reputation feature. We found that random forest clas- sifier accuracy using window size 18,000 is 88.13%. In hybrid model 3 in Table 8 which is the combination between content fea- tures and non-content separated features. We found that random forest classifier accuracy using window size 18,000 is 88.65%. We concluded our finding reporting that using both content and non- content separated features (hybrid model 3) in predicting the best answer is the best and giving us the higher accuracy than all others.


Conclusion

In this paper we worked on the question answering communi- ties. And the issues that impede these communities to achieve their goals of sharing and exchanging to high quality knowledge. Then we discussed our previous model in predicating best answer in QACs and our findings using only content features. After that, we introduced an existing model using non-content features only, this model was used in different platform which is yahoo Answers. Also we compare the result of using the introduced non-content model with two different non-content models. We inspected the impact of using the non-content models alone. Finally, we introduced a different hybrid models which are combination of both our previ- ous content and the non-content models. Moreover, we compared hybrid models results. We concluded the prediction results will increased by the merge process of hybrid model 3 with the content model and separated non-content features. Our experiments are conducted on stack overflow CQA platform .Also we found that using our introduced non-content model alone is better than using stack overflow non-content or using separated non-content model since it affects in increasing the predication accuracy positively.
References

Chen Lin, Nayak Richi. Leveraging the network information for evaluating answer quality in a collaborative question answering portal. Soc Netw Anal Min 2012;2(3):197–215.
Enterprise Social Network. In: Services Computing Conference (APSCC), Asia- Pacific: IEEE; 2012.
Elalfy Dalia, Gad Walaa, Ismail Rasha. Predicting best answer in community questions based on content and sentiment analysis. In: 2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS). IEEE; 2015.
Liu Duen-Ren et al. Integrating expert profile, reputation and link analysis for expert finding in question-answering websites. Inf Process Manage (2013);49 (1):312–329.
Zhan Justin, Jones Nadia M., Purnell Michael D. Top-K algorithm for recommendation in social networking kingdoms. In: Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on. IEEE; 2011.
Ning Ke, Li Ning, Zhang Liang-Jie. Using graph analysis approach to support question & answer on enterprise social network. In: Services Computing Conference (APSCC), 2012 IEEE Asia-Pacific. IEEE; 2012.
Liu Mingrong, Liu Yicen, Yang Qing. Predicting best answerers for new questions in community question answering. Berlin Heidelberg: Web-Age Information Management. Springer; 2010. p. 127–38.
Abeer ElKorany, ElBahnasy Khaled. Personalizing of content dissemination in online social networks. target 4.12 (2013).
Jenders Maximilian, Krestel Ralf, Naumann Felix. Which answer is best?: predicting accepted answers in MOOC forums. In: Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee; 2016.
Baltadzhieva Antoaneta, Chrupała Grzegorz. Question quality in community question answering forums: a survey. ACM SIGKDD Explor Newsl 2015;17 (1):8–13.
Shah Chirag, Pomerantz Jefferey. Evaluating and predicting answer quality in community QA. In: Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM; 2010.
Burghardt Keith et al. The Myopia of crowds: a study of collective evaluation on stack exchange. Robert H. Smith School Research Paper No. RHS 2736568 (2016).
Toba Hapnes et al. Discovering high quality answers in community question answering archives using a hierarchy of classifiers. Inf Sci 2014;261:101–115.
Arai Kohei, Handayani Anik Nur. Question answering system for an effective collaborative learning. IJACSA J (2012);3(1).
Tian Qiongjie, Zhang Peng, Li Baoxin. Towards predicting the best answers in community-based question-answering services. ICWSM 2013.
Chang Shuo, Pal Aditya. Routing questions for collaborative answering in community question answering. In: Proceedings of the 2013 IEEE/ACM International conference on advances in social networks analysis and mining. ACM; 2013.
Liu Qiaoling, Agichtein Eugene. Modeling answerer behavior in collaborative question answering systems. Springer Berlin Heidelberg: Advances in information retrieval; 2011. p. 67–79.
