Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 347 (2019) 161–177
www.elsevier.com/locate/entcs
Neural Nets via
Forward State Transformation and Backward Loss Transformation
Bart Jacobs1	David Sprunger2
Institute for Computing and Information Sciences (iCIS) Radboud University Nijmegen
The Netherlands
ERATO Metamathematics for Systems Design Project National Insitute of Informatics
Tokyo, Japan

Abstract
This article studies (multilayer perceptron) neural networks with an emphasis on the transformations in- volved — both forward and backward — in order to develop a semantic/logical perspective that is in line with standard program semantics. The common two-pass neural network training algorithms make this viewpoint particularly fitting. In the forward direction, neural networks act as state transformers, using
Kleisli composition for the multiset monad — for the linear parts of network layers. In the reverse direction, however, neural networks change losses of outputs to losses of inputs, thereby acting like a (real-valued) predicate transformer. In this way, backpropagation is functorial by construction, as shown in other works recently. We illustrate this perspective by training a simple instance of a neural network.
Keywords: Neural network, backpropagation, multilayer perceptron, state-and-effect triangle, loss transformation

Introduction
Though interest in artificial intelligence and machine learning have always been high, the public’s exposure to successful applications has markedly increased in recent years. From consumer-oriented applications like recommendation engines, speech face recognition, and text prediction to prominent examples of superhuman performance (DeepMind’s AlphaGo, IBM’s Watson), the impressive results of ma- chine learning continue to grow.
Though the understandable excitement around the expanding catalog of suc- cessful applications lends a kind of mystique, neural networks and the algorithms

1 Email:bart@cs.ru.nl
2 Email:sprunger@nii.ac.jp

https://doi.org/10.1016/j.entcs.2019.09.009
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

which train them are, at their core, a special kind of computer program. One per- spective on programs which is relevant in this domain are so-called state-and-effect triangles, which emphasize the dual nature of programs as both state and predicate transformers. This framework originated in quantum computing, but has a wide variety of applications including deterministic and probabilistic computations [6].
The common two-pass training scheme in neural networks makes their dual role particularly evident. Operating in the “forward direction” neural networks are like a function: given an input signal they behave like (a mathematical model of) a brain to produce an output signal. This is a form of state transformation. In the “backwards direction”, however, the derivative of a loss function with respect to the output of the network is backpropagated [8] to the derivative of the loss function with respect to the inputs to the network. This is a kind of predicate transformation, taking a real- valued predicate about the loss at the output and producing a real-valued predicate about the source of loss at the input. The main novel perspective offered by this paper uses such state-and-effect ‘triangles’ for neural networks. This state-and- effect framework has also been used to understand other artificial representations of human reasoning, including Bayesian networks [7].
In recent years, it has become apparent that the architecture of a neural network is very important for its accuracy and trainability in particular problem domains [3]. This has resulted in a profligation of specialized architectures, each adapted to its application. Our goal here is not to express the wide variety of special neural networks in a single framework, but rather to describe neural networks generally as an instance of this duality between state and predicate transformers. Therefore, we shall work with a simple, suitably generic neural network type called the multilayer perceptron (MLP).
We see this paper as one of recent steps towards the application of modern semantic and logical techniques to neural networks, following for instance [1,2]. The main contribution of this paper is that it offers its own representation of network layers as Kleisli maps — as alternative to the representation given in [1]. There, a neural network layer n → m, with a function Rn → Rm as intended meaning, is represented as map P × n → m, where P is an unspecified, abstract object of parameters. These parameters are typically matrices in Rn×m, representing the linear part of the network layer. The updating of parameters is handled in [1] via an ‘update’ function P ×n×m → P , together with a ‘request’ function P ×n×m → n whose role is somewhat mysterious. The sequential (and parallel) composition of such layers can be given in terms of non-trivial string diagrams.
Here we follow a much simpler approach which does not ‘internalise’ the pa- rameters via parameter objects P . Instead, the linear parts of network layers are represented via Kleisli maps of the multiset monad M. Forward propagation of a state x ∈ Rn along a layer is then quite naturally represented via Kleisli composi- tion, involving the familiar linear expressions  i wij · xi with weights wij that are used as scalars for inputs xi. This paper elaborates this approach, which is ‘ob- vious’, from the perspective of monadic computation, and more concrete than the one with abstract parameter objects P . It allows us to describe backpropagation

also concretely, in terms of matrices and derivatives. This part of the story can be compared to predicate transformation in program semantics, which also works in backward direction.
Since the categorical modelling of neural computing is still in its infancy, it is too early to say which one is the ‘best’ and we do not yet have technical results in that direction. Indeed, it makes sense to leave room for different representa- tions. We expect that these formal approaches to neural networks can be of use as the community develops explainable AI, where the goal is to extend automated decisions/classifications with human understandable explanations. In this context, we believe having a diversified pool of techniques will be a valuable resource; we are wary of prematurely attempting to rank by usefulness formal approaches with incomparable aspects.
Outline. In this paper, we begin by describing MLPs, the layers they are com- posed of, and their forward semantics as a state transformation (Section 2). In Section 3, we give the corresponding backwards transformation on loss functions and use that to formulate backpropagation in Section 4. Finally, in Section 5, we discuss the compositional nature of backpropagation by casting it as a functor, and compare our work in particular to [1].


Forward state transformation
Much like ordinary programs, neural networks are often subdivided into functional units which can then be composed both in sequence and in parallel. These subnet- works are usually called layers, and the sequential composition of several layers is by definition a “deep” network 3 . There are a number of common layer types, and a neural network can often be described by naming the layer types and the way these layers are composed.
Feedforward networks are an important class of neural networks where the com- position structure of layers forms a directed acyclic graph—the layers can be put in an order so that no layer is used as the input to an earlier layer. A multilayer perceptron is a particular kind of feedforward network where all layers have the same general architecture, called a fully-connected layer, and are composed strictly in sequence. As mentioned in the introduction, the MLP is perhaps the prototyp- ical neural network architecture, so we treat this network type as a representative example. In the sequel, we will use the phrase “neural network” to denote this particular network architecture.
More concretely, a layer consists of two lists of nodes with directed edges between



3 In contrast, the “width” of a layer typically refers to the number of input and output units, which can be thought of as the repeated parallel composition of yet another architecture.

them. For instance, a neural network with two layers may be depicted as follows.
 • ¸,
•	 • ¸¸,	z • ˛¸¸,
• 	z • ˛¸¸	zz • ˛¸¸
• 	zz • ˛¸
We will represent such a network via special arrows 3 ⇒ 4 ⇒ 2, where the numbers 3, 4, and 2 correspond to the number of nodes at each stage. These arrows involve weights, biases, masks, and activations, see Definition 2.1 below. The (forward) semantics of these arrows is given by functions R3 → R4 → R2. They will be de- scribed in greater detail shortly, in Definition 2.3. We first concentrate on individual layers.
In the definition below we shall write M(n)= Rn and P(n)= {k ∈ N | k ⊆ n}. In this description of the powerset P we identify a natural number n ∈ N with the n-element subset of numbers {0, 1,...,n − 1} below n. We shall have more to say about M and P in Remark 2.2 below.
Definition 2.1 A single layer n ⇒ k between natural numbers n, k ∈ N is given by three functions:
n +1   T  M¸(k)	the transition function
n  M  P¸(k)	the mask function

R   α  R¸
the activation function.

The transition function T can be decomposed into a pair [Tw, Tb], where Tw : n → M(k) captures the weights and Tb ∈ M(k) the biases. The mask function M : n → P(k) captures connections and mutability; it works as follows, for i ∈ n and j ∈ k.
j ∈ M (i)	means	there is a mutable connection
from node i to node j, with weight T (i)(j)
j /∈ M (i) and T (i)(j)=0	means	there is no connection from
node i to node j
⎪
j /∈ M (i) and T (i)(j) /= 0	means	there is a non-mutable con-
⎪


The activation function α : R → R is required to be differentiable.
Mutability is used only to determine which weights should be updated after back propagation. In particular, M is not used in forward propagation, and we often omit M in situations where it plays no role, including forward propagation.

Remark 2.2 The operations M and P are called multiset and powerset. They both form a monad on the category Set of sets and functions. In general, they are defined on a set I as:
P(I) = {S | S ⊆ I}
M(I) = {ϕ : I → R | supp(ϕ) is finite},

where supp(ϕ)= {i ∈ I | ϕ(i) /= 0} is the support of ϕ. Such a function ϕ can also be written as formal sum:


ϕ ≡ r1
|i1⟩ + ··· + rm
|im
⟩	where	supp(ϕ)= {i1,..., im}⊆ I
rk = ϕ(ik) ∈ R.


This explains why such an element ϕ ∈ M(I) is sometimes called a multiset on I: it counts elements ik ∈ I with multiplicity rk = ϕ(ik) ∈ R.
In this paper we shall use these monads P and M exclusively on natural num- bers, as finite sets; in that case M(n)= Rn, as used above.
We shall not really use that P and M are monads, except for the following construction: each function T : I → M(J) has a ‘Kleisli’ or ‘linear’ extension T∗ : M(I) → M(J) given by:

T∗(ϕ)(j) =	T (i)(j) · ϕ(i).	(1)
i∈I
The transistion map T in a layer n ⇒ k is the linear part of the associated function Rn → Rk, and the activation function α is the non-linear part. This linear role of T is emphasised by using this linear extension T∗.
Notice that if T (i)(j) = 0, then the input from node i does not contribute to the outcome. Hence this corresponds to not having a connection i → j in the layer. When it comes to updating, we have to distinguish between a weight being 0 because there is no connection — so that it remains 0 — and weights that happen to be zero at some point in time, but may become non-zero after an update. This is done via the mask function M .
Definition 2.3 Let ⟨T, M, α⟩ be a layer n ⇒ k as in Definition 2.1. It gives rise to a (differentiable) function [ T, α ]] : Rn → Rk in the following manner.
[[ T, α ]](x) := α T∗(x, 1) .	(2)
Notice that we use notation x ∈ Rn to indicate a vector of reals xi ∈ R. Similarly, the notation α is used to apply α : R → R coordinate-wise to T∗(x, 1) ∈ Rk, where T∗ is defined in (1). The additional input 1 in T∗(x, 1) is used to handle biases, as will be illustrated in the example below.
The function [ T, α ]] : Rn → Rk expresses (forward) state transformation. Some-

times we use alternative notation  for state transformation, defined as:
(T, α)  x := [[ T, α ]](x).

This notation is especially suggestive in combination with loss transformation , working backwards.
The interpretation function [ T, α ] performs what is often called forward prop- agation. We will refer to vectors x ∈ Rn as states; they describe the numerical values associated with n nodes at a particular stage in a neural network. We can then also say that forward propagation involves state transformation—a layer n ⇒ k transforms states in Rn to states in Rk.
The following example 4 illustrates how the interpretation function works.
Example 2.4 Consider the following neural network with two layers.


•  	0.15	  • ¸¸
0.25
 • ¸¸
0.5

0.2
0.3
0.45
z • ˛¸¸	z • ˛¸¸	(3)

0.35
◦
0.35
0.6
◦
0.6

We shall describe this network as two layers:


2   ⟨T,M,σ⟩  z2  ⟨S,M,σ⟩  z2 
where	M (i)=2= {0, 1}
σ(z)=	1−z


In this network all connections are mutable, as indicated via the function M which sends each i ∈ 2 to the whole subset M (i)=2 ⊆ 2. The activation function is the so-called sigmoid function σ, for both layers, given by σ(z)= 1/(1+e−z).
The two transition functions T, S have type 3 → M(2). Their definition is given by the labels on the arrows in the network (3):
T (0) = 0.15 |0⟩ + 0.25 |1⟩	S(0) = 0.4 |0⟩ + 0.5 |1⟩
T (1) = 0.2 |0⟩ + 0.3 |1⟩	S(1) = 0.45 |0⟩ + 0.55 |1⟩
T (2) = 0.35 |0⟩ + 0.35 |1⟩	S(2) = 0.6 |0⟩ + 0.6 |1⟩ .
Alternatively, one may see T, S as matrices:
T =  0.15 0.2 0.35	S =  0.4 0.45 0.6 

4 The example is taken from Matt Mazur’s blog,	at https://mattmazur.com/2015/03/17/ a-step-by-step-backpropagation-example/.

We thus get, according to (2):



= ⟨ σ

= ⟨ σ
T (0)(0) · x0 + T (1)(0) · x1 + T (2)(0) · 1 ,
σ T (0)(1) · x0 + T (1)(1) · x1 + T (2)(1) · 1 ⟩
0.15 · x0 + 0.2 · x1 + 0.35 , σ 0.25 · x0 + 0.3 · x1 + 0.35 ⟩

[[ S, σ ]](y0, y1) = ⟨ σ 0.4 · y0 + 0.45 · y1 + 0.6 , σ 0.5 · y0 + 0.55 · y1 + 0.6 ⟩.

We see how the bias is described via the arrows out of the ‘open’ nodes ◦ in (3) and is added in the appropriate manner to the outcome, via the value ‘1’ on the right-hand-side in (2).
The network transforms an initial state ⟨0.05, 0.1⟩∈ R2 first into 5 :
[[ T, σ ]](0.05, 0.1) = ⟨ σ(0.3775), σ(0.3925) ⟩ = ⟨ 0.59326999, 0.59688438 ⟩
Subsequently it yields as final state:
[[ S, σ ]](0.59327, 0.59688) = ⟨ σ(1.10591), σ(1.22492) ⟩ = ⟨ 0.75136507, 0.77292847 ⟩.

We write NN for the category of neural networks, as in [1]. Its objects are natural numbers n ∈ N, corresponding to n nodes. A morphism n → k in NN is a sequence of layers n ⇒ ··· ⇒ k, forming a neural network. Composition in NN is given by concatenation of sequences; a (tagged) empty sequence is used as identity map for each object n.
Next, we write RF for the category of real multivariate differentiable functions: objects are natural numbers and morphisms n → k are differentiable functions Rn → Rk.
Proposition 2.5 Forward state transformation (propagation) yields a functor NN → RF, which is the identity on objects. A morphism n → k in NN, given by a sequence of layers ⟨l1, ··· , lm⟩, is sent to the composite [[ lm ]] ◦ · · · ◦ [[ l1 ]] : Rn → Rk, with the understanding that an empty sequence ⟨⟩ : n → n in NN gets sent to the identity function Rn → Rn. This yields a functor by construction.	 
In  line  with  this  description  we  shall  interpret  a  morphism  N	=
⟨l1,..., lm⟩ : n → k in the category NN as a function [ N ] = [ lm ]] ◦ · · · ◦
[[ l1 ]] : Rn → Rk. We also write N  x for [ N ]](x).
Backward loss transformations
In the theory of neural networks one uses ‘loss’ functions to evaluate how much the outcome of a computation differs from a certain ‘target’. A common choice is the

5 The calculations here, and in Example 4.5 have been done with simple Python code, using the numpy
library.

following. Given outcomes y ∈ Rk and a target t ∈ Rk one takes as loss:
1	(yi − ti)2
i
Here we abstract away from the precise form of such computations and use a function L for loss. In fact, we incorporate the target t in the loss function, so that for the above example we can give L the type L : Rk → R, with definition:
y |= L := L(y)
The validity notation |= emerges from the view that vectors y ∈ Rk are states (of type k), and loss functions L : Rk → R are predicates (of type k). The notation y |= L then expresses the value of the loss L in the state y.
We now come to backward transformation of loss along a layer. We ignore mutability because it does not play a role.
Definition 3.1 Let (T, α): n ⇒ k be a single layer. Each loss function L : Rk → R on the codomain k of this layer can be transformed into a loss function (T, α) L : Rn → R on the domain n via:
(T, α)  L := L ◦ [[ T, α ]] : Rn → Rk → R.

For a morphism N = ⟨l1,..., lm⟩ : n → k in the category NN of neural networks we define:
N  L := l1  · · · (ln  L) = L ◦ [[ lm ]] ◦ · · · ◦ [[ l1 ]] = L ◦ [[ N ]].
We can now formulate a familiar property for validity and transformations, see
e.g. [4,6].
Lemma 3.2 For any neural network N : n → k in NN, any loss function L : Rk →
R and any state x ∈ Rn, one has:
N  x |= L	=	x |= N  L.	(4)
Proof By the definition of these notations:
N  x |= L = L N  x
= L ◦ [[ N ]] (x)

= x |= N  L.	 

Many forms of state and predicate transformation can be described in the form of a ‘state-and-effect triangle’, where ‘effect’ is used as alternative name for ‘predicate’, see [6]. Here this takes the following form.

Theorem 3.3 There are state and predicate functors Stat and Pred in a triangle:

Hom(−,R)
Setop ¸	T	zSe t


Pred
Hom(−,R)

NN

Stat

given by:
Pred(n) = RRn	Stat(n) = Rn
Pred(N ) = N  (−) = (−) ◦ [[ N ]	Stat(N ) = N  (−) = [[ N ]] ◦ (−).
 
The above triangle commutes in one direction: Hom(−, R) ◦ Stat = Pred. In or- der to obtain commutation in the other direction one typically restricts the category Set to an appropriate subcategory of algebraic structures. For instance, in prob- abilistic computation, states form convex sets and predicates form effect modules, see e.g. [4,5]. In the present situation with neural nets it remains to be investigated which algebraic structures are relevant. That is not so clear in the current general set up, for instance because we impose no restrictions on the loss functions that we use.

Back propagation
In the setting of neural networks, back propagation is a key step to perform an update of (the linear part of) a layer. Here we shall give an abstract description of such updates, in terms of a loss function L as used in the previous section. In fact, we assume that what is commonly called the learning rate η is also incorporated in L.
Let ⟨T, M, α⟩ : n → k be a layer. Given an input state a ∈ Rn and an (differen- tiable) loss predicate L : Rk → R we will define a gradient
∇(a,L)(T )	and use it to change T into	T − M ⊙ ∇(a,L)(T ),

where the mutability map M : n → P(k) is used as k × n Boolean matrix (with 0’s and 1’s only), and where ⊙ is the Hadamard product, given by elementwise multiplication. It ensures that only mutable connections are updated.
Definition 4.1 In the situation just described, the gradient can be given as:
∇(a,L)(T ) :=  ∂  (X, α)  a |= L  (T ).	(5)
We have introduced a new bound variable X, to clearly indicate the derivative that we are interested in. The type of X is the same as T , namelya k × (n + 1) matrix.

In order to compute this gradient, we recall that the derivative of a (differen- tiable) function f : Rn → Rm is the m × n ‘Jacobian’ matrix of partial derivatives:


∂f1
∂x1
··· ∂f1 ⎞

fj = ⎜⎝
.
∂fm
∂x1
.
··· ∂fm
n

Lemma 4.2 In the situation of Deﬁnition 4.1,
The gradient ∇(a,L)(T ) can be calculated as:

∇(a,L)(T ) = s · (a, 1)T	where	sj = Lj((T, α)  a)j · αj(T∗(a, 1)j).

(The superscript T in (−)T is for ‘matrix transpose’, and is unrelated to the transition map T.)
In the special case where α is the sigmoid function σ, the vector s in point (i)
is a Hadamard product:
s = Lj(b) ⊙ b ⊙ (1 − b)	where	b = (T, σ)   a.

Proof The chain rule for multivariate functions gives a product of matrices:
∇(a,L)(T ) = Lj (T, α)  a) · αj T∗(a, 1) ·  ∂ X∗(a, 1) (T ).	(6)
We elaborate the three parts one-by-one.
The derivative of the loss function L : Rk → R is given by its partial derivatives, written as Lj : Rk → Rk. Thus, the first part Lj (T, α)  a) of (6) is in Rk.
The derivative of the coordinate-wise application α : Rk → Rk of α : R → R, applied to the sequence T∗(a, 1) ∈ Rk consists of the k×k diagonal matrix with entries αj(T∗(a, 1)j) at position j, j. We shall write this diagonal as a vector
⟨αj(T∗(a, 1)j)⟩∈ Rk.
The product of the first two factors in (6) can thus be written as a Hadamard (coordinatewise) product ⊙:
Lj (T, α)  a) ⊙ ⟨αj(T∗(a, 1)j)⟩.

For the third part in (6) we notice that X '→ X∗(a, 1) is a function Rk×(n+1) → Rk. The jth row of its Jacobian consists of the k × (n + 1) matrix with a, 1 at row j and zeros everywhere else. Indeed, the jth coordinate X∗(a, 1)j is given by:
X∗(a, 1)j = Xj1a1 + ··· + Xjnan + Xj(n+1).
Taking its derivative with respect to the variables Xji yields the k × (n + 1)

matrix:

0 ··· 0 0
a1 ··· an 1
0 ··· 0 0


→ row j

Thus,  ∂ X∗(a, 1) (T ) consists of k-many of such matrices stacked on top of each
other.
Writing sj = Lj((T, α)  a)j · αj(T∗(a, 1)j) we can put the previous three bullets together and write the gradient ∇(a,L)(T ) as an outer product:

⎛⎜a1s1 ··· ans1 s1⎞
⎛s1⎞

a1sk ··· ansk sk⎠
⎝sk⎠


Directly from (i) since σj = σ(1 − σ).	 
Next we are interested in gradients of multiple layers.
(S,β) (T,α)
Proposition 4.3 Consider two consecutive layers m=⇒n =⇒ k, with initial state
a ∈ Rm and loss function L : Rk → R. The gradient for updating S is:

  ∂  (T, α)  (X, β)  a |= L 

(4) ∂

 (X, β)  a |= (T, α)  L  (S)

= ∇(a,(T,α)  L)(S).

The derivative of the transformed loss (T, α)  L is by the chain rule:
 (T, α)  L j(y) =  Lj [[ T, α ]](y) ⊙ αj T∗(y, 1)  · [T ],	(7)

where [T ] is the k × n matrix obtained from the k × (n + 1) matrix T by omitting the last column.
More generally, for appropriately typed neural nets N, M,
∂  N  (X, α)  M  a |= L  (S) = ∇(M  a,N  L)(S).

Proof The first equation in the above proposition obviously holds. We concentrate on the second equation (7):
 (T, α)  L j(y) = L ◦ [[ T, α ]] j(y)

=	Lj [[ T, α ]](y) ⊙ αj T (y, 1)	· T j(y, 1)
= Lj [[ T, α ]](y) ⊙ αj T∗(y, 1)  · [T ].

We still need to prove T j(y, 1) = [T ], where [T ] is obtained from T by dropping the last column. The function T∗(−, 1) has type Rn → Rk, so the derivative T j(y, 1) is
a k × n matrix with entry at i, j given by:


∂T∗(y,1)i
∂yj
∂(Ti1y1+···+Tinyn+Ti(n+1))
∂yj
= Tij.

Together these Tij, for 1 ≤ i ≤ k and 1 ≤ j ≤ n, form the k × n matrix [T ]. 
Remark 4.4 Equation (7) reveals an important point: for actual computation of backpropagation we are not so much interested in loss transformation, but in erosion transformation, where we introduce the word ‘erosion’ as name for the derivative Lj of the loss function L.
For this erosion transformation we introduce new notation ≪. Let (T, α): n ⇒ k be a single layer, and let E : Rk → Rk be a ‘erosion’ function. We transform it into another erosion function (T, α) ≪ E : Rn → Rn, by following (7):
 (T, α) ≪ E (x) := E [[ T, α ]](x) ⊙ αj T∗(x, 1)  · [T ].	(8) By construction we have:
 (T, α)  L j = (T, α) ≪ Lj.	(9)
Conceptually, we consider loss transformation more fundamental than erosion tran- formation, because loss transformation gives rise to the ‘triangle’ situation in The- orem 3.3. In addition, erosion transformation can be expressed via derivatives and loss transformation, as the above equation (9) shows.
In the obvious way we can extend ≪ in (8) from single to multiple layers (neural networks). In case α is the sigmoid function σ, the right-hand-side of (8) simplifies to:
(T, σ) ≪ E (x) =	E(y) ⊙ y ⊙ (1 − y) · [T ]	where	y = (T, σ)  x.
(10)
We illustrate back propagation for the earlier example.
Example 4.5 We continue Example 2.4 and compute the relevant gradients for updating the transition maps/matrices T, S in the neural network (3) with two layers:
2   (T,σ)  z2  (S,σ) z2 
We shall write input, intermediary, and final states, as computed in Example 2.4, respectively as:
a = ⟨0.05, 0.1⟩
b = (T, σ)  a = ⟨0.59326999, 0.59688438⟩
c = (S, σ)  b = ⟨0.75136507, 0.77292847⟩.

The target in this example is ⟨0.01, 0.99⟩∈ R2, so that the loss function L : R2 → R
and its ‘erosion’ derivative E = Lj : R2 → R2 are:
L(x) = 1 η (x1 − 0.01)2 + (x2 − 0.99)2	E(x) = η⟨x1 − 0.01, x2 − 0.99⟩.
The learning rate η is set to 0.5.
The updating of the transition matrices T, S works in backward direction. By Lemma 4.2 we get as gradient:
∇((T,σ)  a,L)(S) = s · (b, 1)T	where s = E(c) ⊙ c ⊙ (1 − c)
=	0.08216704	0.08266763	0.13849856	.
−0.02260254 −0.02274024 −0.03809824
Hence the updated last transition function / matrix S is:
S − s · (b, 1)T =	0.35891648 0.40866619 0.53075072
0.51130127 0.56137012 0.61904912
Our next aim is to update the preceding, first transition function / matrix T .
∇(a,(S,σ)  L)(T ) = t · (a, 1)T
where	t = ((T, σ) ≪ E)(b) ⊙ b ⊙ (1 − b)
(10) 
=	s · [T ] ⊙ b ⊙ (1 − b)
=	0.00043857 0.00087714 0.00877135	.
0.00049771 0.00099543 0.00995425
The updated first matrix of the neural network is then:
T − t · (c, 1)T =	0.14978072 0.19956143 0.34561432
0.24975114 0.29950229 0.34502287
This corresponds to the numbers given in Mazur’s blog mentioned in footnote 4, except that there the biases are not updated. This example illustrates that back- propagation can be done in a recursive manner, since the values s in the first step are re-used in t in the second step.
Functoriality of backpropagation
In a recent paper [1] a categorical analysis of neural networks is given. Its main result is compositionality of backpropagation, via a description of backpropaga- tion as a functor. In this section we first give a description of the functoriality of backpropagation in the current framework, and then give a comparison with [1].

We write SL for the category of ‘states and losses’.
The objects of SL are triples (n, a, L), where a ∈ Rn is a state of type n and
L : Rn → R is a (differentiable) loss function of the same type n.
A morphism N : (n, a, L) → (k, b, K) is a neural network N : n → k, in the category NN, such that both: b = N  a and K = N  L.
There is an obvious forgetful functor U : SL → NN given by U (n, a, L) = n and
U (N )= N .

Definition 5.1 Define backprop B : SL → NN in the following way. On objects, we simply take G(n, a, L)= n. Next, let N = ⟨l1,..., lm⟩ be a morphism (n, a, L) → (k, b, K) in SL, where li = (Ti, αi, Mi). We write:
a0 := a and ai+1 := li   ai; this gives a list of states ⟨a0, a1,..., am⟩ with
am = b, by assumption;
Km := K and Ki—1 := li  Ki; this gives a list of loss functions ⟨K0,..., Km⟩
with K0 = L.
Then B(N ): n → k is defined as a list of layers, of the same length m as N , with components:
B(N )i := ⟨Ti − Mi ⊙ ∇(ai−1,Ki)(Ti), αi, Mi⟩.
(Recall, Mi is a Boolean ‘mask’ matrix that takes care of mutability, and ⊙ is the Hadamard product.)
Theorem 5.2 Backprop B : SL → NN is a functor.
Proof This is ‘immediate’, but writing out the details involves a bit of book keeping. Let (n, a, L) −N→ (k, b, K) −P→ (l, c,F ) be (composible) morphisms in SL, where N = ⟨l1,..., lu⟩ and P = ⟨p1,..., pv⟩. We write li = ⟨TN , αN ,MN ⟩ and similarly
i	i	i
pj = ⟨TP , αP ,MP ⟩. The procedures in the two bullets in Definition 5.1 yield for
j	j	j
the maps N and K separately:
⟨a0,..., au⟩ and ⟨b0,..., bv⟩ where a0 = a, ai+1 = li  ai and b0 = b, bj+1 =
pj  bi; we have b0 = b = N  a = N  a0 = au;
⟨K0,..., Ku⟩ and ⟨F0,..., Fv⟩ with Ku = K, Ki—1 = li   Ki and Fv =
F, Fj—1 = pj  Fj; then Ku = D = P  F = P  Fv = F0.
From the perspective of the composite sequence ⟨l1,..., lu, p1,..., pv⟩ we can go
through the same process and obtain sequences ⟨aj ,..., aj	⟩ and ⟨F j,...,Fj	⟩

with:
0	u+v
0	u+v

j	ai	if i ≤ p
i
bi—p otherwise
j	Kj	if j ≤ p
j
Fj—p otherwise.

We can now describe the components of the updated network B(P ◦ N ).  For

1 ≤ i ≤ u and 1 ≤ j ≤ v,

B(P ◦ N )i = ⟨TN − MN ⊙ ∇(a′	,F ′)(TN ), αN , MN ⟩
i	i	i−1 i	i	i	i
= ⟨TN − MN ⊙ ∇(a	,K )(TN ), αN , MN ⟩

i	i
= B(N )i
i−1  i	i	i	i

= B(P ) ◦ B(N ) i
B(P ◦ N )u+j = ⟨TP − MP ⊙ ∇(a′



,F ′

)(TP ), αP , MP ⟩

j	j	u+j−1
u+j	j	j	j

= ⟨TP − MP ⊙ ∇(b	,F )(TP ), αP , MP ⟩

j	j
= B(P )j
j−1 j	j	j	j

= B(K) ◦ B(N ) p+j.	 


We conclude this section with a comparison to [1], where it was first shown that backpropagation is functorial. The approach in [1] is both more abstract and more concrete than ours.
Here, a layer (T, α): n ⇒ k of a neural network consists of linear part T : n +1 → M(k) and a non-linear part α : R → R. We ignore the muta- bility matrix M for a moment. As shown in Definition 2.3, the layer (T, α) gives rise to an interpretation function [ T, α ]] : Rn → Rk that performs for- ward state transformation (T, α) (−). In [1] there is no such concrete description of a layer. Instead, the paper works with ‘parametrised’ functions P × Rn → Rk. Our approach fits in this framework by taking the set of lin- ear parts P = M(k)n+1 = (Rk)n+1 as parameter set. These parametrised functions are organised in a category Para, which is shown to be symmetric monoidal.
The comparison of the outcome of a state transformation by a network n ⇒ k and a target t ∈ Rk is captured here abstractly via a loss function L : Rk → R. This more general perspective allows us to define loss transformation N L along a network N . We have thus developed a view on neural network computation, with forward and backward transformations, that is in line with standard approached to (categorical) program semantics. It gives rise to the pattern of a state-and-effect triangle in Theorem 3.3. Moreover, we show that there is an associated ‘erosion transformation’ function, that is suitable related to loss transformation via derivatives, see (9).
In the formalism of [1] backward computation also plays a role, via a request function ‘r’, of type P × Rn × Rk → Rn, for a network n ⇒ k. It corresponds to our erosion transformation (8), roughly as: r(l, a, b)= (l ≪ Lj )(a), where
j is the derivative of the loss function Lb associated with the ‘target’ b.
Here we have concentrated on the sequential structure. In [1], parallel compo- sition is also taken into account in the form of symmetric monoidal structure. For us, such additional structure is left as future work.

Conclusions
In this paper, we have examined neural networks as programs in a state-and-effect framework. In particular, we have characterized the application of a neural network to an input as a kind of state transformation via Kleisli composition and backprop- agation of loss along the network as a kind of predicate transformation on losses. We also observed that the compositionality of backpropagation corresponds to the functoriality of a mapping between a category of states-and-effects to the category of neural networks.
For the sake of illustrating this perspective on neural networks, we have delib- erately chosen a simple subclass of the known network architectures and built a category of multilayer perceptron (MLPs). However, we believe it is possible to develop a richer categorical structure capable of capturing a much wider variety of network architectures. This may be the focus of future work.
We also considered a single training scheme: backpropagation paired with stochastic gradient descent (with a fixed learning rate). We are interested in mod- eling other kinds of neural network training categorically.
As mentioned in the discussion following Theorem 3.3, there is typically a cate- gory of algebraic structures in the upper right vertex of the state-and-effect triangle which we have not determined yet.
Acknowledgment
The first author (BJ) acknowledges support from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) / ERC grant agreement no 320571. The second author (DS) is supported by the JST ERATO HASUO Metamathematics for Systems Design Project (No. JPM- JER1603).

References
Fong, B., D. Spivak and R. Tuy´eras, Backprop as functor: A compositional perspective on supervised learning (2019), lICS 2019, to appear; see arxiv.org/abs/1711.10455.
Ghica, D., K. Muroya, S. Chung, V. Darvariu and R. Rowe, A functional perspective on machine learning via programmable induction and abduction, in: J. Gallagher and M. Sulzmann, editors, Functional and Logic Programming, number 10818 in Lect. Notes Comp. Sci. (2018), pp. 84–98.
Goodfellow, I., Y. Bengio and A. Courville, “Deep Learning,” MIT Press, 2016, http://www. deeplearningbook.org.
Jacobs, B., New directions in categorical logic, for classical, probabilistic and quantum logic, Logical Methods in Comp. Sci. 11(3) (2015), see https://lmcs.episciences.org/1600.
Jacobs, B., From probability monads to commutative effectuses, Journ. of Logical and Algebraic Methods in Programming 94 (2017), pp. 200–237.
Jacobs, B., A recipe for state and effect triangles, Logical Methods in Comp. Sci. 13(2) (2017), see
https://lmcs.episciences.org/3660.
Jacobs, B. and F. Zanasi, The logical essentials of Bayesian reasoning, in: Probabilistic Programming
(book chapter, to appear in 2019), see arxiv.org/abs/1804.01193.

Rumelhart, D. E., G. E. Hinton and R. J. Williams, Learning representations by back-propagating errors, Nature 323 (1986), pp. 533–536.
