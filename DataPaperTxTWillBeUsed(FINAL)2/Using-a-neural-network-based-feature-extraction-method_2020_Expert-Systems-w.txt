Expert Systems with Applications: X 6 (2020) 100030

		





Using a neural network-based feature extraction method to facilitate citation screening for systematic reviews
Georgios Kontonatsios a,∗, Sally Spencer b, Peter Matthewa, Ioannis Korkontzelosa
a Department of Computer Science, Edge Hill University, United Kingdom
b Faculty of Health and Social Care, Edge Hill University, United Kingdom


a r t i c l e	i n f o	a b s t r a c t

	

Article history:
Received 4 June 2019
Revised 19 March 2020
Accepted 30 April 2020
Available online 4 May 2020

Keywords: Citation screening Text mining
Neural feature extraction
Citation screening is a labour-intensive part of the process of a systematic literature review that identi- fies citations eligible for inclusion in the review. In this paper, we present an automatic text classification approach that aims to prioritise eligible citations earlier than ineligible ones and thus reduces the man- ual labelling effort that is involved in the screening process. e.g. by automatically excluding lower ranked citations. To improve the performance of the text classifier, we develop a novel neural network-based feature extraction method. Unlike previous approaches to citation screening that employ unsupervised feature extraction methods to address a supervised classification task, our proposed method extracts doc- ument features in a supervised setting. In particular, our method generates a feature representation for documents, which is explicitly optimised to discriminate between eligible and ineligible citations.
The generated document representation is subsequently used to train a text classifier.
Experiments show that our feature extraction method obtains average workload savings of 56% when evaluated across 23 medical systematic reviews. The proposed method outperforms 10 baseline feature extraction methods by approximately 6% in terms of the WSS@95% metric.
© 2020 Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)





Introduction

Systematic reviews of the effects of interventions constitute the cornerstone of modern evidence-based medicine (Greenhalgh, Howick, & Maskrey, 2014). High qual- ity reviews, such as those produced by Cochrane, are frequently used to inform healthcare guidelines and to provide policy makers with the best and most up-to-date evidence on a specific medical topic (Volmink, Siegfried, Robertson, & Gülmezoglu, 2004).
Higgins and Green (2011). However, owing to the prolifera- tion of the published literature (Bastian, Glasziou, & Chalmers, 2010), the manual production of a systematic review has be- come a time-consuming process, with an average completion time of approximately 2.4 years (Bekhuis & Demner-Fushman, 2012). Greenhalgh et al. (2014). In addition, Shojania et al. (2007) re-


∗ Corresponding author.
E-mail addresses: georgios.kontonatsios@gmail.com (G. Kontonatsios), sally.spencer@edgehill.ac.uk (S. Spencer), peter.matthew@edgehill.ac.uk (P. Matthew), Yannis.Korkontzelos@edgehill.ac.uk (I. Korkontzelos).
ported that 23% of the published systematic reviews need to be updated with new relevant studies within 2 years from the time they are completed. In practice, this means that review authors are required to repeat the same resource-intensive tasks of the system- atic review pipeline, such as literature searches, citation screening, data extraction and evidence synthesis, at regular intervals.
To reduce the average completion time of systematic reviews, we present a novel text mining method that semi-automates the citation screening task, i.e. a critical process of the systematic review pipeline that identifies relevant citations for inclusion in the review (O’Mara-Eves, Thomas, McNaught, Miwa, & Ananiadou, 2015). Our text mining method requires a seed of manually la- belled citations to learn to discriminate between relevant/positive and irrelevant/negative instances. In succession, the trained classi- fier is used to automatically process the unlabelled citations, min- imising the manual labelling effort that is associated with the ci- tation screening task.
In practical application scenarios, our text mining method can be used to aid (human) systematic reviewers in screening more eﬃciently citations for inclusion in a review. More specifically, a human reviewer needs to manually label only a subset of


https://doi.org/10.1016/j.eswax.2020.100030
2590-1885/© 2020 Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)



the citations, i.e. a training dataset. This manually labelled sub- set of citations is used to train the underlying text classifica- tion algorithm, which is subsequently used to automatically la- bel the remaining unlabelled citations. In addition to systematic reviews, our proposed method can be used in a wide range of different application areas relevant to expert and intelligent sys- tems, such as information retrieval (Sethi & Dixit, 2015), text categorisation (Miron´ czuk & Protasiewicz, 2018), knowledge dis- covery (Bandaru, Ng, & Deb, 2017) and recommendation sys- tems (Wei, He, Chen, Zhou, & Tang, 2017). Text categorisation task,
e.g. sentiment analysis (Cambria, 2016), constitutes a direct ap- plication area of our method. Our experiments demonstrate that the proposed method can substantially improve text classification performance. Moreover, our method could be integrated with text search engines, e.g. Apache Solr (Smiley, Pugh, Parisa, & Mitchell, 2015), in order to learn to identify documents that are relevant to individual user preferences.
In the context of citation screening, existing text mining meth- ods can be coarsely classified into: a) automatic text classifica- tion (Adeva, Atxa, Carrillo, & Zengotitabengoa, 2014; Bekhuis & Demner-Fushman, 2012; Cohen, Hersh, Peterson, & Yen, 2006; Frunza, Inkpen, & Matwin, 2010) and b) automatic screening priori- tisation (Cohen, 2008; Cohen, Ambert, & McDonagh, 2012; Cohen et al., 2015; Howard et al., 2016) techniques. Both types of meth- ods follow a similar approach to firstly train a supervised classifi- cation algorithm, e.g. Support Vector Machines (Wallace, Trikalinos, Lau, Brodley, & Schmid, 2010), Naive Bayes (Matwin, Kouznetsov, Inkpen, Frunza, & O’blenis, 2010), Random Forest (Khabsa, Elma- garmid, Ilyas, Hammady, & Ouzzani, 2016), on a subset of the cita- tions that are manually labelled with include/exclude codes by hu- man reviewers. The trained classification algorithm is subsequently used to automatically process the remaining unlabelled citations. Although automatic text classification methods have been shown to achieve substantial workload savings (Cohen et al., 2006; Frunza et al., 2010), Bekhuis and Demner-Fushman (2012) noted that such methods may not always converge to a high recall performance of at least 95%, which is a key requirement of the citation screening task.
Automatic screening prioritisation techniques, including our
proposed method, aim at re-ordering the citations in the un- labelled set so that citations that are likely to be eligible for inclusion in the review are ranked higher than ineligible cita- tions (Howard et al., 2016). In contrast to automatic text classifica- tion approaches that frame the screening task as a binary classifi- cation problem, automatic screening prioritisation methods assign a classification confidence value to each citation rather than a bi- nary label. The confidence value determines the likelihood of a ci- tation being relevant to the review and it is used by the model to prioritise the unlabelled citations. Automatic prioritisation meth- ods can reduce the screening workload, considering that human reviewers need to process only the top ranked citations, whereas the lower ranked citations are automatically excluded from the re- view (Cohen et al., 2015; O’Mara-Eves et al., 2015).
The vast majority of existing semi-automatic citation screen- ing methods adopts unsupervised document representation tech- niques, such as bag-of-words, to address an inherently super- vised classification task. Therefore, the induced feature represen- tation of documents naturally ignores the readily available class- membership information of manually labelled citations. In this pa- per, we present a new supervised feature representation technique that leverages the class-membership information of the manu- ally screened citations to generate informative document features. The proposed method uses a multi-layer feed forward neural net- work to learn a latent representation of documents that encodes discriminative and class-specific information about the citation screening task.
More specifically, our proposed feed forward neural network is trained on the manually labelled citations, while the hidden layers of the network are iteratively optimised to better discriminate be- tween eligible and ineligible studies. We then extract an embedded feature representation of documents using the fixed weights of the hidden layers. The document embeddings can be integrated with any classification algorithm used for automatic screening prioriti- sation. Following previous approaches (Cohen et al., 2015; Wallace et al., 2010), we use a Support Vector Machine with a linear ker- nel to assign a classification confidence to each citation set and we rank the citation list in order of relevance to the review.
To further improve the performance of our neural network- based feature extraction method, we investigate pre-training tech- niques that aim to enhance the initialisation process of the feed forward neural network. In our approach, we employ a deep de- noising autoencoder (Vincent, Larochelle, Lajoie, Bengio, & Man- zagol, 2010), a type of unsupervised neural network that learns to denoise an artificially corrupted version of the input feature space. The reconstructed version of the input feature space is sub- sequently used to initialise the feed forward component of our method.
For evaluation, we conduct a series of experiments to investi- gate the performance of our supervised feature induction method when applied to the citation screening task of 23 publicly available systematic review datasets from the medical domain (Cohen et al., 2006; Howard et al., 2016). Experimental results demonstrate that our proposed feature extraction method can reduce the number of items that need to be manually screened without decreasing the sensitivity of the review, i.e. at least 95% of relevant studies are identified by the semi-automatic screening method. Moreover, our neural network-based feature extraction method shows substantial performance improvements when compared to 10 baseline feature extraction methods. The contributions of this paper can be sum- marised as follows:
We develop a new neural network-based feature extraction method to accelerate the citation screening task of system- atic reviews.
We conduct large-scale experiments across a total number of 23 medical systematic reviews datasets to evaluate the ef- fectiveness of the proposed method.
Our feature extraction method yields significant workload savings of at least 10% in 22 out of 23 review datasets.
Our method outperforms 10 baseline feature extraction methods by approximately 6%, in terms of the average work- load saving (Cohen et al., 2006).
We make the source code of our tool publicly available at: https://github.com/gkontonatsios/DAE-FF.

Related work

Prior work to semi-automatic citation screening, concerning both document classification and document ranking techniques, has investigated the use of different document representation tech- niques, such as bag-of-words (BoW), topic modelling, bibliographic metadata or a combination of the above, to improve the per- formance of the underlying text classification algorithm. More- over, existing document representation techniques used for semi- automatic citation screening have been evaluated across a number of domain topics, including clinical medicine (Cohen et al., 2006; Wallace et al., 2010), social science (Miwa, Thomas, O’Mara-Eves, & Ananiadou, 2014) and software engineering (Marshall & Brere- ton, 2013).
The BoW model is a standard document representation tech- nique that has been widely adopted by previous semi-automatic citation screening methods (Cohen et al., 2006; Frunza et al., 2010;



Kim & Choi, 2012; Wallace et al., 2010). In the BoW model, each document is represented as a sparse, high-dimensional feature vec- tor, wherein the dimensions of the vector correspond to words or phrases that occur in the document. Bekhuis and Demner- Fushman (2012) demonstrated that an automatic text classifica- tion method trained on BoW features achieved substantial work- load savings of 35%-46% on two medical systematic reviews. More- over, the authors showed that single-word features yielded an op- timal performance when compared to bi-gram or tri-gram fea- tures, i.e. phrases consisting of two or three words, respectively. However, a limitation of the BoW model is that the resulting fea- ture space consists of a large number of word-features and there- fore the model is associated with increased memory and com- putational costs when applied to large-scale systematic review datasets (Forman, 2003).
Feature selection methods, e.g. forward feature selec- tion (Cohen et al., 2015) or information gain filters (Bekhuis & Demner-Fushman, 2012), have been previously used to reduce the size of the BoW space, although Adeva et al. (2014) re- ported that such feature selection methods result in insignificant performance improvements.
Several  studies  proposed  using  bibliographic  meta- data to enhance the BoW space with additional features. Wallace et al. (2010) presented an automatic text classifica- tion system, trained via active learning, that used multiple feature types, including BoW, the publication type and indexing keywords. Each feature type was firstly used to train a text classification model. The screening decisions made by individual classification models were subsequently combined using a voting scheme. The experiments that they conducted showed that their ensemble classification model that exploited multiple feature types obtained a robust performance by reducing the screening workload of 4 medical reviews by 40%-50%. Although, bibliographic metadata can be used to improve upon the performance of the BoW feature space, Miwa et al. (2014) noted that such bibliographic features may not always be available for every citation or domain topic (e.g. social science). In response, the authors used an unsuper- vised topic modelling method, namely Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), to automatically identify la- tent topics in a collection of documents. Experimental results demonstrated that automatically identified topics can be used to complement potentially missing bibliographic metadata.
One-hot encoding feature extraction methods (e.g. as BoW fea-
tures and bibliographic metadata) are strong baselines which yield a robust performance across a wide range of different classification tasks(Miron´ czuk & Protasiewicz, 2018). Moreover, one-hot encod- ing methods are easy to implement while the underlying feature model is highly interpretable(Wang & Manning, 2012). However, one-hot encoding methods are known to discard the order and the semantics of words and phrases (Le & Mikolov, 2014; Mikolov, Sutskever, Chen, Corrado, & Dean, 2013). In practice, this means that the text classifier, trained on one-hot encoding features, may yield a decreased performance due to the high ambiguity of the technical terminology used in complex, multi-disciplinary topics (e.g., public health) (Hashimoto, Kontonatsios, Miwa, & Ananiadou, 2016; Miwa et al., 2014).
In  a  study  closely  related  to  our  work,  Hashimoto et al. (2016) presented a variation of the widely popular paragraph vectors (PV) model (Le & Mikolov, 2014), a document representa- tion technique for extracting informative document features. The PV model is a neural network-based feature extraction method that follows a distributional semantics approach to better account for words and documents semantics. More specifically, the PV model trains a shallow neural network, consisting of one hid- den layer, by maximising the conditional probability of a word given its context and the document that it appears. Hashimoto
et al. (2016) modified the original implementation of the PV method in order to model each document as a distribution of latent topics. The authors further showed that their proposed PV method achieved a superior performance on complex, multi- disciplinary reviews when compared to the LDA topic modelling method. However, a limitation of the PV model is that it follows an unsupervised approach to feature representation and therefore the generated feature space is not explicitly optimised to discriminate between eligible and ineligible studies.
The main advantage of our method when compared to previous feature extraction methods is that it follows a supervised approach to extract discriminative document features. Moreover, our method generates a dense and low-dimensional feature space which is eas- ier to manage when compared to the sparse and high-dimensional feature space produced by the BoW model. We further show that our supervised feature extraction method can enhance the per- formance of semi-automatic citation screening when compared to previously used unsupervised feature extraction methods, includ- ing BoW, bibliographic metadata, a low dimensional projection of the BoW space using the Singular Value Decomposition, a topic- based feature extraction method based on Latent Dirichlet Alloca- tion (Bekhuis & Demner-Fushman, 2012; Howard et al., 2016; Miwa et al., 2014; Mo, Kontonatsios, & Ananiadou, 2015) and a topic- based feature induction method which exploits a shallow neural network (Hashimoto et al., 2016). Moreover, we report statistical significant improvements over the baseline methods in several re- view datasets.

Methodology

In this section, we detail the methodology that we follow to semi-automate the citation screening process of systematic re- views. Firstly, we describe the automatic screening prioritisation framework that we use to evaluate different feature representation methods. We then provide implementation details of our proposed neural network-based feature extraction method.

Automatic screening prioritisation framework

Fig. 1 shows the overall architecture of the automatic screen- ing prioritisation framework that we use in our experiments. We follow the same experimental settings reported elsewhere in the literature (Cohen et al., 2006), by randomly partitioning the initial citation list into two equal sized sets, namely labelled and unla- belled. Both sets consist of 50% of the citations, whereas there is no overlap between the citations in the labelled set and the cita- tions in the unlabelled set.

Fig. 1. Architecture of the automatic screening prioritisation framework.



The labelled set is manually annotated by a human reviewer with include/exclude codes and it is used by the text classification method to learn to discriminate between eligible and ineligible studies. It should be noted that in our experiments we use pub- licly available datasets which were manually annotated with in- clude and exclude codes in prior work (Cohen et al., 2006; Howard et al., 2016; Wallace et al., 2010).
The text classification method firstly uses a feature extraction component to transform the textual content of citations into a nu- merical representation, i.e. feature vectors. In our approach, we de- velop a new supervised feature extraction method that uses a neu- ral network model to generate a discriminative feature represen- tation of documents. Document features extracted by our method are then used as input to a linear SVM classifier. The proposed su- pervised feature extraction method is described in the following section, Section 3.2.
The linear SVM classifier is trained to discriminate between eligible and ineligible citations, given the document features ex- tracted previously. More specifically, a linear SVM constructs a lin- ear hyperplane to best separate eligible from ineligible citations. After training the linear SVM model, we use the trained model to prioritise the citations in the unlabelled set, so that higher ranked citations are more likely to be eligible for inclusion in the review than lower ranked citations. More specifically, we rank the unla- belled citations according to the classification confidence of a ci- tation being relevant to the eligible class. The classification confi- dence of a citation is computed based on the signed-margin dis- tance of the feature vector for that citation to the SVM hyper- plane, i.e. the higher the distance, the higher the classification con- fidence. Once the citations are prioritised in order of relevance to the eligible class, the top ranked citations are included in the re- view, whereas the lower ranked citations are deemed ineligible and they are thus automatically excluded from the review. Follow- ing previous studies Howard et al. (2016) we fix a cut-off threshold (i.e. minimum confidence value that discriminates between eligible and ineligible studies) at a recall level of 95%.

Supervised feature extraction

Fig. 2 illustrates the architecture of our supervised feature ex- traction method. The proposed method coordinates two types of neural networks: a) a denoising autoencoder and b) a feed forward network.
A denoising autoencoder (Vincent et al., 2010) aims to re- construct the input BoW feature space given an artificially cor-




Fig. 2. Architecture of the supervised feature extraction method.


The encoded representation y(i) is then mapped back, i.e. de- coded, into a BoW reconstruction z(i) ∈ Rd through the decoder mapping function:
z(i) = f (W r y˜(i) + br )	(2)
The parameters {W, b} and {Wr, br} of the encoder and de- coder function, respectively, are optimised using the Adadelta op- timiser (Zeiler, 2012), a variation of the stochastic gradient descent algorithm, by minimising the cross entropy of the reconstruction error according to:
Σ

rupted version of the BoW space. More specifically, consider X =
{x(1), ··· , x(i), ··· , x(k) } a set of k input BoW feature vectors where
LH (X, Z) = −

i=1
xi log zi + (1 − xi ) log(1 − zi )	(3)

x(i) ∈ Rd is the BoW vector of the ith citation. Each BoW feature
vector consists of d word-dimensions, where each dimension cor- responds to a word that appears in the title or in the abstract of a citation. The value of a word-dimension is the raw frequency of that word in a given citation.
Previous studies have demonstrated how a denoising au- toencoder learns meaningful data representations by learning to remove the input noise in the data, in contrast to conven- tional autoencoders which are trained on cleaned input data (Vincent, 2011). Based on this, we artificially corrupt the input BoW feature using additive Gaussian noise of a standard deviation
σ = 0.5, so that x˜(i) is the corrupted version of x(i).
The goal of an one-layer denoising autoencoder is to firstly en- code the corrupted feature vector x˜(i) into a lower dimensional rep- resentation y(i) ∈ Rh using the encoder mapping function:
y(i) = f (W x˜(i) + b)	(1)
where f is a non-linear activation function, such as the logistic sig- moid function, W is the weight matrix and b is the bias vector.
In our approach, we use a straightforward variation of the one- layer denoising autoencoder (DAE), namely a deep DAE, which simply adds additional intermediate hidden layers into the net- work to learn more complex non-linear projections of the input data (Hinton & Salakhutdinov, 2006). Moreover, we use three dif- ferent DAEs to learn potentially different reconstructions of the BoW space. The experiments that we conducted, presented in Section 4.5.3, demonstrate that a multi-branch model architecture that uses multiple DAE components obtains a statistically signifi- cantly better performance, in comparison to a single-branch archi- tecture that uses only a single DAE component. Each DAE consists of 5 hidden layers, whereas we vary the dimensionality of the first and last hidden layer across the three DAEs to obtain different re- constructions of the BoW space. The reconstructed output of each DAE is then used to initialise the supervised feed forward neural network. This type of unsupervised pre-training, where the feed forward neural network is initialised by deep DAEs, has been previ- ously shown to substantially improve the performance of the feed forward network (Erhan et al., 2010).



The feed forward neural network consists of 6 hidden fully con- nected layers, i.e. {L1, L2···, L6}, and an output softmax layer L7 that is used to compute the probability distribution over the eli-
gible and ineligible class for a given citation. The first three hid- den layers of the network {L1, L2, L3} are parallel to each other,
i.e. there is no connection between the units of the three layers,
and they are initialised by the output reconstructions of the three DAEs, namely {z1, z2, z3}, respectively. The three parallel layers are subsequently concatenated into a wide fully connected layer L4 of 3072 units. Following the wide layer, we coordinate two additional hidden fully connected layers, i.e. L5 and L6, of 1024 units. We should further note that the size of the hidden layers is empiri- cally defined. In Section 4.5.3, we report the performance of the DAE component when using hidden layers of varying dimensional- ity.
The feed forward neural network is trained in a supervised manner by minimising the cross entropy between the probability distribution of the gold standard classes and the probability dis- tribution of classes estimated by the softmax layer. The weights of the feed forward network are fine-tuned during training using vanilla stochastic gradient descent.
After training the feed forward network, we extract supervised feature vectors, that correspond to the whole set of the learned data, using the weight matrix of the wide fully connected layer L4 according to:
h(z) = WL4 · [σL1 (z); σL2 (z); σL3 (z)]	(4)
where WL4 is the weight matrix of the wide fully connected layer L4, [ · ; · ] denotes feature concatenation and σL1 (z), σL2 (z) and σL3 (z) are Rectified Linear Unit (ReLU) activation functions (Nair & Hinton, 2010) of the L1, L2 and L3 hidden layers, respectively.
The extracted document vectors, i.e. the output of the con-
nected layer L4 of our proposed neural network-based feature ex- traction method, are subsequently used as input to a linear SVM text classifier. We further note that the feature extraction step is not dependent on the text classification model and similarly the text classification step does not rely upon the feature extraction method. Consequently, different feature extraction methods can be used with the same text classifier, and different text classifiers can
and c) SWIFT reviews (Howard et al., 2016). Both the clinical and the drug review datasets consist of a relatively small num- ber of citations ranging from 310, for the Antihistamines review, to 4751 citations, for the Proton Beam review). The 5 SWIFT review datasets are substantially larger in size in comparison to the clin- ical and drug datasets, containing between 4479 and 48,637 cita- tions. Howard et al. (2016) noted that the SWIFT review datasets were constructed using broad search strategies, whereas the el- igibility criteria of the reviews include multiple study designs (e.g. human/animal/in vitro clinical trials), which explains the large size of these reviews. The 3 clinical review datasets are relevant to clinical or health outcomes of different treatments (e.g. clini- cal outcomes of Proton Beam radiation treatment), while the 15 drug review datasets investigate the eﬃcacy of drug therapies (e.g. Skeletal Muscle Relaxant treatment).
In order to tune the hyper-parameters of our feature extraction method, we used two development reviews, namely the Statins and the BPA dataset that consist of 3465 and 7699 citations, respec- tively.

Evaluation settings

As evaluation metric, we use the widely adopted Work Saved over Sampling at r% recall (WSS@r%) (Cohen et al., 2006; Frunza et al., 2010; Howard et al., 2016; Kanoulas, Li, Azzopardi, & Spi- jker, 2017), which estimates the reduction of the (human) screen- ing workload at a fixed recall level of r%. According to the WSS@r%, the workload reduction achieved by an automatic prioritisation method is equivalent to the percentage of citations that are ranked lower in the prioritised citation lisT, i.e. citations that are automat- ically excluded from the review and thus reviewers do not need to manually read those citations, than the cut-off threshold which is fixed at a recall level of 95%. The recall performance of the method is the proportion of eligible studies out of the total number of eli- gible studies that is ranked higher in the prioritised list. Thus, for a given recall performance of r%, the WSS@r% can be computed as follows:
penalty term

be used with the same feature extraction method. In the context of this study, we seek to assess the performance of our novel fea- ture extraction method and we therefore evaluate different base- line feature extraction methods against our proposed method us-
WSS@r%	TN + FN
          
(%) excluded citations
z(1I−c r˛)	(5)

ing the same linear SVM text classifier.

Experiments

Data

We evaluate our proposed supervised feature extraction method on 23 publicly available systematic review datasets from the med- ical domain. Table 1 summarises the descriptive characteristics for each dataset, including a) the publication source of the dataset,
b) the size of the dataset in terms of number citations that need to be screened, c) the percentage of eligible citations and d) the availability of bibliographic metadata. Each citation in the review datasets consists of a title, abstract and a classification label, i.e. el- igible or ineligible, associated with that citation. Moreover, 18 out of 23 review datasets include additional bibliographic metadata for
where TN is the number of true negative predictions, FN the num-
ber of false negative predictions and N the total number of cita- tions. The penalty term, i.e. 1 − r, determines the proportion of ci- tations that is falsely excluded from the review, i.e. eligible cita- tions that are falsely ranked lower in the prioritised list.
Previous studies (Bekhuis & Demner-Fushman, 2012; Cohen et al., 2006; O’Mara-Eves et al., 2015; Wallace et al., 2010) noted that an acceptable recall performance of an automatic prioritisa- tion method needs to be at least 95%. A lower recall performance of less than 95% may impact the quality of the underlying re- view considering that a substantial proportion of eligible studies is falsely excluded during the screening process. Based upon this, we fix the recall performance of our automatic prioritisation method at 95% and we compute the obtained work saved (i.e. WSS@95%) according to:
WSS@95% = TN + FN − (1 − 0.95) = TN + FN − 0.05	(6)

each citation in the form of Medical Subject Heading (MeSH) tags1.	N	N

We further organise the 23 review datasets into the follow- ing 3 groups according to their publication source: a) clinical re- views (Wallace et al., 2010), b) drug reviews (Cohen et al., 2006)
In addition to the WSS@95% metric, we further compute the precision performance of our method at a fixed recall level of 95% according to:



1 indexing terms used by the Medline bibliographic database
TP
precision@95%recall = TP + FP , given tha trecall = 95%	(7)


Table 1
23 publicly available review datasets used in the experiments of this paper.


Table 2
Baseline feature extraction methods with selected parameter settings used in the experiments of this paper.

Baseline method	Hyper-parameters

BoW	Top (Stemmed) words: 10,000
SVD	eigenvalues: 300
LDA	topics: 300, iterations: 500
PV	topics: 300, iterations: 500, document vector: 1000, word vector: 300 MeSH tags (bibliographic metadata)	–


For all evaluation tests, we report average values of the WSS@95% and precision@95%recall metrics over 10 cross-validation folds. More specifically, we follow a stratified 10 × 2 cross- validation setting. Each cross-validation round firstly partitions the initial dataset into two equally sized subsets. One subset is used for training and the second subset is used for computing the eval- uation metrics. Both the training and the evaluation subsets con- sist of the same ratio of ineligible to eligible citations. We repeat this cross-validation process 10 times and we then average the 10 WSS@95% and precision@95%recall scores to obtain a final estimate.

Automatic prioritisation system

The automatic prioritisation system that we use in our exper- iments employs an L2-regularised linear SVM classifier to rank the citations according to the signed-margin distance between the citation feature vectors and the SVM hyperplane. We devel- oped the linear SVM classifier using the Scikit-learn python li- brary (Pedregosa et al., 2011). In order to better account for the class imbalance between eligible and ineligible citations, we used a reduced misclassification cost, i.e. a trade-off between maximis- ing the margin between the two classes and minimising classifi- cation errors, by setting the regularisation parameter C = 1 × 10−6. We used the same hyper-parameter settings for the SVM classifier across all review datasets and across all feature extraction meth- ods.

Baseline methods

Table 2 shows the hyper-parameter settings for 5 baseline fea- ture extraction methods. With regard to the BoW method, we apply basic pre-processing steps following recommendations by
Matwin et al. (2010). Firstly, we remove stop words found in NLTK’s stop word list (Bird & Loper, 2004) and then we convert the original surface form of the words (e.g. therapies) into their corresponding base forms (e.g. therapi) using the Porter stemmer (Porter, 2001). After pre-processing the words that occur in the ti- tle and in the abstract of each citation, we construct BoW feature vectors consisting of the 10,000 most frequent words in the col- lection. As feature values, we consider the frequency of occurrence of a word-dimension in a given citation. Feature weighting tech- niques, such as term frequency-inverse document frequency (tf-idf) weighting, could be potentially used to normalise word frequency values. However, Matwin et al. (2010) showed that such feature weighting techniques yield approximately the same performance as the unnormalised word frequencies on the drug development reviews.
The singular value decomposition (SVD) method is a dimen- sionality reduction technique that projects an input high dimen- sional feature space into a dense, lower dimensional space. SVD is different than the widely used Principal Component Analysis (PCA), as it computes eigenvalues and eigenvectors directly on the input data matrix, whereas PCA computes eigenvalues and eigenvectors on the covariance matrix of the input data (Wall, Rechtsteiner, & Rocha, 2003). In the context of this study, we use the SVD base- line method to derive a low dimensional projection of the BoW feature space. The SVD baseline method is implemented using the Scikit-learn library. It should be noted that no prior work has pre- viously evaluated SVD derived features for semi-automatic citation screening. We therefore identify optimal parameter settings, i.e. di- mensionality of the projected space according to the top K eigen- values, for the SVD method using a grid search method on the same two development reviews that we used to fine-tune our su- pervised feature extraction method. Experimental results showed



that an SVD feature space of 300 dimensions (K = 300) yields an optimal WSS@95% performance on both development reviews.
Table 3
Hyperparameter settings of the supervised feature extrac- tion method.

The two topic modelling methods, namely the Latent Dirich-		

let Allocation (LDA) (Blei et al., 2003) and Paragraph Vectors (PV) (Hashimoto et al., 2016) model, represent each citation as a mixture of M latent topics. The LDA method is a popular feature extraction technique among existing semi-automatic ci- tation screening systems. Here, we implement a baseline LDA method using the MALLET library (McCallum, 2002). We further tune the parameters of the LDA method by setting the number of latent topics to 300 and the number of iterations to 500 as in Miwa et al. (2014). The PV method (Hashimoto et al., 2016) is an alternative topic modelling feature extraction technique that it was previously shown to outperform the LDA method on three public health reviews. In our experiments, we use the publicly available implementation of the PV method2. Moreover, we use the same parameter settings as in (Hashimoto et al., 2016) by setting the di- mensionality of the word embeddings to 300, the dimensionality of the document embeddings to 1000, the number of latent topics to 300 and the number of training iterations to 500.
MeSH tags are single word or multi-word keywords that are manually assigned to every citation indexed by the Medline bibli- ographic database (Lipscomb, 2000). MeSH tags aim at summaris- ing the textual content of citations using a set of descriptive key- words. Considering that MeSH keywords may not always appear in the title or in the abstract of a citation, MeSH-based features can potentially provide complimentary information to BoW fea- tures (Trieschnigg et al., 2009). In order to retrieve MeSH tags from the Medline database, we use the Biopython library (Cock et al., 2009). We then construct binary feature vectors, where each di- mension of the vectors corresponds to a different MeSH tag, while feature values determine the presence or absence of a MeSH tag in a given citation.
Previous studies investigated the performance of compos- ite features consisting of a column-wise concatenation of dif- ferent single-view feature spaces (e.g. BoW-LDA). As an exam- ple, Cohen et al. (2015) experimented with a combination of BoW and MeSH features and showed that such composite features achieve statistical significant improvements over single-view fea- tures (e.g. BoW features alone). Similarly, Howard et al. (2016) re- ported that BoW-LDA composite features enhance the WSS@95% performance of an automatic prioritisation method by approxi- mately 4.4% when compared to single-view BoW features. Based upon this, in addition to the five baseline methods that extract single-view features, we report the WSS@95% performance of the following 5 composite features : BoW-LDA, BoW-SVD, BoW-LDA, BoW-PV, BoW-MeSH and BoW-SVD-LDA-PV.

Results

Hyper-parameter settings
In this section, we present experiments that we conducted to optimise the hyper-parameters and the network architecture of our supervised feature extraction method3 We do not perform any dataset-specific tuning of the hyper-parameters with the exception of the number of epochs required to train the DAEs. We show that the number of DAE epochs is sensitive to the size of the underly- ing review. Based on this, we use the Statins development review to fix the number of DAE epochs across the 18 review datasets (i.e. clinical and drug reviews) which are relatively small in size while the BPA development review is used to tune the number of DAE epochs across the 5 SWIFT review datasets which are larger

2 nactem.ac.uk/pvtopic
3 All experiments are performed using an NVIDIA TITAN Xp GPU.
Hyper-parameter	Value
size of minibatch (DAE)	32
dropout regularisation	0.7
number of training epochs (FF)	100
size of minibatch (FF)	128



in size. The remaining hyper-parameters, which are summarised in Table 3, are constant across all datasets. After optimising the num- ber of DAE epochs, we investigate the WSS@95% performance of our method when using different network architectures.

Effect of number of DAE epochs
Figs. 3 a and b illustrate the WSS@95% performance of our method, i.e. DAE-FF, on an increasing number of DAE epochs across the two development reviews, namely Statins and BPA, respec- tively. We further report the WSS@95% performance of the BoW baseline method. With regard to the Statins development review, we observe that the DAE-FF method yields a maximum WSS@95% score of 0.566 when using 50 DAE epochs. However, the perfor- mance of the method substantially decreases for a larger number of epochs, e.g. WSS@95% of 0.549 and 0.489 when using 100 and
200 epochs, respectively.
The WSS@95% performance of the DAE-FF method shows a dif- ferent pattern on the larger BPA development review, when com- pared to the performance recorded on the smaller Statins review. Here, the performance of the method continuously improves as the number of DAE epochs increases. An optimal WSS@95% score of 0.792 is observed when training the DAE components for 150 epochs, whereas for 200 epochs the performance of the method slightly decreases to 0.782.

Effect of model architecture
We next investigate the performance of our method when using different model architectures. More specifically, we compare the performance of a baseline model architecture (i.e. model_1) that does not exploit unsupervised pre-training using deep DAE com- ponents against 6 model architectures that use different combina- tions of the three DAE components (i.e. DAE_1, DAE_2 and DAE_3) to initialise the feed forward network. We further evaluate both single-branch model architectures that use a single DAE compo- nent (model_2, model_3 and model_4) and multi-branch architec- tures that use two (model_5 and model_6) or three (model_7) DAE components. With regard to the single-branch model architectures, we co-ordinate 4 fully connected layers of 1024 units each fol- lowing the single DAE component of the network. The two-branch model architectures consist of 5 fully connected layers: a) two fully connected layers of 1024 units which are parallel to each other and they are initialised by the two DAE components of the net- work, b) a wide fully connected layer of 2048 units (i.e. concatena- tion of the first two parallel layers) and c) two subsequent layers of 1024 units. Finally, our proposed three-branch model architec- ture (i.e. model_7) co-ordinates 6 fully connected layers: a) 3 par- allel layers which are initialised by the three DAE components, b) a wide fully connected layer of 3072 units and c) two layers of 1024 units.
Table 4 shows the WSS@95% performance of the 7 model archi- tectures on the Statins and BPA development reviews. It can be ob- served that model_1 (i.e. baseline architecture) obtains the lowest WSS@95% performance across the two development reviews. Our proposed three-branch model architecture, i.e. model_7, improves upon the performance of the baseline architecture by  ~ 10% to
~ 15%. Moreover, model_7 achieved a statistically significant im-


	


Fig. 3. WSS@95% performance of the proposed method (i.e. DAE-FF) on an increasing number of DAE epochs across the Statins and BPA development reviews. The figures also illustrate the WSS@95% performance of the BoW baseline method. The thick lines are average WSS@95% values. The bands surrounding the thick lines represent the 95% confidence interval of the mean WSS@95% values across 10 validation rounds.


Table 4
WSS@95% performance of 7 different network architectures of our method (i.e. model_1 to model_7) on the two development reviews. The superscript ∗∗ shows that the corresponding model obtained a statistically significant lower per- formance when compared to the WSS@95% performance of model_7 according to a two-tailed paired t-test at p < 0.01 level. The superscript ∗ denotes statistically significant difference at p < 0.05 level.
Model	DAE_1	DAE_2	DAE_3	WSS@95% (1024,512,	 (2048,512,	 (3072,512,
256,512,1024)  256,512,2048)  256,512,3072)  Statins	BPA
model_1 —	 —	—	0.414∗∗ 0.687∗∗ model_2 √	 —	—	0.514∗∗ 0.709∗∗ model_3 —	√	—	0.488∗∗ 0.697∗∗ model_4 —	 —	√	0.492∗∗ 0.703∗∗ model_5 √	√	—	0.534  0.786
The results show that the composite feature extraction meth- ods improved upon the performance of the BoW single-view base- line. Performance gains in terms of the average WSS@95% range be- tween ~ 1% to ~ 6%. The concatenation of LDA with BoW features (i.e. BoW-LDA) achieved the best average WSS@95% of 0.492 among the two-view composite baselines while the four-view composite method obtained a slightly higher average WSS@95% of 0.5 when compared to the BoW-LDA baseline. The DAE-FF method showed a superior WSS@95% score in 15 out of the 23 review datasets and a statistically significant improved performance over the 5 composite baselines in 7 datasets. Finally, our method increased the average WSS@95% score of the composite baselines by ~ 6% to ~ 11%.
Fig. 4 shows the average precision at recall level of 95% ob-





provement over the single-branch model architectures on both de- velopment reviews, although performance improvements over the two-branch model architectures were small and statistically in- significant in most cases.

Comparison with baseline methods
We evaluate our proposed three-branch DAE-FF method against 5 single-view baseline methods, i.e. BoW, SVD, LDA, PV and MeSH, on 23 review datasets. The results in Table 5 show that the DAE-FF method yielded an optimal WSS@95% performance in 16 out of the 23 review datasets, while the performance improvements over the baseline methods were statistically significant in 9 datasets. More- over, our method obtained the best overall performance, i.e. the average WSS@95% scores across all 23 datasets, and improved upon the performance of the 5 baselines by ~ 10% to ~ 28%. The MeSH baseline method achieved the lowest performance, because MeSH terms are sparsely distributed across the different citations. The re- maining 4 single-view baselines produced approximately the same average WSS@95% performance.
Table 6 compares the performance of the DAE-FF method against 5 composite feature extraction methods. The composite baselines augment the BoW feature space with additional features derived by different single-view feature extraction methods by column-wide concatenation. More specifically, we experiment with a column-wide concatenation of two single-view feature spaces (i.e. BoW-SVD, BoW-LDA, BoW-PV, BoW-MeSH) and a column-wide concatenation of four single-view features spaces (BoW-SVD-LDA- PV).



formance with the exception of the BoW-MeSH that shows a sub- stantially lower average precision at recall level of 95% of ~ 13%.

Discussion

The results that we obtained demonstrate that our neural network-based feature extraction method substantially reduced the screening workload of 23 systematic reviews by approximately 56%. However, the workload savings varied across the 23 reviews from a low WSS@95% score of ~ 9% on the Oral Hypoglycemics review to a higher WSS@95% score of ~ 84% on the PFOA/PFOS review. Moreover, we observed a weak correlation (R2 = 0.279) be- tween the WSS@95% performance and the size of the correspond- ing review dataset which was statistically insignificant (p = .197). This indicates that our method can obtain meaningful workload savings on both small and large review datasets.
According to Cohen et al. (2006), a significant and meaningful workload saving should be at least 10% in terms of the WSS@95% metric. This stems from the fact that the citation screening pro- cess of a systematic review, when conducted manually, requires on average 332 person hours to be completed. Therefore, a WSS@95% score of 10%, i.e. 10% of correctly excluded citations + 5% of incor- rectly excluded citations, results in a workload reduction of ~ 50 person hours, which according to expert reviewers is a signifi- cant reduction of their citation screening labour. The experiments that we conducted showed that our proposed feature extraction method yields significant workload savings of at least 10% in 22 out of 23 review datasets and thus it could be potentially used in


Table 5
WSS@95% performance of our method against 5 single-view feature extraction baselines. WSS@95% scores are averages across 10 validation runs for each of the 23 review datasets. The superscript ∗∗ shows that the DAE-FF method achieved a statistically significant better performance according to a two-tailed paired t-test over all 5 baseline methods at p <
0.01 level. The superscript ∗ denotes statistically significant improvements over the 5 baselines at p < 0.05 level.

Dataset	BoW	SVD	LDA	PV	MeSH	DAE-FF
COPD	0.458	0.605	0.555	0.633	—	0.666
Proton Beam	0.746	0.722	0.787	0.709	—	0.816∗∗
Micro Nutrients	0.510	0.597	0.430	0.590	—	0.662∗
ACEInhibitors	0.752	0.791	0.548	0.708	0.375	0.787
ADHD	0.744	0.712	0.485	0.481	0.567	0.665
Antihistamines	0.048	0.053	0.042	0.211	0.192	0.310
Atypical Antipsychotics	0.136	0.038	0.076	0.150	0.199	0.329∗∗
Beta Blockers	0.470	0.455	0.507	0.130	0.237	0.587
Calcium Channel Blockers	0.177	0.262	0.234	0.169	0.130	0.424∗∗
Estrogens	0.288	0.292	0.360	0.271	0.238	0.397
NSAIDs	0.719	0.698	0.569	0.593	0.331	0.723
Opioids	0.304	0.251	0.350	0.472	0.116	0.533
Oral Hypoglycemics	0.081	0.046	0.106	0.055	0.065	0.095
Proton PumpInhibitors	0.239	0.299	0.293	0.503	0.323	0.400
Skeletal Muscle Relaxants	0.102	0.186	0.148	0.345	0.050	0.286
Statins	0.309	0.306	0.415	0.293	0.236	0.566∗∗
Triptans	0.417	0.356	0.331	0.295	0.241	0.310
Urinary Incontinence	0.291	0.504	0.443	0.451	0.220	0.531
PFOA/PFOS	0.773	0.794	0.797	0.833	0.405	0.848∗∗
Bisphenol A (BPA)	0.591	0.709	0.702	0.629	0.631	0.793∗∗
Transgenerational	0.619	0.579	0.612	0.542	0.432	0.707∗∗
Fluoride and neurotoxicity	0.719	0.843	0.847	0.828	—	0.799
Neuropathic pain	0.471	0.428	0.534	0.442	—	0.608∗∗
Average (all datasets)	0.433	0.458	0.442	0.449	0.277	0.564
Table 6
WSS@95% performance of our method against 5 composite feature extraction methods (i.e. column-wide concatenation of different single-view feature spaces).
Dataset	BoW-SVD	BoW-LDA	BoW-PV	BoW-MeSH	BoW-SVD-LDA-PV	DAE-FF
COPD	0.598	0.609	0.599	—	0.640	0.666
Proton Beam	0.734	0.778	0.733	—	0.772	0.816∗∗
Micro Nutrients	0.568	0.416	0.574	—	0.607	0.662∗∗
ACEInhibitors	0.798	0.801	0.798	0.773	0.768	0.787
ADHD	0.719	0.624	0.719	0.738	0.633	0.665
Antihistamines	0.053	0.229	0.054	0.273	0.253	0.310
Atypical Antipsychotics	0.042	0.152	0.040	0.134	0.148	0.329∗∗
Beta Blockers	0.469	0.532	0.468	0.552	0.499	0.587
Calcium Channel Blockers	0.249	0.308	0.250	0.398	0.291	0.424
Estrogens	0.297	0.300	0.295	0.408	0.293	0.397
NSAIDs	0.699	0.684	0.698	0.595	0.692	0.723
Opioids	0.256	0.318	0.255	0.332	0.296	0.533
Oral Hypoglycemics	0.042	.114	0.043	0.112	0.109	0.095
Proton PumpInhibitors	0.304	0.302	0.305	0.252	0.345	0.400
Skeletal Muscle Relaxants	0.182	0.465	0.184	0.318	0.435	0.286
Statins	0.316	0.364	0.311	0.252	0.398	0.566∗∗
Triptans	0.366	0.437	0.361	0.241	0.445	0.434
Urinary Incontinence	0.500	0.381	0.504	0.426	0.362	0.531
PFOA/PFOS	0.819	0.833	0.796	0.815	0.826	0.848∗∗
Bisphenol A (BPA)	0.759	0.775	0.690	0.717	0.711	0.758
Transgenerational	0.598	0.646	0.576	0.641	0.644	0.707∗∗
Fluoride and neurotoxicity	0.835	0.778	0.835	—	0.849	0.799
Neuropathic pain	0.484	0.472	0.441	—	0.477	0.608∗∗
Average (all datasets)	0.465	0.492	0.458	0.450	0.500	0.564

practical application scenarios for accelerating the citation screen- ing task of systematic reviews.
It should further be noted that the workload reduction (i.e. WSS@95% score) achieved by our method is relative to the size of the underlying review dataset. As an example, the DAE-FF method obtained approximately the same WSS@95% performance of 0.7 on both the NSAIDs and the Transgenerational dataset. How- ever, the validation sample of the Transgenerational dataset con- sists of 24,318 citations and it is substantially larger than the vali- dation sample of the NSAIDs dataset (196 citations). In practice this means that a WSS@95% score of 0.7 is equivalent to a workload re- duction of 18,238 citations, which are automatically excluded from the Transgenerational review, while a WSS@95% score of 0.7 trans-


lates to a workload reduction of only 147 automatically excluded citations for the NSAIDs dataset.

Study limitations and future work

A potential limitation of our proposed method, which also applies to previous automatic screening methods, is that the WSS@95% metric assumes that an optimal cut-off threshold, i.e. the minimum value of the ranked list that discriminates higher ranked eligible studies from lower ranked ineligible studies, is pre-defined and fixed at 95% recall. However, in practical scenar- ios such a threshold value is diﬃcult to define, considering that the optimal cut-off threshold varies greatly across different re-


Acknowledgements

This work has been carried out as part of the TYPHON Project, which has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement No. 780251. Our research was further supported by the NVIDIA Corporation through the donation of a Titan GPU card.

References





Fig. 4. Average (all datasets) precision@95% recall of our method against 5 compos- ite feature extraction methods.


views (Howard et al., 2016). Here, threshold estimation techniques, such as the S-D rank optimisation (Arampatzis, Kamps, & Robert- son, 2009), can be used to approximate an optimal threshold value. A second limitation of our method is that the underlying neural network-based feature extraction method is trained independently for each systematic review dataset. As an example, in our exper- iments we produced 23 neural network models corresponding to the 23 review datasets. However, different systematic reviews may share one or more more eligibility criteria (e.g. if included studies are randomised control trials) and thus learned document features could be applied to different reviews. As future work, we plan to investigate the use of domain adaptation and transfer learning in order to domain adapt a single feature extraction model across
multiple reviews.


Conclusions

In this paper, we have presented a text classification method to accelerate the citation screening process of systematic reviews. The method aims to minimise the human workload involved in citation screening so that human reviewers need to manually label only a subset of the citations, while the remaining unlabelled citations are automatically labelled by the text classification method.
We have demonstrated that by initialising the feed forward neural network using multiple denoising autoencoders of varying dimensionality we can improve upon the performance of our fea- ture extraction method. We have further performed a number of experiments to assess the performance of our method across 23 publicly available systematic review datasets. It was shown that for 22 out of 23 review datasets the proposed method achieved signif- icant workload savings on at least 10%, while in several cases our method yielded a statistically significantly better performance over 10 baseline feature extraction methods.

Declaration of Compecting Intereest

We confirm that we have no conflict of interest to declare.


CRediT authorship contribution statement

Georgios Kontonatsios: Conceptualization, Methodology, Soft- ware, Writing - original draft, Formal analysis, Writing - review & editing. Sally Spencer: Supervision, Writing - review & editing. Peter Matthew: Software, Validation, Writing - review & editing. Ioannis Korkontzelos: Conceptualization, Methodology, Software, Supervision, Writing - review & editing.

Adeva, J. G., Atxa, J. P., Carrillo, M. U., & Zengotitabengoa, E. A. (2014). Automatic text classification to support systematic reviews in medicine. Expert Systems with Ap- plications, 41(4), 1498–1508.
Arampatzis, A., Kamps, J., & Robertson, S. (2009). Where to stop reading a ranked list?: Threshold optimization using truncated score distributions. In Proceedings of the 32nd international ACM SIGIR conference on research and development in information retrieval (pp. 524–531). ACM.
Bandaru, S., Ng, A. H., & Deb, K. (2017). Data mining methods for knowledge dis- covery in multi-objective optimization: Part a-survey. Expert Systems with Appli- cations, 70, 139–159.
Bastian, H., Glasziou, P., & Chalmers, I. (2010). Seventy-five trials and eleven system- atic reviews a day: How will we ever keep up? PLoS Medicine, 7(9), e1000326.
Bekhuis, T., & Demner-Fushman, D. (2012). Screening nonrandomized studies for medical systematic reviews: A comparative study of classifiers. Artificial Intel- ligence in Medicine, 55(3), 197–207.
Bird, S., & Loper, E. (2004). Nltk: the natural language toolkit. In Proceedings of the ACL 2004 on interactive poster and demonstration sessions (p. 31). Association for Computational Linguistics.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan), 993–1022.
Cambria, E. (2016). Affective computing and sentiment analysis. IEEE Intelligent Sys- tems, 31(2), 102–107.
Cock, P. J., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., . . . Wilczyn- ski, B., et al. (2009). Biopython: Freely available python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11), 1422–1423.
Cohen, A. M. (2008). Optimizing feature representation for automated systematic review work prioritization. In AMIA annual symposium proceedings: vol. 2008 (p. 121). American Medical Informatics Association.
Cohen, A. M., Ambert, K., & McDonagh, M. (2012). Studying the potential impact of automated document classification on scheduling a systematic review update. BMC Medical Informatics and Decision Making, 12(1).
Cohen, A. M., Hersh, W. R., Peterson, K., & Yen, P.-Y. (2006). Reducing workload in systematic review preparation using automated citation classification. Journal of the American Medical Informatics Association, 13(2), 206–219.
Cohen, A. M., Smalheiser, N. R., McDonagh, M. S., Yu, C., Adams, C. E., Davis, J. M., & Yu, P. S. (2015). Automated confidence ranked classification of randomized con- trolled trial articles: An aid to evidence-based medicine. Journal of the American Medical Informatics Association, 22(3), 707–717.
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., & Bengio, S. (2010). Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb), 625–660.
Forman, G. (2003). An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3(Mar), 1289–1305.
Frunza, O., Inkpen, D., & Matwin, S. (2010). Building systematic reviews using auto- matic text classification techniques. In Proceedings of the 23rd international con- ference on computational linguistics (pp. 303–311). Association for Computational Linguistics.
Greenhalgh, T., Howick, J., & Maskrey, N. (2014). Evidence based medicine: A move- ment in crisis? BMJ, 348, g3725.
Hashimoto, K., Kontonatsios, G., Miwa, M., & Ananiadou, S. (2016). Topic detection using paragraph vectors to support active learning in systematic reviews. Journal of Biomedical Informatics, 62, 59–65.
Higgins, J., & Green, S. (2011). Cochrane handbook for systematic reviews of interven- tions version 5.1.0. The Cochrane Collaboration.
Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504–507.
Howard, B. E., Phillips, J., Miller, K., Tandon, A., Mav, D., Shah, M. R., . . . Thayer, K. (2016). SWIFT-review: A text-mining workbench for systematic re- view. Systematic Reviews, 5(1).
Kanoulas, E., Li, D., Azzopardi, L., & Spijker, R. (2017). Clef 2017 technologically as- sisted reviews in empirical medicine overview. In CEUR workshop proceedings: vol. 1866 (pp. 1–29).
Khabsa, M., Elmagarmid, A., Ilyas, I., Hammady, H., & Ouzzani, M. (2016). Learning to identify relevant studies for systematic reviews using random forest and ex- ternal information. Machine Learning, 102(3), 465–482.
Kim, S., & Choi, J. (2012). Improving the performance of text categorization models used for the selection of high quality articles. Healthcare Informatics Research, 18(1), 18.
Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In Proceedings of the 31st International conference on machine learning (icml-14) (pp. 1188–1196).
Lipscomb, C. E. (2000). Medical subject headings (mesh). Bulletin of the Medical Li- brary Association, 88(3), 265.



Marshall, C., & Brereton, P. (2013). Tools to support systematic literature reviews in software engineering: A mapping study. In Empirical software engineering and measurement, 2013 ACM/IEEE international symposium on (pp. 296–299). IEEE.
Matwin, S., Kouznetsov, A., Inkpen, D., Frunza, O., & O’blenis, P. (2010). A new algo- rithm for reducing the workload of experts in performing systematic reviews. Journal of the American Medical Informatics Association, 17(4), 446–453.
McCallum, A. K. (2002). Mallet: A machine learning for language toolkit
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed rep- resentations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111–3119).
Miron´ czuk, M. M., & Protasiewicz, J. (2018). A recent overview of the state-of-the-art elements of text classification. Expert Systems with Applications, 106, 36– 54.
Miwa, M., Thomas, J., O’Mara-Eves, A., & Ananiadou, S. (2014). Reducing systematic review workload through certainty-based screening. Journal of Biomedical Infor- matics, 51, 242–253.
Mo, Y., Kontonatsios, G., & Ananiadou, S. (2015). Supporting systematic reviews us- ing LDA-based document representations. Systematic Reviews, 4(1).
Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 807–814).
O’Mara-Eves, A., Thomas, J., McNaught, J., Miwa, M., & Ananiadou, S. (2015). Using text mining for study identification in systematic reviews: A systematic review of current approaches. Systematic Reviews, 4(1), 5.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(Oct), 2825–2830.
Porter, M. F. (2001). Snowball: A language for stemming algorithms.
Sethi, S., & Dixit, A. (2015). Design of personalised search system based on user interest and query structuring. In 2015 2nd international conference on computing for sustainable global development (INDIACom) (pp. 1346–1351). IEEE.
Shojania, K. G., Sampson, M., Ansari, M. T., Ji, J., Doucette, S., & Moher, D. (2007). How quickly do systematic reviews go out of date? A survival analysis. Annals of Internal Medicine, 147(4), 224.
Smiley, D., Pugh, E., Parisa, K., & Mitchell, M. (2015). Apache solr enterprise search server. Packt Publishing Ltd.
Trieschnigg, D., Pezik, P., Lee, V., De Jong, F., Kraaij, W., & Rebholz-Schuh- mann, D. (2009). Mesh up: Effective mesh text classification for improved doc- ument retrieval. Bioinformatics, 25(11), 1412–1418.
Vincent, P. (2011). A connection between score matching and denoising autoen- coders. Neural Computation, 23(7), 1661–1674.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec), 3371–3408.
Volmink, J., Siegfried, N., Robertson, K., & Gülmezoglu, A. M. (2004). Research syn- thesis and dissemination as a bridge to knowledge management: The cochrane collaboration. Bulletin of the World Health Organization, 82, 778–783.
Wall, M. E., Rechtsteiner, A., & Rocha, L. M. (2003). Singular value decomposition and principal component analysis. In A practical approach to microarray data analysis (pp. 91–109). Springer.
Wallace, B. C., Trikalinos, T. A., Lau, J., Brodley, C. E., & Schmid, C. H. (2010). Semi-au- tomated screening of biomedical citations for systematic reviews. BMC Bioinfor- matics, 11(1), 55.
Wang, S., & Manning, C. D. (2012). Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th annual meeting of the associa- tion for computational linguistics: Short papers-volume 2 (pp. 90–94). Association for Computational Linguistics.
Wei, J., He, J., Chen, K., Zhou, Y., & Tang, Z. (2017). Collaborative filtering and deep learning based recommendation system for cold start items. Expert Systems with Applications, 69, 29–39.
Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method. arXiv:1212.5701.
