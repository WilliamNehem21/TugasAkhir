

Electronic Notes in Theoretical Computer Science 253 (2009) 73–82
www.elsevier.com/locate/entcs

The Kantorovich Metric in Computer Science: A Brief Survey
Yuxin Deng1
Shanghai Jiao Tong University, China
Wenjie Du2
Shanghai Normal University, China

Abstract
In contrast to its wealth of applications in mathematics, the Kantorovich metric started to be noticed in computer science only in recent years. We give a brief survey of its applications in probabilistic concurrency, image retrieval, data mining, and bioinformatics. This paper highlights the usefulness of the Kantorovich metric as a general mathematical tool for solving various kinds of problems in rather unrelated domains.
Keywords: Kantorovich metric, probabilistic concurrency, information retrieval, bioinformatics.


Introduction
The transportation problem has been playing an important role in linear program- ming due to its general formulation and methods of solution. The original trans- portation problem, formulated by the French mathematician G. Monge in 1781 [21], consists of finding an optimal way of shovelling a pile of sand into a hole of the same volume. In the 1940s, the Russian mathematician and economist L.V. Kantorovich, who was awarded a Nobel prize in economics in 1975 for the theory of optimal allocation of resources, gave a relaxed formulation of the problem and proposed a variational principle for solving the problem [16]. Unfortunately, Kantorovich’s work went unrecognized during a long period of time. The later known Kantorovich metric has appeared in the literature under different names, because it has been rediscovered historically several times from different perspectives. Many metrics

1 Email: yuxindeng@sjtu.edu.cn
2 Email: wenjiedu@shnu.edu.cn

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.10.006

known in measure theory, ergodic theory, functional analysis, statistics, etc. are special cases of the general definition of the Kantorovich metric [34]. The elegance of the formulation, the fundamental character of the optimality criterion, as well as the wealth of applications, which keep arising, place the Kantorovich metric in a prominent position among the mathematical works of the 20th century. In addition, this formulation can be computed in polynomial time [22], which is an appealing feature for its use in solving applied problems. For example, it is widely used to solve a variety of problems in business and economy such as market distribution, plant location, scheduling problems etc. However, as far as we know the metric at- tracted the attention of computer scientists only in recent years. In this short paper, we give a brief survey of recent applications of the Kantorovich metric in computer science. In order to give the reader a feel for how the metric has been used, we take five examples from four different areas in computer science: probabilistic concur- rency, image retrieval, data mining, and bioinformatics. In some cases, the metric is directly used to compute similarities of the objects we are interested in; in the other cases, variations of the metric are adapted to meet our requirements. The purpose of this paper is to review some existing applications so as to highlight the usefulness of the Kantorovich metric as a general mathematical tool for solving various kinds of problems in rather unrelated domains.

Kantorovich metric
Roughly speaking, the Kantorovich metric provides a way of measuring the distance between two distributions. Of course, this requires first a notion of distance between the basic features that are aggregated into the distributions, which is often referred to as the ground distance. For example, in the case of color, the ground distance measures dissimilarity between individual colors. In other words, the Kantorovich metric defines a “lifted” distance, or dissimilarity, between two distributions of mass in a space that is itself endowed with a ground distance. For color, this means finding distances between image color distributions. There are a host of metrics available in the literature (see e.g. [13]) to quantify the distance between probability measures; see [24] for a comprehensive review of metrics in the space of probability measures. The Kantorovich metric has an elegant formulation and a natural interpretation in terms of the transportation problem.
We now recall the mathematical definition of the Kantorovich metric. Let (S, d) be a separable metric space. (This condition will be used by Theorem 2.4 below.)
Definition 2.1 Given any two Borel probability measures P and Q on S, the Kan- torovich distance between P and Q is defined by
K(P, Q) = sup  ∫ f dP − ∫ f dQ : ||f|| ≤ 1 .


where ||· || is the Lipschitz semi-norm defined by ||f|| = sup
|f (x)—f (y)|
for a

function f : S →R with R being the set of all real numbers.
x/=y d(x,y)

The Kantorovich metric has an alternative characterisation. We denote by P(S) the set of all Borel probability measures on S such that for all z ∈ S, if P ∈ P(S) then S d(x, z)P(x) < ∞. We write M (P, Q) for the set of all Borel probability measures on the product space S × S with marginal measures P and Q, i.e.  if
μ ∈ M (P, Q) then ∫y∈S dμ(x, y) = dP(x) and ∫x∈S dμ(x, y) = dQ(y) hold.
Definition 2.2 For P, Q ∈ P(S), we define the metric L as follows:
L(P, Q) = inf  ∫ d(x, y)dμ(x, y) : μ ∈ M (P, Q)  .

Lemma 2.3 If (S, d) is a separable metric space then K and L are metrics on
P(S).
The famous Kantorovich-Rubinstein duality theorem gives a dual representation of K in terms of L.
Theorem 2.4 (Kantorovich-Rubinstein [17]) If (S, d) is a separable metric space then for any two distributions P, Q ∈ P(S) we have K(P, Q) = L(P, Q).
In view of the above theorem, many papers in the literature directly take Def- inition 2.2 as the definition of the Kantorovich metric. Here we keep the original definition, but it is helpful to understand K by using L. Intuitively, a probability measure μ ∈ M (P, Q) can be understood as a transportation from one unit mass distribution P to another unit mass distribution Q. If the distance d(x, y) repre- sents the cost of moving one unit of mass from location x to location y then the Kantorovich distance gives the optimal total cost of transporting the mass of P to
Q. We refer the reader to Villani’s book [35] for an excellent exposition on the Kantorovich metric and the duality theorem.
Many problems in computer science only involve finite state spaces, so discrete distributions with finite supports are sometimes more interesting than continuous distributions. For two discrete distributions P and Q with finite supports {x1, ..., xn} and {y1, ..., ym}, respectively, minimizing the total cost of a discretized version of the transportation problem reduces to the following linear programming problem:


minimize	Σn

m j=1
μ(xi, yj)d(xi, yj)

subject to	• ∀1 ≤ i ≤ n : Σm
∀1 ≤ j ≤ m : Σn
μ(xi, yj) = P(xi)
μ(xi, yj) = Q(xj)

(1)

i=1
∀1 ≤ i ≤ n, 1 ≤ j ≤ m : μ(xi, yj) ≥ 0.

Since (1) is a special case of the discrete mass transportation problem, some well-known polynomial time algorithm like [22] can be employed to solve it, which is an attractive feature for computer scientists.

Applications in probabilistic concurrency
In this section we present two applications of the Kantorovich in probabilistic con- currency: one is in defining behavioural pseudometrics and the other in defining (bi-)simulations.

Behavioural pseudometrics
The Kantorovich metric has been used by van Breugel et al. for defining behavioural pseudometrics on fully probabilistic systems [30,33,29] and reactive probabilistic sys- tems [31,32,27,28]; and by Desharnais et al. for labelled Markov chains [8,10] and labelled concurrent Markov chains [9]; and later on by Ferns et al. for Markov deci- sion processes [11,12]; and by Deng et al. for action-labelled quantitative transition systems [4]. Given a pseudometric m on a finite set of states, a typical problem to measure similarities of the behaviour of probabilistic processes is how to lift m to be
a pseudometric mˆ on distributions over states 3 . An elegant definition of lifting is
via the Kantorovich metric. We illustrate the basic idea by considering the simple case of dealing with distributions over a finite state space.
Let P and Q be distributions on a finite set S of states. Suppose the distance m(s, t) between any two states s and t is bounded by 1. In [30] the distance mˆ (P, Q) is given by the value of the following linear programming problem:
maximize	Σs∈S (P(s) − Q(s))xs

subject to	• ∀s, t ∈ S : xs − xt ≤ m(s, t)
∀s ∈ S : 0 ≤ xs ≤ 1.
(2)

This problem can be dualized and then simplified to yield the following problem: minimize	Σs,t∈S ystm(s, t)

subject to	• ∀s ∈ S : Σt∈S yst = P(s)
∀t ∈ S :	s∈S yst = Q(t)
∀s, t ∈ S : yst ≥ 0.

(3)


Now (3) is in the same form as (1).

(Bi-)simulations
Given a state space S, a probabilistic bisimulation or simulation is a relation R over states. However, in many models a step of transition leads a state to a distribution and so when defining (bi-)simulations we need to lift R to be a relation R† over

3 To some extent, this is related to measuring the distances between quantum states, so it is reasonable to expect applications of the Kantorovich metric in quantum mechanics as well [37].

distributions. One way of lifting [15] is given as follows; there are other equivalent definitions (see e.g. [18,5,7,6]).

Definition 3.1 Given a relation R ⊆ S×S, we lift it to a relation R† ⊆ P(S)×P(S) by letting P R† Q whenever there exists a weight function w : S × S → [0, 1] such that
∀s ∈ S :	t∈S w(s, t) = P(s)
∀t ∈ S :	s∈S w(s, t) = Q(t)
∀s, t ∈ S : w(s, t) > 0 ⇒ s R t.

This way of lifting binary relations has an intrinsic connection with the lifting of pseudometrics via the Kantorovich metric given in (3), as stated by the next proposition which is a novel result of the paper.
Proposition 3.2 Let R be a binary relation and m a pseudometric on a state space
S satisfying
s R t	iff	m(s, t) = 0	(4)
for any s, t ∈ S. Then it holds that

P R† Q	iff	mˆ (P, Q) = 0 

for any distributions P, Q ∈ P(S).
Proof. Suppose P R† Q. From Definition 3.1 we know there is a weight function
w such that
∀s ∈ S :	t∈S w(s, t) = P(s)
∀t ∈ S :	s∈S w(s, t) = Q(t)
∀s, t ∈ S : w(s, t) > 0 ⇒ s R t.
By substituting w(s, t) for ys,t in (3), the three constraints there can be satisfied. For any s, t ∈ S we distinguish two cases:
either w(s, t) = 0 
or w(s, t) > 0. In this case we have s R t, which implies m(s, t) = 0 by (4).
Therefore, we always have w(s, t)m(s, t) = 0 for any s, t ∈ S.	Consequently,
s,t∈S w(s, t)m(s, t) = 0 and the optimal value of the problem in (3) must be 0, i.e. mˆ (P, Q) = 0, and the optimal solution is determined by w.
The above reasoning can be reversed to show that the optimal solution of (3) determines a weight function, thus mˆ (P, Q) = 0 implies P R† Q.	 

By the way, the lifting operation given in Definition 3.1 can also be characterised as a maximum flow problem in a network [1].

Application in image retrieval
The Earth Mover’s distance (EMD) was introduced by Rubner et al. [25,26] as an empirical way to measure color and texture similarities. It was shown to out- perform many other texture similarity measures when used for texture classifica- tion and segmentation [23].  Formally, it is defined for “signatures” of the form
{(x1, p1), ..., (xm, pm)}, where xi is the center of data cluster i and pi is the num-
ber of points in the cluster. Given two signatures P = {(x1, p1), ..., (xn, pn)} and Q = {(y1, q1), ..., (ym, qm)}, whose total masses may not be equal, the EMD is defined in terms of the value of the linear programming problem:


minimize	Σn

m j=1
fijd(xi, yj)

subject to	• ∀1 ≤ i ≤ n : Σm
∀1 ≤ j ≤ m : Σn
fij ≤ pi
fij ≤ qj


(5)


n i=1

m j=1
i=1
fij = min(Σn
pi, Σm

qj)

∀1 ≤ i ≤ n, 1 ≤ j ≤ m : fij ≥ 0.

where fij is the flow (the amount of earth moved) from cluster i to cluster j, and d(xi, yj) is some measure of dissimilarity between xi and yj, say the Euclidean distance in R. In the EMD terminology, the value of the objective function in (5) is the work required to move earth from one signature to another. Once the optimal
flow f∗ is found, the Earth Mover’s distance between P and Q is defined as


n
i=1
m
j=1
f∗ d(xi, yj)

EMD(P, Q) = 
n i=1
m	∗
j=1 ij
(6)


For signatures with the same total mass the EMD is a true metric on distributions, and it is exactly the same as the Kantorovich metric, as noticed in [19] 4 .
In [3], three other special forms of the Kantorovich metric were proposed to compare color histograms of images. Moreover, as a generalization of the problem of measuring image similarity, video clips can also be measured with the help of the Kantorovich metric, which turns out to be an effective technique [14].

Application in data mining
In [36] a Kantorovich distance based metric was proposed to compare clusterings. Given a dataset D = {x1, ..., xr}, suppose two clustering results Cls1 and Cls2 are obtained. They contain n and m clusters, respectively. Denote the n clusters in Cls1 by C1, ..., Cn, and the m clusters in Cls2 by C' , ..., C' .  Let the probability
1	m
matrix generated by Cls1 be P = (pij), where pij denotes the probability that

4 In [19] the Mallows metric is used, which is a special case of the Kantorovich metric.

object xi belongs to cluster Cj. Let the corresponding matrix generated by Cls2 be
Q = (qij).
A cluster Cj is characterized by the r-dimensional vector (p1j, p2j, ..., prj)T ,
denoted by ξj.	Similarly, denote the vector characterizing cluster C' by γj =
(q1j, q2j, ..., qr,j)T . To reflect the significance of each cluster for the purpose of comparison, we assign a weight to each cluster. Let the weight assigned to Cj be αj
with Σn	αj = 1, and those to C' be βj with Σm  βj = 1. So Cls1 corresponds to
the distribution P = {(ξ1, α1), ..., (ξn, αn)} and Cls2 to Q = {(γ1, β1), ..., (γm, βm)}.
The distance between Cls1 and Cls2 is the value of the following linear programming
problem:

minimize	Σn
m k=1
wjk Σr
|pij − qik|

subject to	• ∀1 ≤ j ≤ n : Σm
∀1 ≤ k ≤ m : Σn
wjk = αj
wjk = βk

(7)

j=1
∀1 ≤ j ≤ n, 1 ≤ k ≤ m : wjk ≥ 0. The clustering distance defined above is a Kantorovich distance.
Application in bioinformatics
An important problem in bioinformatics is to determine similarities and dissimilar- ities among DNA sequences, which can be used to detect the structural signature of a genome as well as to identify phylogenetic relationships among different species. One approach to solving this problem is based on statistical analysis of large DNA sequences using distribution of DNA words, which is a simple yet effective statis- tical tool to capture information about structural patterns, and these can reveal biologically significant features in a DNA sequence (see e.g. [2]). A DNA sequence is formed using an alphabet of four letters {A, T, C, G} denoting four DNA bases: adenine, thymine, cytosine and guanine, respectively. A statistical summarization relies on various frequencies of DNA k-words, which are k-tuples formed via these four letters. Let k ≥ 1 and Wk denote the set of all possible k-words formed using the alphabet {A, T, C, G}. Clearly, the size of the set Wk is 4k. For a given DNA sequence and a word w ∈ Wk, let fw be the relative frequency of the word w in the sequence, where the words in the sequence may have one or more overlapping letters. For example, in a sequence like ATTCGGCA..., the first 4-word is ATTC, the second one is TTCG, the third one is TCGG and so on. The 4k-dimensional frequency vector (fw)w∈Wk constitutes a statistical summary of the given DNA se- quence, and we have  w∈W fw = 1 with fw ≥ 0 for all w ∈ Wk. A comparison between a pair of DNA sequences to judge their similarities and dissimilarities can be carried out by comparing their associated frequency vectors, say (fw)w∈Wk and (gw)w∈Wk . In [20] an appropriate metric for calculating the distance of two k-words is chosen. It is then lifted to be a metric over frequency vectors in terms of the Kan- torovich metric. This idea has been implemented in a tool and some encouraging experimental results have been obtained.

Concluding remarks
We have briefly surveyed some recent applications of the Kantorovich metric in computer science. In general, production planning problems spread over a large variety of research areas, but lead to the same mathematical problem, namely the transportation problem. Therefore, the Kantorovich metric will probably find many more applications in the future. For example, this can be envisaged in relevant areas such as knowledge representation, statistical clustering, data mining, information retrieval, bioinformatics etc., all of which demand appropriate ways of computing similarity/dissimilarity between objects.
Acknowledgement
We thank the anonymous referees for insightful comments on an earlier version of the paper. Deng would like to acknowledge the support of the National Natural Science Foundation of China (Grant No. 60703033).

References
C. Baier, B. Engelen, and M. E. Majster-Cederbaum. Deciding bisimilarity and similarity for probabilistic processes. Journal of Computer and System Sciences, 60(1):187–231, 2000.
P. Chaudhuri and S. Das. Statistical analysis of large DNA sequences using distribution of DNA words.
Current Science, 80(9):1161–1166, 2001.
K. Chen, S. Demko, and R. Xie. Similarity-based retrieval of images using color histograms. In Storage and Retrieval for Image and Video Databases VII, volume 3656 of SPIE, pages 643–652, 1999.
Y. Deng, T. Chothia, C. Palamidessi, and J. Pang. Metrics for action-labelled quantitative transition systems. Electronic Notes in Theoretical Computer Science, 153(2):79–96, 2006.
Y. Deng and W. Du. Probabilistic barbed congruence. Electronic Notes in Theoretical Computer Science, 190(3):185–203, 2007.
Y. Deng, R. van Glabbeek, M. Hennessy, and C. Morgan. Characterising testing preorders for finite probabilistic processes. Logical Methods in Computer Science, 4(4:4):1–33, 2008.
Y. Deng, R. van Glabbeek, M. Hennessy, C. Morgan, and C. Zhang. Characterising testing preorders for finite probabilistic processes. In Proceedings of the 22nd Annual IEEE Symposium on Logic in Computer Science, pages 313–325. IEEE Computer Society, 2007.
J. Desharnais, R. Jagadeesan, V. Gupta, and P. Panangaden. Metrics for labeled Markov systems. In Proceedings of the 10th International Conference on Concurrency Theory, volume 1664 of Lecture Notes in Computer Science, pages 258–273. Springer-Verlag, 1999.
J. Desharnais, R. Jagadeesan, V. Gupta, and P. Panangaden. The metric analogue of weak bisimulation for probabilistic processes. In Proceedings of the 17th Annual IEEE Symposium on Logic in Computer Science, pages 413–422. IEEE Computer Society, 2002.
J. Desharnais, R. Jagadeesan, V. Gupta, and P. Panangaden. Metrics for labelled Markov processes.
Theoretical Computer Science, 318(3):323–354, 2004.
N. Ferns, P. Panangaden, and D. Precup. Metrics for finite Markov decision processes. In Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence, pages 162–169. AUAI Press, 2004.
N. Ferns, P. Panangaden, and D. Precup. Metrics for Markov decision processes with infinite state spaces. In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, pages 201–208. AUAI Press, 2005.
A. L. Gibbs and F. E. Su. On choosing and bounding probability metrics. International Statistical Review, 70(3):419–435, 2002.


C. Grana, D. Borghesani, and R. Cucchiara. Video shots comparison using the Mallows distance. In Proceedings of the 18th International Conference on Database and Expert Systems Applications, pages 49–53. IEEE Computer Society, 2007.
B. Jonsson and K. Larsen. Specification and refinement of probabilistic processes. In Proceedings of the 6th Annual IEEE Symposium on Logic in Computer Science, pages 266–277. Computer Society Press, 1991.
L. V. Kantorovich. On the translocation of masses. Doklady Akademii Nauk SSSR, 37(7-8):227–229, 1942.
L. V. Kantorovich and G. S. Rubinshtein. On a space of totally additive functions. Vestn Lening. Univ., 13(7):52–59, 1958.
K. G. Larsen and A. Skou. Bisimulation through probabilistic testing. Information and Computation, 94(1):1–28, 1991.
E. Levina and P. J. Bickel. The Earth Mover’s distance is the Mallows distance: Some insights from statistics. In Proceedings of the 8th International Conference On Computer Vision, volume 2, pages 251–256. IEEE Computer Society, 2001.
X. Li and Y. Deng. Automatic measurement of large DNA sequences, 2009. In preparation.
G. Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Academie des Science de Paris, page 666, 1781.
J. B. Orlin. A faster strongly polynomial minimum cost flow algorithm. In Proceedings of the 20th ACM Symposium on the Theory of Computing, pages 377–387. ACM, 1988.
J. Puzicha, Y. Rubner, C. Tomasi, and J. Buhmann. Empirical evaluation of dissimilarity measures for color and texture. In Proceedings of the 6th International Conference On Computer Vision, pages 1165–1173. IEEE Computer Society, 1999.
S. Rachev. Probability Metrics and the Stability of Stochastic Models. Wiley New York, 1991.
Y. Rubner, C. Tomasi, and L. Guibas. The Earth Mover’s distance as a metric for image retrieval. Technical Report STAN-CS-TN-98-86, Department of Computer Science, Stanford University, 1998.
Y. Rubner, C. Tomasi, and L. Guibas. A metric for distributions with applications to image databases. In Proceedings of the 5th International Conference on Computer Vision, pages 59–66. IEEE Computer Society, 1998.
F. van Breugel, C. Hermida, M. Makkai, and J. Worrell. An accessible approach to behavioural pseudometrics. In Proceedings of the 32nd International Colloquium on Automata, Languages and Programming, volume 3580 of Lecture Notes in Computer Science, pages 1018–1030. Springer, 2005.
F. van Breugel, C. Hermida, M. Makkai, and J. Worrell. Recursively defined metric spaces without contraction. Theoretical Computer Science, 380(1-2):143–163, 2007.
F. van Breugel, B. Sharma, and J. Worrell. Approximating a behavioural pseudometric without discount for probabilistic systems. In Proceedings of the 10th International Conference on Foundations of Software Science and Computational Structures, volume 4423 of Lecture Notes in Computer Science, pages 123–137. Springer, 2007.
F. van Breugel and J. Worrell. An algorithm for quantitative verification of probabilistic transition systems. In Proceedings of the 12th International Conference on Concurrency Theory, volume 2154 of Lecture Notes in Computer Science, pages 336–350. Springer-Verlag, 2001.
F. van Breugel and J. Worrell. Towards quantitative verification of probabilistic transition systems. In Proceedings of the 28th International Colloquium Automata, Languages and Programming, volume 2076 of Lecture Notes in Computer Science, pages 421–432. Springer-Verlag, 2001.
F. van Breugel and J. Worrell. A behavioural pseudometric for probabilistic transition systems.
Theoretical Computer Science, 331(1):115–142, 2005.
F. van Breugel and J. Worrell. Approximating and computing behavioural distances in probabilistic transition systems. Theoretical Computer Science, 360(1-3):373–385, 2006.
A. Vershik. Kantorovich metric: Initial history and little-known applications. Journal of Mathematical Sciences, 133(4):1410–1417, 2006.
C. Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics. American Mathematical Society, 2003.
D. Zhou, J. Li, and H. Zha. A new Mallows distance based metric for comparing clusterings. In Proceedings of the Twenty-Second International Conference on Machine Learning, volume 119, pages 1028–1035. ACM, 2005.
K. Z˙ yczkowski and W. Slomczyn´ski. The Monge distance between quantum states. Journal of Physics A: Mathematical and General, 31(45):9095–9104, 1998.

A	Mathematical properties of the pseudometric mˆ
Given the ground distance function m, the pseudometric mˆ on distributions (cf.
Section 3.1) enjoys some simple but useful mathematical properties which we list below.
(Nonnegativity) mˆ (P, Q) ≥ 0, with mˆ (P, Q) = 0 if P = Q.
(Symmetry) mˆ (P, Q) = mˆ (Q, P).
(Triangle inequality) mˆ (P, R) ≤ mˆ (P, Q)+ mˆ (Q, R).
(Possibility of extension) mˆ (P, Q) = mˆ (P', Q') where dom(P') = dom(P) ∪ {s}
and P'(s) = 0, similarly for Q' with respect to Q.
(Convexity) For 0 ≤ p ≤ 1, we have

mˆ (p · P + (1 − p) · R, p · Q + (1 − p) · R) = p · mˆ (P, Q).
(Joint convexity) For 0 ≤ p ≤ 1, we have

mˆ (p · P + (1 − p) · Q, p · P' + (1 − p) · Q') ≤ p · mˆ (P, P')+ (1 − p) · mˆ (Q, Q').
