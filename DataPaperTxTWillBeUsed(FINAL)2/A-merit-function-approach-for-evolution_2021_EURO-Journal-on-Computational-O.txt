EURO Journal on Computational Optimization 9 (2021) 100001

		




A merit function approach for evolution strategies
Youssef Diouane1
ISAE-SUPAERO, UniversitÃ© de Toulouse, Toulouse Cedex 4 31055, France


a r t i c l e	i n f o	a b s t r a c t

	

Keywords:
Constrained optimization Derivative-free optimization Evolution strategy
Merit function Global convergence
In this paper, we extend a class of globally convergent evolution strategies to handle general constrained opti- mization problems. The proposed framework handles quantifiable relaxable constraints using a merit function approach combined with a specific restoration procedure. The unrelaxable constraints, when present, can be treated either by using the extreme barrier function or through a projection approach. Under reasonable as- sumptions, the introduced extension guarantees to the regarded class of evolution strategies global convergence properties for first order stationary constraints. Numerical experiments are carried out on a set of problems from the CUTEst collection as well as on known global optimization problems.





Introduction

In this paper, we are interested in constrained derivative-free opti- mization problems (Audet and Hare, 2017), i.e.,
min	ğ‘“ (ğ‘¥)
In Diouane et al. (2015b), the authors proposed a general glob- ally convergent framework for unrelaxable constraints using two differ- ent approaches. The first relies on techniques inspired from directional direct-search methods (Conn et al., 2009; Kolda et al., 2003), where one uses an extreme barrier function to prevent unfeasible displacements

s.t.	ğ‘¥ âˆˆ Î© = Î©
ğ‘ğ‘Ÿ
(1)
ğ‘¢ğ‘Ÿ
together with the possible use of directions that conform to the local geometry of the feasible region. The second approach was based on en-

ous. The feasible region Î© âŠ‚ â„ğ‘› of this problem includes two categories where the objective function f is assumed to be locally Lipschitz continu- of constraints (Le Digabel and Wild, 2015). The first, denoted by Î©qr and
known as quantifiable relaxable (QR) constraints, or soft constraints, is allowed to be violated during the optimization process and may need to be satisfied only approximately or asymptotically. Such a set of con- straints will be assumed, in the context of this paper, to be of the form:
Î©ğ‘ğ‘Ÿ = {ğ‘¥ âˆˆ â„ğ‘› âˆ€ğ‘– âˆˆ {1, â€¦ , ğ‘Ÿ}, ğ‘ğ‘–(ğ‘¥) â‰¤ 0},
egory of constraints, denoted by Î©ğ‘¢ğ‘Ÿ âŠ‚ â„ğ‘›, pools all unrelaxable (UR) where the functions ci are locally Lipschitz continuous. The second cat-
constraints (also known as hard constraints), for such constraints no violation is allowed and they require satisfaction during the entire op- timization process.
Evolution strategies (ESâ€™s) (Rechenberg, 1973) are evolutionary al- gorithms designed for global optimization in a continuous space, and that lead to promising results on practical optimization problems (Auger et al., 2009; Bouzarkouna, 2012; Rios and Sahinidis, 2010). In Diouane
certain number ğœ† of points (called offspring) are randomly generated in et al. (2015a,b), the authors dealt with a large class of ESâ€™s, where a each iteration, among which ğœ‡ â‰¤ ğœ† of them (called parents) are selected.
ESâ€™s have been growing rapidly in popularity and used for solving chal- lenging optimization problems (Auger et al., 2013; Hansen et al., 2010).

E-mail address: youssef.diouane@isae-supaero.fr
1 https://orcid.org/0000-0002-6609-7330
forcing all the generated sample points to be feasible, while using a
projection mapping approach. Both proposed strategies were compared to some of the best available solvers for minimizing a function with- out derivatives. The numerical results confirmed the competitiveness of the two approaches in terms of eï¬ƒciency as well as robustness. Moti- vated by the recent availability of massively parallel computing plat- forms, the authors in Diouane et al. (2016) proposed a highly parallel globally convergent ES (inspired by Diouane et al. (2015b)) adapted to the full-waveform inversion setting. By combining model reduction and ESâ€™s in a parallel environment, the authors contributed solving realistic instances of the full-waveform inversion problem.
In the context of ESâ€™s, many algorithms have been proposed in the literature to adapt ESâ€™s to solve constrained optimization problems (Coello, 0000). Coello (2002) and Kramer (2010) out- lined a comprehensive survey of the most popular constraints han- dling methods currently used with ESâ€™s. Recently, the authors in Atamna et al. (2018) proposed an adaptation of a class of ESâ€™s to han- dle QR constraints by using an augmented Lagrangian framework. The proposed approach was showed to enjoy good local and invariant con- vergence properties. To the best of our knowledge, all the ESâ€™s proposed suffer from the lack of global convergence guarantees when applied to general constrained optimization problems.
In the context of deterministic derivative-free optimization (DFO), only few works looked at both kinds (relaxable and unrelaxable) of con-


https://doi.org/10.1016/j.ejco.2020.100001  Received 24 August 2020; Accepted 25 August 2020
2192-4406/Â© 2020 The Author. Published by Elsevier Ltd on behalf of Association of European Operational Research Societies (EURO). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)



straints separately. For instance, Audet and Dennis Jr. (2009) outlined a globally convergent direct-search approach based on a progressive bar- rier, which combined an extreme barrier approach for unrelaxable con- straints and non-dominance filters (Fletcher and Leyffer, 2002) to han- dle QR constraints. More recently, the authors in Audet et al. (2018) ex- tended the progressive barrier approach, developed in Audet and Den- nis Jr. (2009), to cover the setting of a derivative-free trust-region method. Within the framework of directional direct-search methods, Gratton and Vicente (2014) proposed an alternative where one handles QR constraints by means of a merit function. Under the appropriate assumptions, the latter approach ensured global convergence by impos- ing a suï¬ƒcient decrease condition on a merit function combining infor- mation from both objective function and constraint violation. Another
size parameter is also possible. The latter increases or decreases depend- ing on the landscape of the objective function. One relevant instance of such an ES is covariance matrix adaptation ES (CMA-ES) (Hansen et al., 1995).
In Diouane et al. (2015a,b), the authors proposed a framework for making a class of ESâ€™s enjoying some global convergence properties while solving optimization problems possibly with UR constraints. In fact, in Diouane et al. (2015a), by imposing a suï¬ƒcient decreasing con- dition on the objective function value, the proposed algorithm moni-
tored the step size ğœk to ensure its convergence to zero (which leads
creasing condition is applied directly to the weighted mean ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ of then to the existence of a stationary point). The imposed suï¬ƒcient de-
the new parents. By suï¬ƒcient decreasing condition we mean ğ‘“ (ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ ) â‰¤

two-phases derivative-free approach was proposed in MartÃ­nez and So-
ğ‘“ (ğ‘¥ ) âˆ’ ğœŒ(ğœ ), where ğœŒ
ğ‘˜+1

ğ‘˜	ğ‘˜
( Â·) is a forcing function (Kolda et al., 2003), i.e., a

bral (2013) to specifically handle the case where finding a feasible point is easier than minimizing the objective function.
In this paper, inspired by the merit function approach for direct search methods (Gratton and Vicente, 2014), we propose to adapt a class of ES algorithms (as proposed in Diouane et al. (2015b)) to handle both QR and unrelaxable constraints. The class of ES algorithms ob- tained relies essentially on a merit function (eventually with a restora- tion procedure) to decide and control the distribution of the offspring
points. The merit function is a standard penalty-based function that has
positive, nondecreasing function satisfying ğœŒ(ğœ)/ğœ â†’ 0 when ğœ â†’ 0. To
handle UR constraints (Diouane et al., 2015b), one starts with a feasible
iterate x0 and then aviods stepping outside the feasible region by means of a barrier approach. In this context, the suï¬ƒcient decrease condition is
applied not to f but to the extreme barrier function ğ‘“Î©ğ‘¢ğ‘Ÿ associated with f
with respect to the constraints set Î©ur (Audet and Dennis Jr., 2006) (also
known as the death penalty function in the terminology of evolutionary
algorithms), which is defined by:

already been proposed in the context of ES (Coello, 2002). The main advantage of the proposed approach is to ensure a form of global con-
ğ‘“Î©ğ‘¢ğ‘Ÿ
ğ‘“ (ğ‘¥)	if ğ‘¥ âˆˆ Î©ğ‘¢ğ‘Ÿ,
+âˆ	otherwise.

vergence. Namely, under reasonable assumptions, this paper presents the first globally convergent ES framework handling both QR and UR constraints.
The proposed convergence theory generalizes the ES framework in Diouane et al. (2015b) by including QR constraints, all in the spirit of the proposed merit function for directional direct search meth- ods (Gratton and Vicente, 2014). The contribution of this paper is twofold. First, we propose an adaptation of the merit function approach algorithm to the ES setting, a detailed convergence theory of the pro- posed approach is given. Second, we provide a practical implementation and extensive tests on a set of problems from the CUTEst collection as well as on known global optimization problems. The performance of our proposed solver is compared to (a) the progressive barrier approach im- plemented in the NOMAD solver (Le Digabel, 2011), (b) the directional direct search method as proposed in Gratton and Vicente (2014) and
(c) an adaptation of a well known ES using an augmented Lagrangian approach to handle QR constraints (Atamna et al., 2018).
The paper is organized as follows. The proposed merit function ap- proach is given in Section 2 with a detailed description of the changes introduced in a class of ES algorithms in order to handle general con- straints. The convergence results of the adapted approach are then de- tailed in Section 3. In Section 4, we test the proposed algorithm on a set of problems from the CUTEst collection as well as on known global
The extreme barrier function is formally introduced in Audet and Hare (2017). The obtained ES approach is detailed in (Diouane et al., 2015b, Algorithm 2.1). The global convergence of the algorithm is achieved by establishing that some type of directional derivatives are nonnegative at limit points of refining subsequences along certain limit directions (see Diouane et al., 2015b, Theorem 2.1).
The challenge of this paper consists in extending (Diouane et al., 2015b, Algorithm 2.1) to a globally convergent framework that takes into account both QR and UR constraints. The author acknowledges that a preliminary version of this work was produced during his PhD thesis (Diouane, 2014, Chapter 5). In what comes next, we define the merit function as follows:
ğ‘“ (ğ‘¥) + ğ›¿Ì„ğ‘”(ğ‘¥)	if ğ‘¥ âˆˆ Î©ğ‘¢ğ‘Ÿ,
+âˆ	otherwise.
where ğ›¿Ì„ > 0 is a given positive constant and g defines a constraint viola-
tion function with respect to QR constraints. The ğ“1-norm is commonly
used to define the constraint violation function, i.e.,
ğ‘Ÿ
ğ‘”(ğ‘¥) =  max{ğ‘ğ‘– (ğ‘¥), 0}.
ğ‘–=1
Other choices for g exist, for instance, using the ğ“2-norm i.e., ğ‘”(ğ‘¥) =

optimization problems. Finally, we make some concluding remarks in
ğ‘Ÿ
ğ‘–=1
max{ğ‘ğ‘– (ğ‘¥), 0}2 . We note that the same constraint violation function

Section 5.

A globally convergent ES for general constraints

This paper focuses on a class of ESâ€™s, denoted by (ğœ‡/ğœ‡W, ğœ†)-ES, which evolves a single candidate solution. In fact, at the ğ‘˜âˆ’th iteration, a new population ğ‘¦1 , â€¦ , ğ‘¦ğœ†  (called offspring) is generated around a
bol â€œ/ğœ‡W â€ in (ğœ‡/ğœ‡W, ğœ†)-ES specifies that ğœ‡ parents are â€œrecombinedâ€ into weighted mean xk of the previous parents (candidate solution). The sym- a weighted mean. The parents are selected as the ğœ‡ best offspring of the
previous iteration in terms of the objective function value. The muta-
g is used within the progressive barrier approach (Audet and Dennis Jr.,
2009), that was in turn inspired by the filter approach of Fletcher and Leyffer (2002). The merit function will be used to evaluate a trial step and hence decide whether such step will be accepted or not. The ex- tension of the globally convergent ES to a general constrained setting can be seen as a combination of two approaches, a feasible one where either the extreme barrier or a projection operator will be used to han- dle the UR constraints, and a merit function approach (possibly with a restoration procedure) to handle QR constraints.
iteration k, a trial mean parent ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ is computed as the weighted mean The description of the proposed framework is as follows. For a given

tion operator of the new offspring points is done by ğ‘¦ğ‘–
= ğ‘¥ğ‘˜ + ğœğ¸ğ‘† ğ‘‘ğ‘– ,
of the ğœ‡ best points in terms of the merit function value. The current

ğ‘˜+1
ğ‘˜	ğ‘˜

ğ‘– = 1, â€¦ , ğœ†, where ğ‘‘ğ‘– is drawn from a certain distribution Cğ‘˜ and ğœğ¸ğ‘†
trial mean parent will be considered as a â€œSuccessful pointâ€ if one of

ğ‘˜	ğ‘˜

long to the simplex set ğ‘† = {(ğœ”1, â€¦ , ğœ”ğœ‡) âˆˆ â„ğœ‡ âˆ¶ âˆ‘ğœ‡  ğ‘¤ğ‘– = 1, ğ‘¤ğ‘– â‰¥ 0, ğ‘– = is a chosen step size. The weights used to compute the means be-
is suï¬ƒciently away from the feasible region (i.e., g(xk ) > CğœŒ(ğœk ) for the two following situations occur. The first scenario arises when one

W )-ES adapts the sampling distribution to the land- scape of the objective function. An adaptation mechanism for the step
lation function g (i.e., ğ‘”Î©
ğ‘˜+1
(ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™) < ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ), where ğ‘”Î©
denotes the

ğ‘¢ğ‘Ÿ
ğ‘˜+1
ğ‘¢ğ‘Ÿ



extreme barrier function associated with g with respect to Î©ur). The sec-
(i.e.,     ğ‘€  (ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™)     <     ğ‘€  (ğ‘¥ğ‘˜ )    âˆ’    ğœŒ(ğœğ‘˜ )). ond situation occurs when the merit function is suï¬ƒciently decreased
Before checking whether the trial point is successful or not, the al- gorithm will try first to restore the feasibility or at least decrease the


Algorithm 1: A globally convergent ES for general constraints (Main).

Data: choose positive integers ğœ† and ğœ‡ such that ğœ† â‰¥ ğœ‡.Select an initial ğ‘¥0 âˆˆ Î©ğ‘¢ğ‘Ÿ and evaluate ğ‘“ (ğ‘¥0 ).Choose initial step
lengths ğœ0 , ğœğ¸ğ‘† > 0 and initialweights (ğœ”1, â€¦ , ğœ”ğœ‡) âˆˆ ğ‘†.

constraint violation if needed. The restoration process will be activated	0	0	0

trial point ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ suï¬ƒciently decreases the constraint violation function g if the current mean parent xk is far away from the feasible region and the
but not the merit function. More specifically, a â€œRestoration identiï¬erâ€ will be activated if one has
ğ‘”Î© (ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ ) < ğ‘”(ğ‘¥ ) âˆ’ ğœŒ(ğœ ) and  ğ‘”(ğ‘¥ ) > ğ¶ğœŒ(ğœ )
Choose constants ğ›½1 , ğ›½2 , ğ‘‘min , ğ‘‘max suchthat 0 < ğ›½1 â‰¤ ğ›½2 < 1
and 0 < ğ‘‘min < ğ‘‘max. Select a forcing function ğœŒ(â‹…)
1 for ğ‘˜ = 0, 1, â€¦ do
2	Step 1: compute new sample points
ğ‘Œğ‘˜+1 = {ğ‘¦1 , â€¦ , ğ‘¦ğœ† }such that

ğ‘¢ğ‘Ÿ
and
ğ‘˜+1
ğ‘˜	ğ‘˜
ğ‘˜	ğ‘˜
ğ‘–
ğ‘˜+1
= ğ‘¥ğ‘˜
+ ğœğ‘˜ ğ‘‘Ìƒğ‘– , ğ‘– = 1, â€¦ , ğœ†,

ğ‘€ (ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ ) â‰¥ ğ‘€ (ğ‘¥ğ‘˜ ).
The restoration algorithm will be left as far as progress on the re-
where the directions ğ‘‘Ìƒğ‘– â€™s are computed from the original ES
directions ğ‘‘ğ‘– â€™s(which in turn are drawn from a chosen ES
distribution Cğ‘˜ and scaled if necessary to

duction of the constraint violation can not be achieved all without any considerable increase in f. The complete description of the restoration
satisfyğ‘‘
min
â€– ğ‘˜â€–
â‰¤ ğ‘‘
max
).;

procedure is given in Algorithm 2.
As a result, the main iteration of the proposed merit function ap- proach can be divided into two steps: restoration and minimization. In
essentially the function ğ‘”Î©ğ‘¢ğ‘Ÿ ) while in the minimization step the objec- the restoration step the aim is to decrease infeasibility (by minimizing
tive function f is improved over a relaxed set of constraints by using the
ğ‘˜+1
points in ğ‘Œğ‘˜+1 = {ğ‘¦Ìƒğ‘˜+1 , â€¦ , ğ‘¦Ìƒğ‘˜+1 }by increasing order:
ğ‘€ (ğ‘¦Ìƒ1 ) â‰¤ â‹¯ â‰¤ ğ‘€ (ğ‘¦Ìƒğœ† ).Select the new parents as the best ğœ‡
offspring sample points{ğ‘¦Ìƒ1  , â€¦ , ğ‘¦Ìƒğœ‡  }, and compute their
weighted mean
ğœ‡

merit function M. The final approach obtained is described is given in
Algorithm 1.
ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™
ğ‘˜+1
ğ‘– ğ‘–
ğ‘˜ ğ‘˜+1
ğ‘–=1

For both algorithms (main and restoration), our global convergence
analysis will be performed independently of the choice of the distribu-
tion Cğ‘˜, the weights (ğœ”1 , â€¦ , ğœ”ğœ‡) âˆˆ ğ‘†, and the step size ğœğ¸ğ‘† . Therefore,
Step 3: if ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ âˆ‰ Î©ğ‘¢ğ‘Ÿ then
the iteration is declared unsuccessful;
else

ğ‘˜	ğ‘˜	ğ‘˜

the update of the ES parameters is left unspecified at this stage. How-	6
ever, the distribution Cğ‘˜ will be very useful in ensuring that a central	7
if ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ is a â€œRestoration identiï¬erâ€ then
enter Restoration (with ğ‘˜ğ‘Ÿ = ğ‘˜);

convergence assumption (related to the density of the directions in the unit sphere) can be seen as reasonable. In fact, by choosing the distribu-
tion Cğ‘˜ to be multivariate normal distribution with mean zero, one can
guarantee the density of the directions with a probability one. We will
give more details on that in the next section.
Note that we also impose bounds on all directions ğ‘‘ğ‘– used by the al-
gorithm. This modification is, however, very mild since the lower bound
a very large number. The construction of the set of directions {ğ‘‘Ìƒğ‘– } can dmin can be chosen very close to zero and the upper bound dmax set to
be done with respect to the local geometry of the UR constraints as pro- posed in (Diouane et al., 2015b, Section 2.2).
Global convergence

The convergence results presented in this section are in the vein of those first established for the merit function approach for direct search methods (Gratton and Vicente, 2014). For the convergence analysis, we will consider a sequence of iterations generated by Algorithm 1 with- out any stopping criterion. The analysis is organized depending on the number of times restoration is entered.
Case 1: the restoration algorithm is never entered after a certain order

a subsequence of the step sizes {ğœk } will converge to zero. In fact, due to When the restoration is entered finite times, one can guarantee that
the suï¬ƒcient decrease condition imposed on the merit function along the iterates (or in the constraints violation function if the iterates are
size (reduced at least by ğ›½2 for unsuccessful iterations), one can ensure suï¬ƒciently away from the feasible region) and the control on the step
the existence of a subsequence K of unsuccessful iterates driving the step size to zero.
Lemma 3.1. Let f be bounded below and assuming that the restoration is not entered after a certain order. Then,
lim inf ğœ = 0.
ğ‘˜â†’+âˆ
else
if ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ is a â€œSuccessful pointâ€ then
declare the iteration successful, set ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ , andğœğ‘˜+1 â‰¥ ğœğ‘˜ (for example ğœğ‘˜+1 = max{ğœğ‘˜ , ğœğ¸ğ‘† });
else
the iteration is declared unsuccessful;
end
end
end
if the iteration is declared unsuccessful then
17	set ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜ andğœğ‘˜+1 = ğ›½ğ‘˜ ğœğ‘˜ , with ğ›½ğ‘˜ âˆˆ (ğ›½1 , ğ›½2 );
18	end
19	Step 4: update the ES step length ğœğ¸ğ‘† , the distribution Cğ‘˜+1 , and the weights(ğœ”1 , â€¦ , ğœ”ğœ‡ ) âˆˆ ğ‘†;
20 end

Proof. Suppose that there exists a ğ‘˜Ì„ > 0 and ğœ > 0 such that ğœk > ğœ and
ğ‘˜ â‰¥ ğ‘˜Ì„ is a given iteration of Algorithm 1. If there is an infinite sequence
J1 of successful iterations after ğ‘˜Ì„ , this leads to a contradiction with the
fact that g and f are bounded below.
In fact, since ğœŒ is a nondecreasing positive function, one has
ğœŒ(ğœk ) â‰¥ ğœŒ(ğœ) > 0. Hence, if ğ‘”(ğ‘¥ğ‘˜+1 ) < ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ) and g(xk ) > CğœŒ(ğœk ) for all k âˆˆ J1, then
ğ‘”(ğ‘¥ğ‘˜+1 ) < ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœ),
there must exist an infinite subsequence J2âŠ†J1 of iterates for which which obviously contradicts the boundness below of g by 0. Thus
ğ‘€ (ğ‘¥ğ‘˜+1 ) < ğ‘€ (ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ). Hence,
ğ‘€ (ğ‘¥ğ‘˜+1 ) < ğ‘€ (ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœ)	for all ğ‘˜ âˆˆ ğ½2 .
Thus M(xk ) tends to -âˆ which is a contradiction, since both f and g are
bounded below.
The proof is thus completed if there is an infinite number of success- ful iterations. However, if no more successful iterations occur after a




Algorithm 2: A globally convergent ES for general constraints (Restoration).
Data: Start from ğ‘¥ğ‘˜ğ‘Ÿ âˆˆ Î©ğ‘¢ğ‘Ÿ given from the Main algorithm and
consider the same parameter as therein.
1 for ğ‘˜ = ğ‘˜ğ‘Ÿ , ğ‘˜ğ‘Ÿ + 1, ğ‘˜ğ‘Ÿ + 2, â€¦ do
2	Step 1: compute new sample points
sequences along certain limit directions (known as refining directions). By refining subsequence (Audet and Dennis Jr., 2006), we mean a subse- quence of unsuccessful iterates in the Main algorithm (see Algorithm 1) for which the step-size parameter converges to zero.
Assuming that h is Lipschitz continuous around the point xâˆ— âˆˆ Î©ur,
it is possible to use the Clarke-Jahn generalized derivative along a di-
rection d

ğ‘Œğ‘˜+1 = {ğ‘¦1
, â€¦ , ğ‘¦ğœ†
}such that
â„â—¦(ğ‘¥ ; ğ‘‘) =	lim sup
â„(ğ‘¥ + ğ‘¡ğ‘‘) âˆ’ â„(ğ‘¥) .

ğ‘–
ğ‘˜+1
= ğ‘¥ğ‘˜
+ ğœğ‘˜ ğ‘‘Ìƒğ‘– , ğ‘– = 1, â€¦ , ğœ†,
âˆ—	ğ‘¡
ğ‘¥â†’ğ‘¥âˆ— ,ğ‘¥âˆˆÎ©ğ‘¢ğ‘Ÿ

where the directions ğ‘‘Ìƒğ‘– â€™s are computed from the original ES directions ğ‘‘ğ‘– â€™s(which in turn are drawn from a chosen ES distribution Cğ‘˜ and scaled if necessary to
satisfyğ‘‘	â‰¤  ğ‘‘ğ‘–  â‰¤ ğ‘‘	);
ğ‘¡â†“0,ğ‘¥+ğ‘¡ğ‘‘âˆˆÎ©ğ‘¢ğ‘Ÿ
The latter derivative, proposed by Jahn (1996), can be seen as an adap- tation of the Clarke generalized directional derivative (Clarke, 1983) to
the presence of constraints. We note that definition of hâˆ˜(xâˆ— ; d) required

min
â€– ğ‘˜â€–
max
that ğ‘¥ + ğ‘¡ğ‘‘ âˆˆ Î©  for x âˆˆ Î©  arbitrarily close to xâˆ— which can be guar-

ğ‘¢ğ‘Ÿ  ğ‘˜+1
theoffspring points in ğ‘Œğ‘˜+1 = {ğ‘¦Ìƒğ‘˜+1 , â€¦ , ğ‘¦Ìƒğ‘˜+1 }by increasing
order: ğ‘”Î© (ğ‘¦Ìƒ1 ) â‰¤ â‹¯ â‰¤ ğ‘”Î© (ğ‘¦Ìƒğœ† ).Select the new parents as
anteed if d is hypertangent to Î©ur at xâˆ— . In what comes next, B(x; ğœ–) will
denote the closed ball formed by all points with a distance of no more

ğ‘¢ğ‘Ÿ
ğ‘˜+1
ğ‘¢ğ‘Ÿ
ğ‘˜+1	1	ğœ‡
than ğœ– to x.

the best ğœ‡ offspring sample points{ğ‘¦Ìƒğ‘˜+1 , â€¦ , ğ‘¦Ìƒğ‘˜+1 }, and
compute their weighted mean
ğœ‡
Definition 3.1. A vector ğ‘‘ âˆˆ â„ğ‘› is said to be a hypertangent vector to the set Î©ğ‘¢ğ‘Ÿ âŠ† â„ğ‘› at the point x in Î©ur if there exists a scalar ğœ– > 0 such

ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™
ğ‘˜+1
ğ‘– ğ‘–
ğ‘˜ ğ‘˜+1
ğ‘–=1
that
ğ‘¦ + ğ‘¡ğ‘¤ âˆˆ Î©ğ‘¢ğ‘Ÿ,  âˆ€ğ‘¦ âˆˆ Î©ğ‘¢ğ‘Ÿ âˆ© ğµ(ğ‘¥; ğœ–),  ğ‘¤ âˆˆ ğµ(ğ‘‘; ğœ–),  and  0 < ğ‘¡ < ğœ–.

Step 3: if ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ âˆ‰ Î©ğ‘¢ğ‘Ÿ then
the iteration is declared unsuccessful;
else
6	if ğ‘”(ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ ) < ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ )âª¯âª¯âª¯andâª¯âª¯âª¯âª¯ğ‘”(ğ‘¥ğ‘˜ ) > ğ¶ğœŒ(ğœğ‘˜ )
then
the iteration is declared successful, set ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ , andğœğ‘˜+1 â‰¥ ğœğ‘˜ (for example ğœğ‘˜+1 = max{ğœğ‘˜ , ğœğ¸ğ‘† });
else
the iteration is declared unsuccessful;
end
end
if the iteration is declared unsuccessful then
13	if ğ‘€ (ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ ) < ğ‘€ (ğ‘¥ğ‘˜ ) then
(starting  at  a  new  (ğ‘˜ + 1)âˆ’thiteration  using  ğ‘¥ğ‘˜+1  and 14	leave Restoration and return to the Main algorithm
The hypertangent cone to Î©ur at x, denoted by ğ‘‡ ğ» (ğ‘¥), is the set of all
ğ‘¢ğ‘Ÿ
hypertangent vectors to Î©ur at x. Then, the Clarke tangent cone to Î©ur at
x (denoted by ğ‘‡ ğ¶ğ¿(ğ‘¥)) can be defined as the closure of the hypertangent
ğ‘¢ğ‘Ÿ
cone ğ‘‡ ğ» (ğ‘¥). The Clarke tangent cone generalizes the notion of tangent
ğ‘¢ğ‘Ÿ
original  definition  ğ‘‘   âˆˆ  ğ‘‡  ğ¶ğ¿(ğ‘¥)  is  given  below. cone in Nonlinear Programming (Nocedal and Wright, 2006), and the
ğ‘¢ğ‘Ÿ
Definition 3.2. A vector ğ‘‘ âˆˆ â„ğ‘› is said to be a Clarke tangent vector to the set Î©ğ‘¢ğ‘Ÿ âŠ† â„ğ‘› at the point x in the closure of Î©ur if for every sequence
{yk } of elements of Î©ur that converges to x and for every sequence of
of vectors {wk } converging to d such that ğ‘¦ğ‘˜ + ğ‘¡ğ‘˜ ğ‘¤ğ‘˜ âˆˆ Î©ğ‘¢ğ‘Ÿ. positive real numbers {tk } converging to zero, there exists a sequence

generalized  derivative  to  Î©ur  at  xâˆ—   as  the  limit For a direction v in the tangent cone, we consider the Clarke-Jahn
â„â—¦(ğ‘¥ ; ğ‘£) =	lim	â„â—¦(ğ‘¥ ; ğ‘‘)

ğœğ‘˜+1 );
15	else
âˆ—	ğ‘‘âˆˆğ‘‡ ğ» (ğ‘¥âˆ— ),ğ‘‘â†’ğ‘£	âˆ—

16	set ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜ andğœğ‘˜+1 = ğ›½ğ‘˜ ğœğ‘˜ , with ğ›½ğ‘˜ âˆˆ (ğ›½1 , ğ›½2 );
end
end
Step 4: update the ES step length ğœğ¸ğ‘† , the distribution Cğ‘˜+1 , and the weights(ğœ”1 , â€¦ , ğœ”ğœ‡ ) âˆˆ ğ‘†;
(see Audet and Dennis Jr., 2006). A point xâˆ— âˆˆ Î©ur is considered Clarke
stationary if hâˆ˜(xâˆ— ; d) â‰¥ 0, âˆ€ğ‘‘ âˆˆ ğ‘‡ ğ¶ğ¿(ğ‘¥âˆ—).
An important ingredient used in our convergence analysis is the no- tion of refining direction (Audet and Dennis Jr., 2006), associated with a convergent refining subsequence K. A refining direction is defined as
the limit point of {ak /  ak  } for all k âˆˆ K suï¬ƒciently large such that

end
		ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ âˆˆ Î©ğ‘¢ğ‘Ÿ, where ğ‘ğ‘˜ =
ğœ”ğ‘– ğ‘‘Ìƒğ‘– .



that one must have a subsequence of iterations driving ğœk to zero. â–¡ certain order, then this also leads to a contradiction. The conclusion is
Theorem 3.1. Let f be bounded below and assuming that the restoration is not entered after a certain order.
There exists a subsequence K of unsuccessful iterates for which
limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Moreover, if the sequence {xk } is bounded, there exists an
The following convergence result concerns the determination of fea- sibility.
Theorem 3.2. Let ğ‘ğ‘˜ = ğœ‡ ğœ”ğ‘– ğ‘‘ğ‘– and assume that f is bounded below. Suppose that the restoration is not entered after a certain order. Let xâˆ— âˆˆ Î©ur
for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g is Lipschitz continuous near xâˆ— with be the limit point of a convergent subsequence of unsuccessful iterates {xk }K constant ğœˆg > 0.
If ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) is a refining direction associated with {ak /  ak  }K , then

Î©ğ‘¢ğ‘Ÿ	âˆ˜

xâˆ— and a refining subsequence Kâ€² such that limğ‘˜âˆˆğ¾ ğ‘¥ğ‘˜ = ğ‘¥âˆ— .
unsuccessful iterates for which ğœğ‘˜+1 goes to zero. In such a case we have Proof. From Lemma 3.1, there must exist an infinite subsequence K of
ğœğ‘˜ = (1âˆ•ğ›½ğ‘˜ )ğœğ‘˜+1 , ğ›½k âˆˆ (ğ›½1 , ğ›½2 ), and ğ›½1 > 0, and thus ğœk â†’ 0, for k âˆˆ K,
either ğ‘”(ğ‘¥âˆ—) = 0 or g (xâˆ— ; d) â‰¥ 0.
Proof. Let d be a limit point of {ak /  ak  }K . Then, a subsequence Kâ€² of K must exist such that ak /  ak  â†’ d on Kâ€². On the other hand, we have for all k

too.
The second part of the theorem is proved by extracting a convergent
ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™
ğ‘˜+1
ğœ‡
ğ‘– ğ‘–
ğ‘˜ ğ‘˜+1
ğ‘–=1
= ğ‘¥ğ‘˜
+ ğœğ‘˜
ğœ‡
ğ‘–  ğ‘–
ğ‘˜ ğ‘˜
ğ‘–=1
= ğ‘¥ğ‘˜
+ ğœğ‘˜ ğ‘ğ‘˜ ,

subsequence Kâ€² âŠ‚ K for which xk converges to xâˆ— .  â–¡
Since the iteration k âˆˆ Kâ€² is unsuccessful, ğ‘”(ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™) â‰¥ ğ‘”(ğ‘¥ ) âˆ’ ğœŒ(ğœ ) or

ğ‘˜+1
ğ‘˜	ğ‘˜

Global convergence will be achieved by establishing that some type of directional derivatives are nonnegative at limit points of refining sub-
g(xk ) â‰¤ CğœŒ(ğœk ), and then either there exists an infinite number of the
first inequality or the second one as follows:



For the case where there exists a subsequence K1âŠ†Kâ€² such that g(xk ) â‰¤ CğœŒ(ğœk ), it is trivial to obtain ğ‘”(ğ‘¥âˆ—) = 0 using both the con- tinuity of g and the fact that ğœk tends to zero in K1.
For the case where there exists a subsequence K2âŠ†Kâ€² such that the
Proof. See the proof of (Gratton and Vicente, 2014, Theorem 4.2).  â–¡
We point out that the assumption regarding the directions
{ak /  ak  }K , in particular their density in the unit sphere, applies to a

sequence {ğ‘ğ‘˜ âˆ• ğ‘ğ‘˜ }ğ¾2
quence {
converges to ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) in K2 and the se-
given refining subsequence Kâ€³ and not to the whole sequence of iter-
ates. However, such a strengthening of the requirements on the density

ğ‘˜  ğ‘˜ ğ‘˜âˆˆğ¾2	2	k
all k, and so ğœk  ak  tends to zero when ğœk does). Thus one must have necessarily for k suï¬ƒciently large in K2 , ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ âˆˆ Î©ğ‘¢ğ‘Ÿ such
that
ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) â‰¥ ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ).
From the definition of the Clarke-Jahn generalized derivative along
of the directions seems necessary for these types of directional meth- ods (Audet and Dennis Jr., 2006). By choosing the distribution Cğ‘˜ in the algorithm to be a multivariate normal distribution with mean zero
(the most commonly used choice in the literature), the density of the
particular for such choice of Cğ‘˜, one has for any ğ‘¦ âˆˆ â„ğ‘› such that ğ‘¦ = 1 directions ak in the unit sphere is guaranteed with a probability 1. In and for any ğ›¼ âˆˆ (0, 1), there exists a positive constant ğœ‚ such that

directions ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—),
ğ‘”â—¦(ğ‘¥âˆ—; ğ‘‘) =	lim sup
ğ‘”(ğ‘¥ + ğ‘¡ğ‘‘) âˆ’ ğ‘”(ğ‘¥)
ğ‘¡
â„™(cos(ğ´ğ‘˜ âˆ• ğ´ğ‘˜ , ğ‘¦) â‰¥ 1 âˆ’ ğ›¼, ğ´ğ‘˜  â‰¥ ğœ–) â‰¥ ğœ‚,
where Ak is a random variable whose realization is ğ‘ğ‘˜ = âˆ‘ğœ‡
ğœ”ğ‘– ğ‘‘Ìƒğ‘– .

ğ‘¥â†’ğ‘¥âˆ— ,ğ‘¡â†“0,ğ‘¥+ğ‘¡ğ‘‘âˆˆÎ©ğ‘¢ğ‘Ÿ
â‰¥ lim sup ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) âˆ’ ğ‘”(ğ‘¥ğ‘˜ )
The justification of such a claim is discussed in further detail in Diouane et al. (2015a).

ğ‘˜âˆˆğ¾2 	ğ‘˜ â€– ğ‘˜ â€–	
Vicente (2014), we will not use xâˆ— âˆˆ Î©  explicitly in the proof but

ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–(ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–)) âˆ’ ğ‘”(ğ‘¥ğ‘˜ )	âˆ˜


where,
= lim sup
ğ‘˜âˆˆğ¾2
ğ‘˜â€– ğ‘˜â€–
â€“ ğ‘”ğ‘˜ ,
only g (xâˆ— ; d) â‰¤ 0. The latter inequality describes the cone of first order linearized directions under feasibility assumption xâˆ— âˆˆ Î©qr.
Theorem 3.4. Let ğ‘ğ‘˜ = âˆ‘ğœ‡  ğœ”ğ‘– ğ‘‘ğ‘– and assume that f is bounded below.

ğ‘” = ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘)
Suppose that the restoration is not entered after a certain order.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of unsuccessful

ğ‘˜	ğœ
ğ‘˜â€– ğ‘˜â€–
iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g and f are Lipschitz
continuous near xâˆ— .

from the Lipschitz continuity of g near xâˆ—
If ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) is a refining direction associated with {a /  a }
such

ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘)
Î©ğ‘¢ğ‘Ÿ
that g (xâˆ— ; d) â‰¤ 0. Then f (xâˆ— ; d) â‰¥ 0.
k	k  K

ğ‘”ğ‘˜ =
ğ‘˜â€– ğ‘˜â€–
Proof. By assumption there exists a subsequence Kâ€²âŠ†K such that the

 ğ‘
â‰¤ ğœˆğ‘” â€–	âˆ’ ğ‘‘â€–
sequence {ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–}ğ¾â€² converges to ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) in Kâ€² and the sequence

ğ‘ğ‘˜
{â€–ğ‘ â€–ğœ }	goes to zero in K
ğ‘¢ğ‘Ÿ
k

tends to zero on K2. Finally,
Since the iteration
ğ‘˜+1
ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™

ğ‘”â—¦(ğ‘¥ ; ğ‘‘) â‰¥ lim sup ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘”(ğ‘¥ğ‘˜ ) + ğœŒ(ğœğ‘˜ ) âˆ’  ğœŒ(ğœğ‘˜ )
â€“ ğ‘”
ğ‘€ (ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ), and thus
k âˆˆ Kâ€² is unsuccessful, one has ğ‘€ (ğ‘¥ğ‘˜+1 ) â‰¥

âˆ—	ğ‘˜âˆˆğ¾2
ğ‘˜â€– ğ‘˜â€–
ğ‘˜â€– ğ‘˜â€–
ğ‘“ (ğ‘¥ + ğœ ğ‘ ) âˆ’ ğ‘“ (ğ‘¥ )
ğ‘”(ğ‘¥ + ğœ ğ‘ ) âˆ’ ğ‘”(ğ‘¥ )
 ğœŒ(ğœ )

= lim sup
.
ğœ  ğ‘
â€– ğ‘˜â€– ğ‘˜
â€– ğ‘˜â€– ğ‘˜
ğ‘˜â€– ğ‘˜â€–

One then obtains gâˆ˜(xâˆ— ; d) â‰¥ 0. â–¡
Moreover, assuming that the set of the refining directions ğ‘‘ âˆˆ

ğ‘“ â—¦(ğ‘¥âˆ—; ğ‘‘) =	lim sup
ğ‘¥â†’ğ‘¥âˆ— ,ğ‘¡â†“0,ğ‘¥+ğ‘¡ğ‘‘âˆˆÎ©
ğ‘“ (ğ‘¥ + ğ‘¡ğ‘‘) âˆ’ ğ‘“ (ğ‘¥)
ğ‘¡

ğ‘‡ ğ» (ğ‘¥âˆ—), associated with {ak /  ak  }K , is dense in the unit sphere. One
â‰¥ lim sup
ğ‘“ (ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) âˆ’ ğ‘“ (ğ‘¥ğ‘˜ )

can show that the limit point xâˆ— is Clarke stationary for the flowing
â€²	ğœ  ğ‘

ğ‘˜âˆˆğ¾ 	ğ‘˜ â€– ğ‘˜ â€–	

optimization problem, known as the constraint violation problem:
= lim sup ğ‘“ (ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–(ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–)) âˆ’ ğ‘“ (ğ‘¥ğ‘˜ ) âˆ’ ğ‘“ ,

min	ğ‘”(ğ‘¥)	(2)
ğ‘ .ğ‘¡. ğ‘¥ âˆˆ Î©ğ‘¢ğ‘Ÿ.
where,
ğ‘˜âˆˆğ¾ â€²
ğ‘˜â€– ğ‘˜â€–

Theorem 3.3. Let ğ‘
= âˆ‘ğœ‡
ğœ”ğ‘– ğ‘‘ğ‘– and assume that f is bounded below.
ğ‘“ = ğ‘“ (ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘“ (ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) ,

Suppose that the restoration is not entered after a certain order. Assume that
the directions ğ‘‘Ìƒğ‘– â€™s and the weights ğœ”ğ‘– â€™s are such that (i) ğœk  ak  tends to zero
ğ‘˜  ğ‘˜
which then implies from (3)

ğ‘˜	ğ‘˜

when ğœk does, and (ii) ğœŒ(ğœk )/(ğœk  ak  ) also tends to zero.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of unsuccessful
ğ‘“ â—¦(ğ‘¥ ; ğ‘‘) â‰¥ lim sup ğ‘“ (ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–(ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–)) âˆ’ ğ‘“ (ğ‘¥ğ‘˜ ) âˆ’ ğ‘“ ,

iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0 and that ğ‘‡Î© (ğ‘¥âˆ—) â‰  âˆ…. Assume that g
ğ‘˜âˆˆğ¾		ğ‘˜ â€– ğ‘˜ â€–	

is Lipschitz continuous near xâˆ— with constant ğœˆ > 0
â‰¥ lim sup âˆ’ğ›¿Ì„ ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ )
â€“ ğ‘“

Then either (a) ğ‘”(ğ‘¥ ) = 0 (implying xâˆ— âˆˆ Î© 
and thus xâˆ— âˆˆ Î©) or (b)
â€²	ğ‘  ğœ
ğœ  ğ‘	ğ‘˜

âˆ—	qr
ğ¶ğ¿
ğ‘˜âˆˆğ¾
â€– ğ‘˜â€– ğ‘˜	 ğ‘˜â€– ğ‘˜â€–

if the set of refining directions ğ‘‘ âˆˆ ğ‘‡Î©ğ‘¢ğ‘Ÿ (ğ‘¥âˆ—) associated with {ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–}ğ¾â€²
Ì„ ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) âˆ’ ğ‘”(ğ‘¥ğ‘˜ )	Ì„
ğœŒ(ğœğ‘˜ )

o
is dense in ğ‘‡ ğ¶ğ¿(ğ‘¥âˆ—) âˆ© {ğ‘‘ âˆˆ â„ğ‘› âˆ¶ â€–ğ‘‘â€– = 1}, then g (xâˆ— ; v) â‰¥ 0 for all ğ‘£ âˆˆ

ğ‘˜âˆˆğ¾ â€²
ğ‘˜â€– ğ‘˜â€–
ğ‘˜â€– ğ‘˜â€–

lem (2).
ğ‘” = ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) âˆ’ ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) .

ğ‘˜	ğœ
ğ‘˜â€– ğ‘˜â€–



From the assumption gâˆ˜(xâˆ— ; d) â‰¤ 0, one has
ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ â€–ğ‘ğ‘˜ â€–ğ‘‘) âˆ’ ğ‘”(ğ‘¥ğ‘˜ )
ğ‘”(ğ‘¥ + ğ‘¡ğ‘‘) âˆ’ ğ‘”(ğ‘¥)
Since at the unsuccessful iteration k âˆˆ Kâ€², Restoration is never left, so one has ğ‘€ (ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) â‰¥ ğ‘€ (ğ‘¥ğ‘˜ ), and the proof follows an argu-

lim sup
ğ‘˜âˆˆğ¾ â€²
ğ‘˜â€– ğ‘˜â€–
â‰¤	lim sup
ğ‘¥â†’ğ‘¥âˆ— ,ğ‘¡â†“0,ğ‘¥+ğ‘¡ğ‘‘âˆˆÎ©ğ‘¢ğ‘Ÿ	ğ‘¡
â‰¤ 0,
ment already seen (see the proof of Theorem 3.3).
The same proof as (Gratton and Vicente, 2014, Theo-

one obtains then
ğ‘“ â—¦(ğ‘¥ ; ğ‘‘) â‰¥ lim sup ğ›¿Ì„ğ‘”
â€“  ğœŒ(ğœğ‘˜ )
â€“ ğ‘“ .	(4)
rem 4.4).  â–¡

3.3. Case 2: the restoration algorithm is entered and left infinite times

âˆ—	ğ‘˜âˆˆğ¾ â€²
ğ‘˜	ğœ
ğ‘˜â€– ğ‘˜â€–

The Lipschitz continuity of both g and f near xâˆ— guaranties that the quan- tities fk and gk tend to zero in Kâ€². Thus, the proof is completed since the right-hand-side of (4) tends to zero in Kâ€². â–¡
Finally, we derive the complete optimality result.
Theorem 3.5. Assuming that f is bounded below and that Restoration is not
Theorem 3.7. Consider Algorithm 1 and assume that f is bounded below. Assume that Restoration is entered and left an infinite number of times.
Then there exists a refining subsequence.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of unsuc- cessful of iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g is Lipschitz
continuous near xâˆ— , and let ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) be a corresponding refining direc-

Î©ğ‘¢ğ‘Ÿ	âˆ˜

entered after a certain order.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of unsuccessful iterates {xk }k âˆˆ K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g and f are Lipschitz continuous near xâˆ— .
Assume that the set
tion. Then either ğ‘”(ğ‘¥âˆ—) = 0 (implying xâˆ— âˆˆ Î©r and thus xâˆ— âˆˆ Î©) or g (xâˆ— ;
d) â‰¥ 0.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of un- successful of iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g and f
are Lipschitz continuous near xâˆ— , and let ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) be a corresponding

o	Î©ğ‘¢ğ‘Ÿ

ğ‘‡ (ğ‘¥âˆ—) = ğ‘‡ ğ» (ğ‘¥âˆ—) âˆ© {ğ‘‘ âˆˆ â„ğ‘› âˆ¶ ğ‘‘ = 1, ğ‘”â—¦(ğ‘¥âˆ— , ğ‘‘) â‰¤ 0}	(5)
has a non-empty interior.
Let the set of refining directions be dense in T(xâˆ— ). Then fâˆ˜(xâˆ— , v) â‰¥ 0 for all ğ‘£ âˆˆ ğ‘‡ ğ¶ğ¿(ğ‘¥âˆ—) such that gâˆ˜(xâˆ— , v) â‰¤ 0, and xâˆ— is a Clarke stationary point
of the problem (1).
Proof. See the proof of (Gratton and Vicente, 2014, Theorem 4.4).  â–¡
Now, we provide the analysis of the two other cases, namely when
(a) an infinite run of consecutive steps inside Restoration or (b) one enters the restoration an infinite number of times.

Case 2: the restoration algorithm is entered and never left

In this case, by a refining subsequence below, we mean a subse- quence of unsuccessful Restoration iterates for which the step-size pa- rameter converges to zero.
Theorem 3.6. Assume that f is bounded below and that the restoration is entered and never left.
Then there exists a refining subsequence.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of unsuc- cessful of iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g is Lipschitz
continuous near xâˆ— , and let ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) be a corresponding refining direc-
refining direction such that g (xâˆ— ; d) â‰¤ 0. Then fâˆ˜(xâˆ— ; d) â‰¥ 0.
(iv) Assume that the interior of the set T(xâˆ— ) given in (5) is non-empty. Let the set of refining directions be dense in T(xâˆ— ). Then fâˆ˜(xâˆ— , v) â‰¥ 0 for all
ğ‘£ âˆˆ ğ‘‡ ğ¶ğ¿(ğ‘¥âˆ—) such that gâˆ˜(xâˆ— , v) â‰¤ 0, and xâˆ— is a Clarke stationary point.
Proof. (i) Let K1âŠ†K and K2âŠ†K be two subsequences where Restoration
is entered and left respectively.
Since the iteration k âˆˆ K2 is unsuccessful in the Restoration, one knows that the step size ğœk is reduced and never increased, one then obtains that ğœk tends to zero. By assumption there exists a subsequence Kâ€²âŠ†K2 such that the sequence {ğ‘ğ‘˜ âˆ•â€–ğ‘ğ‘˜ â€–}ğ‘˜âˆˆğ¾â€² converges to ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—)
For all k âˆˆ Kâ€², one has ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) â‰¥ ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ) or
g(xk ) â‰¤ CğœŒ(ğœk ), one concludes that either ğ‘”(ğ‘¥âˆ—) = 0 or gâˆ˜(xâˆ— ; d) â‰¥ 0.
For all k âˆˆ Kâ€², one has ğ‘€ (ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) â‰¥ ğ‘€ (ğ‘¥ğ‘˜ ), and from this we conclude that fâˆ˜(xâˆ— ; d) â‰¥ 0 if gâˆ˜(xâˆ— ; d) â‰¤ 0.
The same proof as (Gratton and Vicente, 2014, Theo- rem 4.4). â–¡
To sum up, the analysis of the global convergence of Algorithm 1 was provided depending on the number of times the restoration pro- cedure is entered. When the restoration is entered finite times, Theorem 3.2 showed that the limit points of certain subsequences of it- erates are either feasible or Clarke stationary for the constraint violation problem (2). Theorem 3.5 showed then that such limit points are Clarke stationary for the optimization problem (1). Our analysis provide sim-

o  Î©ğ‘¢ğ‘Ÿ

tion. Then either ğ‘”(ğ‘¥âˆ—) = 0 or g (xâˆ— ; d) â‰¥ 0.
Let xâˆ— âˆˆ Î©ur be the limit point of a convergent subsequence of un- successful of iterates {xk }K for which limğ‘˜âˆˆğ¾ ğœğ‘˜ = 0. Assume that g and f
are Lipschitz continuous near xâˆ— , and let ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) be a corresponding
ilar feasibility and optimality results for the two remaining cases (i.e., when the restoration is â€œentered but never leftâ€ or â€œentered and left an infinite number of timesâ€), see Theorems 3.6 and 3.7.

refining direction such that gâˆ˜(xâˆ— ; d) â‰¤
Î©ğ‘¢ğ‘Ÿ
0. Then fâˆ˜(xâˆ— ; d) â‰¥ 0.
Numerical experiments

Assume that the interior of the set T(xâˆ— ) given in (5) is non-empty. Let the set of refining directions be dense in T(xâˆ— ). Then fâˆ˜(xâˆ— , v) â‰¥ 0 for all
ğ‘£ âˆˆ ğ‘‡ ğ¶ğ¿(ğ‘¥âˆ—) such that gâˆ˜(xâˆ— , v) â‰¤ 0, and xâˆ— is a Clarke stationary point of
the problem (1).
Proof. (i) There must exist a refining subsequence K within this call
one has ğ‘”(ğ‘¥ğ‘˜+1 ) < ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ) and g(xk ) > CğœŒ(ğœk ) for an infinite of the restoration, by applying the same argument of the case where
By assumption there exists a subsequence Kâ€²âŠ†K such that the se- subsequence of successful iterations (see the proof of Theorem 3.1). quence {ğ‘ğ‘˜ âˆ• ğ‘ğ‘˜ }ğ‘˜âˆˆğ¾â€² converges to ğ‘‘ âˆˆ ğ‘‡ ğ» (ğ‘¥âˆ—) in Kâ€² and the sequence
{ ğ‘ğ‘˜ ğœğ‘˜ }ğ‘˜âˆˆğ¾â€² goes to zero in Kâ€². Thus one must have necessarily for k
suï¬ƒciently large in Kâ€², ğ‘¥ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ = ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ âˆˆ Î©ğ‘¢ğ‘Ÿ.
(ii) Since the iteration k âˆˆ Kâ€² is unsuccessful in the Restoration,
ğ‘”(ğ‘¥ğ‘˜ + ğœğ‘˜ ğ‘ğ‘˜ ) â‰¥ ğ‘”(ğ‘¥ğ‘˜ ) âˆ’ ğœŒ(ğœğ‘˜ ) or ğ‘”(ğ‘¥ğ‘˜+1 ) â‰¤ ğ¶ğœŒ(ğœğ‘˜ ), and the proof follows
an argument already seen (see the proof of Theorem 3.2).

In this section, we evaluate the performance of the proposed merit function approach using different solvers, different comparison proce- dures, and a large collection of non-linear constrained optimization problems. All the procedures were implemented in Matlab and run using Matlab 2019a on a MacBook Pro-2,4 GHz Intel Core i5, 4 GB RAM.

Problems tested and testing strategies

In what comes next, as a benchmark test, we will use 40 small- scale constrained test problems as given in Audet et al. (2018) (those problems are extracted from the CUTEst collection (Gould et al., 2015). The dimensions of the tested problems do not exceed 9 variables, with eventually bound constraints and no more than 13 nonlinear constraints (see Audet et al., 2018, Table 1 for a detailed description on all the tested problems). For each test problem, the initial point provided by CUTEst



is used, the latter respects the bound contraints but does not necessarily satisfy the nonlinear constraints.
To illustrate the obtained results, we will use the two well-known testing strategies: data profiles MorÃ© and Wild (2009) and performance profiles (Dolan et al., 2006). For data profiles, we use the following convergence test
Regarding the ğ›¿Ì„ parameter, we tested 8 different values varied in range 10âˆ’2 and 105, see Fig. 1(b). The obtained profiles show that, for a small budget of evaluations, ES-MF is not sensitive to the value of ğ›¿Ì„. For
a larger budget, the performance changes slightly probably due to the
value of ğ›¿Ì„ = 103 is shown to be very favorable to the ES-MF solver. stochastic nature of the solver. However, on the tested problems, one

0	0	Next, for the parameter C, we tested 8 different values varied in range

ğ‘“max âˆ’ ğ‘“Î©(ğ‘¥) â‰¥ (1 âˆ’ ğ›¼)(ğ‘“max âˆ’ ğ‘“min),
while for the performance profiles, we make use of
ğ‘“Î©(ğ‘¥) âˆ’ ğ‘“min â‰¤ ğ›¼(ğ‘“min + 1),
10âˆ’2 and 105, see Fig. 1(c). Again, the obtained profiles change slightly.
We suspect that the slight changes in the performance are just due to
the stochastic nature of the solver and consider that ES-MF is not very sensitive to the choice of the parameter C.
In what comes next, for the solver ES-MF, we set by default ğ›¿Ì„ = 1,

where ğ›¼ is the level accuracy and ğ‘“ 0
represents the largest value
ğ¶ = 1, and use the ğ“2-norm to define the constraint violation function

among all the feasible objective function values initially visited by all the	g.
tested solvers (i.e., ğ‘“ 0	= maxğ‘  ğ‘“ 0 where ğ‘“ 0 represents the objective
max	ğ‘ 	ğ‘ 

function value at the first feasible point visited by the solver s). The value
tolerance of 10âˆ’7 for constraint violation is used to consider a point as fmin represents the best feasible solution found by the tested solvers. A
being feasible. We note that, if a solver fails to find a feasible starting point for a given problem, the problem is considered as unsolved, in this case the convergence test is not used. The performance and data profiles are computed for a maximum of 3000 function evaluations. For the stochastic solvers, we will describe our results using the median data/performance profile obtained over 20 runs.

Implementation choices

Algorithm 1 and Algorithm 2 are implemented in Matlab. The ob- tained implementation will be called ES-MF. Most of the parameter choices followed those in Diouane et al. (2015b) (where some of the user-specified parameters are the same used by directional direct search
methods and CMA-ES). In particular, the values of ğœ† and ğœ‡ and of
tion (see Hansen, 2011): ğœ† = 4 + floor(3 log(ğ‘›)), ğœ‡ = floor(ğœ†âˆ•2), where the initial weights are those of CMA-ES for unconstrained optimiza- floor( Â· ) rounds to the nearest integer, and ğœ”ğ‘– = ğ‘ğ‘– âˆ•(ğ‘1 + â‹¯ + ğ‘ğœ‡ ), ğ‘ğ‘– = log(ğœ†âˆ•2 + 1âˆ•2) âˆ’ log(ğ‘–), ğ‘– = 1, â€¦ , ğœ‡. The choices of the distribution Cğ‘˜ and of the update of ğœğ¸ğ‘† also followed CMA-ES for unconstrained
tions, the forcing function selected was ğœŒ(ğœ) = 10âˆ’4 ğœ2. To reduce the optimization. As used in most directional direct search implementa- step length in unsuccessful iterations we used ğœğ‘˜+1 = 0.9ğœğ‘˜ which cor- responds to setting ğ›½1 = ğ›½2 = 0.9. For successful iterations we set ğœğ‘˜+1 = max{ğœğ‘˜ , ğœğ¶ğ‘€ğ´âˆ’ğ¸ğ‘† } (with ğœğ¶ğ‘€ğ´âˆ’ğ¸ğ‘† the CMA step size used in ES). The directions ğ‘‘ğ‘– , ğ‘– = 1, â€¦ , ğœ†, were scaled if necessary to obey the safeguards
The extreme barrier versus the merit function for ES

In this subsection, we present a comparison between ES-MF and ES- EB from Diouane et al. (2015b) (ES-EB can be seen as a particular in- stance of ES-MF where all the constraints are UR). Since the solver ES-EB requires a feasible starting point, when the starting point is infeasible, finding a feasible point is accomplished by minimizing the constraint violation function g.
ing two levels of accuracy 10âˆ’3 and 10âˆ’7 . One can see that the extreme Fig. 2 depicts the resulting performance and data profiles consider-
barrier approach is not able to solve more than 50% of the problems (as shown by the performance profiles). The data profiles indicate that the extreme barrier can be competitive for small budgets. Overall, the merit function approach is outperforming the extreme barrier approach. Thus, relaxing the constraints clearly makes it possible to reach better optimal solutions which motivates the use of the merit function approach ES-MF instead of ES-EB.

Comparison of solvers using the problems from the CUTEst collection

To quantify the eï¬ƒciency of ES-MF, we include in our numerical comparison the solvers MADS-PB, DDS-MF, and CSA-AL:
MADS-PB Audet and Dennis Jr. (2009): a mesh adaptive direct search (MADS) method where a progressive barrier (PB) approach has been implemented (Audet and Dennis Jr., 2009) to handle QR constraints. The progressive barrier approach, proposed in MADS, enjoys similar convergence properties as for our algorithm, hence, a comparison between the two solvers is very meaningful. For the

ğ‘‘min
â€– ğ‘˜â€–
â‰¤ ğ‘‘
max
, with ğ‘‘
min
= 10âˆ’10 and ğ‘‘
max
= 1010 . The initial step
MADS solver, we used the implementation given in the NOMAD

finite lower and upper bounds for a variable, then ğœ0 is set to the half size is estimated using only the bound constraints: If there is a pair of of the minimum of such distances, otherwise ğœ0 = 1.

Sensitivity analysis

The proposed evolution strategy introduces some user-specified con- trol parameters and their performances might depend on the setting of these parameters. A full sensitivity analysis of all the control parame- ters of the merit function approach can be computationally demanding and is beyond the scope of this paper. Hence, this subsection focuses on
parameters, namely, the constants ğ›¿Ì„ and C as well as the choice of norm the sensitivity of ES-MF with respect to the newly introduced control
type used to evaluate g.
choices for the constants ğ›¿Ì„ and C as well as for the norm type used to Fig. 1 shows their performance and the data profiles using different
evaluate the constraint violation function g. With respect to the choice the norm in g, see Fig. 1(a), one can see that the use of ğ“2-norm is clearly favorable to our approach in particular with a large budget of objective function evaluations. The choice of working with the ğ“2-norm to evaluate g was shown to perform better for the progressive barrier approach used in MADS (Audet and Dennis Jr., 2009).
package (Le Digabel, 2011), version 3.9.1 (C++ version linked to Matlab via a mex interface). This solver is deterministic.
DDS-MF Gratton and Vicente (2014): a Matlab implementation of a directional direct search (DDS) method where a merit function (MF) is used to handle QR constraints. The parameter choices followed those given in the numerical section of Gratton and Vicente (2014). We recall that ES-MF is inspired from the DDS-MF method, hence including the latter solver in the comparison can be also very mean- ingful. We note also that this is the first time DDS-MF is compared using an extensive test set. The behavior of the solver is stochastic
as it generates randomly (at most) ğ‘› + 1 directions at each iteration
of the algorithm.
CSA-AL Atamna et al. (2018): a Matlab implementation of CMA-ES using an augmented Lagrangian approach to handle QR constraints. For the CMA-ES part, we used the same choice of parameters as for ES-MF, for the parameters associated with the augmented La- grangian part we chose the values given in Atamna et al. (2018).
For all the solvers, we consider that all the nonlinear constraints are QR except the bounds which are treated using an ğ“2-projection.
two accuracy levels 10âˆ’3 and 10âˆ’7 . Clearly, for all the runs, CSA-AL is Fig. 3 reports the median (out of 20 runs) profiles considering the


 

	








Fig. 1. Median profiles for the solver ES-MF computed using 40 problems from the CUTEst set and different control parameters.


 

	





Fig. 2. Median profiles for the solvers ES-MF and ES-EB using 40 problems from the CUTEst set.


performing the worst among all the tested solvers. For the resulting data profiles, one can see that with a small budget, DDS-MF and MADS-PB exhibit better performance than the ES-MF. However, when the budget is getting larger, ES-MF performs the best. From the resulting perfor- mance profiles, one can see that in terms of eï¬ƒciency (i.e., small values
of ğœ), DDS-MF is shown to be best. The ES-MF solver performs better in
terms of robustness (i.e., large values of ğœ).
In conclusion, first, clearly the ES-MF solver leads to very good re-
sults compared to CSA-AL. In fact, in our tests, CSA-AL showed diï¬ƒ- culties finding feasible points while making progress on the objective function. We stress that the main difference between the two evolu- tion strategies is the restoration procedure, the latter helps ES-MF to progress better towards feasible zones without severe deterioration in terms of the objective function value. Second, ES-MF can be very com- petitive with both solvers DDS-MF and MADS-PB, in particular when using a large number of function evaluations.

Comparison of solvers using global optimization test problems

To confirm the results obtained when using CUTEst problems, we perform complementary tests using a set of problems with a diver-
sity of features and the kind of diï¬ƒculties that appear in constrained global optimization. The test set is that used in Hock and Schit- tkowski (1981), Koziel and Michalewicz (1999) and Michalewicz and
Schoenauer (1996) and comprises 12 well-known test problems (see Table 1). The problems G2, G3, and G8 are originally maximization problems and were converted to minimization.
In addition to such problems, we include three realistic prob- lems. The first one is the tension-compression string (TCS) prob- lem (Coello and Montes, 2002), the aim is to minimize the weight of
a tension-compression string subject to constraints on minimum deflec- tion, shear stress, surge frequency, limits on outside diameter and on de- sign variables. The design variables are the mean coil diameter; the wire diameter and the number of active coils. The second problem is the well
known welded beam design (WBD) problem (Coello and Montes, 2002)
where a welded beam is designed with a minimum cost subject to con- straints on shear stress; bending stress in the beam; buckling load on
the bar; end deflection of the beam; and side constraints. The third opti- mization problem is a multidisciplinary design optimization (MDO) prob- lem (Gramacy and Digabel, 2015; Tribes et al., 2005) where a simplified
wing design (built around a tube) is looked at. For this problem, one tries to minimize the range of the aircraft under coupled aero-structural


 

	





Fig. 3. Median profiles for the solvers ES-MF, MADS-PB, DDS-MF, and CSA-AL, using 40 problems from the CUTEst set.


Table 1
Description of the features of the 15 global optimization problems: the dimension n, the number of the QR constraints m, the number of the lower bounds # LB, the number of the upper bounds # UB, the initial objective value f(x0), the initial constraints violation g(x0), and the best known feasible solution fopt .


 




Fig. 4. Median profiles for the solvers ES-MF, MADS-PB, and DDS-MF, using 15 global optimization test problems.


constraints. The problem has 7 optimization variables corresponding to the geometry of the wing. The details of the three realistic problems features are included in Table 1.
To allow the analysis of the asymptotic eï¬ƒciency and the robustness of the tested solvers, we generate performance and data profile using a larger maximal number of function evaluation of 104. The starting point
x0 is chosen to be the same for all solvers and set to (ğ¿ğµ + ğ‘ˆğµ)âˆ•2 where
LB are the lower bound constraints and UB are the upper bound con-
straints. We consider that all the constraints as QR except the bounds on the design variables which are treated using the ğ“2-projection for all the
solvers. We note that problems G3, G11, and WBD contain equality con- straints. When a constraint is of the form ğ‘ğ‘’(ğ‘¥) = 0, we use the following relaxed inequality constraint instead ğ‘ğ‘– (ğ‘¥) = ğ‘ğ‘’(ğ‘¥) âˆ’ 10âˆ’5 â‰¤ 0. We de-
scribe our finding using the median performance and data profiles over 20 runs.
Fig. 4 reports the obtained profiles for the solvers MADS-PB, DDS- MF and ES-MF using a maximal budget of 104. Additionally, we include the profiles of a variant of the solver MADS-PB where the variable neigh- borhood search (VNS) strategy is enabled to enhance its global perfor-
mance (by setting the flag vns_search to 1 in the NOMAD package).
The latter solver is denoted by MADS-PB (with VNS) in Fig. 4. We note
also that the solver CSA-AL is no longer included in the comparison as it displayed the worst results in our tests (it produced unfeasible solu- tions on most of the tested problems). Clearly, one can see that, unlike the previous test bed, the ES-MF solver outperforms the solvers MADS- PB and DDS-MF, particularly when considering a large function evalua-
tions. For the low accuracy level (i.e., ğ›¼ = 10âˆ’3 ), enabling the VNS option
improves significantly the eï¬ƒciency of MADS-PB. For such accuracy,
the solver MADS-PB (with VNS) reaches better eï¬ƒciency performance compared to ES-MF. However, considering a higher accuracy level (i.e.,
ğ›¼ = 10âˆ’7 ) tends to degrade the performance of MADS-PB (with VNS)
compared to ES-MF.
Tables 2 and 3 depict the final obtained results for the solvers MADS- PB, DDS-MF, MADS-PB (with VNS) and ES-MF, using a maximal bud- get of 104 function evaluations. For each problem, we display the opti-
mal objective value found by the solver f(xâˆ— ), the associated constrained violation g(xâˆ— ), and the number of objective function evaluations #ğ‘“
needed to reach xâˆ— . When a solver returns a flag error or encounters an internal problem, we display â€œâˆ—â€. At the solution xâˆ— , one requires at least a tolerance of 10âˆ’5 on the constraint violation to consider xâˆ—
as feasible with respect to QR constraints. Considering the median run,
ES-MF converged to a feasible solution for all the problems, MADS-PB



Table 2
Obtained results with MADS-PB and DDS-MF, using 15 global optimization test problems.



Table 3
Obtained results with ES-MF and MADS-PB (with VNS), using 15 global optimization test problems.




converged as well to a feasible point for all the problems, except the TCS problem for which MADS-PB returns a flag error. The DDS-MF solver could not converge to a feasible solution for three problems G2, G4, and G5. In terms of the objective function value, one can see clearly
that ES-MF is outperforming both solvers MADS-PB and DDS-MF. As ex- pected, in terms of function evaluations, MADS-PB required in general less function evaluations than ES-MF to converge to a solution (but not necessarily better then the one found by ES-MF). The use of the vari- able neighborhood search option within MADS improves significantly its performance, MADS-PB (with VNS) is displaying similar performances compared to the ES-MF.

Conclusion

In this paper, we proposed a globally convergent class of ES algo- rithms where a merit function is used to decide and control the dis- tribution of the generated points. The proposed approach included a restoration procedure which is entered whenever a decrease on the con- straint violation function is achieved while the objective function is being considerably increased. The obtained algorithm generalized the work (Diouane et al., 2015b) by including quantifiable relaxable con- straints. In the spirit of what is achieved in Gratton and Vicente (2014), the proposed convergence analysis was organized depending on the number of times Restoration is entered.
We provided numerical tests on problems from the CUTEst collection and a global optimization test bed. The results showed the potential of the proposed merit approach compared to existing direct search DFO solvers, in particular when using a large number of function evaluations.
References

Atamna, A., Auger, A., Hansen, N., 2018. On invariance and linear convergence of evolu- tion strategies with augmented Lagrangian constraint handling. Theor. Comput. Sci. 0, 1â€“53.
Audet, C., Conn, A.R., Digabel, S.L., Peyrega, M., 2018. A progressive barrier deriva- tive-free trust-region algorithm for constrained optimization. Comput. Optim. Appl. 71 (2), 307â€“329.
Audet, C., Hare, W., 2017. Derivative-Free and Blackbox Optimization. Springer Interna- tional Publishing, Cham, Switzerland.
Audet, C., Dennis Jr., J.E., 2006. Mesh adaptive direct search algorithms for constrained optimization. SIAM J. Optim. 17, 188â€“217.
Audet, C., Dennis Jr., J.E., 2009. A progressive barrier for derivative-free nonlinear pro- gramming.. SIAM J. Optim. 20 (1), 445â€“472.
Auger, A., Brockhoff, D., Hansen, N., 2013. Benchmarking the local metamodel CMA-ES on the noiseless BBOBâ€™2013 test bed. In: Proceedings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation. ACM, New York, NY, USA,
pp. 1225â€“1232.
Auger, A., Hansen, N., Perez, Z.J., Ros, R., Schoenauer, M., 2009. Experimental compar- isons of derivative free optimization algorithms. In: 8th International Symposium on Experimental Algorithms. Springer Verlag, pp. 3â€“15.
Bouzarkouna, Z., 2012. Well Placement Optimization. University Paris-Sud - Laboratoire de Recherche en Informatique.
Clarke, F.H., 1983. Optimization and Nonsmooth Analysis. John Wiley & Sons, New York.
Reissued by SIAM, Philadelphia, 1990
Coello, C. A. C.,. List of references on constraint-handling techniques used with evolution- ary algorithms. https://www.cs.cinvestav.mx/~constraint/constbib.pdf.
Coello, C.A.C., 2002. Theoretical and numerical constraint-handling techniques used with evolutionary algorithms: a survey of the state of the art. Comput. Methods Appl. Mech. Eng. 91, 1245â€“1287.
Coello, C.A.C., Montes, E.M., 2002. Constraint-handling in genetic algorithms through the use of dominance-based tournament selection. Adv. Eng. Inf. 16, 193â€“203.
Conn, A.R., Scheinberg, K., Vicente, L.N., 2009. Introduction to Derivative-Free Optimiza- tion. SIAM, Philadelphia.
Diouane, Y., 2014. Globally Convergent Evolution Strategies With Application to Earth Imaging Problem in Geophysics. Institut National Polytechnique de Toulouse.
Diouane, Y., Gratton, S., Vasseur, X., Vicente, L.N., Calandra, H., 2016. A parallel evolution strategy for an earth imaging problem in geophysics. Optim. Eng. 17, 3â€“26.
Diouane, Y., Gratton, S., Vicente, L.N., 2015. Globally convergent evolution strategies.
Math. Program. 152, 467â€“490.
Diouane, Y., Gratton, S., Vicente, L.N., 2015. Globally convergent evolution strategies for constrained optimization. Comput. Optim. Appl. 62, 323â€“346.
Dolan, E.D., MorÃ©, J.J., Munson, T.S., 2006. Optimality measures for performance profiles.
SIAM J. Optim. 16, 891â€“909.
Fletcher, R., Leyffer, S., 2002. Nonlinear programming without a penalty function. Math.
Program. 91, 239â€“269.
Gould, N.I.M., Orban, D., Toint, P.L., 2015. CUTEst: a constrained and unconstrained test- ing environment with safe threads for mathematical optimization.. Comput. Optim. Appl. 60, 545â€“557.
Gramacy, R.B., Digabel, S.L., 2015. The mesh adaptive direct search algorithm with treed gaussian process surrogates. Pac. J. Optim. 11, 719â€“747.
Gratton, S., Vicente, L.N., 2014. A merit function approach for direct search. SIAM J. Optim. 24, 1980â€“1998.
Hansen, N., 2011. The CMA Evolution Strategy: A tutorial. Available at https://www. lri.fr/~hansen/cmatutorial.pdf
Hansen, N., Auger, A., Raymond, R.R., Finck, S., PoÅ¡Ã­k, P., 2010. Comparing results of 31 algorithms from the black-box optimization benchmarking bbob-2009. In: Proceed- ings of the 12th Annual Conference Companion on Genetic and Evolutionary Compu- tation. ACM, New York, NY, USA, pp. 1689â€“1696.
Hansen, N., Ostermeier, A., Gawelczyk, A., 1995. On the adaptation of arbitrary normal mutation distributions in evolution strategies: the generating set adaptation. In: Es- helman, L. (Ed.), Proceedings of the Sixth International Conference on Genetic Algo- rithms, Pittsburgh, pp. 57â€“64.
Hock, W., Schittkowski, K., 1981. Test Examples for Nonlinear Programming Codes.
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
Jahn, J., 1996. Introduction to the Theory of Nonlinear Optimization. Springer-Verlag, Berlin.
Kolda, T.G., Lewis, R.M., Torczon, V., 2003. Optimization by direct search: new perspec- tives on some classical and modern methods. SIAM Rev. 45, 385â€“482.
Koziel, S., Michalewicz, Z., 1999. Evolutionary algorithms, homomorphous mappings, and constrained parameter optimization. Evol. Comput. 7, 19â€“44.
Kramer, O., 2010. A review of constraint-handling techniques for evolution strategies.
Appl. Comp. Intell. Soft Comput. 2010, 1â€“19.
Le Digabel, S., 2011. Algorithm 909: NOMAD: nonlinear optimization with the MADS algorithm. ACM Trans. Math. Softw. 37, 1â€“15.
Le Digabel, S., Wild, S., 2015. A Taxonomy of Constraints in Simulation-Based Optimiza- tion. Technical Report. Les cahiers du GERAD.
MartÃ­nez, J.M., Sobral, F.N.C., 2013. Constrained derivative-free optimization on thin do- mains. J. Glob. Optim. 56 (3), 1217â€“1232.
Michalewicz, Z., Schoenauer, M., 1996. Evolutionary algorithms for constrained parame- ter optimization problems. Evol. Comput. 4, 1â€“32.
MorÃ©, J.J., Wild, S.M., 2009. Benchmarking derivative-free optimization algorithms. SIAM
J. Optim. 20, 172â€“191.
Nocedal, J., Wright, S.J., 2006. Numerical Optimization, second ed. Springer, New York, NY.
Rechenberg, I., 1973. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzip- ien der Biologischen Evolution. Frommann-Holzboog.
Rios, L., Sahinidis, N., 2010. Derivative-free optimization: a review of algorithms and comparison of software implementations. J. Glob. Optim. 56, 1247â€“1293.
Tribes, C., DubÃ©, J.-F., TrÃ©panier., J.-Y., 2005. Decomposition of multidisciplinary opti- mization problems: formulations and application to a simplified wing design. Optim. Eng. 37, 775â€“796.
