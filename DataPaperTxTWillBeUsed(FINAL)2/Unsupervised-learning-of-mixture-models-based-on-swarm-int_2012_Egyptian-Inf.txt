
ORIGINAL ARTICLE

Unsupervised learning of mixture models based on swarm intelligence and neural networks with optimal completion using incomplete data
Ahmed R. Abas *

Department of Computer Science, University College in Leith, Umm Al-Qura University, Makka Al-Mukarrama, Leith, Saudi Arabia

Received 7 September 2011; revised 7 March 2012; accepted 26 March 2012
Available online 21 April 2012

Abstract In this paper, a new algorithm is presented for unsupervised learning of finite mixture models (FMMs) using data set with missing values. This algorithm overcomes the local optima problem of the Expectation-Maximization (EM) algorithm via integrating the EM algorithm with Particle Swarm Optimization (PSO). In addition, the proposed algorithm overcomes the problem of biased estimation due to overlapping clusters in estimating missing values in the input data set by integrating locally-tuned general regression neural networks with Optimal Completion Strategy (OCS). A comparison study shows the superiority of the proposed algorithm over other algorithms commonly used in the literature in unsupervised learning of FMM parameters that result in mini- mum mis-classification errors when used in clustering incomplete data set that is generated from overlapping clusters and these clusters are largely different in their sizes.
© 2012 Faculty of Computers and Information, Cairo University.
Production and hosting by Elsevier B.V. All rights reserved.



		1. Introduction
* Address: Department of Computer Science, Faculty of Computers

and Informatics, Zagazig University, Zagazig, Egypt.
E-mail addresses: armohamed@uqu.edu.sa, arabas@zu.edu.eg

1110-8665 © 2012 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.

Peer review under responsibility of Faculty of Computers and Information, Cairo University. http://dx.doi.org/10.1016/j.eij.2012.03.002

Finite mixture models (FMMs) is a semi-parametric method for density estimation and pattern recognition [1] that is used for fitting complex data distributions. This method has the advantage of the analytic simplicity and the advantage of the flexibility to model complex data distributions [2]. Parameters of FMM are usually estimated by the Expectation Maximiza- tion (EM) algorithm [3]. The EM algorithm cannot handle incomplete data sets. Therefore, several algorithms are pro- posed in the literature to modify the EM algorithm to estimate parameters of FMM using incomplete data set [4–6]. Part of these algorithms is affected by the occurrence of outliers in the data, and all of them are affected by the overlap among classes in the data space and the bias in generating the data

from its classes [5]. When there is sufficiently large number of observed values, these algorithms outperform the EM algo- rithm combined with either the unconditional mean imputation or the conditional mean imputation of the missing values according to the classification performance of the resulting FMM [5]. It is shown that better results can be obtained by imputing missing values using the distribution of the input fea- ture vectors rather than using a priori probability distribution function used in the FMM [5,7]. However, these modified EM algorithms have poor performance in learning FMM parameters when clusters of the input data set are largely over- lapping and unbalanced in their numbers of feature vectors [5]. This is due to the fact that these algorithms estimate missing values in a certain feature vector only from either parameters or members of one component of the FMM to which this feature vector has the maximum posterior probability. The pos- terior probability is computed using complete values of each feature vector. This ignores the overlapping among clusters of the data set that is represented by FMM components. This overlapping means that feature vectors of the data set are generated from different components of FMM with different probabilities. Therefore, these algorithms produces inaccurate estimation of missing values which in turn leads to inaccurate estimation of FMM parameters learned from the whole data set.
In this paper, a new algorithm is proposed to overcome problems of the modified EM algorithms for unsupervised learning of the FMM parameters using incomplete data [4,5]. These problems are is the sensitivity to the occurrence of out- liers in the data, the overlap among classes in the data space, and the bias in generating the data from its classes i.e., clusters of the input data set are unbalanced in their numbers of feature vectors. The proposed algorithm is less sensitive to the learning problems of the EM algorithm in cases such as the occurrence of outliers in the data set, the overlapping among data classes, and the unbalanced representation of data classes. The rest of this paper is organized as follows. Section 2 presents the pro- posed algorithm. Section 3 shows results of the comparison study that is carried out to evaluate the performance of the proposed algorithm. These results are discussed in Section 4. Conclusions are presented in Section 5.

The proposed algorithm

This section presents a new algorithm that overcomes prob- lems of the modified EM algorithms [4,5] when dealing with a small incomplete data set that may contain outliers, overlap- ping clusters, or a large difference in the sizes of its clusters. In the rest of this paper, the modified EM algorithm proposed in
[4] is referred to as the MEM algorithm while the modified EM algorithm proposed in [5] is referred to as the LGREM algorithm.

A swarm intelligence based EM algorithm

The EM algorithm [3] is an iterative algorithm that is used in producing maximum likelihood estimation of FMM parame- ters. However, this algorithm is sensitive to initialization be- cause it stops at the nearest local maximum to the initial point of the likelihood function [8]. To overcome this problem the proposed algorithm uses a new proposed EM algorithm
that is based on Particle Swarm Optimization (PSO) [9,10,11] to find the best estimation of FMM parameters that corre- sponds to the global maximum of the likelihood function. The proposed algorithm uses a swarm of several particles to find the best FMM that fits the input data set. Fig. 1 shows the particle structure.
Each particle in the swarm corresponds to a FMM whose parameters are learnt by the EM algorithm. Its structure con- tains the mixing weights, the centroid, the covariance matrix of a FMM, and the log-likelihood of this mixture model to repre- sent the input data. After learning, the particle that has the maximum log-likelihood (LOGLH) is selected and its FMM is considered the best model for clustering the input data set. All swarm particles are generated such that the initial values of their mixing weights are equal and sum to one, centroids are feature vectors chosen randomly from the input data set, covariance matrixes are equal to 0.01Id, where Id is the identity matrix of order d and d is the number of features of the data set. This removes any outside bias towards certain components during learning of FMM parameters.

Locally-tuned general regression neural networks combined with Optimal Completion Strategy (OCS)

The OCS [12] allows the fuzzy c-means algorithm (FCM) [13] to be used in clustering incomplete data sets [14,15]. Based on this strategy, missing values in the data set are estimated in every iteration of the FCM algorithm using cluster centroids averaged with fuzzy membership values of feature vectors con- taining missing values in the data set to different clusters. In the proposed algorithm, locally-tuned general regression neu- ral networks [5] (see, Eq. (2) for description) are modified using the OCS to estimate missing values in the data set (see, Eq. (3) for description). Missing values are estimated in every iteration using non-parametric estimation values obtained from the lo- cally-tuned general regression neural networks [5] averaged by posterior probabilities of feature vectors containing missing values. This agrees with the basic assumption of clustering using FMM that is feature vectors in the data set are generated from different FMM components with different probabilities. The proposed algorithm uses locally-tuned general regression neural networks in estimating missing values [5] because this algorithm produces more accurate estimation of missing values and better learning of FMM parameters than using FMM parameters [4] in estimating missing values in the data set [5]. The proposed algorithm uses the PSO-based EM algorithm and the locally-tuned general regression neural networks com- bined the OCS to learn FMM parameters from incomplete data. The proposed algorithm is referred to as the POLGREM
algorithm in the rest of this paper.

Description of the POLGREM algorithm
Suppose that the data set R = {x1, x2,..., xN} consists of N feature vectors each of which is a vector in d-feature space such that each feature vector xi = [xi1, xi2,.. . , xid]T. This data set is


Figure 1	Particle structure.

assumed to be generated from a FMM of K multivariate nor-
where K P(c)= 1, and 0 6 P(c) 6 1. Let the probability mal distributions with unknown mixing coefficients P(c), density of the feature vector xi, which is fully observed, given
the kth component in the FMM be p(xi|hk)= N(xi; lk, Rk),
networks [5]. The proposed algorithm, based on the OCS, estimates the missing value as the average of these multiple estimated values weighted by the posterior probabilities of the feature vector containing the missing value to different components of the FMM (see, Eq. (3)).

where lk and Rk are the mean and the covariance matrix of this
component. The total density of xi from the FMM is then

no
c  o	k=1

xkq
2
k 
2r2  kc

PK	E(x^ |x ; R)= P		 
(2)

vector for each feature vector z = [z , z ,..., z ]T; the second
o	PK	c  o

type is the missing values in the different features of R. To rep-
resent the second type let each feature vector in R be rewritten
as xi = (xo; xm), where o and m superscripts denote the
c=1
where R = {rkc} is the matrix of memberships for each fea-

i	i

observed and the missing values in this feature vector,
ture vector xk to each component c in the FMM such that

_	_

respectively.
rkc is either one, if z > z for all t „ c, or zero otherwise, no

kc	kt


Step 1: Linearly scale the values of each feature in the input data set to make them lie in the interval [0,1]. This process removes the effect of different unit scales on different fea- tures, and therefore it is essential for finding the true cluster structure of this data set and the contribution of each fea- ture in the creation of this structure. A comparison of several methods of data normalization for cluster analysis has shown the superiority of the linear scaling method over many other normalization methods before applying cluster- ing algorithms in terms of the resulting cluster separation and error condition [16,17]. In addition, determine the opti-
is the number of feature vectors that are observed on both
of the observed subspace for the feature vector xi and the qth feature, and D2 is the squared Euclidean distance between feature vectors xk and xi on the observed subspace of the fea- ture vector xi.

After estimating its missing values, the qth feature is added to the group of fully observed features and this new group is then used in estimating the missing values in the next feature that has the minimum missing rate.
The necessary statistics for the M-step.

mum smoothing parameter r for each incomplete feature in


( z^ x
 
x  ∈ xo

method. This parameter is used in the locally-tuned general
regression neural networks for estimating missing values
ic	iq  i
iq	i

from neighboring feature vectors in every cluster [5]. The group of fully observed features used in determining r for
z^ic xiqxiq'	xiq ; xiq' ∈ xo
>< z^icxiqE(xc ' |xo; R)	xiq' ∈ xm

>>: ic
iq  i	iq
iq	i

feature. The feature vectors that are used in the leave- one-out cross validation method should be observed in
iq  i
iq  i
i
(5)

the entire fully observed feature group.
Step 2: As the EM algorithm converges toward the nearest local maximum of the likelihood function to the starting point [8], the EM algorithm is initialized several times using a swarm of particles each of which represents a FMM as
where i, q, q' = 1, 2,.. ., d, E is the expectation operator.
Step 4: In the M-step, compute parameters of each compo-
nent c in the FMM.
b	PN



sented in this paper. The FMM corresponding to the max- imum log-likelihood function after convergence is selected as the best model for the input data set.
-
lc =
1
-
N P(c)
N
E
j=1
zjcxj|xo; R!
(7)

Step 3: In the E-step, compute the following quantities for
each model component c in the FMM.

 

-	 1	  PN	T  o	!
_ _T

tors in the data set R.
Pb(c)p(xo|hc)
Step 5: After convergence, save the resulting FMM param- eters and the total log-likelihood of the feature vectors in

ic	PK
Pb(j)p(xo|h )
the data set in the corresponding particle. Since the EM




The estimates of the missing values in R starting with
those values in the feature that has the minimum miss-
ing rate. Multiple estimated values are computed for each missing value in the data set (see, Eq. (2)). Each estimated value is computed from one component in the FMM using the locally-tuned general regression neural
to the nearest local maximum of the likelihood function to
the starting point [18]. Therefore, the convergence criterion of the EM algorithm used in experiments presented in this paper is identical to the one used by Hunt and Jorgensen [19], which is to cease iterating when the difference in the
log-likelihood function between iterations (t) and (t — 10)

is less than or equal to 10.0E — 10. The use of this criterion does not affect the speed of the algorithm so much as the
main interest of this algorithm is to handle small data sets. Step 6: Repeat Steps 2–5 for every particle in the swarm and then select the best FMM stored in the particle that has the maximum log-likelihood of the data. This PSO is necessary to overcome the sensitivity to the initialization problem of the EM algorithm.
Step 7: Use the best FMM in estimating missing values in the data set as explained in Step 3 and in clustering feature vectors in the input data set according to Bayes decision rule such that each fully observed feature vector x is assigned to a certain component i if P(i|x)> P(j|x); for
all j „ i, where P(i|x) is the probability that x is generated
from the component i. The pseudo code of the POLGREM algorithm is shown in Fig. 2.

The POLGREM algorithm uses local tuning of the general regression (see, Eq. (2)) to produce multiple imputations for each missing value in the data set. Each imputation is obtained via a non-linear multivariate regression using a group of fully observed feature vectors belonging to one of the FMM compo- nents. These groups are obtained using Bayes decision rule. Then, the POLGREM estimates the missing value as the average of these multiple imputations weighted by the posterior probabilities of the feature vector containing the missing value to different components of the FMM. This makes the POLGREM algorithm be insensitive to global correlations between features of the input data set. Therefore, the POL-
GREM does not have the limitation of the general regression neural networks [20] when used for estimating missing values without local tuning with weakly correlated data [5]. In the experiments presented in this paper, the algorithm that uses general regression neural networks with the EM algorithm for learning FMM parameters is referred to as the GREM algo- rithm. In addition, local tuning of the general regression neural networks used in the POLGREM algorithm makes the algo- rithm be less dependent of FMM parameters. Therefore, the POLGREM algorithm overcomes the limitation of the MEM algorithm with small data sets which may contain outliers, overlapping clusters, or large differences in the sizes of their clusters [5]. Finally, due to the use of the OCS in estimating missing values the POLGREM algorithm produces accurate estimation of both the missing values in the data set and FMM parameters, especially when clusters of the data set are largely overlapping and different in their sizes. Therefore, the POLGREM algorithm overcomes problems of cluster overlap- ping and unbalanced cluster sizes that are limitations of the algorithm proposed in [5] and is referred to as the LGREM algorithm in the experiments presented in this paper.

Experiments and results

The POLGREM algorithm is evaluated and compared with a number of algorithms proposed in the literature in learning FMM parameters for clustering incomplete data sets. These algorithms are the LGREM algorithm [5], the MEM algorithm [4], the GREM algorithm and the common EM algorithm after




Figure 2	Pseudo code of the POLGREM algorithm.




estimating the missing values in the input data set using the unconditional mean imputation method (MENEM) and the nearest neighbor imputation method (NNEM). In the MENEM algorithm, the missing values in each feature are replaced with the mean of the observed values in that feature. In the NNEM algorithm, each missing value in a certain feature vector is replaced with the observed value in the same feature that is in the nearest feature vector according to the Euclidean distance in the subspace that is composed of the fully observed features. The comparison study in this paper uses three data sets. The missing values are randomly placed with different missing rates in two features of each data set. These features are selected such that the visual separation among data classes is maximum. The mechanism of the occurrence of the missing values in all data sets is missing completely at random (MCAR) [21]. These data sets are described as follows.
Correlations between different pairs of features of this data set are too weak. The missing values are put in the second and in the fifth features of the data set. Each one of the FMMs learnt by all algorithms compared in this study consists of two Gauss- ian components that have non-restricted covariance matrices.
The evaluation criterion used in this study to compare the different algorithms is the Mis-Classification Error (MCE). It is computed by comparing the clustering results, obtained using Bayes decision rule, of the learned FMM with the true classification of the data feature vectors, assuming each class is represented by a component in the FMM. Components of the FMM are allocated to different data classes such that the total number of misclassified feature vectors in the data set is minimum. Let the number of feature vectors belonging to class i be Ni, from which Nm feature vectors are not clustered into the component that represents this class in the FMM. Then
the MCE for class i is computed as MCEclass = Nm/Ni.

i	i

The first data set
Assuming the data set is generated from K classes the total
MCE is the average of all the class-MCEs and it is computed

This data set is an artificial data set, which contains 150 fea-
as MCE
=  1 ÿPK
MCE	 .

ture vectors (rows) generated with equal probabilities from
three well-separated 4D-Gaussian distributions. The mean
Tvectors  of  these  distributions  are  l = [ 2  2  2  2 ] ,

l = [ 4 4 4 4 ]T, and l = [ 6 6 6 6 ]T. Each one of these  Gaussian  distributions  has  a  covariance  matrix
R = 0.5I4, where I4 is the identity matrix of order four. In this data set, all features are strongly correlated.

The second data set

This data set is similar to the first data set but an outlier fea- ture vector is added to it. The outlier feature vector is [max (d1), min (d2), 0.5 max (d3), 0.5 max (d4)]T, where di, i = 1:4 are the features of the data set.
When each one of these two data sets is used, the missing values are put in the third and in the fourth data features. In addition, each one of the FMMs learnt by all algorithms com- pared in this study consists of three Gaussian components that have non-restricted covariance matrices.

The Pima Indians Diabetes data set

The Pima Indians Diabetes data set1 contains 768 feature vectors each of which is a vector in eight-feature space. These feature vectors represent two classes; the first class has 500 fea- ture vectors belonging to it; the second class has 268 feature vec- tors belonging to it. These classes are largely overlapping.

1 The Iris and the Pima data sets are available at: http:// archive.ics.uci.edu/ml/datasets.html.
Tables 1, 3 and 5 show comparisons of different pairs of the
algorithms using the Student’s paired t-test statistic with each one of the data sets. The P-value is the significance and the T-value is the t-statistic. This test examines the statistical sig- nificance of the difference in performance of pairs of algo- rithms using their total MCEs obtained from ten different experiments. In each experiment, a different group of feature vectors is randomly selected to contain missing values. The results of this test are shown for each pair of percentages of missing values in two different features. In the first two data sets the third and the fourth features contain missing values while in the Pima data set the second and the fifth features con- tain missing values. The shaded cells in each table represent the cases in which the difference in performance of certain pairs of algorithms is statistically significant according to the 5% sig- nificance level.
Tables 2, 4 and 6 show comparisons of the algorithms using the mean (MCE) and the standard deviation (STD) of the total Mis-Classification Error obtained from ten different experi- ments using each one of the data sets. The shaded cells in each table represent the minimum value of the MCE among all algorithms compared.

Discussion of results

Tables 1–4 show that the performance of the POLGREM algorithm is not significantly different from the LGREM, the GREM, and the MEM algorithms. On the other hand, it is significantly different (P 6 0.05) from the NNEM and the










MENEM algorithms and outperforms them (T < 0 and MCE is the minimum). These results show that the POLGREM algorithm is superior in estimating missing values and learning FMM parameters when the input data set is generated from well-separated clusters and contains features that are strongly correlated.
Tables 5 and 6 show that the performance of the POL- GREM algorithm is significantly different from all other algo- rithms (P 6 0.05) and better than them (T < 0 and MCE is the minimum). These results show that the performance of the POLGREM algorithm is superior when the input data set is generated from largely overlapping clusters and these clusters are of different sizes. Although the Pima data set contains
weakly correlated features and largely overlapping clusters of different sizes the POLGREM has the best performance among all other algorithms compared. This is because clusters of this data set are largely overlapping in the whole feature space and in the subspace that is composed of complete features.
In general, the performance of the POLGREM algorithm proves superiority over the other algorithms compared in this paper when the input data set is generated from overlapping clusters that are of different sizes (see the results with the Pima data set in Tables 5 and 6). In addition, this algorithm is sim- ilar to the LGREM, the GREM and the MEM algorithms in their superior performances when the input data set contains outliers and is generated from well-separated clusters that




are of the same sizes (see the results with the first and the third data sets in Tables 1–4). This is due to the use of the OCS and the locally-tuned general regression neural network in the POLGREM algorithm that considers the contribution of dif- ferent clusters in the data set in estimating the missing values which results in better estimation of both missing values in the data set and FMM parameters.

Conclusions

In this paper, the POLGREM algorithm is proposed to over- come the local optima problem of the EM algorithm and the bias problem in estimating missing values when the input data set contains overlapping clusters in learning FMM parameters for clustering using incomplete data sets. A comparison study shows the superiority of the POLGREM algorithm over other algorithms compared when the input data set may contain few outliers, clusters that are largely overlapping, or clusters that have large differences in their sizes. Examples of these algo- rithms are the LGREM algorithm [5], the MEM algorithm [4], the GREM algorithm and the common EM algorithm after estimating the missing values in the input data set using the unconditional mean imputation method (MENEM) and the nearest neighbor imputation method (NNEM).

References

McLachlan G, Peel D. Finite mixture models. New York: Wiley; 2000.
Tra˚ve´n H. A neural network approach to statistical pattern classification by semiparametric estimation of probability density functions. J IEEE Trans Neural Netw 1991;2(3):366–77.
Dempster A, Laird N, Rubin D. Maximum likelihood from incomplete data via the em algorithm (with discussion). J Roy Stat Soc 1977;B(39):1–38.
Ghahramani Z, Jordan M. Supervised learning from incomplete data via an EM approach. In: Cowan J, Tesauro G, Alspector J, editors. Advances in neural information processing systems. San Francisco, CA, USA: Morgan Kaufmann Publishers; 1994. p. 120–7.
Abas AR. Using general regression with local tuning for learning mixture models from incomplete data sets. Egypt Inform J 2010;11(2):49–57.
Abas AR. Using incremental general regression neural network for learning mixture models from incomplete data. Egypt Inform J 2011;12(3):185–96.
Yoon S, Lee S. Training algorithm with incomplete data for feed- forward neural networks. J Neural Process Lett 1999;10:171–9.
Vlassis N, Likas A. A greedy EM algorithm for gaussian mixture learning. J Neural Process Lett 2002;15:77–87.
Kennedy J, Eberhart R. Particle swarm optimization. In: Pro- ceedings of IEEE international conference on neural networks; 1995. p. 1942–48.
El_Sherbiny MM. Particle swarm inspired optimization algorithm without velocity equation. Egypt Inform 2011;12(1):1–8.
Das S, Abraham A. Pattern clustering using a swarm intelligence approach. In: Maimon O, Rokach L, editors. Data mining and knowledge discovery handbook. Springer Science + Business Media, LLC; 2010.
Hathaway RJ, Bezdek JC. Fuzzy c-means clustering of incomplete data. IEEE Trans Syst Man Cybern Art B: Cybern 2001;31(5):735–44.
Bezdek JC. Pattern recognition with fuzzy objective function algorithms. Norwell: Kluwer Academic Publishers; 1981.
Himmelspach L, Conrad S. Fuzzy clustering of incomplete data based on cluster dispersion. In: Hullermeier E, Kruse R, Hoffmann F, editors. IPMU 2010, LNAI 6178. Berlin, Heidel- berg: Springer-Verlag; 2010. p. 59–68.
Park DC. Gradient-based fcm and a neural network for clustering of incomplete data. In: Wang L, Chen K, Ong YS, editors. ICNC 2005, LNCS 3610. Berlin, Heidelberg: Springer-Verlag; 2005. p.
1266–69.
Milligan GW, Cooper MC. A study of standardization of variables in cluster analysis. J Classif 1988;5:181–204.
Schaffer CM, Green PE. An empirical comparison of variable standardization methods in cluster analysis. J Multivar Behav Res 1996;31(2):149–67.
Yin H, Allinson NM. Comparison of a Bayesian SOM with the EM algorithm for gaussian mixtures. In: Proceeding of workshop on self-organising maps (WSOM’97); 1997. p. 118–23.
Hunt L, Jorgensen M. Mixture model clustering for mixed data with missing information. J Comput Stat Data Anal 2003;41:429–40.
Specht D. A general regression neural network. J IEEE Trans Neural Netw 1991;2(6):568–76.
Little R, Rubin D. Statistical analysis with missing data. New York: John Wiley & Sons; 1987.
