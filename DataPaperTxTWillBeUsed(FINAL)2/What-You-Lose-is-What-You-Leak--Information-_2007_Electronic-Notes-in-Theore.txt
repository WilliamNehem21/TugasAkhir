

Electronic Notes in Theoretical Computer Science 173 (2007) 47–66
www.elsevier.com/locate/entcs

What You Lose is What You Leak: Information Leakage in Declassification Policies

Anindya Banerjeea,1	Roberto Giacobazzib,2 Isabella Mastroenib,2
a Kansas State University, Manhattan, KS 66506, USA
b Universita` di Verona, Verona, Italy

Abstract
This paper suggests the following approach for checking whether a program satisfies an information flow policy that may declassify secret information: (a) Compute a finite abstract domain that over-approximates the information released by the policy and (b) Check whether program execution may release more infor- mation than what is permitted by the policy by completing the finite abstract domain wrt. weakest liberal preconditions. Moreover, techniques based on the Paige-Tarjan algorithm for partition refinement can be
used to generate counterexamples to a declassification policy: the counterexamples demonstrate that more information is released by the program than what the policy permits. Subsequently the policy can be refined so that the least amount of confidential information necessary for making the program secure is declassified.
Keywords: Abstract interpretation, completeness, declassification, information flow


1  Introduction
The secure information flow problem is concerned with protecting data confiden- tiality by checking that secrets are not leaked during program execution. In the simplest setting, program variables are first partitioned into high security (or pri- vate or classified) and low security (or public or unclassified) variables, where high
(H) and low (L) are levels in a two point security lattice, L ≤ H; next, one checks that L output variables do not leak information about the initial values of H input variables. To perform the check, a variety of information flow analyses for confi- dentiality policies have been developed using technologies like data flow analysis,

1 Email: ab@cis.ksu.edu
2 Email: roberto.giacobazzi@univr.it,isabella.mastroeni@univr.it



1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.02.027

48	A. Banerjee et al. / Electronic Notes in Theoretical Computer Science 173 (2007) 47–66
security type systems, program logics, etc. (See the survey by Sabelfeld and My- ers [23] and references therein). The correctness of such analyses is governed by noninterference (NI) [13]: for any two runs of a program, L indistinguishable input states yield L indistinguishable output states, where two program states are said to be L indistinguishable iff they agree on the values of the L variables.
Joshi and Leino [15] give a semantic definition of secure information flow (that has been shown equivalent to NI [24]): a program P containing H and L variables (ranged over by h and l respectively) is secure iff HH; P ; HH = P ; HH, where HH is an assignment of an arbitrary value to h. “The postfix occurrences of HH on each side mean that we are only interested in the final value of l and the prefix HH on the left-hand-side means that the two programs are equal if the final value of l does not depend on the initial value of h” [24]. In practice, noninterference is too strong a property to be enforced and downgrading of information, or declassiﬁcation, is a necessity. For example, a password checker makes public the (H) result of the comparison between the actual password and the password entered at the login prompt.
This paper is based on the central observation that Joshi and Leino’s semantic definition permits a view of noninterference as completeness of an abstract inter- pretation [10], and the paper explores the consequences of this observation. An abstract interpretation is (backwards) complete for a function, f , if the result ob- tained when f is applied to any concrete input, x, and the result obtained when f is applied to an abstraction of the concrete input, x, both abstract to the same value. Thus, the essence of completeness is this: an observer who can see only the final abstraction cannot distinguish whether the concrete input value was x or any other concrete value x' with the same abstract value as that of x. The completeness connection is implicit in Joshi and Leino’s definition of secure information flow and
the implicit abstraction in their definition is: “each H value is associated with T, i.e., the set of all possible H values”. (This is discussed in Sects. 2 and 3).
In this paper, we consider more flexible abstractions than the one considered by Joshi and Leino and show that such abstractions naturally describe declassifi- cation policies that are concerned with what information is declassified [25]. Our primary contribution (Sects. 4,5.1) is to show that “declassified NI” (DNI), i.e, NI with a declassification policy, is also a completeness problem: the program points where completeness fails are the ones where some private information is leaked, thus breaking the policy. Hence, we can mechanically check if a program satisfies a declassification policy by checking whether its semantics is complete wrt. the policy. Moreover, we show that when a program does not satisfy a declassification policy (i.e, when completeness fails), (a) counterexamples that expose the failure can be generated (Sect. 5.2); (b) there is an algorithm that generates the best refinement of the given policy such that the program respects the refined policy (Sect. 5.3). Finally, (c) we connect abstract model checking with secure information flow by showing that the absence of spurious counterexamples in the former can be under-
stood as the absence of information leaks in the latter (Sect. 6).

Notational summary. VH, VL are the sets of possible H and L values. The set of program states is Σ = VH × VL. Σ is implicitly indexed by the H variables followed by the L variables. For any X ⊆ Σ, XH (resp. XL) is the projection of the H (resp. L) variables. L indistinguishability of states s1, s2 ∈ Σ, written s1 =L s2, denotes that s1, s2 agree when indexed by L variables.
Semantic noninterference a` la Joshi-Leino. We start with Joshi and Leino’s se-
mantic definition of security [15], HH; P ; HH = P ; HH, where HH assigns to h an arbitrary value. Because of the arbitrary assignment, the semantics of HH can be modelled as an abstraction function, H, on sets of concrete program states, Σ; that is, H : ℘(Σ) → ℘(Σ), where ℘(Σ) is ordered by subset inclusion, ±. For each possible value of an L variable, H associates all possible values of the H variables in P . Thus H(X) = VH × XL, where VH = T, the top element of ℘(VH). Now the Joshi-Leino definition can be rewritten [10] in the following way, where P ) is the concrete, denotational semantics of P .
H◦  P ) ◦H = H◦  P )	(1)
For example, let h1, h2 ∈ {0, 1} and let l ∈ {0, 1}. Then VH = {0, 1} × {0, 1}, VL = {0, 1}. Consider any X ⊆ Σ; for example, let X = {⟨0, 0, 1⟩}, i.e., X denotes the state where h1 = 0, h2 = 0, l = 1. Then H(X)= VH × {1}. Let P be l := h1, so that,  P )(X) = {⟨0, 0, 0⟩} and H( P )(X)) = VH × {0}. On the other hand,
 P )(H(X)) = {⟨0, 0, 0⟩, ⟨0, 1, 0⟩, ⟨1, 0, 1⟩, ⟨1, 1, 1⟩} so that we have H( P )(H(X))) =
VH × {0, 1}; hence H( P )(H(X))) ⊇ H( P )(X)). Because H( P )(H(X))) contains triples ⟨1, 0, 1⟩ and ⟨1, 1, 1⟩ not present in H( P )(X)), the dependence of l on h1 has been exposed. Thus P is insecure: for any two distinct values, 0 and 1 of h1 in H( P )(H(X))), two distinct values, 0 and 1, of l may be associated.
Declassiﬁcation. For l := h1, had the security policy allowed declassification of h1, the program would be secure. Equation (1) must naturally be modified by “filtering” H through a declassifier, φ : ℘(VH) → ℘(VH), that provides an abstraction of the secret inputs. The “filtered” H, written Hφ, models the declassification policy. Thus we enforce the equality
H◦  P ) ◦ Hφ = H◦  P )	(2)
That is, P ) applied to a concrete input, x, and P ) applied to the abstraction of x where the H component of x has been declassiﬁed by φ, both abstract to the same value.
As before, let P be l := h1 and X = {⟨0, 0, 1⟩}. We are interested in φ’s behavior on {⟨0, 0⟩}, because {⟨0, 0⟩} specifies the values of h1, h2 in X. We have, φ({⟨0, 0⟩}) = {⟨0, 0⟩, ⟨0, 1⟩}: φ is the identity on what must be declassi- fied – we are releasing the exact value of h1 – but φ is T on what must be protected, which explains why both ⟨0, 0⟩ and ⟨0, 1⟩ appear. Now Hφ(X) = φ{⟨0, 0⟩} × XL = {⟨0, 0, 1⟩, ⟨0, 1, 1⟩} so that  P )(Hφ(X)) = {⟨0, 0, 0⟩, ⟨0, 1, 0⟩} and H( P )(Hφ(X))) = VH × {0}. This is equal to H( P )(X)). We can show equa- tion (2) for any X ⊆ Σ; hence l := h1 is secure. Note how φ partitions ℘(  ) into

blocks {⟨0, 0⟩, ⟨0, 1⟩} (the range of ⟨0, 0⟩ and ⟨0, 1⟩) and {⟨1, 0⟩, ⟨1, 1⟩} (the range of
⟨1, 0⟩ and ⟨1, 1⟩). Intuitively, φ permits exposing distinctions between blocks at the public output, e.g., between ⟨0, 0⟩ and ⟨1, 0⟩; in standard NI, φ’s range is T and no distinctions should be exposed (as in the earlier example).

Review: Completeness of abstract interpretation
Abstract interpretation is typically formulated using Galois connections (GC) [5], but an equivalent framework [6] which we use in this paper, uses upper closure operators 3 . For example, in Sect. 2, H : ℘(Σ) → ℘(Σ) defined as H(X)= VH ×XL, is an upper closure operator on ℘(Σ), because H is monotone, idempotent and extensive. We often call a closure operator an abstract domain. In particular, H is called the output (i.e., observed) abstract domain, that ignores private information. Likewise, Hφ in Sect. 2 is also an uco.
Completeness of abstract interpretation based static analysis has its origins in
Cousot’s work, e.g., [5,6], and means that the analysis is as expressive as possible. The following example is taken from Schmidt’s excellent survey [26] on complete- ness. To validate the Hoare triple, {?} y := −y; x := y +1 {isP ositive(x)}, a sound analysis may compute the precondition isN egative(y). But if able to express prop-
erties like isN onN egative and isN onP ositive, a complete analysis will calculate the weakest precondition property isN onP ositive(y).
An abstract domain is complete for a concrete function, f , if the “abstract state transition function precisely mimics the concrete state-transition function modulo the GC between concrete and abstract domains” [26]. There exist two notions of
completeness – backward (B) and forward (F) – according as whether the concrete and the abstract computations are compared in the abstract domain or in the con- crete domain [11]. Formally, let C be a complete lattice and f be the concrete state transition function, f : C → C. Abstract domain ρ is a sound abstraction for f provided ρ ◦ f ◦ ρ ± ρ ◦ f . For example, in Sect. 2, H( P )(H(X))) ⊇ H( P )(X)), so H is a sound abstraction for P ). Completeness is obtained by demanding equal- ity: ρ is a B (resp. F)-complete abstraction for f iff ρ ◦ f = ρ ◦ f ◦ ρ (resp. f ◦ ρ = ρ ◦ f ◦ ρ). Completeness can be generalized to pairs (ρ, η) of abstract do- mains: B-completeness holds for (ρ, η) when ρ ◦ f ◦ η = ρ ◦ f ; F-completeness holds for (ρ, η) when ρ ◦ f ◦ η = f ◦ η (see [12] for details). For example, in Sect. 2, the de- classification example asserts that equation (2) holds, i.e., (H, Hφ) is B-complete for
 P ). Algorithms for completing abstract domains exist – see [12,11] and Schmidt’s
survey [26] for details. Basically, F-completeness is obtained by adding all the direct images of f to the output abstract domain; B-completeness is obtained by adding all the maximal of the inverse images of the function to the input domain (see Appendix A for details).


3 An upper closure operator (uco) ρ : C → C on a poset C is monotone, idempotent, and extensive, i.e.,
∀x ∈ C. x ≤C ρ(x). The set of all upper closure operators on C is denoted by uco(C).

F-completeness	and	satisfaction	of	confidentiality policies
Equations (1) and (2) give us a way to dynamically check whether a program satis- fies a confidentiality policy: indeed, both equations use the denotational semantics of a program in the process. But can we do this check statically?
We will see presently that static checking involves F-completeness, instead of B-completeness, and the use of weakest liberal preconditions instead of the deno- tational semantics. With weakest liberal preconditions, (written WlpP ), equation
(1) has the following equivalent reformulation:
H◦ WlpP ◦H = WlpP ◦H	(3)
Equation (3) says that H is F-complete for WlpP . In other words: consider the abstraction of a concrete input state, X, via H; this yields a set of states where the private information is abstracted to “any possible value”. The equation asserts that WlpP (H(X)) is a fixpoint of H, meaning that WlpP (H(X)) yields a set of states where each public input is associated with any possible private input: a
further abstraction of the fixpoint (c.f., the lhs of equation (3)) yields nothing new. Because no distinctions among private inputs get exposed to an observer, the public output is independent of the private input. Hence equation (3) asserts standard NI.
The following theorem asserts that the two ways of describing noninterference by means of B- and F-completeness are equivalent.
Theorem 4.1 H ◦ P ) ◦ H = H ◦ P ) iff H ◦ WlpP ◦ H = WlpP ◦ H.
Proof. By [11,12] we know that, if f is additive, then for any ρ we have ρ ◦ f ◦ ρ = ρ ◦ f iff ρ ◦ f + ◦ ρ = f + ◦ ρ. By [4] we have that P )+ = WlpP . Choosing H, P ) as ρ, f resp., we are done.
Example 4.2 Consider the program P , let VH = {0, 1}× {0, 1}, VL = {0, 1}.


⎡ if h /= h	then l := h + h
 P ) : ⟨h1, h2, l⟩ '→ ⟨h1, h2, 1⟩

def	1	2	1	2
P =
⎧⎨ ⟨h1, h2, 1⟩ '→ {⟨h1, h2⟩} × VL

else l := h1 − h2 + 1;
WlpP :
⎩ ⟨h1, h2, l⟩ '→ ∅	l /=1 

The public output l is always 1, hence P is secure, as the following calculation shows. Given VH × {l} ∈ H, we can prove that B-completeness for P ) holds:
H( P )(VH × {l})) = H(VH × {1})= VH × {1} = H(⟨h1, h2, 1⟩)= H( P )(⟨h1, h2, l⟩))
F-completeness for WlpP holds also:
H(WlpP (VH × {1})) = H(VH × VL)= VH × VL = WlpP (VH × {1}) H(WlpP (VH × {l /= 1})) = H(∅)= ∅ = WlpP (VH × {l /= 1})

Completeness and Declassified NI (DNI)
When does a program P satisfy noninterference declassiﬁed by φ? Consider any
two runs with states s1, s2 ∈ Σ. Suppose s1 =L s2. Let sH and sH denote the secret
1	2
values in s1, s2 that are declassified by φ and suppose that the distinction between
the declassified values is not exposed in the two runs of P , i.e, φ(sH)= φ(sH). Then
1	2
P satisfies noninterference declassified by φ provided  P )(s1) =L  P )(s2). Formally:
s1 =L s2 ∧ φ(sH)= φ(sH) ⇒  P )(s1) =L  P )(s2)
Note that ordinary noninterference is obtained by setting φ(sH)= T = φ(sH).
1	2

Modelling declassiﬁcation
The discussion in the previous section has not motivated why we might want WlpP
and this is what we proceed to do in the context of declassification.
Consider secrets h1, h2 ∈ {0, 1} and the declassification policy “at most one of the secrets h1, h2 is 1”. The policy releases a relation between h1 and h2 but not
def
their exact values. Does the program P = l := h1 + h2 satisfy the policy?
Here VH = {0, 1} × {0, 1} and the declassifier, φ, is defined as: φ(∅) = ∅; φ{⟨0, 0⟩} = φ{⟨0, 1⟩} = φ{⟨1, 0⟩} = {⟨0, 0⟩, ⟨0, 1⟩, ⟨1, 0⟩} (i.e., we collect together all the elements with the same declassified property) and φ{⟨1, 1⟩} = VH; φ(X) = x∈X (φ({x})). A program that respects the above policy should not expose the distinctions between inputs ⟨0, 0⟩, ⟨0, 1⟩ and ⟨1, 0⟩ at the public output. But it is permissible to expose the distinction between ⟨1, 1⟩ and any pair from the partition block {⟨0, 0⟩, ⟨0, 1⟩, ⟨1, 0⟩}, because this latter distinction is supported by the policy.
Does P expose distinctions it should not?
To answer, we consider WlpP (l = a), where a is some generic output value. Why Wlp? Because then we can statically simulate the kind of analysis an attacker would do for obtaining initial values of (or initial relations among) secret information. Why l = a? Because this gives us the most general Wlp, parametric on the output value.
def
Now, note that WlpP (l = a) = (h1 + h2 = a); let Ha = (h1 + h2 = a). Because
def
a ∈ {0, 1}, we have H0 = (h1 + h2 = 0). This allows the attacker to solve for h1, h2:
h1 = 0, h2 = 0. Thus when l = 0, a distinction, {⟨0, 0⟩}, in the partition block,
{⟨0, 0⟩, ⟨0, 1⟩, ⟨1, 0⟩} gets exposed. Hence the program does not satisfy the policy. So consider a declassified confidentiality policy, and model the declassified in-
formation by means of the abstraction φ, of the private inputs, which collects to- gether all the elements with the same property, declassified by the policy. Let Hφ : ℘(Σ) → ℘(Σ) be the corresponding abstraction function. Accordingly, let
X ∈ ℘(Σ) be a concrete set of states and let XL be the L slice of X. Consider
def
any l ∈ XL. Define set Hl = {h ∈ VH | ⟨h, l⟩ ∈ X}; i.e., given an l, H contains
all the H values associated with l in X. Then the “declassified” abstract domain,
Hφ(X), corresponding to X is defined as Hφ(X) =	l∈XL φ(Hl) × {l}. Note that the domain, H, for ordinary noninterference is the instantiation of Hφ, where φ

maps any set to T. The analogue of equation (2)
7φ ◦ WlpP ◦ 7 = WlpP ◦ 7	(4)
asserts that (7φ, 7) is J-complete for WlpP . For example, J-completeness fails for the program P . With X = ⟨0, 0, 0⟩, we have 7(X)= UH × {0} and WlpP (7(X)) =
{⟨0, 0, 0⟩}. But 7φ(WlpP (7(X))) = {⟨0, 0, 0⟩, ⟨0, 1, 0⟩, ⟨1, 0, 0⟩} э WlpP (7(X)).
We are now in a position, via Theorem 5.1 below, to connect 7φ to NI: the only caveat is that φ must partition the input abstract domain, i.e., 6x. φ(x) =
{y | φ(x)= φ(y)}. The intuition behind partitioning is that φ’s image on singletons is all we need for deriving the property of any possible set.
Theorem 5.1 Consider a partitioning φ. Then P satisﬁes noninterference declas- siﬁed by φ iff 7 ◦ P ) ◦ 7φ = 7 ◦ P ).
Together with Theorem 4.1 we are led to
Corollary 5.2 Consider a partitioning φ. Then P satisﬁes noninterference declas- siﬁed by φ iff 7φ ◦ WlpP ◦ 7 = WlpP ◦ 7, i.e., (7φ, 7) is J-complete for WlpP .
The equality in the corollary asserts that nothing more is released by the Wlp than what is already released by φ. If J-completeness did not hold, but (7φ, 7) was merely sound, then 7φ ◦ WlpP ◦ 7 ± WlpP ◦ 7. In this case Wlp (i.e., the rhs) releases more information (technically: is more concrete) than that declassified
(i.e., the lhs). Our goal is not only to check whether a program satisfies a particular confidentiality policy, but also to find the public observations that may breach the confidentiality policy and also the associated secret that each offending observation reveals. Consider, for example, the following program [7] where l, h ∈ Nats.

def
P = while (h > 0) do (h := h — 1; l := h) endw
If we observe l = 0 at output, all we can say about input h is h ≥ 0. But with output observation l /= 0, we can deduce h = 0 in the input: the loop must not have been executed.
Because Wlp relates the observed (public) output to the private (secret) inputs, therefore, from the final observation we can derive the exact secret which is released
by that observation in the following manner: (a) Compute wlp wrt. each observation obtaining a most general predicate on the input states. (b) Check whether the states described by the wlp are “more abstract”, i.e., do not permit more distinctions of private input than those permitted by the policy. If so, there is no breach.
Example 5.3 Consider the following code [22]

def
P = h := h mod 2; if h =0 then (h := 0; l := 0) else (h := 1; l := 1);
Let UH = Nats = UL. Suppose we wish to declassify the test, h = 0. Then φ({0}) = {0} and φ({h}) = Nats z {0}. Thus {h | h /= 0}, {0} is the partition induced by φ on UH and we obtain 7φ = {ø, T} ∪ {{h | h /= 0}× UL}∪ {{0}× UL}.

Let Ha
d=ef UH × {a}. Consider now the wlp of the program, Wlp
(l = a), where

a ∈ UL.
{(a =0 ∧ h mod 2= 0) ∨ (a =1 ∧ h mod 2= 1)}
h := h mod 2;
{(a =0 ∧ h = 0) ∨ (a =1 ∧ h = 1)}
if (h = 0) then (h := 0; l := 0) else (h := 1; l := 1)
{l = a}
Thus, Wlp maps output set of states H0 to the input states {⟨h, l⟩ | h mod 2 = 0,l ∈ UL}. But this state is not more abstract than the state, {h | h /= 0} × UL, specified by 7φ: it distinguishes, e.g., (8, 1) from (7, 1) - a distinction not permitted under the policy. Indeed, consider two runs of P with initial values 8 and 7 of h and 1 for l; φ(8) = φ(7); yet we get two distinct output values of l.


def
Example 5.4 Consider P = if (h ≥ k) then (h := h — k; l := l + k) else skip [22],

and its Wlp semantics. Consider H
a,b
d=ef {⟨h, l, k⟩ | h ∈ UH, l = a, k = b}. Suppose

the declassification policy is T, i.e., nothing has to be released.

{(h ≥ b ∧ l = a — b ∧ k = b) ∨(h < b ∧ l = a ∧ k = b)}
if (h ≥ k) then (h := h — k; l := l + k) else skip
{l = a ∧ k = b}

WlpP : Ha,b '→ {⟨h,a — b, b⟩ | h ≥ b} ∪ {⟨h, a, b⟩ | h < b}
In this case, we can say that the program does not satisfy the security policy. In fact, in presence of the same public inputs we can distinguish between values h greater than the initial value of k, and lower than this value. Note that, in this
case the way WlpP (Ha,b) partitions the private value domain depends also on the
public input. This is not a problem, since by completing the input domain with
these elements we are able to induce a partition of the private domain only. In

this way, H
a,b
d=ef {⟨h, l, k⟩ | h ∈ UH, l = a, k = b} has to be split in the ele-

ments H'
d=ef {⟨h, l, k⟩ | h ∈ UH, l = a, h ≥ k = b} distinguishing h ≥ k, and

''  def
a,b
H

Ha,b = {⟨h, l, k⟩ | h ∈ U , l = a, h < k = b} distinguishing h < k, and hence the
initial policy T does not guarantee security.
Example 5.5 Consider the Oblivious Transfer Protocol [21], with principals Alice and Bob. Alice has two messages. Bob knows the messages by name but not by content. Bob asks for a message by name. But Alice does not know which message

Bob asked for, and Bob has not to find out the content of the other message.
⎡ r0, r1 :∈ M ; d :∈ {0, 1};
r := rd;
P =	e := c ⊕ d;
⎢⎢ f0, f1 := m0 ⊕ re, m1 ⊕ r1⊕e;
c

The protocol is implemented via a trusted third party, Ted, who sends the random messages r0, r1 to Alice and the random bit d to Bob. In the implementation (due to C. Morgan) M denotes the set of messages and ⊕ is xor; the table above shows what is Hid (“hidden” or H) and Vis (“visible” or L) for Alice and Bob. Bob
randomly chooses bit c and sends Alice e = c ⊕ d. Alice sends Bob f0, f1 whence Bob can now obtain mc as fc ⊕ r.
If we compute the Wlp we derive that the relations disclosed are f0 = m0 ⊕ r0
and f1 = m1 ⊕ r1 when c = d, and f0 = m0 ⊕ r1 and f1 = m1 ⊕ r0 when c /= d. In both cases, the message mc that Bob can read is combined with the random message Bob knows (since r = rd and r public to Bob). We can summarize the Wlp in the following way: fc = mc ⊕ rd and f1⊕c = m1⊕c ⊕ r1⊕d. Hence, f1⊕c tells
almost nothing about the hidden message m1⊕c, expressing only if it is equal or not
with an unknown random message, r1⊕d 4 .
Deriving counterexamples
Can we mechanize the derivation of counterexamples? That is, can we derive exactly where the policy fails by demonstrating two input states that break noninterference? We have advanced the thesis that noninterference is a completeness problem in abstract interpretation. Ranzato and Tapparo [20] studied completeness in abstract interpretation from a more algorithmic point of view. They show a correspondence between completeness and the Paige-Tarjan (PT) algorithm [19] for partition re- finement, that derives the coarsest bisimulation of a given partition. Hence, we have a correspondence between completeness and absence of unstable elements of a
closure wrt. a function f : Given a partition Π ⊆ ℘(C) and f : ℘(C) —→ ℘(C), an element X ∈ Π is stable for f with respect to Y ∈ Π if X ⊆ f (Y ) or X ∩ f (Y )= ø; otherwise X is unstable. The understanding of completeness in terms of stability
guarantees that if an abstract domain is not complete than there exist at least two of its elements which are not stable. In our context, f , is Wlp; the element for which we want to check stability is a set of private inputs in the partition of UH
induced by the declassifier, φ; and the element against which we check stability (Y in the definition) is the particular output observation (e.g., l = a).
Proposition 5.6 Unstable elements of 7φ provide counterexamples to φ.

4 We leave a probabilistic analysis of “almost nothing” as future work.

Proof. Suppose that ∃l ∈ UL such that (the input states described by) Wlp(Hl) ∈/

7φ. Then there exist x ∈ Wlp(Hl), and h ∈ φ(xH) such that ⟨h, xL⟩ ∈/
Wlp(Hl).

Note that φ(xH) × {xL} ∩ Wlp(Hl) /= ø since x is in both; and, φ(xH) × {xL} /⊆
Wlp(Hl) since ⟨h, xL⟩ ∈ φ(xH) × {xL} and ⟨h, xL⟩ ∈/ Wlp(Hl). Hence, the abstract domain 7φ is not stable. To find a counterexample consider h1 ∈ φ(xH) z {k |
⟨k, xL⟩ ∈ Wlp(Hl)} and h2 ∈ {k | ⟨k, xL⟩ ∈ Wlp(Hl)}. The latter set is obtained
by wlp for the output observation l, hence any of its elements, e.g., h2, leads to the
observation l, while all the elements outside the set, e.g., h1, cannot lead to l.	 

Example 5.7 Consider the following program with h’s parity declassified. We can
def
compute WlpP wrt. l = a ∈ Z, and Ha = {⟨h, l⟩ | h ∈ UH,l = a}.

{(h =0 ∧ l = a) ∨ (h > 0 ∧ a = 0)}
while (h > 0) do (h := h — 1; l := h) endw
{l = a}
⎧⎨ H0 '→ {⟨h, l⟩ | h > 0,l ∈ UL}∪ {⟨0, 0⟩}

Wlp :
⎩ Ha '→ {⟨0, a⟩}	(a /= 0)

Hence, Wlp(H0) = (UH × {0}) ∪ {⟨h, l⟩ | h > 0,l /= 0}. Thus, all the input states where l = 0 are not counterexamples to the declassification policy. On the contrary, for any two runs agreeing on input l /= 0, whenever h1 =0 and h2 ∈ Evens z {0}, we observe different outputs. Hence, we can distinguish more than the declassified partition {Evens, Odds}.

The following example [16] shows that this approach provides a weakening of noninterference which corresponds to relaxed noninterference. Both approaches provide a method for characterizing the information that flows and that have to be declassified, indeed they both give the same result since they are driven by (para- metric on) the particular output observation. However, let us underline that the abstract interpretation-based approach allows also to derive the maximal informa- tion disclosed independently from the observed public output [17].

Example 5.8 Consider the program P [16] with sec, x, y : H, and in, out : L, where
hash is a function:
⎡ x := hash(sec); y := x mod 264;
P =	if y = in then out := 1 else out := 0;
z := x mod 3;

Consider its Wlp semantics where out, in and z are respectively a, b, c ∈ Z:
{(a = 1, out = a, hash(sec) mod 264 = b, hash(sec) mod 3= c) ∨
(a = 0, out = a, hash(sec) mod 264 /= b, hash(sec) mod 3= c)}
x := hash(sec); y := x mod 264;
{(a = 1, out = a, y = b, x mod 3= c) ∨ (a = 0, out = a, y /= b, x mod 3= c)}
if y = in then out := 1 else out := 0;
{out = a, in = b, x mod 3= c}
z := x mod 3;
{out = a, in = b, z = c}
Let us consider first P without the last assignment to z and consider the domain

formed by the sets H
def	64

and H
def
in,1 = {⟨sec, in, out, x, y⟩ | in = hash(sec) mod 2
64
, out = 1}

in,0 = {⟨sec, in, out, x, y⟩ | in /= hash(sec) mod 2
, out = 0}. The set of all

these domains embodies the declassification policy since it collects together all the tuples such that sec has the same value for hash(sec) mod 264. At this point note that the WlpP semantics does the following associations: Wlp : Hin,a '→ Hin,a and this clearly means that the domain is complete, i.e., the declassification policy is
sufficient to protect the program.
Let us consider now also the last assignment, then we have one more variable and we redefine Hin,a as sets of tuples containing also z but without any condition on z since it is not considered in the declassification policy. In this case, the Wlp
semantics does the following associations:
⎧⎪⎨ Hin,1 '→ Hin,1 ∩  ⟨sec, in, out, x, y, z⟩ hash(sec) mod 3= z ,

Wlp :
⎪⎩
Hin,0 '→ Hin,0 ∩
⟨sec, in, out, x, y, z⟩ hash(sec) mod 3= z ,

The new elements added to the domain have one more condition on the private variable sec, which can distinguish further the private inputs by observing the public output. This makes the initial declassification policy unsatisfied.

Reﬁning conﬁdentiality policies
The natural use of the method previously described is for a semantic driven reﬁne- ment of confidentiality policies. The idea is to start with a confidentiality policy stating what can be released in terms of abstract domains (or equivalence relations). In the extreme case, the policy could state that nothing about private information must be released.
A consequence of Corollary 5.2 is that whenever 7φ is not forward complete for WlpP , more information is released than what declassification φ permits. Thus

the partition induced on the private domain by φ must be reﬁned by the comple- tion process. To derive the refined policy, φ', we perform the following steps: (a)
Consider the domain, 7φ', obtained by completion from 7φ; (b) for each Y ∈ 7φ'
compute sets, πl(Y ), that are parametric on a fixed public value l ∈ UL, where:
π (Y ) d=ef {h ∈ UH | ⟨h, l⟩ ∈ Y }; (c) for each l, compute the partition, π , induced on

def	'
def 
l
π . The declassifica-

tion policy, φ', can now be defined as a refinement, R(φ) of φ, by computing the
partitioning closure corresponding to π [14], i.e., the disjunctive completion,  , of

the sets forming the partition: φ'
def
= R(φ) =
(π).

For instance, in Example 5.4, each output observation k = b induces the partition
πb = {{h | h ≥ b}, {h | h < b}}, which is the information released by the single observation. If we consider the set of all the possible observations, then we derive π = b πb = id, namely we have φ = id.
Proposition 5.9 Let φ model the information declassiﬁed. If 7φ ◦ WlpP ◦ 7 N WlpP ◦ 7, then R(φ) и φ, i.e., R(φ) is a reﬁnement of φ, and it is the closest to φ 5
Example 5.10 Consider the program P [22] with UH = UL = Z and its Wlp
semantics

def
P = h1 := h1; h2 := h1; ... hn := h1; avg := declassify((h1 + h2 + ... + hn)/n)
{h1 = a}
h1 := h1; h2 := h1; ... hn := h1;	⎧⎪ X  '→ ø	if 6a ∈ UL.X /= Ha
{(h1 + h2 + ... + hn)/n = a} Wlp : ⎪⎨	⎧⎨	 6i. hi ∈ Z ⎫⎬

avg := (h1 + h2 + ... + hn)/n
{avg = a}
⎪⎪⎩ Ha '→ ⎩ ⟨a, h2,... , hn, a⟩ 
avg = a	⎭

where H
def
= {⟨h
,... ,h , avg⟩ | h
∈ Z, (h
+ h + ... + h
)/n = avg = a ∈ Z}. Sup-

a	1	n	i	1	2	n
pose the input declassification policy releases the average of the private values, i.e,
def	'	'	'	'
φ(⟨h1,... , hn⟩) = {⟨h1 ,... , hn⟩| (h1 +.. .+hn)/n = (h1+.. .+hn)/n}; the policy col-
lects together all the possible private inputs with the same average value. Hence, the
average is the only property that this partition of states allows to observe. Clearly, the program releases more. Consider n = 4, hi ∈ {1, ..., 8}, and X = H4. The parti-
tion induced by 7φ(X) on the states with avg =4 is {⟨5, 2, 3, 6, 4⟩, ⟨7, 3, 1, 5, 4⟩,. . .}. But WlpP (H4)= {⟨4, 3, 7, 2, 4⟩, ⟨4, 8, 3, 1, 4⟩,.. . }. Thus, we need to refine the origi- nal policy, completing 7φ(X) wrt. WlpP : we add elements WlpP (Ha) for all a ∈ Z.
φ'
In each such element, h1 has the particular value a. Formally, the domain, 7 (X),
contains all the sets {⟨h1, h2,... , hn, avg⟩ | h1 = avg = a, 6i > 1. hi ∈ Z}; 7φ'(X) distinguishes all tuples that differ in the first private input, where φ' is obtained as disjunctive completion of the computed partition and declassifies the value of h1.

5 In theory, this refinement can also be computed as the intersection between the policy φ and the re- finement of the undeclassified policy T. Efficiency comparison between these two approaches is left to the implementation phase of our work.

This is the closest domain to φ since, if we add any other element in the resulting domain, we would distinguish more than what is necessary, i.e., more than the dis- tinction on the value of h1. Indeed, still abstracting the average of the elements, we could add other sets of tuples with the same average value, but the only ones that we can add (i.e, which are not yet in the domain) must add some new distinctions. For example, if we add sets like {⟨4, 6, 1, 5, 4⟩, ⟨4, 6, 3, 3, 4⟩,.. . }, where also h2 is
fixed, then we allow also to distinguish the value of h2, which is not released by the
program.

Reﬁning Abstract Noninterference policies
The method described for checking and refining a security policy is parametric on public observations, but one could carry out the same process on properties. If some information about the execution context of the program is present then we can restrict (abstract) the possible observations. These restrictions can be modeled as abstract domains, and therefore by means of abstract noninterference policies. In particular, it has been proved [10] that the more we observe about public infor- mation, the less private information can be kept secret. This means that a security policy, unsafe in a general context, can become safe if we consider a weaker obser- vation of the public output.
Consider, for example, the following program P with two private inputs x, y, and two public outputs xL, yL. d, dx, dy are constant public inputs with dx > d and dy > d:

⎡ if(d ≤ x + y ≤ d + dx + dy ∧ —dy ≤ x — y ≤ dx) then







def
⎢	if(x ≥ 0 ∧ x ≤ d) then xL := d;
⎢	if(x > d ∧ x ≤ dx) then xL := x;
if(x > d	∧ x ≤ d + d) then x  := d ;

P =	x	x	L	x
⎢
if(y ≥ 0 ∧ y ≤ d) then yL := d;
⎢	if(y > d ∧ y ≤ dy) then yL := y;
⎣	if(y > dy ∧ y ≤ dy + d) then yL := dy;

Instead of concrete inputs and outputs, we might want to track properties, e.g., in what interval a particular variable lies. The figure above represents the input and output of the program in graphical form: the program transforms an input property, namely, an octagon (in the private variables x, y, represented by sets of constraints of the form ±x ± y ≤ c) to an output property, namely, a rectangle (in the variables
xL, yL): Thus if we take WlpP wrt. the property of intervals – this corresponds to
rectangles in the 2-dimensional space – then the WlpP semantics returns an octagon

abstract domain [18], i.e., we derive an octagonal relation between the two private inputs. Thus the security policy has to declassify at least the octagon domain in order to make the program secure.
Moreover, abstract noninterference policies can be useful in order to make the algo- rithm computable. In fact by abstracting the public domain we can make finite the amount of possible observations of the attacker; in practice, this means that when
we compute Wlp(ρ(l)= A) we are guaranteed that there will be finitely many Wlp
computations whenever the abstract domain is finite.
This example shows that we can combine (narrow) abstract non interference
def
[9] with declassification in the following completeness equation: Let 7ρ = λX.UH ×
L	φ def	def	'	'

ρ(X
) [10] and 7η = λX.φ(Hη(l)) × η(l), where Hη(l) = {h | η(l )= η(l), ⟨h, l ⟩ ∈ X}
7φ ◦ WlpP ◦ 7ρ = WlpP ◦ 7ρ

Abstract model checking and information flow
In the previous sections we have seen how we can verify and refine confidentiality policies that admit some leak of private information. The whole study is done by considering I/O semantics (denotational and wlp) and modelling DNI as a com- pleteness problem. On the other hand, the strong relationship between complete- ness and stability (in the Paige-Tarjan sense) existing in the framework of abstract model checking (AMC) has been studied [20,11]: the completeness in question is
B-completeness for the post function induced by an equivalence relation on the do- main of states. Since the denotational semantics is the post for a transition system where all traces are two-states long – because they are I/O states – a straightforward
generalization of our work and of the notion of noninterference can be obtained via a generic post function. Hence, two traces are L indistinguishable, i.e, =L, if they have the same public projection.
Theorem 6.1 Let ⟨|P |⟩ the standard trace semantics of P (deterministic program). The noninterference on traces, i.e., 6σ1, σ2 ∈ Σ.σ1 =L σ2 ⇒ ⟨|P |⟩(σ1) =L ⟨|P |⟩(σ2), holds iff 7 ◦ postP ◦ 7 = 7 ◦ postP , where postP is the post function associated with
the transition system modelling P.
This theorem implies that we can characterize the declassification property on the private information of states also when we have to protect the whole trace seman- tics from malicious observations. Moreover, the completeness equation, rewritten as
◦ pre ◦ 7 = pre ◦ 7 (via Theorem 4.1), asserts (in the context of AMC) that there
are no spurious counterexamples. In the NI context, this means that there is no leakage of information. In particular, for declassiﬁcation, if 7φ ◦ preP ◦ 7 = preP ◦ 7 holds, there is no need to further declassify private information via refinement, even
if we suppose that the attacker can observe every intermediate step of computation. In the following simple example we show how this approach works by also providing its relationship with the equivalence relation transformer defined by Zdancewic and Myers [28] for characterizing leakages of private information. First, we rewrite their

transformer as follows: Let ≈ be an equivalence relation, we define σ1 S(≈) σ2 if and only if 6i > 0. [posti([σ1]≈)]≈ = [posti([σ2]≈)]≈. (posti is the composition of post with itself i times.) This new equivalence relation, if different from ≈, tells us that something is released. With our method, we can characterize exactly what
is released. When we deal with equivalence relations, the backward completeness equation for post can be rewritten as [14]: [post[σ1]≈]≈ = post[σ1]≈; and so we are led to
Theorem 6.2 S(≈) =≈ iff ≈ is backward complete for post.
Example 6.3 Consider the following transition system [28] which uses a password system to launder confidential information:


⟨t, h, p, q, r⟩ '→ ⟨t, h, p, q, r⟩
⟨0, h, p, p, 0⟩ '→ ⟨1, h, p, p, 1⟩
⟨0, h, p, p, 1⟩ '→ ⟨1, h, p, p, 0⟩
⟨0, h, p, q, 0⟩ '→ ⟨1, h, p, q, 0⟩ p /= q
⟨0, h, p, q, 1⟩ '→ ⟨1, h, p, q, 1⟩ p /= q
where t ∈ {0, 1} is the time (1 indicates that the program has been executed), r ∈ {0, 1} denotes the result of the test (it is left un- changed if the test of equality between the password p and the query q fails).

The public variables are t, q, r, hence the partition induced by 7 is:
⟨t, h, p, q, r⟩ ≡ ⟨t', h', p', q', r'⟩ iff t = t' ∧ q = q' ∧ r = r'
The above says we are considering two states that are L indistinguishable (as in ordinary NI). By checking completeness we characterize the information that can be released. For example, consider the set of possible input states which are in the same equivalence class and for which the state ⟨0, h, p, q, 0⟩ is a representative. Applying
the transition rules, we see (below, left) that this state reveals a different public
output. Thus there is a leakage of confidential information. In order to characterize
what information is released we complete the domain 7 by p˜reP (below, right):

⟨0, h, p, q, 0⟩ '→ ⎧⎨ ⟨1, h, p, p, 1⟩
⎩ ⟨1, h, p, q, 0⟩
p˜reP
: ⎨⎧ ⟨1, h, p, p, 1⟩ '→ ⟨0, h, p, p, 0⟩
⎩ ⟨1, h, p, q, 0⟩ '→ ⟨0, h, p, q, 0⟩ p /= q

Hence we have to refine the original partition by adding the new blocks ⟨0, h, p, p, 0⟩ and ⟨0, h, p, q, 0⟩ where p /= q, i.e., we release the information whether p = q or p /= q.
AMC techniques are usually applied to Kripke structures. A Kripke structure consists of a set of states, a set of transitions between states, and a function that labels each state with a set of properties that are true in the state. The Kripke model for a program corresponds to the standard transition system associated with the program where states are labelled with the values of the variables. The connec- tion between declassification and AMC suggests the use of existing algorithms for

AMC in order to derive the information released by a system, whenever the con- fidential information is fixed. Indeed, the existence of a spurious counterexample in the AMC (abstraction corresponding to the declassification policy) corresponds to the existence of an insecure information flow in the concrete system. Suppose we interpret the initial abstract domain of a system as a declassification policy (the distinction between all the states mapped to different properties is declassified). Then whenever an AMC algorithm finds a spurious counterexample it means that there is a breach in the security, and hence some more secrets, i.e., some more distinctions among states, are released. For instance, in the example above, the given trace (from ⟨0, h, p, q, 0⟩) would be identified as a spurious counterexample,
and the refinement for erasing it is exactly the refinement we describe. When no
more spurious counterexamples exist, then we have characterized, in the resulting abstract domain, the secure declassification policy.

7	Discussion
In this paper we exploit completeness of abstract interpretation for modelling non- interference for confidentiality policies based on declassification. Starting with Joshi and Leino’s semantic formulation of NI [15], it is possible to characterize NI as a problem of B-completeness for denotational semantics [10]. This paper provides an
equivalent formulation of NI as J-completeness for the wlp semantics, and extends
the formulation to declassification. Semantically, we represent a declassification pol-
icy as an abstraction of the H inputs that induces a partition on them. A program that satisfies the policy is guaranteed not to expose distinctions within a partition block. J-completeness formalizes “not exposing distinctions”. The advantage of our formalization, compared to other approaches, is that we can associate with
each possible public observation the exact secret released. Moreover, the strong connection between completeness and declassification, together with the connection between completeness and abstract model checking, allows the use of standard tech- niques in abstract model checking for checking and refining declassification policies. In particular, model checking can be applied to generic finite state systems, and abstractions allow to consider even infinite state systems. As future work, we are studying the practical use of these techniques applied to more complex systems.
The relation between the abstract interpretation approach to NI [9,10] and many extant approaches for noninterference and declassification has been studied by means of examples [14,17]. Sabelfeld and Sands note that most extant proposals suffer from lack of a compelling semantics for declassification. In earlier work they use the PER model [24] for defining selective dependency [3] by means of equiva- lence relations instead of abstract domains. They also show, via an example, that the PER model can be used to show that nothing more is learnt by an attacker than what the policy itself releases [25]; in our model we derive this formally (Corollary 5.2) and also show how, in the case where a policy is not satisfied, coun- terexamples may be generated and the policy may be refined. Joshi and Leino [15] introduce abstract variables in order to obtain a more general notion of security.

In this case they substitute the private variables with functions, i.e., properties, of them. This corresponds to abstract noninterference where we fix what we want to protect instead of what we admit to flow [9,17], hence it is not helpful for comput- ing what information is released. Da´rvas et al. [7] use dynamic logic to dynamically analyze the declassification property. The information flow property is modelled as a dynamic logic formula. Next, they fix some declassifying preconditions and execute the analysis. If the analysis succeeds then there is an upper bound on the information disclosed; otherwise the precondition must be refined. Because of the connections of completeness to PT, our approach can provide a more systematic method for designing and refining these preconditions. Our approach differs from quantitative characterizations [2,8] of the information released since we provide a qualitative analysis of the leaked secrets.
In a recent paper, Unno et al. [27] have proposed a method for automatically finding counterexamples of secure information flow, which combines security type- based analysis for standard NI and model checking. Our context is more general, since standard NI is a particular case of DNI. Nevertheless, as future work, we plan to investigate whether their approach can be directly derived from ours.
Alur et al. [1] consider preservation of secrecy under refinement and present a simulation-based technique to show when one system is a refinement of another wrt. secrecy. They contend that their approach is flexible because it can express arbitrary secrecy requirements. In particular, if the specification does not maintain secrecy of a property then the implementation does not need to either. Our notion of refinement is slightly different: if a program leaks more information than the policy, we consider how the policy might have to be refined to admit the program. It is possible that there might be strong connections to their work and we plan to explore these connections.
In other future work, we plan to further exploit the strong relation of NI with AMC and stability. One direction is to implement algorithms for deriving the maximal amount of information disclosed and for refining declassification policies, by erasing counterexamples. Moreover, the example above shows that it is possible to combine both abstract noninterference and declassification. So existing abstract model checking techniques can be used not only to derive the amount of information disclosed, but also to characterize the strongest harmless attacker. Finally, we plan to extend the framework in this paper to handle heap-manipulating programs.


Acknowledgement
Thanks to the anonymous referees for their suggestions. Banerjee was supported in part by NSF grants CCR-0209205, CCR-0296182, ITR-0326577, CNS-0627748
and by the AIDA Project (MIUR-COFIN 2005-2007). Mastroeni was supported by NSF grant CCR-0209205 and Giacobazzi by the AIDA Project (MIUR-COFIN 2005-2007).

References
R. Alur, P. Cerny, and S. Zdancewic. Preserving secrecy under refinement. In M. Bugliesi, B. Preneel,
V. Sassone, and I. Wegener, editors, Proc. of the 33rd Internat. Colloq. on Automata, Languages and Programming (ICALP ’06 ), volume 4052 of Lecture Notes in Computer Science, pages 107–118, Berlin, 2006. Springer-Verlag.
D. Clark, S. Hunt, and P. Malacaria. Quantified interference: Information theory and information flow (extended abstract), 2004.
E. S. Cohen. Information transmission in computational systems. ACM SIGOPS Operating System Review, 11(5):133–139, 1977.
P. Cousot. Constructive design of a hierarchy of semantics of a transition system by abstract interpretation. Theor. Comput. Sci., 277(1-2):47–103, 2002.
P. Cousot and R. Cousot. Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. In Proc. of Conf. Record of the 4th ACM Symp. on Principles of Programming Languages (POPL ’77 ), pages 238–252, New York, 1977. ACM Press.
P. Cousot and R. Cousot. Systematic design of program analysis frameworks. In Proc. of Conf. Record of the 6th ACM Symp. on Principles of Programming Languages (POPL ’79 ), pages 269–282, New York, 1979. ACM Press.
A. Darvas, R. H¨ahnle, and D. Sands. A theorem proving approach to analysis of secure information flow. In D. Hutter and M. Ullmann, editors, Security in Pervasive Computing: Second International Conference (SPC 2005), volume 3450, pages 193–209, Berlin, 2005. Springer-Verlag.
A. Di Pierro, C. Hankin, and H. Wiklicky. Approximate non-interference. In Proc. of the IEEE Computer Security Foundations Workshop, pages 1–17, Los Alamitos, Calif., 2002. IEEE Comp. Soc. Press.
R. Giacobazzi and I. Mastroeni. Abstract non-interference: Parameterizing non-interference by abstract interpretation. In Proc. of the 31st Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages (POPL ’04), pages 186–197, New York, 2004. ACM-Press.
R. Giacobazzi and I. Mastroeni. Adjoining declassification and attack models by abstract interpretation. In S. Sagiv, editor, Proc. of the European Symp. on Programming (ESOP ’05), volume 3444 of Lecture Notes in Computer Science, pages 295–310, Berlin, 2005. Springer-Verlag.
R. Giacobazzi and E. Quintarelli. Incompleteness, counterexamples and refinements in abstract model- checking. In P. Cousot, editor, Proc. of The 8th Internat. Static Analysis Symp. (SAS’01), volume 2126 of Lecture Notes in Computer Science, pages 356–373, Berlin, 2001. Springer-Verlag.
R. Giacobazzi, F. Ranzato, and F. Scozzari. Making abstract interpretations complete. J. of the ACM., 47(2):361–416, 2000.
J. A. Goguen and J. Meseguer. Unwinding and inference control. In Proc. IEEE Symp. on Security and Privacy, pages 75–86, Los Alamitos, Calif., 1984. IEEE Comp. Soc. Press.
S. Hunt and I. Mastroeni. The PER model of abstract non-interference. In C. Hankin and I. Siveroni, editors, Proc. of The 12th Internat. Static Analysis Symp. (SAS ’05), volume 3672 of Lecture Notes in Computer Science, pages 171–185, Berlin, 2005. Springer-Verlag.
R. Joshi and K. R. M. Leino. A semantic approach to secure information flow. Science of Computer Programming, 37:113–138, 2000.
P. Li and S. Zdancewic. Downgrading policies and relaxed noninterference. In Proc. of the 32st Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages (POPL ’05), pages 158– 170, New York, 2005. ACM-Press.
I. Mastroeni. On the rle of abstract non-interference in language-based security. In K. Yi, editor, Third Asian Symp. on Programming Languages and Systems (APLAS ’05), volume 3780 of Lecture Notes in Computer Science, pages 418–433, Berlin, 2005. Springer-Verlag.
A. Min´e. The octagon abstract domain. Higher-Order and Symbolic Computation, 19:31–100, 2006.
R. Paige and R. E. Tarjan. Three partition refinement algorithms. SIAM Journal on Computing, 16(6):977–982, 1987.
F. Ranzato and F. Tapparo. An abstract interpretation-based refinement algorithm for strong preservation. In N. Halbwachs and L. Zuck, editors, Proc. of TACAS: Tools and Algorithms for the Construction and Analysis of Systems, volume 3440 of Lecture Notes in Computer Science, pages 140–156, Berlin, 2005. Springer-Verlag.


Ronald Rivest. Unconditionally secure commitment and oblivious transfer schemes using private channels and a trusted initializer. Unpublished note, 1999.
A. Sabelfeld and A. C. Myers. A model for delimited information release. In N. Yonezaki K. Futatsugi,
F. Mizoguchi, editor, Proc. of the International Symp. on Software Security (ISSS’03), volume 3233 of
Lecture Notes in Computer Science, pages 174–191, Berlin, 2004. Springer-Verlag.
A. Sabelfeld and A.C. Myers. Language-based information-flow security. IEEE J. on Selected Areas in Communications, 21(1):5–19, 2003.
A. Sabelfeld and D. Sands. A PER model of secure information flow in sequential programs. Higher- Order and Symbolic Computation, 14(1):59–91, 2001.
A. Sabelfeld and D. Sands. Dimensions and principles of declassification. In Proc. of the IEEE Computer Security Foundations Workshop (CSFW-18), pages 255–269, Los Alamitos, Calif., 2005. IEEE Comp. Soc. Press.
D. A. Schmidt. Comparing completeness properties of static analyses and their logics. In N. Kobayashi, editor, Proc. 2006 Asian Programming Languages and Systems Symposium (APLAS’06), volume 4279 of Lecture Notes in Computer Science, pages 183–199, Berlin, 2006. Springer-Verlag.
H. Unno, N. Kobayashi, and A. Yonezawa. Combining type-based analysis and model checking for finding counterexamples against non-interference. In Proc. of the 2006 Workshop on Programming Languages and Analyses for Security (PLAS’06), pages 17–26, New York, NY, USA, 2006. ACM Press.
S. Zdancewic and A. C. Myers. Robust declassification. In Proc. of the IEEE Computer Security Foundations Workshop, pages 15–23, Los Alamitos, Calif., 2001. IEEE Comp. Soc. Press.

A  Relevant background
Making abstract domain complete
The problem of making abstract domains B-complete and J-complete has been solved [12,11]. The key point in these constructions is that both J and B complete- ness are properties of the underlying abstract domain A relative to the concrete function f . To make a domain J-complete, one adds all the direct images of f to the output abstract domain; to make a domain B-complete, and one adds all the maximal of the inverse images of the function to the input domain. (see Fig. A.1). In a more general setting let f : C1 → C2 be a function on complete lattices C1
and C2, and ρ ∈ uco(C2) and η ∈ uco(C1) be abstract domains ⟨ρ, η⟩ is a pair of
B(J)-complete abstractions for f if ρ ◦ f = ρ ◦ f ◦ η (f ◦ η = ρ ◦ f ◦ η). In any case the idea of making a domain complete is to add all the direct images of the concrete function to the output abstract domain for J-completeness, and to add all the maximal of the inverse images of the function to the input domain for B- completeness (see Fig. A.1). Formally, we refine the corresponding domains wrt., a





ρ1	f	ρ1

ρ2	ρ2

f
x	x
C2	C2
C1	C1
Fig. A.1. Making F and B complete.
generic function f : C1 —→ C2 by using the following operations:



Let l ∈ {J, У}. In [12] the authors proved that the most abstract β ± η such that
⟨ρ, β⟩ is l-complete, i.e., given ρ ∈ uco(C2) the l-complete shell of η ∈ uco(C1), is
l,ρ	def	l
Yƒ  (η) = η H Rƒ (ρ).
The Paige-Tarjan algorithm
The Paige Tarjan algorithm is a well known algorithm for computing the coarsest bisimulation of a given partition. Consider a relation R such that f = pre(R). The algorithm is provided below, where P is a partition, PTSplitR(S, P ) partitions each unstable block in P wrt. R with B ∩ f (S) and B z f (S), while PTRefinersR(P ) is the set of all the blocks in P , or obtained as union of blocks in P , which make other blocks unstable.
P : Partition
Partition obtained from P by replacing
R
⎩ each block B ∈ P with B ∩ f (S) and B z f (S)

def
PTRefinersR(P ) =
B P /= PTSplitR(S, P ) Λ E{Bi}i ⊆ P. S =
⎧⎪ while (P is not R-stable) do
i B)i ,


PT-AlgorithmR
: ⎪⎨
⎪
choose S ∈ PTRefinersR(P );
P := PTSplitR(S, P );

⎪⎩ endwhile

Fig. A.2. A generalized version of the PT algorithm.

This algorithm has been shown to be a forward completeness problem for the function f [20].
