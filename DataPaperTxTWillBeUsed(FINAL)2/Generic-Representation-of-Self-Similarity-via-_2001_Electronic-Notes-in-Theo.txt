Electronic Notes in Theoretical Computer Science 46 (2001)
URL: http://www.elsevier.nl/locate/entcs/volume46.html 20 pages


Generic Representation of Self-Similarity via Structure Sensitive Sampling of Noisy Imagery

Kohji Kamejima 1
Faculty of Engineering Osaka Institute of Technology
Osaka, Japan


Abstract
An adaptive sampling scheme is presented for discrete representation of complex patterns in noisy imagery. In this paper, patterns to be observed are assumed to be generated as fractal attractors associated with a fixed set of unknown contraction mappings. To maintain geometric complexity, the brightness distribution of self- similar patterns are counted on 2D array of Gaussian probability density functions. By solving a diffusion equation on the Gaussian array, capturing probability of unknown fractal attractor is generated as a multi-scale image. The totality of local maxima of the capturing probability, then, yields a pattern sensitive sampling of fractal attractors. For eliminating background noise in this sampling process, two filters are introduced: input filter based on local structure analysis on the Gaussian array, and, output filter based on probabilistic complexity analysis at feature points. The sampled image through these filters are structure sensitive so that extracted feature pattrers support invariant subset with respect to mapping sets associated with observed patterns. As the main result, a generic model is established for unknown self-similar patterns in background noise. The detectability of the generic model has been verified through simulation studies.


Introductory Remarks
Complex patterns are captured as computable entities through coding on dis- crete image plane. In many practical applications, discrete representation should maintain complete information for exact restoration of complex im- agery. However, it is not easy to generate such “visible” code of random imagery within conventional statistical – computational frameworks. For in- stance, sampling on “very fine” lattice often yields “fragile” discrete represen- tation that is susceptive to non-essential pattern deformation. To recognize

1 Email: kohji.kamejima@nifty.ne.jp
◯c 2001 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.





Fig. 1. Collage
For any pattern Λ ⊂ Ω, there exists a set of contraction mapping ν = µi , µi : Ω Ω that yields an invariant subset Ξ Ω approximating the pattern Λ within arbitrary small imaging error.


intrinsic features out of representation noise, thus, such susceptive represen- tation must be handled through sophisticated matching processes.
The representation difficulty arises from discrete image modeling. Logi- cally, image model must be independent on pattern structures to be detected because pattern grammar should be applied to a priori fixed lattice. Geo- metrically, however, sampling process of discrete image should be adapted to specific feature distribution. Thus, without “generic” representation, pat- tern sampling easily falls into a serious self-contradiction: to adjust lattice to not-yet-identified patterns to be represented.
A potential way to bypass the self-contradiction is to introduce the self- similarity as a priori pattern structure. Noticing logical – geometric coordina- tion in self-similarity imaging processes, in this paper, we assume that patterns to be observed are generated as fractal attractors associated with unknown set of contraction mappings. Without serious loss of generality, in the follow- ing discussions, we suppose that the number of contraction mappings can be guessed. The assumption of self-similarity is not so restrictive because we can
approximate any patterns in terms of the following “Fractal Collage[1]”: For arbitrary pattern Λ in a fixed image plane Ω, there exists a set of contraction mappings ν = {µi , that yields an invariant subset Ξ  Ω for approximating the pattern within arbitrary small imaging error (Fig. 1). This implies that any observed patterns can be coded in terms of finite symbols. The finite code
completely specifies imaging process for generating fractal attractor of infinite geometric complexity.
In contrast with conventional statistical – computational representation, fractal model conveys complete information to specifies invoked contraction mappings. In fact, we have enough data for determining mapping parame- ter as the distribution of attractor points. Hence, we have logical bases for pattern coding as the following “Structural Observability[2]”:  The attractor




Fig. 2. Finite Composite
The attractor Ξ is covered by the totality of fixed points θi, θj,. . . , asso- ciated with all finite composite of the mappings ··· µjµi.




Fig. 3. Invariant Measure
For arbitrary self-similar pattern Ξ generated by random application of

fixed contraction mappings µi, there exists a measure χp
that is invariant

with respect to the transform by the mappings.


Ξ is covered by the totality of fixed points θi, θj,. . . , associated with all finite composite of the mappings (Fig. 2). Thus, pattern coding results in identi- fying origin – destination pairs in complex attractors. Since each attractor point deterministically “jumps” into the attractor by a contraction mapping, we have exact origin – destination associations in observed imagery.
In addition, the self-similarity induces definite association between geomet- ric order, i.e., spatial distribution of attractor points, and probability for pat- tern capturing, i.e., gray level distribution. This implies that we can analyze pattern structure via the estimation of the “Invariant Measure[1]”: For arbi- trary attractor generated by random application of fixed contraction mappings,
there exists a measure χp that is invariant with respect to transform by the
mappings (Fig. 3). The existence of invariant measure implies the association


between the distribution pattern and the density function of fractal attractors. The self-similarity of the density function introduces the self-similarity in the distribution of statistical parameter.
We can exploit the collage theorem as a general framework for fractal pat- tern coding. Fractal code is described in terms of finite contraction mappings that restore observed patterns of infinite complexity. By invoking the struc- tural observability, we can design the mappings through origin – destination association on discrete points. To discriminate the discrete image from back- ground noise, the invariance of observed “brightness distribution”with respect to the mappings to be designed should be analyzed. In this paper, hence, we consider the integration of these three aspects of the self-similarity to develop a unified sampling scheme for unknown complex patterns.

Self-Similarity on Continuous Image
Let Ω  R2 be a continuous image plane and suppose that patterns are generated within the Borel field [Ω] of the totality of subsets of Ω. The disparity between patterns A, B  [Ω] is indexed in terms of the Hausdorff distance η[A, B] defined by
η[A, B] = max {←−η [A, B], ←−η [B, A]},	(1a)
←−η [A, B] = max  min |ω − λ| .	(1b)

Consider a fixed set of unknown contraction mappings ν = {µi,i = 1, 2,..., m}
sµi
with length  ν	= m where µi : Ω	Ω is a mapping from Ω into itself with
contractivity factor sµi , 0 < sµi < 1, i.e.,
|µi(ω1) − µi(ω2)| ≤ sµi|ω1 − ω2|,
for any ω1, ω2 ∈ Ω. By the contractivity, we can program pattern generation processes as the collage of mapping images µi(Ω). For self-similar patterns, particularly, we have the following exact pattern generation scheme on pro- gram set ν:
Proposition 2.1 (Fractal Attractor) Let Ξ be the attractor generated by the “program” ν to satisfy
Ξ =	µi(Ξ).
µi∈ν
The attractor Ξ can be successively approximated by the dynamical system on
F[Ω]:
ξt+1 ∈	µi(Ξt),	(4a)
µi∈ν


Ξt = {ξτ ∈ Ω,τ ≤ t}.	(4b)

The sequence Ξt converges to the attractor Ξ in the following sense


(5)
lim η[Ξt, Ξ] = 0.
t→∞

If the initial value of the process (4) is confined within target attractor
Ξ0 ⊂ Ξ,
the imaging process is monotone, i.e.,
(6)	Ξ0 ⊂ Ξ1 ⊂ Ξ2 ⊂ · · · ⊂ Ξt ⊂ · · · ⊂ Ξ.
2D Gaussian Sampling
By assigning the following basic measure to the image plane
dω
dP (ω) =	,
dω
Ω
we can introduce a probability space (Ω, F[Ω],P ) as the basis of image anal- ysis. For instance, the “brightness” of the patterns Ξt and Ξ are represented by distributions on the probability space as follows:
Proposition 3.1 (Convergent Distribution) [3] Let Ξt, t = 1, 2,.. ., be a sequence of point sets with initial value Ξ0 ⊂ Ξ. Assume that Ξt ⊂ Ω for any t ≥ 1. Define

(8)

χΞt
  1 
=	δξt.
Ξt
ξt∈Ξt

Then Ξt ⊂ Ξ and there exists a distribution χΞ satisfying
χΞt → χΞ as t → ∞,	(9a)
in the following sense:
lim χΞt (f ) = χΞ(f ),	(9b)
t→∞
for arbitrary locally summable “test function” f.
By using a system of test functions, we can extend sampling mechanism to distributions. For this purpose, consider the following one parameter family of test functions:
(10)	T = {fξ(ω),ξ ∈ Ω}.


Generally, sample values of the distribution Ξ on T is represented as the following linear functional:


(11)
χΞ(fξ) = fξ ∗ χΞ(ω) = 
Ω
f (ξ − ω)χΞdP (ω).

The representation error can be reduced arbitrary by introducing the following
δ-convergent sequence:
(12)	fє → δξ as ϵ → 0,
in the space of test functions. For instance, let the following simple averaging functions be introduced as test functions:


(13)

fє =
 1  ;	for ω	ξ	ϵ, 2πϵ2
0;	otherwise.

By “counting” the distribution on fє, we have “average value” of distribution
χΞ in small region around ξ as the the intensity of image Ξ such that
fє → δξ then χΞ(fє) → χΞ.
By this convergence, we have discrete representation for the image Ξ with brightness distribution χΞ as follows:
Definition 3.2 (Generalized Sampling) Let D be discrete subset of Ω. The set of the values of measure χΞ on one parameter family of test functions FD = {fd,d ∈ D}
χD = χΞ(fd) | d ∈ D,
is called generalized sampling of imagery χΞ.
Consider the representation of the self-similarity on the generalized sam- pling χD. Let the discrete subset D be a priori given as a “uniform” lattice
Ξ	√

with resolution ϵ/
 ··· 
2
є ···	є
√	√

··· є

є ···	є
√	√
· ·· 

···	(i −

,j +
2



2 )	(i, j + √2 )	(i +

,j +
2


2 )	··· 


			
···	···	···	···	· ·· 
and consider partitioning of the image plane Ω by unit disks {dl,l ∈ L}:
dl = ω ∈ Ω | |ω − l|≤ ϵ,.


By evaluating the ”brightness” of the image χΞ at the “point” l	L in terms of the mean value

є
χΞ ∗ δl ∼ χΞ(f ),
we have local representation of the self-similarity in dl as follows:
χΞ(fє)P (D+) ≥ ν  χΞ(fє)P (dl),	(19a)

where D+ denotes discrete support of the brightness distribution given by
D+ = dk,k ∈ L | η[k, l] ≤ ϵ and χΞ(fє) > 0,.	(19b)

Hence, we have the following “input filter” to discrete image plane L for observing unknown self-similar patterns with complexity factor  ν  :
(20)	P (D+) ≥ ν  P (dl).
Due to the loss of information by simple averaging, however, it is not easy to evaluate the expansion P (D+) relative to the measuring unit P (dl). As another version of sampling mechanism, consider the family of Gaussian probability density functions gl ,σ > 0 , where


(21)

gl (ω) = 

 1  2πσ

exp
  |ω − l|2



Noticing gl , σ > 0 yields a δ-convergent sequence:
σ → δl as σ → 0,
we have the following stochastic sampling scheme on deterministic image plane
L:
G =  gl ,l ∈ L}.
By testing the value of distributions on G, we have the following stochastically sampled image:


where χΞ(gl )ω = gl
χG =  χ (gl ) ,l ∈ L},
∗ χΞ(ω).  In the sampling scheme (G, L), complete in-

formation χΞ of infinite resolution is associated with discrete image plane L.
Following the zero-cross method[5], for instance, the boundary point ω' asso- ciated with a point image δξ should be detected by


1	 1 
|ω' − ξ|2  |ω' − ξ|2
	



(25)	= 0.

The combination of the local complexity (20) with pixel boundary evaluation
yields the following “counting rule” for input filtering on digital image plane L:
P (N +) ≥ ν  ,
where N + means the number of pixels located in D+ (Fig. ??).
l	l



Fig. 4. 2D Gaussian Array for Visualizing Point Images
The pixel of interest ( ) is connected with eight neighborhood pixels ( ) in the lattice L. To each pixel, a Gaussian distribution is assigned to clarify local area in which connectedness of pixels is tested. The variance parameter of the Gaussian distribution specifies the zero-cross boundary of the pixel (•).



Self-Similarity on Measures
The “painting”process (4) combined with the generalized “brightness”control
induce the self-similarity on the distribution χΞ. Noticing static constraint (3), we have
Proposition 4.1 (Invariant Measure)	For the programs µi to be selected with probability pµi, the measure χΞ on F[Ω] is invariant with respect to the

following Markov operation:
χp (·) = Σ pµ χp [µ−1(·)],	(·) ∈ F[Ω],

where pµi and µi ∈ ν are nonnegative constants such that	pµi  = 1.
µi∈ν
Let the measure χΞ be regularized by the following one-parameter family of test functions

|ω|2
e− 2τ


g = {gτ ,τ ≥ 0} =
2πτ ,τ ≥ 0  ,


and consider the adaptation of “scale parameter” τ to the self-similar pattern Ξ to be detected.
The imaging process (4) expands initial points Ξ0 through nondeterministic scattering within a fixed domain Ξ. This implies that the process should be modeled by 2D dynamical system with the following antagonistic imaging mechanisms
diffusion of point image δξ within image plane Ω, and,
successive reduction of imaging domain via not-yet-identified contraction mappings µi ∈ ν.
Let the model be described in terms of the following system
∃µi ∈ ν : ωt+1 = µi(ωt),
where random shift of a point image is considered to be observed as a sample path on a “tectonic plate” successively reduced by randomly selected mapping µi ν. By identifying “observation error” with 2D Brownian motion, we have the following stochastic evaluation for capturing the point image within an ordinary domain Γ ∈ F[Ω]:


Pw(t, ω, Γ) =
Γ
∫
gt(γ − ω)dγ

|ω−γ|2
—

e
=	cΓ(γ)
Ω

2t
2πt	dγ = gt ∗ cΓ(ω),	(30a)

where t is the time elapse for capturing the point ξ and cγ denotes the char- acteristic function of “regular” set Γ:


c (ω) =	1;	for ω	Γ,
Γ	0;	otherwise.
(30b)




In (30), the point image is assumed to be emitted from ω	Ω. The evaluation
can be extended to self-similar patterns of geometric singularity as follows:

Pw(t, ω, Ξ) = gt ∗ χΞ(ω),	(31a) and to its mapping image, as well:
Pw(t, ω, µi(Ξ)) = gt ∗ χµ (Ξ).	(31b)

Suppose that the time elapse is counted in terms of the activation of the painting process (4). For such situation, we have
P [1](t, ω, Ξ) = Σ pµ Pw(t, ω, µi(Ξ)),
µi∈ν

where pµi denotes the probability for selecting a mapping µi	ν. Noticing the Chapman’s Equation
gt(γ − ω) = gt((·) − ω) ∗ g0(γ − (·)),


∀ξ, γ ∈ Ξ : g0(γ − ξ) = δ(γ − ξ),
we have the following stochastic evaluation for capturing the point image at
γ ∈ Ω under the selection of µi:
pµi ∼ pξ(γ|µi) = χΞ(ξ) · δ(γ − µi(ξ)),
where pξ(γ|µi) denotes the transition density function from ξ to γ conditioned by the selection µi ν with a priori probability pµi . Hence, the transition function associated with the imaging process (4) is computed by

t	χΞ(µ−1(γ)) · gt(γ − ω)dγ
Ω
= χΞ(µ−1(·)) ∗ gt(ω) = χµ (Ξ) ∗ gt(ω)
i	i
(36)	= Pw(t, ω, µi(Ξ)),
for one step reduction by fixed µi	ν. Assume that the mapping µi is selected
1
uniformly in a fixed set ν with size  ν . By setting pµi =	, it follows that

P [1](Ξ|µi) =  ν  −1Pw(t, ω, µi(Ξ)).


This implies that, the transition function conditioned by a fixed “programming framework” ν is obtained as follows:
 1  Σ	 1 
P [1](Ξ|ν) =	Pw(t, ω, µ (Ξ)) =	Pw(t, ω, Ξ).

Iterating such capturing process, we have
P [n](Ξ|ν) =  ν  −1P [n−1](n, ω, Ξ) = ··· =  ν  −nPw(n, ω, Ξ),


or, equivalently,


Pt (Ξ|ν) = exp[−ρt]gt ∗ χΞ(ω),	(40a)


where ρ is the complexity parameter defined by
ρ = log ν  .	(40b)

Noticing that the transition function Pt (Ξ|ν) is generated by the following

evolution equation

∂Pt (Ξ|ν)





1
=	∆Pt (Ξ|ν) − ρPt (Ξ|ν),	(41a)



with the initial distribution


P 0(Ξ|ν) = χΞ,	(41b)


we have the following
Proposition 4.2 (Multi-Scale Image, Fig. 5) Let ϵ be a small positive con- stant and consider the weighted average of Gaussian probability density func- tion



(42)

Gρ = ρ
t−є
e−ρ(t−є−τ )gτ dτ.
0

Then the multi-scale image of Gaussian distribution Gρ satisfies the following

equation:

(43)


Mє :


 ∂Jρ



=  ∆Gρ + ρ[gє − Gρ].

The dynamical system Mє generates stochastic evaluation for a point image observed through Gaussian array. The system Mє converges to the following

system	to generate the multi-scale image associated with “exact” point image δ:


(44)
 ∂Jρ
M :
=  ∆Jρ + ρ[δ − Jρ],


as ϵ → 0.
∂t	2	t	t





Fig. 5. Multi-Scale Image
A version of the weighted average (42) with ρ = 4 and ϵ = 0.2 is indicated.
ρ visualizes the stochastic evaluation of a point image emitted from the
origin as a smooth field.


The steady state of the dynamical system (43) yield the final estimate for capturing the point images Ξ as a computable entity structured by ν. By superimposing the evaluation on the initial distribution, thus, we have the following
Proposition 4.3 (Capturing Probability) Let χΞ be a given brightness dis- tribution to be collaged by ν = µi . Assume that the distribution is observed through the Gaussian array G. Then the probability for regenerating Ξ within the framework of maximum entropy capturing is visualized as a smooth field ϕ(ω|ν) satisfying


(45)
1
∆ϕ(ω|ν)+ ρ[χ
(gl)
— ϕ(ω|ν)] = 0,	l ∈ L.


Generally, the probability distribution can be generated via the following equa- tion:

(46)
1
2 ∆ϕ(f|ν)+ ρ[χΞ(f ) − ϕ(f|ν)] = 0,


where f denotes a test function.
Pattern Boundary on Invariant Measures
By invoking zero-cross criterion (25), we have the estimate of the capturing probability at pattern boundary as follows:


ϕ(ω'|ν) = γr ,	√ 
(47a)

γr ∼ gr (ω'),	|ω'| =
2τ.	(47b)


It should be noted that the level γr can be specified without specification of mappings. For instance, the level γr of imaging process with  ν  = 3 can be computed by


γr ∼

e−1
2πτ  0.04829.

This implies that the expansion Ξˆν and the boundary ∂Ξˆν of the distribution χΞ can be specified by estimating the scale parameter τ . By paraphrasing the generator (46) as


ϕ(gl|ν) = χ
(gl
1
ϕ(gl|ν) · τ (ρ),	l ∈ L,	(48a)

є	Ξ	є)+ 2 ∆	є
1	1

τ (ρ) = 
ρ
=
,	(48b)

we have the following association:



(49)

ϕ((·)|ν) ∼ g
r (ρ)
⇐⇒ gl +
1
∆g dt,

r (ρ)
є	2	t
0

on Gaussian array (G, L). Hence, we have the following estimates,
∂Ξˆν on continuous image plane, respectively:
Ξˆν and

Ξˆν = ω ∈ Ω | ϕ(ω|ν) ≥ γ(ρ),,	(50a)
∂Ξˆν = ω ∈ Ω | ϕ(ω|ν) = γ(ρ),.	(50b)

Consider self-similar patterns generated in noisy background. For such patterns, we can generate the capturing probability and a version of condi- tional probability for evaluating possible variation of brightness, as follows:

p(ω|ν) = ϕ(ω|ν) ,	(51a)

Cq = ∫ ϕ(ω|ν)dω.	(51b)
Ω
By using the conditional probability, we can index the complexity of brightness variation in terms of the following Shannon’s entropy
−	p(ω|ν) log p(ω|ν)dω = −E log p(ω|ν) | ν, = Hˆν .

The existence of self-similarity structure should be verified through the com- parison with the entropy evaluation under “null condition”:
E log p(ω|∅) | ∅, = −Hˆ∅,

where p(ω|∅) = const. on Ω. Hence, we have
Proposition 5.1 Assume the background noise χΩ is uniformly distributed in the image plane Ω and suppose that observed measure χΛ is represented by
χΛ = χΞ + χΩ.
Then the boundary level is given by
γ = Cqp¯ν,	(55a)


where

log p¯ν

= 1 − 1 (1 − eHˆν−Hˆ∅ ) − Hˆ


∅.	(55b)


Proof. Noticing the maximum entropy estimate of variance is given by


(56)

we have

σ(·)
1
=
2πe
exp[Hˆ

(·)],


(57)

p¯ν =	sup
  1 
exp
  |ξΩ|2 




= exp

 1 −
 σΩ 

— Hˆ∅ .

|ξΩ|2>σΩ 2πσ∅
Since σ∅ = σν + σΩ, and

ˆ
2σ∅


  1 

2σ∅


|ω|2 



= log 2πeσ∅,	(58a)


in the exterior of the support of χΞ for mutually independent generalized random fields χΞ and χΩ, it follows that


(59)
σΩ = 1	σν
σ∅	σ∅
= 1 − exp[Hˆν
— Hˆ

∅],

as was to be proved.	✷

Self-Similarity on Stochastic Features
The capturing probability ϕ(ω ν) is the smoothing of gray level distribution of self-similar patterns. By modeling the imaging via unknown contraction mappings in terms of 2D Brownian motion on dynamically regenerated do- main, a unified framework is introduced for information compression: the maximum entropy. Due to the infinite differentiability of generated field, on the other hand, the capturing probability maintains complete information of self-similarity processes. The association (49), particularly, implies that the generator of the capturing probability is adapted to the complexity of the patterns to be observed.
Consider a discrete image defined by
(60) Θ˜ = θ˜ ∈ Ω | ∇ϕ(θ˜|ν) = 0, det ∇∇T ϕ (θ˜|ν) > 0, ∆ϕ(θ˜|ν) < 0,.

Through the adaptation of the field ϕ(ω|ν) to unknown generator, the image
Θ˜ yields a version of structurally sensitive sampling. The mapping structure
is said to be uniformly observable if, for arbitrary ξ ∈ Ξ, there exists a finite composite ⟨µi⟩t generating the fixed point ξt
ξt = ⟨µi⟩t (ξt),	µi ∈ ν,	(61a)

satisfying the following condition[4]:
|ξ − ξt| < ϵ.	(61b)


The observability condition can be tested on discrete image Θ˜
as follows:

Proposition 6.1 (Invariant Features)	Assume that there exists an subset

Θ ⊂ Θ˜

(62)
invariant with respect to ν, i.e.,
Θ = θ ∈ Θ˜ | ∃µi ∈ ν : µ−1(θ) ∈ Θ,.

Suppose that for arbitrary µi ∈ ν there exist θo, θd ∈ Θ such that
(63)	θd = µi(θo).
Then the imaging process (4) is uniformly observable.

Proof. By definition, invariant features Θ is made up of two types of feature points: periodic points Θp and destination points Θd. Periodic points are fixed points of some finite composite on ν, i.e.,

Θp =  θp ∈ Θ˜ | ∃ ⟨µi⟩

: θp = ⟨µi⟩
(θp),,	(64a)



where


⟨µi⟩t = µitµit−1 µit−2 ··· µi2 µi1 ,	µiτ ∈ ν.	(64b)


On the other hand, each destination point is a mapping image of some finite composite, i.e.,


(65)
Θd =  θd ∈ Θ˜ | ∃ ⟨µi⟩

: θd = ⟨µi⟩
(θp),,


where θp ∈ Θp. Since Θp ⊂ Ξ, and Θd ⊂ Ξ, as well, it follows that

Θ ⊂ Ξ.
By applying imaging process (4) to initial set Θ, we have a point ξt ∈ Ξ, for any ξ ∈ Ξ, such that,
	ξt = ⟨µi⟩t (θ),	θ ∈ Θ, ϵ
with |ξ − ξt| < 2 , and
 t	ϵ



Since ξf ∈ ⟨µi⟩
(Ξ), where ξf = ⟨µi⟩
(ξf ), it follows that





Hence
f	ϵ
|ξt − ξt | < 2 .


|ξ − ξ | ≤ |ξ − ξt| + |ξt − ξ | < ϵ,
as was to be proved.	✷
Obviously, Θ ⊂ Ξ if Θ˜ ⊂ Ξ. This implies that we can restrict the domain
for extracting invariant features by the following pointwise “output filter”:
(70)	Θˆ = θˆ ∈ Θ˜ | p(θˆ|ν) ≥ p¯ν,.


Thus, we have generic representation of the self-similarity on noisy discrete imagery as follows:
(71)	Θ = θ ∈ Θˆ | ∃µi ∈ ν : µ−1(θ) ∈ Θ,.

In this representation, the constraint for imaging process is grammatically

specified on discrete pattern Θˆ. The discrete pattern Θˆ
is extracted within

sampled image Θ˜
through pointwise filtering.  The discrete information
Θ˜,

conversely, is generated through adaptive sampling based on stochastic eval- uation ϕ(ω|ν) for unknown mappings ν.


Experiments
Pattern detection on proposed sampling scheme was verified via simulation studies. In these simulations, fractal attractors were generated by Monte- Carlo simulation on continuous image model. To each attractors, uniformly distributed random dots were added as background noise. Result of simulation studies are illustrated in Figs. 6 – 9.
Figure 6 illustrate an observation of a fractal pattern χΞ in background noise χΩ satisfying  χΩ  = 2 χΞ  . Pattern detection results in this situation are shown in Fig. 7. In these figures, the distribution of attractor points are “counted” on test functions fє on 2D lattice L with ϵ = 1 (Observables view). By selecting locally connected lattice point satisfying (26) with  ν  = 3, we have the initial value for generating capturing probability χΞ(gl). Extracted
stochastic features are illustrated in “Features View” where Θˆ is estimated
via in-out discriminator (70) and indicated by () in background noise (). As shown in Fig. 7, the generator of observed self-similar pattern is observ-
able so that the generator yields invariant subset Θ ⊂ Θˆ (Coding View) and
regenerates fractal attractor (Restoration View).  Thus, we can detect the

generator of observed pattern via structure sensitive sampling Θˆ image (G, L).
on discrete

Figures 8 and 9 illustrate another results where background noise satisfies χΩ  = 4  χΞ  . As shown in this figure, the generator of observed self-similar pattern was detected successfully on sampled distribution (G, L).
The results of simulation studies are summarized as follows:
Proposed input- and the output-filters jointly generate discrete subset of unknown fractal attractors.
Sampled patterns is well structured to support origin – destination associ- ations with respect to not-yet-identified mapping set.
Structural consistency of sampled pattern with mapping descriptions can be evaluated by invariance test.




Fig. 6. Noisy Observation of Leaves





Fig. 7. Uniformly Observable Fractal Model



Concluding Remarks
A method was presented for structure sensitive sampling of unknown self- similarity in noisy imagery. By counting locally connected distribution on 2D




Fig. 8. Noisy Observation of Leaves





Fig. 9. Uniformly Observable Fractal Model



Gaussian array, the capturing probability for self-similar region is evaluated to generate discrete feature patterns. The capturing probability is sensitive to self-similar structure so that generated discrete pattern specifies the totality


of most probable attractor points. As the main result, a generic model is established for unknown self-similar patterns in background noise. Through simulation studies, extracted discrete patterns have been verified to maintain sufficient information to regenerate observed attractors.

References
M. F. Barnsley. Fractals Everywhere. Academic Press, Cambridge, Massachusetts, second edition, 1993.
J. E. Hutchinson. Fractals and self similarity. Indiana University Mathematical Journal, 30:713–747, 1981.
K. Kamejima. Multi-scale image analysis for stochastic detection of self- similarity in complex texture. In Proceedings of 1997 IEEE International Conference on Systems, Man and Cybernetics (SMC’97), pages 4192–4197, Orlando, Florida, 1997. IEEE.
K. Kamejima. Propositional-grammatical observability of self-similar imaging processes for feature based pattern verification. In Proceedings of the 5th International Workshop on Parallel Image Analysis (IWPIA’97), pages 270–289, Hiroshima, Japan, 1997.
D. Marr and E. Hildreth. Theory of edge detection. Proc. R. Soc. London, B-207:187–217, 1980.
