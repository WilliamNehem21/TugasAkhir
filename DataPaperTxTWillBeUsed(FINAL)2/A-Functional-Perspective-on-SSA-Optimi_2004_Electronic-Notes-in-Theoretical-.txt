Electronic Notes in Theoretical Computer Science 82 No. 2 (2003)
URL: http://www.elsevier.nl/locate/entcs/volume82.html 15 pages


A Functional Perspective on SSA Optimisation Algorithms

Manuel M. T. Chakravarty 1, Gabriele Keller 1 and Patryk Zadarnowski 1
School of Computer Science and Engineering University of New South Wales
Sydney, Australia


Abstract
The static single assignment (SSA) form is central to a range of optimisation algo- rithms relying on data flow information, and hence, to the correctness of compilers employing those algorithms. It is well known that the SSA form is closely related to lambda terms (i.e., functional programs), and, considering the large amount of energy expended on theories and frameworks for formal reasoning in the lambda calculus, it seems only natural to leverage this connection to improve our capa- bilities to reason about compiler optimisations. In this paper, we discuss a new formalisation of the mapping from SSA programs to a restricted form of lambda terms, called administrative normal form (ANF). We conjecture that this connec- tion improves our ability to reason about SSA-based optimisation algorithms and provide a first data point by presenting an ANF variant of a well known SSA-based conditional constant propagation algorithm.


Introduction
The static single assignment (SSA) form is a popular intermediate represen- tation for compiler optimisations [7], and hence, of considerable significance when it comes to reasoning about the correctness of those optimisations. Un- fortunately, a formal treatment of the semantics of SSA programs and SSA- based optimisation algorithms is complicated due to, the so-called φ-functions, which control the merging of data flow edges entering code blocks [4].
Kelsey [12] and Appel [3,2] pointed to a correspondence between programs in SSA form and lambda terms (i.e., functional programs). We believe that this correspondence can be leveraged to simplify reasoning about compiler optimisations that hitherto were based on the SSA form. In particular, we

1 Email: {chak,keller,patrykz}@cse.unsw.edu.au
◯c 2003 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.


suggest that intermediate forms based on the lambda calculus lead to clearer algorithms, which we expect to positively impact the correctness of concrete implementations, even if they are not formally verified. In this paper, we concentrate on a restricted form of lambda terms, called administrative normal form (ANF) [8], as they have a more clearly defined operational interpretation than general lambda terms.
Kelsey [12] related the SSA form to a different form of restricted lambda terms, called continuation passing style (CPS). However, Flanagan et al. [8,17] showed that, for data flow analysis, there is no real advantage to using CPS over direct-style representations, such as ANF. In fact, CPS requires additional transformations and, without special measures, non-distributive flow analysis in CPS is more costly than necessary. Hence, instead of using Kelsey’s CPS- based approach, we prefer to formalise the mapping of programs from SSA to ANF (in Section 2); we do so more formally than Appel [2].
Section 3 exploits the correspondence of SSA and ANF by rephrasing Weg- man and Zadeck’s [22] sparse conditional constants algorithm, which performs constant propagation and unreachable code elimination. In the following, we call Wegman and Zadeck’s original SSA-based algorithm Sccssa and call our new ANF-based algorithm Sccanf. We present Sccanf in a notation that has a well-defined semantics, as opposed to the informal notation used by Wegman and Zadeck. We believe that the semantic rigour of our notation in combi- nation with the well-defined semantics of our intermediate language (namely ANF) implies that Sccanf is significantly better suited to formal analysis.
In summary, this paper makes the following technical contributions:
We formalise the mapping of programs in SSA form to programs in ANF (Section 2.3).
We introduce the algorithm Sccanf, of which we claim that it implements the same analysis on ANF programs as Sccssa does on SSA programs (Section 3). However, Sccanf is more rigorously defined.
We establish that Sccanf is conservative; i.e., that the variables marked as constant are indeed constant (Section 4).
However, we do not actually prove that Sccanf and Sccssa implement the same analysis. In fact, this would be hard to achieve due to Wegman and Zadeck’s [22] rather informal presentation. We do, however, present formal statements about the soundness of our mapping of SSA programs to ANF programs and about the soundness of Sccanf.
We like to regard the results in this paper as a step towards reaping the well established benefits of typed, functional intermediate languages in com- pilers for conventional languages. These benefits include simplified reason- ing about the correctness of compiler optimisations, type-based validation of optimised code at compile time, and support for the generation of certified binaries [9,20,16,14,18]. Moreover, ANF naturally integrates intra-procedural with inter-procedural analysis.

Making Dataflow Explicit
In this section, we define a concrete notation for both SSA and ANF. In ad- dition, we characterise the relationship between these two intermediate forms by a translation procedure that takes programs in SSA form into equivalent programs in ANF.
Static Single Assignment Form
The static single assignment (SSA) form [1] is an imperative representation of programs which encodes data flow information explicitly by requiring that there be exactly one assignment for every variable. Figure 2 presents the factorial function in SSA form. Except for the two φ-functions, the program resembles standard three-address code: “fac” consists of two basic blocks con- nected by jumps, with the second (labelled L1) constituting the main loop of the program. For brevity, we allow inlining of blocks in the branches of a con- ditional statement; in reality, the two assignments in the if statement should be placed in a separate block, so that if indeed represents a conditional jump. The values of the variables x and r are updated in three places in “fac”.
To achieve the single assignment property, we create a new variable for each update, splitting x into x, x0 and x1, and similarly for r. Since, the block L1 can be reached either from the start block or from L1 itself, at the beginning of L1 we must merge the two sources of values for x and r using the so-called φ-function, which selects a value based on the source block of the jump. Note that, in SSA, the single-assignment property is purely syntactic, since, in the loop L1, variables are still updated at runtime on every iteration.
A procedure can be put in the SSA form by arranging the basic blocks into a dominator tree, where the parent of each node dominates its children. An assignment dominates an expression if every path to the expression from the start of the procedure includes that assignment. The SSA form has been popularised by Cytron et al. [7], who describe an algorithm for computing the dominance information in linear time and demonstrate how SSA increases the power of many optimisations which require data-flow analysis.
Figure 1 presents an abstract syntax of the structured SSA form; i.e., to our variant of SSA in which the dominator tree is explicitely encoded in the block structure. The body of each procedure consists of a sequence of labelled expressions structured into the dominator tree (the first block in each braced group dominates the remaining blocks in that group). Intuitively, the braces provide a traditional scoping hierarchy for block labels. Further, instead of packing variables into a CFG, we follow Kelsey [12] in annotating each parameter to a φ function with the label of the basic block that computes the corresponding value, or start when the value is computed in the unlabelled entry block of the procedure.
A complete SSA program p consists of a set of (possibly-recursive) pro- cedures and an entry point e. An SSA expression consists of a sequence of



p ::= proc x(x) {b} p | e
b ::= e | b; x:e | b1; x:{b2}
e ::= x ← φ(g); e |
x ← v; e | x ← v(v); e |
goto x; |
ret v; | ret v(v); |
if v then e1 else e2
g ::= l:v
l ::= x | start
v ::= x | c
x ::= x, x | ϵ
v ::= v, v | ϵ
g ::= g, g | ϵ
x ::= variable or label
c ::= constant
e ::= v | v(v) |
let x = v in e | let x = v(v) in e | letrec f in eS |
if v then e1 else e2
f ::= x(x) = e
v ::= x | c
x ::= x, x | ϵ
v ::= v, v | ϵ
	
f ::= f ; f | ϵ
x ::= variable
c ::= constant

(SSA)	(ANF)	
Figure 1. Static Single Assignment and A-normal Forms
assignments ending with a jump. For the purpose of this discussion, we leave the set of constants unspecified; in general, it will include integers, floating point numbers and machine opcodes (primitives). For simplicity, we assume that all variables and labels in a program are unique.

Administrative Normal Form
The right-hand side of Figure 1 presents an abstract syntax of programs in the administrative normal form (ANF) [8], a direct-style form of lambda terms which, like SSA, restricts function parameters to atomic expressions. As demonstrated by the example ANF code in Figure 2, functions are introduced using letrec expressions, which correspond both to a complete program and the CFG structure of a particular procedure in the SSA form. Tail calls are made explicit by their placement within a let expression: function applica- tions appearing on the right-hand side of a variable binding represent normal calls, while those appearing in the body represent tail calls (jumps).
Like the SSA form, ANF encodes data flow explicitly by naming all subex- pressions within the program and permitting only a single definition of any particular variable. However, in ANF this restriction is dynamic, since, at runtime, a new scope is created for every invocation of a function. This makes formulation of the ANF semantics straight-forward and intuitive. Further, it reduces the number of mechanisms present for expressing data flow from two (φ-functions and procedures parameters) to one (parameters only), simplifying program analysis and eliminating the artificial distinction between intra- and inter-procedural data flow in SSA. Finally, the syntactically-clear scoping of definitions simplifies formulation of many valid and useful optimisations that involve code motion across basic blocks. In SSA, design of those algorithms is hampered by the need to preserve the dominance property of a program [2]. A formal operational semantics of SSA and ANF programs are presented in the



proc fac(x){ 
r ← 1; goto L1;;
L1: r0 ← φ(start:r, L1:r1); x0 ← φ(start:x, L1:x1); if x0 then
r1 ← mul(r0, x0); x1 ← sub(x0, 1); goto L1;
else
ret r0;}
ret fac(10);
letrec
fac(x)= letrec
fac′(x0,r0)= if x0 then
let r1 = mul(r0, x0) x1 = sub(x0, 1)
in fac′(x1, r1)
else
r0
in fac′(x, 1) in fac(10)

(SSA)	(ANF)	
Figure 2. SSA and ANF representations of the factorial function
unabridged version of this paper [5]. Variants of ANF are used as the inter- mediate representation in many compilers for functional languages, including GHC [11,21] for Haskell and TIL [20] for ML.
From SSA to ANF
The similarities between the SSA and ANF forms of the factorial function in Figure 2 are immediately obvious: SSA blocks are translated into ANF functions, with the list of arguments derived from the list of φ-expressions in the block. In Figure 3, we define a function F which formalises the translation of a well-formed structured SSA program into ANF.
The translation follows the structure of a program in the SSA form. The complete program is translated by F and Fp into an all-encompassing outer- most letrec. Within each procedure, Fb and Fl generate an ANF function for every SSA block, with each level of the dominator tree translated into a separate nested letrec. This makes variables defined along the dominator path visible through the usual scoping rules for nested procedures, while con- structing a new dynamic scope for each iteration through a translated SSA block. The dominator of a group is selected for the body expression of the letrec. Further, since the leaves of the dominator tree cannot be reached except through their immediate dominator, the scoping rules for ANF en- force the dominance property of the SSA form. If desired, the nested letrec structure in the resulting program can be flattened using the standard lambda- lifting [10] transformation.
Each SSA block is translated into a separate ANF function. A jump is translated into a tail call to the corresponding function, with the list of pa- rameters obtained by Fg from the list of φ-nodes in the destination block. When translating a block into a function, the corresponding list of formal pa- rameters is computed using Fφ. Since, in SSA, a variable not defined along the dominator path must be accessed through a φ-node, this enforces the well-formedness of the resulting program.


The translation function:
F ( p)) = letrec Fp(p) in Fe(Sp(p), start, ⟨⟩) Collect SSA procedures for the outer letrec: Fp( e)) = ϵ
Fp( proc x(x) {b} p)) = x(x)=Fb(b, start, B(b, ⟨⟩));Fp(p)
Construct the inner letrec structure from the dominator tree:
Fb( e), l, B) = letrec Fl(b, B) in Fe(Sb(b), l, B) Collect the SSA blocks into an inner letrec list: Fl( e), B) = ϵ
Fl( b; x:e), B) = x(Fφ(e, ϵ))=Fe(e, x, B);Fl(b, B)
Fl( b1; x:{b2}), B) = x(Fφ(Sb(b2), ϵ))=Fb(b2, x, B(b2, B));Fl(b1, B)
Convert an SSA block to an ANF expression:
Fe( x ← φ(g); e), l, B) = Fe(e, l, B)
Fe( x ← v; e), l, B) = let x = v in Fe(e, l, B)
Fe( x ← v(v); e), l, B) = let x = v(v) in Fe(e, l, B) Fe( ret v;), l, B) = v

Fe( ret v(v);), l, B) = v(v)
Fe( if v then e1 else e2), l, B) = if v then Fe(e1, l, B) else Fe(e2, l, B)
Fe( goto x;), l, B) = x(Fg(β(B, x), l, ϵ))
Construct a list of function parameters from the φ-nodes:
Fφ( x ← φ(g); e), x) = x,Fφ(e, x)
Fφ( x ← v; e), x) = Fφ(e, x)
Fφ( x ← v(v); e), x) = Fφ(e, x)
Fφ( if v then e1 else e2), x) = Fφ(e1, Fφ(e2, x))
Fφ( e), x) = x
Construct an argument list for a translated jump:
Fg( x ← φ(g); e), l, x) = κ(g, l),Fg(e, l, x)
Fg( x ← v; e), l, x) = Fg(e, l, x)
Fg( x ← v(v); e), l, x) = Fg(e, l, x)
Fg( if v then e1 else e2), l, x) = Fg(e1, l, Fg(e2, l, x))
Fg( e), l, x) = x
Find the entry expression for the program:
Sp( e)) = e
Sp( proc x(x) {b} p)) = Sp(p)
Find the entry block for a procedure:
Sb( e)) = e
Sb( b x:e)) = Sb(b)
Sb( b1; x:{b2})) = Sb(b1)
Construct the label list for a procedure:
B( e), B) = B
B( b; x:e), B) = B(b, B), x '→ e
B( b1; x:{b2}), B) = B(b1, B), x '→ Sb(b2)
Search the list of φ parameters for a label:
κ( x1:v,g), x2) = if x1 = x2 then v else κ(g, x2)
Look up a label environment:
β( B, x1 '→ e), x2) = if x1 = x2 then e else β(B, x2)
Auxiliary syntax for the label environment:
B ::= ⟨⟩ | B, x '→ e
Figure 3. Translation of SSA to ANF


Theorem 2.1 For any well-formed SSA program p, J(p) generates a well- formed ANF program.
The proof of Theorem 2.1 relies on well-formedness properies for SSA and ANF, which is presented in the unabridged version of this paper [5].
Sparse Conditional Constants in Functional Form
To demonstrate the advantages of ANF over SSA, we introduce an ANF vari- ant of the SSA-based sparse conditional constants algorithm Sccssa. Due to space constraints, we are not able to present the original algorithm in this paper; please refer to the article of Wegman & Zadeck [22] for a comparison. The reminder of this section defines Sccanf, which we claim discovers the same constants as Sccssa and removes the same unreachable code, given the ANF term computed from the SSA program using the function J from Figure 3.
Prerequisites
We assume that the analysed program is in ANF (as defined in Figure 1), that it is first-order, and that all variable names are unique and contained in the set Var. (We discuss higher-order programs in Section 3.5.) Moreover, the set Prim ⊂ Var contains the names of all primitive functions. The analysis proceeds over an abstract domain Abs = {⊥, T} ∪ Const, where the c ∈ Const are the constant values of the concrete value domain. A partial order и is defined on Abs, which has ⊥ as its least element and T as its largest element. 2 More precisely, for each constant c ∈ Const, we have ⊥ и c и T. In addition, we define, for x, y ∈ Abs, z = x Hy to be the least value in Abs such that x ± z and y ± z. The intuition underlying this abstract domain is that whenever a variable or function maps to ⊥, it never receives a concrete value; 3 whenever it maps to a constant, this is the only value it ever assumes; and whenever it maps to T, it is non-constant.
We require a function S that implements the evaluation of primitives from Prim; i.e., for p ∈ Prim and ci ∈ Const, S p(c1,..., cn)) computes the result of applying p to the ci. Moreover, SAbs extends S to the abstract domain Abs— i.e., if any argument to the primitive is ⊥ or T, it yields ⊥ or T, respectively; otherwise, it behaves as S .
Apart from the input program, the central data structure of the algorithm is an environment Γ : Var → Abs that maps variable names to values of the abstract domain. The environment includes entries for value and function variables, where the latter determine the result value of the function. We write

2 We use the abstract interpretation convention that ⊥ represents no result and T represents conflicting values. This is opposite to the use of the same symbols in the dataflow analysis literature.
3 Note that this includes non-terminating functions; hence, it is not a sufficient condition
for concluding that the function is dead code.


Γx to denote the lookup of the value associated with x in the environment Γ and Γ[x '→ a] to denote updating of the value associated with x to be a. In addition, Dom Γ denotes the variables that have an entry in Γ. Finally, we define the reﬁnement of the entry for x by a as
Γ H [x '→ a] | x ∈/ Dom Γ = Γ[x '→ a]
| otherwise  = Γ[x '→ (Γ x H a)]
The initial value for the environment is ΓPrim = [p '→ T | 6p ∈ Prim].
We use FV f to denote all variables that are free in the body of function f ; note that this includes all argument variables and, in the case of a recursive function, the function name itself. Conversely, Occ x denotes all functions f , in the program, for which x ∈ FV f . The algorithm maintains a work list Ω of names of functions that need to be processed; Ω is initially empty.
The Algorithm
The algorithm computes the optimised form of the program in two phases: The first phase, A, analyses the program by computing the variable environment Γ and the second phase, £, computes the optimised version based on Γ. Both phases are syntax-directed (i.e., proceed according to the ANF grammar from Fig. 1). Note that in the definition of A and £, the meta-variable v may either be a constant c or a value variable x. We assume the canonical extension of Γ to constant values, so that Γc = c.
We denote A and £ using a notation that can be understood as a LATEX- enhanced version of the purely functional programming language Haskell [15]. Consequently, the semantics of our algorithm is accurately defined by the language definition of Haskell.
The ﬁrst phase: program analysis
The function A, which is being displayed in Figure 4 (see the next page), gets three arguments: (1) an ANF expression e that is being traversed recursively,
(2) the current variable environment Γ, and (3) the current work list Ω. It returns a triple containing (a) the abstract value of e, (b) an updated variable environment, and (c) a new work list. The latter contains functions that are used, but not defined in e, and whose usage has increased the knowledge about the range of argument values with which the functions are invoked. The computationally most costly case of A is that of letrec expressions, where we need to iterate until Γ does not collect any new information.
Given a program e in ANF, if A e) ΓPrim {} = ⟨a, Γ, {}⟩, we can distinguish three cases:
If a = ⊥, the program does not terminate.
If a = c, for a constant c, the program invariably results in c.
If a = T, the environment Γ characterises the usage of all variables in the


A v )	Γ Ω = ⟨Γ v, Γ, Ω⟩
A f (v1, ... ,vn ))	Γ Ω 
| f ∈ Prim	= SAbs f (Γ v1, ... , Γ vn ))
| otherwise	= ⟨Γ' f , Γ', if changed then Ω ∪ {f } else Ω⟩
where
f is defined as f (x1, ... ,xn)= e
Γ'	= Γ H [f '→ ⊥, x1 '→ Γ v1, . . . , xn '→ Γ vn ]
changed = ∃ i . Γ xi и Γ vi	-- indicates whether Γ changed
A let x =e1 in e2)	Γ Ω = let
⟨a, Γ', Ω'⟩ = A e1) ΓΩ Γ''	= Γ' H [x '→ a]
changed	= Γ x и Γ a affected	 = Occ x ∩ Dom Γ
-- ∩ Dom Γ removes not yet used functions


A if v then e1 else e2) Γ Ω 
in
A e2) Γ'' (if changed then Ω ∪ affected else Ω)

| Γ v == ⊥	= ⟨⊥, Γ, Ω⟩
| Γ v == True	= A e1) ΓΩ 
| Γ v == False	= A e2) ΓΩ 
| Γ v == T	= let
⟨a1, Γ1, Ω1⟩ = A e1) ΓΩ 
⟨a2, Γ2, Ω2⟩ = A e2) Γ1 Ω1


A letrec f1,..., fn in e)Γ Ω =
let
⟨a, Γ', Ω'⟩ = A e) Γ {}
in
⟨a1 H a2, Γ2, Ω2⟩

⟨Γ'', Ω''⟩	= Afix f1, . . . , fn ) Γ' (Ω ∪ Ω')
in
if Γ' == Γ'' then ⟨a, Γ', Ω''⟩ else A letrec f1, . . . , fn in e) Γ'' Ω''
Afix fun1, ... ,funn ) ΓΩ | ∄ i .fi ∈ Ω= ⟨Γ, Ω⟩
| otherwise =
let
⟨a, Γ', Ω'⟩ = A e) Γ {}
Γ''	= Γ' H [fi '→ a]
Ω''	= Ω ∪ Ω' \ {fi }
in
Afix fun1, . . . , funn ) Γ'' (if Γ fi и Γ a then Ω'' ∪ (Occ fi ∩ Dom Γ) else Ω'')
where
(fi (x1, ... , xm )= e) = funi

Figure 4. The analysis function of Sccanf


program. In particular, all variables mapping to constant values may be

replaced by said constants. Moreover, all functions f
unreachable code.
∈/ Dom Γ constitute

The various equations of A operate as follows. If the analysed expression is simply a constant or variable, we use Γ to determine its abstract value. In the case of the application of a primitive p(v1, ... ,vn), we use Γ to obtain the abstract values of the arguments vi and apply the abstract evaluation function SAbs. More interesting is the case of the application of a user-defined function, f (v1, ... ,vn). The environment Γ is refined to Γ' by refining the mapping of each xi (the formal parameters to f ) by Γ vi (the concrete values at which f is called). If Γ actually changes during this refinement, f is added to the work list Ω, as we need to (re)process its definition in view of the new environment. The refinement f '→ ⊥ is important to ensure that f occurs in Γ; i.e., it is flagged as reachable code.
If the analysed expression has the form let x =e1 in e2, we refine the map- ping of x in Γ based on the abstract value of e1; if this refinement changes Γ, all functions that contain x as a free variable are added to the work list, as their abstract value may change as a consequence. Note that we need to intersect Occ x with Dom Γ to ensure that only functions that are guaranteed to be reachable are added to the work list. If the analysed expression is a con- ditional if v then e1 else e2, we form the meet a1 H a2 of the abstract values of the two branches if the condition variable v is non-constant; otherwise, we choose the branch determined by v.
Finally, in the case of an expression letrec f1, . .., fn in e, we first tra- verse the body expression e, to collect all uses of functions from the mutually recursive set of bindings f1,..., fn in the modified work list Ω'; afterwards, we analyse the fi using the auxiliary function Afix. The latter is a recursive function that, on each call, picks a function fi that occurs in the current work list and analyses its right hand side. It uses the result to refine the entry of fi in Γ; as in the case of plain let bindings, the work list is extended by Occ fi if Γ changes. The function Afix terminates if none of the fi occur in the work list Ω anymore. This does not mean that Ω is necessarily empty; it may still contain functions defined in enclosing letrec expressions.
The second phase: program specialisation
On the basis of the assignment of abstract values to variables in Γ, the func- tion £ transforms the original program into an optimised program that has constant variables and unreachable code removed. The function £ operates in a single sweep over the program and exploits the following properties: When- ever Γ maps a variable (function) to a constant c, this variable (function) will contain (return) c in any possible run of the program that uses the variable (invokes the function). Moreover, a value of ⊥ for a function indicates that it does not terminate. Hence, any occurrence of such a function may be re- placed by an arbitrary diverging computation, which we again denote by ⊥.

Any function not in Dom Γ is dead code. The specialisation function that takes the original program in combination with the environment Γ computed by A to the optimised program is defined as follows:

where
wi	= £ vi ), 6 i ∈ {1,..., n}
eval = f ∈ Prim and 6 i ∈ {1,..., n}. wi ∈ Const
£ let x =e1 in e2)	| Γ x ∈/ {T, ⊥} = £ e2)	-- x is constant
| otherwise	= let x =£ e1) in £ e2)
£ if v then e1 else e2)
| Γ v == True	= £ e1)
| Γ v == False	= £ e2)
| otherwise	= if v then £ e1) else £ e2)
£ f (x1, ... ,xn )= e)	| f ∈/ Dom Γ	= ϵ	-- unused code
| Γ f /= T	= ϵ	-- constant function
| otherwise	= f (x1, ... , xn )= £ e)
£ letrec f1,..., f2 in e)	=
letrec £ f1), . . . , £ f2) in £ e)


Side-Effects
So far, we have not discussed how Sccanf treats side-effecting primitives (such as I/O operations). They require some extra care, but do not pose any funda- mental problems. Whenever A encounters a side-effecting primitive p (second equation in Figure 4), the result of evaluating the application of p is assumed to be T. Moreover, any function whose body contains a side-effecting primitive is mapped to T in Γ. The rationale for the latter is that, even if the function is constant, it must not be removed as its invocation carries an effect.
Note that the existence of side-effecting primitives is the only reason why we need to be able to distinguish between functions that are dead code and non-terminating functions. If a non-terminating function has an effect, we cannot replace it by an arbitrary diverging computation. In other words, the functions
undeﬁned () = undeﬁned ()
ones ()	= let x = write char (’1’) in ones ()
need to be treated differently; although, A would assign ⊥ to both.

Inlining
As ANF already includes a notion of function abstraction, it lends itself to a uniform representation of a set of functions or procedures, which opens a path to inter-procedural analysis and, in our case, inter-procedural constant propagation. It is well known that combining inlining or procedure integration with constant propagation improves the results. For non-recursive functions (that do not contain side-effecting primitives), this can be easily achieved in Sccanf, by treating them as primitives in A and £.
More precisely, in the second equation of the definition of both A and £, we replace the condition f ∈ Prim by a more liberal condition of the form “f ∈ Prim or f is non-recursive.” Moreover, we extend S and SAbs by the ability to evaluate non-recursive functions.
Higher Order Functions
Sccanf requires a first-order program, as it does not explicitly handle higher- order variables. Nevertheless, the treatment of higher-order variables is impor- tant for modern object-oriented and functional languages. There are various ways—of varying sophistication— in which Sccanf can be extended to handle higher-order variables. The simplest scheme, giving the least precise results, refines Γ to Γ H [x1 '→ T,..., xn '→ T] for all functions f (x1,..., xn)= e that are assigned to a variable (as opposed to being invoked). This reflects that we cannot gain any knowledge about the range of argument values at which f is invoked. Moreover, A needs to return T for any higher-order call site x(x1,..., xn), where x is an unknown function value.
Although the above mapping of the xi to T is correct, it is too conservative as we may be able to statically infer the call sites of some functions that are assigned to variables. In particular, we improve on the basic scheme by using constant propagation to identify higher-order call sites that, for all possible executions of a program, invoke the same function. To do so, it suffices to extend the set of constants contained in the abstract domain Abs with symbols representing the various functions that are used as values. Whenever A derives Γx = f for a call site x(x1,..., xn), we can treat this call site as f (x1,..., xn). However, by noting the similarity of Sccanf and abstract interpretation [6]
as well as the fact that Sccanf corresponds to what is known as a 0CFA anal-
ysis [19], we can use power sets of abstract closures as the abstract domain for higher-order variables. A detailed description of the resulting algorithm is beyond the scope of this paper, but it is related to the constant propagation algorithm studied by Sabry and Felleisen [17].
Soundness of the New Algorithm
To establish soundness, we need three auxiliary definitions. The first one relates environments w.r.t. the free variables of an expression. It is oblivious

to the entries relating to bound variables.
Definition 4.1 Γ0 ≤FV(e) Γ1 ⇔ 6 x ∈ FV e ∩ Dom Γ0, Γ0 x = Γ1 x .
The next two definitions cover the validity of the algorithm A and its auxiliary function Afix for one particular expression.
Definition 4.2 A is valid for an expression e (denoted A e)) iff for any two environments Γ0 and Γ', with Γ0 ≤FV(e) Γ', A e) Γ0 {} = ⟨a, Γ, Ω⟩ implies SAbs e) Γ' = SAbs £ e Γ) Γ'; furthermore, if Ω = {}, SAbs e) Γ' = a.
Definition 4.3 Afix is valid for a set of function bindings fs and a work list Ω0 (denoted Afix fs) Ω0) iff for any two environments Γ0 and Γ', with Γ0 ≤FV(fs) Γ', we have for every binding f (x1,..., xn ) = e, Afix fs) Γ0 Ω0 = ⟨Γ, Ω⟩ im- plies SAbs e) Γ' = SAbs £ e Γ) Γ' whenever f ∈ Ω0.
Since the relation ≤FV(e) is oblivious to bound variables, validity implies that the algorithm yields a suitable environment even if bound variables in the input environment differ from the actual value assigned to a variable. This property is important as the algorithm is optimistic—i.e., intermediate envi- ronments constructed during execution of the algorithm may make incorrect assumptions about the values of some variables. However, the algorithm does not terminate unless all of these bindings are replaced by T.
Theorem 4.4 (Soundness of A) A is valid for all ANF expressions e.
The proof proceeds by induction over the nesting depth of letrecs in an expression and contains the following three main steps:
Base Case (A(0)): We assert that for every ANF expression e that contains no letrec expression, we have A e).
Auxiliary Step (A(n) ⇒ Afix(n)): Given a list of function bindings fs and a work list Ω, we assert that Afix fs) Ω holds only if, for all bindings f (x1,..., xn ) = e from fs, A e) holds.
Inductive Step (A(n) ⇒ A(n + 1)): Given a list of function bindings fs and an ANF expression e, we assert that A e) and Afix fs) together imply A letrec fs in e).
We detail these three steps in the unabridged version of this paper [5].
Conclusion
We have formalised the mapping of programs in SSA form to ANF and pre- sented an ANF version of Wegman & Zadeck’s conditional constant propaga- tion algorithm, which we called Sccanf. Moreover, we have outlined how the ANF-based algorithm can be extended to include inlining and higher-order functions.
We are interested in assessing the usefulness of typed, functional inter- mediate languages in compilers for conventional languages.  The algorithm

Sccanf is more rigorously defined than Wegman & Zadeck’s original algorithm as, firstly, ANF has a well-defined semantics and, secondly, our notation is essentially a nicely typeset version of the programming language Haskell. 4 Such semantic clarity is a prerequisite for any rigorous formal reasoning about compiler optimisations. Moreover, it is a prerequisite for a serious comparison of the work on abstract interpretation with that in the classic dataflow anal- ysis literature. An alternative method for formalising conditional constant propagation in a graph-based framework was introduced by Lerner et al. [13]. However, their focus is on the modular composition of dataflow analyses.
Acknowledgements. We thank the referees for their helpful comments.

References
Alpern, B., M. N. Wegman and F. K. Zadeck, Detecting equality of variables in programs, in: 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (1998), pp. 1–11.
Appel, A. W., “Modern Compiler Implementation in ML,” Cambridge University Press, 1998.
Appel, A. W., SSA is functional programming, ACM SIGPLAN Notices 33
(1998), pp. 17–20.
Ballance, R. A., A. B. Maccabe and K. J. Ottenstein, The program dependence web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages, in: Proceedings of the Conference on Programming Language Design and Implementation (1990), pp. 257–271.
Chakravarty, M. M. T., G. Keller and P. Zadarnowski, A functional perspective on SSA optimisation algorithms—unabridged, Technical Report 0217, University of New South Wales (2003).
Cousot, P. and R. Cousot, Abstract interpretation: a uniﬁed lattice model for static analysis of programs by construction or approximation of ﬁxpoints, in: Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (1977), pp. 238–252.
Cytron, R., J. Ferrante, B. K. Rosen, M. N. Wegman and F. K. Zadeck, Efficiently computing static single assignment form and the control dependence graph, ACM Transactions on Programming Languages and Systems 13 (1991),
pp. 451–490.
Flanagan, C., A. Sabry, B. F. Duba and M. Felleisen, The essence of compiling with continuations, in: Proceedings ACM SIGPLAN 1993 Conf. on Programming Language Design and Implementation, PLDI’93 (1993), pp. 237– 247.

4 A concrete implementation of the two functions A and £, in Haskell, is available from
http://www.cse.unsw.edu.au/~patrykz/ssa-lambda/.


Harper, R. and G. Morrisett, Compiling polymorphism using intensional type analysis, in: 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (1995), pp. 130–141.
Johnsson, T., Lambda lifting: Transforming programs to recursive equations, in: Jouannaud, editor, Proceedings of the International Conference on Functional Programming and Computer Architecture, number 201 in Lecture Notes in Computer Science (1985).
Jones, S. P. and A. L. M. Santos, A transformation-based optimiser for Haskell, Science of Computer Programming 32 (1998), pp. 3–47.
Kelsey, R. A., A correspondence between continuation passing style and static single assignment form, ACM SIGPLAN Notices 30 (1995), pp. 13–22.
Lerner, S., D. Grove and C. Chambers, Composing dataflow analyses and transformations, in: Proceedings of the 29th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (2002), pp. 270–282.
Morrisett, G., D. Walker, K. Crary and N. Glew, From System F to typed assembly language, ACM Transactions on Programming Languages and Systems 21 (1999), pp. 528–569.
Peyton Jones, S. et al., Haskell 98: A non-strict, purely functional language
(1999).
URL http://haskell.org/definition/
Peyton Jones, S. L., Compiling Haskell by program transformation: a report from the trenches, in: H. R. Nielson, editor, Proceedings of the European Symposium on Programming, Lecture Notes in Computer Science 1058 (1996), pp. 18–44.
Sabry, A. and M. Felleisen, Is continuation-passing useful for data flow analysis?, in: Proceedings of the ACM SIGPLAN’94 Conference on Programming Language Design and Implementation (PLDI) (1994), pp. 1–12.
Schneider, F. B., G. Morrisett and R. Harper, A language-based approach to security, in: Informatics: 10 Years Back, 10 Years Ahead, Lecture Notes in Computer Science 2000 (2000), pp. 86–101.
Shivers, O., Control-flow analysis in Scheme, in: Proceedings of the SIGPLAN ’88 Conference on Programming Language Design and Implementation (1988).
Tarditi, D., G. Morrisett, P. Cheng, C. Stone, R. Harper and P. Lee, TIL: A type-directed optimizing compiler for ML, in: SIGPLAN Conference on Programming Language Design and Implementation, 1996, pp. 181–192.
Tolmach, A. et al., An external representation for the GHC core language
(2001).
Wegman, M. N. and F. K. Zadeck, Constant propagation with conditional branches, ACM Transactions on Programming Languages and Systems 13 (1991), pp. 181–210.
