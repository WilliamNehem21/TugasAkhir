Electronic Notes in Theoretical Computer Science 133 (2005) 81–100 
www.elsevier.com/locate/entcs


Modeling Fault-tolerant Distributed Systems for Discrete Controller Synthesis
Alain Girault, Eric Rutten1
INRIA Rhoˆne-Alpes, POP ART,
655 avenue de l’Europe, 38334 Saint-Ismier cedex, FRANCE.

Abstract
Embedded systems require safe design methods based on formal methods, as well as safe execution based on fault-tolerance techniques. We propose a safe design method for safe execution systems: it uses discrete controller synthesis (DCS) to generate a correct reconfiguring system. The properties enforced concern consistent execution, functionality fulfillment (whatever the faults, under some failure hypothesis), and several optimizations. We propose model patterns for a set of periodic tasks, a set of distributed, heterogeneous and fail-silent processors, and an environment model that expresses the potential fault patterns. We outline an implementation of our method, using the Sigali symbolic DCS tool and Mode Automata.
Keywords: Discrete controller synthesis, fault-tolerance, real-time systems.


Introduction
Safety critical embedded systems
Embedded systems account for a major part of critical applications (space, aeronautics, nuclear. . .) as well as public domain applications (automotive, consumer electronics. . .). Their main features are:
duality automatic-control/discrete-event : they include control laws modeled as differential equations in sampled time, computed iteratively, and discrete event systems to sequence the control laws according to mode switches;

1 Email: Alain.Girault@inrialpes.fr, Eric.Rutten@inrialpes.fr


1571-0661 © 2005 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.08.059


critical real-time: unmet timing constraints may involve a system failure leading to a disaster;
limited resources: they rely on limited computing power and memory be- cause of weight and encumbrance, power consumption (autonomous vehicles or portable devices), radiation resistance (nuclear or space), or price con- straints (consumer electronics);
distributed and heterogeneous architecture: they are often distributed to provide enough computing power and to keep computing sites close to the sensors and actuators.
Problem statement
An embedded system being intrinsically critical, it is essential to insure that it is tolerant to processor failures. This can even motivate its distribution itself. In such a case, at the very least, the loss of one computing site must not lead to the loss of the whole application. We are interested in formal methods to model systems with guarantees on their fault-tolerance. Among the various existing formal methods, we investigate the use of discrete controller synthesis (DCS). The advantages of using DCS are the correctness of the resulting system and the easy modifiability of the controller (thanks to automatic tools), i.e., the possibility to study and test several fault-tolerance objectives or failure hypotheses on the same system model, without the need to re-design the system. Specifically, our objective is:
To produce automatically a controller enforcing fault-tolerance for a given distributed system.
Fault-tolerance is the faculty to maintain functionality of a system, whatever the faults under some failure hypothesis. To achieve this, we will need first to model our distributed systems, and second to express formally some fault- tolerance objective, in terms of events and states of the system.
We propose to designers a methodology for modeling a system and studying the existence of fault-tolerant solutions according to several failure hypotheses and system’s configurations. When a solution is found, it can be used either as a guideline for implementation (if the model was an abstract one [9]) or for deployment with a dynamic failure reconfiguring feature (this paper).
In our approach, a system consists of a set of tasks placed in a conﬁguration onto a set of processors. Upon occurrence of a fault, one or several processors become unusable, and tasks must be placed anew in another configuration, by restarting them onto another processor, so that execution can proceed. These reconﬁgurations of the system have to be controlled according to a fault- tolerance policy, enforced by a task manager. The latter is specified in terms


of properties concerning placement constraints, reachability of termination, and optimization of costs and qualities.
We propose to automatically produce the task manager with DCS tech- niques, applied to a model of the system in all its possible configurations. This model will consist of several components, each modeled as a labeled transi- tion system (LTS), and composed in parallel; DCS will produce a property- enforcing layer on top of the components [1].
The technical context of our work is the synchronous approach 2 for the design of reactive systems [4]. This choice is motivated by the existence of a corpus of available results (laguages, compilers, formal tools) and technologies, which already have an industrial impact. Our method is compatible with synchronous models, and this influences some of our choices in the LTSs and composition, as well as in already existing DCS, applied as such [17].

Related work
Formal approaches to the design of fault-tolerant systems have mostly con- sidered the problem of verification, in the context of process algebra [21,6,5]. They verify that an existing, hand-made design (replicas interaction control, voters, etc) satisfies a certain equivalence with the nominal functionality spec- ification, even in case of faults. In contrast, DCS approaches [12] synthesise automatically a controller that will insure this by construction. The princi- ple is to consider faults as uncontrollable events, and fault-tolerance as the existence of behaviors able to achieve the functionality whatever the occur- ing faults. Planning under uncertainty is another existing approach [12], so far only demonstrated with 1-fault tolerant paths. We place ourselves in the framework of reactive systems, finite state machines, and the use of synthesis of exact most permissive controllers. Moreover, we tolerate several failures, not only one. The reachability of marked final states defines the ability to achieve functionality, and can be used as a criteria on the existence of a solution [8]; we take it as a synthesis objective. Other works on applying DCS to real-time systems exist [13], taking into account timed aspects, for the generation of cor- rect application-specific schedulers, but they do not consider fault-tolerance specifically. Also, we concentrate on Boolean models; synthesis in timed or hybrid systems [2] would be more powerful, while remaining in the decidable problems, but at a very high efficiency cost. There exist results in process algebra comparable with a form of synthesis, but that comparison is out of our scope. Finally, in another parallel work, we have used DCS for distributed controller synthesis, a more difficult objective that was achieved manually [9],

2 http://www.synalp.org


whereas here we synthesise a centralised controller, but automatically.

Background
Fault-tolerance
Fault-tolerance has been extensively studied in the literature: [14] gives an exhaustive list of the basic concepts and terminology, [20] gives a short survey and taxonomy for fault-tolerance and real-time systems, and [11] treat in details the special case of fault-tolerance in distributed systems.
The three basic notions are fault, failure, and error : a fault is a defect or flaw that occurs in some hardware or software component; an error is a manifestation of a fault; a failure is a departure of a system from the service required. A failure in a sub-system may be seen as a fault in the global system. Hence the following causal relationship:


··· −→ fault
−a−ct−iv−at−io→n
propagation
error −−−−−−→ failure
causality
−−−−−→ fault −→ ··· 

We assume the following failure hypothesis : only the processors can fail, with a fail-silent model. That is, a processor is either active and works fine, or faulty and does not produce any output. To tolerate such faults, we are going to make use of the intrinsic hardware redundancy offered by the dis- tributed architecture: i.e., we do not wish to add extra processors but to use only the existing ones. Our goal is to apply error treatment techniques, such that whenever a processor will fail, the tasks that were active on it will be dynamically restarted on some other non faulty processor. The new state of the system reached after such an error treatment is degraded in the sense that less processors are now available, but the functionality is maintained since all the tasks are still being executed.

Discrete controller synthesis
This section gives a very brief description of DCS. As we adopt an existing framework [17], we do not reproduce the definitions or technicalities of the tools, but just summarize the functionality. DCS emerged in the 80’s [19], with foundations in language theory. Its purpose is, given two languages P
and D, to obtain a third language C such that P ∩ C ⊆ D. This is a kind of
inversion problem, since one wants to find C from D and P. Here, P is called the plant, D the desired system or objective, and C the controller.
Recently, several teams proposed extensions and applications of this lan-
guage theory technique to labeled transition systems (LTS). Formally, an LTS


is a tuple ⟨Q, q0, I, O, T ⟩, where Q is a finite set of states, q0 is the initial state, I is a finite set of input signals (produced by the environment), O is a finite set of output signals (issued to the environment), and T is the transition relation, i.e., a subset of Q× Bool(I) × O∗ × Q. Each transition has a label of the form g/a, where g ∈ Bool(I) must be true for the transition to be taken (g is the guard of the transition), while a ∈ O∗ is a conjunction of outputs that are issued when the transition is taken (a is the action of the transition).
In our approach, P is specified as a LTS, and D is an objective to be sat- isfied by the controlled system, typically making a subset of states invariant in the controlled system, or keeping it always reachable. The controller C ob- tained with DCS is a constraint restricting the transitions of P, i.e., inhibiting those that would jeopardize the objective. The key point is that the set of inputs I is partitioned into two subsets, Ic and Iu, respectively the set of con- trollable and uncontrollable inputs. The principle of DCS is that the controller C can only constrain those transitions of P for which the guard contains at least one controllable signal, i.e., in Ic.
As illustrated in Figure 1, the objective is expressed in terms of the sys-
tem’s outputs and the controller is obtained automatically and its purpose is precisely to act on the controllable inputs in order to achieve the objective.

Iu
O
Ic




Fig. 1. From uncontrolled system (left) to closed-loop control (right).
It is also possible to consider weights assigned to the states and/or in- puts/outputs of P, and to specify that some upper or lower bound must never be reached. Optimal controller synthesis [16] can then be used to control transitions so as to minimize/maximize, in one step (or on bounded paths),
some function w.r.t. these weights; i.e., go only to next states with optimal weight. There can be several equally weighted solutions, so optimization does not necessarily lead to determinism. It can be noted that this gives us only a one step choice i.e., a local optimal, not a global optimal on all the behaviors. With respect to our problem, such weights can model the worst-case execution time (WCET) of a given task onto a given processor, its power consumption, the amount of processor load it requires, or the quality of its results when executed on this particular processor.
The order in which synthesis operations are applied does matter: indeed, their sequence is not commutative. Reachability can not be considered before


an invariance constraint, because the latter might compromise the former by removing paths and breaking reachability. On the contrary, considering reach- ability after invariance does not jeopardize the invariance, as it will not result in paths going out of the invariant set. Optimization should be considered last, as a choice among correct solutions; even after reachability, it will keep only some paths, which should always satisfy it.
The result of the synthesis is a constraint, which, as it is computed auto- matically, is not quite readable or usable “brainually” by the designer, but is meant to be coupled with the system as in Figure 1, or more precisely com- posed as shown in Figure 6. An implementation of this coupling is proposed in Section 6.
Property-enforcing layers
Our approach follows a framework for the automatic generation of property- enforcing layers, in a mixed imperative/declarative style, based on DCS [1].
A system is designed as a set of local components, each modeled by a LTS describing its relevant control states and transitions, and local constraints
w.r.t. the environment or other components. Particularly, they feature inputs enabling the control of choices between configurations. The synchronous prod- uct of these LTSs gives a global model of the system which is a first approxi- mation of the set of constraints that should be respected. Global constraints involving several components are expressed as logic properties of this prod- uct. In the absence of a management of these global constraints, they are not satisfied; in other words, the product models the behaviors of the uncontrolled global system. We use general DCS techniques and tools, as presented above, in order to automatically compute and generate a property-enforcing layer, which, when combined with the set of communicating parallel automata, will guarantee the satisfaction of the global constraint. This controller will give values to the controllable inputs of components so that remaining behaviors are correct, whatever the values of the other inputs.
Advantages of this method are twofold: on the one hand, the property- enforcing layer is correct, because of the fact that it is the result of an exact computation. On the other hand, the automated nature of the process makes for an easy modifiability of designs, be it in the components behaviors or in the declarative properties; hence, a variety of global constraints can be experimented for a given system under study, providing for effective support in the design space exploration.
In this paper, this general framework is applied specifically to fault-tole- rance. The components are tasks and processors, for which local models repre- sent configurations and failures. Global constraints specify the fault-tolerance


as execution coherence and the maintaining of functionality, in terms of in- variance and reachability. The synthesized controller manages tasks reconfig- urations in order to enforce the required fault-tolerance policy.

Abstract model of a distributed system
In this section, we specify our abstract model, and failure hypothesis. All the while, we keep in mind our objective, so as to make these abstract models suitable for DCS. So, we consider real-time systems composed of:
a distributed heterogeneous architecture, consisting of a set of fail-silent processors, fully connected by point-to-point communication links,
a set of periodic tasks, with the possibility to run them on the different processors, with varying characteristics (quality, power or time cost),
an application, invoking the tasks, which can be considered simply as a task management layer, or as a scheduler or program, enforcing precedence constraints between the tasks.
The real-time aspect of such systems comes from the time costs of the periodic tasks. The time cost of each task is measured thanks to a WCET analysis. Then, each task being periodic, we consider that, when executing on a pro- cessor, it uses some CPU load, computed by dividing its WCET by its period. Enforcing real-time constraints amounts thus to assigning to each processor a CPU load maximal bound, which should never be overtaken.


Architecture model
Local processor model
Each processor is modeled by the LTS of Figure 2, where OKi means that the processor i is running fine, while ERRi means that it has crashed. We assume that only the pro- cessors can fail, with a fail-silent model. Recent studies on modern processors have shown that a fail-silent behavior can be achieved at a reasonable cost [3]. Failures are also per- manent, hence a processor cannot go back from the ERR to the OK state. To model intermittent failures, we would just need to add such a transition.
Processors can be used by tasks in a time-sharing manner,



Fig. 2: Processor model.

so that several tasks can be active on the same processor at the same time. Related to this, one might consider exclusions between tasks, forbidden to share the same processor because of the use of some exclusive


resource. Also, related to the weights and particularly costs in power and load, individual tasks weights are to be additive: on a given processor, the global load is the sum of that of all the active tasks. Each processor i has a quantitative bound bi, specifying its maximum power capacity.
Heterogeneous architecture model


The processors are embedded inside a fully con- nected network of point-to-point communication links, like the one presented in Figure 3. We note S the set
of all these processors. We assume that the communi-
cation links cannot fail. One processor is dedicated to executing the controller, P0, and only the other proces- sors are available for executing the system’s tasks. Each processor must detect in real-time the other processors’ failures. This can be easily implemented by making all


S = {P1 , P2, P3}
Fig. 3: Distributed ar- chitecture.

the processors in S send an “I am alive” message to each other at periodic interval, like in group membership protocols [10].
The model consists of the composition of all LTSs as above. In the example, we have three of them, one for each of the processors P1, P2, and P3, for which capacity bounds bi w.r.t. power consumption are, respectively, 5, 3, and 6.
This distributed architecture is heterogeneous, meaning that the WCET and power consumption of each task is not the same on each processor. There may be tasks that cannot run on some processor, for instance because they require a specific hardware device (input sensor, dedicated co-processor...).
Environment or fault model
We now need to model what failures can occur in the system. For instance, how many failures can occur? Can they occur simultaneously? In terms of our processor model of Figure 2, the question is how can the fi events occur?
It seems natural that all the fi events be uncontrollable (i.e., ∈ Iu), since a failure is a event intrinsically uncontrollable. But this would mean that there would be no constraints whatsoever on them. In particular, all events fi could occur, meaning that all processors could fail. Of course, this would
result in a total failure of the system, with no possibility at all to ensure the fault-tolerance of the system. No one expects a system to tolerate a failure of all the processors it is made of. Therefore, we need to specify the way the failures do occur in the patterns that we consider.
To model this, we choose to have a LTS modeling the environment. Its purpose is to issue the signals fi from signals ei produced by the environment.


These signals ei will be uncontrollable (i.e., ∈ Iu), reflecting the fact that a failure can occur at any time, while the signals fi will be local, i.e., neither in Iu nor in Ic, and will be used only for building the synchronous product of all the LTSs.




	 

(a)
(b)
(c)

Fig. 4. Fault model: (a) only one failure; (b) one or two; (c) failure pattern.





The environment model of Figure 4(a) allows only one failure to occur in the system, while the one of Figure 4(b) allows two failures to occur, possibly simultaneously (if simultaneous occurrences of failures are forbidden, it suf- fices to remove the three transitions from B to F1,2, F1,3, and F2,3). In both cases, B is the initial state while the state Fi,j,k... records the occurrences, not necessarily simultaneous, of the failures of processors Pi, Pj, Pk ... 
As a variant, according to the available knowledge about the system, one can directly specify the failure patterns by giving directly the LTS producing the local signals fi from the input signals ei. This is more expressive than specifying the number of processors that can fail. For example, Figure 4(c) corresponds to the failure pattern where up to two processors can fail, except that processors P1 and P3 cannot fail together.
Providing such an environement model is up to the designer. His choice will depend on his knowledge of the system and the related failure assump- tions. For instance, if it is unlikely for two failures to occur simultaneously, he will remove from the automaton 4(a) the three transitions from B to Fi,j. Alternatively, if he wants to consider malicious attacks, he will keep them.

Task model


Basic control structure pattern
Each task j is formally modeled by the LTS of Figure 5, drawn assuming that the task can be executed on the three processors of the con- sidered architecture. It features an initial idle state Ij, a ready state Rj after reception of the request signal rj, a terminal state T j, and sev- eral active states Aj, representing task conﬁg-
urations, one for each processor in the system. Here, i indicates which processor the task j is active on; since our architecture has three pro- cessors, each task LTS has three active states.













rj












Fig. 5: Task invocation model.

By convention, subscripts/superscripts refer to processors/tasks. In the state
Aj, task j is periodically executed on processor i, until the occurrence of the
event tj: this is what we mean by periodic tasks. Such periodic tasks can be directly and easily modeled by Mode Automata [15].
Implicitly, each state has an additional self-loop labeled with the comple- ment guard w.r.t. all its other outgoing transitions. For instance, state Ij has a self-loop labeled with rj , which enables the LTS to remain inside Ij until the occurrence of the signal rj.

A transition from state Aj
to state Aj
represents the re-conﬁguration of

the system, by stopping task j on processor i and restarting it onto processor
k. We call this operation a migration. They will be decided in order to maintain the system in a global configuration such that it keeps offering its nominal service. In particular, a migration could be decided as a reaction to a processor failure (in which case the task does not need to be stopped of course). But it could also serve to balance the load between several active processors, or to comply to the energy consumption bound of a processor.
In terms of controller synthesis, the signals rj and tj will be uncontrollable (i.e., ∈ Iu), while the signals aj will be controllable (i.e., ∈ Ic).
Quantitative characteristics
Some interesting characteristics can be modeled as weights associated with states [18]; we consider just simple mappings from states to integers.
Execution time is the CPU load required by each task, as measured by a WCET analysis. Since the tasks are executed periodically, we assume that, when a task migrates from one processor to another one, its execution restarts


Fig. 6. A complete system with 3 processors and 3 tasks, and a controller.
from the beginning on its new processor. Hence its new processor must fully accept the task’s CPU load.
Power consumption Cj of a task j is given relatively to each processor
i. It is related to the WCET, but not in a linear way [7]. For our example, the values of Cj are given in Table 1, along with each bound bi, which is the maximum consumption admissible by the processor i.

Quality Qj
of a task j is given relatively to each processor i. It can

account, e.g., for the accuracy of the results produced either by a numerical computation according to the presence of special co-processors, or by different versions of an algorithm of varying depth in a heuristic search, or by an image processing operation. For our example, the values of Qj are given in Table 1.

Table 1
Consumption Cj , quality Qj of tasks Tj on processors Pi, with bound bi.
i	i

Application model
An application is built upon the invocation of a set of tasks, considering it as a server receiving requests, or including a scheduler or program.

Tasks server
If the system consists of n tasks, there will be n corresponding LTSs in paral- lel. Their synchronous composition as in Mode Automata [1,15]represents all behaviors, i.e., all possible configurations, in response to all possible sequences of requests and termination events.
The composition of quantitative characteristics is considered, in this paper, to be additive.  It is clear for CPU loads or power consumption on each
processor Pi, where we have for tasks j: Ci = Σj C .
j
Regarding quality, we consider overall quality to be the means of that of
active tasks, which can be understood in the same way as papers submitted for a conference receive a global mark that is the means of the various markings. We will use quality just to choose the transitions towards the next states with
the highest quality; hence, we do not need to divide and can just use the sum
of qualities, for processors i and tasks j: Q = Σ Σ Qj.


Scheduler or program
A scheduler or program can be in charge of emit- ting the task requests in a given sequence. Its pur- pose is to schedule the tasks according to the prece- dence graph specified by the user: it must issue the signals rj in the correct order, so that the tasks be- come ready (in the Rj state) in such a way that the precedence constraints are satisfied.


Fig.	7: Precedence con- straints.

If we consider the example of Figure 7, the scheduler first issues r1, then after receiving t1, it issues r2 and r3, therefore executing T 2 and T 3 in parallel, and finally, once it has received t2 and t3, it issues r1.

System model
Finally, the model of the multi-processor, multi-task system is built by com- posing the different local models introduced previously: one for the environ- ment model, one for each processor, one for each task, one for the scheduler, and one for the controller. This is illustrated in Figure 6 for a complete system made of 3 processors and 3 tasks.
Scheduling (deciding in which order tasks are executed) and distribution (deciding where they are executed) are decoupled here: the scheduler sched- ules the tasks according to the precedence constraints, while the controller dynamically distributes the tasks according to the fault-tolerance policy.

Properties, objectives and fault-tolerance
The fault-tolerance policy is specified declaratively by a set of properties and objectives. The fault-tolerance specificity of these properties is twofold. On the one hand, they are meant to be considered upon models as described above, where all faults, recoveries or failures behaviors are represented. On the other hand, they characterize failed states (e.g., consistent placement constraints characterize states where the system is not viable), as well as the tolerance, meaning the notion of fulfilling functionality whatever the faults.
Properties
Insuring consistent execution
Property 1 (No task is active on a failed processor)
j i
j	i
Property 1 is contradicted whenever a task T j is active on processor Pi (i.e., in state Aj) while Pi is in Erri. The synthesis objective is to make it invariantly true. If the system, as modeled by the designer, is such that in each state there exists a transition to a safe state (i.e., one where Property 1 holds), then the synthesis will succeed and the controlled system will allways be able to react to a processor failure by moving to a safe state. Otherwise the synthesis will fail, indicating to the designer that her/his system cannot be made fault-tolerant.
Property 2 (Tasks active are within processor capacity)
∀i, Ci ≤ bi.
Property 2 is contradicted whenever the cumulated cost of all tasks active on a given processor exceeds its capacity bound. Again, the synthesis objective is to make it invariantly true. Typically, this objective can have the effect

of inhibiting the transition from Rj to any active state Aj
for a task j, if

taking this transition means that a later processor failure, specified in the environment model, will not be tolerated without bounding problems. Here, the DCS computes the most permissive controller such that all failures are guaranteed to be tolerated without bounding problems. A terminating task can then release another waiting task.
Insuring functionality
The previous properties were just simple state properties, used to avoid in- consistent configurations. The discrete controller can inhibit indefinitely the


start of a task if there is a possibility that the only remaining processor has too low a bound for it (after the other ones have failed). In that case, there is no solution for insuring functionality, defined here as reaching termination. In other terms, tasks are activated only when “the path is clear and wide enough all the way down” to termination, even in case of failures.
Property 3 (The functionality is fulfilled) From all reachable states, the terminal conﬁgurations such that i T i are reachable.
Property 3 states that whatever the faults, as specified in the environment model, in any sequence and possible simultaneity, a terminal configuration can be reached, for any occurrences and orders of incoming task requests and terminations. This property is instrumental in characterizing fault-tolerance, as it excludes behaviors where all activity would be frozen in the waiting states in order to avoid jeopardizing Properties 1 and 2. It can serve to detect systems which do not have the capacity (logical or quantitative) to actually tolerate faults while continuing to deliver their nominal functionality.
It is different from previous properties in the sense that it considers not only the current state, but the trajectories of the system, requiring them to be able to reach termination.
Optimizing costs and qualities
It is a matter of adopting a policy, by making switches only to the next con- figurations such that they:
maximize the overall quality, when the quality of tasks varies according to the processor;
minimize the global consumption, which can be defined as the sum of costs of tasks on processors.
Also, having this notion of quality (zero on inactive states and positive when active) provides for a way of imposing progress to the controlled system, where the option of remaining in the waiting state endlessly is removed; hence, pro- ceeding to activity, and nearing to completion, is pushed forward. Another, more self-standing, way of doing things would be to have a separate weight accounting for the cost for waiting, and to minimize it [1].
Discussion
Fault-tolerance for embedded systems can be divided into two classes of ap- proaches: static or dynamic. In the static approach, task redundancies are added such that any occurrence of failures be tolerated during the execution; the drawback is that this is expensive since one has to pay the overhead of


redundancy even in the absence of failures; the advantage is that a bound on the system’s reaction time can be computed prior to the system’s deploy- ment, with the guarantee that this bound will hold whatever the occurrence of failures during the execution. In the dynamic approach, mechanisms are added to the system such that the system will be able to react dynamically to any occurrence of failures during the execution; the drawback is that no bound can be computed on the system’s reaction time since this depends on the unpredictable occurrence of failures; the advantage is that no overhead has to be paid in the absence of failure; that is, until a failure occurs, the execution cost of the fault-tolerant system is (almost the same as) that of the corresponding non fault-tolerant one.
We believe that our approach is interesting in the sense that, when the DCS actually succeeds in producing a controller, we obtain a system equipped with a dynamic reconfiguring mechanism to handle failures (i.e., the controller), with a static guarantee that all specified failures will be tolerated during the execution, and with a known bound on the system’s reaction time. In other words, we have the advantages of both approaches. But remember that this is true only when the DCS succeeds. If it fails, since the DCS tool explores all its state space (be it symbolically), it means that no solution exists for these failures to be tolerated and bounds on the processors’ consumption.

Illustrative scenarii
Property 1: consistent execution
In our example, and as illustrated in Figure 8, if P2 becomes faulty (event e2, state ERR2), then no task should be active on it (states A1, A2, and A3). The
2	2	2
same goes for P1 and P3. Obviously, for a fault model where all processors
can fail, no controller can be found satisfying the objective: it can not start a task without risking all processors to fail before its termination, and therefore the behavior will remain stuck in the ready state for all requested tasks.
Along the same lines, tasks with placement constraints can make a system harder to control: indeed, once active on a processor Pi, there must always be another processor able to host them in case of a failure of Pi.

Property 2: bounded capacity
For the sake of the example, we consider a global configuration where we have T 1 onto P1 (4 ≤ 5), T 2 onto P2 (2 ≤ 3), and T 3 onto P3 (4 ≤ 6) (hence not taking into account the precedence constraints of Figure 7). Then, if P2 crashes, T 2 is forced to migrate either onto P1 or onto P3. However, none





quality 3+2+5=10
power
4 ≤ 5 2 ≤ 3 4 ≤ 6




{e2 , a1 , a2 , a3 }

{e2 , a1 }
{e2 }

{e2 , a2 }

{e2 , a2 , a3 }


2+2+3=7
3  1  1
2
3+2+5=10
3	1  1

3+2+5=10


 				 

2+2 ≤ 5	2 ≤ 6
4 ≤ 5
2 ≤ 3
4 ≤ 6
2 ≤ 5	2 + 4 ≤ 6

3+2+5=10	inconsistent (ERR2)

2+ 4 > 5	4 ≤ 6
inconsistent (b1 )
3+5+5=13

4 ≤ 5	3 + 4 > 6
inconsistent (b3 )

Fig. 8. Example of states (only configuration is shown) and transition control.
of these two choices meet the constraint on the processor maximal utilization bound. Indeed, the sum of costs of T 1 and T 2 on P1 would be 2 + 4 > 5, while T 2 and T 3 on P3 would give 3 + 4 > 6. Hence, the controller forces more migrations, e.g., T 1 onto P3 and T 2 onto P1. This time constraints on the bounds will be met both on P1 (2 ≤ 5) and on P3 (2 + 4 ≤ 6).
A solution can be found when, after the other processors have failed as far
as the environment model says, the remaining processors with the smallest capacity are still able to host all the active tasks. This constraint can also block the system in the ready states, because the path is not clear and wide enough for execution. Here, as well as for the previous objective, the environ- ment model can have a determining influence: if it excludes pathological fault patterns, then a solution can be found.
Also, a task model without the possibility to have the control waiting in the ready state until a favorable configuration is reached, allows less solutions. In that case, having a program or scheduler can have an impact, in that only certain subsets of tasks can be activated in parallel. This requires less capacity on the processors than a task server where the worst case is that all tasks are active in parallel. On the other hand, with tasks with a waiting state, the actual sequencing is under control of the controller, and a solution can exist, which proceeds sequentially one task after another. For such tasks, considering a program or scheduler is therefore not useful in the search of control solutions.

Property 3: functionality fulﬁlment
The results vary depending on the environment model:
for a one fault model (Fig 4-a), everything works fine, as capacity is sufficient

on any group of two remaining processors;
for a two faults model (Fig 4-b), capacity of P2 is insufficient to accommo- date for task T 1, therefore no controller can insure functionality whatever the sequence of faults and requests;
for the fault pattern example (Fig 4-c), a solution can be found, as the pathological processor configuration (P2 only survivor) is not considered.
One can note that, would the capacity bound of P2 be a little higher, a so- lution would exist for the two faults model: changing the bounds allow us to obtain different controllability solutions. When no solution is found, the user must relax some of the system’s constraints: either the environment model, or the power consumption bounds. . . When one solution is found, it means that we have a controller that will dynamically allocate the tasks onto the live processors, while guaranteeing that all processor failures will be tolerated and that the cumulative power consumption will always remain smaller than the bound on each processor.

Optimizations
This enables us to further restrain behavior, using values as in Table 1, to maximize quality (and possibly forcing migrations just to achieve this), and then to minimize the power consumption cost. As said in Section 2.2, there may be several solutions with equal weights. The example in Figure 8 shows two remaining configurations, with qualities 7 (left) and 10 (right)
These criteria can be played around with, for the same system under study: minimal consumption can be applied first, before maximizing quality in the remaining solutions.

Implementation
As we mentioned in the beginning, we are using existing synchronous and DCS techniques as such, and hence will not present, in this limited space,

details available elsewhere.
Matou 3 [15] was used for writing the model of

our systems as sets of mode automata, while the symbolic model-checker and DCS tool Sigali 4 [17] was successfully used to automatically synthesize fault- tolerant systems from a high-level specification, and SigalSimu was then used to co-simulate the system and the controller, as illustrated in Figure 9.

3 http://www-verimag.imag.fr/~maraninx/MATOU
4 http://www.irisa.fr/vertecs/Logiciels/sigali.html



Fig. 9. Tools used.

We first considered simple tasks, and consistent execution objectives, and then extended our objectives with functionality fulfillment and optimization. Our method is limited by the technological state of the existing DCS tools, basically the same limitations as with model checking tools. Given the current trend in this domain (symbolic state space exploration, abstract interpreta- tion, widening operators ... ), we believe that future improvements in DCS
tools will make it an efficient solution for industrial size problems.

Conclusion
We have shown how to model a real-time distributed system, its heteroge- neous architecture, and its environment in order to produce automatically a controller enforcing fault-tolerance. It reacts to the occurrences of failures by migrating tasks according to the fault-tolerance policy. For this, we have applied DCS to LTS models of the whole system, with objectives regarding consistent execution, functionality fulfillment, and optimizations.
From the point of view of fault-tolerance, our approach is interesting in the sense that, when the DCS actually succeeds in producing a controller, we obtain a system equipped with a dynamic reconfiguring mechanism to handle failures, with a static guarantee that all specified failures will be tolerated during the execution, and with a known bound on the system’s reaction time.
Interesting perspectives concern:
variants on the model of tasks, for instance having several modes to account for Dynamical Voltage Scaling (DVS), where a slower speed is cheaper in terms of power, or degraded modes for the same functionality,
other logical properties of interest are exclusions between tasks, and sequenc- ing constraints, using observers; other quantitative properties of interest are the use of devices (sensors, co-processors), managing memory use, bounds on migration costs, minimum levels of quality, ...
control in order to optimize the cost on transitions, modeling for instance the cost of migrating the tasks, or cumulated on paths between significant sets of states, like start and end states,
the same system can sometimes be reused in different environments: for


example, an image processing coder/decoder sub-system in a system-on- chip, can be embedded into different devices, e.g., a home DVD player, where power supply is not at all an issue, or a cell phone or cam-recorder, where power is indeed crucial; reusing the same model submitted to differ- ent synthesis objectives opens perspectives in further applications of these techniques, in the framework of platform-based design.

References
K. Altisen, A. Clodic, F. Maraninchi, and E. Rutten. Using controller-synthesis techniques to build property-enforcing layers. In Proceedings of the European Symposium on Programming, ESOP’03, Warsaw, Poland, April 2003.
R. Alur, O. Bournez, T. Dang, O. Maler, and A. Pnueli. Effective synthesis of switching controllers for linear systems. Proc. of the IEEE, 88:1011–1025, 2000.
M. Baleani, A. Ferrari, L. Mangeruca, M. Peri, S. Pezzini, and A. Sangiovanni-Vincentelli. Fault-tolerant platforms for automotive safety-critical applications. In International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES’03, San Jose, USA, November 2003. ACM.
A. Benveniste, P. Caspi, S.A. Edwards, N. Halbwachs, P. Le Guernic, and R. de Simone. The synchronous languages twelve years later. Proc. of the IEEE, 91(1):64–83, January 2003. Special issue on embedded systems.
C. Bernardeschi, A. Fantechi, and L. Simoncini. Formally verifying fault tolerant system designs. The Computer Journal, 43(3), 2000.
G. Bruns and I. Sutherland. Model checking and fault tolerance. In Proceedings 6th International Conference on Algebraic Methodology and Software Technology, AMAST’97, Sidney, Australia, 1997.
A.P. Chandrakasan, S. Sheng, and R.W. Broderson. Low-power CMOS digital design. IEEE Journal of Solid-State Circuits, 27(4):473–484, 1992.
K.-H. Cho and J.-T. Lim. Synthesis of fault-tolerant supervisor for automated manufacturing systems: A case study on photolothographic process. IEEE Trans. on Robotics and Automation, 14(2):348–351, April 1998.
E. Dumitrescu, A. Girault, and E. Rutten. Validating fault-tolerant behaviors of synchronous system specifications by discrete controller synthesis. In IFAC Workshop on Discrete Event Systems, WODES’04, Reims, France, Sept. 2004.
R. Guerraoui and A. Schiper. Consensus service: A modular approach for building agreement protocols in distributed systems. In 26th IEEE Int. Symp. on Fault-Tolerant Computing, FTCS’96, Sendai, Japan, June 1996.
P. Jalote. Fault-Tolerance in Distributed Systems. Prentice Hall, 1994.
R. Jensen. DES controller synthesis and fault tolerant control – a survey of recent advances. Res. report TR-2003-40, ITU, Copenhagen, Denmark, Dec. 2003.
Ch. Kloukinas and S. Yovine. Synthesis of safe, QoS extendible, application specific schedulers for heterogeneous real-time systems. In 5th Euromicro Conference on Real-Time Systems (ECRTS’03), Porto, Portugal, July, 2003.
J.-C. Laprie et al. Dependability: Basic Concepts and Terminology. Dependable Computing and Fault-Tolerant Systems. Springer-Verlag, 1992.


F. Maraninchi and Y. R´emond. Mode-automata: a new domain-specific construct for the development of safe critical systems. Science of Computer Programming, 46(3):219–254, 2003.
H. Marchand, O. Boivineau, and S. Lafortune. Optimal control of discrete event systems under partial observation. In Proc. of the 40th IEEE Conf. on Decision and Control, CDC’01, Orlando, Florida, dec, 2001.
H. Marchand, P. Bournai, M. Le Borgne, and P. Le Guernic. Synthesis of discrete-event controllers based on the Signal environment. Discrete Event Dynamic System: Theory and Applications, 10(4):325–346, October 2000.
H. Marchand and E. Rutten. Managing multi-mode tasks with time cost and quality levels using optimal discrete controller synthesis. In Euromicro Conference on Real-Time Systems, ECRTS’02, Vienna, Austria, June 2002.
P.J. Ramadge and W.M. Wonham. Supervisory control of a class of discrete event processes.
SIAM Journal on Control and Optimization, 25(1):206–230, January 1987.
J. Rushby. Critical system properties: Survey and taxonomy. Reliability Engineering and Systems Safety, 43(2):189–219, 1994.
H. Schepers and J. Hooman. Trace-based compositional proof theory for fault tolerant distributed systems. Theoretical Computer Science, 128, 1994.
