Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 319 (2015) 121–136
www.elsevier.com/locate/entcs
Towards Compositional Graph Theory
Apiwat Chantawibul and Pawe-l Sobocin´ski
ECS, University of Southampton Dedicated to the memory of R.F.C. Walters.

Abstract
Decomposing graphs into simpler graphs is one of the central concerns of graph theory. Investigations have revealed deep concepts such as modular decomposition, tree width or rank width, which measure—in different ways—the structural complexity of a graph’s topology. Courcelle and others have shown that such concepts can be used to obtain efficient algorithms for families of graphs that are amenable to decomposition (e.g. those that have bounded tree-width). These algorithms, in turn, are of course of use in computer science, where graphs are ubiquitous. In this paper we take the first steps towards understanding notions of decomposition in graph theory compositionally, and more generally, in a categorical setting: category theory, after all, is the mathematics of compositionality.
We introduce the concept of ∪−matrices (cup-matrices). Like ordinary matrices, ∪-matrices are the arrows of a PROP: we give a presentation, extending the work of Lafont, and Bonchi, Zanasi and the second author. A variant of ∪-matrices is then used in the development of a novel algebra of simple graphs, the lingua franca of graph theory. The algebra is that of a certain symmetric monoidal theory: ∪-matrices—akin to adjacency matrices—encode the graphs’ topology.
Keywords: Graph decomposition, simple graphs, modular decomposition, (rank) width, cup-matrices

Introduction
When category theorists talk about the category of graphs, they usually mean the presheaf category Graph = Set·⇒·. The objects of this category, however, are not what graph theorists would typically refer to as graphs tout court – rather, as directed multigraphs, because there may be more than one directed edge between any two vertices. The most basic—and important—objects of study in graph theory are simple graphs: these are undirected with at most one edge between any two vertices, and no self loops. In this paper, we are concerned with simple graphs.
A structural metric of a graph is a way of assigning a numerical value, typically a natural number, to a graph. The intention is for the number to indicate, in some way, the graph’s inherent structural complexity. Some well-known structural metrics in graph theory include path-width, tree-width, branch-width, clique-width and rank-width. The notion of tree-width is perhaps the best known in Computer Science through the work of Courcelle [6] who showed that monadic second-order

http://dx.doi.org/10.1016/j.entcs.2015.12.009 1571-0661/© 2015 Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

logic can be decided in linear time for families of graphs with bounded tree-width. Courcelle’s theorem has found several algorithmic applications (see e.g. [13]).
The structural metric rank-width, due to Oum and Seymour, has been a hot topic in graph theory over the last ten years and is of particular relevance for us. We refer to [17, 18] for the technical details, here we give an intuitive description. A rank-decomposition of a simple graph with vertex set V can be considered as a binary tree, where the tree-nodes are labelled with nonempty subsets of V and the tree-edges are labelled with natural numbers, such that:
the labels (vertex sets W1, W2) of the children w1, w2 of any tree-node w are a (binary) partition of the label (vertex set W ) of the parent tree-node: i.e. W = W1 ∪ W2 and W1 ∩ W2 = ∅,
the root is labelled with V ,
the leaves are labelled with singletons,
the tree-edge from a parent to a child labelled with vertex set W is labelled with the rank of the |V \W|× |W| adjacency Z2-matrix that tabulates the edges from W to the remainder of the graph. Note that matrix algebra is performed over the field Z2, i.e. 1 + 1 = 0.
The width of a particular rank-decomposition is its maximum edge label. The rank- width of a graph is then the width of an optimal rank-decomposition: one with the smallest width. Discrete graphs and cliques both enjoy a rank-width of 1.
The concepts of rank-width and other structural metrics have proved to be very important in graph theory and related areas. There are two shortcomings, however, where category theory can help:
Generality. The definition, as stated, is specialised to simple graphs. Yet, the underlying concept is quite robust and can be stated mutatis mutandis for other kinds of graphical structures: multigraphs, directed graphs (bi-rank-width), hy- pergraphs, Petri nets etc. This suggests that the fundamental theory ought to be done in a more general setting.
Compositionality. The notion of a rank-decomposition (and equivalent notions for other structural metrics) is not inherently compositional in the sense that knowing how to decompose a graph G may not help in constructing decom- positions of a graph H that has G as a sub-component. Intuitively speaking, rank-decompositions forget too much: only rank is recorded – the adjacency in- formation ought to be recorded as well, in some form.
What, then, is a fruitful way of treating simple graphs categorically, in a way that will lead us to understanding structural metrics generally and compositionally? Graphs as objects and homomorphisms as arrows is the traditional approach, yet it does not immedietaly yield an algebra of graphs: for that, we need graphs to be the arrows. Cospans are a standard technique for turning objects into arrows, and the algebra of cospans of directed graphs was considered in [9], which is close in spirit to our work, although we do not consider cospans.
Similarly to Fiore and Campos’ work on an algebra of directed acyclic graphs [8],

our approach is to use symmetric monoidal theories, by which we mean presentations of PROPs with generators and equations. The paper culminates with the symmetric monoidal theory ABUV, which we consider as an algebra of simple graphs. The role of matrix algebra is central: we build on Lafont’s [15] and Bonchi, Zanasi and the second author’s [3] work on presentations of the PROP of matrices.
In order to understand the semantic counterpart of ABUV, we introduce the concept of ∪-matrices (“cup” matrices) that generalise matrix algebra and somewhat relax the straightjacket of directionality in ordinary matrices. The main technical contribution in this paper is a presentation of ∪-matrices. We conclude with some remarks on a structural metric, monoidal width, for symmetric monoidal theories.
The use of the algebra of monoidal categories to model systems of various kinds was pioneered by R.F.C. Walters and collaborators [11, 12]. Although we do not concentrate on applications in this paper, we believe that our theory will be relevant in several settings as there has been a recent surge in the applications of symmetric monoidal theories: amongst other works we mention signal flow graphs [1, 4], Petri nets [19–21], asynchronous circuits [10] and quantum circuits [5, 7].
Structure of the paper.
In §2 we recall the presentation of the PROP of matrices. In §3 we introduce
∪-matrices and develop their algebra. In §4 we give a presentation, and in §5 we use the earlier technical developments to introduce an algebra of simple graphs.
Notation and background.
Given an m1 × n1 matrix A and m2 × n2 matrix B, we write A ⊕ B for the
(m + m ) × (n + n ) matrix	A 0	. If A is an m × n matrix, AT denotes its
0 B
transpose, an n ×m matrix, where AT = Aji. A matrix A is upper triangular when
it has only 0 entries below the main diagonal, i.e. Aij =0 if i > j.
A PROP [14, 16] is a symmetric strict monoidal category with natural numbers as objects and with addition as the monoidal product on objects: m ⊕ n = m + n. PROP homomorphisms are identity-on-objects strict symmetric monoidal functors: that is they preserve objects, composition, monoidal products and symmetries on the nose. We reserve the term symmetric monoidal theory for PROPs that are freely generated from a presentation: a pair (Σ, E) where Σ is the set of generators and E is a set of equations (some authors say relations).
Matrices as a Symmetric Monoidal Theory
The results in this section can be found in [3], based on the work of [15].
Fix a ring R (e.g. the integers); this is the source for the entries of our matrices, which we will not explicitly reference during the development in order to reduce the number of subscripts. We use r, s to range over R.
Definition 2.1 The PROP Mat has m×n matrices as arrows n → m; composition is matrix multiplication B ◦ A = BA. The symmetries are permutation matrices.

Definition 2.2 (HA) The symmetric monoidal theory HA is the free PROP on generators {Δ : 1 → 2, ⊥ : 1 → 0, ∇ : 2 → 1, T : 0 → 1}∪ {r : 1 → 1}r∈R and equations listed in Fig. 1. In string diagrams the generators are rendered:
Δ = 	⊥ = 	∇ = 	T = 	r =
Theorem 2.3 ( [3, 15]) HA ∼= Mat.	2

∪-Matrices
Matrices are common in graph theory: for example, they are a convenient tool to record the connectivity information within a graph (adjacency matrices). They enjoy a rich and widely used algebra on which several techniques and results rely.
In our categorical algebra of graphs, introduced in §5, an arrow G : m → n
combines three elements:
a simple graph G on k vertices,
a m × k matrix,
a n × k matrix.
The idea is that the two matrices tell us how the vertices of G connect with other graphs that may be attached, at either end, through composition. Since G can itself be represented as a k × k adjacency matrix, we are dealing with three matrices. We will see that relaxing the “directed” nature of matrices allows us to roll the three matrices into one ∪-matrix (pronounced “cup”-matrix).
Turns
Before introducing ∪-matrices, we first need one important component, the notion of turn – an equivalence class of n×n matrices that is closely connected with the con- cept of adjacency matrix in graph theory: two matrices are turn equivalent exactly when they encode the same connectivity information (in an undirected setting).
Definition 3.1 (Turn equivalence, Turn) We say that two square k × k matri- ces K and L are turn-equivalent when K +KT = L+LT , and write K da L. Clearly da is an equivalence relation. Note that if K da L then K and L have equal size.
By a (k-)turn we mean a da equivalence class of some k × k matrix. We use
M, N to range over turns, and given square K, we denote the induced turn by [K].
In certain cases, a turn has an obvious representative: an upper triangular matrix.
Lemma 3.2 Given a turn [K], there exists an upper triangular matrix Kj such that [K]= [Kj]. Suppose also that ∀r, s ∈ R, we have 2r = 2s implies r = s 1 . If K and L are upper triangular square matrices, and [K]= [L], then K = L.
Proof. The first claim follows easily from the fact that addition in R is an abelian

1 This is the case, for instance, for the integers, or any field of characteristic not equal to 2


Fig. 1. Equations of HA.

group. Next, if [K] = [L] then K + KT = L + LT . Since K and L are upper triangular, they clearly agree in all non diagonal entries. The ith diagonal entry of K +KT = L+LT is 2Kii = 2Lii. Using the additional assumption on R, Kii = Lii.2
The additional assumption on R in Lemma 3.2 excludes the case R = Z2 where we have [(1)] = [(0)] so the second statement does not hold. In fact, the case of Z2 is of particular importance in for our applications, and we will return to it in §5.

∪-Matrices
A m × n ∪-matrix consists of an ordinary m × n matrix together with an n-turn.
Definition 3.3 (∪-matrix) An m × n ∪-matrix U is a pair (A, M ) where A is a m × n matrix and M is an n-turn. We will refer to A as the underlying matrix of U and to M as its turn. We will use U, V to range over ∪-matrices.
It turns out that there is a very natural algebra of ∪-matrices that extends that of ordinary matrices. We develop this algebra below.
Given an m × n ∪-matrix V = (B, [L]) and n × p ∪-matrix U = (A, [K]) their
multiplication, written V U , is defined as the m × p ∪-matrix:
(BA, [K + AT LA])
Lemma 3.4 Multiplication of ∪-matrices is well-deﬁned.
Proof. Suppose that K da Kj and L da Lj. We need K + AT LA da Kj + AT LjA.
K + AT LA + (K + AT LA)T = K + KT + AT LA + AT LT A
= K + KT + AT (L + LT )A
= Kj + KjT + AT (Lj + LjT )A
= Kj + AT LjA + (Kj + AT LjA)T	2
Lemma 3.5 Multiplication of ∪-matrices is associative.
Proof. This is true of the underlying matrices, hence it suffices to check associa- tivity on the turns. Suppose that W = (C, [M ]), V = (B, [L]), U = (A, [K]) are
∪-matrices on which WV and VU are defined. Then their turns are, respectively, [L + BT MB] and [K + AT LA]. The turn of (WV )U is thus the equivalence class of
K + AT (L + BT MB)A = K + AT LA + AT BT MBA
= K + AT LA + (BA)T M (BA)
which is matrix that induces the turn of W (V U ).	2
The n × n identity ∪-matrix is (In, [0n]) where In is an identity matrix and 0n is a zero matrix. We abuse notation by also referring to the identity ∪-matrix as In.
Lemma 3.6 Suppose that U is an m × n ∪-matrix. Then ImU = U In = U.

Proof. Suppose U = (A, [K]). Clearly it suffices to focus on the turns. The turn of ImU [K + AT 0mA]= [K]. The turn of UIn is [0n + IT KIn]= [K].	2
Definition 3.7 (Category of ∪-matrices, UMat) UMat has:
as its set of objects, the set of natural numbers,
and as arrows from n to m, m × n ∪-matrices.

Composition is ∪-matrix multiplication: U ◦ V d=ef
identity ∪-matrices.
UV , with identity arrows the

Given an m1 × n1 ∪-matrix U = (A, [K]) and m2 × n2 ∪-matrix V = (B, [L]), their direct sum, written U ⊕ V , is the (m1 + m2) × (n1 + n2) ∪-matrix
(A ⊕ B, [K ⊕ L]).
This is well-defined—if K Da KJ and L Da LJ then clearly K ⊕ L Da KJ ⊕ LJ. The direct sum yields a strict monoidal product on UMat.
There is an faithful identity-on-objects monoidal functor U : Mat → UMat defined U (A) = (A, [0n]). We have seen that Mat is a PROP (Definition 2.1); in fact, UMat is also a PROP, inheriting its permutations from Mat via U .
Lemma 3.8 UMat is a PROP.
Proof. It suffices to show that symmetries are natural, i.e. the following diagram commutes for any mJ × m ∪-matrix U = (A, [K]) and nJ × n ∪-matrix V = (B, [L]).
m + n  σm,n  n ¸+ m

U⊕V
V ⊕U
  

mJ + nJ σ

m′,n′
 nJ¸+ mJ

On the underlying matrices we clearly have σm′,n′ (A ⊕ B)= (B ⊕ A)σm,n. Notice
that σT = σl,k for any k, l ∈ N. We can use this to show that the turns agree:


T
m,n
(L ⊕ K)σm,n = σn,m(L ⊕ K)σm,n = K ⊕ L	2

Indeed, the functor U : Mat → UMat is a PROP homomorphism.
We end this section with a simple, yet useful way to factorise ∪-matrices. By a pure turn we mean an arrow of type m → 0 in UMat, i.e. a 0 × m ∪-matrix. By a turn-free ∪-matrix we mean an arrow in the image of U : Mat → UMat, i.e., a ∪-matrix of the form (A, [0]). In the following, let Δn denote the turn-free
(n + n) × n ∪-matrix (  In   , [0 ]).
In
Lemma 3.9 Suppose that U is an m × n ∪-matrix. Then
U = (A ⊕ P )Δn
where P is a pure turn and A is turn free.


Fig. 2. Additional equations of HAU.
Proof. Easy calculation: (A, [K]) = ((A, [0]) ⊕ (!, [K]))Δn.	2

∪-Matrices as a Symmetric Monoidal Theory
The goal of this section is to give a presentation of UMat, extending the presenta- tion of Mat recalled in §2. We introduce a symmetric monoidal theory, HAU, and prove that HAU ∼= UMat as PROPs.
Definition 4.1 (HAU) The symmetric monoidal theory HAU is the free PROP on generators {Δ : 1 → 2, ⊥ : 1 → 0, ∇ : 2 → 1, T : 0 → 1, ∪ : 2 → 0}∪ {r : 1 → 1}r∈R and the set of equations listed in Fig. 1 together with the additional equations concerning ∪ (pronounced “cup”) listed in Fig. 2. In string diagrams, ∪ is drawn

 .

Our main goal for this section is to prove the following.
Theorem 4.2 Suppose that R satisﬁes the condition 2r = 2s implies r = s, for all
r, s ∈ R. Then HAU ∼= UMat.
The remainder of this section consists of a proof of the above result; we thus assume that R satisfies the required condition for the remainder of this section.
We start by defining a PROP morphism Θ : HAU → UMat recursively.
Δ '−→   1  , [(0)]	∇ '−→   1 1  ,   0 0	∪ '−→  !,  0 1 

⊥ '−→ (!, [(0)]) T '−→ (¡, [()])
Note that we are being somewhat sloppy with notation since we denote by ! all unique arrows to 0 and by ¡ all unique arrows from 0 in Mat. Since we are defining a symmetric monoidal functor, the recursion is forced, e.g.:
Θ(q ◦ p)= Θ(q) ◦ Θ(p)	Θ(p ⊕ q)= Θ(p) ⊕ Θ(q)

This procedure is well-defined: since both HAU and UMat are symmetric monoidal it suffices to show that all equations in HAU hold also in UMat. This is the case for HA and Mat (as shown in [3]) – it suffices to check the equations involving ∪.
Lemma 4.3
(Θ∪)(Θ∇⊕ id)= (Θ∪)(id ⊕ Θ∪⊕ id)(id2 ⊕ ΘΔ)
(Θ∪)(id ⊕ ΘT)= Θ⊥
(Θ∪)(Θtw)= Θ∪
(Θ∪)Θ(id ⊕ r)= (Θ∪)Θ(r ⊕ id).
Proof. Straightforward calculations.	2
Lemma 4.4 The following diagram commutes.

HA
Φ
  
Mat	U
 HA¸U
Θ
  
 U¸Mat

(1)

Proof. Since the diagram consists of PROP homomorphisms, it suffices to verify that it is the case for the generators of HA.	2
The above observation allows us to conclude that the additional axioms of HAU
are conservative wrt the equations in HA.
Lemma 4.5 Suppose t, u ∈ HA. Then t =HA u iff t =HAU u.
Proof. The ‘only if’ direction is obvious. For the ‘if’, observe that in the dia- gram (1), Φ is an isomorphism and U is faithful, so their composition is faithful. Since the diagram commutes, the homomorphism HA → HAU is faithful.	2
Since Θ is identity on objects, to show it is an isomorphism it suffices to show that it is full and faithful.
Proposition 4.6 Θ is full.
Proof. Suppose that U = (A, [K]) is an m × n ∪-matrix. We must construct a term tU of HAU such that Θ(tU ) = U . Using the factorisation of Lemma 3.9, we have U = ((A, [0n]) ⊕ (!, [K]))Δn. We can break up the problem into two parts: construct tA : n → m such that Θ(tA) = (A, [0n]) and tK : n → n such that Θ(tK)= (idn, [K]). Since HA ∼= Mat, it is straightforward to obtain tA.
We are left with (!, [K]), where w.l.o.g. we assume that K is upper triangular. The problem becomes: given an upper triangular n × n K, construct a term of tK such that Θ(tK)= (idn, [K]). We can do this by constructing two terms in HAU:
given p < q ≤ n, a term incp,q : n → n, satisfying, for any (B, [L])


Θ(inc

)(B, [L]) = (B, [Lj]) where Lj
=  Lij +1	if i = p and j = q

given p ≤ n, inci : n → n, satisfying,

Θ(inc )(B, [L]) = (B, [Lj]) where Lj


=  Lij +1	if i = j = p


For sake of simplicity, we only give the appropriate string diagrams below and let the reader verify that Θ(incp, q) and Θ(incp) work as advertised:

.



p	.
p


q
.	.
	

incp,q	incp
Then, clearly, tK can be constructed by an appropriate composition of these two families of terms, finally composing with ⊥n to obtain tK.	2
Proving that Θ is faithful is slightly more involved. We start by showing how turns “add” in HAU; the proofs are simple exercises in diagrammatic reasoning.
Lemma 4.7
=
Lemma 4.8
= 	

The crux of the proof is a certain “normal form” for representations of pure turns in HAU. Basically, one can write any such term without using ∇ or T.
Lemma 4.9 Suppose that t : k → 0 in HAU. Then t can be written t∪ ◦ tΔ where
tΔ uses only generators Δ, ⊥, and t∪ only ∪.
Proof. We use a rewriting argument: start with a string diagrammatic represen-

tation of t. Next, we can write t = t1 ◦ t1	where t1
does not use the ∪ generator

∪	HA	HA

and t1
only the ∪ generator. Now t1	can be put in matrix form (see [3]): a com-

position t1
t1
where all comonoid structure comes before the monoid structure.

The following are the rewrite rules that we will apply:

⇒	⇒


⇒ 	⇒

at each point obtaining a step tk ◦ tk	⇒ tk+1 ◦ tj	⇒ tk+1 ◦ tk+1 where tk+1 results
∪	HA	∪	HA	HA	∪	HA

from tj
by putting it in matrix form.

The only tricky issue is proving termination: we can show it using the concept of monoid depth. Given a term tHA : k → l, we let the monoid depth of right port 0 ≤ i < l of md(tHA, i) be the number of non-zero entries in the ith row of the matrix that corresponds to tHA. The isomorphism HA ∼= Mat guarantees that this definition makes sense. The goal is to reach a term where all ports have monoid depth 1, i.e. one whose matrix has exactly one non-zero entry in each row.
Now, given two ports with monoid depth k > 1,l connected by a turn, each of the first two rewrite rules decrements the monoid depth of the k port, and adds another port of depth l. The last two rules simply remove a port of multiplication depth 0 (in other words, remove a zero row from the matrix). By always rewriting at a port with the highest monoid depth, and cancelling any ports of depth 0, clearly one eventually arrives at a term with all ports of monoid depth 1.	2
Lemma 4.10 (Turn matrix form) Any t : k → 0 can be written in the form

t = Υk ◦ ( 
ri,j) ◦ ⎛⎝ 
Δk+1⎞⎠


	
where ri,j =1 whenever j = k +1 or i > j and Υ: k(k + 1) → 0 is constructed by connecting, for each 1 ≤ i ≤ k:
the (i − 1)(k + 1)+ ith port and ((i − 1)(k + 1)+ k + 1)th port, and
for each j > i, the (i − 1)(k + 1)+ jth port with the (j − 1)(k + 1)+ ith port
with cups. A recursive deﬁnition can be given, but it is unenlightening: the diagram below does a better job of illustrating the structure of Υk.



k +1 {


k +1 {



k +1 { .

Let K be the upper triangular matrix with j, ith entry rij when i ≤ j and 0 otherwise. Then Θ(t)= (!, [K]).
Proof. Using the procedure of Lemma 4.9 we can write any term k → 0 in the form
t∪ ◦ tΔ, then add up the coefficients using the equations of Lemmas 4.8 and 4.7. 2



Example 4.11 The turn matrix form of	is calculated below:




⇒	=	⇒2


where we did not draw the 0-coefficient turns. It is easily checked that this gives the upper triangular matrix
0 0 1 1 

0 0 0 0 
0 0 0 0 
Proposition 4.12 Θ is faithful.
Proof. Suppose that t, u : n → m such that Θ(t)= Θ(u). Using the laws of HA we can write t = (tHA ⊕ tj) ◦ Δn and u = (uHA ⊕ uj) ◦ Δn, where tHA, uHA : n → m and tj, uj : n → 0.
Now Θ(Δn) is mono, thus Θ(t)= Θ(u) implies
Θ(tHA ⊕ tj)= Θ(uHA ⊕ uj).
By definition of direct sum in UMat, we have
Θ(tHA)= Θ(uHA)	Θ(tj)= Θ(uj)
Since HA ∼= Mat and U : Mat → UMat is faithful, we have tHA = uHA. It remains to show that Θ(tj) = Θ(uj) implies that tj = uj. Transform tj and uj into turn matrix form. Since (!, [K]) = Θ(tj) = Θ(uj) = (!, [L]), we must have [K] = [L]. Since K and L are upper triangular, the conclusion of Lemma 3.2 yields K = L. Thus tj = uj.	2
We have shown that Θ is full in Proposition 4.6, and that it is faithful in Proposi- tion 4. 12. This completes the list of ingredients needed for the proof of Theorem 4.2.
A compositional theory of simple graphs: ABUV
In this section we focus on the case R = Z2. We give a presentation for UMatZ2 , and show how one can use this to obtain an algebra of simple graphs.
∪-Matrices over Z2
Definition 5.1 (AB and ABU) The symmetric monoidal theory AB is the free PROP on generators {Δ : 1 → 2, ⊥ : 1 → 0, ∇ : 2 → 1, T : 0 → 1}, and



Fig. 3. Equations particular to ABU and ABUV.

equations (A1)-(A10) listed in Fig. 1 and (C1) in Fig. 3.
The symmetric monoidal theory ABU is the free PROP on generators {Δ:1 → 2, ⊥ : 1 → 0, ∇ : 2 → 1, T : 0 → 1, ∪ : 2 → 0}, the equations of AB, equations (B1)-(B3) of Fig. 2 and (C2) in Fig. 3.
First recall [2, 15] that AB ∼= MatZ2 . Likewise, presenting UMatZ2 is simpler than the general case: there is no need for additional scalars, and the equation 1 + 1 = 0 is captured by the “anti-separable” law – equation (C1) in Fig. 3 [2]. The equation (C2) may seem surprising, since it does not hold in the general case. Indeed, the requirement of “no self loops” in simple graphs is already taken care of
by the algebra of ∪-matrices with Z2 entries.
Lemma 5.2 Θ(∪◦ Δ) = Θ(T)
Proof. For Z2-matrices we have (0) da (1), so [(0)] = [(1)].	2
Theorem 5.3 ABU ∼= UMatZ2 .
Proof. Proceeds along the lines of the proof of Theorem 4.2: the only difference is that instead of using upper triangular matrices as representatives of turns, we use upper triangular matrices with 0 diagonals. We omit the details.	2
An algebra of simple graphs
We will use the theory ABU(∼= UMatZ2 ) as an adjacency algebra of simple graphs. The only thing missing is the vertices.
Definition 5.4 (ABUV) The symmetric monoidal theory ABUV is the free PROP on generators {Δ:1 → 2, ⊥ :1 → 0, ∇ :2 → 1, T :0 → 1, ∪ :2 → 0,v :0 → 1}, and the equations of ABU. In string diagrams, we will draw v :0 → 1 as follows:


Since there are no additional equations involving v, ABUV is the coproduct (in the category of PROPs) of ABU and the free PROP on generator v and no equations.
Definition 5.5 Let CGraph be the 2-PROP with arrows n → m pairs (k, U ) where
k ∈ N and
U is an m × (n + k) ∪-matrix.

Composition (V, l) ◦ (U, k) is
(k + l, V (U1 ⊕ Ik))
For the 2-cells, given a bijection σ : k → k, let Iσ denote the permutation matrix induced by σ. Then σ : (U, k) ⇒ (Uj, k): n → m is a 2-cell if
Uj(In ⊕ Iσ)= U.

Note that all 2-cells are invertible. Let CGraph~= denote the category obtained by identifying isomorphic 1-cells in CGraph.
Theorem 5.6 ABUV ∼= CGraph~=.
Proof. Omitted.	2
Corollary 5.7 There is a 1-1 correspondence between ABUV[0, 0] and (isomor- phism classes of) simple graphs.
Proof. A simple graph G on k vertices can be identified with an upper-triangular k × k Z2 matrix MG with 0s on the diagonal—its adjacency matrix. Two simple graphs G, Gj are isomorphic precisely when there exists bijection σ : k → kj s.t. MG′ = IT MGIσ. The statement thus follows directly from Theorem 5.6.	2
We conclude the paper with an illustrative, yet simple example of how the algebra of ABUV can be used to decompose graphs. It is well-known that cliques have unbounded tree-width, but are very simple from the point of view of rank- width: their rank-width is 1. This is easy to observe: suppose that {V1, V2} is any partition of the vertex set V of a clique. Then the |V2|× |V1| adjacency matrix that describes how vertices of V1 connect to the vertices of V2 is simply the matrix with 1 in each entry, and this matrix has rank 1.
Cliques also enjoy a very simple description in ABUV, as we show in the following.

Example 5.8 [Clique] The clique on n vertices can be constructed as follows
⊥◦ cn ◦T 
where c is the following term:


.

Here we do not define “simplicity”, which is measured with a structural metric called monoidal width; it will be introduced in a sequel to this paper. The ba- sic idea is the following: a monoidal decomposition is simply a binary tree with internal nodes labelled with ◦ or ⊕, and leaves generators, identities and twists. The width associated to the decomposition tied to the largest object along which a

composition is ever performed when evaluating the tree as a term of the ambient symmetric monoidal theory. Monoidal width of a term is then the width of an optimal decomposition, i.e. one that has the smallest width.
As we shall show in the sequel, monoidal width in ABUV is closely related to rank-width, and concepts such as tree-width can be characterised in a similarly natural way by restricting to sub-PROPs of ABUV. Note also that the definition of monoidal width is not tied to ABUV and makes sense in any monoidal theory, which solves the problem of generality outlined in the Introduction. Our claims are that
(i) ABUV is a canonical, compositional algebra of simple graphs, and (ii) monoidal width is a robust concept that we believe will be useful in a number of different applications areas [1, 4, 7, 10, 19] where monoidal theories are used.

References
F. Bonchi, P. Sobocin´ski, and F. Zanasi. A categorical semantics of signal flow graphs. In CONCUR‘14, 2014.
F. Bonchi, P. Sobocin´ski, and F. Zanasi. Interacting bialgebras are Frobenius. In FoSSaCS ‘14, volume 8412 of LNCS, pages 351–365. Springer, 2014.
F. Bonchi, P. Sobocin´ski, and F. Zanasi. Interacting Hopf algebras. submitted, available at http:
// arxiv. org/ abs/ 1403. 7048 , 2014.
F. Bonchi, P. Sobocin´ski, and F. Zanasi. Full abstraction for signal flow graphs. In Principles of Programming Languages, POPL‘15., 2015.
B. Coecke and R. Duncan. Interacting quantum observables. In ICALP‘08, pages 298–310, 2008.
B. Courcelle and J. Engelfriet. Graph Structure and Monadic Second-Order Logic - A Language- Theoretic Approach. Number 138 in Encyclopedia of mathematics and its applications. CUP, 2012.
R. Duncan. A graphical approach to measurement-based quantum computing. In Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse, chapter 3. OUP, 2013.
M. P. Fiore and M. D. Campos. The algebra of directed acyclic graphs. In Computation, Logic, Games, and Quantum Foundations. The Many Facets of Samson Abramsky, volume 7860 of LNCS, 2013.
F. Gadducci and R. Heckel. An inductive view of graph transformation. In Recent Trends in Algebrac Development Techniques, volume 1376 of LNCS. Springer, 1998.
D. R. Ghica. Diagrammatic reasoning for delay-insensitive asynchronous circuits. In Computation, Logic, Games, and Quantum Foundations. The Many Facets of Samson Abramsky, pages 52–68. Springer, 2013.
P. Katis, N. Sabadini, and R. F. C. Walters. Bicategories of processes. J. Pure Appl. Algebra, 115:141– 178, 1997.
P. Katis, N. Sabadini, and R. F. C. Walters. Span(Graph): an algebra of transition systems. In Algebraic Methodology and Software Technology (AMAST ’97), volume 1349 of LNCS, pages 322–336. Springer, 1997.
S. La Torre and G. Parlato. Scope-bounded multistack pushdown systems: Fixed-point, sequentialization, and tree-width. In FSTTCS 2012, 2012.
S. Lack. Composing PROPs. Theor App Categories, 13(9):147–163, 2004.
Y. Lafont. Towards an algebraic theory of boolean circuits. J Pure Appl Alg, 184:257–310, 2003.
S. Mac Lane. Categorical algebra. Bull Amer Math Soc, 71:40–106, 1965.
S.-I. Oum. Graphs of bounded rank-width. PhD thesis, Princeton, 2005.
S.-I. Oum and P. Seymour. Approximating clique-width and branch-width. Journal of Combinatorial Theory, Series B, 96(4):514 – 528, 2006.


J. Rathke, P. Sobocin´ski, and O. Stephens. Compositional reachability in petri nets. In Reachability Problems, volume 8762 of LNCS, pages 230–243. Springer, 2014.
P. Sobocin´ski. Representations of Petri net interactions. In Concurrency Theory (CONCUR ‘10), number 6269 in LNCS, pages 554–568. Springer, 2010.
P. Sobocin´ski. Nets, relations and linking diagrams. In Algebra and Coalgebra in Computer Science (CALCO ‘13), volume 8089 of LNCS, pages 282–298, 2013.
