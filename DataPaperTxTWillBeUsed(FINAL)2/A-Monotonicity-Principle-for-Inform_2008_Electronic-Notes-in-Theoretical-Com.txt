

Electronic Notes in Theoretical Computer Science 218 (2008) 111–129
www.elsevier.com/locate/entcs

A Monotonicity Principle for Information Theory
Konstantinos Chatzikokolakis1
Oxford Computing Lab and LIX E´cole Polytechnique
Keye Martin2
Naval Research Laboratory

Abstract
We establish a monotonicity principle for convex functions that enables high-level reasoning about capacity in information theory. Despite its simplicity, this single idea is remarkably applicable. It leads to a sig- nificant extension of algebraic information theory, a solution of the capacity reduction problem, intuitive graphical methods for comparing channels, new inequalities that provide useful estimates on the informa- tion transmitting capability of a channel operating in an unknown environment, further explication of the fascinating relationship between capacity and Euclidean distance, and the solution of an open problem in quantum steganography.
Keywords: binary channels, capacity, euclidean distance, information theory, quantum steganography


Introduction
A notion of particular interest in the field of information theory is that of a channel. A channel is a device that transforms symbols of an input alphabet  to symbols of an output alphabet  in a probabilistic way: when x	is the input, p(y x) gives the probability of getting y	in the output. For such channels, capacity is a measure of the maximum correlation between the input and the output. It is 0 when they are independent (all inputs produce the same output with the same probability) and takes its maximum value when there are no transmission errors (each output is produced by exactly one input). The importance of capacity rises from Shannon’s theorem: capacity gives the maximum achievable rate at which

1 kostas@lix.polytechnique.fr
2 kmartin@itd.nrl.navy.mil

1571-0661 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.10.008

we can transmit information using the channel, with arbitrarily low probability of error.
Apart from their use in information theory, the notions of channel and capacity have been proven quite useful in the area of security. It has been shown that in many scenarios, systems or protocols can be fruitfully viewed as channels, and the capacity of these channels can be regarded as a measure of the security guarantees of the system. Techniques from information theory have been applied to a broad range of security fields, including those of information flow ([12,4]), quantum cryptography ([15]), anonymity ([13,3]) and trust ([14]).
However, an important drawback of capacity is its complexity. Despite its sim- ple definition, there is no analytical formula that gives the capacity of a discrete channel in the general case. It can be only computed approximately using numerical algorithms such as the Arimoto-Blahut algorithm ([5]). And even in simple cases where an analytical formula does exist, for example in the case of binary channels having only two inputs and outputs, it is complicated and difficult to use in practice. For example, in many problems we need to be able to predict how a channel will perform, but find that its noise matrix varies with several parameters that depend on random aspects of the environment which arise during the transmission. In such cases we cannot compute the capacity of the channel, but we would still like to obtain bounds on it or compare the performance of different classes of channels. The resulting formulae, however, make this goal very challenging.
It is thus natural to seek tools that allow high-level reasoning about capacity. Developed in a recent line of work, algebraic information theory ([11]) offers such tools for binary channels. In that work, studying the relation of order, algebra and topology, a domain of binary channels is considered and it is shown that capacity is Scott-continuous, providing a tool to compare channels. Moreover, in [9] it is shown that capacity is a measurement on this domain.
In this paper we exploit convexity, a property of capacity that has been in general underused in the literature, but which turns out to be a fundamental property on which a lot of results can be based. Convexity provides us with a monotonicity principle that we use to give simpler and more general proofs of results from [11,9], as well as numerous news ones, outlined in the next section.

Contribution
We establish a monotonicity principle for convex functions: a convex function decreases on a line segment iff it assumes its minimum value at the end of that line segment. Though quite simple, this single idea has an unusual number of important consequences for information theory and the areas which benefit from it (e.g. information hiding, security, quantum information).
The first of these it that it offers a significant extension of algebraic information theory: a new partial order is introduced on binary channels with respect to which capacity is monotone. This new order is much larger than the interval order consid- ered in [11], and can be characterized in at least three different ways, each of which has its own value: by means of a simple formula, which makes it easy to apply in

practice; geometrically, which makes it easy to understand and reason about; and algebraically, which establishes its canonical nature, mathematically speaking.
These results provide graphical methods for reasoning about the capacity of channels, which are of value to practitioners in information theory and security. The mathematics of information theory often prevent reviewers of high assurance devices from using it. The graphical methods we introduce avoid this problem. There is a ‘geometry of binary channels’, in which, roughly speaking, a line of channels either hits the diagonal, or is parallel to it. We determine the behavior of capacity in both these cases, which allows one to answer most (but not all) questions when it comes to comparing channel behavior.
Another use of the monotonicity principle is in establishing inequalities relat- ing different measurements on the domain of channels. These inequalities have several uses. As already explained, the channel matrix often depends on external parameters. While determining these parameters precisely in advance is usually not possible, one can surprisingly often obtain useful bounds on them. Thus, we need methods for estimating the capacity of a channel given only partial information about its noise matrix. Specifically, methods that provide estimates of capacity from estimates of a channel’s noise matrix which themselves are derived from es- timates of underlying experimental parameters. Our inequalities provide exactly these kinds of methods.
As a case in point, we derive a lower bound on the capacity of a hidden channel within quantum key distribution in the presence of noise. Previously, a bound was known ([8]) only for the case where the noise is due solely to eavesedropping, in which the noise matrix has a simple symmetric form. We establish a fundamental theoretical limit of 1 H(1/4) 0.18 for the case of arbitrary noise caused by any combination of eavesdropping and/or environment. Our results also make it clear that there is a best way to interrupt this hidden communication, and they even tell us what this way is.
Note that, even though most of these results are limited to binary channels, the monotonicity principle itself holds in general.
The proofs of all results can be found in the report version of this paper ([2]).





Channels

Nearly all the results in this paper concern binary channels, so we devote the ma- jority of this section to discussing their specifics. A binary channel has two inputs (“0” and “1”) and two outputs (“0” and “1”). An input is sent through the channel to a receiver. Because of noise in the channel, what arrives may not necessarily be what the sender intended. The effect of noise on input data is modeled by a noise matrix u. If data is sent through the channel according to the distribution x, then

the output is distributed as y = x · u. The noise matrix u is given by
u = ⎛a a¯⎞
⎝b ¯b⎠

where a = P (0|0) is the probability of receiving 0 when 0 is sent and b = P (0|1) is the probability of receiving 0 when 1 is sent and x¯ := 1 − x for x ∈ [0, 1].
Thus, the noise matrix of a binary channel can be represented by a point (a, b) in the unit square [0, 1]2 and all points in the unit square represent the noise matrix of some binary channel.
The composition of two binary channels x and y is the channel whose noise matrix is the usual product of matrices x y = xy. The multiplication of two noise matrices x = (a, b) and y = (c, d) in the unit square representation is
(a, b) · (c, d) = ( a(c − d)+ d, b(c − d)+ d ) = c(a, b)+ d(a¯, ¯b)

where the expression to the right uses scalar multiplication and addition of vectors. By contrast, the representation for the sum of noise matrices is simply the sum of each representing vector. 3
A monoid is a set with an associative binary operation that has an identity. The set of binary channels is a monoid under the operation of multiplication whose iden- tity is the noiseless channel 1 := (1, 0). A binary channel can be classified according to the sign of its determinant, det(a, b) = a − b, which defines a homomorphism det : ([0, 1]2, ·) → ([−1, 1], ·) between monoids.
Definition 2.1 A binary channel x is called positive when det(x) > 0, negative when det(x) < 0 and a zero channel when det(x) = 0. A channel is nonnegative if it is either positive or zero.
Notice that det(x) (0, 1] for positive channels, and that det(x) [ 1, 0) for negative channels. Thus, the set of positive channels is a submonoid of [0, 1]2 as is the set of nonnegative channels; the determinant is a homomorphism from the nonnegative channels into ([0, 1], ·).
Notation 2.2 The set of nonnegative binary channels is denoted N. The set of positive binary channels is denoted P.
A nice property of positive channels is that composition can be inverted (even though the “inverse” of a channel is not a channel).
Lemma 2.3 For a ∈ P, x, y ∈ N we have ax = ay iff x = y iff xa = ya.
The amount of information that may be sent through a channel (a, b) is given

3 More specifically we mean the convex sum tM1 + t¯M2,t	[0, 1] of noise matrices M1, M2 which itself is a noise matrix

by its capacity:



(	) = log




 2 a¯H(b)−¯bH(a)




bH(a)−aH(b) 



where H(x) =	x log2(x)	(1	x) log2(1	x) is the base two entropy. Capacity is continuous on the unit square ([9]).

The general case: (m, n) channels.
In the general case, we have m inputs and n outputs, so the noise matrix u has m rows and n columns, each entry is a probability, and as before, each row sums to one. Noise matrices for (m, n) channels are closed under finite convex sums, but are only closed under composition when the associated matrix multiplication is defined.
If we write the noise matrix u of a channel as a list of rows u = (u1,..., um) and p is a distribution over the channel’s inputs, then the mutual information between the input and output of channel is

m
Ip(u) = H(p · u) −	piH(ui).
i=1

The capacity of the channel u is then c(u) = supp Ip(u).

The monotonicity principle

We will denote 1 − t by t. A subset S of a vector space is convex iff tx1 + tx2 ∈ S
for all x1, x2 ∈ S, t ∈ [0, 1]. A function f : S → R is convex iff

tf (x1)+ tf (x2) ≥ f (tx1 + tx2)	∀x1, x2 ∈ S, ∀t ∈ [0, 1]
A function f is strictly convex if the equality (in the above inequality) holds iff x1 = x2 or t  0, 1 . We now come to the monotonicity principle: a convex function decreases along a line segment iff it assumes its minimum value at the end of that line segment.
Theorem 3.1 If S is a set of vectors, x, y  S, π(t) = ty + t¯x is the line from x to y and c : S  R is a function (strictly) convex on π[0, 1], then the following are equivalent:
The function c ◦ π : [0, 1] → R is (strictly) monotone decreasing,
The minimum value of c ◦ π on [0, 1] is c(π(1)) = c(y).
It is by no means obvious that the monotonicity principle is of any value in problem solving. However, as we will see shortly, there are many situations in information theory where it is far easier to establish a minimum value along a line than it is to establish monotonicity itself.

It is well-known ([5]) that mutual information Ip is a convex function of u for a fixed p. An important consequence of this, first observed by Shannon ([16]), though not particularly well-known, is that capacity itself is convex:
Theorem 3.2 Capacity c(u) is a convex function of u.
In the case of binary channels we make the previous result more precise by showing that the capacity is strictly convex everywhere, except on the zero channels.

Theorem 3.3 The capacity on binary channels is strictly convex everywhere except on the zero channels. That is, given u1, u2 [0, 1]2, u1 = u2 and t  (0, 1), we have c(tu1 + t¯u2)  tc(u1) + t¯c(u2), with equality if and only if both u1, u2 are zero channels.
Because Theorem 3.1 can be applied to any line that ends on a minimum capacity channel, it provides a powerful technique for comparing the capacity of channels. One immediate application of it is that we can solve the capacity reduction problem for arbitrary (m, n) channels. In the capacity reduction problem, we have an (m, n) channel x and would like to systematically obtain a channel whose capacity is smaller by some specified amount. The monotonicity principle offers a solution:
Proposition 3.4 Let x be any (m, n) channel, y be any (m, n) channel with zero capacity and π denote the line from x to y. Then c(π[0, 1]) = [0, c(x)] and the function c ◦ π is monotone decreasing.
Thus, given any 0 < r < c(x), we need only solve the equation c(π(t)) = r for t. This equation can be solved numerically since c π r changes sign on [0, 1]. Notice that this enables us to systematically solve a problem that otherwise would have m(n  1) unknowns but only a single equation. Moreover, the channel obtained is a linear degradation of the original. Similarly, we can systematically increase the capacity using the line from x to a maximum capacity channel. We now turn to another use of the monotonicity principle.
Relations between channels
In this section, we consider partial orders on binary channels with respect to which capacity is monotone. Their importance stems from the fact that a statement like “x  y” is much easier to verify than a statement like “c(x)  c(y)”. In situations where the noise matrix of a channel is only partially specified, by means of bounds on experimental parameters for instance, or where it is known but varies with a parameter like time or the probability of losing a photon [9], their usefulness is especially apparent.
Algebraic information theory
Algebraic information theory uses the interplay of order, algebra and topology to study communication.

Definition 4.1 The interval domain is the set of nonnegative binary channels (N, ±) together with the relation ± defined by x ± y iff b ≤ d and c ≤ a, for x = (a, b),y = (c, d) ∈ N. The natural measurement μ : N → [0, 1]∗ is given by
μx = det(x) = a − b	x ∈ N

This is not the usual notation in domain theory for the interval domain, but experience has taught us that this is the simplest way of handling things in the context of information theory. The following result is proven in [11]:
Theorem 4.2 Let (N, ) denote the monoid of nonnegative channels. The right zero elements of N are precisely the zero channels. The maximally commutative submonoids of N are precisely the lines which join the identity to a zero channel. For any maximally commutative submonoid π ⊆ N:
(∀x, y ∈ π) x ± y ⇔ μx ≥ μy ⇔ cx ≥ cy
Capacity on N is monotone: x ± y ⇒ c(x) ≥ c(y).
We will now see that the monotonicity principle offers a new order on channels that leads to a clear and significant extension of algebraic information theory.

A new partial order on binary channels
By the monotonicity principle, capacity decreases along any line that ends on a zero capacity channel. This suggests a new way of ordering positive channels:
Definition 4.3 Let x = (a, b) and y = (c, d) ∈ P,
x ≤ y ≡ c · μx ≥ a · μy	and	c¯ · μx ≥ a¯ · μy
Recall that a partial order on a set is a relation which is reflexive, transitive and antisymmetric.
Proposition 4.4
The relation ≤ is a partial order on the set P of positive channels,
For x, y ∈ P, if x ± y, then x ≤ y. In particular, the least element of (P, ≤) is the identity channel ⊥ = (1, 0),
For x, y ∈ P, we have x ≤ y iff there is a line segment that begins at x, passes through y and ends at some point of {(t, t) : t ∈ [0, 1]},
Capacity c : P	[0, 1]∗ is strictly monotone: if x	y, then c(x)	c(y) with equality iff x = y.
Notice that the monotonicity of capacity on (N,  ), given in Theorem 4.2, is now a trivial consequence of (ii) and (iv) in Proposition 4.4, showing also that capacity is strictly monotone wrt ±.



Fig. 1. Geometric representation of ±, ≤.

The coincidence of algebra, order and geometry
Each order is given by a simple formula (Definitions 4.1 and 4.3) that is easy to verify in practice. Each also has a clear geometric significance which makes it easy to reason about: for x = (a, b) ∈ P and y ∈ P,
x ± y iff y is contained in the triangle with vertices {(a, a), x, (b, b)} iff there is a line segment from x to a point of {(t, t) : t ∈ [b, a]} that passes through y.
x ≤ y iff y is contained in the triangle with vertices {(0, 0), x, (1, 1)} iff there is a line segment from x to a point of {(t, t) : t ∈ [0, 1]} that passes through y.
A geometric interpretation of these orders is shown in Figure 1. Remarkably, each of these orders can also be characterized algebraically:
Lemma 4.5 For x, y ∈ P,
x ± y iff (∃z ∈ P) zx = y,
x ≤ y iff (∃z ∈ P) xz = y.
Thus, despite the somewhat awkward formulation of ≤ given in Definition 4.3, we see that ≤ is nevertheless quite natural. In fact, from the point of view of information theory, it is more natural than ±:
Theorem 4.6 Let (P, ·, 1) denote the monoid of positive binary channels.
The relation x ≤ y ≡ (∃z ∈ P) xz = y deﬁnes a partial order on P with respect to which capacity c : P → [0, 1]∗ is strictly monotone,
For all a, x ∈ P there exists y ∈ P s.t. xa = ay,
The operator lx : P → P :: lx(y) = xy is monotone with respect to ≤,
The operator rx : P → P :: rx(y) = yx is monotone with respect to ≤.
By contrast, rx is monotone with respect to , but lx is not. The reason for this difference is that P x  x P holds for all x P, and this inclusion is strict. So even though P is not commutative, it has a special property commutative monoids have which ensures that both lx and rx are monotone with respect to . The monotonicity of lx and rx implies that

(∀ a, b, x, y ∈ P) x ≤ y ⇒ c(axb) ≥ c(ayb)

with equality iff x = y since axb  ayb and c(axb) = c(ayb) implies axb = ayb which from Lemma 2.3 implies that x = y. The above inequality, in turn, has an important and new consequence for information theory:
Corollary 4.7 For all a, b, x, y ∈ P,
c(axyb) ≤ min{c(axb), c(ayb)}
with equality iff x = 1 or y = 1.
In particular, for a = b = 1, the well-known inequality c(xy)	min c(x), c(y) follows. It is interesting indeed that it may be derived from an order which itself may be derived from algebraic structure. This illustrates the value of knowing about the coincidence of algebra, order and geometry.
Relations between monotone mappings on channels
Having just considered relations between binary channels, we now turn to rela- tions between monotone mappings on binary channels. Of particular interest is the fascinating relationship between capacity and Euclidean distance.

Algebraic relations
Both capacity and Euclidean distance are invariant under multiplication by the idempotent e = (0, 1):
Lemma 5.1 Let e := (0, 1). For any x = (a, b) ∈ [0, 1]2,
e · (a, b) = (b, a)	&	(a, b) · e = (a¯, ¯b)
c(ex) = c(xe) = c(x)
| det(ex)| = | det(xe)| = | det(x)|
We now establish our first result which relates capacity to distance:
Theorem 5.2 For two binary channels x, y ∈ [0, 1]2,
c(xy) ≤ min{ c(x)|det(y)|, |det(x)|c(y) }.
with equality iff x (or y) is 1, e or a zero channel.
The last result extends to any convex function on N. It gives a new proof of a well-known result in information theory,
Corollary 5.3 For x, y	[0, 1]2, c(xy)	min c(x), c(y) with equality iff x (or y) is 1, e or a zero channel.
and also sheds light on the relation between Euclidean distance and capacity:
Corollary 5.4 For a binary channel x	[0, 1]2, c(x)	det(x) with equality iff x
is 1, e or a zero channel.

Intuitively, the Euclidean distance det is a canonical upper bound on capacity. Our goal now is to prove this. First, det is determined by its value on the set N of nonnegative channels. Next, as a function on N, it preserves multiplication, convex sum and identity. There are only two functions like this in existence:
Theorem 5.5 If f : N → [0, 1] is a function such that
f (1) = 1
f (xy) = f (x)f (y)
f (px + p¯y) = pf (x)+ p¯f (y)
then either f ≡ 1 or f = det.
Thus, there is only one nontrivial convex-linear homomorphism above capacity: the determinant. This raises the question of how close in value the two are.

Inequalities
In the formulation of  given in Definition 4.3, the case μx = μy is specifically excluded i.e. channels that lie on a line of constant determinant do not compare with respect to unless they are equal. The behavior of capacity on such lines is more involved than it is for lines that hit the diagonal. We now turn to this important special case, and once again, find the monotonicity principle indispensable.
Consider a line in N of fixed determinant, that is, a line joining the Z-channels

(d, 0) and (1, 1 − d):

πd(t) = t(1, 1 − d)+ t¯(d, 0)

Let c(t) denote the capacity of the channel πd(t).
Theorem 5.6 The function c	πd for d > 0 is strictly monotonically decreasing on [0, 1 ] and strictly monotonically increasing on [ 1 , 1]. For d = 0, it is identically
2	2
zero.
We have derived the following lower and upper bounds on the capacity:
Corollary 5.7 For any binary channel x ∈ [0, 1]2,

1	 1 −| det(x)| 

( )	log


−H(| det(x)|) 

— H	2
≤ c x ≤
2	1+2 
| det(x)|

with the understanding that the expression on the right is zero when det(x) = 0.
The bounds in Corollary 5.7 are canonical:
Definition 5.8 A function f : [0, 1]2 → R is called det-invariant if | det(x)| = | det(y)| implies f (x) = f (y) for all x, y ∈ N.
Thus, a det-invariant function is one whose value depends only on the magnitude of the channel’s determinant – in particular, such functions are symmetric.

Corollary 5.9	The supremum a(x) and inﬁmum b(x) of all det-invariant lower bounds on capacity are equal to


( ) = 1 
  1 −| det(x)| 

( ) = log

−H(| det(x)|) 

a x	− H	2
b x	2
1+2 
| det(x)|

The best det-invariant lower bound in Corollary 5.7 is the key idea in determining how close in value | det | is to c:
Theorem 5.10

sup
(a,b)∈[0,1]2
|det(a, b)|− c(a, b) = log2(5/4)

which is attained by the channels (4/5, 1/5), (1/5, 4/5).
The number log2(5/4) is approximately equal to 0.3219. Because det itself is a det-invariant upper bound on capacity, b(x)  det(x) by Corollary 5.9, and we have the following chain of inequalities:

a(x) ≤ c(x) ≤ b(x) ≤ | det(x)|≤ c(x)+ log2
 5 


Results like these can be applied to the difficult problem of bounding the capacity of a timing channel: a channel where each output symbol has an associated cost ti > 0 and one seeks to determine capacity per unit time or put simply, timed capacity. In this problem, bounds are especially useful because there are no formulae available for computing timed capacity – even in the case of a noiseless binary timing channel, one must resort to numerical methods [10]. However, any lower and upper bound on capacity c leads to one on timed capacity ct because

c


max ti
c
≤ ct ≤ min t

so we obtain a bound on the capacity of a binary timing channel that in some cases may be quite useful.
A topological relation
Earlier we studied partial orders on binary channels, each offers a different way of relating a pair of channels to one another. We then jumped up a level of abstrac- tion and studied relations that exist between fundamental monotone mappings on binary channels. However, the partial orders are not merely partial orders, and the monotone mappings are not merely monotone. In each case, more mathematical structure is present, and by taking this additional structure into account, a new relation between capacity and distance emerges. This one is topological.
The extra structure that the poset N has is that it is a domain: a partially ordered set with intrinsic notions of completeness and approximation defined by the order. The extra structure that capacity has is that it is a measurement: a

function μ that to each informative object x assigns a number μx which measures the information content of the object x. We now define each of these terms precisely before discussing them further.
The intrinsic notion of completeness that a domain has is that it forms a dcpo:
Definition 5.11 Let (P,  ) be a partially ordered set or poset. A nonempty subset S   P is directed if ( x, y  S)( z  S) x, y  z.  The supremum  S of S   P is the least of its upper bounds when it exists. A dcpo is a poset in which every directed set has a supremum.
The intrinsic notion of approximation possessed by a domain is formalized by continuity:
Definition 5.12 Let (D, ) be a dcpo. For elements x, y  D, we write x  y iff for every directed subset S with y ± . S, we have x ± s, for some s ∈ S. We set
↓x := {y ∈ D : y  x} and ↑x := {y ∈ D : x  y}
and say D is continuous if ↓x is directed with supremum x for each x ∈ D.
Definition 5.13 A domain is a continuous dcpo.
The poset of nonnegative channels N is order isomorphic to the compact subin- tervals of the unit interval

I[0, 1] = {[a, b] : a, b ∈ [0, 1] & a ≤ b}
ordered by reverse inclusion with an explicit isomorphism given by N  I[0, 1] :: (a, b)  [b, a]. This correspondence implies that N forms a domain, called the interval domain, where  S =  S, for directed S  I[0, 1] and x  y iff y  int(x). Notice that int(x) refers to the interior of the interval x in its relative Euclidean topology.
Definition 5.14 The Scott topology on a continuous dcpo D has as a basis all sets of the form ↑x for x ∈ D.
A function f : D → E between domains is Scott continuous if the inverse image of a Scott open set in E is Scott open in D. Let [0, ∞)∗ denote the poset of nonnegative reals in their dual order: x ± y ≡ y ≤ x.
Definition 5.15 A Scott continuous μ : D → [0, ∞)∗ is said to measure the content
of x ∈ D if for all Scott open sets U ⊆ D,

x ∈ U ⇒ (∃ε > 0) x ∈ με(x) ⊆ U

where με(x) := {y ∈ D : y ± x & |μx − μy| < ε}.
Definition 5.16 A measurement μ : D → [0, ∞)∗ is a Scott continuous map that measures the content of ker(μ) := {x ∈ D : μx = 0}.

The order on a domain D defines a clear sense in which one object has ‘more information’ than another: a qualitative view of information content. The definition of measurement attempts to identify those monotone mappings μ which offer a quantitative measure of information content in the sense specified by the order. The essential point in the definition of measurement is that μ measure content in a manner that is consistent with the particular view offered by the order. There are plenty of monotone mappings that are not measurements – and while some of them may measure information content in some other sense, each sense must first be specified by a different information order. The definition of measurement is then a minimal test that a function μ must pass if we are to regard it as providing a measure of information content.
We now consider a few properties that measures of information content have which arbitrary monotone mappings in general need not have: qualities that make them ‘different’ from maps that are simply monotone. Other such properties may be found in [7]. Define d : D2 → [0, ∞)∗ by

d(x, y) = inf{μz : z ± x, y}

where we assume that D has either a least element or more generally is ‘filtered’. Denote the ε balls with respect to d by Bε(x) := {y ∈ D : d(x, y) < ε}.
Theorem 5.17 (Martin[7]) Let μ : D → [0, ∞)∗ be a measurement.
x ∈ ker(μ) ⇒ x ∈ max(D) = {x ∈ D : ↑x = {x}}.
If μ measures the content of y ∈ D, then
(∀x ∈ D) x ± y & μx = μy ⇒ x = y.

If μ measures X	D, then Bε(x)	X : x	X, ε > 0 is a basis for the Scott topology on X.
Theorem 5.17 says that any measurement on N induces the Euclidean topology on its kernel.
Theorem 5.18 (Martin[9]) Capacity c : N → [0, 1]∗ is a measurement.
In the case of capacity c : N	[0, 1]∗, the associated distance function on ker(c) = max(N) works out to be ρ([a], [b]) = c(a, b) = c(b, a). Then, just like Euclidean distance, capacity also has the following three properties: (i) c(a, b) = c(b, a), (ii) c(a, b) = 0 iff a = b, (iii) the sets y	[0, 1] : c(x, y) < ε  for ε > 0 form a basis for the Euclidean topology on [0, 1].
Capacity does not satisfy the triangle inequality, so it is not a priori obvious that the sets in (iii) form a basis for any topology, let alone the Euclidean topology. Thus, this is another relationship between capacity and the determinant: each of them is a measure of distance that induces the Euclidean topology. What we now seek is a better understanding of why this happens. For this, we need to think about how this result is proved.

The proof that capacity is a measurement given in [9] depends on the result of Majani and Rumsey [6], and is interesting since it connects the study of mea- surement in domain theory to a fundamental and beautiful result in information theory. Specifically, it is shown that c(a, b) (det(a, b))2/(e2 ln(2)). This lower bound has the form ν det where ν : [0, 1]  [0, 1/(e2 ln(2))] is the order isomor- phism ν(t) = t2/(e2 ln(2)). The proof in [9] relies heavily on specific results only known to hold for binary channels as well as intricate arguments from analysis. We now give a new proof of this result which has several advantages over the one in [9].
Proposition 5.19 Let ϕ : [0, 1]  [0, 1] be the function ϕ(t) = 1  H((1  t)/2). If λ : N [0, )∗ is Scott continuous and λ ϕ det, then λ is a measurement. In particular, capacity is a measurement.
In fact, every function in the string of inequalities


1	 1−| det(x)| 

( )	log


−H(| det(x)|) 




is a measurement and has properties (i), (ii) and (iii) discussed earlier. This new proof is an improvement over the one in [9]:
ϕ ◦ det is a better lower bound than ν ◦ det: by Corollary 5.9, ν ◦ det ≤ ϕ ◦ det,
The inequality ϕ det	c is derived using only the monotonicity principle, a fact known to hold for arbitrary channels.
Moreover, the measurement ϕ det has profound applied significance, as we now demonstrate by using it to solve an open problem in quantum steganography.

Quantum steganography
In this section, we will learn a few of the fascinating implications the results in this paper have within the realm of communication. We first review the basic protocol for quantum key distribution. Because we intend for this paper to be readable by someone with no prior knowledge of quantum mechanics, we discuss only the minimal background needed to understand quantum key distribution. The few ideas we make use of are very simple.

Quantum information
Like all systems, a quantum system has state. The state of a quantum system is represented by a unit vector in a vector space that has a lot more structure than most, known as a Hilbert space. The state of a quantum system is also called a ket. Here are two examples of kets: 0 and 1 . It is useful to think about these two particular kets as being quantum realizations of the classical bits 0 and 1. Each refers to a legitimate state of a quantum system. A photon is an example of a quantum system and its polarization (state) is something we need kets to describe.

One of the neat things about a quantum system is that it can also be in any state ‘in between’ |0⟩ and |1⟩, such as |+⟩ = √1 (|0⟩ + |1⟩) or |−⟩ = √1 (|0⟩− |1⟩),
which we also think of as representing the classical bits 1 and 0 respectively. Any ket |ψ⟩ that can be written as |ψ⟩ = a|0⟩ + b|1⟩, for |a|2 + |b|2 = 1, is called a qubit. There are only four qubits that we care about in this paper: |0⟩, |1⟩, |+⟩ and |−⟩.
Like all systems, one would like to extract information from a quantum system. One way to extract information from a quantum system is to perform a measurement on it. Before an observer can perform a measurement on a quantum system, they must say what they want to measure. One way an observer can specify what they want to measure is by specifying a basis and then “performing a measurement in the specified basis.” Two examples of bases are X = + ,   and Z = 0 , 1 . They are the only bases we will use in this paper 4 . What happens when we measure a quantum system?
If the state of a quantum system is described by the qubit ψ = a 0 + b 1 , then a measurement in the Z basis will yield the result 0 with probability a 2 and the result 1 with probability b 2. Notice that these are the only possible outcomes of this measurement because qubits satisfy a 2+ b 2 = 1, a property they have because they are unit vectors. In this paper, we only care about measuring the following four states in the Z basis: |0⟩, |1⟩, |+⟩ and |−⟩. If we measure a system with state
|0⟩ in the Z basis, we get |0⟩ with probability 1; the same is true of the state |1⟩. If we measure either |+⟩ or |−⟩ in the Z basis, we obtain |0⟩ with probability 1/2 and |1⟩ with probability 1/2.
It is also possible to measure a system in the X basis. If a system is in the state
+ and we measure it in the X basis, we get + with probability 1, similarly for
. But what happens when we measure a system with state 0 or 1 in the X
basis? Well, first we have to express these states as sums of states in the X basis:
1	1
|0⟩ = √2 (|+⟩ + |−⟩)	|1⟩ = √2 (|+⟩ − |−⟩)
Now we can see that if we measure |0⟩ in the X basis, we get |+⟩ with probability 1/2 and |−⟩ with probability 1/2, similarly for |1⟩.

Quantum key distribution
We now recall one of the standard accounts of quantum key distribution (QKD), the BB84 protocol [15]: (1) Alice chooses a random string k of about 4n bits containing the eventual key. (2) Alice randomly codes each bit of k in either the X =  + ,
or Z = 0 , 1 bases. (3) Alice sends each resulting qubit to Bob. (4) Bob receives the 4n qubits, randomly measuring each in either the X or the Z basis. (5) Alice announces in which basis she originally coded each bit of k. (6) Bob tells Alice which bits he received correctly; they now share about 2n bits. (7) Alice selects a subset of n bits from the group she formed in step (6) that will be used. to check on Eve’s

4 Many bases are possible, and each offers a different way of representing the classical bits 0 and 1. The ability to alternate between such representations helps prevent eavesdropping in QKD.

interference, and tells Bob which bits she selected. (8) Alice and Bob compare their values of the n check bits; if more than an acceptable number disagree, they abort the protocol (eavesdropping). (9) Alice and Bob perform information reconciliation and privacy amplification to select a smaller m-bit key from the remaining n bits.
The bits in step (6) are called the sifted bits. If Alice has coded a classical bit in either of the X or Z bases, and later Bob measures in the same basis, he will receive the bit sent by Alice with probability 1. Such a bit will be one of the sifted bits. But now suppose that an eavesdropper wishes to know the bit Alice is sending Bob. Well the eavesdropper, named Eve, has to guess which basis Alice coded the bit in, and then measure it herself. When Eve guesses, she introduces an error into the sifted bits with probability 1/4 – but an error that Alice and Bob will know about, and this is the reason they are able to detect the presence of an eavesdropper.
It is fundamental in QKD that Alice and Bob insist on an error rate within the sifted bits that is less than 1/4 to defend themselves from precisely this type of attack, or else the security of QKD cannot be guaranteed [1]. For instance, assuming errors only due to Eve, if Eve has measured all the qubits sent from Alice to Bob, then Eve knows which of the sifted bits Bob and Alice share, and which of the sifted bits they may not share 5 . This is something that Bob himself does not even know. With an error rate beyond 1/4, Bob cannot have more information than Eve about any key generated – remember that after the sifted bits are identified, Eve can listen in on the rest of the protocol, since it takes place over a public channel.

Analysis of hidden channels within quantum key distribution
As explained in [8], QKD is not communication, for the simple fact that neither Alice nor Bob has any control over the sifted bits, or the key their interaction ultimately produces. However, as first shown in [8], it can be modified so that communication is possible: a quantum protocol can be used to obtain a new protocol which is physically indistinguishable from the original, but which also contains a channel whose existence is undetectable by any currently known technology. The potential of such ‘hidden channels’ is discussed in [8].
Let us give a simple illustration of how such hidden channels arise. Assume Alice would like to send Bob a single bit of information. All we have to do is make a simple change to step (7) in QKD: “(7) Alice randomly selects a bit from the group of 2n whose value is the information she wants to transmit. Then she randomly selects n  1 check bits from the remaining 2n  1. The nth check bit is chosen from the remaining n + 1 as being the bit to the immediate left of the information.” Bob now has the information Alice sent: he knows its relation to the last check bit, because the two parties have agreed on this scheme in advance. They have agreed that Alice will covertly send Bob a ‘pointer’ to the information.
Here is an example: Alice and Bob share the 2n bits, Alice selects the information
bit

0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 

5 Eve knows when she guessed the right basis and when she did not.

Now she selects n	1 check bits at random, which leaves Alice and Bob with n +1 remaining bits

0 ∗ 1 0 ∗ 0 1 ∗ 1 ∗ 0 ∗ 0 ∗ 0 ∗ ∗ 1 ∗ 1 0 ∗ 1 ∗ 1 1 ∗ ∗ 
Alice now selects the last check bit as being the pointer to the information i.e. the bit to the immediate left of the information bit:
0 ∗ 1 0 ∗ 0 1 ∗ 1 ∗ 0 ∗ 0 ∗ −→0 ∗ ∗ 1 ∗ 1 0 ∗ 1 ∗ 1 1 ∗ ∗ 

Is Bob guaranteed to receive the information sent by Alice? No. There are many reasons why. Suppose an eavesdropper just happens to measure only the qubit that holds the information in the wrong basis, then there is a 50 percent chance that Bob has the wrong bit, even though he believes he has the right bit. Or suppose that background light acts as noise which causes the information bit to flip. In either case, Bob would have no idea, and neither would Alice. But chances are good that such errors, whether caused by the environment or an eavesdropper, would also manifest themselves in the check bits as well, which would then enable them to estimate the likelihood that their attempt to communicate will succeed. Alice and Bob always have the option of aborting the protocol if their chances of success are not deemed high enough. The question we want to answer is: what is the capacity of this channel?
Theorem 6.1 (Martin [8]) If the error rate α  [0, 1/4) is due solely to inter- ference caused by Eve, then the capacity of the Alice-Bob steganographic channel is 1 −H(α). In particular, the capacity of the Alice-Bob channel is never smaller than 1 − H(1/4) ≈ 0.1887.
It is important to understand in the last result that the phrase “interference caused by Eve” means not only that Eve causes all errors, but that she causes these errors by essentially performing random combinations of X and Z measurements. This point is fundamental, since it implies that the probability of a 0 flipping to a 1 is the same as the probability that a 1 flips to a 0. The reason is that Eve cannot tell which classical bit a qubit represents before she performs a measurement and that Alice sends bits with equal frequency. This means the hidden channel is binary symmetric where the probability of a flip is α: the noise matrix of the hidden channel is (α¯, α), so its capacity is 1 H(α). Moreover, because the parameter α is an experimentally determined quantity that must be calculated in any realization of QKD to check for the presence of an eavesdropper, the last result allows us to calculate the capacity of any hidden channel based on sifted bits any time that a QKD experiment is performed [8].
But now suppose Eve does something other than perform measurements in the
X and Z bases at random. Or suppose some of the noise is caused by the en- vironment. Then it is no longer necessarily the case that 1 and 0 flip with the same probability, so whatever the noise matrix of the hidden channel is, we know that it is not necessarily binary symmetric. For instance, a well-known effect like

amplitude damping [15] does not affect 0 but does flip 1 . Thus, in the case of general noise, the matrix for the channel is unknown, in the sense that it cannot be determined from experimentally necessary quantities. We can nevertheless establish the following important lower bound on its capacity:
Theorem 6.2 Let the error rate be α ∈ [0, 1/4). Then the capacity of the Alice- Bob steganographic channel is at least 1 − H(α). In particular, the capacity of the Alice-Bob channel is never smaller than 1 − H(1/4) ≈ 0.1887.
The principle underlying this last result is clear as well as surprising: from the point of view of Alice and Bob, noise caused by an arbitrary combination of environment and eavesdropper is preferable to noise caused by an eavesdropper alone. The reason is that an eavesdropper causes bits to flip with equal probability, leading to a binary symmetric channel, whereas the environment may not.
Put another way, any attempt to interrupt the hidden channel should necessarily
employ a random combination of X and Z measurements. Even for error rates α arbitrarily close to 1/4, any scheme that does not flip bits with equal probability will permit a capacity higher than the theoretical limit of 1 − H(1/4), up to a possible
maximum of c(1, 1/2) = log2(5/4) ≈ 0.32.
As these results make clear, det-invariant bounds on capacity provide a valuable way to approximate the capacity of a channel when its determinant is known, but its noise matrix is not. As we have seen, such channels arise naturally in experimental situations.
Acknowledgments
As far as we aware, Ira S. Moskowitz of NRL was the first to conjecture the behav- ior of capacity along a line of constant determinant, in a private communication to the authors. We are grateful to him for this important insight. We thank Catus- cia Palamidessi for a series of valuable discussions on the work contained herein, and Prakash Panangaden, who introduced the authors to each other at last year’s Bellairs workshop on information theory and security.

References
Brassard, G., N. Lu¨tkenhaus, T. Mor and B. C. Sanders, Limitations on practical quantum cryptography, Phys. Rev. Lett. 85 (2000), pp. 1330–1333.
Chatzikokolakis, K. and K. Martin, A mononoticity principle for information theory, report version. Available at http://www.lix.polytechnique.fr/∼kostas/papers/mfps08.pdf.
Chatzikokolakis, K., C. Palamidessi and P. Panangaden, Anonymity protocols as noisy channels, Information and Computation (2007), to appear.
Clark, D., S. Hunt and P. Malacaria, Quantified interference for a while language, in: Proc. of QAPL 2004, ENTCS 112 (2005), pp. 149–166.
Cover, T. M. and J. A. Thomas, “Elements of Information Theory,” John Wiley & Sons, Inc., 1991.
Majani, E. and H. Rumsey, Two results on binary-input discrete memoryless channels, in: Proceedings of the IEEE International Symposium on Information Theory, 1991, pp. 104–104.

Martin, K., “A foundation for computation,” Ph.D. thesis, Tulane University, Department of Mathematics (2000).
Martin, K., Steganographic communication with quantum information, in: Proceedings of Information Hiding ’07, LNCS (2007), to appear.
Martin, K., Topology in information theory in topology, Theoretical Computer Science (2007), to appear.
Martin, K. and I. S. Moskowitz, Noisy timing channels with binary inputs and outputs, in: Proceedings Information Hiding ’06, LNCS 4437 (2006), pp. 124–144.
Martin, K., I. S. Moskowitz and G. Allwein, Algebraic information theory for binary channels, ENTCS
158 (2006), pp. 289–306.
Millen, J., Covert channel capacity, in: Proceedings of the 1987 IEEE Symp. on Computer Security and Privacy, 1987.
Moskowitz, I. S., R. E. Newman and P. F. Syverson, Quasi-anonymous channels, in: IASTED CNIS, 2003, pp. 126–131.
Nielsen, M., K. Krukow and V. Sassone, A bayesian model for event-based trust, ENTCS 172 (2007),
pp. 499–521.
Nielsen, M. A. and I. L. Chuang, “Quantum Computation and Quantum Information,” Cambridge University Press, 2000.
Shannon, C. E., Some geometrical results in channel capacity, in: Collected Papers of C.E. Shannon, IEEE Press, 1993 pp. 259–265.
