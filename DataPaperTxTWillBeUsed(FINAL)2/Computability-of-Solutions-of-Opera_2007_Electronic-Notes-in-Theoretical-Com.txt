	Electronic Notes in Theoretical Computer Science 167 (2007) 179–202	
www.elsevier.com/locate/entcs

Computability of Solutions of Operator Equations
Volker Bosserhoff1 ,2
Institut fu¨r Theoretische Informatik und Mathematik Universit¨at der Bundeswehr
Munich, Germany

Abstract
We study operator equations within the Turing machine based framework for computability in analysis. Is there an algorithm that maps pairs (T, u) (where T is given in form of a program) to good approximate solutions of Tx = u? Here we consider the case when T is a bounded linear mapping of Hilbert spaces. We are in particular interested in computing the generalized inverse
T †u which is the standard concept of solution in the theory of inverse problems. Typically, T † is discontinuous (i.e. the equation Tx = u is ill-posed ) and hence no computable mapping. However, we will use effective versions of theorems from the theory of regularization to show that the mapping
(T, T ∗, u, T †u  ) '→ T †u is computable. We then go on to study the computability of average-case solutions with respect to Gaussian measures which have been considered in information based complexity. Here T † is considered as an element of an L2-space. We define suitable representations for such spaces and use the results from the first part of the paper to show that (T, T ∗, T †  L2 ) '→ T † is computable.
Keywords: computable functional analysis, operator equations, regularization, Gaussian measures


Introduction
Ill-posed operator equations
We investigate the following question: Given two computable (real or complex) normed spaces X and Y and a program which computes a bounded linear mapping T : X → Y , can we effectively find a programm which computes

1 The work was supported by DFG grant HE 2489/4-1.
2 Email: volker.bosserhoff@unibw.de





1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2006.08.013

T −1? The model of computability on continuous objects on which we will found our considerations shall be the Turing machine based approach of [20] and especially its extension to separable normed spaces (as described e.g. in the introductory sections of [3]). We assume familiarity with these concepts. 3 Of course, T −1 is only well-defined for injective T . In the case of X and
Y being Banach spaces and T being bijective, Brattka’s computable version of Banach’s Inverse Mapping Theorem applies: T −1 is computable if T is computable, but there is no effective way to transform a program for T into a program for T −1 (see [3] or [4]).
If we restrict ourselves to X and Y being Hilbert spaces, there is a broader concept of solution for Tx = u which is well-established in the theory of inverse problems and allows us to also handle non-injective T – the Moore-Penrose generalized inverse T †. It is defined as a best approximate solution which is a minimizer of the residual Tx  u  and minimizes  x  among all minimizers of the residual; T †u is well-defined for all u  range T  (range T )⊥ and is a closed linear mapping. A detailed treatment of generalized inverses is given in [9]; the most important facts can also be found in [8, Section 2.1].
T † is bounded exactly when range T is closed. In many important ap- plications however – especially when T is a compact 4 operator with infinite dimensional range – this is not fulfilled. Some examples are given in [11,8]. The problem of approximating unbounded linear mappings S : Y  X is considered as an ill-posed problem, a term going back to Hadamard.
It is a fundamental fact in computable analysis that computable mappings are necessarily continuous. So there is no hope for T † to be computable in the ill-posed case. Pour-El and Richards’ First Main Theorem [14] even states the following (here we give the formulation of Brattka [2]):
Theorem 1.1 Let Y , X be computable Banach spaces and let S :⊆ Y → X be a closed and unbounded linear operator. Suppose there is a computable sequence (en)n∈N in dom S such that span{en}n∈N is dense in Y and (Sen)n∈N is a computable sequence in X. Then there exists a computable point u0 ∈ Y such that Su0 ∈ X is not computable.	 
It is easy to construct a computable compact operator T : l2  l2 such that T † fulfills the assumptions of the theorem 5 .
General linear ill-posed problems have also been studied in the context

3 In particular, we will make use of the representations δX, νN, ρ and ρ> as defined in [20,3].
4 B bounded	T (B) compact
5 For example the diagonal operator mapping each unit vector ei to 1 ei.

of information-based complexity (IBC) 6 . In this framework, one question is whether the “information” on some u ∈ D which is retrieved by applying a finite collection of continuous linear real functions to it already determines the element Su up to a finite precision. Werschulz [21, Theorem 2.1] obtained the following negative result:
Theorem 1.2 If Y and X are normed spaces and S : D  X is a linear unbounded mapping deﬁned on some linear subspace D of Y , then for any f0,... , fn−1 ∈ L(Y, R) and any C > 0 there are u1, u2 ∈ D,  u1  , u2  ≤ 1, such that


and
fi(u1)= fi(u2) for all 0 ≤ i ≤ n S(u1) − S(u2)  > C.

Traub and Werschulz (in Chapter 6 of [17]) put this result in analogy to Pour-El and Richards’ First Main Theorem.

Regularization
In numerical mathematics, methods have been developed to partly overcome the difficulties related to solving ill-posed operator equations Tx = u. A standard approach (which goes back to ideas of Tikhonov and Phillips) is to substitute the original equation by a sequence of “near by” well-posed equations, so called regularized versions. The solutions of these equations then converge to the solution of the original equation. In general however, one does not know how close an approximation obtained via regularization is to the actual solution, unless a-priori information on the solution is available. There is a vast literature on regularization methods; see for example [8].
The first part of the paper is devoted to an effective version of a general regularization method: In Section 2 we will first give some background on the Spectral Theorem for bounded self-adjoint operators and on operator calculus. Then, in Section 3, we give an effective version of a theorem of Groetsch and Jacobs, which says that we can compute T †u from u, T , T ∗ and T †u  .

Average case solutions
In information based complexity, Werschulz and others have also studied ill- posed linear approximation problems S :⊆ Y → X in an average case setting.

6 Introductions to IBC can be found e.g. in the monographs [15,22,13,17].

In this setting, one additionally assumes that D := dom S and S are mea- surable and considers a measure μ on X such that μ(D) = 1. The question now is if for any given precision ϵ there are elements f0,... , fn ∈ LY, R and a mapping (a so called “algorithm”) Φ : Rn → X such that the expected error
∫	Φ(f0(u),... , fn(u)) − Su  2 μ(du)
is smaller than ϵ.
An important positive result in this context (see [21,12,22,18] and the survey [16]) is that for Y and X being real separable Banach spaces and γ being a centered Gaussian measure on Y , every γ-measurable linear mapping (see below for the definition) is in Lp(Y, γ; X), and the finite rank mappings of the form
[y '→ f0(y)x0 + f1(y)x1 + ... + fn−1(y)xn−1],  fi ∈ Y ∗, xi ∈ X
are Lp-dense among these for all p ≥ 1. This clearly implies that linear ill- posed problems are solvable on the average with respect to Gaussian measures. In [22, Section 7.5.1] this is shown for X being a Hilbert space and p = 2; in
[18] the version just stated is implicit. For completeness we give a proof in Section 5 after having collected prerequisites on Gaussian measures in Section 4.
We will see below that T † is measurable, so the just stated IBC result fully applies to it. From the point of view of computability theory the question arises under what circumstances suitable functionals fi and elements xi can be found effectively. More precisely, we ask the following question:
Let X and Y be computable real Hilbert spaces and let γ be a Gaussian measure on Y . Is there an effective procedure which transforms every m ∈ N and every program for some T : X → Y into a number n ∈ N and a vector (a0,... , an−1, b0,... , bn−1) ∈ Y n × Xn such that
 T † − ⟨a0, .⟩b0 + ... + ⟨an−1, .⟩bn−1  L2 (Y,γ;X) ≤ 2−m?

In order to study this question properly we will define an effectivity struc- ture on L2(Y, γ; X) in Section 6. If we allow both a program for T ∗ and a list of all rational upper bounds of T †  L2(Y,γ;X) as additional inputs, then we can construct an algorithm for the above task. This is our main result and will be proved in Section 7. Our algorithm relies in an essential way on the effective regularization methods from the first part of the paper.

Acknowledgement
The author would like to thank Peter Hertling for helpful discussion and com- ments.

The Spectral Theorem and operator calculus for bounded self-adjoint operators
For normed spaces X and Y , let L(X, Y ) denote the space of bounded linear mappings 7 from X into Y . For convenience: L(X) := L(X, X).
For a (real or complex) Hilbert space X and any self-adjoint operator T ∈ L(X) let σ(T ) denote the spectrum and let mT and MT be the spectral bounds of T , i.e. mT is the smallest and MT the largest element of σ(T ) (cf. [10, p. 117]). Remember T  = max{|mT |, |MT |}.
We now state (a reduced version of) the Spectral Theorem (cf. [10, Theo- rem 5.2.7]):
Theorem 2.1 Let X be a Hilbert space and let T ∈ L(X) be self-adjoint. Let
(Eλ)λ∈R ∈ (L(X))	be the spectral family	generated by T. Then we have
Eλ2 − Eλ1 is non-negative self-adjoint for all λ2 ≥ λ1. (ii)
b
T =	λ dEλ
a
for all a < mT and b ≥ MT .

Here the integral is to be interpreted as an operator valued Riemann- Stieltjes integral : For every f ∈ C[a, b]
∫ b


is defined in analogy to the classical Riemann-Stieltjes integral with Riemann sums of the form
k
f (λˆi)(Eλi − Eλi−1 )
i=1
with a = λ0 < ... < λn = b and λˆi ∈ [λi−1, λi].

7 It is more common to denote this space by	(X, Y ), but we prefer to use the symbol in connection with Borel σ-algebras.
8 See [10, p. 182]. The definition of the spectral family and its further properties will not
be important in this paper.

The following facts can be found in Section 3.3 of [10]:
Theorem 2.2 Let T and (Eλ)λ∈R be as in the Spectral Theorem. Let f be a real function which is continuous on the interval [a, MT ] for some a < mT .
Then ∫ b f (λ) dE =: f (T ) exists for all b ≥ M	(and does not depend on the
choice of a, b). The following properties hold:
The operator f (T ) is self-adjoint.
The mapping f '→ f (T ) is linear.
(fg)(T )= f (T )g(T ).

For any real polynomial p(x)= Σn	aixi, we have p(K)= Σn
aiKi.

 f (T )  ≤ max{|f (x)| : x ∈ [a, MT ]}.

The next lemma will be very useful below.
Lemma 2.3 Let T and (Eλ)λ∈R be as in the Spectral Theorem. Let (fn)n∈N be a sequence of continuous functions such that fn ∈ C[an, MT ] for some an < mT . Further assume that
(∀ t ∈ [max{an, am}, MT ]) [n ≤ m ⇒ fn(t) ≤ fm(t)].

We then have:
(i)
(∀ h ∈ X) [n ≤ m ⇒  fn(T )h  ≤  fm(T )h  ] .
Under the additional assumption that
(∀ n ∈ N)(∀ t ∈ [an, MT ]) fn(t) ≥ 0
and that
lim fn(T )h =: u
n→∞
exists for some h ∈ X, we have
(∀ n ∈ N) u − fn(T )h  2 ≤  u  2 − fn(T )h  2.

Proof. Let h ∈ X be arbitrary and n ≤ m. We consider Riemann sums over

[max{an, am}, MT ].
k
fn(λˆi)2(Eλ
i=1






— Eλi−1


)h, h  =



Σi=1


fn(λˆi)2 ⟨(Eλ







— Eλi−1
∈R, ≥0


)h, h
x

≤ Σi=1
fm(λˆi)2⟨(Eλ


— Eλi−1
)h, h⟩ =
k


i=1
fm(λˆi)2(Eλ


— Eλi−1
)h, h  .

Taking a limit over Riemann sums we get
⟨fn(T ) h, h⟩ ≤ ⟨fm(T ) h, h⟩.
As fn(T ) and fm(T ) are self-adjoint we get
 fn(T )h  2 ≤  fm(T )h  2.
For the proof of (ii) we proceed similarly:


k


i=1 k

fn(λˆi)2(Eλ



— Eλi−1

)h, h 



i=1 k
≥˛¸0	x

≤ Σ fm(λˆi)fn(λˆi)⟨(Eλi − Eλi−1 )h, h⟩
 Σk
	

Taking a limit over Riemann sums we get
⟨fn(T ) h, h⟩ ≤ ⟨fm(T )fn(T )h, h⟩
and hence by self-adjointness
 fn(T )h  2 ≤ ⟨fn(T )h, fm(T )h⟩.
For m → ∞ this yields
 fn(T )h  2 ≤ ⟨fn(T )h, u⟩.

¿From this we get
 u − fn(T )h  2 = u  2 − 2⟨u, fn(T )h⟩ + fn(T )h  2 ≤  u  2 − fn(T )h  2.


Approximation of T † by computable sequences
The following theorem (going back to Groetsch and Jacobs) can be found in [8, Theorem 4.1]. It is the key to our further investigations.
Theorem 3.1 Let X, Y be Hilbert spaces and T	(X, Y ). Let (fn)n∈N (C[0, T 2])N be a family of continuous functions such that
( t	(0, T 2]) lim fn(t)= 1/t,
n→∞


and Then
sup{|tfn(t)| : t ∈ [0, T 2], n ∈ N} < ∞.

lim fn(T ∗T )T ∗g = T †g
n→∞

for every g ∈ dom T †. If g ∈/ dom T † then


lim
n→∞
 fn(T ∗T )T ∗g  = ∞.



There are of course many possible choices for the fk in the above theorem. We will now fix one (which leads to the method of Tikhonov regularization, cf. [8, Chapter 5]):
Corollary 3.2 Let X, Y be Hilbert spaces. Let T ∈ L(X, Y ). Set
1
fk(t) :=	 1 
k+1
for k ∈ N, t > −(k + 1)−1. Then for any g ∈ dom T †
limk→∞ fk(T ∗T )T ∗g = T †g,
 fk(T ∗T )T ∗g  grows monotonously in k,
 T †g − fk(T ∗T )T ∗g  2 ≤  T †g  2 − fk(T ∗T )T ∗g  2.
Proof. It is easy to verify that the fk fulﬁll the assumptions of Theorem
which yields (i).	(ii) and (iii) follow from Lemma 2.3: One just has

to remember that σ(T ∗T )  [0, T 2] and observe that each fk is continuous on some compact interval [ak, T 2], ak < 0.	 
Corollary 3.3 Let X, Y  be Hilbert spaces and T ∈ L(X, Y ). Then dom T †
and T  are measurable.
Proof. By Theorem 3.1 and Corollary 3.2 there is a sequence of continuous mappings such that dom T † is its domain of convergence and T † is its pointwise limit.	 
Our next aim is an effective version of Corollary 3.2. We introduce com- putable Hilbert spaces:
Definition 3.4 A computable normed space (X, . , α) (see [3]) is a com- putable Hilbert space if (X, .  ) is complete and . is induced by an inner product.
[6] contains effective versions of many classical results on Hilbert spaces. Computable Hilbert spaces are also considered in [5].
Remark 3.5 The inner product of a computable Hilbert space is always com- putable (by the polarization identity).
The operator calculus from the previous section can be made effective. This fact has already been used (in a non-uniform way) by Pour-El and Richards
[14] to prove their Second Main Theorem. A detailed derivation of uniform versions appears in [7]. We give a brief proof for the special case we will need.

Theorem 3.6 Let X be a computable Hilbert space with Cauchy representa- tion δX (see [3]). Let fk be as in Corollary 3.2. The mapping
{(T, k, h) : T ∈ L(X) non-negative self-adjoint, k ∈ N, h ∈ H} → H,
(T, k, h) '→ fk(T )h
is ([δX → δX], νN, δX)-computable.
Proof. It suffices to demonstrate how to effectively find a 2−m-approximation to fk(T )h given m, k, a rapidly converging sequence (hi)i∈N (i.e. h − hi  ≤ 2−i) of approximations for h ∈ X, and a [δX → δX]-name of T .
By [3, Theorem 9.10] we can compute a number s ∈ Q such that T  < s,
hence σ(T ) ⊂ [0, s]. fk can be evaluated on Ik := [− 1 , s]. Remember that
 fk(T )  ≤ sup fk(Ik) =: r. r is simply (k + 1)(k + 2), so we can effectively choose some i ∈ N such that r2−i ≤ 2−m−2. Then it is possible to effectively choose an upper bound q ∈ Q for hi . By the Effective Weierstrass Theorem

(see [20]), we can effectively find a polynomial p such that supI |fk − p| ≤
q−12−m−2 . So fk(T ) − p(T ) = (fk − p)(T ) ≤ q−12−m−1. We have
 p(T )hi − fk(T )h  ≤  p(T )hi − f (T )hi  + fk(T )hi − fk(T )h 
≤ (fk − p)(T )	hi  + fk(T )	hi − h

≤ q	2
−m−2
q + r2−i

−m−1.

p(T )hi can be approximated effectively; we compute a 2−m−1-approximation, say y. Then we finally have y − fk(T )h  ≤ 2−m.	 
It is well-known that the adjoint T ∗ of a bounded linear mapping is itself bounded. The mapping T '→ T ∗ however is not computably invariant and hence not computable (see [6]). In view of this fact the following definition makes sense:
Definition 3.7 Let X, Y be computable Hilbert spaces with Cauchy repre-

sentation δX, δY , respectively. Define a representation Δ→
of L(X, Y ) by



→
X,Y
⟨p, q⟩ = T  :⇐⇒ [δX → δY ](p)= T ∧ [δY → δX](q)= T .

This is the weakest representation of (X, Y ) which allows the evaluation of both the mapping and its adjoint. This fact will be used implicitly from now on.
Remark 3.8 Let K(X, Y ) ⊆ L(X, Y ), K(Y, X) ⊆ L(Y, X) be the subspaces of all compact mappings. There are representations of K(X, Y ), K(Y, X) that are stronger than [δX → δY ]|K(X,Y ), [δY → δX]|K(Y,X) and with respect to which K '→ K∗ is computable. (This is the Computable Schauder Theorem proved

in [6].) That representation of K(X, Y ) is hence stronger than Δ→
K(X,Y )

The next theorem is obtained as a direct combination of Corollary 3.2,
Theorem 3.6 and the definition of Δ→ :
Theorem 3.9 Let X, Y be computable Hilbert spaces. Deﬁne a set
A1 := {(T, g) : T ∈ L(X, Y ), g ∈ dom T †}.


There exists a ([Δ→
, δY ]|A1 , νN, δX)-computable mapping
GI1 : A1 × N → X

such that
limk→∞ GI1(T, g, k)= T †g,

 GI1(T, g, k)   grows monotonously in k,
 T †g − GI1(T, g, k)  2 ≤  T †g  2 − GI1(T, g, k)  2.

Corollary 3.10 Let X, Y be computable Hilbert spaces. Deﬁne a set
A2 := {(T, g, c) : T ∈ L(X, Y ), g ∈ dom T †,c = T †g  }.
The mapping


is ([Δ→
GI2 : A → X, (T, g, c) '→ T †g
, δY , ρ>]|A2 , δX)-computable.

Proof. As we are given T and g in suitable form, we can use GI1 from the previous theorem and compute a sequence converging to T †g. We ad- ditionally have a list of all rational upper bounds of T †g  ; so for ev- ery m ∈ N we can effectively (by exhaustive search) find some km with

T †g 2
−  GI1(T, g, km) 
−2m
. Item (iii) of the previous theorem yields

that GI1(T, g, km) then is a 2−m-approximation for T †g.	 

Prerequisites on Gaussian measures
In this section we collect some definitions and facts from the theory of Gaussian measures. Details can be found in [1]. We also point the reader to [19] which is a comprehensive treatment of general probability distributions on infinite dimensional vector spaces.
Note that while the results of the previous sections hold for real or complex spaces, we will from now on restrict ourselves to real spaces.
Definition 4.1 A Borel probability measure γ on R is called Gaussian if there is some a ∈ R such that γ is either the Dirac measure δa at a or has density

  1	
t '→ σ√2π exp
(t	a)2
—	2σ2

for some σ > 0. a is called the mean, σ2 the variance of γ.
This definition can be generalized to a wide class of topological vector spaces: A locally convex space (l.c.s.) X is a real topological vector space whose topology is generated by a family {pα}α∈A of seminorms separating 9 the points in X. There is a smallest σ-algebra on X with respect to which

9 A family {fα} of functions on a space X is said to separate the points in X if for any
x, y ∈ X, x /= y, there is an α such that fα(x) /= fα(y).

all elements of X∗ are 10 measurable; this σ-algebra is called the cylindrical
σ-algebra on X and is denoted by E(X). E(X) coincides with the σ-algebra
(X) of Borel sets (i.e. the σ-algebra generated by all open sets) if X is complete and metrizable.
Definition 4.2 Let X be a l.c.s. A probability measure γ on (X) is called Gaussian if, for any f X∗, the induced measure γ f −1 on R is Gaussian. Here the mean aγ of γ is the element of the algebraic dual (X∗)' to X∗ defined by

aγ(f )= 
X
f (x) γ(dx),

and the covariance operator Rγ : X∗ → (X∗)' is defined by the formula


Rγ(f )(g)= 
X
[f (x) − aγ(f )] [g(x) − aγ(g)] γ(dx).

Let G(X) denote the set of all Gaussian measures on X, and let G0(X) :=
{γ ∈ G(X) : aγ = 0} be the set of all centered Gaussian measures on X.
A Gaussian measure γ is uniquely defined by its covariance operator and mean. It has strong order p for every p ≥ 1, i.e.
∫	x  p γ(dx) < ∞.
The reproducing kernel Hilbert space X∗ ⊆ L2(γ) of some γ ∈ G(X) is the

closure of the set
{f − aγ(f ) : f ∈ X }

embedded into L2(γ). For centered γ, we will not distinguish between an element f of X∗ and its equivalence class in X∗. The elements of X∗ are real
γ	γ
Gaussian random variables. The covariance operator extends to X∗:


Rγ(f )(g) :=
X

f (x) [g(x) − aγ(g)] γ(dx), f ∈ X∗,g ∈ X∗.

It is an important feature of Gaussian measures that a collection V  X∗ of Gaussian random variables is independent exactly when its elements are pairwise uncorrelated, i.e., Rγ(f )(g) = 0 for all f, g ∈ V , f /= g.
¿From now on we only consider centered Gaussian measures on separable Banach spaces.

10 In this paper, X∗ shall refer to the topological and X′ to the algebraic dual of a (topo- logical) linear space X.

Lemma 4.3 Let X be a separable Banach space and γ ∈ G0(X).
For every f	X∗, there is a unique point xf	X such that Rγ(f )(g) = g(xf ) for all g		X∗. We will from now on consider Rγ as a mapping into X.
The image Rγ(X∗) is the intersection of all linear subspaces of X which have full γ measure.

Remark 4.4 For a Hilbert space X we can identify X∗ with X, so we can define Rγ on X. If X is additionally separable then, by the previous lemma, Rγ maps X into itself. For all x, y ∈ X we have the formula
⟨Rγx, y⟩ = ∫  ⟨x, ω⟩⟨y, ω⟩ γ(dω).
Rγ – considered as an operator on X – is self-adjoint, non-negative and nu- clear 11 .
Lemma 4.5 Let X be a separable Banach space and let γ ∈ G0(X). Let
{fn}n∈N ⊂ X	be a family of functions separating the points in X. (Such
a family always exists; this is a consequence of the Hahn-Banach-Theorem.)
Then X∗ coincides with the closure of the linear span of fn n∈N in the space
L2(γ).	 
Definition 4.6 Let (Ω, A, μ) be a measure space. We denote the completion
of A with respect to μ by Aμ, i.e.
Aμ := {A ∪ N : A ∈ A, (∃ J ∈ A) μ(J)= 0, N ⊂ J}.
Aμ is a σ-algebra.
Definition 4.7 Let X, Y be locally convex spaces and let μ be a measure on E(X). A (E(X)μ, E(Y ))-measurable mapping F : X → Y is called a μ- measurable linear mapping if there is a linear (in the usual sense) mapping F : X  Y such that F = F μ-a.e. For Y = R one also speaks of μ-measurable linear functionals.
Lemma 4.8 Let X be a separable Banach space. Then f is a γ-measurable linear functional if and only if f ∈ X∗.	 

11 An self-adjoint operator A on a separable Hilbert space is nuclear, if for every orthonormal basis {ei}i∈N the sum	⟨Aei, ei⟩ converges.

Lemma 4.9 Let X, Y be locally convex spaces and γ ∈ G0(X). Let F : X → Y be a linear and (E(X)γ, E(Y ))-measurable mapping. Then γ ◦ F −1 ∈ G0(Y ).	 
Lemma 4.10 Let X, Y be locally convex spaces equipped with σ-algebras A1 and A2 respectively. Let μ be a measure on A1. Let L ∈ A1 be a linear subspace of X such that μ(X \ L)= 0. Let F : L → Y be an (A1, A2)-measurable linear mapping. There is an (A1 , A2)-measurable linear mapping F0 : X → Y that coincides with F on L.
A representation theorem
Let X be a separable Banach space and γ ∈ G0(X). By Lemma 4.5 we can choose a complete orthonormal sequence (ei)i∈N in X consisting of elements of X∗. For every γ-measurable linear functional f , Lemma 4.8 and Lemma
4.3 yield that the series
ei(.)f (Rγ(ei))
i
is an orthogonal expansion of f in L2(γ). So the sequence (f (Rγei))i∈N is in l2. This, in combination with the fact that the ei are independent real stan- dard Gaussian random variables yields that the series also converges almost everywhere to f (cf. [1, Theorem 1.1.4]).
Now let Y be another separable Banach space and let F : X → Y be a γ-measurable linear mapping. For every f ∈ Y ∗ we have that f ◦ F is a γ-measurable linear functional. So for every f ∈ Y ∗
ei(.)(f ◦ F )(Rγ(ei))
i


we have
X  F  dγ < ∞, and so [19, Exercise V.3.4(b)] yields that the series
ei(.)F (Rγ(ei))
i

converges to F almost everywhere. As F has a Gaussian distribution we even
have ∫X  F	dγ < ∞ for every p ≥ 1. So Theorem V.3.3 in [19] (implica-
p
tion “3	4” for Φ(x) = xp) yields that the series also converges to F in
Lp(X, γ; Y ). We summarise:
Theorem 5.1 Let X, Y be separable Banach spaces. Let γ ∈ G0(X). Let
F : X → Y be a γ-measurable linear mapping. Let {en}n∈N ⊆ X∗ be a

complete orthonormal system in X∗. Then
∞
en(.)F (Rγ(en))
n=0
converges to F in Lp(X, γ; Y ) for all p ≥ 1 and γ-a.e.	 
Corollary 5.2 Let X, Y be separable Banach spaces. Let γ ∈ G0(X). Let S : D → Y be a linear measurable mapping deﬁned on a measurable linear subspace D of X with γ(D)= 1. Let {en}n∈N ⊆ X be a complete orthonormal
γ
∞
en(.)S(Rγ(en))
n=0
is well-deﬁned and converges to S in Lp(D, γ; Y ) for every p ≥ 1.
Proof. Lemma 4.10 yields that S has a (B(X)γ, B(Y ))-measurable extension F which coincides with S on D. So, by item (ii) of Lemma 4.3, S and F especially coincide on the Rγ(en). We get:


n−1
 S(x)
D	i=0
ei(x)S(Rγ(ei))  p
γ(dx)= 
X
n−1
 F (x)−
i=0
ei(x)F (Rγ(ei))  p
γ(dx).

Theorem 5.1 immediately yields the claim.	 
The following error formula already appears in [15, Section 6.5.3]:
Theorem 5.3 Let X be a separable Banach space and Y a separable Hilbert space. Let γ ∈ G0(X). Let F : X → Y be a γ-measurable linear mapping. Let e0,... en−1 ∈ X∗ be orthonormal in X∗. Then


n−1
 F (x)
X	j=0
ej(x)F (Rγ(ej))  2
γ(dx)= 
X
F (x) 2
n−1
γ(dx)−
j=0
 F (Rγ(ej))  2.

Proof. Let	ai	be a complete orthonormal sequence in Y . The ai,F (.) are γ-measurable linear functionals and hence elements of X∗. Pythagoras’
identity holds:

∫  ⟨ai,F (x)⟩− 


n−1 j=0
2
ej(x)⟨ai,F (Rγ(ej))⟩
∫




γ(dx)





n−1


By this equality and the Monotone Convergence Theorem we get


∫	F (x) −

∫ Σ


n−1 j=0

ej(x)F (Rγ(ej))  2
n−1



γ(dx)



=
i=0
∫  ⟨ai,F (x) −

n−1

j=0
ej(x)F (Rγ(ej))⟩2 γ(dx)



=
X
n−1
⟨ai,F (x)⟩− 
j=0
2
ej(x)⟨ai,F (Rγ(ej))⟩
γ(dx)



=
i=0
∫
  ∫X

⟨ai,F (x)⟩  γ(dx) −
n−1
n−1

j=0

⟨ai,F (Rγ(ej))⟩

=	F (x)  2 γ(dx)
X


j=0
F (Rγ(ej))  2.



Corollary 5.2 implies – translated into the language of information based complexity – that linear problems S : X  Y are solvable on the average with respect to Gaussian measures if X and Y are separable Banach spaces. 12 Of course we would like to benefit from this result in order to solve our model example of a linear ill-posed problem: X, Y are Hilbert spaces and S = T † for unbounded T †. Given (a program for) T and some prescribed error bound 2−m, can we compute approximations to T † such that the expected (quadratic) error with respect to some Gaussian measure γ with γ(dom T †) = 1 is smaller than 2−m? If we wish to apply the series formula from Theorem 5.1 directly, we encounter the problem that we have to compute elements T †(Rγei). But we know (see introduction) that this is not possible in general. Furthermore we need to determine how many summands of the series have to be computed in order to achieve the error bound. 13

12 In the IBC literature we have found this result only for Y being a separable Hilbert space; see [22],[16].
13 Problems of this kind are usually neglected in IBC: One does not demand an algorithm
that is uniform in the precision parameter. Furthermore, one allows an algorithm to make use of “precomputed” constants. This is sometimes justified by the belief that examples in which such constants are “very difficult to precompute” are “exceptional” (cf. [13, NR 2.9.5]).

In the forthcoming sections we will define computability on the space of linear γ-measurable mappings between computable real Hilbert spaces. We will then study the computability of T  T † as a mapping into this space. Our main discovery will be a “higher level” analogon to Corollary 3.10. Inter- estingly, this will be obtained by combining the ideas from this section, which have their origin in IBC, with the ideas from the first part of the paper, which have their origin in the theory of regularization.

Computability of γ-measurable linear mappings
Let (X, .  X, α), (Y, .  Y , β) be computable real Hilbert spaces. 14	In what follows we will denote the norms and inner products in both spaces by .  and
⟨., .⟩ respectively. Let γ be a centered Gaussian measure on X. Consider the set
C := {⟨a0, .⟩b0 + ... + ⟨an−1, .⟩bn−1 : n ∈ N, ai ∈ X, bi ∈ Y }.
The elements of C are linear mappings from X into Y . Their (finite) L2-norm with respect to γ is given by

⎛⎝∫

¨Σn−1


⟨ai, x⟩bi¨

1/2
γ(dx)⎠

  Σn−1


⟨bi, bj⟩ ∫

⟨ai, x⟩⟨aj, x⟩ γ(dx)

  1/2

X	i=0
i,j=0
  Σn−1

X
  1/2


(1)


Let L(X, Y )γ denote the closure of C in L2(X, γ; Y ). If Y = R then L(X, Y )γ is just X . Theorem 5.1 yields that every γ-measurable linear mapping F : X → Y is in L(X, Y )γ. 15
Our aim is to define a fundamental sequence Γ : N → L(X, Y )γ in L(X, Y )γ. As L(X, Y )γ is the closure of C, we have that the linear span of
C' := {⟨a, .⟩b : a ∈ X, b ∈ Y }
is dense in L(X, Y )γ. This implies that any sequence whose linear span is dense in this set is fundamental in L(X, Y )γ. We show that the span of range Γ with
Γ(⟨i, j⟩) := ⟨α(i), .⟩β(j)

14 α and β are notations of fundamental sequences. A sequence is called fundamental in a topological vector space if the span of its elements is dense in that space.
15 For Y = R the converse is also true by Lemma 4.8; the proof of this fact (see Theorem
2.10.9 in [1]) also works for arbitrary Y . So (X, Y )γ is exactly the space of all γ-measurable linear mappings from X to Y .

is dense in C'. In fact: For arbitrary a, a ∈ X and b, b ∈ Y we have the identity
 ⟨a, .⟩b − ⟨˜a, .⟩˜b  2 2	= b  2⟨Rγa, a⟩− 2⟨b, ˜b⟩⟨Rγa, ˜a⟩ + ˜b  2⟨Rγ˜a, ˜a⟩ (2)
which clearly shows that  ⟨a, .⟩b − ⟨a, .⟩b  L2(X,γ;Y ) → 0 as a → a, b → b (remember that Rγ is continuous). As range α is fundamental in X and range β is fundamental in Y we hence have that
{⟨a, .⟩b : a ∈ span range α, b ∈ span range β}
is dense in C'. But one only has to use the linearity of the inner product to see that this set in fact is the span of range Γ.	 
We are now in the position to consider the triple
(L(X, Y )γ, .  L2(X,γ;Y ), Γ)
and ask whether it is a computable normed space (and hence a computable Hilbert space). The only remaining prerequisite is that the mapping
N → [0, ∞),

⟨n, i0,... , in, j0,... , jn, ⟩ '→ 
n−1
νQ(ik )Γ(jk )  L2(X,γ;Y )
k=0

is computable. After one has applied formula (1) and the definition of Γ one directly sees that this will be fulfilled if (i, j) '→ ⟨Rγα(i), α(j)⟩ is computable. 
We summarise:
Theorem 6.1 Let (X, .  X, α), (Y, .  Y , β) be computable real Hilbert spaces. Let γ ∈ G0(X) be a Gaussian measure for which (⟨Rγα(i), α(j)⟩)i,j∈N is a computable double sequence. Let Γ: N → L(X, Y )γ be deﬁned by
Γ(⟨i, j⟩) := ⟨α(i), .⟩β(j).


then
(L(X, Y )γ, .  L2(X,γ;Y ), Γ)

is a computable Hilbert space. We denote the associated Cauchy representation by δγ.	 
Remark 6.2 If D is a linear subspace of X with γ(D) = 1, we can consider every measurable linear mapping S : D → Y as an element of L(X, Y )γ via Lemma 4.10. We can hence also use δγ to represent such mappings. We will do so in Theorem 7.4.

Remark 6.3 Our representation of L(X, Y )γ has some formal similarity to Brattka and Yoshikawa’s [6] representation of K(X, Y ) mentioned in Remark 3.8.
Lemma 6.4 Let the space (L(X, Y )γ, .  L2(X,γ;Y ), Γ) be as in Theorem 6.1.
The form
X × X → R, (x, y) '→ ⟨Rγx, y⟩
is (δX, δX, ρ)-computable.
The mapping
Embed : X × Y → L(X, Y )γ,
Embed(a, b) := ⟨a, .⟩b,
is (δX, δY , δγ)-computable.
Proof.
Let (xi)i∈N and (yj)j∈N be sequences converging rapidly to x, y respectively. We have the estimate
|⟨Rγx, y⟩− ⟨Rγxi, yj⟩| ≤ |⟨Rγx, y − yj⟩| + |⟨Rγ(x − xi), yj⟩|
≤   Rγ  (  x	y − yj   +  x − xi	yj  ).
The rest of the proof is standard.
Let (ai)i∈N and (bi)i∈N be sequences converging rapidly to a, b respectively. Equation (2) implies that ⟨ai, .⟩bi → ⟨a, .⟩b. In connection with item (i),
(2) also provides a way to compute i	a, . b	ai, . bi . The rest of the proof is standard.

Main result
Before we prove our main result we need one last auxiliary lemma:
Lemma 7.1 Let (X, .  , α) be a computable Hilbert space over K ∈ {R, C}.
One can effectively enumerate a sequence (ij)j∈N ∈ N	and a sequence
(c(0)), (c(1), c(1)), (c(2), c(2), c(2)),... 
0	0	1	0	1	2


of tuples of elements of K such that
k
c(k)α(ij)
j=0





k∈N

is a complete orthonormal sequence in X.
Proof. The algorithm layed down in [14, Section 4.7] (to prove the Effec- tive Independence Lemma) can be applied to obtain a sequence (ij)k∈N ∈ NN such that (α(ij))j∈N is a linearly independent dense sequence in X. Then, by the classical Gram-Schmidt algorithm, we can find a suitable sequence of coefficient vectors.	 
Corollary 7.2 In every computable Hilbert space there exists a computable complete orthonormal sequence.	 
Corollary 7.3 Let (X, .  , α) be a computable Hilbert space and let γ be a centered Gaussian measure on X for which (⟨Rγα(i), α(j)⟩)i,j∈N is a com- putable double sequence. There exists a computable sequence (ek)k∈N ∈ XN such that the sequence (⟨ek, .⟩)k∈N is a complete orthonormal system in X∗.
Proof.  First	remember	that	X∗	=	L(X, R)γ	and	note	that
( (X, R)γ, .  L2(X,γ;Y ), Γ) is  a  computable  Hilbert  space  by  Theorem
6.1. Via Lemma 7.1 we obtain a sequence


k


j=0
c(k)Γ(ij)



k∈N

that is complete orthonormal in X∗. Consider the definition of Γ to note that this is in fact a sequence in C with elements of the form


(k)
(k)
(k)
(k)

⟨a0 , .⟩b0  + ... + ⟨aνk , .⟩bνk
such that we can compute the a(k) ∈ X, b(k) ∈ R. Hence we can also compute
i	i
the
νk

i	i
i=0

Theorem 7.4 Let (Y, .  Y , β), (X, .  X, α) be computable Hilbert spaces. Let γ be a centered Gaussian measure on Y such that (⟨Rγβ(i), β(j)⟩)i,j∈N is a computable double sequence. Let ( (Y, X)γ, . L2(Y,γ;X), Γ) be the computable Hilbert space of Theorem 6.1. Deﬁne a set
A3 := {(T, c) ∈ L(X, Y ) × R  : γ(dom T †)= 1, c = ∫	T †  2 dγ}.

The mapping
GI3 : A3 → L(Y, X)γ, (T, c) '→ T †,

is ([Δ→
, ρ>]|A3 , δγ)-computable.


Proof. By Corollary 7.3 there is a computable sequence (ek)k∈N ∈ Y N such that the sequence (⟨ek, .⟩)k∈N is complete orthonormal in Y ∗. By Corollary 7.2 there is a computable complete orthonormal sequence (aj)j∈N in X. Let the (fk)k∈N be as in Corollary 3.2. Put

Ai,k,j := ⟨fk(T ∗T )T ∗Rγei, aj⟩.

By the self-adjointness of fk(T ∗T ) we have

Ai,k,j = ⟨Rγei,Tfk(T ∗T )aj⟩

which yields that the mapping (T, i, k, j) '→ Ai,k,j is (Δ→ , νN, νN, νN, ρ)-
computable (see Theorem 3.6 and Lemma 6.4(i)).
For the proof of the theorem it is sufficient to show that we can compute a sequence of (δγ-names of) elements Fm  L2(Y, γ; X) such that Fm is a 2−m-approximation of T †. Let us fix some m now.
Step 1. By Theorem 5.3 we have that

n−1	n−1

 T † − Σ⟨ei, .⟩T †Rγei  2 2 = T †  2 2 − Σ

 T †Rγei  2


(where we have put  .  L2 :=  .  L2(Y,γ;X) for convenience) and Corollary 5.2 yields that these expressions converge to zero as n → ∞. We have

† 2
L2

∞
=
i=0
∞

 T †Rγei  2

=	lim
k→∞
 fk(T ∗T )T ∗Rγei  2

i=0
∞	∞
= Σ lim Σ |Ai,k,j|2.
i=0	j=0

The input to our algorithm contains a list of all rational upper bounds of

c = T †  2 2 . By exhaustive search we can hence find n0, k0, r0 such that

n0−1 r0−1

	 L
i=0 n0−1

0
j=0

† 2
L
i=0 n0−1
† 2
L
 fk (T ∗T )T ∗Rγei  2

 T †Rγei  2

i=0 n0−1
†	†	2
L
i=0

where we have used Corollary 3.2(ii) for the third estimate.
It is hence sufficient to compute

n0−1
⟨ei, .⟩T †Rγei ∈ L(Y, X)γ.
i=0

In view of Lemma 6.4(ii) it is now sufficient to show that we can compute the elements T †Rγei,0 ≤ i ≤ n0 in X.
Step 2.  We show that we can even compute the whole sequence (T †Rγei)i∈N. To that aim we again exploit the information provided by the input c.
Let an arbitrary m' N be given. We show how to compute a 2−m' - approximation to any T †(Rγ(ei)). We put

∞
Ai,k := fk(T ∗T )T ∗Rγei =	Ai,k,jaj.
j=0

By Corollary 3.2(iii) we have

 T †Rγei − Ai,k  2 ≤  T †Rγei  2 − Ai,k  2.


By repeating Step 1 with m' +1 instead of m we effectively find n' , k' , r'
such

0	0	0

that


2−2(m'+1)

≥  T L2


n' −1 r' −1

i=0 j=0



|Ai,k' ,j |


∞
=	T †(Rγ(ei))  2 +
'
0
Σ 
n' −1 ⎡ 
i=0
n' −1 ⎡ 
r' −1

j=0


|Ai,k' ,j |2⎤⎦







r' −1	⎤

 	



∞
=	T †(Rγ(ei))  2 +
'
0
n' −1
2
'
0	0
i=0
r' −1

j=0
Ai,k' ,jaj  2⎤⎦


∞
 T †(Rγ(ei))  2 +
'
0
n' −1
2
'
0	0
i=0
r' −1

j=0
Ai,k' ,jaj  2⎤⎦



In particular,

as well as

 T †Rγei  2

−2(m'+1)

for all i ≥ n'




 T Rγei − Ai,k'

  + Ai,k' −
r' −1

j=0

Ai,k' ,jaj  2

−2(m'+1)

for all i < n' .

The first estimate immediately yields that we can take 0 as 2−m' -

approximation for all T †Rγei with i ≥ n' . For any i < n'
put (for brevity)

0	0
'
†	0

x1 := T
Rγei, x2 := Ai,k'  and x3 :=	j=0 Ai,k' ,jaj. Then, from the second

estimate we conclude that
 x1 − x3  ≤  x1 − x2  + x2 − x3 

=	x1 − x2   + x2 − x3
2
1

  +2  x1 − x2  · x2 − x3 
 2  1

≤ 2( x1 − x2	+ x2 − x3	)




Σr' −1
−2m'−1


−m'

1

2	'
≤ 2	.

†	'

References
Bogachev, V. I., “Gaussian Measures,” Amer. Math. Soc., Providence, RI, 1998.
Brattka, V., Computable invariance., Theor. Comput. Sci. 210 (1999), pp. 3–20.
Brattka, V., Computability of Banach space principles, Technical Report 286, Fern-Universit¨at Hagen, Fachbereich Informatik, Hagen (2001).
URL http://cca-net.de/vasco/publications/banach.html

Brattka, V., The inversion problem for computable linear operators., in: H. Alt and M. Habib, editors, STACS, Lecture Notes in Computer Science 2607 (2003), pp. 391–402.
Brattka, V. and R. Dillhage, Computability of the spectrum of self-adjoint operators., J. UCS
11 (2005), pp. 1884–1900.
Brattka, V. and A. Yoshikawa, Towards computability of elliptic boundary value problems in variational formulation, accepted for publication in Journal of Complexity.
Dillhage, R., “Berechenbarkeit des Spektrums linearer Operatoren,” Diplomarbeit, Fachbereich Informatik, FernUniversitt Hagen (2005).
Engl, H. W., M. Hanke and A. Neubauer, “Regularization of Inverse Problems,” Kluwer, 1996.
Groetsch, C. W., “Generalized Inverses of Linear Operators,” Dekker, New-York, 1977.
Groetsch, C. W., “Elements of Applicable Functional Analysis,” Dekker, New York, 1980.
Groetsch, C. W., “Inverse Problems in the Mathematical Sciences,” Vieweg, Braunschweig, Wiesbaden, 1993.
Kon, M. A., K. Ritter and A. G. Werschulz, On the average case solvability of ill-posed problems., J. Complexity 7 (1991), pp. 220–224.
Plaskota, L., “Noisy Information and Computational Complexity,” Cambridge University Press, Cambridge, 1996.
Pour-El, M. B. and J. I. Richards, “Computability in Analysis and Physics,” Springer, Berlin, 1989.
Traub, J. F., G. W. Wasilkowski and H. Wo´zniakowski, “Information-based complexity,” Academic Press, New York, 1988.
Traub, J. F. and A. G. Werschulz, Linear ill-posed problems are solvable on the average for all Gaussian measures., Math. Intelligencer 16 (1994), pp. 42–48.
Traub, J. F. and A. G. Werschulz, “Complexity and Information,” Cambridge University Press, New York, NY, USA, 1999.
Vakhania, N. N., Gaussian mean boundedness of densely deﬁned linear operators., J. Complexity 7 (1991), pp. 225–231.
Vakhania, N. N., V. I. Tarieladze and S. A. Chobanian, “Probability Distributions on Banach Spaces,” Kluwer Academic, Dordrecht/Norwell, MA, 1987.
Weihrauch, K., “Computable Analysis: An Introduction,” Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2000.
Werschulz, A. G., What is the complexity of ill-posed problems?, Numerical Functional Analysis and Optimization 9 (1987), pp. 945–967.
Werschulz, A. G., “The Computational Complexity of Differential and Integral Equations: An Information-Based Approach,” Oxford University Press, New York, 1991.
