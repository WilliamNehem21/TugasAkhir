Today, organizations are confronted with the complex task of balancing the adoption of new technology with the protection of their systems and data that underpin critical business operations. Despite significant advancements in security technology, attackers continue to compromise organizational systems and gain unauthorized access. As organizations increasingly integrate new devices and services, such as the Internet of Things, machine learning, and cloud computing, their security teams face the challenge of effectively securing these new connections while preventing potential security breaches.

Our approach draws upon research on human intentions, attitudes, norms, and cybersecurity decision-making behaviors. It is well-established that usable security presents a significant challenge, as users often disregard warnings, misunderstand risks, and have experienced numerous false alarms, leading to rational rejection of security alerts. Furthermore, designing human-computer interaction interfaces is difficult, and inherent cognitive weaknesses can impact security decisions. Individual users also exhibit different behaviors in response to security decisions based on their background, culture, and attitudes.

The first step in our approach involves selecting security behaviors through a literature review to identify expected security behaviors. Once the behaviors are identified, we model them using a formal method-based approach and then label them using formal specifications. Subsequently, user-specific security policies are generated based on the labeled behaviors.

Our primary contribution lies in the design of a formal method-based framework to facilitate the generation of user-specific cybersecurity policies. We systematically identify and select the essential characteristics that define users' security behavior, model these behaviors as formal models for automated reasoning, and detect weaknesses in users' security behavior to propose relevant policies.

The architecture representing knowledge about security behaviors includes different levels of abstractions to enhance readability, maintainability, and reduce complexity. We decompose the structure into different security services on multiple layers, starting from the highest level of abstraction to the lowest. This approach clarifies the security aspects employed for specific dimensions.

Layer (1) represents the most abstract of the security service check layers, assimilating concepts such as device securement, password generation, proactive awareness, and updating. These concepts correspond to the highest level of representation of security tasks.

In conducting experiments, we first set up the experimental setup, which involved modeling the selected security behaviors in timed automata. Subsequently, we conducted experiments to label good and bad security behaviors.

Our model design focuses on formally checking the existence of good and bad user security behaviors in TCTL, guided by measures taken from academic literature. We selected critical security matters within different settings and developed a selection of examples differentiating good decisions from poor ones using behavioral measurement scales in existing literature.

We included rules in the knowledge base to address users' tendencies to store passwords insecurely and develop coping mechanisms to manage the burden of remembering passwords. Additionally, we modeled several test cases with different security behaviors as finite-state automata and classified the behaviors as good or bad based on the results of reachability analysis. Finally, we generated relevant security permissions based on the behavior classification.