The recent progress in artificial intelligence, particularly in the realm of deep learning techniques, has breathed new life into the exploration of chemical space. Compared to conventional methods that rely on chemical fragments and combinatorial recombination, deep generative models produce molecules in an opaque manner that defies simple rationalization. However, this non-transparent nature also holds the promise of uncovering uncharted chemical space in innovative ways that do not depend directly on structural similarity. The complexity of training such models, along with the need to assess novelty, uniqueness, and distribution of generated molecules, are central considerations. This perspective provides an overview of current methods for exploring chemical space with a focus on deep neural network approaches. Key aspects of generative models include the selection of molecular representation, the target chemical space, and the methodology for evaluating and validating coverage of chemical space.

In terms of quality, for general chemical space exploration, it is preferable for each molecule in the pertinent chemical space to have roughly the same probability of being generated, or for any bias towards certain molecules to be reasonable (e.g., larger molecules being less likely to be generated than smaller ones). On the other hand, exploration of chemical space is often conducted with specific goals in mind. Just as in in vitro and in vivo exploration of chemical space in lead optimization campaigns aims to optimize endpoints such as potency or ADME-T properties, computational exploration of chemical space can be carried out with respect to particular goals focusing on molecules with favorable properties for specific endpoints of interest.

Certain aspects such as validity and novelty can be easily quantified using statistical metrics. In addition, computational metrics such as the synthetic accessibility score and the quantitative estimate of drug-likeness can provide indications of the reasonableness and practicality of proposed structures. The representativeness of a chemical space can be assessed by determining distributions of structural and physicochemical properties, while the biological relevance of a chemical space is characterized by the extent to which it is enriched with molecules possessing favorable properties tailored to a specific target. For generative models targeting a focused chemical space, such as chemical spaces containing molecules with specific biological activities or synthetically accessible chemical spaces, traditional benchmark approaches are not applicable, as the chemical space to be covered by generative models can be extremely large. Therefore, it should not be expected that a generative model is able to reproduce molecules from a hold-out test set. Conversely, the recreation of known molecules might indicate that the covered chemical space is relatively small and overpopulated with molecules from already explored chemical spaces.

In a sample of n molecules, it can be expected to observe approximately 37% duplicates. Since the training set size usually represents a small fraction of the targeted chemical space, ideally the fraction of novel molecules should exceed 99%. Low novelty can indicate overfitting, and low uniqueness can suggest a lack of diversity in the generated molecules, known as mode collapse, which is caused by a concentration of the probability distribution around a few molecules. While a generative model should aim to maximize these metrics, its potential to do so might be limited if the size of the targeted chemical space is small, for example, when exploring chemical spaces around specific scaffolds.

Generative models were used to sample one billion compounds each, and the best resulting models achieved a coverage of 39%. According to the preceding argument, a coverage of around 63% would theoretically be the best performance that could be expected, assuming perfect coverage and identical probabilities for each molecule. Furthermore, given that the generative models also produced a significant portion of molecules not covered by the GDB-13, this indicates that the models are indeed able to cover a significant portion of the entire GDB-13.

Analysis of latent space representations can provide insights into how similarity is perceived by a deep generative model. For example, in one study, the relationship between neighborhoods in the latent space and structural similarity of exemplary compounds was investigated. A more recent approach used generative topographic mapping to map the latent space of an autoencoder onto two dimensions, which the authors then used to construct an activity map of adenosine A2A receptors, visualizing how these active compounds were distributed in the latent space.

In another study, input saliency maps of SMILES were used to aid the interpretability of the generative process. These maps assign a score to each token indicating its relevance for the next token to be generated. By mapping the tokens to molecular structures, it becomes possible to interpret them in a chemical context.