For SpectralNet to yield accurate results, it necessitates an affinity matrix that contains comprehensive information about the clusters. Ideally, points within the same cluster should be linked by an edge with a high weight, while points from different clusters should be connected by an edge with a low weight or no weight (represented by a zero entry in the affinity matrix).

SpectralNet utilizes siamese networks to learn informative weights that ensure effective clustering outcomes. However, these networks require prior information in the form of labeled positive and negative pairs. A negative label denotes points from different clusters, whereas a positive label indicates points from the same cluster. Acquiring negative and positive pairs can be accomplished in a semi-supervised or unsupervised manner, with the authors of SpectralNet having implemented both methods. Utilizing ground-truth labels to assign negative and positive labels renders SpectralNet semi-supervised, while using a distance metric to label closer points as positive pairs and farther points as negative pairs makes SpectralNet unsupervised. This study focuses on the unsupervised implementation of SpectralNet.

Furthermore, another issue with Graph Convolutional Networks (GCN) is their susceptibility to adversarial attacks. To address this, Yang et al. applied GCN with domain adaptive learning, which aims to transfer knowledge from a labeled source graph to an unlabeled target graph, allowing for the classification of previously unseen nodes in the target graph.

The output is clustered using a Gaussian Mixture Model (GMM), with the GMM parameters being updated during training. Similarly, Wang et al. employed autoencoders to learn a latent representation, followed by employing the UMAP manifold learning technique to find a low-dimensional space, from which the final clustering and cluster labels are derived. Another approach utilizing deep learning for spectral clustering was proposed by Wada et al., where hub points are identified as the core of clusters and passed to a deep network to assign cluster labels to the remaining points.

Storage efficiency was assessed based on the total number of pairs used, avoiding machine-dependent metrics such as running time. Additional experiments were conducted to explore the impact of rpTree parameters on the similarity metric based on rpTree. The findings indicated that connecting to two neighbors was insufficient for accurately detecting clusters. Yan et al. reported similar results, demonstrating that clustering using rpTree similarity outperformed clustering using a Gaussian kernel with Euclidean distance. They presented a heatmap of the similarity matrix generated by both methods.

Future work could involve altering the method by which pairwise similarity is computed within the siamese network, potentially exploring alternative random projection methods such as random projection forests (rpForest) or rpTrees with reduced space complexity. This exploration could provide valuable insights into the performance of space-partitioning trees for clustering in deep networks.