All previous methods have focused on addressing the static version of the problem being studied. In this paper, we are specifically interested in addressing the dynamic version of the scenario, where the graph can change over time due to the addition, deletion, or alteration of edges and nodes. We propose new algorithms in this study that can dynamically maintain geometric containers as the weights of edges are modified, covering cases such as edge deletions and insertions. Additionally, we present experimental findings using real-world railway data, demonstrating that the new algorithms are 2-3 times faster than the naive approach of recomputing geometric containers from scratch.

In a graph without multiple edges, the number of edges can reach up to n^2, where n represents the number of nodes. We categorize a graph as sparse if the number of edges, denoted as m, is within o(n), and we refer to a graph as large if its memory consumption is limited to o(n). Notably, for large sparse graphs, the space required for n^2 edges is not feasible.

It should be noted that additional nodes may be part of a target container; however, at a minimum, the nodes reachable by a shortest path originating from e must be included in t(e). We will refer to these additional nodes as "wrong nodes," as they lead us in the wrong direction.

The paper provides proof that the edge (x, y) must be part of the new path pnew. Let psy denote the sub-path of pnew from s to v. As a sub-path of a shortest path, psy is also a shortest path, specifically one that concludes with the edge (x, y). For similar reasons, the initial edge of a shortest x-t-path is (x, y).

Further, the paper offers proof that since wnew(x, y) < wold(x, y), the new distance dnew(s, t) must be shorter than the old distance dold(s, t). The new shortest path pnew contains the edge (x, y), in contrast to the old shortest path from s to t.

In the evaluation process, for each graph, we assign a large value to the weight of 100 random edges, effectively simulating the removal of these edges from the graph. After each weight change, the containers are updated in accordance with section 4.1. Furthermore, a second set of containers is created from scratch to assess quality and compare computation time.

In assessing the impact of decreasing edge weights, we begin with a graph where 100 random edges have been assigned a large weight, and subsequently decrease the weights to their original values. Once again, updated containers are compared to newly computed containers.

Other potential simplifications are worth exploring to ensure consistent containers while achieving a balance between optimality and running time. Our findings also indicate the possibility of achieving a speed-up factor of approximately 2 with an optimally provable update strategy. Additionally, it may be feasible to combine edge weight increases and decreases within a single algorithm.