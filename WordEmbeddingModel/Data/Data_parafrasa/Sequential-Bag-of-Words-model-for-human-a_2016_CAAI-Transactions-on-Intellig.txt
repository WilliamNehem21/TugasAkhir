Zhao et al. developed the Local Binary Pattern on Three Orthogonal Planes (LBP-TOP). This method has been successfully applied for dynamic texture description and recognition, particularly in facial expression analysis. The algorithm extracts the LBP while encoding spatial-temporal co-occurrence information in the XT and YT planes. Shao et al. proposed an extension to the computation of LBP, resulting in extended LBP-TOP and extended CSLBPTOP, which can extract more dynamic information and be applied to gradient cuboids.

The paper is structured as follows: Section 2 discusses the rationale for using sub-actions and outlines the framework of the approach, while Sections 3 and 4 describe the segmentation and classification approaches, respectively. In Section 5, experiments are conducted on UT-Interaction and Rochester datasets, and the approach is compared with other Bag of Words-based approaches. Finally, conclusions are drawn in Section 6.

To achieve efficient action segmentation, two goals need to be met. First, sub-actions within the same sub-section of the same action class should be of the same type, regardless of speed differences between actors. Second, all sub-actions should capture enough motion information for classification. A two-stage segmentation approach is proposed to achieve these goals.

Instead of using the number of frames, point density is utilized to segment actions further, ensuring that each clip contains enough motion information. Dense feature points are indicative of strenuous action, and the intensity of the action may vary over the entire process. The salience maps are robust to inner-class variation and environmental change, with the approach proving efficient for segmentation and salience calculation.

In experiments, cuboid size is defined as w=h=1 pixel, t=2 frames. Descriptors such as 3D-SIFT and gradient descriptor are used, with feature extraction results favoring the gradient descriptor due to its ability to describe sub-actions more comprehensively. The proposed approach demonstrates superior performance on UT-Interaction Scene-2 and Rochester datasets, with comparable but faster results on UT-Interaction Scene-1 compared to existing approaches.

The authors acknowledge financial support from various sources and highlight their recognition and involvement in prestigious academic and professional organizations. Additionally, the paper provides brief biographies of the contributing researchers, highlighting their academic backgrounds and research interests.