The precision of machine learning models in predicting protein-ligand binding affinity relies on the quality of the experimental data used for their training. Many of these models are trained and tested using different subsets of the pdbbind database, which is the primary source of protein-ligand complexes with documented binding affinity in the public domain. However, assessing the experimental uncertainty of this data is challenging due to the limited number of protein-ligand complexes with multiple associated measurements. In this study, the authors analyze bioactivity data from chembl to estimate the experimental uncertainty related to three binding affinity measures (ki, kd, and ic50) included in the pdbbind. They also examine the impact of combining these measures. The analysis reveals that the mean absolute error for the combined affinity measures is 0.78 logarithmic units, the root mean square error is 1.04, and the Pearson correlation coefficient is 0.76. These estimates are compared with the performance of state-of-the-art machine learning models for binding affinity prediction, suggesting that these models tend to produce overly optimistic results when evaluated on the core set from pdbbind.

Scoring functions can be divided into classical and machine-learning based scoring functions. The former assumes a functional form to establish relationships between features characterizing a protein-ligand complex and its binding affinity, while the latter learns these relationships through a machine-learning algorithm. Although machine-learning based scoring functions have shown superior performance compared to classical scoring functions for protein-ligand complexes with documented binding affinity, they have biases towards preferred crystallographic targets. Additionally, the data for binding affinity come from various sources, leading to inherent experimental uncertainty that may limit the performance of machine-learning models trained on such data.

Estimating experimental uncertainty in the pdbbind database is challenging due to the limited number of protein-ligand complexes with multiple associated measurements and the bias towards preferred crystallographic targets. An alternative approach for estimating experimental uncertainty is to analyze data from larger bioactivity databases where structural information of the protein-ligand complexes is unavailable, such as chembl. The authors in this study analyze the experimental uncertainty of ki measurements from chembl version 12, which provides more comprehensive data compared to pdbbind. However, pdbbind includes not only ki but also kd and ic50 measurements, and most machine-learning models derived from such data treat them as the same target variable. Although this is a reasonable approximation, combining different types of binding affinity estimations would only increase the uncertainty of predictions.

The dataset is divided based on the binding affinity measure included (ki, kd, or ic50), and in each subset, any repeated bioactivity values from multiple citations are eliminated. Only protein-ligand pairs with at least two independent measures are considered for further analysis. The effect of combining binding affinity measures on experimental uncertainty is also studied, generating subsets containing pairwise combinations between binding affinity measures (ki-kd, ki-ic50, kd-ic50). In the case of multiple measures for an affinity measure, only the highest affinity is preserved, and only protein pairs with at least one measurement of each affinity measure are considered.

The machine-learning models' performance is compared with the statistics calculated for the dataset containing all three binding affinity measures. The values reported for these models are higher than those for the dataset, indicating that machine learning models tend to be overly optimistic when assessing the general behavior of binding affinity values for protein-ligand pairs, although the individual predictions are very close to the experimental uncertainty observed for the dataset. The comparison between the models and the obtained statistics is not straightforward due to differences in the size and composition of the datasets. Considering the size and biases of the core set from pdbbind, such statistics are not surprising.

Experimental uncertainty of binding affinity measures from different sources is estimated, suggesting that combining kd data from distinct sources results in the lowest experimental uncertainty, with a mean absolute error of 0.69 logarithmic units, a root mean square error of 1.03, and a Pearson correlation coefficient of 0.76. Combining different binding affinity measures increases uncertainty, with the lowest uncertainty associated with the combination of ki and ic50 measures, achieving predictions close to experimental precision. This aligns with the results obtained by modeling different levels of noise on synthetic numerical data. Metrics such as the fractional gross error, the variance accounted for, and the mean normalized bias appear to be promising choices for estimating the performance of predictive models, as they are more sensitive to the noise present in the data. This study aims to contribute to the ongoing discussion within the scientific community about the limitations and applicability of machine learning models for binding affinity prediction trained on diverse binding affinity data.