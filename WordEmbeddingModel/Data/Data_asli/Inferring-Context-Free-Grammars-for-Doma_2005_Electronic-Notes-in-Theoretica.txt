machine learning of grammars finds many applications in software engineering, syntactic pattern recognition, computational biology, computational linguistic, speech recognition, natural language acquisition, etc. for example, software engineers usually want to recover grammar from legacy systems in order to automatically generate various software analysis and modification tools. usually in this case the grammar can be semi-automatically recovered from compilers and language manuals. in application areas outside software engineering, grammars are mainly used as an efficient representation of artifacts that are inherently structural and/or recursive(e.g. neural networks, structured data and patterns). here compilers and manuals do not exist and semi-automatic grammar recovery as suggested in is not possible. the grammar needs to be extracted solely from artifacts represented as sentences/programs written in some unknown language.



our work is also related to renovation and legacy systems where renovation tools can be rapidly built once a grammar is available. however, current grammar inference techniques are not able to infer grammars of general-purpose programming languages(e.g. cobol). by using the approach presented in this paper it is possible to infer grammars for small domain-specific languages. the paper is organized as follows: section 2 gives a short overview of the seminal results in the grammar inference literature. details of the genetic



mars is a semi-automatic inference system in the area of domainspecific modeling(dsm). dsm is an example of model-driven software engineering where domain experts can use high-level specifications to describe the solution of a problem in their domain using domain concepts. the motivation of the mars project was to address the issue of metamodel drift, which occurs when instance models in a repository are separated from their defining metamodel. making use of already existing tools along with new grammar inference algorithms, the mars system recovers metamodels that correctly define the mined instance models.



many grammars can be concocted which reject the negative samples. however, our search converges to the desired grammar better when we obtain grammars which accept the positive samples. hence, it is a natural move to search in the space of all grammars which accept the positive samples, only. negative samples are only taken into account when a grammar is capable of accepting all the positive samples. another reason is that negative samples are needed mainly to prevent overgeneralization of grammars. keeping these facts in view, the fitness value of each grammar is defined to be between 0 and 1, where interval 0.. 0.5 denotes that the grammar did not recognize all positive samples and interval 0.5.. 1 denotes that the grammar recognized all positive samples and did not reject all negative samples. a grammar with fitness value of 1 signifies that the generated lr(1) parser successfully parsed all positive samples and rejected all negative samples. for the given grammar[i] its fitness fj(grammar[i]) on the j-fitness case is defined as:



previous attempts at learning context-free grammars resulted in ineffectual success on real examples. we extended this work by introducing grammarspecific heuristic operators and facilitating better construction of the initial population where knowledge from positive samples has been exploited. our future work involves exploring the use of data mining techniques in grammar inference, augmenting the brute force approach with heuristics, and investigating the support vector machine(svm) classification technique for context-free grammar inference.



