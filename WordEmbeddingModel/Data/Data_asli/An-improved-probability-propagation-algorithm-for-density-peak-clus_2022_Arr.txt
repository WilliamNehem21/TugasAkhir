make the algorithm sensitive to parameters and only applicable for some specific datasets. to overcome the limitations of dpc, this paper presents an improved probability propagation algorithm for density peak clustering based on the natural nearest neighborhood(dpc-ppnnn). by introducing the idea of natural nearest neighborhood and probability propagation, dpc-ppnnn realizes the nonparametric clustering process and makes the algorithm applicable for more complex datasets. in experiments on several datasets, dpc-ppnnn is shown to outperform dpc, k-means and dbscan.



clustering, also known as unsupervised classification, aims to divide datasets into subsets or clusters according to the similarity measure of the data sample(physical or abstract) such that the data samples within the subset or cluster have a high degree of similarity and that the data samples belonging to different subsets or clusters have a high degree of dissimilarity. currently, cluster analysis plays an important role in many fields such as social sciences, biology, pattern recognition, information retrieval and so on. it is so useful in machine learning and data mining that many researchers have paid much attention to it. over the past few decades, a number of excellent clustering algorithms have been developed for different types of applications. typical algorithms include k-means and k-medoids based on partitioning, cure and birch based on hierarchy, dbscan and optics based on density, wavecluster and sting based on grids and statistical clustering based on models.



the second aspect is to automatically recognize the numbers of clusters and cluster centers. liang and chen proposed 3dc, which introduces a divide-and-conquer strategy to determine the ideal number of clusters. however, it ignores the local structure of the datasets which may cause missing clusters. xu, wang and deng proposed denpehc, which could automatically detect all possible centers and build a hierarchy presentation for the dataset. nevertheless, both 3dc and denpehc will aggravate the propagation of errors due to the hierarchical clustering strategy. li, ge, and su proposed an automatic clustering algorithm for determining the density of clustering centers. in this algorithm, it is considered that if the shortest distance between a potential cluster center and a known cluster center is less than the nearest neighbors and the propagation will continue until there is no neighbor that can be infected. at this time, we recognize all the infected data points as a cluster. we call the processes of forming a cluster a round of propagations. second, we select the data point that has the



in this part, we select a number of synthetic datasets that are widely used to test the performance of clustering algorithms. these datasets are different in terms of the distribution and numbers of points and clusters. they can simulate different situations to compare the performance of various clustering algorithms in different scenarios.



and dbscan can detect the clusters in the cassini dataset. although dpc can find the correct cluster centers, it fails to allocate the other remaining data points correctly. the three clusters are not uniform in shape which leads to the wrong cluster results by k-means.



