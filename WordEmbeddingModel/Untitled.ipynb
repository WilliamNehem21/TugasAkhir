{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5a2c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import fasttext\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06436f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file asli dari folder\n",
    "directory_path_real = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap7(DataCleaning)'\n",
    "\n",
    "files_real = os.listdir(directory_path_real)\n",
    "list_files_real = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_real:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_real.append(file)\n",
    "        \n",
    "all_text_real = \"\"\n",
    "for file in list_files_real:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap7(DataCleaning)/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_real += content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ceb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file parafrasa dari folder\n",
    "directory_path_paraphrased = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap9(ParaphrasedPaper)'\n",
    "\n",
    "files_paraphrased = os.listdir(directory_path_paraphrased)\n",
    "list_files_paraphrased = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_paraphrased:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_paraphrased.append(file)\n",
    "        \n",
    "all_text_paraphrased = \"\"\n",
    "for file in list_files_paraphrased:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap9(ParaphrasedPaper)/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_paraphrased += content\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a7eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan lowercase lagi\n",
    "all_text_real = all_text_real.lower()\n",
    "all_text_paraphrased = all_text_paraphrased.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e731c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan teks asli dan parafrasa\n",
    "all_text = all_text_real + '\\n' + all_text_paraphrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e334d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanya mengambil huruf dan titik\n",
    "all_text = re.sub(r'[^a-zA-Z.]', ' ', all_text)\n",
    "all_text = re.sub(r'\\s+', ' ', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f36b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat file sementara\n",
    "with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/dataCurr.txt', 'w') as file:\n",
    "    file.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "863dc30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  77333\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  131981 lr:  0.000000 avg.loss:  1.639794 ETA:   0h 0m 0s  0.3% words/sec/thread:  215333 lr:  0.009972 avg.loss:  3.069705 ETA:   0h 4m20s  0.5% words/sec/thread:  234088 lr:  0.009949 avg.loss:  2.746417 ETA:   0h 3m59s  4.6% words/sec/thread:  215173 lr:  0.009535 avg.loss:  2.222255 ETA:   0h 4m 9s 10.3% words/sec/thread:  203078 lr:  0.008965 avg.loss:  2.058223 ETA:   0h 4m 8s 11.2% words/sec/thread:  196004 lr:  0.008877 avg.loss:  2.049758 ETA:   0h 4m15s words/sec/thread:  175901 lr:  0.008657 avg.loss:  2.030978 ETA:   0h 4m37s 13.8% words/sec/thread:  173840 lr:  0.008623 avg.loss:  2.028647 ETA:   0h 4m39s 14.4% words/sec/thread:  170374 lr:  0.008557 avg.loss:  2.024665 ETA:   0h 4m43s lr:  0.008479 avg.loss:  2.020470 ETA:   0h 4m48s 15.9% words/sec/thread:  162876 lr:  0.008414 avg.loss:  2.016406 ETA:   0h 4m51s 17.0% words/sec/thread:  159214 lr:  0.008296 avg.loss:  2.008107 ETA:   0h 4m53s 17.8% words/sec/thread:  156646 lr:  0.008216 avg.loss:  2.001972 ETA:   0h 4m55s 20.0% words/sec/thread:  151965 lr:  0.007997 avg.loss:  1.984758 ETA:   0h 4m56s 22.0% words/sec/thread:  148444 lr:  0.007795 avg.loss:  1.976542 ETA:   0h 4m55s  0h 4m55s 23.1% words/sec/thread:  146529 lr:  0.007690 avg.loss:  1.973461 ETA:   0h 4m55s 23.1% words/sec/thread:  146485 lr:  0.007688 avg.loss:  1.973379 ETA:   0h 4m55s 23.6% words/sec/thread:  145838 lr:  0.007643 avg.loss:  1.972447 ETA:   0h 4m55ss 24.4% words/sec/thread:  144806 lr:  0.007557 avg.loss:  1.970107 ETA:   0h 4m54s 25.0% words/sec/thread:  143975 lr:  0.007504 avg.loss:  1.968801 ETA:   0h 4m53s% words/sec/thread:  141045 lr:  0.007340 avg.loss:  1.963144 ETA:   0h 4m53s 27.2% words/sec/thread:  140355 lr:  0.007279 avg.loss:  1.960984 ETA:   0h 4m52s51s 30.9% words/sec/thread:  138884 lr:  0.006907 avg.loss:  1.947371 ETA:   0h 4m40s 32.1% words/sec/thread:  138636 lr:  0.006793 avg.loss:  1.945932 ETA:   0h 4m36s 35.5% words/sec/thread:  138916 lr:  0.006446 avg.loss:  1.940863 ETA:   0h 4m21s 36.2% words/sec/thread:  138974 lr:  0.006379 avg.loss:  1.939650 ETA:   0h 4m18s 36.9% words/sec/thread:  138915 lr:  0.006308 avg.loss:  1.938597 ETA:   0h 4m15s 38.3% words/sec/thread:  138706 lr:  0.006170 avg.loss:  1.934716 ETA:   0h 4m10s 39.0% words/sec/thread:  138564 lr:  0.006103 avg.loss:  1.932574 ETA:   0h 4m 8s 40.6% words/sec/thread:  138540 lr:  0.005939 avg.loss:  1.927252 ETA:   0h 4m 1s 42.3% words/sec/thread:  138601 lr:  0.005766 avg.loss:  1.925140 ETA:   0h 3m54s 42.8% words/sec/thread:  138441 lr:  0.005720 avg.loss:  1.924622 ETA:   0h 3m52s 45.5% words/sec/thread:  138408 lr:  0.005453 avg.loss:  1.921478 ETA:   0h 3m41s 46.3% words/sec/thread:  138253 lr:  0.005370 avg.loss:  1.920517 ETA:   0h 3m38s0.004883 avg.loss:  1.910771 ETA:   0h 3m19s 52.0% words/sec/thread:  137568 lr:  0.004802 avg.loss:  1.910257 ETA:   0h 3m16s 55.0% words/sec/thread:  137069 lr:  0.004505 avg.loss:  1.903492 ETA:   0h 3m 5s 55.4% words/sec/thread:  136916 lr:  0.004457 avg.loss:  1.902458 ETA:   0h 3m 3s 58.8% words/sec/thread:  136589 lr:  0.004121 avg.loss:  1.879756 ETA:   0h 2m49s 59.8% words/sec/thread:  136301 lr:  0.004016 avg.loss:  1.868401 ETA:   0h 2m46s 61.4% words/sec/thread:  136077 lr:  0.003856 avg.loss:  1.853205 ETA:   0h 2m39s 61.9% words/sec/thread:  135996 lr:  0.003806 avg.loss:  1.849019 ETA:   0h 2m37ss1.845891 ETA:   0h 2m36s lr:  0.003546 avg.loss:  1.829584 ETA:   0h 2m26s 64.9% words/sec/thread:  135982 lr:  0.003513 avg.loss:  1.827250 ETA:   0h 2m25s 65.2% words/sec/thread:  135931 lr:  0.003479 avg.loss:  1.824786 ETA:   0h 2m24s 135864 lr:  0.003436 avg.loss:  1.821934 ETA:   0h 2m22s 67.6% words/sec/thread:  136144 lr:  0.003242 avg.loss:  1.808756 ETA:   0h 2m14s136101 lr:  0.003033 avg.loss:  1.793060 ETA:   0h 2m 5s73.6% words/sec/thread:  136226 lr:  0.002635 avg.loss:  1.768594 ETA:   0h 1m48s 74.0% words/sec/thread:  136215 lr:  0.002600 avg.loss:  1.766742 ETA:   0h 1m47s 74.8% words/sec/thread:  136139 lr:  0.002524 avg.loss:  1.762350 ETA:   0h 1m44s75.3% words/sec/thread:  136200 lr:  0.002469 avg.loss:  1.759265 ETA:   0h 1m42s 76.3% words/sec/thread:  136149 lr:  0.002374 avg.loss:  1.754305 ETA:   0h 1m38s 77.2% words/sec/thread:  135397 lr:  0.002279 avg.loss:  1.749431 ETA:   0h 1m34s 79.9% words/sec/thread:  134468 lr:  0.002012 avg.loss:  1.733272 ETA:   0h 1m24s 80.1% words/sec/thread:  134431 lr:  0.001992 avg.loss:  1.731951 ETA:   0h 1m23s 80.5% words/sec/thread:  134248 lr:  0.001952 avg.loss:  1.729014 ETA:   0h 1m21s 81.0% words/sec/thread:  134070 lr:  0.001897 avg.loss:  1.725520 ETA:   0h 1m19s 83.0% words/sec/thread:  133576 lr:  0.001704 avg.loss:  1.714481 ETA:   0h 1m11s 0.001674 avg.loss:  1.712912 ETA:   0h 1m10s 83.8% words/sec/thread:  133367 lr:  0.001623 avg.loss:  1.710229 ETA:   0h 1m 8s0.001343 avg.loss:  1.695891 ETA:   0h 0m56s 87.5% words/sec/thread:  132767 lr:  0.001249 avg.loss:  1.691194 ETA:   0h 0m52s88.7% words/sec/thread:  132285 lr:  0.001132 avg.loss:  1.684580 ETA:   0h 0m48s 0m45s 90.8% words/sec/thread:  131885 lr:  0.000916 avg.loss:  1.673949 ETA:   0h 0m39s0.000810 avg.loss:  1.670039 ETA:   0h 0m34s 92.6% words/sec/thread:  131751 lr:  0.000741 avg.loss:  1.667483 ETA:   0h 0m31s 0.000489 avg.loss:  1.658451 ETA:   0h 0m20s 97.0% words/sec/thread:  132082 lr:  0.000297 avg.loss:  1.651856 ETA:   0h 0m12s 98.3% words/sec/thread:  131891 lr:  0.000174 avg.loss:  1.647147 ETA:   0h 0m 7s avg.loss:  1.644941 ETA:   0h 0m 5s\n"
     ]
    }
   ],
   "source": [
    "# Membuat model word embedding fasttext menggunakan CBOW\n",
    "model_cbow = fasttext.train_unsupervised(input='/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/dataCurr.txt', model='cbow', lr=0.01, dim=20, ws=5, epoch=100, minCount=1, minn=3, maxn=3, thread=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6aa9b482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persentase kata ada 1.0\n",
      "persentase kata tidak ada 0.0\n",
      "['computer', 'keyboard', 'jerusalem', 'israel', 'planet', 'galaxy', 'canyon', 'landscape', 'opec', 'country', 'day', 'summer', 'dawn', 'citizen', 'people', 'environment', 'ecology', 'maradona', 'football', 'oil', 'money', 'bank', 'software', 'law', 'lawyer', 'weather', 'forecast', 'network', 'hardware', 'nature', 'fbi', 'investigation', 'wealth', 'psychology', 'freud', 'news', 'report', 'war', 'troops', 'physics', 'proton', 'stock', 'market', 'constellation', 'credit', 'card', 'hotel', 'reservation', 'closet', 'clothes', 'soap', 'opera', 'astronomer', 'space', 'movie', 'theater', 'treatment', 'recovery', 'baby', 'mother', 'deposit', 'television', 'film', 'mind', 'game', 'team', 'admission', 'ticket', 'palestinian', 'arafat', 'terror', 'boxing', 'round', 'internet', 'property', 'tennis', 'racket', 'telephone', 'communication', 'currency', 'cognition', 'seafood', 'sea', 'book', 'paper', 'library', 'depression', 'fighting', 'defeating', 'star', 'hundred', 'percent', 'dollar', 'profit', 'possession', 'cup', 'drink', 'health', 'drought', 'investor', 'earning', 'company', 'stroke', 'hospital', 'liability', 'insurance', 'victory', 'anxiety', 'defeat', 'fingerprint', 'withdrawal', 'fear', 'drug', 'abuse', 'concert', 'virtuoso', 'laboratory', 'love', 'sex', 'problem', 'challenge', 'critic', 'peace', 'bed', 'evidence', 'fertility', 'egg', 'precedent', 'minister', 'party', 'clinic', 'coffee', 'water', 'seepage', 'government', 'crisis', 'world', 'dividend', 'calculation', 'victim', 'emergency', 'luxury', 'car', 'tool', 'implement', 'competition', 'price', 'doctor', 'gender', 'equality', 'listing', 'category', 'video', 'archive', 'governor', 'office', 'discovery', 'record', 'number', 'brother', 'monk', 'production', 'crew', 'man', 'family', 'planning', 'disaster', 'area', 'food', 'preparation', 'preservation', 'popcorn', 'lover', 'quarrel', 'series', 'loss', 'weapon', 'secret', 'shower', 'flood', 'registration', 'arrangement', 'arrival', 'announcement', 'warning', 'baseball', 'season', 'mouth', 'life', 'lesson', 'grocery', 'energy', 'reason', 'criterion', 'equipment', 'maker', 'liquid', 'deployment', 'tiger', 'zoo', 'journey', 'laundering', 'decoration', 'valor', 'mars', 'scientist', 'alcohol', 'chemistry', 'disability', 'death', 'change', 'attitude', 'accommodation', 'territory', 'surface', 'size', 'prominence', 'exhibit', 'memorabilia', 'information', 'kilometer', 'row', 'impartiality', 'interest', 'secretary', 'senate', 'inmate', 'oracle', 'journal', 'association', 'street', 'children', 'flight', 'situation', 'conclusion', 'word', 'similarity', 'plan', 'consumer', 'ministry', 'culture', 'smart', 'student', 'effort', 'image', 'term', 'start', 'match', 'board', 'recommendation', 'lad', 'observation', 'architecture', 'coast', 'hill', 'departure', 'benchmark', 'index', 'attempt', 'confidence', 'year', 'focus', 'development', 'issue', 'history', 'isolation', 'media', 'trading', 'chance', 'credibility', 'century', 'population', 'live', 'atmosphere', 'morality', 'marriage', 'minority', 'gain', 'music', 'project', 'seven', 'experience', 'school', 'center', 'five', 'month', 'importance', 'operation', 'delay', 'interview', 'practice', 'institution', 'nation', 'forest', 'shore', 'woodland', 'president', 'medal', 'prejudice', 'recognition', 'viewer', 'serial', 'line', 'crane', 'industry', 'volunteer', 'motto', 'proximity', 'collection', 'article', 'sign', 'recess', 'airport', 'hypertension', 'direction', 'combination', 'wednesday', 'glass', 'magician', 'cemetery', 'possibility', 'girl', 'substance', 'graveyard', 'group', 'hike', 'phone', 'holy', 'cd', 'ear', 'racism', 'jaguar', 'slave', 'wizard', 'sugar', 'approach', 'rooster', 'voyage', 'noon', 'string', 'chord', 'smile', 'professor', 'cucumber', 'king', 'cabbage']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Cek jumlah kata dari data evaluasi yang ada dan tidak ada pada model word embedding word2vec cbow 1\n",
    "\n",
    "dir_path_wordsim_relatedness_goldstandard = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataForEvaluatinWordEmbedding/Relatedness/wordsim_relatedness_goldstandard.txt'\n",
    "list_words_exist = []\n",
    "list_words_not_exist = []\n",
    "\n",
    "\n",
    "with open(dir_path_wordsim_relatedness_goldstandard , 'r') as file:\n",
    "    content = file.read()\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        item_in_line = line.split(\"\\t\")\n",
    "        word_1 = item_in_line[0].lower()\n",
    "        word_2 = item_in_line[1].lower()\n",
    "        \n",
    "        # Cek kata pertama\n",
    "        try:\n",
    "            word_vector = model_cbow.get_word_vector(word_1)\n",
    "            if word_1 not in list_words_exist:\n",
    "                list_words_exist.append(word_1)\n",
    "        except:\n",
    "            if word_1 not in list_words_not_exist:\n",
    "                list_words_not_exist.append(word_1)\n",
    "        \n",
    "        # Cek kata kedua\n",
    "        try:\n",
    "            word_vector = model_cbow.get_word_vector(word_2)\n",
    "            if word_2 not in list_words_exist:\n",
    "                list_words_exist.append(word_2)\n",
    "        except:\n",
    "            if word_2 not in list_words_not_exist:\n",
    "                list_words_not_exist.append(word_2)\n",
    "\n",
    "print(\"persentase kata ada\", len(list_words_exist) / (len(list_words_exist) + len(list_words_not_exist)) )\n",
    "print(\"persentase kata tidak ada\", len(list_words_not_exist) / (len(list_words_exist) + len(list_words_not_exist)))\n",
    "\n",
    "print(list_words_exist)\n",
    "print(list_words_not_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cbd5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca file wordsim_relatedness_goldstandard untuk evaluasi word embedding dengan teknik relatedness cbow\n",
    "dir_path_wordsim_relatedness_goldstandard = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataForEvaluatinWordEmbedding/Relatedness/wordsim_relatedness_goldstandard.txt'\n",
    "list_model_similarity = []\n",
    "\n",
    "list_human_score = []\n",
    "with open(dir_path_wordsim_relatedness_goldstandard , 'r') as file:\n",
    "    content = file.read()\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        item_in_line = line.split(\"\\t\")\n",
    "        word_1 = item_in_line[0].lower()\n",
    "        word_2 = item_in_line[1].lower()\n",
    "        human_score = float(item_in_line[2])\n",
    "        try:\n",
    "            wv_1 = []\n",
    "            wv_2 = []\n",
    "            wv_1.append(model_cbow.get_word_vector(word_1))\n",
    "            wv_2.append(model_cbow.get_word_vector(word_2))\n",
    "            similarity = cosine_similarity(wv_1, wv_2)[0][0]\n",
    "            list_model_similarity.append(similarity)\n",
    "            list_human_score.append(human_score)\n",
    "        except:\n",
    "            continue\n",
    "            #print(\"word_1\", word_1)\n",
    "            #print(\"word_2\", word_2)\n",
    "\n",
    "rho_model, p_model = spearmanr(list_model_similarity, list_human_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00135c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12742615864701087\n"
     ]
    }
   ],
   "source": [
    "print(rho_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04dcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
