{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b834ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f7b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file asli dari folder\n",
    "directory_path_real = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap7(DataCleaning)'\n",
    "\n",
    "files_real = os.listdir(directory_path_real)\n",
    "list_files_real = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_real:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_real.append(file)\n",
    "        \n",
    "all_text_real = \"\"\n",
    "for file in list_files_real:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap7(DataCleaning)/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_real += content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b06e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file parafrasa dari folder\n",
    "directory_path_paraphrased = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap9(ParaphrasedPaper)'\n",
    "\n",
    "files_paraphrased = os.listdir(directory_path_paraphrased)\n",
    "list_files_paraphrased = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_paraphrased:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_paraphrased.append(file)\n",
    "        \n",
    "all_text_paraphrased = \"\"\n",
    "for file in list_files_paraphrased:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataPreprocessing/DataTahap9(ParaphrasedPaper)/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_paraphrased += content\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1af4f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print text asli\n",
    "print(all_text_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6234df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print text paraphrased\n",
    "print(all_text_paraphrased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c89556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan lowercase lagi\n",
    "all_text_real = all_text_real.lower()\n",
    "all_text_paraphrased = all_text_paraphrased.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d3181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer-aided structure-activity based prediction studies in drug design helps to treat diseases with novel biomarkers', ' prediction of activity spectra for substances(pass) database comprised 46,000 biologically well-known active drugs and screening are performed before the establishment of an in vitro experiment', ' pass gives the significant bioactivities of chemical compounds as pa(probable activity) and pi(probable inactivity) values to mention the compounds, whether they are active are inactive', ' the pa values higher than 0', '7 indicated this compound would be active in experiment and pi values indicate theirs inactivate possibilities', ' the admetsar chemoinformatics based tool used to predict absorption, metabolism, excretion, and toxicity of the particular compound', ' based on these criteria, the outcomes of an in vitro experiment will lower the risk of negative results[13,14]', '\\n\\n\\n\\nclc-pred tools performed to predict cytotoxicity of tumor cell lines, and it is based on structure-cell line cytotoxicity relationships designed by pass special training sets with leave-one-out cross-validation procedure', ' the accuracy of in silico prediction results significantly 96% matches with the results of in vivo experimental', ' the efficiency of compounds against cancer could be found and optimized using this pass based clc-pred database in the future to develop potential anti-cancer drugs']\n",
      "['computer-aided prediction studies in drug design, which utilize activity spectra for substances (pass) database, help in the identification and treatment of diseases using novel biomarkers', ' the pass database, which comprises 46,000 biologically well-known active drugs, is used to screen substances before conducting in vitro experiments', ' the database provides important bioactivity information for chemical compounds, distinguishing between probable activity (pa) and probable inactivity (pi) values', ' compounds with pa values higher than 0', '7 are likely to be active in experiments, while pi values indicate their inactivity possibilities', ' additionally, the admet-sar chemoinformatics tool is employed to predict the absorption, metabolism, excretion, and toxicity of specific compounds, thus aiding in lowering the risk of negative in vitro experiment results', '\\n\\nfurthermore, the clc-pred tool is utilized to predict the cytotoxicity of tumor cell lines based on structure-cell line cytotoxicity relationships crafted by special training sets within the pass database', ' this approach exhibits an accuracy rate of 96% when compared to in vivo experimental results, enabling the identification and optimization of compounds with potential anti-cancer properties', ' predicted cytotoxicity results against various human cell lines are indicated by pa values greater than 0', '5, signifying considerable probability of action, while pi values indicate inactivity']\n"
     ]
    }
   ],
   "source": [
    "# Memisahkan text per kalimat (menggunakan delimiter titik)\n",
    "list_of_sentences_real = []\n",
    "list_of_sentences_paraphrased = []\n",
    "\n",
    "list_of_sentences_real = all_text_real.split('.')\n",
    "list_of_sentences_paraphrased = all_text_paraphrased.split('.')\n",
    "\n",
    "print(list_of_sentences_real[0:10])\n",
    "print(list_of_sentences_paraphrased[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83fa13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['computer', 'aided', 'structure', 'activity', 'based', 'prediction', 'studies', 'in', 'drug', 'design', 'helps', 'to', 'treat', 'diseases', 'with', 'novel', 'biomarkers'], ['prediction', 'of', 'activity', 'spectra', 'for', 'substances', 'pass', 'database', 'comprised', 'biologically', 'well', 'known', 'active', 'drugs', 'and', 'screening', 'are', 'performed', 'before', 'the', 'establishment', 'of', 'an', 'in', 'vitro', 'experiment'], ['pass', 'gives', 'the', 'significant', 'bioactivities', 'of', 'chemical', 'compounds', 'as', 'pa', 'probable', 'activity', 'and', 'pi', 'probable', 'inactivity', 'values', 'to', 'mention', 'the', 'compounds', 'whether', 'they', 'are', 'active', 'are', 'inactive'], ['the', 'pa', 'values', 'higher', 'than'], ['indicated', 'this', 'compound', 'would', 'be', 'active', 'in', 'experiment', 'and', 'pi', 'values', 'indicate', 'theirs', 'inactivate', 'possibilities'], ['the', 'admetsar', 'chemoinformatics', 'based', 'tool', 'used', 'to', 'predict', 'absorption', 'metabolism', 'excretion', 'and', 'toxicity', 'of', 'the', 'particular', 'compound'], ['based', 'on', 'these', 'criteria', 'the', 'outcomes', 'of', 'an', 'in', 'vitro', 'experiment', 'will', 'lower', 'the', 'risk', 'of', 'negative', 'results'], ['clc', 'pred', 'tools', 'performed', 'to', 'predict', 'cytotoxicity', 'of', 'tumor', 'cell', 'lines', 'and', 'it', 'is', 'based', 'on', 'structure', 'cell', 'line', 'cytotoxicity', 'relationships', 'designed', 'by', 'pass', 'special', 'training', 'sets', 'with', 'leave', 'one', 'out', 'cross', 'validation', 'procedure'], ['the', 'accuracy', 'of', 'in', 'silico', 'prediction', 'results', 'significantly', 'matches', 'with', 'the', 'results', 'of', 'in', 'vivo', 'experimental'], ['the', 'efficiency', 'of', 'compounds', 'against', 'cancer', 'could', 'be', 'found', 'and', 'optimized', 'using', 'this', 'pass', 'based', 'clc', 'pred', 'database', 'in', 'the', 'future', 'to', 'develop', 'potential', 'anti', 'cancer', 'drugs']]\n",
      "[['computer', 'aided', 'prediction', 'studies', 'in', 'drug', 'design', 'which', 'utilize', 'activity', 'spectra', 'for', 'substances', 'pass', 'database', 'help', 'in', 'the', 'identification', 'and', 'treatment', 'of', 'diseases', 'using', 'novel', 'biomarkers'], ['the', 'pass', 'database', 'which', 'comprises', 'biologically', 'well', 'known', 'active', 'drugs', 'is', 'used', 'to', 'screen', 'substances', 'before', 'conducting', 'in', 'vitro', 'experiments'], ['the', 'database', 'provides', 'important', 'bioactivity', 'information', 'for', 'chemical', 'compounds', 'distinguishing', 'between', 'probable', 'activity', 'pa', 'and', 'probable', 'inactivity', 'pi', 'values'], ['compounds', 'with', 'pa', 'values', 'higher', 'than'], ['are', 'likely', 'to', 'be', 'active', 'in', 'experiments', 'while', 'pi', 'values', 'indicate', 'their', 'inactivity', 'possibilities'], ['additionally', 'the', 'admet', 'sar', 'chemoinformatics', 'tool', 'is', 'employed', 'to', 'predict', 'the', 'absorption', 'metabolism', 'excretion', 'and', 'toxicity', 'of', 'specific', 'compounds', 'thus', 'aiding', 'in', 'lowering', 'the', 'risk', 'of', 'negative', 'in', 'vitro', 'experiment', 'results'], ['furthermore', 'the', 'clc', 'pred', 'tool', 'is', 'utilized', 'to', 'predict', 'the', 'cytotoxicity', 'of', 'tumor', 'cell', 'lines', 'based', 'on', 'structure', 'cell', 'line', 'cytotoxicity', 'relationships', 'crafted', 'by', 'special', 'training', 'sets', 'within', 'the', 'pass', 'database'], ['this', 'approach', 'exhibits', 'an', 'accuracy', 'rate', 'of', 'when', 'compared', 'to', 'in', 'vivo', 'experimental', 'results', 'enabling', 'the', 'identification', 'and', 'optimization', 'of', 'compounds', 'with', 'potential', 'anti', 'cancer', 'properties'], ['predicted', 'cytotoxicity', 'results', 'against', 'various', 'human', 'cell', 'lines', 'are', 'indicated', 'by', 'pa', 'values', 'greater', 'than'], ['signifying', 'considerable', 'probability', 'of', 'action', 'while', 'pi', 'values', 'indicate', 'inactivity']]\n"
     ]
    }
   ],
   "source": [
    "# melakukan tokenisasi\n",
    "\n",
    "list_of_sentences_real_for_model = []\n",
    "list_of_sentences_paraphrased_for_model = []\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "for sentence in list_of_sentences_real:\n",
    "    sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "    list_of_sentences_real_for_model.append(sentence_tokenized)\n",
    "\n",
    "for sentence in list_of_sentences_paraphrased:\n",
    "    sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "    list_of_sentences_paraphrased_for_model.append(sentence_tokenized)\n",
    "\n",
    "print(list_of_sentences_real_for_model[0:10])\n",
    "print(list_of_sentences_paraphrased_for_model[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c2e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model word embedding dengan teknik word2vec dengan cbow\n",
    "model = Word2Vec(sentences=list_of_sentences_real_for_model, vector_size=20, min_count=1, workers = 10, compute_loss=True,  window=5, sg = 0, cbow_mean = 0, alpha = 0.01, seed = 10, epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdde3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word embedding dengan teknik word2vec dengan cbow\n",
    "model.save(\"word2vec_cbow_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a20704db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model word embedding dengan teknik word2vec dengan skipgram\n",
    "model_word2vec_skipgram_1 = Word2Vec(sentences=list_of_sentences_real_for_model, vector_size=20, min_count=1, workers = 10, compute_loss=True,  window=5, sg = 1, alpha = 0.01, seed = 10, epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "362171b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word embedding dengan teknik word2vec dengan skipgram\n",
    "model_word2vec_skipgram_1.save(\"model_word2vec_skipgram_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3d6177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat training loss model word embedding dengan teknik word2vec dengan cbow\n",
    "training_loss = model.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3922bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114762864.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat training loss model word embedding dengan teknik word2vec dengan skipgram\n",
    "training_loss = model_word2vec_skipgram_1.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eccfbc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99935967"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print kesamaan dari 2 kata denganteknik word2vec dengan cbow\n",
    "model.wv.similarity('network', 'internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c8becb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711285"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print kesamaan dari 2 kata denganteknik word2vec dengan skipgram\n",
    "model_word2vec_skipgram_1.wv.similarity('network', 'internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0862dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_1 Jerusalem\n",
      "word_2 Israel\n",
      "word_1 planet\n",
      "word_2 galaxy\n",
      "word_1 canyon\n",
      "word_2 landscape\n",
      "word_1 OPEC\n",
      "word_2 country\n",
      "word_1 day\n",
      "word_2 dawn\n",
      "word_1 Maradona\n",
      "word_2 football\n",
      "word_1 OPEC\n",
      "word_2 oil\n",
      "word_1 law\n",
      "word_2 lawyer\n",
      "word_1 FBI\n",
      "word_2 investigation\n",
      "word_1 psychology\n",
      "word_2 Freud\n",
      "word_1 war\n",
      "word_2 troops\n",
      "word_1 closet\n",
      "word_2 clothes\n",
      "word_1 planet\n",
      "word_2 astronomer\n",
      "word_1 Jerusalem\n",
      "word_2 Palestinian\n",
      "word_1 Arafat\n",
      "word_2 terror\n",
      "word_1 fighting\n",
      "word_2 defeating\n",
      "word_1 FBI\n",
      "word_2 fingerprint\n",
      "word_1 Arafat\n",
      "word_2 peace\n",
      "word_1 bed\n",
      "word_2 closet\n",
      "word_1 lawyer\n",
      "word_2 evidence\n",
      "word_1 dividend\n",
      "word_2 calculation\n",
      "word_1 gender\n",
      "word_2 equality\n",
      "word_1 brother\n",
      "word_2 monk\n",
      "word_1 movie\n",
      "word_2 popcorn\n",
      "word_1 lover\n",
      "word_2 quarrel\n",
      "word_1 equipment\n",
      "word_2 maker\n",
      "word_1 decoration\n",
      "word_2 valor\n",
      "word_1 Mars\n",
      "word_2 scientist\n",
      "word_1 arrangement\n",
      "word_2 accommodation\n",
      "word_1 exhibit\n",
      "word_2 memorabilia\n",
      "word_1 impartiality\n",
      "word_2 interest\n",
      "word_1 death\n",
      "word_2 inmate\n",
      "word_1 monk\n",
      "word_2 oracle\n",
      "word_1 peace\n",
      "word_2 plan\n",
      "word_1 lad\n",
      "word_2 brother\n",
      "word_1 attempt\n",
      "word_2 peace\n",
      "word_1 peace\n",
      "word_2 atmosphere\n",
      "word_1 minority\n",
      "word_2 peace\n",
      "word_1 peace\n",
      "word_2 insurance\n",
      "word_1 Mars\n",
      "word_2 water\n",
      "word_1 sign\n",
      "word_2 recess\n",
      "word_1 Wednesday\n",
      "word_2 news\n",
      "word_1 glass\n",
      "word_2 magician\n",
      "word_1 cemetery\n",
      "word_2 woodland\n",
      "word_1 possibility\n",
      "word_2 girl\n",
      "word_1 forest\n",
      "word_2 graveyard\n",
      "word_1 production\n",
      "word_2 hike\n",
      "word_1 stock\n",
      "word_2 CD\n",
      "word_1 delay\n",
      "word_2 racism\n",
      "word_1 stock\n",
      "word_2 jaguar\n",
      "word_1 monk\n",
      "word_2 slave\n",
      "word_1 lad\n",
      "word_2 wizard\n",
      "word_1 rooster\n",
      "word_2 voyage\n"
     ]
    }
   ],
   "source": [
    "# Baca file wordsim_relatedness_goldstandard untuk evaluasi word embedding dengan teknik relatedness\n",
    "dir_path_wordsim_relatedness_goldstandard = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataForEvaluatinWordEmbedding/Relatedness/wordsim_relatedness_goldstandard.txt'\n",
    "list_word2vec_cbow_similarity = []\n",
    "list_word2vec_skip_gram_similarity = []\n",
    "list_human_score = []\n",
    "with open(dir_path_wordsim_relatedness_goldstandard , 'r') as file:\n",
    "    content = file.read()\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        item_in_line = line.split(\"\\t\")\n",
    "        word_1 = item_in_line[0]\n",
    "        word_2 = item_in_line[1]\n",
    "        human_score = float(item_in_line[2])\n",
    "        try:\n",
    "            word2vec_cbow_similarity = model.wv.similarity(word_1, word_2)\n",
    "            word2vec_skip_gram_similarity = model_word2vec_skipgram_1.wv.similarity(word_1, word_2)\n",
    "            list_word2vec_cbow_similarity.append(word2vec_cbow_similarity)\n",
    "            list_word2vec_skip_gram_similarity.append(word2vec_skip_gram_similarity)\n",
    "            list_human_score.append(human_score)\n",
    "        except:\n",
    "            print(\"word_1\", word_1)\n",
    "            print(\"word_2\", word_2)\n",
    "\n",
    "rho_word2vec_cbow, p_word2vec_cbow = spearmanr(list_word2vec_cbow_similarity, list_human_score)\n",
    "rho_word2vec_skipgram, p_word2vec_skipgram = spearmanr(list_word2vec_skip_gram_similarity, list_human_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1094fba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09467356508314854\n",
      "0.3485980563190698\n"
     ]
    }
   ],
   "source": [
    "# print list relatedness\n",
    "print(rho_word2vec_cbow)\n",
    "print(rho_word2vec_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5890852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model word embedding dengan teknik word2vec dengan cbow 2\n",
    "model_word2vec_cbow_2 = Word2Vec(sentences=list_of_sentences_real_for_model, vector_size=20, min_count=1, workers = 10, compute_loss=True,  window=5, sg = 0, cbow_mean = 0, alpha = 0.01, seed = 10, epochs = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e5c0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word embedding dengan teknik word2vec dengan cbow 2\n",
    "model_word2vec_cbow_2.save(\"word2vec_cbow_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a8f973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat training loss model word embedding dengan teknik word2vec dengan cbow 2\n",
    "training_loss = model_word2vec_cbow_2.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ce5d95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999798"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print kesamaan dari 2 kata denganteknik word2vec dengan cbow 2\n",
    "model_word2vec_cbow_2.wv.similarity('network', 'internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f7e591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model word embedding dengan teknik word2vec dengan skipgram 2\n",
    "model_word2vec_skipgram_2 = Word2Vec(sentences=list_of_sentences_real_for_model, vector_size=20, min_count=1, workers = 10, compute_loss=True,  window=5, sg = 1, alpha = 0.01, seed = 10, epochs = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b62bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word embedding dengan teknik word2vec dengan skipgram 2\n",
    "model_word2vec_skipgram_2.save(\"model_word2vec_skipgram_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f45aeb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat training loss model word embedding dengan teknik word2vec dengan skipgram 2\n",
    "training_loss = model_word2vec_skipgram_2.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "270a63ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69802254"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print kesamaan dari 2 kata dengan teknik word2vec dengan skipgram 2\n",
    "model_word2vec_skipgram_2.wv.similarity('network', 'internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b80ff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_1 Jerusalem\n",
      "word_2 Israel\n",
      "word_1 planet\n",
      "word_2 galaxy\n",
      "word_1 canyon\n",
      "word_2 landscape\n",
      "word_1 OPEC\n",
      "word_2 country\n",
      "word_1 day\n",
      "word_2 dawn\n",
      "word_1 Maradona\n",
      "word_2 football\n",
      "word_1 OPEC\n",
      "word_2 oil\n",
      "word_1 law\n",
      "word_2 lawyer\n",
      "word_1 FBI\n",
      "word_2 investigation\n",
      "word_1 psychology\n",
      "word_2 Freud\n",
      "word_1 war\n",
      "word_2 troops\n",
      "word_1 closet\n",
      "word_2 clothes\n",
      "word_1 planet\n",
      "word_2 astronomer\n",
      "word_1 Jerusalem\n",
      "word_2 Palestinian\n",
      "word_1 Arafat\n",
      "word_2 terror\n",
      "word_1 fighting\n",
      "word_2 defeating\n",
      "word_1 FBI\n",
      "word_2 fingerprint\n",
      "word_1 Arafat\n",
      "word_2 peace\n",
      "word_1 bed\n",
      "word_2 closet\n",
      "word_1 lawyer\n",
      "word_2 evidence\n",
      "word_1 dividend\n",
      "word_2 calculation\n",
      "word_1 gender\n",
      "word_2 equality\n",
      "word_1 brother\n",
      "word_2 monk\n",
      "word_1 movie\n",
      "word_2 popcorn\n",
      "word_1 lover\n",
      "word_2 quarrel\n",
      "word_1 equipment\n",
      "word_2 maker\n",
      "word_1 decoration\n",
      "word_2 valor\n",
      "word_1 Mars\n",
      "word_2 scientist\n",
      "word_1 arrangement\n",
      "word_2 accommodation\n",
      "word_1 exhibit\n",
      "word_2 memorabilia\n",
      "word_1 impartiality\n",
      "word_2 interest\n",
      "word_1 death\n",
      "word_2 inmate\n",
      "word_1 monk\n",
      "word_2 oracle\n",
      "word_1 peace\n",
      "word_2 plan\n",
      "word_1 lad\n",
      "word_2 brother\n",
      "word_1 attempt\n",
      "word_2 peace\n",
      "word_1 peace\n",
      "word_2 atmosphere\n",
      "word_1 minority\n",
      "word_2 peace\n",
      "word_1 peace\n",
      "word_2 insurance\n",
      "word_1 Mars\n",
      "word_2 water\n",
      "word_1 sign\n",
      "word_2 recess\n",
      "word_1 Wednesday\n",
      "word_2 news\n",
      "word_1 glass\n",
      "word_2 magician\n",
      "word_1 cemetery\n",
      "word_2 woodland\n",
      "word_1 possibility\n",
      "word_2 girl\n",
      "word_1 forest\n",
      "word_2 graveyard\n",
      "word_1 production\n",
      "word_2 hike\n",
      "word_1 stock\n",
      "word_2 CD\n",
      "word_1 delay\n",
      "word_2 racism\n",
      "word_1 stock\n",
      "word_2 jaguar\n",
      "word_1 monk\n",
      "word_2 slave\n",
      "word_1 lad\n",
      "word_2 wizard\n",
      "word_1 rooster\n",
      "word_2 voyage\n"
     ]
    }
   ],
   "source": [
    "# Baca file wordsim_relatedness_goldstandard untuk evaluasi word embedding dengan teknik relatedness 2\n",
    "dir_path_wordsim_relatedness_goldstandard = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataForEvaluatinWordEmbedding/Relatedness/wordsim_relatedness_goldstandard.txt'\n",
    "list_word2vec_cbow_similarity_2 = []\n",
    "list_word2vec_skip_gram_similarity_2 = []\n",
    "list_human_score = []\n",
    "with open(dir_path_wordsim_relatedness_goldstandard , 'r') as file:\n",
    "    content = file.read()\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        item_in_line = line.split(\"\\t\")\n",
    "        word_1 = item_in_line[0]\n",
    "        word_2 = item_in_line[1]\n",
    "        human_score = float(item_in_line[2])\n",
    "        try:\n",
    "            word2vec_cbow_similarity = model_word2vec_cbow_2.wv.similarity(word_1, word_2)\n",
    "            word2vec_skip_gram_similarity = model_word2vec_skipgram_2.wv.similarity(word_1, word_2)\n",
    "            list_word2vec_cbow_similarity_2.append(word2vec_cbow_similarity)\n",
    "            list_word2vec_skip_gram_similarity_2.append(word2vec_skip_gram_similarity)\n",
    "            list_human_score.append(human_score)\n",
    "        except:\n",
    "            print(\"word_1\", word_1)\n",
    "            print(\"word_2\", word_2)\n",
    "\n",
    "rho_word2vec_cbow_2, p_word2vec_cbow_2 = spearmanr(list_word2vec_cbow_similarity, list_human_score)\n",
    "rho_word2vec_skipgram_2, p_word2vec_skipgram_2 = spearmanr(list_word2vec_skip_gram_similarity, list_human_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a494f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09467356508314854\n",
      "0.3485980563190698\n"
     ]
    }
   ],
   "source": [
    "# print list relatedness 2\n",
    "print(rho_word2vec_cbow_2)\n",
    "print(rho_word2vec_skipgram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd66f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persentase kata ada 0.8872832369942196\n",
      "persentase kata tidak ada 0.11271676300578035\n",
      "['computer', 'keyboard', 'israel', 'planet', 'landscape', 'country', 'day', 'summer', 'citizen', 'people', 'environment', 'ecology', 'football', 'oil', 'money', 'bank', 'software', 'law', 'weather', 'forecast', 'network', 'hardware', 'nature', 'fbi', 'investigation', 'wealth', 'psychology', 'news', 'report', 'war', 'physics', 'proton', 'stock', 'market', 'constellation', 'credit', 'card', 'hotel', 'reservation', 'clothes', 'soap', 'opera', 'space', 'movie', 'theater', 'treatment', 'recovery', 'baby', 'mother', 'deposit', 'television', 'film', 'mind', 'game', 'team', 'admission', 'ticket', 'arafat', 'boxing', 'round', 'internet', 'property', 'tennis', 'racket', 'telephone', 'communication', 'currency', 'cognition', 'seafood', 'sea', 'book', 'paper', 'library', 'depression', 'defeating', 'star', 'hundred', 'percent', 'dollar', 'profit', 'possession', 'cup', 'drink', 'health', 'drought', 'investor', 'earning', 'company', 'stroke', 'hospital', 'liability', 'insurance', 'victory', 'anxiety', 'defeat', 'fingerprint', 'withdrawal', 'fear', 'drug', 'abuse', 'concert', 'virtuoso', 'laboratory', 'love', 'sex', 'problem', 'challenge', 'critic', 'bed', 'evidence', 'fertility', 'egg', 'precedent', 'minister', 'party', 'clinic', 'coffee', 'water', 'seepage', 'government', 'crisis', 'world', 'calculation', 'victim', 'emergency', 'luxury', 'car', 'tool', 'implement', 'competition', 'price', 'doctor', 'gender', 'listing', 'category', 'video', 'archive', 'governor', 'office', 'discovery', 'record', 'number', 'brother', 'production', 'crew', 'man', 'family', 'planning', 'disaster', 'area', 'food', 'preparation', 'preservation', 'series', 'loss', 'weapon', 'secret', 'shower', 'flood', 'registration', 'arrangement', 'arrival', 'announcement', 'warning', 'baseball', 'season', 'mouth', 'life', 'lesson', 'grocery', 'energy', 'reason', 'criterion', 'maker', 'liquid', 'deployment', 'tiger', 'zoo', 'journey', 'laundering', 'decoration', 'mars', 'scientist', 'alcohol', 'chemistry', 'disability', 'death', 'change', 'attitude', 'territory', 'surface', 'size', 'prominence', 'exhibit', 'information', 'kilometer', 'row', 'interest', 'secretary', 'senate', 'oracle', 'journal', 'association', 'street', 'children', 'flight', 'situation', 'conclusion', 'word', 'similarity', 'plan', 'consumer', 'ministry', 'culture', 'smart', 'student', 'effort', 'image', 'term', 'start', 'match', 'board', 'recommendation', 'observation', 'architecture', 'coast', 'hill', 'departure', 'benchmark', 'index', 'attempt', 'confidence', 'year', 'focus', 'development', 'issue', 'history', 'isolation', 'media', 'trading', 'chance', 'credibility', 'century', 'population', 'live', 'atmosphere', 'morality', 'marriage', 'minority', 'gain', 'music', 'project', 'seven', 'experience', 'school', 'center', 'five', 'month', 'importance', 'operation', 'delay', 'interview', 'practice', 'institution', 'nation', 'forest', 'shore', 'woodland', 'president', 'medal', 'prejudice', 'recognition', 'viewer', 'serial', 'line', 'crane', 'industry', 'volunteer', 'motto', 'proximity', 'collection', 'article', 'sign', 'airport', 'hypertension', 'direction', 'combination', 'glass', 'possibility', 'substance', 'group', 'phone', 'holy', 'cd', 'ear', 'slave', 'wizard', 'sugar', 'approach', 'noon', 'string', 'chord', 'smile', 'professor', 'cucumber', 'king', 'cabbage']\n",
      "['jerusalem', 'galaxy', 'canyon', 'opec', 'dawn', 'maradona', 'lawyer', 'freud', 'troops', 'closet', 'astronomer', 'palestinian', 'terror', 'fighting', 'peace', 'dividend', 'equality', 'monk', 'popcorn', 'lover', 'quarrel', 'equipment', 'valor', 'accommodation', 'memorabilia', 'impartiality', 'inmate', 'lad', 'recess', 'wednesday', 'magician', 'cemetery', 'girl', 'graveyard', 'hike', 'racism', 'jaguar', 'rooster', 'voyage']\n"
     ]
    }
   ],
   "source": [
    "# Cek jumlah kata dari data evaluasi yang ada dan tidak ada pada model word embedding word2vec cbow 1\n",
    "model_cbow = Word2Vec.load(\"/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/word2vec_cbow_1.model\")\n",
    "\n",
    "dir_path_wordsim_relatedness_goldstandard = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataForEvaluatinWordEmbedding/Relatedness/wordsim_relatedness_goldstandard.txt'\n",
    "list_words_exist = []\n",
    "list_words_not_exist = []\n",
    "\n",
    "\n",
    "with open(dir_path_wordsim_relatedness_goldstandard , 'r') as file:\n",
    "    content = file.read()\n",
    "    lines = content.splitlines()\n",
    "    for line in lines:\n",
    "        item_in_line = line.split(\"\\t\")\n",
    "        word_1 = item_in_line[0].lower()\n",
    "        word_2 = item_in_line[1].lower()\n",
    "        \n",
    "        # Cek kata pertama\n",
    "        try:\n",
    "            word_vector = model_cbow.wv[word_1]\n",
    "            if word_1 not in list_words_exist:\n",
    "                list_words_exist.append(word_1)\n",
    "        except:\n",
    "            if word_1 not in list_words_not_exist:\n",
    "                list_words_not_exist.append(word_1)\n",
    "        \n",
    "        # Cek kata kedua\n",
    "        try:\n",
    "            word_vector = model_cbow.wv[word_2]\n",
    "            if word_2 not in list_words_exist:\n",
    "                list_words_exist.append(word_2)\n",
    "        except:\n",
    "            if word_2 not in list_words_not_exist:\n",
    "                list_words_not_exist.append(word_2)\n",
    "\n",
    "print(\"persentase kata ada\", len(list_words_exist) / (len(list_words_exist) + len(list_words_not_exist)) )\n",
    "print(\"persentase kata tidak ada\", len(list_words_not_exist) / (len(list_words_exist) + len(list_words_not_exist)))\n",
    "\n",
    "print(list_words_exist)\n",
    "print(list_words_not_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25333b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1088588   0.8571623  -0.4874063  -0.5994699  -0.6659777  -0.10868979\n",
      " -0.5723601  -0.06273925 -0.511494    0.645631    0.02942969  0.11419068\n",
      " -0.26689652 -0.5178691   1.0146843   0.8188314   0.14361697  1.228495\n",
      " -0.4876462   0.01035709]\n"
     ]
    }
   ],
   "source": [
    "print(model_cbow.wv['street'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e7621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
