particle swarm optimization(pso) only uses primary mathematical operations and can obtain a high convergence speed. for pso converges fast and can obtain high performance, it is widely applied to function optimization and real applications. since the inception of pso in 1995, a lot of pso variants have been reported, the performance of pso has been improved significantly. however, most of the current pso variants can only perform well on a group of certain optimization problems, fewer pso can achieve high performance on various types of optimization problems.



for hybrid de and pso algorithms. developing high performance hybrid de and pso algorithm still attracts the attention of ec community. to fully use the advantages of de and pso algorithms, this study proposed a mutual learning strategy for hybridizing de and pso. furthermore, an elite mutation is introduced to accelerate the convergence speed of de subswarm. the rest of this paper is organized as follows: section 2 reviews the related works, section 3 proposes the methodology, section 4 conducts experiments and



information in both de and pso subswarm. in the mutual learning strategy, the de and the pso subswarm work in parallel, and the elite information is exchanged between two subswarm to improve the population quality. the position vector of pso subswarm is employed for de mutation to speed up the response of de subswarm, furthermore, the elite de mutation is employed for accelerating the convergence speed of de subswarm.



seven selective peer algorithms are compared with mlde-pso covering three state-of-the-art pso algorithms, three recent pso algorithm and one classic de algorithms. clpso adopts comprehensive learning strategy to enhance the exploration of pso. in comprehensive learning, each particle learns from the whole swarm and different dimensions learn from different particles. clpso performs well on multimodal problems. fdr-pso employs the nearby high fitness particles to guide the motion of learning particle. fdr-pso avoids premature convergence at the cost of slowing down convergence speed in the early stage. upso on six unimodal functions out of seven unimodal functions. only on bf5, mlde-pso performs worse than other peer algorithms. mlde-pso achieves zero errors on bf1, bf2, bf3, and bf6. on bf4, mlde-pso achieves very high accuracy too. mlde-pso yields the lowest sp on bf1-bf4, bf6 and bf7, which means the convergence speed of mlde-pso on these functions are faster than other compared algorithms. dmsdl-pso and upso achieve the lowest sp



rand/1 on six, ten, nine, seven, nine, six and eight functions, respectively. for dmsdl-pso employs local search strategy to enhance convergence, dmsdl-pso performs better than mlde-pso on multimodal functions. the overall performance of mlde-pso is better than dmsdl-pso, de/rand/1 and other peer algorithms. for mldepso employs pso subswarm and elite-de to enhance exploitation,



four simple multimodal functions, seven hybrid functions and five composition functions. on functions f9, mlde-pso and de/rand/1 tie for first, and on function f22,ggl-psod, de/rand/1 tie for first. mlde-pso win the best performance on nineteen functions out of thirty cec2017 functions. dmsdl-pso, de/rand/1, ggl-psod and clpso win the best performance on six, five, two and one functions, respectively. mlde-pso outperforms clpso, fdr-pso, upso, tsl-pso, ggl-pso, dmsdl-pso and de/rand/1 on twenty five, twenty seven, thirty, twenty seven, twenty three, twenty two and twenty one functions, respectively. mlde-pso exhibits high performance on different types of cec2017 functions. the advantage of mlde-pso on unimodal functions, simple multimodal functions and hybrid functions are more significant than on composition functions. the overall performance of mlde-pso is better than de/rand/1, dmsdl-pso and other peer algorithms. though dmsdl-pso employs de to construct learning exemplar and local search strategy to enhance exploitation, it is still outperformed by mlde-pso.



between two subswarms. the position information in pso subswarm is employed for de mutation and the de individuals are employed for constructing learning exemplars for pso subswarm. to further improve exploitation of de subswarm, an elite de mutation is adopted. the experiments on basic functions, rotated basic functions and cec2017 functions indicated that the overall performance of mlde-pso is better than other compared classic pso algorithms, recently pso algorithms and classic de. on complex functions and rotated functions, mlde-pso exhibits more advantages. mlde-pso is less sensitive to rotation transformation than compared algorithms. on penalty functions, the performance of mlde-pso is relatively weak. in the future research, mlde-pso will be extended to constraint test functions and real-life optimization problems.





