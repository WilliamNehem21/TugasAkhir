a procedure for learning a lexical assignment together with a system of syntactic and semantic categories given a xed type-logical grammar is brie y described. the logic underlying the grammar can be any cut-free decidable modally enriched extension of the lambek calculus, but the correspondence between syntactic and semantic categories must be constrained so that no in nite set of categories is ultimately used to generate the language. it is shown that under these conditions various linguistically valuable subsets of the range of the algorithm are classes identi able in the limit from data consisting of sentences labeled by simply typed lambda calculus meaning terms in normal form. the entire range of the algorithm is shown to be not a learnable class, contrary to a mistaken result reported in a preliminary version of this paper. it is informally argued that, given the right type logic, the learnable classes of grammars include members which generate natural languages, and thus that natural languages are learnable in this way.



enriched by multiple combination modes and families of unary modalities. any version of lambek's calculus in this landscape of logical systems will be called a type logic in keeping with current practice. a type-logical grammar is then a triple g=(vg; ig; rg) consisting of a vocabulary vg, a lexical assignment function ig, and a type logic rg.



a major purpose of introducing modal operators is to license the use of structural rules, in e ect restricting their applicability to certain contexts. typically, extended type logics also employ more than one\family" of slash operators, and these are indexed using subscripts. the di erent families of slashes are sometimes called modes of combination.



a general algorithm has been developed for learning a syntactic category system for a natural language together with a lexical assignment that determines a completely descriptive grammar. the syntax can in principle be handled using any cut-free decidable multimodal type-logical grammar. the learning data consists of term-labeled strings, i.e. sentences annotated by typed lambda calculus meaning recipes with either variable semantic types on the subterms, or with no types on the subterms. the lambda calculus is used in a standard fashion to model the compositional meaning structures of natural language; an example of a term-labeled string is:



lemma 4.9 given any decidable type logic rg meeting the nitarity condition of def. 3.2, for any learning sample of term-labeled strings which exhausts the vocabulary vg, there is a bounded number of distinct(modulo alphabetic variant) k-valued lexicons ig without useless types for any k.



proof. the argument is combinatorial. any sample of term-labeled strings determines a lexicon of semantic type schemata|a general form semantic lexicon. clearly from the condition of def. 3.2, there is some maximum number of distinct 1-valued lexicons corresponding to any such semantic lexicon, so long as there are no useless types permitted. the precise number can in principle be determined in any case, but it will depend on the nature of the semantic lexicon(which varies from sample to sample) and the speci cs of the syntax-semantics correspondence(which varies from logic to logic). the same will be true of the 2-valued, 3-valued, etc. lexicons for any k. 2



the argument here is provided in part by the above remark to the effect that unlearnability of any class of lexicons depends on the possibility of unbounded lexical ambiguity. this is because the accumulation point of the generated language class must ultimately be generated in two ways|once by a possibly bounded lexicon, and again by an\in nite" lexicon that is the limit of the lexicons generating the chain of sets hsii. by selecting just those optimally uni ed lexicons with least cardinality, the algorithm lctl ensures that these two ways of generating any language cannot exist, since only one of them will have least cardinality.







