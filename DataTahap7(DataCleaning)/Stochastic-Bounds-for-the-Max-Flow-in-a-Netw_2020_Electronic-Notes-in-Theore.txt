we show how to obtain stochastic bounds for the strong stochastic ordering and the concave ordering of the maximal flow in a network where the capacities are non negative discrete random variables. while the deterministic problem is polynomial, the stochastic version with discrete random variables is np-hard. the monotonicity of the min-cut problem for these stochastic orderings allows us to simplify the input distributions and obtain bounds on the results. thus we obtain a tradeoff between the complexity of the computations and the precision of the bounds. we illustrate the approach with some examples.



often, these information change with time, due to noise, contention, incidents. they could not be seen as deterministic anymore and we have to deal with the apparent randomness of our measures. for instance the variation of the delay in a transportation system is more much related to congestion than to noise in the measurement process.



here we propose a method to deal with this randomness for a classical problem in operation research: the computation of the maximal flow. this problem is also important for the analysis of a network reliability. indeed, it is well known that the two terminals reliability problem is a special case of the maximal flow problem with 0/1 capacity. let g=(v, e) a capacited directed graph(or network) where the capacity of directed edge e is an integer(it could be 0 and in this case the edge is broken). the capacity of edge e is denoted as w(e). let nv be the number of edges. here we assume that the capacities are random discrete variables.



computing the maximal flow of such a capacited network is polynomial when the capacities are deterministic. unfortunately, it is not true anymore when the edges are associated with random variables(see for a survey on the complexity for various delays and flow problems for networks or graphs with random discrete costs or durations). the two terminals reliability problem is proved to be np-hard and the stochastic maximal flow is np-hard as well. as it is difficult to solve the problem, several approaches have been proposed to obtain approximations or bounds. we briefly reviewed some of these approaches and put more emphasis of the methods associated with stochastic ordering.



thus we advocate computing bounds with input distributions which have a smaller number of atoms. the number of atoms we want to keep is a parameter of the algorithms we have designed. thus the number of atoms we keep in the bounds gives us a tradeoff between the complexity and the accuracy. we use two key properties. first the strong stochastic ordering and the increasing concave ordering are used to design upper or lower bounding distributions of the capacity of the edges which have a smaller number of atoms. second, we prove the monotonicity of the max-flow problem for the strong stochastic order and the increasing convex order. due to the monotonicity, computing the max-flow for input bounds is easier because they have less atoms and provides a stochastic bound on the results.



the technical part of the paper is as follows. in section 2, we present a brief introduction to strong stochastic bounds and increasing convex stochastic bounds. we show how to build discrete distributions which are lower or upper bound in the sense of these ordering. we also present basic algorithms to change the size of the distributions while building a bound. these results have already been published in[2,4] and they are given here for the sake of readability. the methods allow to build many stochastic bounds for the max-flow with a low complexity. therefore in section 3, we show how to combine them to obtain a more accurate bound. section 4 is devoted to the numerical results.



in[2,4] we have proposed to reduce the number of atoms while keeping some quantitative and qualitative information on the results. this is obtained through the use of stochastic orderings. we begin with the definition of the orders we will use in this paper(see and for more information).



note that the expectation of the distribution is an upper bound for the concave order. thus one may expect that using theorem 2.12 instead one may obtain more accurate concave bounds. the upper bound based on the expectation of all the distributions is the worst bound based on concave ordering of the inputs. finally we add some well-known properties which will be useful to prove our algorithms. their proofs and more results on these stochastic orderings can be found in the literature[14,16].



so we can obtain upper and lower bounds for the input distributions of our problem(i.e. the capacities) with a very low complexity. it remains to decide which atoms to combine to obtain an accurate bound and also which input distributions have to be replaced by a bound. the first problem has partially been solved in for the strong stochastic order and in for the convex and concave order. we do not address the last problem in this paper but we show in section 3 how to improve bounds by combining them.



we now prove some lemmas on the basic operations of fusion of atoms to obtain lower bounds(lemma 2.20) and upper bounds(lemma 2.21) for the convex ordering. note that we only present the simplest actions, one can find in more complex algorithms to design bounds for this stochastic order.



proof: it is a simple application of property 2.15 and 2.16 note that m may already be an atom of d1. in that particular case, the number of atoms in d2 is reduced by 2. otherwise, it is only reduced by 1.



it is worthy to remark that a lower bound for the concave ordering of a distribution with more than 2 atoms has at least two atoms. therefore when we design icv bounds for the maximal flow, we must use strong stochastic bounds of the input distributions(due to item 5 of corollary 2.7) if the complexity of using concave bounds of these distributions is too high.



bf d2 with 5 atoms: h2={1, 4, 5, 8, 9} with probability[0.275, 0.1, 0.225, 0.3, 0.1]. one can easily check that e[d1]= 5.1= e[d2]. now we apply again lemma 2.21 to split atom 8 on atoms 5 and 9. we get support h3={1, 4, 5, 9} and probabilities



when we deal with capacity in a maximal flow problem, one must take into account that the capacities must be an integer for the ford and fulkerson algorithm to converge. thus, when we add one new atom by the fusion operation detailed in lemma 2.20, this atom is not an integer in general. therefore we proceed as follows: atoms 1 and 5 are kept unchanged. atoms 2.5 and 2.6 are merged with atom 3 and become atom 3 in the bound while atom 3.4 becomes atom 4 of d2. the support of d2 is{1, 3, 4, 5} and the distribution is[0.2, 0.4, 0.3, 0.1].



proposition 2.24 the extreme atoms of the initial distributions(say d) are kept in the upper concave bounds(i.e. they have a positive probability) computed by the splitting operation described in lemma 2.21. indeed at each step, we consider three ordered atoms and the atom in the middle is removed. note that this property is also true when we compute the upper bounding distribution(for the increase concave ordering) of the maximal flow by the total probability method.



in the previous section we have found how to compute bounds for the input distributions. let ni be the size of the random variable describing capacity of edge i. we know how to bound these distributions with lower or upper bounds with size ki. the question is to chose ki for all directed edge i. clearly, changing the ki gives a new bounding distribution for the maxflow and a natural question is to combine all these bounds into a more accurate one. of course, the way to combine these distributions depend on the ordering and so on the bounding algorithms.



proposition 3.4 the stop loss function of y is an affine function by intervals. the boundaries of the intervals are the atoms of distribution y. furthermore at an atom di, the difference of the slopes of two consecutive affine functions gives the probability of the atom.



proposition 3.7 one can derive a probability distribution from slw(y)= min(sly(y), slz(y)). the support is included in the union of the support of z and y and the set of intersection nodes. the probability of an atom is obtained by the difference between the slopes of the function before and after the atom. atoms with a null probability are removed from the support.



it is important to remark that we can not use this method to improve increasing concave lower bounds. of course, one can build max(slz(y), sly(y)) but it is easy to remark that the slopes of these functions may be increasing in some cases. and this property implies that we cannot extract a proper distribution from max(slz(y), sly(y)).



we now build the stop loss function slw. from these function, it can be seen that the probability of atom 3 and atom 2 is zero and we remove them from the support. finally, the support of the combined distribution is{1, 2.5, 4}, and the probability vector is[0.4, 0.2, 0.4].



for the increasing concave bound of the maximal flow, we need to compute 28 deterministic maximal flow problems(i.e. a concave bounds with two atoms per distribution and 8 edges). therefore these bounds are reported in the next section with all the bounds using 256 computations of max flow.



to obtain better bounds we investigate now two directions. first we use another constant aggregation scheme of the input distributions with more atoms. we bound each input distribution by a distribution with two atoms, according to the strong ordering or the convex ordering. second, we use some properties of the graph to design an aggregation pattern which does not use the same number of atoms for each input distributions.



as mentioned previously we build stochastic bounds with two atoms for each input distribution. we present two sets of bounds for each input distribution. note that the results of these new schemes and the former ones cannot be compared as the inputs(i.e. the input bounds) are not comparable in general for the two schemes.



icv lower bounds are easily obtained by considering concave lower bounds for the input distributions. such bounds on the input distributions with two atoms are unique. indeed the two extreme atoms must be in the concave lower bound. for instance, the concave lower bound for the distribution of the capacity of edge e0 contains two atoms, 2 and 16 both with probability 0.5.



following and, an edge is important for the accuracy of the bound when it is in the cut-set. we now try a simple heuristic to decide which input distributions must be aggregated and which one must keep all its atoms. note that all the previous bounds are based on a global aggregation pattern which is not related to the graph properties. here we experiment with the following heuristic: keep more atoms for the input distributions associated with the edges of a minimal cut of the deterministic problem associated with the expected capacities(here they are e3 and e4). of course, we keep the same global number of max flow computations(i.e. 256 in this section) to compare with the other analysis. we both present st-bounds and icv-bounds for three schemes.



j.-m. fourneau, m. le coz, n. pekergin, and f. quessette. an open tool to compute stochastic bounds on steady-state distributions and rewards. in 11th international workshop on modeling, analysis, and simulation of computer and telecommunication systems(mascots 2003), orlando, fl. ieee computer society, 2003.



