this paper makes the following contributions:(1) a novel approach to detect performance regressions across two system versions, while utilizing unit tests using source code analysis techniques.(2) an empirical evaluation of the proposed approach is conducted on real performance regressions, across two different versions, for four open source applications, namely: agilefant, apache commons math, apache commons io, and xalan. the results show that our proposed approach could successfully identify performance regressions and improvements across the four systems. even more, our proposed approach relies on finer granularity unit tests, rather than randomly generated test input. accordingly, perfdetect could detect additional performance regressions that were missed by alternative performance regression detection approaches.



this paper is structured as follows. we start with explaining our problem statement in section ii. section iii introduces our proposed approach, whereas section iv shows our experimental setup. section v shows the results our study, whereas section vi discusses the remaining issue section vii concludes the paper, while pointing out future research directions.



current approaches that aim to identify performance regression issues, and analyze their root causes within the source code suffer from limitations in several directions as follows.(1) such approaches demand running a complete test suite to compare and detect performance problems across two releases, which is an extremely time-consuming process for large test suites.(2) existing approaches rely on the presence of execution traces for the analyzed system. such execution traces can hardly exist, especially if the system is undergoing a major change that hinders having a completely running version. furthermore, the quality of the analysis relies on the strength and code coverage of those traces.



currently, developers must run a complete set of test cases for all the system which is an extremely time-consuming process with huge test suites. developers must have a completely running version. test suites used must be accurate to reveal performance regressions.



depending only on the black-box technique for finding performance regression may fail in detecting that such method causes performance regression. such failure is attributed to the fact that such specific code change might not get executed at all with randomized inputs. randomized inputs may not cover all branches of code in this version.



(1) running a complete test suite across two system versions to identify performance problem which is an extremely timeconsuming process for large test suites;(2) relying on the presence of execution traces for the analyzed system, which is hard to exist especially if the system is undergoing a major change that hinders having a completely running version;(3) relying on the quality of the used inputs to create the execution traces. if the execution traces result from inputs that exercise complete system scenarios, such inputs may ignore specific modified source code elements whose modification resulted in performance degradation. accordingly, our approach utilizes different information to avoid executing the whole system to detect performance problems. such information is(a) the version control information of the two system versions,(b) the added/modified code across the two system versions,(c) the dependencies between the modified source code and its corresponding test suites.



to apply the proposed approach, a prototype tool named perfdetect has been built. the proposed approach starts its first step by comparing two versions of source code to determine which parts of code are changed or added. the methods affected by such changes are then identified to serve as inputs used for analyzing and detecting performance regressions.



. eclipse call hierarchy plugin is used to find the references of a method whether such reference represents the callers of such method, or the callees of such method. this step is represented in alg. 2 identify-relevant-test-cases which takes as input the list of changed methods(m), all the test cases in the two system versions(t). in each changed method(mj), perfdetect identifies all caller methods of(mj) and detects if each caller method(tdirect) is test case or not by one of these ways:(1) the caller method has the junit 4@test annotation,(2) the parent class of the caller method is testcase class.



rect) that call other source code methods, which call the modified methods. perfdetect starts by checking if the changed method(mj) has relevant direct test cases. if no relevant direct test cases were found, perfdetect searches for relevant in-direct test case.



in the final stage alg. 3 execute-relevant-test-case takes the list of relevant test cases as input and outputs the list of changes causing the performance regressions. through the relevant unit test case of each modified method, perfdetect profiles the execution time of the unit test case across the two versions(tij, ti+1j) with different workloads. the differences in execution time(tdj) between the two versions is then calculated. finally, perfdetect orders the differences in execution time for all changed methods according to the execution time differences; higher difference are the ones cause performance regression.



for modified methods that have no relevant test cases, perfdetect generates new test cases for changed code which have no relevant test cases whether directly or indirectly. perfdetect utilizes a genetic algorithm, through evosuite, for generating test cases to reveal any performance regressions across the two system versions. evosuite applies a novel hybrid approach that generates and optimizes test cases with different coverage criteria, like individual statements, branches, outputs and mutation testing. the target of evosuite is generating test cases with assertions to help developers to detect deviations from expected behavior. that target varies from perfdetect which targets detecting performance variation behaviour. perfdetect depends on evosuite for generating junit test cases for classes written in java code. once those tests are generated by evosuite, perfdetect applies the above mentioned step(see section d) such step is applied to make those generated test cases to be performance-aware tests, in order to enable them to detect performance regressions.



within the evaluation, the four open source systems were utilized along with their manually written unit tests, to detect performance regressions using our approach(see sections v.a, v.b, v.c, and v.d). additionally, one of those systems(commons math) was used another time, along with automatically generated unit



agilefant is an open-source web application for helping software engineer to manage agile software development faster. agilefant is deployed in server core i5 using tomcat 7.0.47 as a web server environment, and mysql as the backend database. commons math is a lightweight library of mathematics and statistics components addressing common problems not available in the java programming language.



versions of agilefant, commons math, commons io, and xalan applications are located in the version control system(git repository). the approach is applicable to any javabased software systems and pure junit 4 tests without any additional framework like easymock. easymock is used to provide mock objects for interfaces by generating them on the fly so that a dummy functionality can be added to a mock object that can be used in unit testing. perfdetect needs to test a real scenario for each component without any dummy data. hence, perfdetect tests the performance of unit test cases and their impact set rather than those tests functionality only. this is done to get the real execution time of each method in the impact set, and hence identify the cases that can potentially be critical for the performance of the code in the target application context. so easymock is inconsistent with our approach.



this section analyzes the empirical results of the modified code across two system versions(v3.2 and v3.3) after testing. the results are analyzed for forty-nine changed methods including ten added methods, three deleted methods, and thirty-six modified methods whether from the old version or the new version. that is done by(1) identify the source code differences that have changed between the two system versions,(2) find the related test cases of code differences using the caller hierarchy of eclipse,(3) and



apache commons math is the second application we used to evaluate our approach. in this section, we analyze the results of detecting performance regression problems through two versions(v2.1 and v2.2). in 2010, commons math developers submitted a new code version(v2.2) with hidden code portions that caused performance regressions. this problem was discovered and reported9 in 2011. the problem took a long time around one year additional(2012) until developers can detect the code portions caused performance regression and solve it. our proposed approach in this paper targets helping developers for detecting performance regression faster and earlier.



2.1 especially from version v1.1, developers of commons io created a new method called readfiletobytearray() in class fileutils which is called another method called tobytearray(inputstream input) in class ioutils. the problem is the method readfiletobytearray() consumed a long time from the time of creation(v1.1) in 2005 until v2.1 in 2011. the reason for this long time is the calling of method tobytearray(inputstream input). in 2010, the problem was discovered and reported10 with a new solution which is adding a new method tobytearray(inputstream input, int size) for tobytearray. after this modification method readfiletobytearray() now called the new method for tobytearray(). this reported issue was taken around 6 years to resolve.



in this section, we analyze the results of comparing the performance of two versions(v2.7.1 and v2.7.2). some of changed code portions depends on external files as inputs and the test cases we generate cannot cover this kind of code. so we generate manual test cases for them. one of these manual generated test case that we used in xalan is a changed method called compose() in class elemliteralresult. the source code of this changed method rely on composing a template for stylesheet with some attributes to cover all branches of changed code. we generate manual test case to cover all branches of source code for testing the performance of changed code.



we picked two existing approaches[1,3] for performance regression and evaluated our approach against them. those approaches were selected due to utilizing the same real applications(agilefant and commons math) that we utilized in our evaluation. we discuss our results against their results for each application.



use source code analysis but the difference is in time of detecting performance regressions. another approach runs all test cases in the applications and has taken 48 min in v2.2 and 30 min in v2.0. the time of our approach did not exceed 80 s for v2.2 and 11 s for v2.0. that is because our approach depends on running only the test cases related to the changed methods not all test cases in the application. all these results show that perfdetect can be used to effectively identify the changes that are responsible for performance regressions.



results of our proposed approach when comparing the source code between two versions(v2.0, v2.1) observed performance improvement. perfdect detects the changed method readfiletobytearray() in class fileutils and observe that the performance of this method has improved more than 50% comparing to the previous version 2.0. the cause of this performance improvement that the changed method call a new added method instead of old one that was taken a long time.



results of our proposed approach when comparing the source code between two versions(v2.7.1, v2.7.2) observed performance improvement. perfdect detects the changed method elemliteralresult.compose() and observes that the performance of this method has improved slightly comparing to the previous version 2.7.1. the cause of this performance improvement that the changed method changed the method used to locate an element of an array. this results prove that perfdetect can detect the performance whatever that performance is improvement or regression and identify the cause of performance change.



the proposed approach suffers from one basic limitation which is its inability to identify performance regressions introduced by newly added, rather than changed, source code. this limitation is mainly attributed to the absence of execution time data for such newly added source code. yet, such limitation remains as an open issue that we plan to address in our future work.



consuming because they demand running a complete set of test cases, this may not scale well for systems with huge test suites. for solving this problem, other approaches use a regression test selection technique, to reduce the number of test cases that must be run on a changed program. these techniques prioritize functional tests but are not tailored to select such tests that would reveal performance regressions. reichelt et al. is depending on regression test selection to define performance changes using artificial test cases.



presented an approach that automatically detects ui performance degradations in android apps while considering context differences. this approach relies on poor gui responsiveness and number of frames per second for measuring performance regression. many approaches are interested in detecting performance changes in different languages as python language.



works only locally on the individual statements, they extended evosuite with a memetic algorithm enabling a global search algorithm to increase branch coverage. test suite augmentation techniques are used to generate test cases that cover code changes or code elements affected by changes[38,39]. genetic algorithms used to generate test cases or optimize test suites, but these approaches focus on generating new test cases to achieve high code coverage, rather than reveal performance regressions and they are time-consuming for generating all test cases. existing approaches focus on analyzing complete systems to detect performance regressions, rather than focusing their analysis on only the modified/newly added portions of those systems on specific system parts to detect performance regressions.



in this paper we propose a novel approach perfdetect that aims to identify performance regressions and their root causes across two system versions. the approach depends on:(1) detecting changed code across the two system versions through version control systems,(2) identifying the relevant test cases to the changed code within the current version to compare its performance against a reference version,(3) analysis of performance problems to identify code portions and their root causes that may cause performance regression. we evaluated perfdetect on two open-source applications as real applications written in java with their test cases. the results demonstrate that perfdetect can effectively recommend changed code portions caused performance regressions in comparison to other approaches.



