9.1 years of gpu calculation. while these calculations can be run in par- allel across multiple gpus, the cost of cloud computing resources could be prohibitive. for example, at $2 per gpu hour, it would cost $160k to run a set of rbfe calculations for 10,000 molecules.



in a 2022 preprint [7], gusev and coworkers describe the appli- cation of al and rbfe to the design of inhibitors of the sars-cov-2 papain-like protease (plpro). the authors began with a large library of commercially available molecules, which were reduced to a set of 8175 docked structures. a diverse subset of 45 molecules was initially selected and subjected to rbfe calculations using the thermodynamic integration (ti) method implemented in amber 18. an autoqsar sim- [11] (chosen because its cocrystal ligand shares the same chemical scaf- fold as the compounds in our virtual library) using the schrodinger protein preparation and receptor grid generation workflows within maestro [12] using default settings. candidates were docked with stan- dard precision glide into the 4gih receptor using maximum common scaffold (mcs) core constraints (4gih ligand as reference) and en- hanced ligand sampling, resulting in 66,250 hits that docked success- fully. stereo-expansion of compounds with ambiguous stereochemistry produced some duplicates. after docking hits were deduplicated and zwitterionic compounds (zero net charge, nonzero net absolute charge) were removed, 39,527 filtered hits remained.



the solvent leg predicts the dehydration free energy of the target ligand, not evaluating the reference compound. the solvent leg is made up of two stages: conversion and decoupling. the conversion stage decharges the target ligand and converts the lennard-jones terms to a simplified set of atom-type based parameters. the decoupling stage removes the discharged and standardized ligand from the solvent.



we were able to reduce the total number of molecules sampled in a campaign and still maintain a relatively high recall. with 10 cycles and a batch size of 60, the al campaigns end up selecting only 6% of the dataset. the enet, gpr and mlp algorithms were still able to recover between 73 and 77 of the top 100 molecules on average. we do see performance start to degrade more quickly if we continue to lower the batch size, but even with a batch size of 20, which would result in a campaign selecting 1% of the dataset, the best models were able to recover around 40 of the top 100 molecules.



greedy acquisition is an extreme exploitation strategy. we also ran experiments using the half sampling acquisition function, a highly ex- plorative strategy [16]. using half sampling acquisition at every cycle significantly decreases the average recall to 10-20 for all methods when starting from random molecules. we see the expected general improve- ment in the accuracy of models trained throughout an al campaign,



alternating these two acquisition functions provided improvements in rmse over using greedy alone, but it did not lead to a consistent in- crease in the overall recall achieved or the diversity of the molecules re- covered. when utilizing half sampling cycles before permanently switch- ing to greedy cycles, the recall measures can almost catch up to those seen in greedy-only campaigns by the tenth cycle. this approach also did not extend the recall to new clusters of target molecules, however.



in this study, we examined the impact of several key design choices on the performance of al methods as applied to rbfe calculations. to perform an objective comparison, we generated a chemical library of 10,000 molecules and carried out rbfe calculations on all of the molecules in the library. with this set of rbfe results in hand, we could systematically compare the impact of several design choices that direct the search. as a realistic performance metric, we examined the ability



we believe the methods employed in this study are consistent with the way in which al and rbfe calculations would be used to drive de- sign in a drug discovery program. when applying any computational method in a drug discovery project, we must be conscious of two fac- tors, time and cost. drug discovery projects typically run on design, make, and test cycles that typically encompass two to three weeks. first, ideas for compounds are generated either by scientists, algorithms, or a combination of the two. these ideas are then evaluated based on some combination of intuition and computation. finally, compounds are syn- thesized and tested, and new hypotheses are formulated based on the resulting data. for a computational method to be impactful, reliable re- sults should be generated in a few days. if it takes more than a couple of days to generate a computational result, the drug discovery team may have moved on to other ideas.



some previously published al studies have employed rather ex- otic combinations of acquisition functions that attempted to increase exploration by using techniques such as clustering to select diverse subsets from molecules with the highest predicted scores. these sam- pling schedules, which focus on exploration in early iterations and shift to exploitation toward the end of the search, seem to be based on intuition rather than systematic study. for this dataset, our work shows that simple random initialization coupled with a greedy ac- quisition function performs as well as other methods that attempt to balance exploration and exploitation. the number of molecules chosen as each iteration was the only prominent factor affecting performance.



of course, it must be noted that the results presented here may be specific to our dataset and survey design. for instance, it is possible that the dataset we generated is easily optimized, reducing the benefit of ex- plorative selections. it is also possible that a design choice we did not explore (e.g. molecular representation) has a larger impact on perfor- mance than those we did. it is encouraging to note that greedy selection was also a top performer in the recent preprint from khalak, where the authors performed a limited comparison of al approaches. we hope that this work will prompt others to perform similar comparisons and release their datasets.



[26] langevin integrator thermostat at 300 k with a monte carlo baro- stat running every 25 steps. each window was run for 2 nanosec- onds. each stage was run with 64 windows, for a total of 320 win- dows per compound. energies and errors were computed using pair bar [27]. experiments were run with the timemachine github sha d7ad70929271960279dfb5a08c2beac77423745a [28].



random forest (rf). random forest regression [17] is an ensemble technique utilizing a collection of decision trees. each individual tree is fit to both a random set of training examples and a random subset of training features. during prediction, expected value and uncertainty estimates can be computed from the collection of individual trees in the random forest.



[18] is an improved method compared to the random forest. the gbm method aims to minimize prediction error via a gradient-based method. as opposed to fitting training data in each decision tree independently in the rf model, the gbm method progressively builds new gradient boosting regression trees (gbrt) upon the prediction error of the pre- vious ensemble of gbrts. by repeatedly adding new gbrts into the collection of existing predictors, the error or the loss of the prediction can be minimized.



gaussian process regression (gpr). gaussian process regression [20,21] is a bayesian technique that computes probability distributions over a specified set of functions (the prior) that fit the training data. this prior information is encoded by a similarity matrix which is utilized by the technique during training. one of the major benefits of gpr is that ex- pected values and uncertainty estimates for the prediction on new in- stances are directly calculated by the method. for the work presented in this paper, the similarity matrix for gpr is computed using the tanimoto similarity between training instances.



half sampling (hs). half sampling uses a data subsampling technique similar to jackknifing to derive an uncertainty value for each molecule [16]. the molecule with the highest uncertainty is selected first. the half sampling approach can then update the uncertainty measures of other molecules before knowing the ground truth of the initially selected molecule. batches of molecules can be selected by iteratively selecting the molecule with the highest uncertainty and updating the uncertainty values.



probability of improvement (pi). the probability of improvement acqui- sition policy aims to select the molecule that has the highest probability of improving upon the currently identified best score. the pi score for a molecule is computed utilizing the standard deviation associated with that molecule to compute the amount of probability mass that molecule has above the current best solution. it is important to note that pi does not consider the magnitude of the improvement.



expected improvement (ei). the expected improvement policy is analo- gous to the pi policy but considers the magnitude of the improvement. the value is computed for a molecule by calculating the expected value of the probability density that molecule has above the current best so- lution. the ei method can be augmented with a parameter that can encourage more exploration.



random. as a baseline training set construction strategy, we use a naive random selection. ligands are sampled uniformly from the full dataset (without replacement). this approach may generate extremely favor- able or unfavorable starting sets due to chance. to get a better idea of a typical random starting point, experiments were performed across five independently generated variations for each batch size.



cluster that contains at least one molecule that was selected by an al campaign at any cycle. the cluster recall is the percentage of target clusters that are also recovered clusters. we use this metric to probe if active learning strategies are identifying a single motif that is common in the best scoring ligands or if they can identify diverse structures with nearly optimal scores.



