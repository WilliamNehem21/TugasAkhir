an important step in mer is feature extraction and classification. the importance in determination of feature extraction from music signals is in the sense that they represent the music well and computation can be carried out efficiently. much of work on extraction of features from music devoted to timbre features. mfcc is the well known timbre texture feature or spectrum features which is the highest performing individual feature used in speech recognition, can be examined for modeling of music. among regular features used in mer, in this work, there is a search for new features for recognizing the musical emotions.



spectrum features are features computed from the short time fourier transform(stft) of an audio signal. mfcc has proven to be one of the most successful spectrum features in speech and music related recognition tasks. the mel-cepstrum exploits auditory principles, as well as the decorrelating property of the cepstrum. the mfcc features for a segment of music file are computed using the following procedures: prediction error over the analysis frame. this error e(n) is called the linear prediction(lp) residual r(n) of the music signal. the lp residual contains much information about the excitation source. the phase of the analytic signal derived from the lp residual contains better speaker specific information. in this work residual phase is used for extracting the emotion specific information present in the excitation source signal. the analytic signal ra(n) corresponding to r(n) is given by



the activation function of the units in the input and output layers is linear(l), whereas the activation function of the units in hidden layer can be either linear or nonlinear(n). studies on three layer aann models show that the nonlinear activation function at the hidden units clusters the input data in a linear subspace. theoretically, it was shown that the weights of the network will produce small errors only for a set of points around the training data. when the constraints of the network are relaxed in terms of layers, the network is able to cluster the input data in the nonlinear subspace. hence a five layer where mi and r2 are the mean and standard deviation of ith cluster. a clustering algorithm such as k-means clustering is used to cluster the training vectors into nh clusters, where nh is the number of units in the hidden layer. k-means clustering algorithm is employed to determine the centers. the algorithm is composed of the following steps:



the aann structure 40lu 60nu 20nu 60nu 40lu achieves an optimal performance in training and testing the residual phase features for each emotion. the residual phase feature vectors are given as both input and output. the weights are adjusted to transform input feature vector into the output. the number of epochs needed depends upon the training error.



during testing the mfcc features extracted from the test utterances are given as input to the svm model. the distance between each of the feature vectors and the svm hyperplane is obtained. the number of positive score is calculated for each emotion model. based on the score, the category of emotion is decided. this process is repeated for all the test utterances of emotions. the average emotion recognition performance of 94.0% is obtained as a result



in this work lp order 16 is used for deriving the lp residual. the lp residual is extracted from emotional music signal by pre-emphasizing the input music data using first-order digital filter and 20 ms frame size with an overlap of 50 percent between adjacent frames. the highest hilbert envelopes around 40 samples for each frame are extracted. a single svm is developed for one emotion. five svm models are created for residual phase features. the average emotion recognition performance of about 56.0% is obtained after testing all the test utterances.



extracted from each frame is of 40 dimensional is given as input to svm model. for each music emotion a single svm model is created. all music emotions belong to same category is merged into a single category. totally five svm models are created. in the training phase, svm is trained to distinguish acoustic features of one emotion(+1) and acoustic features



