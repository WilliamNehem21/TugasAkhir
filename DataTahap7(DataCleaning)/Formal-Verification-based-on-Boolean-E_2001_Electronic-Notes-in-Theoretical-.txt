symbolic model checking is a technique for verifying temporal specications of nite state machines. it is well known how nite state machines and the evaluation of the temporal speci cations can be expressed using boolean formulae. we show how to do these manipulations using beds. we concentrate on examples which are hard for standard symbolic model checking methods.



determining whether a formula is satis ability is a problem which occurs in veri cation of combinational circuits and in symbolic model checking. often satis ability checking is associated with detecting errors. we examine how satis ability checking can be done using the bed data structure.



structure. among other operations, we introduce an operator for computing minimal p-cuts in fault trees. a fault tree is a boolean formula expressing whether a system fails based on the condition(\failure" or\working") of each of the components. a minimal p-cut is a representation of the most likely reasons for system failure. this method can be used to calculate approximately the probability of system failure given the failure probabilities of each of the components.



i would like to thank my two supervisors at the technical university of denmark(now both with the it university of copenhagen), associate professors henrik reif andersen and henrik hulgaard, for giving me the opportunity to work on this project. during my graduate studies, they have guided me, given me valuable comments and criticism on my work, and patiently answered my many questions. for this i am them grateful.



a special thanks is due to professor edmund clarke from carnegie mellon university. i spent six months working with him and his model checking group in pittsburgh. his vast knowledge of computer science and the inspiring atmosphere surrounding him and his group made it a joy to work with him.



during my research, i have had long and fruitful discussions with fellow doctoral students. especially j rn lind-nielsen from technical university of denmark and anubhav gupta from carnegie mellon university have lent an ear, made helpful suggestions, and answered my numerous questions. rune m ller jensen, ken friis larsen, jakob lichtenberg, jesper m ller and many others have also been helpful.



i am indebted to the members of my ph.d. committee: associate professor hans henrik l vengreen from technical university of denmark, professor parosh abdulla from chalmers university, and nils klarlund from at&t in new jersey. thank you for valuable comments and criticism on my thesis.



stories like this one show that human reasoning is prone to errors. we cannot always rely on our intuition. we need a systematic way of handling these tasks such that we know for sure that we have covered all possibilities and left nothing out.



in much the same way we can recognize structure in the systems we deal with. for example, we may recognize that one input i of the 100 inputs to a circuit is only used if another input j is high. otherwise i is ignored. there is therefore no need to consider input combinations which di er only in i when j is low.



ially simple or that someone else takes care of them. this does not mean that those transitions are uninteresting. on the contrary, they are very interesting. the whole idea of how to formalize a problem, including deciding which parts to abstract away and which parts to model, is a major research area these days.



the word\logic" stems from logos, the greek word for reason. propositional logic is the reasoning of propositions. a proposition is a statement that is either true or false; for example,\the sun is shining" or\4 is prime". we use 0 to mean false and 1 to mean true, and we use boolean variables to represent basic propositions. these constants and variables are our atomic formulae. atomic formulae can be connected using boolean connectives forming compound formulae. there are two connectives of one argument: negation and projection. we only use negation, and we write it:, where negation is de ned as:0= 1 and:1= 0. there are 16 boolean connectives of two arguments. not all 16 boolean connectives are necessary. for example, it is enough to have only negation and disjunction since the remaining



a formula is said to be a tautology if it evaluates to 1 for all possible variable assignments. likewise, a formula is said to be a contradiction if it evaluates to 0 for all possible variable assignments. we say that a contradiction is unsatis able since no variable assignment makes it evaluate to 1. a formula which is not a contradiction is satis able.



in the rest of this dissertation we often encode sets in propositional logic. we use the term characteristic function for the function encoding a set. using characteristic functions it is often possible to greatly reduce the memory needed to represent a set. another advantage is that characteristic functions allow us to work on the whole set as opposed to working on the elements of a set one at a time.



the following example illustrates characteristic functions. assume we want to represent sets of integer numbers f0; 1; 2; 3; 4; 5; 6; 7g. using three bits hs2s1s0i we can represent all eight numbers using their binary representation such that hs2s1s0i= h000i represents the number 0, h001i the number 1, and so on up to h111i for the number 7. now, the characteristic function:



in this dissertation we look at ways to solve problems in the domain of formal veri cation. we want our solutions to be systematic so they can be implemented on a computer. we also want our solutions to be able to deal with complex problems as they occur often in industry.



fault tree analysis is the problem of calculating certain values based on a fault tree for a system. a fault tree is a boolean function describing the conditions under which the system fails based on the condition(\failure" or\working") of each of the components. examples are nuclear power plants and airplanes. for both kinds of systems it is important to keep the probability of failure down.



in model checking we compare two di erent kinds of objects: a nite state machine and a ctl speci cation. we encode the nite state machine in propositional logic. based on the ctl speci cation, we compute a set of states which are valid initial states for the nite state machines. finally we compare this set of states with the actual initial states.



there are, of course, other areas within formal veri cation than the ones we deal with in this dissertation. gupta has written a thorough survey of formal veri cation methods with respect to hardware[gup92]. we do not hesitate to recommend her paper to readers interested in getting an overview of formal veri cation. clarke and wing have written a paper on the stateof-the-art and future directions for formal methods[cw96]. it contains a wealth of references to examples where formal methods(including formal veri cation) have been applied with success.



second, when verifying a system, we implicitly assume that it is isolated from the context in which it is to function. we verify, so to say, a stand-alone version of the system. however, as dr. leveson points out[lev99], many of the failures of complex systems today arise in the interfaces between the components, where the components may be hardware, software or human. a typical error is\mode confusion" where the assisting computer is in one mode but the human believes it to be in another mode. for example, an aircraft control computer may be in\ ight mode", but the operator believes it to be in\landing mode". while the computer works correctly by itself, the interface between the computer and the pilot causes problems.



in this dissertation we only consider formal veri cation. we want, however, to stress that formal veri cation is not the solution to obtaining correct systems. it is one among several methods; each of which has strengths and weaknesses. ideally, one should apply a range of such methods.



chapter 2 introduces boolean expression diagrams as a data structure for representing and manipulating boolean formulae. we explain how to implement the data structure. the chapter gives a number of properties for boolean expression diagrams. finally, the chapter contains a number of algorithms for working with boolean expression diagrams{ especially for constructing them and for converting them to binary decision diagrams.



the input formula is in conjunctive normal form(cnf). however, most problems in formal veri cation are not naturally described in cnf and it is therefore necessary to convert the formulae into cnf. the conversion is expensive as it either enlarges the state space by adding extra variables or results in an explosion in the size of the cnf representation. by doing satis ability checking directly on the boolean expression diagram data structure, we eliminate the conversion to cnf. the chapter is based on the paper[wah01]:



chapter 8 contains the conclusions. we give an outline of the results we have obtained. we characterize both the problems on which our methods work well and the problems on which out methods do not work well. finally, we identify topics for future research.



in order to examine the boolean expression diagram data structure, we have made an implementation in the programming language c. it is a set of library routines for constructing and manipulating boolean expression diagrams. on top of the library, we have built a shell-like interface. here the user can interactively enter, manipulate and examine boolean expression diagrams. appendix a describes this interface. the library and the shell interface form the core with which the experiments in this dissertation have been performed. both are available on online3.



in 1997, andersen and hulgaard proposed a new data structure for representing and manipulating boolean formulae[ah, ah97]. the data structure is called boolean expression diagrams, or beds for short. it is a generalization of bryant's binary decision diagrams(bdds)[bry86, bry92]. in this chapter we present the bed data structure, its properties, and the algorithms for working with it. part of this chapter is a review of andersen and hulgaard's work.



de nition 2.1.1(boolean expression diagram). a boolean expression diagram(bed) is a rooted directed acyclic graph g=(v; e) with vertex set v and edge set e. the vertex set v contains three types of vertices: terminal, variable, and operator vertices.



vertices are created by the mk algorithm; see algorithm 2.3. the call mk(; l; h) returns the identity of a(; l; h) vertex. if is a variable, then(; l; h) is a variable vertex with variable, low child l and high child h. if



using mk as the only means of creating vertices ensures that the beds are always reduced. if we do not create variable vertices with non-terminal children, then the resulting beds are always free. hence, the bed for a formula is both reduced and free.



vertices. once a vertex is created, it stays in the data structure. when the data structure is full, we perform a garbage collection by sweeping through the whole bed array and marking all the vertices which are still in use. the remaining vertices are removed and their corresponding entries in the array are freed. unused vertices are placed on a free-list, which we implement as a linking through the low eld of all entries not in use.



solving the satis ability problem for bdds is easy. because of the canonicity of bdds, there is only one representation of an unsatis able function: the terminal vertex 0. all other bdds represent satis able functions. the satis ability problem can be solved by comparing a given bdd to the bdd 0



assuming that it takes constant time to create a new vertex, we can construct a bed for a formula in time linear in the size of the formula. for bdds this takes much longer. in the worst case it takes time exponential in the size of the formula.



one way to solve the satis ability and the tautology problems using beds is to convert the beds into bdds. as stated in observation 2.2.6, for bdds both problems are solvable in constant time. it might seem counterintuitive to rst construct a bed and then transform it to a bdd when we could have constructed the bdd to begin with. bdds are canonical, we might argue, and thus the end result is the same regardless of how we obtain it. the drops. in some cases it drops to almost nothing. consider constructing the bdd for=$, where the bdd for is large. the size of the end result is only one vertex as is a tautology. however, as an intermediate step we need to construct the bdd for, which is large.



the intermediate results needed to convert a bed to a bdd are not necessarily the same as the ones needed to construct the bdd to begin with. in some cases we can take shortcuts with bed to bdd conversion which we cannot do with standard bdd construction. these shortcuts, although simple, turn out to be quite e ective.



andersen and hulgaard present in[ah, ah97] two algorithms for converting beds to bdds. the algorithms are called up one and up all. the idea behind both algorithms is the following: if we remove all operator vertices from a bed, we are left with a bdd. the algorithms up one and up all also ensure that the resulting bdd is ordered2.



\bed" instead of\bdd"), but this is not a problem since the bed for any boolean formula(as de ned in de nition 2.1.8) is always ordered. if the original bed is not ordered, it is possible to patch the algorithms such that the nal bdd is indeed ordered



consider repeated application of up one to the variables x1; x2;:::; xn, where each variable is pulled to the root. on the way to the root, variable xi passes the variables x1;:::; xi 1. the nal result is a bdd with the order xn; xn 1;:::; x1. a lot of work goes into pulling variables up past the variables previously pulled up. the following\early stop" modi cation to up one would help: pull a variable up until it is at the root of the bed or just below a previously up one'ed variable, whichever occurs rst. this would result in a bdd with the order x1; x2;:::; xn. and no time is wasted doing a reordering at the top of the bed.



as up one pulls a variable x to the root, it creates a lot of intermediate variable x vertices. these vertices are only used as a means to pass information between the recursive calls in up one. algorithm 2.7 shows the pseudo-code for an optimized version of up one where intermediate vertices are not created. the algorithm is somewhat simpler than the original up one algorithm by andersen and hulgaard[ah97](algorithm 2.6). instead of returning a vertex, up one0 returns a pair of vertices. the correspondence between up one and up one0 is:



the up all algorithm works by sifting all variables to the root at the same time. like for repeated use of up one, this eliminates operator vertices and the result is a bdd. up all is related to bryant's applyoperator[bry86, bry92] on bdds. converting a bed to a bdd using up all corresponds to calling apply for each operator vertex in the bed in a bottom up fashion. algorithms 2.8 and 2.9 show the pseudo-code for apply and up all. up all constructs bdds in a bottom up way.



up one pulls one variables up to the root. up all pulls all variables up. both algorithms can be seen as special cases of a more general up algorithm which pulls a set of variables up. up has not been mentioned by



algorithm 2.8: the apply algorithm. it assumes l and h are bdds. the imposed total order on the variable vertices is denoted<. in the code it is assumed that terminal vertices are included at the end of this order when comparing var(l) and var(h).



the syntax of all decision diagrams are directed acyclic graphs. each vertex represents a function f over a set of n variables. the domain is dn and the range is r: f: dn 7! r. many decision diagrams, for example bdds, have d= r= b.



the graph vertices are labeled. all decision diagrams have vertices labeled with variables. for decision diagrams with d= b, a variable vertex typically has two extra attributes: a low and a high child. there are three main types of semantics(or decompositions) for variable vertices: shannon(s), positive davio(pd), and negative davio(nd). assume the range r and domain d are both b. consider a variable vertex v with variable x:



the negative davio decomposition is also referred to as the reed-muller decomposition. the decomposition is xed for each variable in a decision diagram. however, it is possible to use two or more di erent decompositions within one decision diagram. the three composition types can be generalized to functions with non-boolean ranges, see for example[dbr97a]. the moment decomposition used in bmds and bmds is a generalized negative davio decomposition[bc95].



algorithm 2.10: the up algorithm. it pulls a set of variables s to the root of a bed u. the total order< is de ned as for apply(see algorithm 2.8). for readability we use the abbreviation vl for\l is variable vertex and var(l) 2 s". the terms vh and vu are de ned in a similar way with h and u, respectively, instead of l.



edges in decision diagrams are always directed. sometimes edges carry attributes. a typical attribute is a negation mark. a negation mark means that the function of the vertex pointed to by the edge should be negated. most decision diagrams can have negations on the edges, and we do not mention it explicitly. other possible edge attributes are weights( bmds[bc95] and evbdd[vpl96, lpv94]) and existential and universal quanti ers(xbdds[jphs91]).



in 1996 and 1997, hett, drechsler, and becker presented a new method for bdd construction[hdb96, hdb97]. they called it more for multioperand synthesis or-operations based on existential quanti cation. their idea is to introduce extra variables, so called coding variables, in the bdd. the coding variables are implicitly existentially quanti ed. vertices containing a coding variable are in e ect or-vertices since(9s: s! f; g)$(f _ g) assuming that f and g do not depend on s. more constructs the bdd



for simplicity we say that we are comparing two circuits when in fact we are comparing the propositional logic formulae obtained from the descriptions of two circuits. since we always deal with descriptions of circuits, we can never skip a test of the nal hardware circuits. formal veri cation proves that the design works, but testing examines the physical implementation of the design. formal veri cation and testing complement each other.



the functionality of the two circuits may not be 100% speci ed. or the two circuits may be at di erent levels of abstraction. to take care of such situations, it is convenient to specify a set of variable assignments called the care-set. the care-set is the set of variable assignments for which f and g should evaluate alike. outside the care-set, f and g may take di erent values. let c be the characteristic function for the care-set. then this problem can be cast as the tautology problem for c!(f$ g).



we measure the size of a bed as the number of vertices it has. almost all bed algorithms have runtimes depending on the size of the involved beds. by keeping the bed size down, we get faster algorithms and less memory usage.



recall from section 1.2 that not all 16 binary boolean connectives are needed in propositional logic. only a small number of them are necessary to express all the others. a set of connectives able to express all other connectives is called functionally complete. for example, the sets f^; _;:g and f^ g are both functionally complete.



we apply our rules recursively. rule(1) normalizes an operator vertex. since the set of connectives we use is closed under symmetry, this does not introduce extra vertices. rule(3) and(4) may replace an operator with a negation. in all other cases the rewriting rules locally reduce the number of connectives(and thus the number of operator vertices) in the bed. thus, u is identical to u except that shared vertices are duplicated so no vertex has in-degree larger than one. think of as an operator which turns a dag into a tree. consider a bed before(u) and after(u0) the application of a rewriting rule. then the size of u0 is less than u, where size is measured in number of vertices. the only exceptions are rule(1) and part of rules(3) and(4) where the size is unchanged.



one could go a step further and extend the rewriting rules to even greater depth. this poses the question of which rewriting rules to include? with a greater depth it is no longer feasible to tabulate all possibilities. there is also no need to include rules which are combinations of a number of simpler rules. another question is how do we match a bed against the rules? ho mann and o'donnell[ho82] show di erent methods for pattern matching in trees. similar techniques must be developed for the bed structure.



a second simpli cation method, based on a greedy strategy, is up oneminimizing. each variable is in turn pulled to the root of the bed using up one. if it shrinks the bed in size, then the new bed is kept, otherwise it is discarded and we continue to use the old bed. the process continues until no variable shrinks the bed further. the result depends on the order in which the variables are tried. up one-minimizing can be generalized to n variables instead of just one variable. in the general version, n variables are up one'ed, and the result is kept if it is smaller than the original bed. we continue until up one of no combination of n variables shrinks the is small. typically, these heuristics consist of three steps to obtain a single global variable order: rst, an order of the primary outputs is constructed. second, for each of the primary outputs in this order, the variables in the support of the output are ordered. third, the orders for the di erent outputs are merged into a single global variable ordering. we only consider the step of nding a variable ordering for a given primary output, since di erent variable orders can be used for di erent roots of a bed. this allows a greater exibility in nding good variable orders since the orders of the primary outputs are independent. however, the cost is that there is no or little reuse between verifying di erent primary outputs. in most bdd packages, all bdds must respect the same ordering of the variables.



since up all essentially works as apply(property(iii) of observation 2.3.3), the variable orders that are good for bdds are also good orders to use with up all. thus, when using up all we can immediately use the variable ordering heuristics developed for bdds.



has dependent variables close in the order. this allows up one to collapse sub-circuits early in the veri cation process. also, a good variable order has the variables that a ect the output the most early in the order. up one then pull these variables to the root rst which allow the most reductions.



the fanin heuristic does not capture that variables that a ect the output the most should be ordered rst, something which is particularly important for up one. the depth fanout heuristic[min96], shown for beds instead of circuits in algorithm 3.10, attempts to determine the variables that a ect an output the most by propagating a value from the output backward toward the primary inputs. the value is distributed evenly among the input signals to a gate: if a value of c is assigned to the output of a gate with n input signals, the value assigned to each of the n fanin signals is incremented by c=n(the signal may be input to several gates and thus obtains a contribution from each gate). after propagating the value throughout the circuit to the primary inputs, the depth fanout heuristic adds the primary input with the highest value to the variable order. this input is then removed from the circuit and the process is repeated until all variables in the support have been included in the variable order.



bed to bdd conversion using up one and up all is not the only way to solve the satis ability and tautology problems for beds. st almarck's method[ss98] is a patented algorithm for tautology checking of formulae in propositional logic. in this section we show how to adapt st almarck's method to beds.



from a formula f we construct a parse-tree. each leaf vertex contains a boolean variable and we label such vertices with the variable they contain. each internal vertex contains a boolean connective. we label each internal vertex with a new unique boolean variable. from such a parse-tree we construct a set of triplets| one triplet for each internal vertex. a triplet is a constraint on the variables of the internal vertices expressed in terms of the operator of the vertex and the variable labels of the children.



the zero-saturation of deduces from all possible relations between triplet variables based on the rules in a. algorithm 3.13 shows the pseudocode for 0-sat. the sign`1 in line 4 indicates application of exactly one rule in au to one axiom in c. in line 6, we update u with the parents of x and y as well as x and y themselves(discarding any negations as well as 0 and 1).



de nition 3.4.1(hardness). a formula for a tautology is said to be neasy if it can be proven to be a tautology using n-saturation. the same formula is said to be n-hard if it cannot be proven by(n 1)-saturation.



st almarck's method works on triplets constructed from the parse-tree for a formula. beds o er a representation of the parse-tree for a formula where identical subexpression have been merged. a triplet in st almarck's method contains a boolean connective and references to two other triplets. substitute\vertex" for\triplet" and we have a description of operator vertices in beds. this similarity leads us to implement st almarck's method using beds as the underlying data structure.



the parents of a triplet/vertex are not readily available in beds. all edges are directed toward the children and not the parents. however, in a single pass through the bed structure we can label all vertices with the set of their parents. this is linear in the size of the bed and we only have to do it once.



the order in which we iterate through the variables in the set u in algorithm 3.14 has no in uence on the results of the algorithm. however, it does have an e ect on how fast we obtain the results. we have experimentally found that a most-popular-vertexrst strategy(highest fan-in and fan-out) gives good results. the reason for this is that information is spread to many neighbors fast. on our examples, other methods like top-down-breadthrst and top-down-depthrst run slower than most-popular-vertexrst.



ever, the di erences are not large. even though the fanin heuristic gives an order of magnitude better results in some cases, there are only 10 to 15 seconds of time di erence. most of this di erence stems from the fact that calculating the variable ordering using depth fanout is more expensive than using fanin. using up all it was not possible to verify the c6288 case. this is because the c6288 circuit is a 16-bit multiplier. bryant[bry86] has shown that there exists no good bdd variable ordering for multipliers. in[ycbo98], the bdd size of a 16-bit multiplier is given to around 40 million vertices3. with up all we try to construct the bdd for each of the two multipliers. with only 32 mb of memory we fail.



up all does almost as well for the erroneous circuits. only in one case(depth fanout on c2670 versus c2670nr-err) is up all unable to perform the veri cation. in the other cases, up all takes slightly longer for the erroneous circuits than for the correct ones.



like for up all, up one does also well for the erroneous circuits. again the case of depth fanout on c2670 versus c2670nr-err is problematic. in most of the other cases, up one takes slightly longer for the erroneous circuits than for the correct ones.



kunz et. al.[kpr96] and pradhan et. al.[ppc96] use a technique based on recursive learning combined with bdds. the reported runtimes are comparable to the bed runtimes for the smaller circuits, but for the larger circuits their method shows a degradation in performance. the bed approach is in some cases two or three orders of magnitude faster.



matsunaga[mat96] uses a bdd based approach to exploit the structural information on the circuits. his results are comparable to the bed results. for some examples like the c7552 circuits, the bed approach is faster. on other examples like the c3540 circuits, his methods is faster.



only 13 of the 234 lgsynth'91 problems are not veri ed in 32 mb of memory and 15 minutes by all four bed methods. of these 13 problems, only ve are not veri ed by any bed method. two of the ve are veri ed by cudd. the remaining 221 problems are veri ed by all four methods; 165 of these are veri ed in less than 5 seconds for all methods. cudd solves all but four problems. the following tabulars summarize the results:



the two methods using the depth fanout heuristic have generally longer runtimes than the other methods. we attribute some of it to the depth fanout algorithm, which is more complex and time consuming than fanin. for example, the depth fanout heuristic spends around 640 seconds on computing the variable orderings for the 684 outputs of s15850.1-map problem. for comparison, the fanin heuristic uses just a second or two. for problems with many variables(s15850.1-map has 611 input variables), the depth fanout heuristic takes a long time.



except for mm9b-map and mm9b-opt, when cudd is faster, the best bed method is at most 5 seconds slower. there are 17 problems for which cudd is more than 5 seconds slower than the best bed method.(this is not counting the three cases which could only be solved by either bed or cudd, but not both.)^; _;!;;$. at the same time we performed the rewriting rules described in section 3.2.2. these di erences explain why our degree of hardness di ers for a couple of the test cases. our implementation of st almarck's method is the fastest by up to an order or two of magnitude. this is also expected as



the main advantage of implementing st almarck's method in a bed framework is the additional availability of bed and bdd methods for tautology checking. standard bed and bdd methods typically run out of memory before they run out of time. st almarck's method has it the other way around. combining st almarck's method with beds gives the user the choice between time and memory.



test generation techniques are also the basis for the recursive learning technique for nding logical implications between nodes in the circuits by kunz et. al.[kun93, kp94]. to enable the veri cation of larger circuits, the recursive learning techniques can be combined with bdds[kpr96, ppc96]. the learning technique is further extended by jain et. al.[jmf95] and by matsunaga[mat96], introducing more general learning methods based on bdds and better heuristics for nding cuts in the circuits to split the veri cation problem into more manageable sizes. recursive learning is closely related to st almarck's method[ss98]. both methods work by performing a number of 0/1 splits and then combine the knowledge learned in the two cases. for a conjunctive normal form(cnf) formula, the di erence between st almarck's method and recursive learning is that for the former you split on the variables while in the latter you split on the clauses.



cerny and mauras[cm90] present another technique for comparing two circuits without representing their full functionality. a relation that represents the possible combinations of logic values at a given cut is propagated through the two circuits. a key problem with this and the other cut-based techniques[kpr96, mat96, ppc96, ve97, vej94, jmf95] is that the performance is very sensitive to how the cuts are chosen and there is no generally applicable method to chose appropriate cuts.



the technique by kuehlmann and krohm[kk97] and later by ganai and kuehlmann[gk00] represents a recent development of the structural methods, combining several of the above techniques and developing better heuristics for determining cuts. kuehlmann, krohm and ganai represent the combinational circuits using a non-canonical data structure which is similar to beds except that only conjunction and negation operators are used. this data structure is only used to identify isomorphic sub-circuits since no operator reductions are performed. we believe that the structural technique by kuehlmann, krohm and ganai would bene t signi cantly from replacing the used circuit representation with beds.



second, we discussed the variable ordering problem. just like bdds, beds are sensitive to the order in which the variables are pulled up using up one. we examined two ordering heuristics developed for bdds. both gave good results when used on beds.



third, we examined st almarck's method for solving the tautology problem. we discussed the method and illustrated how to implement it on top of the bed data structure. the method works for combinational circuit veri cation, but it is much slower than up one and up all. however, st almarck's method uses little memory.



in this chapter we present a method for verifying that two hierarchical combinational circuits implement the same boolean functions. the key new feature of the method is its ability to exploit the modularity of the circuits to reuse results obtained from one part of the circuits in other parts. we demonstrate the method on large adder and multiplier circuits. this chapter is based on the paper[wha99].



due to the increase in the complexity of design automation tools and the circuits they manipulate, such tools cannot in general be assumed to be correct. instead of attempting to formally verify the design automation tools, a more practical approach is to formally check that a circuit generated by a design automation tool functionally corresponds to the original input. this chapter presents a technique for formally verifying that two hierarchical combinational circuits implement the same boolean functions. the presented technique can also be used to check manual modi cations of a circuit to ensure that the designer has not introduced errors.



the outputs of a hierarchical combinational circuit are determined by the inputs. in the case of the 4-bit adder, the outputs are the sum of two 4bit numbers on the inputs. we use a relation rel(c) to capture this relation between the inputs and the outputs of a cell c. for a logic cell, rel is determined by the logic of the gates(the fct attribute):



de nition 4.2.5(cut). a cut k in a container cell c is a set of variables from vars(c) such that any path p= hp1;:::; pni through c with p1 2 in(c) and pn 2 out(c) contains exactly one variable from the cut k.



the veri cation algorithm works by propagating a cut-relation from the inputs to the outputs. let h0 be the input relation hin, a cut-relation between the input cuts of c1 and c2. we move their cut-relation past instantiations of cells in c1 and c2(assuming that c1 and c2 are container cells). in each step we calculate a new cut-relation, hk+1, based on the previous one, hk. when the cut-relation has reached the outputs, the resulting cutrelation, hn, relates the outputs of c1 to the outputs of c2. if hn is a subset of hout, hn hout, the circuits have the desired output relation.



we distinguish between two ways of moving cuts: build and propagate. build determines the input/output relation rel for a cell c and uses it to calculate the new cut-relation by moving the cut past an instantiation of cell c. propagate moves cuts past two cells simultaneously by calculating the input relation rin between the inputs of the two cells and from that calculate the output relation rout for the same pair of cells. in the example above we only used propagate.



an instantiation i, a cut-relation h, and a cut k. it is assumed that all input variables for i are in the cut. the lines 1 and 2 calculate the input/output relation for cell(i) using instantiation variables, line 3 calculates the new cut-relation, and line 4 calculates the new cut.



the propagate algorithm shown in algorithm 4.5 considers two cell instantiations at a time; one in each circuit. propagate takes ve arguments; two instantiations i1 and i2, two cuts k1 and k2, and a cut-relation h over the cuts. the result is a new cut-relation and two new cuts. it is



algorithm 4.4: the build(i; h; k) algorithm where i is an instantiation, h is a cut-relation, and k is a cut. the output of the algorithm is a pair: a new cutrelation and a new cut. the algorithm moves the cut k past instantiation i and updates the cut-relation h accordingly.



algorithm 4.5: the propagate(i1; i2; h; k1; k2) algorithm. both i1 and i2 are instances of container cell. h is a cut-relation over the cuts k1 and k2. the algorithm moves the cuts past the cell instances and updates the cut-relation accordingly.



in line 1 propagate calculates, using(4.2), the input relation rin between i1 and i2 based on the cut-relation h. the input relation rin is described in cell variables, not in instantiation variables. next, we calculate the output relation rout for i1 and i2. if we have previously propagated a similar cut past the same cells, we reuse the previous result(line 3). otherwise we have two ways of calculating rout. if both i1 and i2 are instantiations of container cells, we can propagate the cut through these instantiations(line 5) by calling prop, which allows us to use propagate on the container cells. alternatively, we compute rout from the input/output relation rel for each of the instantiations i1 and i2(line 6). this resembles calling build twice. the rest of the algorithm updates the cuts and calculates the new cut-relation.



build works by constructing a representation of the input/output relation for a cell which is used to update the cut-relation h. such a relation captures the functionality of the cell. using build on the top cell corresponds to the standard veri cation method of building the bdd for the entire circuit. while this works well for smaller circuits, the bdds tend to become quite large for more complex circuits.



propagate works by moving a relation between input variables of two cells to a relation between output variables of the same two cells. in case of container cells, propagate moves the cuts one step at a time past instantiations of cells in the container cells. it avoids constructing a bdd for the functionality of a cell as long as possible. for logic cells it is necessary to construct such a bdd. however, this bdd represents only the functionality of a part of the circuit, not the whole circuit, and it is therefore more manageable.



cut-relation(s0$ t0)^(s1$ t1) when calling propagate. we want to move the cuts past the negation cells. the new cuts contain the variables s1 and s2, and t0 and t2. we build the input relation rin for the two negation cells:



where i1 and i2 are instantiations of the two negation cells. in this case rin evaluates to 1, meaning that the inputs are unrelated; knowing the value of s0 does not imply a particular value of t1, i.e., they are unrelated. rin= 1 results in h1 also being 1 and not the expected(s1$:t2)^(:s2$ t0).



cerny et. al.[cm90] split circuits into cells and each cell is described by a relation between the inputs and the outputs of the cell. using a sweep strategy, they move either forward or backward through the circuits calculating the relations between the circuits along a cut. the cells are lower level logic primitives and thus cerny et. al. have a modular model, but not a hierarchical one.



we have presented a method based on cut-propagation for obtaining a relation between the outputs of two hierarchically speci ed combinational circuits. the key new feature of the method is its ability to exploit the hierarchy in the circuit description to reuse previously calculated results in the veri cation. we have demonstrated the power of the method by verifying large adders and multipliers.



the performance of the method depends on the order in which we pick the subcells when propagating cuts from the inputs to the outputs of container cells. some orders may result in cut-relations which have large bdd representations. it helps if the hierarchical structure of the two circuits are similar. that way it is easier to pick good candidate subcells for propagating the cuts.



in this chapter we show how boolean expression diagrams can be used in symbolic model checking. we present a method based on standard xedpoint algorithms, and we use both bdds and sat-solvers to perform satis ability checking. as a result we are able to model check systems for which standard bdd-based methods fail. this chapter is partly based on the paper[wbcg00].



symbolic model checking has been performed using xed-point iterations for a number of years[bcm+92, mcm93]. the key to the success is the canonical binary decision diagram(bdd)[bry86] data structure for representing boolean functions. however, such a representation explodes in size for certain functions. biere et. al.[bcc+99, bccz99, bcrz99] introduced bounded model checking as a way of avoiding bdds. instead of performing a xed-point iteration, they construct formulae for possible counterexamples and use sat-solvers to prove or disprove the existence of such counterexamples. abdulla et. al.[abe00] also use sat-solvers but keep the xed-point iterations.



in this chapter we combine bdds and sat-solvers in symbolic model checking based on xed-points. we use boolean expression diagrams as the underlying data structure. the method is theoretically complete as we only change the representation and not the algorithms. going from a bdd to a bed representation, we have to give up canonicity. that has both advantages and disadvantages: non-canonical data structures are more succinct than canonical ones{ sometimes exponentially more succinct.



xed-point iterations. one of the key elements of our method is the quanti cation by substitution rule: 9y: g^(y$ f)$ g[f=y]. the rule is used(1) during xed-point iterations,(2) while deciding whether an initial set of states is a subset of another set of states, and nally(3) while doing iterative squaring.



the quanti cation by substitution rule helps remove some of the quanti cations. however, the remaining quanti cations still cause trouble in the form of a size explosion when we eliminate them. in the last part of the chapter we investigate the possibility of skipping the quanti cation of the remaining variables by simply leaving the variables in the formula.



in this section we describe model checking. we rst use set theory as the underlying theory. then we rede ne model checking in terms of boolean functions3. the systems we consider are nite state machines(fsms) represented by nite kripke structures[hc74, cgp99].



de nition 5.2.1(finite kripke structure). a nite kripke structure m is a tuple(sm; im; t m;`m) with a nite set of states sm, a set of initial states im sm, a transition relation t m sm sm, and a labeling of the states`m: sm 7! p(a) with atomic propositions a.



this example seems to indicate that there are two kinds of state variables: those that are restricted and those that are unrestricted. the restricted variables carry the information on which state we are in. the unrestricted variables carry the information of the inputs to the system. we use a modi-



de nition 5.2.2(modi ed kripke structure). a modi ed kripke structure m is a tuple(s; x; i; t;`) with a nite set of states s, a nite set of inputs x, a set of initial states i s, a transition function t: s x 7! s, and a labeling of the states`: s 7! p(a) with atomic propositions a.



the transition function t speci es all the possible behaviors of a system. an fsm can only move from state si to state sj if there is an input x such that sj= t(si; x). we say that the fsm takes a transition from si to sj on input x.



since the transition function is de ned for all(s; x) 2 s x, it means that it is always possible to take a transition. self-loops, i.e., transitions leading from one state to itself, are allowed. there are no dead-end states and thus all nite paths can be thought of as pre xes of in nite paths.



given a state in a kripke structure m and a ctl speci cation for m, then either holds or does not hold for that state. thus a ctl formula represents a set of states, namely the states for which the ctl formula holds. we denote that set of states[[]].



model checking is the process of determining whether a kripke structure m=(s; x; i; t;`) is a model of a ctl formula. we write m j= to indicate that m models. in the following, when we write ctl formulae, we also assume a given system m.



a ctl formula contains a propositional logic part with constants, negation, conjunction and disjunction. furthermore, it contains a number of temporal operators each consisting of a path quanti er and a path operator. there are two path quanti ers: e(\there exists an in nite path") and a(\for all in nite paths"). there are ve path operators:



in model checking of a kripke structure m=(s; x; i; t;`), we use set transformers of the type: p(s) 7! p(s). algorithm 5.3 shows the pseudo-code for the least xed-point operator lfp and the greatest xedpoint operator gfp, respectively.



the semantics[[]] of a ctl formula is a set of states. for example, the ctl formula eg denotes the set of states[[eg]] such that from each state in[[eg]] there exists an in nite path on which holds globally. four of the most common ctl operators are ef, af, eg, and ag. one may think of them as follows:



for both state and input variables we use subscripts to indicate elements of the vector and superscripts to indicate di erent vectors. for state variables, we use the convention that unprimed variables encode the current state while primed variables encode the next state. for example, s0= t(s; x) indicates a transition on input x from current state s to next state s0.



in the set version of model checking, each state was labeled with a set of atomic propositions. the atomic propositions were not speci ed. now we assume that the atomic propositions are state variables. this means we can change the ctl grammar from de nition 5.2.6 to re ect this. de nition 5.2.12 shows the modi ed ctl grammar.



de nition 5.2.13(modi ed ctl semantics). given a kripke structure m=(s; x; i; t;`), the semantics of a ctl formula is a set of states[[]] p(s) as described in de nition 5.2.10. in terms of characteristic functions,[[]] is a boolean function of the state vector s de ned recursively as follows:



de nition 5.2.14(modi ed model checking). let m=(s; x; i; t;`) be a kripke structure and let be a ctl speci cation for m. we say that m models, and write m j=, if taut(i![[]]).



the second step was to compute a bed for[[]] using de nition 5.2.13. the rst six lines of the de nition are straightforward: the bed is constructed recursively by adding either a terminal, a variable, a negation, a disjunction or a conjunction vertex. the three temporal operators are more interesting.



abdulla et. al.[abe00] also use quanti cation by substitution in their model checking algorithm. they call it inlining. one can think of quanti cation by substitution as a special case of pruning which we described in section 3.2.3. we use the quanti cation by substitution rule in three places: ex com-



quanti cation by substitution works for the quanti cation of the state variables in ex. the reason is our assumption on the form of t, where each next-state variable has a corresponding next-state function. unfortunately, we cannot use quanti cation by substitution for the quanti cation of input variables since their values are not bound to a function as is the case for the next-state variables.



the[ii=si] means a substitution of ii for si. in the third step we use quantication by substitution to replace a quanti cation by a substitution. in many cases the ii functions are quite simple, e.g., a constant. in such situations this method reduces the number of variables and simpli es the formula.



consider the case where i is a singleton set, i.e., there is only one initial state. assume without loss of generality that this initial state is(0;:::; 0). our preprocessing step would simply replace all state variables in[[]] with zeros. there would be no variables left, and thus the whole expression would trivially reduce to either 0 or 1 making the check for tautology trivial.



algorithm 5.5 shows how to compute[[ef]] using a least xed-point method and iterative squaring. ex(2n) is the ex operator with t(2n) as transition function. after each iteration in the while loop, q0 represents the set of states reachable in up to and including 2n 1 steps.



the speci cation holds. this means that i![[]] is a tautology. we could use a sat-solver to prove that the negation of i![[]] is not satis able. however, it is our experience that most sat-solvers are not very good at proving a formula to be unsatis able. we can also use bdds. by using the up one algorithm, we can convert the bed for i![[]] to a bdd. this results in the bed 1.



sat-solvers like grasp[mss99] and sato[zha97] expect their input to be a propositional formula in cnf. we must therefore convert our beds into cnf. for this conversion we use the technique of introducing new variables for every non-terminal vertex[bcc+99].



we convert a bed to cnf by introducing k extra variables{ one for each non-terminal vertex in the bed. this avoids an exponential blowup of the size of the resulting cnf. however, we do increase the size of the state space by a factor of 2k which is unfortunate.



in this section we give an example of model checking using beds. we use a modulo-4 counter which only counts one type of events. the counter should start out being zero. every time an event e happens, the counter should increment by one. the increments are done modulo 4. if an event other than e happens(we also consider idle an event, namely the event that no other event happens), then the counter keeps its value.



to implement such a modulo-4 counter, we need two boolean state variables s0 and s1. the value of the counter is the binary number s1s0. the input variable x models the presence and absence of event e: x is true if event e takes place6.



q1 is s0_ s1, which is the result of the bed simpli cations from section 3.2 applied to:s0^:s1. the q2 approximation is s0_ s1 _ ex q1. the second line in the q2 calculation shows the result after quanti cation by substitution by algorithm 5.4. the third line shows the simpli ed formula. the fourth line shows the formula after quanti cation of the input x. the



we have constructed a prototype implementation of our proposed model checking method. it performs ctl model checking on smv programs. for the experiments presented here we use sato as our sat-solver. we compare our method with the nusmv model checker(release 1.1)[ccgr99] and with bwolen yang's modi ed version of smv7, both of which are stateof-the-art in bdd-based model checking. finally we compare reachability results with fixit from adbulla, bjesse, and e en[abe00].



this example comes from the bmc-1.0f distribution9. it is a 16 16 7! 32 bit shift-and-add multiplier. the speci cation is the c6288 combinational multiplier from the iscas'85 benchmark series[bf85]. for each output bit we verify that we cannot reach a state where the shift-and-add multiplier has nished its computation and the output bits of the two multipliers di er. the multiplier ts into the category of smv programs that we handle well. the operands are not modeled as inputs. instead they are modeled as state variables with an unspeci ed initial state and the identity function as the next-state function. this lets us use quanti cation by substitution for by the bdd to bed conversion of i![[]]. the size of the bed for i![[]] as a function of the output grows as a polynomial with degree around 3/2. since the bdd to bed conversion is exponential, this explains the superexponential curve for the bed method. it looks as if the fixit method has a better asymptotic behavior and will from output 14 or 15 be the faster method. however, both methods have at least exponential runtimes and neither method handles more than the rst 13 outputs at the moment.



we were able to nd a bug in the\correct" speci cation of the multiplier for the two most signi cant outputs. iterative squaring allowed us to quickly compute the xed-points, and sato instantly found the errors. the total runtimes to nd these errors were seven and eight seconds, respectively. it turns out that the two outputs have been swapped. the original net-list for c6288 does not contain information about which gates correspond to which multiplier outputs. however, each gate is numbered and the output numbers seem to be increasing with the gate numbers{ with the exception of the last pair of outputs. this emphasizes the fact that sat-based methods are good at nding bugs in a system.



this example is a barrel shifter from the bmc-1.0f distribution and like the multiplier, it also falls within the category of systems which we handle well. a barrel shifter consists of two register les. the contents of one of the register les is rotated at each step while the other le stays the same. the width of a register is log2 r, where r is the number of registers in the register le.



proof. we rst prove[[]]k[[]]k+1 by induction over the recursive depth of the semantics in de nition 5.5.2. we use set notation in the argumentation. the four base cases 0, 1, si and:si trivially hold. the disjunction case monotonic set transformer. we prove the inclusion for[[ef]]k; the other cases are handled in a similar way. let 1 be the set transformer[[]]k[ex z and 2 the set transformer[[]]k+1[ ex z. we prove by induction that



now we prove that[[]]d=[[]]. let s be some state in[[]]d. then s is also in[[]]d+i for all i> 0 and thus more iterations in the least xed-point calculations do not result in any new states. we must therefore have reached



de nition 5.5.8(-renaming). let be a function of input variables xi;:::; xi+j, then is a renaming of the input variables such that is a function of xk;:::; xk+j where xk;:::; xk+j are new, fresh variable vectors.



the restriction in theorem 5.5.11 that i has to be a singleton set is to prevent an alternation of quanti ers. the typical check is whether i is a subset of[[]]k. in terms of characteristic functions we check 8s: i![[]]k. however, since we compute[]k and not[[]]k, we would introduce an ourselves to exactly one initial state, it is enough to check whether i and[[]]k have a state in common. it is possible to overcome this restriction by constructing a new system with a new single initial state~i and transitions from~i to all states in the old set of initial states i. using the ax operator we go one step backward and only include~i if all states in i were in[[]]. unfortunately, the computation of[ax] contains quanti cations so unless we nd some smart way of handling[ax], we have just pushed the problem from a restriction of i to ax computation.



we use beds to represent the boolean formulae. each state variable corresponds to a bed variable. boolean connectives(negation, conjunction, disjunction) correspond to operator vertices.-renaming corresponds to variable substitution. we use quanti cation by substitution for quanti cation of state variables.



recall that we only have one initial state. since we have a conjunction between this state and[]k, we can compute sat(i^[]k) as sat([]k(i)), where[]k(i) means a substitution of all state variables in[]k with their unique assignment from i. this leaves only input variables. sat-solvers like grasp[mss99] and sato[zha97] can then be used to determine satis ability.



biere, clarke et. al. have proposed bounded model checking(bmc) as an alternative method to bdd-based model checking[bcc+99, bccz99, bcrz99]. they unfold the transition relation and look for repeatedly longer and longer counterexamples, and they use sat-solvers instead of bdds. bmc is good at nding errors with short counterexamples. the diameter of



the three temporal logics have di erent expressive powers. there are ctl formulae not expressible in ltl, and vice versa. ctl* is a superset of both ctl and ltl. any ctl and ltl formula is expressible in ctl*, however, there are ctl* formulae not expressible in either ctl or ltl. please see[cgp99] for a full discussion on ctl, ltl, and ctl*.



converting a formula to cnf is necessary for standard sat-procedures like greedy sat(gsat)[slm92] and davis-putnam[dp60, dll62]. the cnf conversion may lead to an exponential growth. a way to overcome this is to introduce new variables, each representing a subformula in the original formula. unfortunately, this greatly enlarges the search space for the satprocedures. research has been made in the area of applying sat-procedures to formulae not in cnf. giunchiglia and sebastiani[gs99, seb94] have examined gsat and davis-putnam. st almarck's method also works on a non-cnf representation. chapter 6 discusses how to do satis ability checking on the bed data structure.



symbolic model checking can be expressed in qbf. we can think of our model checking algorithm as a decision procedure for qbf. both cadoli et. al.[cgs98] and rintanen[rin99] have presented algorithms for evaluation of qbf. their work has been centered on the ai community and they have, as far as we know, not experimented with their qbf algorithms on model checking problems.



we have presented a bed-based ctl model checking method based on the classical xed-point iterations. quanti cation is often the achilles heel in ctl xed-point iterations but by using quanti cation by substitution we are in some cases able to deal e ectively with it. while our method is complete, it performs best on examples with a low number of inputs. in this case we can fully exploit the quanti cation by substitution rule.



in order to deal with the input variables, we have proposed lfp-ctl model checking. inputs are left in the formulae and sat-solvers are used to implicitly quantify them out when needed. unfortunately, this restricts the power of the logic. only reachability analysis is practically possible.



future work includes investigating two variable ordering problems. one is the variable ordering when converting the bed for i![[]] to a bdd. the variable ordering is known to be very important in bdd construction, and since we, in some cases, spend much time on converting i![[]] to a bdd, our method will bene t from a good variable ordering heuristic. the other problem is the order in which we quantify the variables in the[[ex]] computation. this is interesting especially in cases where we cannot use the quanti cation by substitution rule. finally, it would be worth looking into



in this chapter we show how to determine satis ability of a formula represented by a boolean expression diagram. we compare our method with traditional sat-solvers and with bed to bdd conversion. this chapter is based on the paper[wah01].



or a\no, the formula is not satis able" answer. the resulting bdd encodes all possible variable assignments satisfying the formula. this is overkill as we are only interested in one of them{ and sometimes just the existence of one.



a formula on conjunctive normal form(cnf) consists of a set of clauses. each clause contains a number of literals, where a literal is either a variable or the negation of a variable. the literals within a clause are or'ed together, whereas the clauses are and'ed together.



5 handles unit clauses, and line 8 and 9 handle splitting on literals. there are di erent heuristics for choosing a literal in line 8. one heuristic is to choose the literal in such a way that the assignments in line 9 produce the most unit clauses.



if at any point we reach the terminal 1, then we know that the formula is satis able. this suggests a recursive algorithm which pulls up variables one at a time. the test for the empty set of clauses(line 1 in algorithm 6.1) becomes a test for the terminal 1. the test for whether contains the empty clause(line 3) becomes a test for the terminal 0. we cannot nd unit clauses with beds. the unit clauses are used to reduce the cnf formula. instead we use another type of reductions: the rewriting rules from section 3.2.2.



the function choose-variable in line 6 of algorithm 6.2 picks a variable to split on. with a clause form representation of the formula, it is natural to pick the variable in such a way as to obtain the most unit clauses after the split. this gives the most reductions due to unit propagation. we do not have a clause form formulae. however, we can still choose the variable as to get the most reductions. we perform the splitting using up one. in section 3.3 we discussed di erent heuristics for picking good variable orderings for up one. the rst variable in such an ordering would probably be a good variable to split on. in bedsat we do not need to split on the variables in the same order along di erent branches. we have chosen a simple implementation which does a depthrst traversal of the bed and picks the rst variable it encounters.



in line 9 the algorithm forks in two: one fork for the low child and one fork for the high child. if a satisfying assignment is found in one fork, then it is not necessary to consider the other one. we have implemented a simple strategy of rst examining the fork with the smaller bed size(least number of vertices). we do not have any a priori knowledge of which fork to choose so picking the smaller one makes sense as the runtime of up one depends on the the size of the bed.



to see how well bedsat works in practice, we compare it to other techniques for solving sat problems. the problems we use in the comparison are from the iscas'85 benchmark series(see chapter 3) and from model checking(see chapter 5). all the problems have been turned into satis ability problems.



73 are satis able. we indicate this with\s/u" in the second column. the up one and up all methods take slightly longer on the erroneous circuits since not all bdds collapse to a terminal. the sat-solvers(sato, grasp and bedsat) perform strictly better on the erroneous circuits compared to the correct circuits; sometimes going from impossible to possible as for



\result" column,\u" indicates unsatis able problems while\s/u" indicates both satis able and unsatis able problems. both up one and up all use the fanin variable ordering heuristic. all methods were limited to 32 mb of memory and 15 minutes of cpu time. a dash indicates that the computation could not be done within the resource limits.



10 final problem which the sat-solvers are unable to handle. the satsolvers perform quite well{ both on the satis able and the unsatis able problems. most of the problems are solved in less than a second by all three sat-solvers. while both sato and grasp take a long time on a few of the problems, bedsat seems to be more consistent in its performance.



\u" indicates unsatis able problems while\s" indicates satis able problems. both up one and up all use the fanin variable ordering heuristic. all methods were limited to 32 mb of memory and 15 minutes of cpu time. a dash indicates that the computation could not be done within the resource limits.



a boolean expression diagram is an extension of a binary decision diagram with operator vertices. the operators in the new vertices are binary boolean connectives. in this chapter we add other types of operators to the data structure and examine their advantages compared to a data structure without them.



the proof of 7.3 is straightforward. the rst part is a shannon expansion of the if-then-else operator and then a replacement of h for x. in the second part we push the substitution to the children of a variable y vertex. this is correct as y is not the variable to be substituted.



in the one variable version of existential quanti cation, we use the var eld to hold the variable to quantify. now we need a vector of variables. we represent the vector as a conjunction of the variables. the high eld contains the bed for the conjunction.



let x be a vector of variables. let x be one of those variable, and let y be a variable not in x. let f and g be functions represented by beds. we use; to denote the empty set of variables(a vector of dimension zero). the terminal cases for universal quanti cation are:



vector substitution is a substitution of a vector of functions for a vector variables; each element in the function vector is substituted for the corresponding element in the variable vector. this means we need two vectors and a function in which to perform the substitutions. to t it all in the data structure we need an auxiliary vertex, which we call map.



let x be a vector of variables. let x be one of those variable, and let y be a variable not in x. let f and g be functions represented by beds. we use; to denote the empty map list. the terminal cases for substitution are:



in symbolic model checking, we compute[[ex]] as 9s0;x: t(s; x; s0)^[[]][s0=s]. bdd-based model checkers often have one specialized function for performing the vector quanti cation and the conjunction in one single step. we now show how to combine the vector quanti cation and the vector substitution in one new operator type in beds4. we call the new operator esub.



the previous vector operators had explicit vector arguments represented as beds. we could actually do the same with esub, but we chose not to do so. instead we illustrate another method. the idea is that we need relatively few di erent vectors: it is the same set of variables we constantly quantify out, or it is the same vector of variables we substitute with the same vector of functions. we let the vectors be implicitly given.



in the case of esub, we quantify out the input variables and the primed state variables. then the unprimed state variables are replaced with their primed versions. internally in the bed each variable is assigned a number. assume that all unprimed state variables are assigned even numbers and the primed state variables are assigned odd numbers. a primed variable has a number one higher than the corresponding unprimed variable. the input variables are assigned odd numbers. this means that odd variables should be quanti ed and even variables should be replaced by the variable with one higher number.



in this section, we propose an algorithm based on beds for computing the minimal p-cuts of boolean reliability models such as fault trees. beds make it possible to bypass the bdd construction, which is the main cost of fault tree assessment. this section is based on the paper[wnr00].



the two constants 0; 1 2 b and the usual operators^, _,:, x! y; z(ifthen-else) etc. a literal is either a variable x or its negation:x. a product is a set of literals that does not contain a literal and its negation. a product is assimilated with the conjunction of its elements. a minterm over x is a product that contains either positively or negatively all variables of x.



minimal p-cuts play a central role in the assessment of fault trees. boolean formulae describe the potential failures of the system under study; variables represent component failures. minimal p-cuts represent minimal sets of component failures that induce a failure of the whole system. this notion should be preferred to the classical notion of prime implicants5 that also captures the idea of minimal solutions[dr97, dr98].



positive literals represent failures of the individual components. the failure probabilities are assumed to be independent. it is the failures that are of practical interest. the failure probability of each literal is generally quite low and thus products with a large positive part represent a negligible probability. therefore, it is in general a safe approximation to consider only products with very few positive literals.



theorem 7.3.2(dutuit& rauzy[dr97]). let f= x! f1; f0 be a boolean formula with f1 and f0 not depending on x. then, k(f) can be obtained as the union of two sets k(f)= v: 1[ 0 where 0= k(f0), minimal p-cut of f. in each step of the new algorithm, the up one transformation lifts the smallest variable in a set l over any boolean operators or other variable nodes, until it reaches a pc operator. a p-cut vertex v denotes a boolean function which encodes the set of k-truncated p-cuts for the function f over the variables in a set l. we write pc(f)[k; l] to indicate such a vertex, and we use the following attributes:



these results should be compared with the standard method(the up all algorithm), which is unable to calculate the p-cuts for cea9601 and wes9701. the former could not be build using 300 mb while the latter could not be build in 48 hours. for das9601 it succeeds in building the bdd for the fault tree in about 2 hours. the variable orderings used in the experiments are the ones given in the anonymous data les. the standard method depends on the variable ordering, and using improved heuristics to determine a good initial variable ordering will de nitely improve the performance. however, the up one method will also bene t from the use of an improved variable ordering heuristic.



extending binary decision diagrams with operators has been done by other researchers. section 2.4 mentions some of the new data structures. the most common operators are the boolean connectives. jeong et. al. use existential and universal quanti ers in their xbdds[jphs91]. hett, drechsler, and becker add existential quanti ers to obtain a new method for bdd construction[hdb96, hdb97].



the idea of giving the operators vectors as arguments is also known from standard bdds. most bdd implementations have quanti cation and substitution algorithms which allow substitution and quanti cation of vectors. the esub operator combines two di erent operations in the computation of[[ex]]: the existential quanti cation and the substitution. in bdd-based model checking, existential quanti cation and apply are often combined. combinations of this kind are very e ective compared to



in this chapter we have explained how to extend the boolean expression diagram data structure with new types of vertices. we have given examples of existential and universal quanti cation vertices and of substitution vertices. furthermore, we have presented an operator vertex, esub, combining existential quanti cation and substitution.



we have shown what properties an operator must have in order to be implemented as a vertex type in the beds. the properties are not very strict: the operator must have a terminal case and it must distribute in some form over if-then-else. furthermore, only a limited number of attributes are available per vertex. an operator with these properties can be implemented in the beds by minor modi cations to the mk, up one and up all algorithms.



as a more detailed example of a new type of vertex in the beds, we have chosen the pc operator. based on this operator we proposed a new method to compute minimal truncated p-cuts. it makes it possible to compute minimal truncated p-cuts directly from the bed without ever constructing the bdd representation of the fault tree(that is often of gigabyte size). the experimental results show that our method has an advantage over the bdd methods.



st almarck: reasoning-based method. as opposed to the other two methods, st almarck's method does not convert or change the formula it works on. in terms of speed, st almarck's method does not work well on circuits. however, it uses little memory.



use beds as bdds with up all. the rewriting rules form a preprocessing step which helps speed up the veri cation by reducing the size of the initial bed. especially the combination of up all, rewriting rules and the fanin heuristic gives good results. since the rewriting rules are used only as a preprocessing step, all bdd speci c techniques can be used in the construction of the bdd.



use up one to convert beds to bdds. this method works best if the two circuits being compared have a high degree of structural similarity. for example, the two 16-bit multipliers in the iscas'85 benchmark suite are easily veri ed using up one.



circuits described in a hierarchical way may be converted to at circuits and then veri ed by the techniques above. however, we have focused on exploiting the structural information in the hierarchical circuit descriptions. the main idea is to reuse previously calculated results.



our method works best if the two circuits being compared have similar hierarchical structures. the advantage is that instead of working with representations of the functionality of the circuits, we work with representations of the relation between the circuits. we call such a relation between the circuits for a cut-relation. in some cases, the cut-relation has a simpler bed/bdd representation than the functionality of the circuits. in cases where the hierarchical structure of the circuits are quite similar, the method performs well. for example, we have successfully veri ed large adder and multiplier circuits using this technique. unfortunately, if the circuits have dissimilar hierarchical structures, then the cut-relation may become complex and the performance of our method degrades.



the second problem domain is symbolic model checking. we have presented a method for ctl model checking based on xed-point iterations. we use quanti cation by substitution for the quanti cation whenever possible. quanti cation by substitution works well with beds. quanti cation of all state variables can be done in just one traversal of the bed. however, we still need to quantify out the inputs. for this purpose we use scope reduction rules to press the quanti ers as far down as possible in the formulae. then we perform the quanti cation using up one.



systems with input variables is a problem as we are not able to use quanti cation by substitution to quantifying out the input variables. we propose lfp-ctl as a means of model checking such systems. the lfp-ctl logic is weaker than ctl. however, the nature of the lfp-ctl logic is such that we(often) do not need to quantify out the inputs.



the third problem domain is fault tree analysis. we have chosen to focus on the computation of p-cuts. a p-cut is a representation of the most likely reasons for system failure. we have extended the bed data structure to facilitate p-cut computation. using up one, we are able to gradually transform a bed for a fault tree to a bdd for the p-cuts.



our p-cut algorithm utilizes the fact that not all the information in a fault tree is necessary to nd the p-cuts. the standard method is to construct the bdd for the fault tree and then compute the p-cuts. however, the bdds are often huge and in many cases it is not possible to construct them. our method avoids the bdd construction by only concentrating on the parts of the fault tree which contribute to the p-cuts.



we have proposed a method for solving the satis ability problem based on beds. the algorithm bedsat uses splitting on variables to divide the problem into smaller pieces. the splits are done using up one, which allows us to take advantage of the rewriting rules during satis ability checking. we have compared a simple implementation of bedsat with state-of-theart sat-solvers. on satis ability problems from model checking, bedsat performs well.



{ use up one rst on a number of variables. then switch to up all to nish the bed to bdd conversion. we have suggested up one-minimizing in section 3.2.3. this may be a starting point for further research.



our proposed sat-procedure bedsat does not utilize premature backtracking. as a result it sometimes gets stuck{ especially when working on unsatis able problem instances. we expect that the addition of premature backtracking to bedsat will make it more robust. it would also be interesting to compare bedsat with the sat-solvers proposed by giunchiglia and sebastiani in[seb94, gs99]. their methods also work on non-clausal formulae.



formulae represented as boolean expression diagrams(beds). a bed is a generalization of a binary decision diagram(bdd) which can represent any boolean circuit in linear space and still maintain many of the desirable properties of bdds. this bed package contains a number of algorithms for transforming a bed into a reduced ordered bdd. one(called up all) closely mimics the bdd apply-operator. another(called up one) can exploit the structural information of a boolean circuit.



the bed tool is used to manipulate boolean formulae represented by the bed data structure. one typical use of it is to transform a bed into a(reduced and ordered) binary decision diagram(bdd). there are di erent ways to do this: by using upall which mimics the standard bdd apply-call, by using upone to lift the variables up to the outputs one at a time, or by using upsome which lefts a set of variables to the outputs. another usage is to determine satis ability of a function represented by a bed. this can be done with the bedsat command. a number of other commands are available to obtain information about the bed data structure. all available commands a listed below. the syntax of the arguments is described after each command.



the operators bind as you would expect. imp is the implication operator, limp is left implication, nimp is negated implication, and nlimp is negated left implication. the expression expr1< var> expr2 is the if-then-else operator where expr1 denotes the low-child and expr2 denotes the high-child.



misc-assign represents a vertex in the extended version of beds described in chapter 7. the rst no is a unique vertex identi er. the id is a variable eld. the two last no's are the identi ers of the low and high children.



the operators imp, limp, nimp, nlimp, and biimp are implication, left implication, negated implication, negated left implication, and biimplication. the operators:=,?,! and esub correspond to substitution, existential quanti cation, universal quanti cation and esub. the rest of the operators have standard names. the negation operator not takes two identical arguments.



the substitution assignment denotes a boolean function in which an input is replaced with another boolean function. the existential(universal) quanti cation assignment denotes a boolean function in which an input is existentially(universally) quanti ed. in this case the input is no longer free and thus not a real input anymore. however, it should still be listed in the input part.



bed reads circuits.bed and executes in turn each command in script.com. the rst command iterates through all outputs in circuits.bed. for each output it rst sets the variable ordering according to the fanin heuristic. then it calls up all to convert the bed for the output to a bdd.



after converting all output beds to bdds, the script executes the command stat outputs which gives the number of tautologies among the outputs. in this way we can see whether the veri cation succeeded(all outputs are tautologies) or failed(some outputs were not tautologies). the command halt exits the bed tool.



[ccgr99] a. cimatti, e.m. clarke, f. giunchiglia, and m. roveri. nusmv: a new symbolic model veri er. in n. halbwachs and d. peled, editors, proceedings eleventh conference on computer-aided veri cation(cav'99), volume 1633 of lecture notes in computer science, pages 495{499, trento, italy,



[dbr97b] r. drechsler, b. becker, and s. ruppertz. manipulation algorithms for k*bmds. in ed brinksma, editor, tools and algorithms for the construction and analysis of systems(tacas), volume 1217 of lecture notes in computer science, pages 4{18. springer-verlag, 1997. 7



