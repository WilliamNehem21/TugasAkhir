0.001 t/ha and 0.136 t/ha on the holdout group. the two models also revealed consistent and better performance using scatter plot analysis across three datasets. the approach developed is useful to predict wheat yield at field scale, which is a rarely available but vital in many developmental projects, using optical sensors.



nonetheless, several issues have challenged the application of vis for crop yield prediction. three major categories could be identified: sensor(obstruction of images by cloud, missing orbits, and geometry artifacts), data(absence of long years of crop yield data), and statistics(the complex relationship between predictor and response variables). besides, images obtained at some crop growth stages, which are commonly referred to as critical stages, have paramount importance for crop yield prediction. a study revealed that using quickbird image at end of the heading and in particular after inflorescence fully emerged, ndvi is highly correlated with yield with an average correlation coefficient of



therefore, this study motivated to address some of the key challenges of farm-level crop yield estimation using optical sensors. this study is novel as it integrates and implements four methods: application of gapfill on high-resolution s2 sensor, application of gapfill following crop phenology stage, the use of tilling approach within gapfill framework, and evaluation of observations from cloud restored images for yield prediction using seven machine learning and ensemble methods. we hypothesized that by discriminating the spatio-temporal potential of the s2 sensor, the gapfill method will provide a good estimate of cloudy pixels and the machine learning methods will learn the nonlinearity between the ndvi variable and winter wheat yield. hence, the two methods: gapfill and machine learning potentially complement and offer an increased skill for wheat yield prediction. therefore, the purpose of this study is twofold. first, we implement the gapfill algorithm for cloud restoration on ndvi variables derived from high spatial and temporal resolution s2 sensor. second, observations from restored images combined with original(from cloud-free images) values used for wheat yield prediction.



the input dataset for gapfill should be prepared in fourdimensional arrays: x(dimension one), y(dimension two), s(season: dimension three), and a(year index: dimension four). the input dataset in this study has a spatial extent of 1486 grids along the x and 1837 grids along the y. to the season index, seven images found in the study season used. we grouped images per phenology considering the hypothesis that images found in one phenology stage represents a single physiological crop development and could yield a composite that offers reliable prediction values. the study applied data from three years: 2017, 2018, and 2019, which



the key field-collected data in this study was dry weighted wheat yield measured in ton per hectare(t/ha). it was collected from 67 farmers' fields for two crop seasons: 2017 and 2018. furthermore, in this study, with the primary purpose of getting a better yield prediction method, some potential predictors added. specifically, two types of fertilizers that have been widely used in the study area such as urea fertilizer that contains 46% nitrogen; and nps, which contain nitrogen, phosphorous and sulfur with the ratio of 19% n, 38% p2o5 and 7% s were used(meessen and petersen, 2010).



rank the image. at this stage, for all the sub-images within the prediction set, a scoring algorithm ranks each of them. the score of a sub-image is the proportion of values in the sub-image that are larger compared to the values at the same spatial coordinates in all other sub-images. the sub-images are ranked by increasing score, i.e., a sub-image with the smallest score will be assigned rank 1 and so on.



estimate quantile. for sub-images within the prediction set having an observed value at the spatial location of the target pixel, here, the algorithm determines to which empirical quantile level that value corresponds relatively to all values of the image. the observed values in the prediction set are used to estimate an empirical cumulative distribution function for each sub-image. the final quantile level will be the mean of all quantiles levels for all the sub-images in the prediction set. a tuning parameter, v, which is the minimum number of quantiles, controls the process.



computation of rmse using a cross-validation procedure. the procedure starts with true data and removes some validation points to obtain artificially generated points, which referred to as data observed. then, the observed data will be predicted and filled with values. the comparison between data filled and the original data set will yield the metric rmse(gerber, 2018). in addition, the validity of the restored dataset assessed visually employing spatial coherence and a non-artificial(natural) look.



k-nearest neighbors(knn) is a supervised machine learning algorithms used for classification and regression. in knn, the number of neighbors(k) is a constant used to calculate nearest neighbors distances vector(rsrivastava, 2020). knn is an instance-based learning; the output is the mean of the values of its k nearest neighbors. the study tuned k for the range of 1:16 and selected k= 12 as the final value.



it aggregates the predictions made by multiple decision trees. each tree relies on the values of a random vector sampled independently and with the same distribution(breiman, 2001). the most important parameter, mtry that is the number of predictors that are picked randomly is searched for mtry= 2:6(breiman, 2002). the random forest implementation in this study obtained mtry= 2 as best value.



it generalizes linear regression by allowing the linear model to be related to the response variable via a link function by allowing the magnitude of the variance of each measurement to be a function of its predicted value. this model allows us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear. the error distribution of the response variable need not to have normal distribution, however it assumed to follow an exponential family of distribution(i.e. normal, binomial, poisson, or gamma distributions). it applies a unique link function for each of the probability distribution(mccullagh and nelder, 1989). thus, this study used a gaussian probability distribution with identity link function.



the dataset that contains both predictors and response variables was partitioned into 80:20 ratios for training and testing(holdout) group. all parameters put on the same scale using data scaling method. in the learning process, parameter tuning was implemented for each method. caret package parameter tuning was implemented using the train control function in r software, and the expand grid function supported the search process.to estimate the generalization error, 25 numbers of subsamples were created as training groups using bootstrapping resampling method. the bootstrap method is a resampling approach used to calculate statistics on a population by iteratively sampling a dataset with replacement. it is a widely applicable and powerful statistical technique used to measure the uncertainty associated with a given statistical learning method. since we work with a small sample size in this study, the use of boot strapping is very relevant to estimation problems with small sample size and unknown distribution of the actual population(karthik and abhishek, 2019).



for each model, the best tuning parameters were selected using root mean squared error(rmse) metric. then, the seven models were combined in caret list function using the best tuning parameters on the training group using bootstrapping resampling procedure. the caret list function is used as it supports building lists of caret models on the same training data.



ensemble methods are the third major machine learning approach applied. ensemble learning is the process of learning from the advantage of combining multiple algorithms for better model performance. in applying a single method of supervised algorithms, the task is to search for a solution in parameter space, while ensembles combine multiple parameter spaces to form a better hypothesis. the above seven base models were combined in an ensemble learning called stacking or super learning(wolpert, 1992) that trains a second-level meta learner to get the optimal combination of the base learners.



in this study, using caret stack function we applied the 5 fold crossvalidation resampling method on the training dataset. cross-validation is one of the commonly used techniques for model evaluation and considered as a better technique than residual-based metrics(kohavi, 1995). this method ensures that every data point gets to be in a test set exactly once and disadvantageous to when used in large dataset due to increasing computation cost, which was not relevant as this study applied small dataset(karthik and abhishek, 2019).



outputs from the seven base models combined in a caret stack using each of the seven methods, which resulted in seven ensemble versions(deane-mayer and knowles, 2019). the seven ensemble models include ensemble regularized regression(en.glmnet), ensemble generalized linear regression(en.glm), ensemble neural network(en. nnet), ensemble k-nearest neighbors(en.knn), ensemble recursive partitioning and regression trees(en.rpart), ensemble support vector machine(en.svm) and ensemble random forest(en.rf).



finally, in addition to the rmse metric, this study used scatter analysis to validate the performance of machine learning methods. in particular, scatter plots of measured grain versus estimated grain values prepared for training and holdout groups helped to validate model performances.



as expected with an increasing percentage of cloud coverage per subset, the rmse increases too. for instance, in column 15, the rmse has increased from 0.04 to 0.07 and then to 0.26 as the cloud percentage increases from 28% to 54% and then to 58%. this is because with increasing cloud coverage the prediction method iteratively enlarges the prediction subset along the spatio-temporal dimension to get the required statistics. it could add also heterogeneous values in the subset that increases the probability of offering poor prediction estimates. in addition, it contradicts the theoretical assumption that a much narrow prediction subset is expected to offer good quality prediction in line with tobler's first law of geography that states near things are more related than distant things.



under section 3.2, phenology-based cloud restoration implemented using gapfill. the restored ndvi used to create the ndvi variable called sum-ndvi. sum-ndvi is the sum of all ndvi values per each of the study farm boundaries(mirasi et al, 2019). we also validated the accuracy of the restoration process. in this section as a prediction problem, we implement linear regression analysis by combining observations both from original cloud-free and restored images. the addition of observations from the restored image increases the total number of observations. nevertheless, whether such increment positively influences the prediction problem should be explained. hence, the purpose of the analysis is to identify how the inclusion of observations from restored images affects the regression process.



2018/10/26 that got increment of observations resulted in improvement of adj r2 values from 0.44, 0.56, 0.27 and 0.42 to 0.55, 0.57, 0.41 and 0.46 respectively. this revealed the incorporation of restored observations in the regression process is advantageous in increasing prediction capability and implies the values are good enough.



nonetheless, an image from the date of 2017/09/26 did not show any improvement. furthermore, in 2018/10/16 image, though the total number of observations increased from 40 to 67, the adj r2 decreased from 0.65 to 0.44. in this study, we applied similar tuning parameters across the spatio-temporal image set; nonetheless, the performance of these tuning parameters could vary across the scenes in space and time domain. this in turn could influence the quality of the restored images and ultimately the derived variables.



in general, across the seven methods and along with the three datasets rpart, glmnet, glm and nnet are the four models yielding the lowest rmse values on the training dataset. all the four models resulted in rmse values of 0.001 t/ha for sum-ndvi dataset of the year 2018 and 2017, while rpart revealed the same result on the third dataset too. random forest is also among the models yielding smallest rmse values on sum-ndvi and inputs(2018) dataset. conversely, two models such as svm and knn showed in relative terms lower performance across the three datasets except knn revealed one of the lowest outputs(0.001 t/ha) on sum-ndvi& inputs(2018) dataset.



0.001 t/ha on the training group to 0.006 t/ha on holdout group using sum-ndvi(2018) dataset. likewise, the glm model on sum-ndvi(2017) showed a slight increase of rmse from 0.001 t/ha on the training group to 0.015 t/ha on holdout group. the neural net model has rmse of



conversely, rpart and rf in relative terms showed overfitting across the three datasets. for instance, rpart and rf models on sumndvi(2018) have rmse 0.001 t/ha and 0.028 t/ha on the training dataset that increased to 0.161 t/ha and 0.136 t/ha on the holdout group. on the other hand, svm and knn models demonstrate properties of underfitting where performance on the training is lower than the holdout.



the ensemble models, presented under section 3.3.5, were implemented using seven base learners as input and do not show improvement over base models. nonetheless, previous studies revealed, to develop an ensemble learner with better predictive potential, two conditions need to be met(krogh and vedelsby, 1995; zhou, 2009). first,



accordingly, the same type of ensembles such as en.rf, en.knn, en.glm and en.glmnet that were better using the full base learners found to be also the better ones using the ranking method too. nonetheless, the new sets of ensembles based on ranking method do not show improvement over the first group of ensembles using full base learners. in summary, in this study, though ensembles developed using two different approaches, they did not result in improved performance over base learners. this could be associated with the limited diversity of base learners and the use of a small dataset where each observation point has a large weight on model performance affecting model stability.



therefore, in terms of getting a fully restored subset, the cloud percentage per day of the year is the most important parameter. in this study, we used square tiles that resulted in full cloud cover in some sub-images and get predictions for most of the study area. future studies might consider other tiling procedures that potentially avoid or minimizes the number of sub-images with full cloud cover.



in applying gapfill, in this study, we started by considering two key challenges. first, since the s2 sensor is recently available, images are available only for three years. second, s2 is a high spatial resolution sensor(10 m) making the total number of missing values to be too large. hence, it demands the use of high computing infrastructure, which is unfortunately not available in many situations.



given such limitations, the study has successfully applied gapfill under the current study area. in this study, our main goal was to understand the predictive skill of the sum-ndvi parameter using original and restored observations using gapfill. except for a single date(2018/10/ 16), the incorporation of restored observations in regression analysis offered an increased prediction skill. thus, the gapfill method offered 0.001 t/ha, 0.136 t/ha and 0.001 t/ha for sum-ndvi(2018) dataset as well as 0.001 t/ha, 0.192 t/ha and 0.001 t/ha for sum-ndvi(2017) dataset. in contrast, rpart and rf models in relative terms showed overfitting across the three datasets. on the other hand, models such as svm and knn demonstrate properties of underfitting in which performance on the training is lower than the holdout. the addition of other hyperparameters besides the limited parameters used in the current study could improve the performance of these models.



random forest is an ensemble algorithm that use bootstrap aggregation method, in particular, it computes average prediction from various decision trees predictions. this enables rf models to be less influenced by outliers. this study is implemented using small number of observation and decided to keep all the sample observations that potentially constitute some outliers. thus, algorithms such as rf, which are less prone to outliers, are expected to yield increased performance. besides, rf algorithm is powerful in handling both linear and non-linear relationship(murthy, 2020; trehan, 2020).



as a growing field of research, the use of ml under small dataset domain various across various disciplines. in some fields, for instance, in materials physics and chemistry, numerous studies applied ml to small dataset. a backpropagated neural network model using 53 points used successfully to predict the properties of ultrahigh-performance concrete. a strategy named as crude estimation property was proposed to improve ml on small datasets(around 100 data points)(zhang and ling, 2018). similarly, shaikhina et al., 2015 designed neural net and decision tree algorithms for prediction of antibody-mediated kidney transplant rejection using 35 bone specimens and 80 kidney transplants and achieved high accuracy of 98.3% and 85% respectively.



in some previous studies, ensemble models showed superiority to base models. for example, a stacking ensemble of ann revealed a relative rmse of 6.8% and r2 of 0.68 at the predicted yield in sugar cane crop using modis ndvi time series. the superior performance of ensemble models is also reported on other biophysical datasets. an ensemble of gradient boosting, multi-narrative adaptive regression spline, random forest, and support vector machine outperformed the individual models for surface soil organic carbon stocks. an average-ensemble model is recommended as the best model in materials design using 25 numbers of observations.



overall, comparison of the current outputs with previous study outputs could be problematic due to factors such as discipline difference and difference in study design and procedure. even for same discipline, the implementation of different study design(resampling procedures, outlier treatments, data split, and the number and type of hyperparameters searched) might contribute to reveal different result. therefore, the difference between the two models(glm and rf) and the rest five models could be explained partly by the study design implemented.



the study evaluated base machine learning and ensemble methods for wheat yield prediction. ensemble methods produced a similar performance with that of base models. ensemble models using the seven predictors from base models as well as using three selected predictors based on accuracy following the ranking methodology showed similar performance. the study, employing sum-ndvi dataset, predicts wheat grain yield with rmse of 0.001 t/ha and 0.136 t/ha using glm and rf models respectively. the two models showed consistent and better performance across the three dataset groups. besides, they showed good generalization on holdout dataset that is also supported using scatter plot analysis.



