hand signs are an effective form of human-to-human communication that has a number of possible applications. being a natural means of interaction, they are commonly used for communication purposes by speech impaired people worldwide. in fact, about one percent of the indian population belongs to this category. this is the key reason why it would have a huge beneficial effect on these individuals to incorporate a framework that would understand indian sign language. in this paper, we present a technique that uses the bag of visual words model background subtraction. surf (speeded up robust features) features have been extracted from the images and histograms are generated to map the signs with corresponding labels. the support vector machine (svm) and convolutional neural networks (cnn) are used for classification. an interactive graphical user interface (gui) is also developed for easy access.



communication has always played a vital role in human life. the calibre to interact with others and express ourselves is a basic human necessity. however, based on our upbringing, education, society, and so on, our perspective and the way we communicate with others can differ to a great extent from those around us. in addition to this, ensuring that we are understood in the way we intend, plays a very important role.



1978. but since no standard type of isl existed, its use was restricted to short-term courses only. in addition, the gestures used in most of the deaf schools varied significantly from each other and nearly 5% of the total deaf people attended these schools. it was in 2003 when isl got standardized and grabbed the attention of researchers [4].



indian sign language (isl) involves both static and dynamic signs, single as well as double-handed signs, and in different regions of india, there are many signs for the same alphabet. it makes it very difficult to introduce such a scheme. in addition, no standard dataset is available. all these things manifest the complexity of indian sign language.



however, for real-time systems, researchers needed a faster way to solve this problem. the advancements in deep learning technologies have enabled automation of image recognition using various image recognition models. for e.g., convolutional neural networks have made great strides in the field of deep learning in recent years [21,22].



g. jayadeep et al. [23] used a cnn (convolutional neural network) to extract image features, lstm (long short term memory) to classify these gestures and translate them into text. bin et al. [24] proposed the inceptionv3 model to use depth sensors to identify static signs. it eliminated the steps of gesture segmentation and feature extraction. in ref. [25], vivek bheda et al. proposed a methodology for using a mini-batch supervised learning method of stochastic gradient descent to



looking into these works, authors were motivated towards creating a custom dataset and an algorithm that would work fully on that dataset without affecting the accuracy of the video detection. we decided to use surf features because it would decrease the time of measurement and make the system invariant to rotation. the authors of the paper have also addressed the problem of background dependency so that the sys- tem can be used anywhere and not only in controlled environments.



it is a very crucial part of the research works in all the arenas as it is fundamental to foster the development of any machine or deep learning model. however, it is full of challenges. during data collection, the biggest challenge we faced was that there were no standard datasets for indian sign language available. therefore, as part of this project, we attempted to manually construct a dataset that could help us overcome this problem.



the camera is very critical. to add variations in the dataset, two options were used for capturing the images. the first one is the default method, which performs the skin segmentation on the image and can be used with a plain colour background.



in the default option, the captured video frame is converted into hsv colour space for the images acquired with the plain background. as the hue colour of the skin is different from that of the background, it gets extracted easily. an experimental threshold is then applied to the frame that calculates hue and filters out the skin coloured pixels from the image. further, the image is binarized, blurring is done to remove noises and maximum contour is obtained from the result assuming that the contour with the largest area represents the hands. errors are further removed by applying the median filter and morphological operations.



in the second method for the images with the running background, the first 30 frames are considered as background and for the remaining frames the absolute difference is calculated between the additive sum of those 30 frames and the new frame, which gives us the foreground re- gion of the current frame. the images are first converted into grayscale and then gaussian filter is applied. for hand segmentation, a mask is created by extracting the maximum connected region in the foreground assuming it to be the hand. noises are further removed by applying morphological operations like erosion and dilation.



after this, the canny function is used in which the gradient of each pixel calculates the edge strength and direction of the images. compared to the original image, this results in a shift of intensity and the edge is detected easily. the pre-processed images from both the options are shuffled to add variation in the dataset.



very large. it is comparable to k-means, but is better in terms of pro- cessing time and memory use. it utilises small random batches of fixed- size data at a time, thus reducing the need to have all the data in the memory at the same time. a new random sample from the dataset is obtained in each iteration and used to update the clusters, which is repeated until convergence. we have a value of k as 180 for this purpose. for codebook generation, the resulting cluster centres (i.e., cen- troids) are treated as our code vectors. a codebook is used for quantizing features where it takes a feature vector as input and maps it to the index of the nearest code vector. the constructed vocabulary can be repre-



for this classification, we have used svm with a linear kernel. we have passed the histograms of visual words to the svm as feature vectors for the classification and recognition of isl signs. the training is done using a total of 28,800 images. after the training is completed, the performance of the classifier is checked on the testing set which has a total of 7236 images, and its performance is evaluated on various pa- rameters like accuracy, precision, recall, etc.



slides over the local patches of the image. such pieces are called fea- tures, and they compare two images by finding approximately the same features at approximately the same locations. cnns have a better ability to see images and classify them than other neural networks.



this is done to provide better communication and ease to the user. once the label is identified by the classifier, it is passed to a dictionary as a key which returns the corresponding sign as value. this is then shown to the user. for text to speech conversion, python text to speech module, pyttsx3 is used. as it causes a delay in the live video stream by making the frames process at a very slow rate, threading is performed. due to this, the prediction of signs and the translation of text to speech can be achieved at the same time. this ensures that the sound is played continuously, without any disturbance.



the suggested method of recognition based on the surf character- istics has the benefit of fast computations and is robust against rotation, orientation, etc. being a user-independent model, it is also capable of solving the problem of background dependency with the condition of keeping the camera still. however, with plain backgrounds, it can be used freely. the previous works have either used a plain background or have used complex background under some controlled environments. the recognition accuracy is close to 0.94 for most of the models. how- ever, most of the signs reported use single hands or simple hand movements [29]. our model is capable of recognizing double hand signs and machine translation of visual information into text or speech with high accuracy of 99%. it is a basic step further in removing some of the drawbacks by helping researchers to use this approach with a standard dataset.



the main goal of our work is to provide a more real-time recognition utility so that the system can be used anywhere. it is achieved by con- structing a custom data set, making the system invariant to rotation and solving the background dependency problem. the system is successfully trained on all 36 isl static alphabets and digits with an accuracy of 99%. in future, the dataset can be expanded by adding more signs from different language of various countries, thereby achieving a more effective framework for real-time applications. the method can be extended for the formation of simple words and expressions for both continuous and isolated recognition tasks. the secret to true real-time applications is enhancing the response time.



shadman shahriar, ashraf siddiquee, tanveerul islam, abesh ghosh, rajat chakraborty, asir intisar khan, celia shahnaz and shaikh anowarul fattah. real- time american sign language recognition using skin segmentation and image category classification with convolutional neural network and deep learning. in tencon, ieee region 10 international conference.



