understanding the context of the scene is one of the most important aspects for new generation of autonomous vehicles. it is a very trivial task for a human to recognize the scene context by only a single look at the picture, however, for a computer, it is still a challenging task. this problem can be solved by automatic data labeling using deep-learning models for scene classification. relying on scene type labels we can select relevant scenes to prepare a balanced dataset to train more advanced instance detection models using data from a specific road condition.



this study presents a novel framework based on a deep convolutional neural network (cnn) for the automatic road scene classification of street-level automotive images. for the evaluation of our approach, we use a well-known autonomous benchmark dataset, from which we extract geo-position data and combine them with predictions from the scene classification model to get ground truth labels to train and evaluate a resnet-50 model for scene classification. the results and comparison with state-of-the-art methods are presented.



current achievements in image classification are significant, al- though the majority of efforts build to classify a wide range of image categories [1,2]. deep convolutional neural networks have been show- ing impressive classification results and continuous improvement on the object-centric imagenet classification benchmark [3]. however, scene categorization is an important step for further improvement of visual perception for autonomous driving. the scene-centric places- cnn, for scene classification/recognition introduced in work [4], which has been trained on 2.5 million images from 205 scene categories of places, including outdoor scenes. an extended version of the dataset with 2.1 images for classification of 365 categories has been presented in work [5]. the scene-level task is more challenging in comparison to the object-centric task feature learning due to the larger diversity of scenes and the number of possible combinations in scenarios.



in most cases, autonomous datasets do not provide traffic scene labels, but only object-level attributes and a general description of the dataset in original paper [10]. in some cases, we have scene categories per video clip without precise per frame annotation [10]. however, the annotation of traffic condition is not provided. visual analysis and labeling of road scenes would be very time-consuming for automotive datasets.



in this paper, we study image classification in the context of road scene classification for automotive datasets. our focus is on scene categorization/classification is based on a single image for categories as urban, rural, highway road scenes. in order to achieve it, we use two convolutional neural networks, one to obtain a scene labels out of object-level classes. the image-based scene labels have been combined with the output of our map-based approach, which is using spatial



the remainder of the paper is organized as follows: section 2 gives a review of related works and section 3 explain our proposed methodology and system architecture. section 4 presents the results of scene classification for proposed methods. finally, we conclude by discussing of our results and avenues for further research.



image classification in general. the problem of image classification was initially on developing classification systems which would archive the best possible accuracy results on pascal voc competitions [1]. one of the best where best approaches [12,13] are based on extraction of many low-level local features, coined to one histogram and non-linear kernel classifiers such as support-vector machine (svm) are used to perform classification. the next step in performance has been obtained with spatial fisher vectors (sfv) [14,15] which aggregate local descriptors features, encoding them with statistics, fit a gaussian mixture model (gmm), and feeding them to kernel svm classifiers. however, in 2012 it was shown that convolutional neural networks (cnns), so-called alexnet [16] trained in a supervised fashion on imagenet dataset outperformed the fisher vectors. next-generation of depth network architectures further improved classification accuracy: vggnet [17], googlenet [18], resnet [11] shown that network depth is a crucially important for reach a success.



traffic scene classification. early work in the image-based scene clas- sification proposes to use the probability of each object detected on the image converted to a vector in order to classify the scene, where each object consider as a typical for one of the scene classes [19]. another common approach for the scene classification is using semantic labels. in the work, [20] scene semantic segmentation labels were used to classify 13 urban texture classes, including different types of road layouts, the presence of cars, pedestrians, and a pedestrian crossing in front of the vehicle. in the papers [21,22] semantic labels used for the understanding of traffic scenes, cnn semantic segmentation network was trained on images taken from the same location, but under different weather or illumination conditions.



in the paper, [9] pooled features extracted from densenet-121 deep network architecture were used to train svm classifier with rbf kernel for such traffic scene classification: highway, road, tunnel, exit, settlement, overpass, booth, traffic. another recent work [23] presented a traffic scene classification method based on resent-50, fine-tuned and pretrained on the places365 dataset. in addition, standard long short- term memory modules (lstm) architectures were added, which allow improving results by the temporal aspect.



image classification for spatial data. an urban scene recognition based on spatial data, which in most cases satellite images, which has im- portant application value in improving urban land use rate and land use monitoring [24]. a deep-network strategy actively used in order to predict geographical objects by exploring deep features of the high- resolution imagery [25]. such an approach helps to extract semantic contents of the area and generate high-resolution semantic maps, which might play a crucial role in carving out the path ahead for autonomous vehicles. another way to get semantic labels is using openstreetmap (osm) [26], the project is established to share a knowledge collective that provides user-generated street maps. each map is consists from



the residual network resnet-50, which was trained on a custom dataset containing images from popular autonomous datasets (e.g., kitti, nuscenes) to detect the occurrence of urban, rural, and highway scenes, despite weather and light conditions, with distributed attention to a different level of details on the scene.



urban condition. by urban area condition, we assume that it is a built- up area, with a high population density and infrastructure of the human-made environment. mostly, it is a public road in the city district or residential neighborhood, with dens residential buildings, where potentially could be largely populated by people and many public places.



rural condition. in the rural area condition, we could consider an opposite situation to urban, with a low-density population and with a low number of total man-made infrastructure. such areas are away from the city, on the scene you can see a lot of vegetation.



highway condition. the highway definition can be simply described as the main or public road. most public roads have at least two lanes, one for traffic in each direction, separated by lane markings. the highway road is comparably easy to distinguish from urban and rural since the most distinctive feature is that the dominant place is taken by the road itself. however, highways can lay down through a city or countryside, which implies many elements inherent in urban and rural conditions. on most road maps the highway roads are usually marked as a dark yellow or orange color.



the experiment is implemented using pytorch v1.8.1. out of 3276, we left 1080 images to have the same number of images for each class. then data was divided into train, validation, and test sets, in proportion 70/10/20, where 756 images belong to train, 108 images to validation,



the hyper-parameters for training are set as follows. the learning rate is initialized to 0.001 with momentum 9%, a decay learning rate by a factor of 0.1 every 7 epochs. the size of the batch is 4. also, the last layer of the pretrained model is replaced with a new softmax layer, which is relevant to the number of 3 classes.



in order to improve the training process, we apply random trans- formation functions. at first, with a certain probability algorithm crop a random portion of an image and resize it to size 224 to 224 pixels. secondly, with a probability of 0.5, it horizontally flips our images. finally, we do normalization of the images.



the paper [29] shown that data augmented by adding new images provide better results of accuracy and the training process goes faster comparing to the traditional way of data augmentation with the same amount of images to train on. in order to improve the result of our model, we decide to extend our dataset up to 1779 images, where 1419 images belong to train, and the same 177 images for validation and test sets respectively. adding horizontally flipped images as a new image in combination with random crop and resize should provide the model more features and generalize the model.



taking into account all those factors it is planned to experiment with other state-of-the-art datasets where image conditions might be different and more diverse. another promising direction would be to use unsupervised methods for scene classification based on clustering techniques using embedding features from convolution layers. we be- lieve that scene condition labeling can be used for both online and offline applications such as semantic mapping of autonomous datasets and driving assistance.



