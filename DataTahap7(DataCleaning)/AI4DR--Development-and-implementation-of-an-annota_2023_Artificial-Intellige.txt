high throughput screening (hts) is one of the major strategies used in the pharmaceutical industry for hit finding. lately, screening tech- nologies have become more sophisticated [1], leading to approaches like quantitative hts [2] and strategies where more counter-screens or selectivity assays are used to qualify hits. in turn, these techniques have raised the volume of dose-response (dr) results generated. other large dose-response datasets are obtained after the interrogation of protein libraries by selection techniques such as phage display, yeast display or fluorescence-activated cell sorting (facs). the quality of the dose- response curves (drcs, also known as concentration-response curves) is dependent on the screening conditions, on the protocols and overall assay robustness, and on the behavior of the compounds. the basic auto- matic analysis of these curves relies on a fitting algorithm which might be unreliable in suboptimal settings [2] because of the presence of out- liers due to interference effects or other technical artifacts. in practice, drcs need to be manually reviewed and acted upon in order to lead to a decision concerning the follow-up of the corresponding compound in the project. thus, the visual inspection step is time consuming, even more when the hit rate is high, and the outcome of this step is dependent on the quality of the curves, the experience of the expert and the time available for the analysis. when dealing with large amounts of results, this approach delays the project and might lead, over time, to a lack of consistency and robustness in the analyses.



dr experiment can be perturbed, leading to curves that do not belong to these ideal shapes. some are only related to the compound proper- ties (e.g. colloidal aggregation or toxicity issues at high concentration) [3,4,5], others also depend on the assay type [6], the cell lines [7] or on the experiment protocol (e.g. colored and fluorescent compounds interfere with luminescence assays) [5]. the pains concept [8] has been promoted for the identification of structural characteristics asso- ciated with false positive compounds in assays, but pains compounds tested in dr assays do not necessarily show curves with defects, and vice-versa [9].



we kept those in ordinate in order to differentiate between e.g. partial and full inhibition. other featurizations of the drcs and other machine learning approaches could also lead to good quality classifiers and in- deed, using a cnn was not the most obvious choice. however, it allowed not only to benefit from all the recent improvements of this very popu- lar algorithm, but also to seamlessly manage the classification of drcs measured with a different number of doses than our training set or with missing points: control experiments showed that the performance of the classifier is not much affected by these differences in the input data (see below). overall, the architecture and the parametrizations of the cnn were optimized to extract the general shape of the drc regardless of its details and the presence of outliers.



the non-consensual dataset, so we examined if the classifier agreed with at least one of the annotations given by the experts. interestingly, the model performance is high over the three final labels, and comparable to the one in the consensus dataset (96.5% vs 95.3%) but is markedly lower (81.0% vs 87.5%) on the 12-categories shape model. indeed, the classifier is more easily confused by examples that are borderline enough for experts to disagree on them.



for classifying drc with different parameters. as it shows several weakly active compounds annotated as active antagonists, it might also be use- ful for questioning studies which use the tox21 datasets for building models [20]. we provide a github repository hosting the ai4dr shape and dispersion models, the rf model, scripts for performing the drc annotation by both methods on the tox21 dataset together with two analysis jupyter notebooks for the ai4dr and the rf shape classifiers.



for the hts expert, the main benefits of using the ai4dr plugin are related to speed and robustness: having the possibility to review similar curves together allows if needed to perform the same adjustments once for the whole set, to end up with the same final annotation. furthermore, it allows to spot more easily specific profiles that might be related to the assay biology, as it can happen for compounds with complex mode of action identified from cellular screening. considering drc specificities could also benefit for the understanding of structure-activity relation- ships by enriching the number of hits among the chemical series, re- vealing series-specific assay interference issues, and rescuing singleton compounds that would otherwise have probably been discarded.



in summary, we have developed and implemented a novel approach to ease the analysis of high-throughput dr experiments using a combi- nation of ml models. indeed, if a few approaches have been described for comparing and clustering drc [3,21], the use of deep learning ap- proaches for the classification of dr experiments has to our knowledge not been reported yet. to build this solution, we had to solve three main issues: first, getting to a deep understanding of the business needs in or- der to tailor correctly the solution.



robustness and usefulness in quickly correctly classifying a large set of drc. in addition, during this exercise, we have identified interesting cases where questionable assignments could be easily identified and re- vised by experts. this could be important in two contexts. first, when using experimental datasets to develop a predictive model using ma- chine learning, where the quality of data is crucial for the predictivity of the model made. in this context, ai4dr could serve to filter out ques- tionable results and keep only clearly active compounds. second, when large dataset of drc is analyzed by several scientists (e.g. in the con- text of collaborations or consortia), ai4dr could remove some inherent bias between experts to have a homogeneous primary assignment and clustering before validation.



