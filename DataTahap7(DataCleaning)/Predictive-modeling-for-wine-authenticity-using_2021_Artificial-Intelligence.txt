support vector machines (cortes and vapnik, 1995) is one of the best known and most used classification algorithms which aim to find a hyperplane with maximum margin to separate the classes of data. this hyperplane is a special type of linear model. in the linear case a database with two class of data is easily separated by a hyperplane in two groups, which one represents one of the classes and the other group represents the other class. the samples that are closest to the maximum-margin hyperplane are called support vectors. the non- linear cases begin to be solved in 1995 when cortes & vapnik intro- duced the use of the soft-margin, which uses slack variables to penalize samples that characterize the non-linearity of data. when there are no linear decision boundaries the original dataset (x) is converted into a new space f(x) by the kernel trick. in the new space f(x) there is a linear decision boundary that separates the samples into their classes.



the feature selection (variable elimination) helps in understanding data, reducing the effect of high dimensionality, reducing computa- tional time and improving the classifier performance (chandrashekar and sahin, 2014). we use correlation-based feature selection (cfs) to select a subset of the most important variable and random forest im- portance (rfi) to order the variables selected. the cfs selected a feature subset that contains features highly correlated with the class, yet uncor- related with each other (hall, 1999). the random forest importance (rfi) uses the random forest classifier to measure the importance of input variables (breiman, 2001). this technique is a filter feature selec- tor, which uses variable ranking as the principle criteria for variable se- lection by ordering.



the classifications occurred by the use of 10-fold cross validation to evaluating the performance of svm. this technique splits the dataset (in our case, 83 samples) into 10 folds and performs 10 classifications. each classification is accomplished in the following way: nine folds are used to train the model and the tenth fold is used to test the model, generat- ing a precision rate (accuracy). this process is performed until all ten folds are used for testing. at the end of the process, the performance of the classification model is given as the average of the accuracy separation between bm and cc can be observed. the samples of am and ut are separated from bm and cc, but there is an overlap between these classes. the variables selected by cfs showed more discriminative power than all phenolic and volatile compounds when analyzing the distance among the samples.



classification models. it is presented the model id, the variables used as input to svm and the accuracy values for each classification model. the higher result is the one that used nine variables, catechin, gallic, octanoic acid, myricetin, caffeic, isobutanol, resveratrol, kaempferol and orac, with 93.97% of accuracy. just one variable results in the lower accuracy, 68.49%. these value increases when variables are added. the peak of accuracy occurs with the #09 classification model. this value decreases and stabilizes between a range of values when more variables are added.



however, as the results of this study demonstrated, there is some vari- ables that do not contributed to the classification model. the use of all chemicals resulted on a low predictive performance and there is no clear separation on the pca score plot. the selection of a feature subset played an important role to classify the commercial categories of south american wines.



it was found that a group of phenolic and volatile was the more dis- criminative compounds. the model #09 used seven phenolic and two volatile compounds. the orac was previously identified by the use of feature selection as one of the main chemicals that are able to classify chilean and brazilian sauvignon cabernet wines (da costa et al., 2016), uruguayan and brazilian tannat wines (costa et al., 2018), and south american merlot wines (costa et al., 2019). zhang et al. (2010) selected 11 volatiles compounds from chinese red wines by the use of stepwise linear discriminant analysis. the authors found that esters and alcohols are responsible for varietal classification of chinese cabernet sauvignon, cabernet gernischt and merlot wines.



