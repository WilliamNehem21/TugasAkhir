almost all software contain defects. some defects are found easily while others are never found, typically because they emerge seldom or not at all. some defects that emerge relatively often even go unnoticed simply because they are not perceived as errors or are not sufficiently severe. software defects may give rise to several types of errors, ranging from logical/functional ones(the program sometimes computes



the main objective of this study is(1) to identify significant static analysis functionality provided by the tools, but not addressed in a normal compiler, and(2) to survey the underlying supporting technology. the goal is not to provide a ranking of the tools; nor is it to provide a comprehensive survey of all functionality provided by the tools. providing such a ranking is problematic for at least two reasons: static analysis is generally only part of the functionality provided by the tool; for instance, klocwork k7 supports both refactoring and software metrics which are not supported by the two other tools. even if restricting attention only to static analysis functionality the tools provide largely non-overlapping functionality. secondly, even when the tools seemingly provide the same functionality(e.g. detection of dereferencing of null pointers) the underlying technology is often not comparable; each tool typically finds defects which are not found by any of the other tools.



studying the internals of commercial and proprietary tools is not without problems; in particular, it is virually impossible to get full information about technical solutions. however, some technical information is publicly available in manuals and white papers; some of the tools also originate from academic tools which have been extensively described in research journals and conference proceedings. while technical solutions may have changed somewhat since then, we believe that such information is still largely valid. we have also consulted representatives from all three providers with the purpose to validate our descriptions of the tools. still it must be pointed out that the descriptions of suggested technical solutions is subject to a certain amount of guessing in some respects.



languages such as c and, to a lesser extent, c++ are designed primarily with efficiency and portability in mind 4, and therefore provide little support to avoid or to deal with runtime errors. for instance, there is no checking in c that read or write access to an array is within bounds, that dereferencing of a pointer variable is possible(that the variable is not null) or that type casting is well-defined. such checks must therefore be enforced by the programmer. alternatively we must make sure that the checks are not needed, i.e. guarantee that the error conditions will never occur in practice.



by the term static analysis we mean automatic methods to reason about runtime properties of program code without actually executing it. properties that we consider include those which lead to premature termination or ill-defined results of the program, but precludes for instance purely syntactic properties such as syntax errors or simple type errors. 5 nor does static analysis address errors involving the functional correctness of the software. hence, static analysis can be used to check that the program execution is not prematurely aborted due to unexpected runtime events, but it does not guarantee that the program computes the correct result. while static analysis can be used to check for e.g. deadlock, timeliness or non-termination there are other, more specialized, techniques for checking such properties; although relying on similar principles. static analysis should be contrasted with dynamic analysis which concerns analysis of programs based on their execution, and includes e.g. testing, performance monitoring, fault isolation and debugging.



if the property checked for is a defect then we refer to case(ii)(b) as a false positive. hence, if the analysis reports that a program may divide by zero we cannot tell in general whether it is a real problem(item(i)) or if it is a false positive(item(ii)(b)). the precision of the analysis determines how often false positives are reported. the more imprecise the analysis is, the more likely it is to generate false positives.



pathand context-sensitivity rely on the ability to track possible values of program variables; for instance, if we do not know the values of the variables in the boolean expression of a conditional, then we do not know whether to take the thenbranch or the else-branch. such value analysis can be more or less sophisticated; it is common to restrict attention to intervals(e.g. 0< x< 10), but some approaches rely on more general relations between several variables(e.g. x> y+z). another important issue is aliasing(see e.g.[14,28]); when using pointers or arrays the value of a variable can be modified by modifying the value of another variable. without a careful value and aliasing analyses we will typically have large numbers of false positives, or one has do ungrounded, optimistic assumptions about the values of variables.



it is sometimes claimed that static analysis can be applied to incomplete code(individual files and/or procedures). while there is some truth to this, the quality of such an analysis may be arbitrarily bad. for instance, if the analysis does not know how a procedure or subprogram in existing code is called from outside it must, to be sound, assume that the procedure is called in an arbitrary way, thus analyzing executions that probably cannot occur when the missing code is added. this is likely to lead to false positives. similarly incomplete code may contain a call to a procedure which is not available, either because it is not yet written, or it is a proprietary library function. such incomplete code can be analyzed but is also likely to lead to a large number of false positives and/or false negatives depending on if the analysis makes pessimistic or optimistic assumptions about the missing code.



6 soundness can be used in two completely different senses depending on if the focus is on the reporting of defects or on properties of executions. in the former(less common) sense soundness would mean that all positives are indeed defects, i.e. there are no false positives. however, the more common sense, and the one used here, is that soundness refers to the assumptions made about the possible executions. even if there is only a small likelihood that a variable takes on a certain value(e.g. x=0) we do not exclude that possibility. hence if the analysis infers that x may be zero in an expression 1/x, there is a possibility that there will be a runtime error; otherwise not. this is why a sound analysis may actually result in false positives, but no false negatives.



while polyspace appears to be aiming primarily for the embedded systems market, klocwork and coverity have targeted in particular networked systems and applications as witnessed, for instance, by a range of security checkers. klocwork and coverity address essentially the same sort of security issues ranging from simple checks that critical system calls are not used inappropriately to more sophisticated analyses involving buffer overruns(which is also supported by polyspace) and the potential use of so-called tainted(untrusted) data. the focus on networked application also explains the support for analyzing resource leaks since dynamic management of resources such as sockets, streams and memory is an integral part of most networked applications.



an incremental analysis may take significantly less time than analyzing the whole system from scratch. with the other tools analysis of the whole system has to be redone. all of the tools provide the possibility to analyze a single file. however such an analysis will be much more shallow than analyzing a whole system where complete paths of execution can be analyzed.



trivial and writing new, non-trivial checkers is both cumbersome and error prone. there are no explicit guidelines for writing correct checkers and no documented support for manipulation of abstract values(e.g. interval constraints). there is also no support for reusing the results of other checkers. termination of the checker is another issue which may be problematic for users not familiar with the mathematical foundations of static analysis, see e.g.[6,27].



all three tools support analysis of the c programming language and c++. at the initial time of this study only klocwork supported analysis of java but coverity was announcing a new version of prevent with support for java. only polyspace supported analysis of ada. klocwork was the only provider which claimed to handle mixed language applications(c/c++/java).



on the more exotic side coverity provides a checker for stack use. it is unclear how useful this is since there is no uniform way of allocating stack memory in different compilers. klocwork is claimed to provide similar functionality but in a separate tool. polyspace set themselves aside from the others by providing checkers for non termination, both of functions and loops. again it is unclear how useful such checkers are considering the great amount of research done on dedicated algorithms for proving termination of programs(see e.g.[13,2]). coverity has a checker for uncaught exceptions in c++ which was still a beta release. polyspace provides a useful feature in their support for writing general assertions in the code. such assertions are useful both for writing stubs and may also be used for proving partial correctness also of functional properties; see.



none of the tools provide very sophisticated support for dealing with concurrency. klocwork currently provides no support at all. coverity is able to detect some cases of mismatched locks but does not take concurrency into account during analysis of concurrent threads. the only tool which provides more substantial support is polyspace which is able to detect shared data and whether that data is protected or not.



several users had expected the tools to find more defects and defects that were more severe. on the other hand, several users were surprised that the tools found bugs even in applications that had been tested for a long time. there might be a difference in what users find reasonable to expect from these tools. there might also be large differences in what users classify as a false positive, a bug or a severe bug.



many of the defects found could not cause a crash in the system as it was defined and used at the moment. however if the system would be only slightly changed or the usage was changed the defect could cause a serious crash. therefore these problems should be fixed anyway.



evaluation 1(coverity and flexelint): the chosen application had been thoroughly tested, both with manually designed tests and systematic tests that were generated from descriptions. flexelint was applied and produced roughly 1,200,000 defect reports. the defects could be reduced to about 1,000 with a great deal of analysis and following filtering work. these then had to be manually analyzed. coverity was applied to the same piece of code and found about 40 defects; there were very few false positives and some real bugs. the users appreciated the low false positive rate. the opinion was that the defects would hardly have been found by regular testing.



the users had expected coverity to find more defects. it was believed that there should be more bugs to be found by static analysis techniques. it was not known if this was the price paid for the low false positive rate or if the analyzed application actually contained only a few defects. the users also expected coverity to find more severe defects. many of the findings were not really defects, but code that simply should be removed, such as declarations of variables that were never used. other defects highlighted situations that could not really occur since the code was used in a restricted way not known to the analysis tool.



was known to have some memory leaks was analyzed using coverity and klocwork. in total klocwork reported 32 defects including 10 false positives and coverity reported 16 defects including 1 false positive. only three defects were common to both tools! hence klocwork found more defects, but also had a larger false positive rate. although the tools looked for similar defects the ones actually found were largely specific to each tool. this suggests that each of the tools fail in finding



looking at only the memory leaks the results were similar. klocwork reported 12 defects of which 8 were false, totalling 4 real defects and coverity reported 7 defects all of which were true defects. none of the tools found any of the known memory leaks.



evaluation 4(coverity and klocwork): old versions of two c++ products were analyzed with coverity and klocwork. trouble reports for defects that had been detected by testing were available. one purpose was to compare how many faults each of the tools found. another purpose was to estimate how many of the faults discovered in testing were found by the static analysis tools.



coverity found significantly more faults and also had significantly less false positives than klocwork. one of the major reasons for this was the handling of third party libraries. coverity analyzed the existing source code for the libraries and found many faults in third party code! klocwork did not analyze this code and hence did not find any of these faults. besides that the analysis of the libraries that coverity did resulted in fewer false positives in the application code since it could be derived that certain scenarios could not occur.



codepro analytix(developed and marketed by instantiations) is a tool aimed for analysis during development. it is integrated into the eclipse ide and the results of an analysis cannot be persistently saved, but only exist during the development session with the ide. the analysis is not as deep as that of coverity or clockwork, but is faster and can easily be done interactively during development. the tool generates a great deal of false positives, but these can be kept at a tolerable level by choosing an appropriate set of analysis rules. no detailed analysis was done of the number of faults and if they were real faults or not.



string and pattern matching approaches: tools in this category rely mainly on syntactic pattern matching techniques; the analysis is typically pathand context-insensitive. analyses are therefore shallow, taking little account of semantic information except user annotations, if present. tools typically generate large volumes of false positives as well as false negatives. tools(often derivatives of the lint program) have been around for many years, e.g. flexelint, pc-lint and splint. since the analysis is shallow it is possible to analyze very large programs, but due to the high rate of false positives an overwhelming amount of post-processing may be needed. these tools are in our opinion more useful for providing almost immediate feedback in interactive use and in combination with user annotations.



sound dataflow analyses: tools in this category are typically pathand context-sensitive. however, imprecision may lead to analysis of some infeasible paths. they typically have sophisticated mechanisms to track aliasing and relationships between variables including global ones. the main difficulty is to avoid excessive generation of false positives by being as precise as possible while analysis time scales. the only commercial system that we are aware of which has taken this route is polyspace verifier/desktop. the great advantage of a sound analysis is that it gives some guarantees: if the tool does not complain about some piece of code(the code is green in polyspace jargon) then that piece of code must be free of the defects checked for.



tool only means that the tool was unable to find any potential defects. as witnessed in the evaluations different unsound tools tend to find largely disjoint defects and are also known not to find known defects. hence, analyzed code is likely to contain dormant bugs which can only be found by a sound analysis.



