predicting defect-prone software components can play a significant role in allocating relevant testing resources to fault-prone modules and hence increasing the business value of software projects. most of the current software defect prediction studies utilize traditional supervised machine learning algorithms to predict defects in software applications. the software datasets utilized in such studies are imbalanced and therefore the reported results cannot be reliably used to judge their performance. moreover, it is important to explain the output of machine learning models employed in fault-predication techniques to determine the contribution of each utilized feature to the model output. in this paper, we propose a new framework for predicting software defects utilizing eleven machine learning classifiers over twelve different datasets. for feature selection, we employ four different nature-inspired search algorithms, namely, particle swarm optimization, genetic algorithm, harmony algorithm, and ant colony optimization. moreover, we make use of the synthetic minority oversampling technique(smote) to address the problem of data imbalance. furthermore, we utilize the shapley additive explanation model for highlighting the highest determinative features. the obtained results demonstrate that gradient boosting, stochastic gradient boosting, decision trees, and categorical boosting outperform others tested model with over 90% accuracy and roc-auc. additionally, we found that the ant colony optimization technique outperforms the other tested feature extraction techniques.



the rest of the paper is structured as follows: section 2 showcases a variety of related studies conducted in the field. section 3 illustrates the proposed software defects prediction framework, used datasets, feature selection approaches, and employed classifiers. section 4 shows the used evaluation metrics and hyperparameters tuning process. section 5 discusses the experimental results and shows classifiers output explanations using shap. finally, the conclusion and future directions are shown in section 6.



many experimental, investigational, empirical, and comparative research approaches have recently been proposed in the domain of software defect prediction. in using seven datasets obtained from the national aeronautics and space administration, six machine learning classifiers were tested: logistic regression, random forest, naive bayes, gradient boosting, support vector machine, and neural networks. moreover, the results showed that neural networks offer the greatest results with an accuracy of 93% while utilizing 10 folds cross-validation. in deep learning and bio-inspired feature selection methods are being used. particle swarm optimization(pso) was used to evaluate neural network performance by reducing strongly correlated features. in addition, four nasa datasets were gathered and preprocessed with



using a firefly search-based algorithm(fa) for feature selection. three machine learning classifiers were used to evaluate the utility of fa for selecting the best features and enhancing model performance: support vector machine, naive bayes, and k-nearest neighbors. when compared to the original findings, the support vector machine resulted in the best results and improved accuracy by 4.53%. the study in proposed a new method comprising the use of several deep learning approaches for software defects prediction utilizing the genetic algorithm(ga) for features selection and particle swarm optimization(pso) for data clustering. moreover, five nasa datasets were collected and cleaned from missing values. the results revealed that deep neural networks outperform others with an accuracy of 98.47% using 10 folds cross-validation.



several research recommends combining numerous feature approaches into a single list rather than using individuals. in introduced a novel method to handle the problem of high dimensionality data and filter method ranking in software defect prediction by combining various feature filter methods: chi-square, information gain, and relief. the suggested method compares the outcomes by assessing the effectiveness of decision trees(dt) and naive bayes(nb). in addition, nine nasa datasets were used and sanitized. the findings show that the suggested method outperforms the employment of individual methods, with nb accuracy increasing by 6.73% and dt increasing by 1.87%.



also obtained and preprocessed. however, for minimizing highly correlated features, a greedy search-based algorithm was used. the findings of the ensemble learning techniques indicated that boosted smo yields the best results when compared to others, with an accuracy of 88.2%, and voting produces an accuracy of 87.9%.



imbalanced data, also known as imbalanced target classes, is a data distribution problem that causes machine learning classifiers to be biased toward one category over others, negatively affecting prediction. nasa datasets, on the other hand, exhibit a wide distribution gap across target classes. as a result, this study is based on the employment of smote for data balance. smote is an oversampling technique that generates synthetic data samples to increase minority class data samples by locating the nearest kneighbors, computing the distance between them then multiplying a random value between 0 and 1 for the new sample.



referred to as a particle and a group of birds is referred to as a swarm(i. e., population). the core notion of pso is that each particle represents the best solution to the problem. however, following the random initialization, the location of each particle is modified, resulting in new



knn is a distance-based method that determines the distance between training and test samples. knn classifier is a sluggish algorithm since it learns during the evaluation phase and saves data samples during the learning stage. without standing, the knn passes through many phases. begin by determining the value of k(number of nearest samples), then calculate the distance between the training samples and the new data sample using a distance function. furthermore, the distance values will be sorted and the nearest samples identified based on the value of k, and the prediction value will be determined based on the majority of the class values.



rf is a bagging ensemble-based classifier. as the basic learners, it divides the training data into several decision trees. rf is also a nonparametric model. however, a restricted random number of rows is chosen from the data population, and a set of decision trees is built for each segment to generate classification output. an individual decision tree might be bigger than others. a majority vote in rf indicates a metaclassifier, which is a classification decision made based on the judgments of the basic learners. it supports numeric and categorical data types, as well as complicated predictor functions.



xgboost is an ensemble method based on boosting. it is a more advanced version of gradient boosting. it was also created for large and intricate datasets. unlike adaptive and gradient boosting, the xgboost employs unique regression trees. forming what is known as born-again trees. xgboost tree formation, on the other hand, begins with a single leaf, much like gradient boosting. furthermore, gradient boosting and regularization are critical steps in xgboost. for many years, xgboost has been one of the most effective boosting methods. in their experimental investigation, ismail and faisal demonstrated that xgboost outperforms other classifiers such as rf, svm, radial basis function neural network, and naive bayes. it is simple to use and provides a high level of discriminative prediction accuracy. in addition to its ability to



the findings of the proposed classifiers for predicting software defects are discussed in this section. smote was utilized for data balance after data preprocessing. four meta heuristics methods, on the other hand, were tuned and used for feature selection. we employ the logistic map function as a chaotic type and 20 initial particles with 20 iterations in aco. we utilized a population size of 20 with 20 iterations in pso. in ga, we employed a crossover probability of 6% with a maximum generation of 20. in addition, we employed the logistic map function in ha with 20 iterations and population size.



kc1 datasets before and after using meta-heuristic feature selection algorithms. the testing accuracy average, on the other hand, was computed to highlight the comparison of outcomes. overall, the results showed an improvement in testing accuracy. gb, sgb, and catboost, on the other hand, outperform the others. this is not surprising given that boosting methods enhance variance while decreasing bias. in addition, by expanding minority class samples, smote increases the number of data samples.



comments have the strongest impact in cat boost at the 10th observation, at 25.17, relative to the base value of 0.6, however, the design complexity metric has a negative impact. overall, it can be notated that lines of code comments, design density, and halsted content metrics have the highest discriminative power to the predictive models.



however, the mc1 dataset is regarded to be one of the largest nasa datasets, with a total of 9277 occurrences, and we were able to retain the same accuracy by applying the ga algorithm due to the broad data dispersion over training and testing sets. the findings of the mc2 dataset revealed that six classifiers, including dt, knn, gb, sgb, adaboost, and rf, out of eleven, had the best accuracy by comparison. additionally, smote and pso yield the best outcomes, with an average accuracy of 99%.



the plots show the contributions of 35 data samples. the x-axis in global force reflects the number of data samples, while the y-axis shows shap predicted values. in addition, we employ a bee swarm plot to emphasize the features with the highest discriminative power. for clarity, the most significant features of overall classifiers are call pairs, global data density, and essential density metrics.



