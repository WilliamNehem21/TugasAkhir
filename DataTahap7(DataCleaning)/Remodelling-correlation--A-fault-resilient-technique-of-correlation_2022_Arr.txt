computation on binary numbers using stochastic computing is gaining popularity nowadays because it offers several advantages compared to conventional weighted-binary computation. it is a low power and low cost alternative to complex arithmetic functions. with a remarkable reduction in size, circuit complexity and power consumption, stochastic architecture has proved to be noise immune compared to the conventional implementation of binarization algorithms and various other image processing tasks also. different types of errors, such as soft errors, correlation induced errors, and random fluctuation errors are identified that affect the accuracy and reliability of stochastic circuits. thus, to generate the desired function using unreliable components in the presence of errors has become a challenging task. transient or soft errors are caused due to exposure to external radiation and are greatly increased by manufacturing defects in a chip. these introduce false logic at the output of the circuit. if multiple faults occur at the nodes of a gate, the output may be erroneous. soft errors are the cause of bit-flips in a stochastic bitstream and may lead to an undesirable correlation between two numbers.



ability, bitstreams are injected with desired correlations. experiments are conducted on sles whose functional behaviour vary with changes in correlation. the objective is to develop a technology-independent framework for observing error-free output in complex circuits with minimal hardware. a study is also conducted to demonstrate that the reliability of a circuit is unaffected by subtle changes in correlation status. the contributions are highlighted as follows:



circuits are becoming more vulnerable to transient faults as feature sizes, operating voltages, and design margins continue to shrink. cosmic rays, capacitive coupling, electromagnetic interference, and power transients are some of the leading physical phenomena. transient faults induced by radiation have gained a lot of attention in recent years because they are seen as a potential roadblock to further technological advancement. in stochastic computing another major challenge is the in the probability of a given number when a single level circuit is considered. but for multi-level circuits this can effectively change the overall value. in the next section we focus on two major error sources and study their effect on the behaviour on logic circuits.



correlation between two bitstreams has been identified as a major source of inaccuracy in certain stochastic circuits. correlation in stochastic computing indicates that the bitstreams generated by lsfr or sng inherit some sort of dependence between them(cross-correlation) or between the bits of the same bitstream(auto-correlation). earlier, correlation in stochastic circuits could only be vaguely identified as inaccurate output caused by a pair of bitstreams when passed through an and gate. but, recently, correlation in stochastic computing has been quantified and identified with definiteness.



as semiconductor technology advances with reduced feature size and increased scalability, it is becoming more prone to soft errors. the sources of such soft errors have been traced to mainly alpha particles and high energy cosmic rays. although soft errors are not unique to stochastic circuits, its properties make it more tolerant to soft errors than weighted-binary logic circuits. soft errors do not affect the circuit physically, but they introduce behavioural changes in the circuit in the form of bit flips by introducing false logic[27,28]. transient or soft errors are caused due to exposure to external radiation and are greatly increased by manufacturing defects in a chip. these introduce false logic at the output of the circuit. so, if multiple faults strike environment where the circuit is prone to bit-flip errors. for nano-scale devices, transient or soft errors are growing prominence as the device features are downscaled to sub-micron ranges. the observed output might exceed the error threshold due to the change in the expected value of signals and also due to unwanted correlation introduced during bit flips. for larger circuits, this may be a major concern for accuracy.



a similar analysis to reduce errors at different degrees of transient faults. the target function is obtained considering an initial non-zero and positive value of correlation. it is observed that for an existing negative scc between input variables the effect of transient errors in the circuit element is enhanced. thus, such cases are excluded in our analysis.



rates. the deviation in output from the actual value(without error) using the proposed scheme is much less compared to the non-prioritybased approach. it has been found that this method can deal with high error rates while using less hardware. furthermore, the desired value can be achieved with fewer iterations when using a priority-based approach. the deviation graph with the priority-based(red) approach is obtained with one reco block whereas the deviation with non-priority based approach(gray) is obtained with two reco blocks to model the output. the blue line represents to the observed error(without reco). this demonstrates the efficacy of the proposed priority-based approach in terms of hardware design. the efficacy of the proposed method relies heavily on the presence of correlation sensitive logic blocks in the circuit. if there are no correlation sensitive logic blocks in the circuit the method fails drastically. also, care needs must be taken while placing the reco blocks at specific targets without which errors could be propagated to non-erroneous output node.



