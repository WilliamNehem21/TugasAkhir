machine learning(ml) is one of the intelligent methodologies that have shown promising results in the domains of classification and prediction. one of the expanding areas necessitating good predictive accuracy is sport prediction, due to the large monetary amounts involved in betting. in addition, club managers and owners are striving for classification models so that they can understand and formulate strategies needed to win matches. these models are based on numerous factors involved in the games, such as the results of historical matches, player performance indicators, and opposition information. this paper provides a critical analysis of the literature in ml, focusing on the application of artificial neural network(ann) to sport results prediction. in doing so, we identify the learning methodologies utilised, data sources, appropriate means of model evaluation, and specific challenges of predicting sport results. this then leads us to propose a novel sport prediction framework through which ml can be used as a learning strategy. our research will hopefully be informative and of use to those performing future research in this application area.



in this paper, we provide a critical survey of the literature on ml for sport result prediction, focusing on the use of neural network(nn) for this problem. several studies in the statistical and operations research literature have previously considered sport results prediction, but the use of the nn paradigm for this purpose is a more recent area of study. the powerful nn technique has proven to be effective in deriving highly accurate classification models in other domains. discussions on the challenges that arise when using these intelligent models for sport results prediction is also provided. our main contribution is that a crisp-dm type framework for sport result prediction is proposed(srp-crisp-dm), based on the six steps of the standard crisp-dm framework. this paper serves researchers, sport fans, club managers, bookmakers, academics, and students who are interested in intelligent solutions based on nn for the challenging problem of sport results prediction. this paper will be of use to those who are interested in pursuing future research within this application domain.



this remainder of this paper is organized as follows. in section 2, studies that have used ann exclusively, which was the key approach used in earlier research papers in the sport prediction application, are reviewed. section 3 then provides critical discussion and observations on prior work in this application domain, in the context of the proposed srp-crisp-dm framework, conventional measures of model performance, and how we propose that model performance should be measured for the problem of sport results prediction. finally, section 4 concludes the paper.



purucker conducted one of the initial studies on predicting results in the national football league(nfl) using an ann model. data from the first eight rounds of the competition and five features were used, consisting of yards gained, rushing yards gained, turnover margin, time of possession, and betting line odds. unsupervised methods based on clustering were used to distinguish between good and poor teams. an ann with backwardpropagation(bp) was then used. purucker achieved 61% accuracy compared with 72% accuracy of the domain experts. the bp algorithm was found to be the most effective approach. a limitation of this study is that only a relatively small number of features were used.



and the trunk, specific power of the abdominal muscles, and grip power. the numeric class variable used was the average distance of three throws from a full run-up after a 30 min warm up. through experimentation, the best architecture in terms of normalized root mean squared error, of the neural network was found to be 4-3-1(four input neurons/variables, one hidden layer with three neurons, and one outcome). the javelin throws of 20 javelin throwers from the polish national team were predicted using the models, and were compared with the actual length of the throws. their results showed that the neural network models offered much higher quality of prediction than the nonlinear regression model. the absolute network error was found to be 16.77 m, versus the absolute regression error of 29.45 m.



wiseman predicted winning pga golf score based on scores after round 1 of a competition. note that they were predicting winning score, not tournament winner itself. the authors compared the performance of: linear regression, neural network regression, bayesian linear regression, decision forest regression and boosted decision tree regression, in the microsoft azure service. the authors performed correlation matrix analysis of different features and selected round 1 leading score, round 1 average score, course par, major event, course yardage and total prizemoney as the predictors. r-squared value and mse were used to evaluate algorithm accuracy. data from 2004 to 2015 was used to construct the models, and tournaments from 2016 were used to validate them. linear regression and bayesian linear regression were the best performing models on the 2016 data set, predicting the winning score to within 3 shots 67% of the time.



matches will be bet on. for example, there will likely be some betting odds threshold where, although the model predicts a victory for that team, the betting odds are so low that the return does not warrant betting on that match at all(e.g. if a team is paying$1.01 to win, meaning that a$100 bet placed would only return$1).



data for sport prediction is often able to be obtained online from publically available sources. some prior studies have automated the data collection process, writing scripts that automatically extract the online data and then load it into some form of database. some studies have also built an end-user interface, where users can input data for an upcoming match and the prediction is then generated.



hucaljuk and rakipovic included an expert-selected feature set in addition to their initial feature set. although this was not found to result in improved accuracy, this could depend on the characteristics of the sport, and perhaps the experts themselves. another way that expert opinion can be used is in comparing the predictive accuracy of the predictive models, with the predictions of the experts. to incorporate expert opinion, one could either generate an expert-selected feature set to compare with machine-learned feature selection approaches, or alternatively, compare their model with expert predictions.



to evaluate model performance, one would classify match results into home wins, away wins and draws(if the sport has draws) and then look at the number of matches that the model has correctly identified, using a standard classification matrix. there is unlikely to be a great degree of imbalance in the class values for the dataset, although given the commonly observed home advantage phenomenon, one is likely to see a slight skew in favor of home wins. in this case, classification accuracy is a reasonable measure of evaluation. in cases where the data is highly imbalanced, roc curve evaluation may be more appropriate.



rather than round-by-round prediction, another possibility is to update the training data set after every match has been played. in this case, all past matches up to the current match as training data, and the upcoming match as the training data(i.e. only having that one record as the training data). this is essentially like orderpreserved leave-one-out cross-validation. this match-by-match approach is probably not necessary unless teams play more than one match over the same competition round.



uate models that predict basketball match results. prior seasons may not be relevant to predict matches in future seasons, particularly in sports where team rosters and strengths can change significantly from year to year. this approach may not give a reliable picture of model performance(although this could be mitigated to some extent if player level data is included, and so player changes would be captured from season to season). we would argue that, although more computationally intensive and laborious, our round-by-round training test split approach mentioned above should be used within each season. an average model classification accuracy could then be produced for each season, and a plot could be shown of model accuracy by season.



