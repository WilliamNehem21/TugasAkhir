in the area of nlu, a few concurrent methods were proposed based on continuous prompts, focusing on improving knowledge probing(qin and eisner, 2021; zhong et al., 2021). lester et al.(2021) showed that with large pretrained models, only tuning continuous prompts with a frozen lan-



in this paper, we present a method p-tuning that uses continuous prompts in concatenation with discrete prompts. p-tuning improves performance and stabilizes training for pretrained language model adaptation. p-tuning is effective with both tuned and frozen language models under both the few-shot and fully-supervised setings.



