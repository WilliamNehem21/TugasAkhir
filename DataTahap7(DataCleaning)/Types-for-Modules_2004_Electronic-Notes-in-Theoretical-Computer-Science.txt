the programming language standard ml is an amalgam of two, largely orthogonal, languages. the core language expresses details of algorithms and data structures. the modules language expresses the modular architecture of a software system. both languages are statically typed, with their static and dynamic semantics specified by a formal definition.



over the past decade, standard ml modules has been the source of inspiration for much research into the type-theoretic foundations of modules languages. despite these efforts, a proper type-theoretic understanding of its static semantics has remained elusive. in this thesis, we use type theory as a guideline to reformulate the unconventional static semantics of modules, providing a basis for useful extensions to the modules language.



our first extension generalises modules to higher-order, allowing modules to take parameterised modules as arguments, and return them as results. we go beyond previous proposals for higher-order modules by supporting a notion of type generativity. we give a sound and complete algorithm for type-checking higher-order modules. our second extension permits modules to be treated as first-class citizens of an ml-like core language, greatly extending the range of computations on modules. each extension arises from a natural generalisation of our type-theoretic semantics.



modules allows definitions of identifiers denoting core language types and terms to be packaged together into possibly nested structures. access to structure components is by the dot notation and provides good control of the name space in a large program development. structures are transparent: by default, the identity of a type component within a structure is evident from outside the structure. this provides good support for the incremental construction of large programs.



standard ml is distinguished from most other programming languages by the existence of a formal definition of both its syntax and semantics. it is a strongly typed language with static type checking of programs performed prior to program execution. these two phases are defined, respectively, by separate static and dynamic semantics. type checking ensures the absence of certain run-time type errors, such as accessing an undefined component of a structure, or using a core value component at a type incompatible with its definition. in this sense, standard ml is similar to the formal languages studied in type theory. this field of logic has close connections to computer science and provides a rational basis for the design of programming languages. indeed, the type-theoretic underpinnings of the core language are well-understood.



in recent years, largely inspired by the success of standard ml modules, the study of the type-theoretic foundations of module languages has become an active topic of research. nevertheless, despite numerous attempts, a proper type-theoretic understanding of standard ml modules, and its static semantics in particular, has remained elusive. the benefits of a typetheoretic understanding are twofold. type theory provides us with a framework for analysing existing features of the language and for synthesising new features by generalisation.



this thesis has two main objectives. the first is to provide a better, more type-theoretic formulation of the static semantics of modules. the second is to use this formulation as the rational basis for designing proper extensions of the static semantics.



this approach is rather different from those of other researchers in the area. reductionist approaches aimed at providing type-theoretic semantics of modules by a translation into existing type theories have either failed to capture significant features and properties of the language, or imposed severe limitations inherited from the chosen model. others approaches have relied on introducing new, and often badly behaved, type-theoretic constructs.



in chapter 5, we extend the modules language of chapter 3 to higherorder. functors are given the same status currently enjoyed by structures: they may be bound as components of structures, specified as functor arguments and returned as functor results. we give a sound and complete algorithm for signature matching that forms the basis of a type checking algorithm for higher-order modules. this chapter builds on the alternative semantics of generativity given in chapter 4 and on previous work by biswas[bis95] that extends a skeletal fragment of modules to higher-order. we reformulate, generalise and clarify his definitions, and use them to prove analogous results. our work furthers his by capturing a notion of type generativity, and by catering for more realistic core languages, e.g. languages supporting polymorphic values and parameterised types.



in chapter 6 we briefly discuss the foundations of a separate compilation system for modules. one of the main criticisms of standard ml modules is its perceived lack of support for separate compilation. we review the simple approach to separate compilation in traditional programming languages and explain why previous attempts to adopt this approach in standard ml have failed. unlike other researchers, we place the blame for this failure on an inappropriate choice of compilation unit, not on the semantics of modules.



in chapter 7 we turn our attention to the particular core language presented in chapter 3, core-ml, and consider relaxing the stratification between core and modules. we obtain a language with first-class modules: modules may be passed as arguments to core-ml functions, returned as results of arbitrary computations, selected using conditional expressions, stored in data structures and so on. our approach is novel in maintaining the distinction between core and modules. instead of amalgamating the features of both in a single language, we provide constructs for packing and unpacking module terms as core values, allowing programs to alternate between core and modules level computation.



the components of a structure identifier can be accessed by using the dot notation. for instance, the type intnat.nat refers to the type component nat of intnat and denotes the type int. the term intnat.zero refers to the value component zero of intnat. it evaluates to 0, and has type int. structure bodies may themselves contain definitions of(sub)structures.



functors ensures that conceptually distinct applications of the same functor return distinct abstract types. in standard ml, a functor may only be defined at the outermost or top-level of a program. in particular, a functor may not be defined as a component of a structure, applied to a functor, or return another functor as a result. these restrictions mean that functors are first-order mappings on structures.



tional, stronger notion of sharing, called structure sharing. encapsulating a body of declarations within struct and end generated a new internal structure name, or stamp, for the expression, similar to the generation of a fresh abstract type. however, the name identified the entire structure, not just individual types. in signatures, structure sharing constraints could be used to specify structure components with shared names. this made it possible to specify sharing not only of types, but also of values, since any two structures which shared the same name must have originated from a common ancestor. this interesting but little used feature has been abandoned in the revised definition of standard ml[mth96]. the move simplifies the semantics, benefitting both the working programmer and language implementor.



although the published definition of standard ml[mth90, mt91, mth96] formally defines the modules language, it makes few concessions to help the reader understand its features. it particular, no attempt is made to relate these features to well-known concepts developed in the theory of programming languages. this has presented an obstacle not only to the understanding of the language, but also to its further development.



remark 2.2.2(the phase distinction). a typed programming language is said to obey a phase distinction[car88b] if the type of any term in the language can be checked without evaluating arbitrary terms. this allows the semantics of the language to be split into a static semantics of type checking, that is performed at compile-time, and a dynamic semantics of evaluation, that is performed at run-time. the phase distinction is important because it ensures that the tractability of type checking is independent of term evaluation, which, in typical programming languages, may fail to



from a programming language perspective, adding universal quantification allows us to express polymorphic programs. a polymorphic program is a program whose operation is generic in a type. the operation of appending two lists is a good example of a polymorphic operation, since it is independent of the type of elements stored in the list. with simple types, we have to define a new, but essentially identical, append operation for each type of list element. polymorphism allows us to get away with a single definition, leading to substantial savings in code and maintenance.



remark 2.2.4(the logical interpretation). from a logical perspective, this extension alone is not particularly meaningful. however, if we combine it with the extension to quantified types, we obtain a system that corresponds to higher-order propositional logic. that is, we can now quantify not only over propositions, but also over functions from propositions to propositions and so on.



research related to this thesis can be divided naturally into three categories: type-theoretic approaches to modular programming, type-theoretic accounts of standard ml modules, and type-theoretic alternatives to standard ml modules. we shall discuss each of these in turn and finish with a section on miscellaneous related work. in chapter 9 we will revisit some of this work to compare it with the results of this thesis.



despite these similarities, dl fails to model most of the other important features of modules. it is impossible to specify type definitions in signatures, preventing the expression of shared type components. components are accessed not by identifier but by positional notation. dl has no notion of subtyping(corresponding to signature matching), making it impossible to treat a structure as if it had a type declaring fewer components, a type higher-order functors in[mt94]. the behaviour of this static semantics is reflected in an early implementation of higher-order functors in the standard ml of new jersey compiler[at 93]. the static semantics is very complicated and departs radically from the existing first-order semantics of standard ml. in particular, it relies on the compile-time execution of a nontrivial language of identity stamps to account for full transparency.(on the other hand, the dynamic semantics for this proposal admits a straightforward formalisation that is studied by maharaj and gunter[mg93].)



taking a different tack, biswas[bis95] proposes an alternative static semantics for higher-order functors that is also fully transparent. this semantics is based on a direct generalisation of a fragment of the existing standard ml semantics. his ideas, which we believe have not received the attention they deserve, will be discussed in detail, reworked and extended to the full language in chapter 5.



duction, which runs foul of the phase-distinction. the fact that xml type checking becomes undecidable in the presence of impredicative strong existential types leads harper and mitchell to conclude that the extension of standard ml with first-class modules is incompatible with decidable type checking. hml[hmm90], a further refinement of xml, is a predicative theory that preserves the phase distinction by adopting a non-standard formulation of dependent types.



one of the characteristics of standard ml is that the static semantics of the language is defined, not in terms of the type syntax of the language, but with respect to an intermediate language of static semantic objects. during classification, type phrases are elaborated to semantic objects, and the classification of term phrases is done in terms of these semantic objects. this style of semantics can be criticised for two reasons. from a software engineering perspective, the classification of terms using semantic objects means that the type of a term cannot be reported to the programmer in the syntax of the language. in particular, this makes it more difficult to relate type errors to the source text of the program. it also raises the possibility that some terms have semantic objects that are not expressible in the syntax of the language, impeding simple approaches to separate compilation. from a proof engineering perspective, the classification of terms using semantic objects makes it difficult, and perhaps impossible, to prove the type soundness property of the language in terms of its type syntax. these properties are at odds with the syntactic nature of types and soundness proofs in type theory.



in this section, we define the modules language. given a core language supporting a notion of definable types and terms, modules provides a typed calculus for manipulating collections of core type and term definitions. although the choice of core language is largely arbitrary, we do need to make some assumptions on its structure. these are stated as hypotheses. they are sufficiently weak to accommodate a wide variety of core languages.



we can present the syntax of modules as a collection of phrase classes defined by a grammar. modules is an explicitly typed language. for this reason, it is convenient to group the phrase classes of both modules and the core according to whether they belong to the syntax of types or the syntax of terms. the concrete grammar of core definable types and terms depends on the core language in question:



example 3.1.1(core-ml). in core-ml, a definable type is a parameterised simple type describing a family of simple types. the kind of a definable type is the number of type parameters it expects. a value type, on the other hand, is a universally quantified simple type, reflecting the polymorphism of core-ml value expressions. finally, a value expression is either a function, a function parameter, a function application, or an occurrence of a core-ml value defined within a module.



following standard ml[mth90, mth96], the static semantics of modules distinguishes between the type phrases of the language and the semantic objects they denote. as we shall see in section 3.1.3, a type phrase is wellformed provided it denotes some semantic object, according to a denotation judgement. similarly, a term phrase is well-typed provided it can be classified by some semantic object, according to a classification judgement. in this section, we define the semantic objects of modules.



in fact, in standard ml[mth90, mth96], semantic structures are defined as finite maps on identifiers: we prefer to use an inductive definition to make it easier to prove properties about them. the difference is not significant. in particular, our retrieval functions and the soon to be defined enrichment relation(definition 3.17), are immune to the order in which components appear, as in standard ml.



(e-13) the structure body defines t as the denotation of d: the remainder of the body b is classified in the context extended with the assumption[t= d]. the types generated by the entire phrase are just the types generated by classifying b. the component is added to the resulting structure.



(e-14) the structure body defines x as the value expression e. provided e has value type v, the remainder of the body b is classified in the context extended with the assumption[x: v]. the types generated by the entire phrase are just the type generated by classifying b. the type of x is recorded in the resulting structure.



(e-16) the rule is similar to rule(e-15). the difference is that the definition of x is local b, and does not become a component of the resulting structure. note that the type variables generated locally by s are still recorded in the output set.



remark 3.2.1(relating type phrases to their denotations). observe that the structure of corresponding syntactic and semantic objects is almost isomorphic. the essential difference between them is this: while simple type phrases may contain applications of type occurrence phrases, semantic simple types may not; instead, they allow for(well-kinded) applications of type names. it should come as no surprise that the denotation judgements merely replace type occurrences by expanding their denotations, preserving the structure of the original phrase in all other respects.



[mac86, hm93, hmm90]. indeed, the use of dependent types lingers on in the more recent type-theoretic alternatives to standard ml proposed in[hl94, ler94, ler96b, ler95, lil97, sh96, hs97]. by contrast, in the following sections and the next chapter, we shall present evidence to suggest that dependent types play no discernible role in the semantics of mini-sml. since mini-sml is merely a cut down version of standard ml, we will conclude with the counter-claim that dependent types have no role in the semantics of standard ml.



c. the judgement states that the term p has type o. we stress that o is a semantic object, not a syntactic type phrase. the classification judgements are clearly of a different nature from the denotation judgements discussed in the previous section: while a denotation judgement merely translates a syntactic type phrase to its semantic representation, a classification judgement relates a term to its type. indeed, the denotation of a term phrase would be defined by the dynamic semantics of terms, which we have not presented. classification judgements are thus instances of the familiar typing relations we encountered in our introduction to type theory(section 2.2).



researchers in the area to propose that dependent types underly the type structure of standard ml. however, by inspecting the static semantics of mini-sml, we have found ample evidence to suggest that mini-sml can be understood by relying only on the simpler type-theoretic notions of type parameterisation, type quantification and subtyping. since mini-sml is merely a cut down version of standard ml, we can make the counter-claim that dependent types have no role in the static semantics of standard ml. the generative classification judgements, however, do not sit nicely with our type-theoretic understanding of other aspects of the static semantics. in chapter 4, we focus our attention on the generative judgements, and expose them as a particularly operational incarnation of existential quantification over types. by the end of that chapter, we hope to have discredited the claim that dependent types are necessary to explain the type structure of



of course, if we rule out the redefinition of type identifiers already bound in the context, the need to distinguish them using type variables disappears. but this is a harsh restriction and goes against one of the main motivations for modules: to provide a mechanism of name space control allowing component names to be reused within different modules. after all, outside of the structure body, the definitions are referred to by distinct phrases t and x.t.



the unsuccessful but sound classification of the same phrase. the example demonstrates how the generation of fresh types at each and every functor application preserves soundness. the denotations of y.u and z.u are correctly distinguished. the offending subphrase is underlined.



remark 4.3.2(motivating groundedness). although we will not need this property, we should also point out that insisting on ground functors ensures that the type of a functor application is uniquely determined by the type of the actual argument. as a counter-example, consider the mini-sml functor j defined as:



proof(sketch). the proof follows easily by simultaneous induction on the rules defining the judgements. the first three clauses are core language dependent, but must be proven together with the remaining statements in order deal with subphrases containing type and value occurrences.



in this chapter, we extend the modules language of chapter 4 to higherorder. functors are given the same status that structures enjoy in modules: they may be bound as components of structures, specified as functor arguments and returned as functor results. we will continue to refer to the first-order language collectively as modules. its generalisation will be called higher-order modules. the core language remains the same.



with the first implementation of n, which used the abstract module nat, we implicitly assumed that every value of type nat.nat corresponded to a natural number. since we are now using a proper subset of the integers, we should enforce the invariant that only positive integers are ever used as natural numbers. grouping the definitions of n, a and m into a single module and



the ability to define functors as structure components is crucial in this example. if we could only define functors at top-level, as in(first-order) modules, we would be faced with only two design options, both of which are bad: we could either make the implementation of n.nat public, compromising integrity but supporting efficient implementations of a and m, or private, preserving integrity, but rendering efficient implementations of a and m impossible.



while it is possible to do this, the practical ramifications for programming in the language are rather severe. to ensure data abstraction(one of the key motivations for using a modules language), programs have to be written in a fully functorised form. by this we mean the following. suppose p[m] is a program with an occurrence of a module m, implementing the signature s, and we wish to ensure that the module expression m can be replaced by any other implementation m' matching s. using an abstraction, we can isolate m from its context p by writing p[m\ s]. if this program type-checks, then so does p[m'\ s]. if we remove generativity from the language then we can no longer accommodate the abstraction phrase. without abstractions, we can only ensure the above property if p is written as the outermost application of a functor to m, i.e. if p has the form(functor(x: s)m'')[m]. notice that if the application is not outermost, i.e. p is merely in the form p'[(functor(x: s)m'')[m]] for a non-empty program context p', then the inner functor application may propagate the actual implementations of types in m that are meant to be abstract according to the signature s. with access to the concrete implementations, the outer context p' can inadvertently make use of this information and violate the intended abstraction, preventing the replacement of m by m'. unfortunately, insisting on fully functorised code leads to an unnatural and unintelligible coding style in which all abstract modules must be anticipated early on and imported as initial functor arguments, possibly at considerable distance from their point of use. as macqueen[mac86] rightly points out, this seriously impedes the incremental construction of programs, which is, after all, the main motivation for using a modules language. notice, also,



remark 5.2.1(for type theorists). there is also a more theoretical motivation for generalising projections. if we want to prove a syntactic subject reduction result for modules, then a key lemma we will need is that the type of a functor application is preserved when substituting the actual argument for the formal argument of the functor. it is easy to see that the phrase class strpath is not even syntactically closed under substitution of module phrases for identifiers, making it impossible to state this lemma, let alone prove it. by generalising projections from paths to projections from arbitrary module expressions, the syntax of modules becomes closed under substitution, bringing us one step closer3 to proving syntactic subject reduc-



in short, we have generalised the first-order concepts of definition 3.4 by allowing higher kinds, well-kinded applications of type names to types, and well-kinded type functions. as in the first-order setting, type names enter core semantic objects when they arise as the denotations of type occurrences.



unfortunately, specification 5.12 cannot be taken as a proper(inductive) definition of the enrichment relations, since one of the relations we are specifying occurs in the antecedent(i.e. in a negative position) of the clause relating functors. however, it is relatively easy for a programmer to understand, and can be treated as a specification of the enrichment relations we will define. in the next section, we will give a proper inductive definition of enrichment and show that it satisfies specification 5.12. for the moment, we can observe that the property expressed by the specification is obviously reflexive and transitive, forming a good basis for a subtyping relation.



as we discussed, specification 5.12, although intuitive, cannot serve as a definition of the enrichment relations between structures, functors and modules. in this section, we define enrichment as a collection of inductive relations and show that they form a pre-order that is closed under realisation. using these properties, we can then prove that our definition satisfies specification which succumbs to rule induction and an appeal to hypothesis 5.19. the stronger induction hypothesis corresponds to splitting the proof of transitivity into a simultaneous proof of transitivity for objects enriched by realisations of o', together with a proof of transitivity for objects enriching realisations of o.



the classification rules for structure bodies are the same as their first-order counterparts(cf. section 4.2.1): rules(h-12) and(h-13) merely generalise the first-order rules(t-15) and(t-16) by catering for module definitions instead of structure definitions, but are otherwise unchanged.



the classification rules for module expressions deserve the most comment. rules(h-15) and(h-16) subsume the role of the first-order rules(t-19),(e-9) and(e-10), but also cater for generalised module projections. rules(h-18) and(h-19) are new and formalise the applicative semantics of section 5.2.2 for anonymous functors and module applications. the remaining rules for encapsulating a structure body, curtailing a module by a signature and abstracting a module by a signature are unchanged from the first-order rules of section 4.2.1, except that the last two now apply to both structures and functors.



in the preceding sections we focussed on producing an algorithm for matching. although the algorithm is only sound and complete provided its inputs are ground and solvable, we can now turn to the static semantics of higherorder modules to show that, whenever we need to invoke the algorithm, the matching problem will indeed be well-posed. to this end, we first define a notion of ground contexts, ground existential modules, and solvable signatures.



in the previous section, we identified a programming style that supports separate compilation. as long as the programmer adheres to this style, she can separately type-check(and compile) the components of her program. the style is flexible enough for practical programming, because the syntax of signatures provides fine control over the abstraction of individual type components.



in mini-sml, as in standard ml, it is possible to redeclare an identifier which is already declared in the current context. this is useful, as it allows the same component name to be re-used within substructures and subsignatures. the meaning of an identifier is resolved by static scoping: each occurrence of an identifier refers to its textually most recent declaration.



x.t in a context where the outermost definition of x is in scope. the range of v can only be specified as x.u in a context where the inner specification of x is in scope. since the definition and specification of x both declare the same identifier, one must eclipse the other and they cannot be in scope at the same time. because the actual definition of its v-component cannot be specified, the type of y cannot be fully captured by a signature.



we will sketch another solution, that relies on distinguishing declarations by their binding depth. observe that the example above is problematic only because the syntax of mini-sml does not allow us to distinguish between different declarations of the same identifier, in this case the structure identifier x. every reference to an identifier is resolved by static scoping. in the semantics, this behaviour is ensured by defining contexts as finite maps. extending a context by a new declaration overrides any previous declaration of that identifier.



our alternative solution relies on defining contexts, not as finite-maps, but as lists of declarations. new declarations are added to the head of the list, without forgetting the effect of previous declarations. in this way, all declarations are preserved in the inverse order in which they were added.



our technique is essentially a combination of named identifiers and de bruijn[deb72] indices. although terms written in pure de bruijn notation are notoriously difficult for humans to read, our scheme seems more acceptable in realistic programming situations. first, we need only use an index when we need to refer to an eclipsed identifier(this rarely occurs in practice and can easily be avoided by disciplined programming). second, the counting scheme is relative to identifiers of the same name: hence indices, when they need to be used, are small and manageable.



the aim of this section is to sketch a modules language that, by adopting the changes discussed in sections 6.4.1 and 6.4.2, namely the introduction of indexed identifiers to prevent eclipsing, and the removal of abstractions to rule out anonymous abstract types, enjoys the following, informal property: base kind? classifying definable types. semantic structures and contexts no longer declare value components. the removal of the abstraction phrase means that existentially quantified types are no longer needed. finally, instead of being finite maps, contexts are defined inductively as lists of declarations to support the use of indexed identifiers. for readability, contexts are defined to extend to the right2, so that the head of a context is its rightmost declaration, and its tail the context preceding that declaration.



unfortunately, we cannot simply recover a representation by defining an inductive translation of the semantic object. semantic objects arise by erasing the dependency of type phrases on module terms. thus, given a arbitrary semantic object, it is typically impossible to infer the module terms that produced it.



the first operation we will need is a notion of substituting a module expression for a module identifier, and a definable type for a type identifier. this is the technical motivation for introducing de bruijn indexed identifiers. the indices allow us to define substitution in a way that avoids the capture of free identifiers.



that remains unchanged. note that if p is a declaration binding an identifier, then we need to prevent the capture of any free occurrences of that identifier in m. this is achieved by lifting references to that identifier in m, before substituting in the scope of the declaration. if the bound identifier happens to be x itself, then we also need to ensure that the substitution does not affect occurrences bound by this declaration of x. this is achieved by incrementing the index n of the substitution before descending into the scope of the declaration.



however, from a theoretical perspective, the problems posed by eclipsed identifiers and anonymous abstract types mean that some module expressions may fail to possess syntactic representations of their types. we sketched a simplified(higher-order) modules language that, by introducing indexed identifiers and removing the abstraction phrase, always admits syntactic representations. this property is captured by theorem 6.13.



we should point out that the proof of theorem 6.13 is constructive. in principle, the proof can be implemented in a compiler to report module types to the user as syntactic signatures, instead of semantic objects. such an implementation can relieve the programmer of the burden of understanding semantic objects.



in chapter 5 we promoted the status of functors by making them first-class citizens of the modules language. although much more expressive than firstorder modules, higher-order modules still maintains a rigid stratification between modules and the core. the notion of computation at the level of modules is very weak, consisting solely of functor application, to model the linking of modules, and projection, to provide access to the components of structures. this weakness reflects the historical intention that modules should merely be used to express the architecture of core programs: actual algorithms and data structures are expressed by core values and types.



7.2 we present an example illustrating the elegance of first-class modules. in section 7.3 we give another example illustrating the additional expressive power of first-class modules. in section 7.4 we propose an alternative, intuitively more natural elimination phrase for first-class modules but show that it is unsound. this violation of soundness highlights an important distinc-



tion between type abstraction at the level of modules and its counterpart in the extended core. in section 7.4.1 we sketch a dynamic semantics for firstclass modules and give a sketched proof that our static semantics is sound for this dynamic semantics. section 7.5 closes with a brief assessment.



we adopt a different approach. we maintain the distinction between core and modules, but relax the stratification by enriching the core language with a family of core types, called package types, corresponding to first-class modules. a package type is introduced by encapsulating, or packing, a module as a core expression. a package type is eliminated by breaking an encapsulation, unpacking an expression as a module in the scope of another expression. because package types are ordinary core types, packages are first-class citizens of the core. the introduction and elimination phrases allow computation to alternate between computation at the level of modules and computation at the level of the core, without having to identify the notions of computation.



the advantage of preserving the distinction between modules and the core language is that we do not have to make an a priori commitment to a particular core language in order to give the definition of modules; the approach of amalgamating the core and modules into a single language, on the other hand, forces such a commitment from the outset. the advantage of distinguishing between modules computation and core computation is that each form of computation can be designed to satisfy different invariants. for instance, the invariant needed to support applicative functors, namely that the abstract types returned by a functor depend only on its type arguments and not the value of its term argument, is violated if we extend modules computation directly with computational mechanisms such as conditional computation and recursion. on the other hand, these are precisely the sort of mechanisms that core computation should provide. applicative functors provide good support for programming with higher-order modules; recursion and conditional computation are necessary in order to support realistic core programs. by keeping modules computation and core computation separate, we can accommodate both. by contrast, although the amalgamated languages proposed by harper and lillibridge[hl94], lillibridge[lil97], and harper and stone[sh96, hs97] support higher-order functors, because there is only a single notion of computation, there is a trade-off between supporting either applicative functors or recursion and conditional computation. since ruling out the latter is too severe a restriction, functors are not applicative.



from now on we will refer to a first-class module as a package, and its type as a package type. for concreteness, we will describe our extension of the core with package types as an extension of a particular core language, core-ml. the technique should apply to other core languages as well.



in chapter 3, we were able give a presentation of core-ml parameterised by an arbitrary modules language, exploiting the fact that the points of contact between core and modules are few. this level of abstraction enabled us to generalise first-order modules to higher-order modules while keeping the definition of core ml essentially fixed. since the aim of this chapter is to extend core-ml with first-class modules, we will need to make a stronger commitment to a particular modules language. for maximum generality, we will fix the modules language to be higher-order modules.



age type<s>. for instance, in a call-by-value dynamic semantics, the phrase is evaluated by evaluating the module expression m and encapsulating the resulting module value as a core-ml value. the classification rule will ensure that the module expression matches the signature s, via some realisation. the signature determines the package type of the expression, and is also used to make the actual realisation of types in m abstract.



with package types, it is perfectly possible to make the actual realisation of an abstract type depend on the result of some core-ml computation. in this way, package types strictly extend the class of types that can be defined in core-ml with higher-order modules alone. since this feature is not illustrated by our implementation of the sieve of eratosthenes, we will give a different example exploiting it here.



entry. the function init x returns the initial value x of its entry, viewed as an array. since(i mod 20)= 0, for any i, the function sub a i merely returns the value of the entire array a, viewed as an entry. similarly, the function sub a i x updates the array a by simply returning the updated entry x, viewed as an array.



the functor arraysucc represents an array of size 2n+1 as a pair of arrays of size 2n. entries with even indices are stored in the first component of the pair. entries with odd indices are stored in the second component of the pair. the function init a returns a pair of arrays of size 2n, initialised using the function a.init on arrays of size 2n. the function sub a i uses the parity of i to determine which component of the array to inspect, returning its(div i 2)-th entry using the function a.sub on arrays of size 2n. the function update a i x uses the parity of i to determine which component of the array to update, returning the pair of the unaltered component and the result of updating the other component using the function a.update on arrays of size 2n. it is easy to see that sub and update are of time-complexity o(n+ 1), provided a.sub and a.update are of complexity o(n).



a realisation of p that can depend both on the static interpretation of the type variables in c and on the dynamic interpretation of the module and value identifiers in c. it is sound to treat a purely static realisation as if it had a vacuous dynamic dependency. this forward direction justifies the soundness of the package introduction rule. it is not sound to treat a possibly dynamic realisation as if it were purely static. this explains why the stronger elimination rule for unpack e as s is unsound.



unfortunately, the addition of the phrase means that we can no longer prove the stronger property on module expressions used in the proof of property 7.3. in particular, we can no longer establish the induction hypothesis needed to argue that the classification rule for applicative functors(rule(h-18)) is sound.



the aim of this chapter is to design an algorithm that integrates coreml type inference with modules type checking. the primary motivation for this work is that it paves the way for the correct integration of our proposals with existing implementations of standard ml. although we take care to present our algorithms with their intended correctness properties, the verification of these properties is left to future work.



the soundness and completeness of this algorithm is predicated on the existence of analogous type checkers for type and term phrases of the core. as an hypothesis concerning an arbitrary core language, this simplifying assumption is reasonable: a wide variety of strongly typed core languages admit type checking algorithms of this kind. unfortunately, as we shall see, core-ml does not.



i.e. that we can get away with the simple-minded approach of interleaving core type inference and modules type checking, alternating between(a variant) of algorithm w and the type checker for higher-order modules presented in chapter 5. such an algorithm would invoke type inference when entering a core expression, reverting back to type checking when entering a modules subphrase, and so on. the following examples demonstrate that this approach cannot be made to work.



here, the type of i is determined by a core-ml phrase occurring within the type phrase(struct type u= int val x=+ i 1 end).u. clearly, the algorithm used to determine the denotation of a type phrase must also be modified to support type inference.



in the original algorithm, the first argument p merely records the set of type variables, other than r, allowed to occur free in the objects o and o'. in the generalised algorithm, q subsumes the role of p. like p, q records the set of type variables, other than r, allowed to occur free in both objects o and o', by declaring these variables as parameters. however, it also sets the scene for any unification problems encountered during matching by declaring the role and scope of any simple type variable occurring free in o and o'.



the rules for inferring the denotations of core-ml type phrases are straightforward adaptations of their counterparts in the static semantic of core-ml(cf. section 3.2.3). each rule has been altered to propagate the additional substitution from the output of one recursive call to the inputs of the next, and to return the appropriate denotation and composite substitution, taking care to apply any intervening substitution to a previously computed component of the denotation.



remark 8.4.1. clearly, adding this rule means that the proofs of termination, soundness and completeness of our unification and matching algorithms must now be carried out simultaneously. even though it is easy to motivate and describe the extension, it is by no means obvious that the properties of the original, stratified, algorithms are preserved: we have tied the knot and made them mutually recursive.



for instance, one of the challenges is to devise a decreasing measure on unification problems that establishes termination. the measure traditionally used for first-order unification relies on the fact that the unification algorithm cannot introduce new meta-variables during its execution. this property no longer holds in the extended algorithm since it may indirectly invoke rule(v-1) and, in example 8.3.4, we demonstrated how this rule can increase the set of new meta-variables.



however, empirical evidence does support our claim of correctness. the algorithms, including the extension to first-class modules, are implemented in the prototype interpreter accompanying this thesis[rus98a]. the algorithms behave correctly on a small but representative range of tests that includes all of the examples in this thesis.



the standard ml modules language has been both the subject and the source of much of the recent research into the type-theoretic foundations of module languages. despite these efforts, a proper type-theoretic understanding of the static semantics of modules has not emerged. such an understanding offers two potential benefits: type theory provides us with a rational basis for analysing existing features of the language, and for synthesising new features by generalisation.



thus we undertook the work in this thesis with two main objectives. the first was to provide a better, more type-theoretic formulation of the existing static semantics of modules. the second was to use this formulation as the rational basis for designing proper extensions of modules.



in chapter 5, we extended the modules language of chapter 3 to higherorder, using the revised semantics in chapter 4 as our starting point. functors were given the status previously enjoyed only by structures: they could now be defined as components of structures, specified as functor arguments and returned as functor results. we were able to present the semantics of higher-order modules as a natural generalisation of the definitions underlying the first-order language of the preceding chapters. the crucial ideas of introducing higher-order realisations and of generalising the enrichment relation to functors, by combining polymorphic subsumption with contravariant enrichment, were adapted and reworked from the original proposals of biswas[bis95]. the applicative semantics for functor generativity was inspired by our own results in chapter 4. we also addressed the practical concern of type-checking modules by providing, and proving correct, a sound and complete algorithm for signature matching. the algorithm is similar to, but simpler than, the one proposed by biswas.



in chapter 7 we turned our attention to a particular core language, core-ml, and relaxed the stratification between core and modules. we obtained a language with first-class modules and gave examples of programs exploiting them. our approach is novel in maintaining the distinction between core and modules. instead of amalgamating the features of both in a single language, we provide constructs for packing module values as core values and opening core values as module values, allowing programs to alternate between modules and core level computation. our ability to define a simple notion of first-class module directly contradicts the claims made by harper and mitchell[hm93]: their analysis implies that standard ml is incompatible with first-class modules.



in our semantics, the value component is still available. of course, in this example, the value component is useless because there are no operations that can manipulate this value. however, it is easy to construct larger examples where the result is a full-blown abstract data type together with useful operations.



in chapter 5 we gave a semantics for applicative functors. the terminology is borrowed from leroy, who has proposed an applicative version of his own module calculus[ler95]. although similar in spirit, the two notions of applicative functor are subtly different.



notation of a signature that is qualified by structure sharing constraints. however, this problem is very similar to the one encountered in determining the principal denotation of a signature that is qualified by type sharing constraints. in the latter case, it is known that the problem of ensuring principality becomes trivial if type sharing constraints are abandoned in favour of definitional type specifications in signatures. it is very plausible that structure sharing constraints may also be replaced by a simpler form of specification that eliminates the principality problem.



the presentation in earlier chapters to avoid introducing new keywords and grammatical ambiguities. more significantly, it has forced me to generalise the semantics of higher-order functors described in chapter 5 to accommodate standard generative functors as well as non-standard applicative ones. i have also taken the opportunity to revise the semantics of first-class modules slightly, making their use more convenient for programmers, especially those working in interactive sessions[rus00a, rus00b]. recursive modules are an entirely new extension, built on the foundations of this thesis, but introduced only briefly here; see[rus01] for a complete formalisation in the spirit of mini-sml.



in moscow ml, the types of functors are specified using functor signatures. similar to the distinction between generative and applicative functor expressions, functor signatures may be opaque or transparent. whether a functor signature is opaque or transparent affects the interpretation of any datatype or opaque type specifications in its range signature.



all that remains is to extend the definition of the enrichment relation for higher-order mini-sml to cater for our generalised notion of semantic functor with existentially quantified range. this, too, is straightforward. informally, we modify the relevant clauses of specification 5.12 as follows:



the local binding has no run-time effect and is only elaborated at compiletime for its type information. this new phrase is nothing more than a syntactic variant of the generalised dot notation for types, m.t, introduced in section 5.2.3, but fits better with the concrete grammar of sml. moscow ml does not add any new syntax for generalised projections of value or module components because these can already be expressed using existing constructs.



note that the restriction only applies to functor bodies, not top-level structures or their substructures, which may still contain package-eliminating module bindings. recalling the discussion in sections 7.4 and 7.4.1, intuitively, the reason it is sound to admit top-level package-elimination is that any existential types introduced at top-level will never be skolemised on the type parameters of an outer functor argument(since there can be no such outer functor). for this reason, it is perfectly legal to allow the realisations of these variables to depend on the dynamic as well as the static interpretation of the context.



here, x is a forward declaration of a structure implementing the body of the signature that allows the specification of even.t to refer to the type x.odd.t before it has been fully specified. in this new form of signature expression, rec(x: s)s', x is a forward declared structure, s is its signature, and s' is the body of the signature, which may refer to its own components via x. in a recursive signature, the body of the signature must match the forward declaration and specify an implementation for any opaque types or datatypes declared within the forward specification.



in sml, functors and signatures may only be declared at top-level, and structures may only be declared at top-level and within structures. none of these may be declared within core let-expressions. moscow ml removes these restrictions so that functors, signatures and structures may be declared anywhere, which is particularly useful when programming with first-class modules.



in sml, every parameterised type definition, and every type scheme occurring within a signature, must be closed: it must not mention any(simple) type variables that are not explicitly listed as type parameters. moscow ml does not impose this restriction, and allows free simple type variables, provided they are(explicitly or implicitly) bound in an enclosing scope. again, this is useful when programming with first-class modules.



