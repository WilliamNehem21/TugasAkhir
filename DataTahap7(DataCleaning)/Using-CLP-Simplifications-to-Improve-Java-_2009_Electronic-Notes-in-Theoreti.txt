in an earlier work, a termination analyzer for java bytecode was developed that translates a java bytecode program into a constraint logic program and then proves the termination of the latter. an efficiency bottleneck of the termination analyzer is the construction of a proof of termination for the generated constraint logic program, which is often very large in size. in this paper, a set of program simplifications are presented that reduce the size of the constraint logic program without changing its termination behavior. these simplifications remove program clauses and/or predicate arguments that do not affect the termination behavior of the constraint logic program. their effect is to reduce significantly the time needed to build the termination proof for the constraint logic program, as our experiments show.



termination analyses have been developed for logic[8,10,7], functional programs and term rewrite systems, whose semantics is relatively simple and well understood. more recently, termination analysis has been applied to imperative programs, dealing with primitive values only[9,15], lists[13,6,5,4] or any dynamic data-structure. in all cases, termination is typically proved by showing that some well-founded measure decreases along loops and recursion, so that divergence cannot occur. this measure can be the value of a variable of primitive type, the length of a list, the maximal path of pointers reachable from a given variable or a mix of such values. when generic data structures are considered, the shape of the computer memory must be somehow approximated, since destructive updates mute dynamic data through shared pointers. possibly cyclical data structures must be detected, since iterations over them might diverge.



other frameworks, such as the termination analysis of logic programs, since one needs the removed clauses there, in order to take care of instantiation patterns due to the presence of logical variables(which do not exist in our setting). also the simplifications based on removing variables which are irrelevant for termination are new(subsection 4.4). moreover, we present all such simplifications together and prove them correct in a uniform setting, which was not the case before. furthermore, we experiment with their effects on the termination analysis of real, large software, which was never the case before; in particular, those simplifications have never been applied to the termination analysis of java bytecode.



osn for their path-length at the end of the block; iln and oln are the same for the nth local variable. this constraint is then used to build clp clauses. in principle, there is a clp clause for each arrow in the graph of basic blocks. let blocki be a predicate expressing the path-length of the variables in scope at the beginning of block i. its arity depends on which local variables and stack elements are in scope at the beginning of block i. we build clauses since two arrows connect block 6391 with blocks 6392 and 6560. two local variables l0 and l1 are in scope there(l0 implements this and l1 implements other). at the beginning of block 6391 there is only one stack element s0, while there are 5 at its end. those clauses form a clp program whose termination entails that of the original java bytecode program. the clauses of that program have exactly one predicate on their right.



definition 3.3 formalizes a loop-local termination. this means that an entry terminates if it terminates by using the predicates of the loop where it occurs. this is importamnt to report a feedback to the user about which loop of which method might introduce the non-termination, without considering entries that diverge just because the computation, after executing the loop where the entry occurs, continues



the following result formalizes of well-known technique used in many termination analyzers. it allows us to prove termination for the loops of the program. a clause p(n):c, q(m) occurs in a loop if p and q are inside the same stronglyconnected component of predicates.



proof. programs p and ps have the same set of entries. let p be an entry of ps. if p diverges in ps then there is an infinite derivation d for p in ps. some steps of this derivation might use clauses derived from unfolding r(m):c1, q(n) with q(v):c2, s(w). we can replace those steps in d with two steps using those two clauses instead. the result is an infinite derivation for p that uses clauses of



if we apply this simplification to the clp program obtained at the end of subsection 4.1, the number of clauses decreases from 12 to 8 and the time needed to prove all the entries terminating goes down from 2.72 to 1.48 seconds(including the time for unfolding).



proof. any divergent resolution in ps is also a divergent resolution in p since ps has less clauses than p. any divergent resolution in p is also a divergent resolution in ps since a divergent resolution in p cannot use any unsupported clause, or otherwise it would be finite.



if we apply these simplifications to the clp program obtained at the end of subsection 4.2, the number of clauses decreases from 8 to 7 and the time needed to prove all the entries terminating goes down from 1.48 to 1.25 seconds(including the time to apply all the simplifications discussed up to now).



by removing an argument from the clauses of a clp program, the time needed to build a termination proof of the program decreases, since less arguments means less variables in the data structure implementing the linear constraints and hence better efficiency. moreover, by removing variables there are chances that distinct clauses get merged because one subsumes another(subsection 4.3).



if we apply this simplification to the clp program obtained at the end of subsection 4.3, the number of clauses goes down from 7 to 6(because of entailment checks) and there are less arguments in predicates. the time needed to prove all the entries terminating goes down from 1.25 to 1.02 seconds(including the time to apply all the simplifications discussed up to now).



if we apply this simplification to the clp program obtained at the end of subsection 4.5, the number of clauses remains 6 but there are less arguments in predicates. the time needed to prove all the entries terminating goes down from 1.02 to 0.67 seconds(including the time to apply all the simplifications).



processor 280 running at 2.4ghz, with 2 gigabytes of ram and 1 megabyte of cache, by using sun java development kit version 1.5 and sicstus prolog version 3.12.8. for each program, we report the number of methods(without the java libraries) and the time for building a proof of termination with the original, unlocalized technique of and with the successive application of more and more simplifications, described in this paper(the time for the simplifications is included). the header of each column reports the subsection where the simplification is described. the original technique failed to conclude the analysis after 15 minutes for nqueens, jlex and kitten. in general, more simplifications means better efficiency. this relation is not always true. for instance, building a proof of termination for jlex takes



