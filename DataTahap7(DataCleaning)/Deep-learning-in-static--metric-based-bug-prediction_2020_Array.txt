our best deep learning model achieved an f-measure of 53.59% using a dynamically updated learning rate on the quite imbalanced bug dataset, which contains 8780 (18%) bugged and 38,838 (82%) not bugged java classes. the only single approach capable of outperforming it was a random forest classifier with an improvement of 0.12%, while an ensemble model combining these two reached an f-measure of 55.27%. additionally, a separate experiment suggests that these deep learning results could increase even further with more data points, as data quantity seems to be more beneficial for neural networks than it is for other algorithms.



paper organization. the rest of the paper is structured as follows: section 2 overviews the related literature, while section 3 contains a detailed account of our methodology. then, we describe our process and our corresponding findings in section 4, with the possible threats to the validity of our results being listed in section 5. finally, we summarize our conclusions and outline potential future work in section 6.



deep learning and bug prediction. with the advent of more computing performance, deep learning [20] became practically appli- cable to a wide spectrum of problems. we have seen it succeed in image classification [21,22], speech recognition [23,24], natural language processing [25,26], etc. it is reasonable, then, to try and apply it to the problem of bug prediction as well.



we ended up choosing the 50% upsampling because it was the best performing option for our sdnnc strategy and produced comparably good results for the other algorithms as well. similarly to above, it is also considered a fixed dimension from here on out so we can concentrate on the actual algorithm-specific hyperparameters. we do note, however, that while it was out of scope for this particular study, replicating the experiments with different resampling amounts definitely merits further research.



