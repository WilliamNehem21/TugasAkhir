vector machines(chapelle et al., 2010; zhu and goldberg, 2009). in particular, self-training combines information from unlabeled data with labeled data to iteratively identify labels for unlabeled data within the dataset. thus, the labeled training dataset is enlarged at each iteration until the entire dataset is labeled. the self-training algorithm can be applied as a wrapper to any given supervised base learner. the cotraining method basically trains two models, while self-training trains one. thus, self-training can be seen as a particular case of co-training where just one model is trained.



manifold should have the similar label). these assumptions are the foundation of most, if not all, semi-supervised learning algorithms, which generally depend on one or more of them being satisfied, either explicitly or implicitly. thus, if sufficient unlabeled data is available and under certain assumptions about the distribution of the data, the unlabeled data can help in the construction of a better predictive model. if, on the other hand, this condition is not met, it is inherently impossible to improve the accuracy of predictions based on the additional unlabeled data(zhu and goldberg, 2009). it is worth pointing out that blindly selecting a semi-supervised learning method for a specific task will not necessarily improve performance over supervised learning. in fact, unlabeled data can lead to worse performance with the wrong link assumptions. for a detailed review on semi-supervised learning, refer to van engelen and hoos(2020), pise and kulkarni(2008).



the rest of the paper is organized as follows. section 2 describes the different ingredients and steps of the proposed geostatistical semisupervised learning approach. section 3 illustrates the proposed semisupervised machine learning method on synthetic spatial data. an application example with real-world spatial data is given in section 4. sections 3 and 4 also include a comparison with classical supervised and semi-supervised learning methods. concluding remarks are summarized in section 5.



supervised machine learning model can be improved by exposing it to as much data as possible. the pseudo labeled spatial data provide context that will aid the training of a supervised machine learning model. it is important to note that each pseudo training dataset contains the original training dataset. geostatistical conditional simulation is used here as a label generator and data augmentation approach. in



the r package rgeostats(renard et al., 2021). this simulated example depicts a situation where there is a non-linear relationship between the target variable and auxiliary variables with some interactions between auxiliary variables. also, the target variable shows some spatial autocorrelation, and its distribution is non-gaussian.



the geostatistical semi-supervised learning based on random forest is first applied to synthetic spatial data for which the ground truth is exhaustively available over the study region. the comparison is carried out with classical random forest, random forest with unlabeled spatial data treated as additional covariates, and self-training random forest. the first two machine learning methods are supervised, while the last one is semi-supervised. all competing machine learning methods are based on random forest. the number of decision trees(1,000) is the same for all the competing machine learning methods. the other hyper-parameters are optimized through cross-validation. the classical random forest is implemented in the r packages ranger(wright and ziegler, 2017) and tuneranger(probst et al., 2018). the self-training random forest is implemented in the r package ssr(garcia-ceja, 2019).



uncertainty; only aggregated predictions for all trees are provided. the prediction uncertainty map resulting from the proposed method differs notably from the others. in particular, under the proposed method, the prediction uncertainty tends to be lower at the original training data locations than at other locations, which is not the case for the other methods.



