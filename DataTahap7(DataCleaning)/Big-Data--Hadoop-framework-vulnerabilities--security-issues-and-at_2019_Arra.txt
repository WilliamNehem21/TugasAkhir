big data is a collection of different hardware and software technologies, which have heterogeneous infrastructure. hadoop framework plays a leading role in storing and processing big data. it offers fast and cost-effective solution for big data and is used in different sectors like healthcare, insurance and social media. hadoop is an open source framework based on a distributed computing model and is applied for processing and storing data on a cluster of commodity computers. due to the flexibility of framework, some vulnerabilities arise. these vulnerabilities are threats to the data and lead to attacks. in this paper, different types of vulnerabilities are discussed and possible solutions are provided to reduce or eliminate these vulnerabilities. the experiment setup used to perform common attacks to understand the concept and implementation of a solution to avoid those attacks is presented. the results show the effect of attacks on the performance. according to results, there is need to protect data using defense-in- depth to security.



big data is a collection of huge amount of historical and important data, which is the most valuable asset of every organization and being utilized intelligently for business can support decisions based on real facts rather than perceptions. the term big data was coined by charles tilly in oxford dictionary in 1980 [1]. at present, considerable data is generated from multiple sources including social media sites, different remote sensors, cell-phone gps signals, transaction records, and log files. owing to the socialization of the internet, terabytes of structured, un- structured, and semi-structured data are produced online every day and much of this information has inherent business values. thus, if it's not captured and analyzed properly, then considerable vital data will be getting lost. big data is to archive a collection of historical data, using hadoop framework with the help of different tools for analytics, which are faster than previous traditional analytic tools.



this new technology has been developed to ensure data privacy and security in contrast of traditional technologies. however, this technology can also be used for negative purposes. as a growing number of busi- nesses and individuals store and process their private data using this technology, it has become a significant target of data attacks.



hadoop distributed file system is a fault tolerant storage system. it distributes large files across the cluster consisting of many data nodes with local storage. during this process name node partitions, the original file into a block, size of 64 mb by default and replicates it to the different data nodes based on pre-defined rules. name node also maintains the metadata for this replication and allocation. each data block is replicated three times for high availability, two on same rack data nodes and one from different rack data node. data node of the cluster stores a small fragment of the entire file. name node is always aware of which data block belongs to which file,where the data blocks are placed, and where storage capacities are occupied. using periodically transmitted signals,



mapreduce is a parallel processing framework work based on the master-slave principle, similar to hdfs. it is a combination of one master (jobtracker) daemon and 3 slaves (tasktracker), daemons per slave in a cluster. map readuce processing data parallel based on different algo- rithms for a map and reduce. this works in two steps, map task and reduce task. this jobtracker divides the dataset into multiple chunks called tasks (map tasks) and distribute them to three data nodes (task- tracker) by default across the distributed connected commodity com- putrers on a network for parallel processing.



typically, the map tasks run on the same cluster data nodes where data resides (data locality). if a node is already heavily loaded, another node that is close to the data, i.e., preferably a node in the same rack, is selected. intermediate results are unavailable to the user and are exchanged among the nodes(shuffling), and thereafter, merged by the reduced tasks to obtain the results.



intermediate results of map phases can be aggregated by keeping the data volume as low as possible during trasnfer from map tasks to reduce tasks. the intermediate results are deposited in the local file system of the data nodes. the jobtracker is responsible for monitoring to reschedule any task in case of disruption.if a task does not notify any progress in a pre-determined time, or if a data node fails completely, all tasks are restarted on another server including tasks but, they are not finished. if a task runs extremely slowly, the jobtracker also restarts the task on another server in order to execute the overall job in appropriate time (speculative execution).



vulnerabilities are divided into three major categories: infrastructure security, data privacy, and data management [16]. these are further categoriesd in three dimensions: architecture dimension, life cycle of data dimension, and data value chain dimension. according to the author, infrastructure involves the hardware and software vulnerabilities which are in architecture dimension. data privacy involves the data at rest and data in transit which involve the life cycle of the data.



as per another work [17], research on security and privacy issues of big data is divided into five heads: hadoop security, monitoring and auditing, key management and anonymization. this paper specifically discusses security and privacy issues of data in cloud environment of hadoop. author also discusses various encryption mechanisms to impalement security in hadoop hdfs (hadoop distributed file system) to achieve authentication in hadoop, kerberos network authentication protocol used. other methods and algorithms discussed for monitoring and security of sensitive information are bull eye algorithm and the nnse (name node security enhance) method, used between the master node and data nodes in hadoop.



verizon released a white paper [18] on cloud security. in that paper, the cloud infrastructure layered security model is proposed. that model is divided into four parts: basic security, logical security, value-added security, and governance. in that paper verizon also explained the sticky policy framework architecture. this is proposed for securing the big data applications on cloud infrastructure.



hardware and software. national vulnerability database (nvd) is a vulnerability management repository containing security-related soft- ware flaws and impact metrics. the computer emergency readiness team (cert) is another vulnerability database providing information about software vulnerabilties. microsoft security bulletins is also related to security issues discovered in microsoft software, published by the microsoft security response center (msrc). common vulnerabilities and exposures (cve) is another database list of vulnerabilities with an identification number. open source vulnerability database (osvdb) provides accurate and unbiased information about security vulnerabil- ities. cve contains numerous vulnerabilities of cyber security products and services from around the world.



vulnerability of patch management is the process of rooting out and eliminating these weaknesses before these are abused. efficiency of patch management system depends on how fast vulnerability is detected, cor- rected, and patched through periodic penetration (pen testing) and code review using vulnerability scanning. as per the 2017 report, usage of vulnerability scanning has increased by 41.7% globally [22]. in patch management, newly discovered vulnerabilities are patched using vendor-provided patches. input validation/sanitization should be con- ducted by the filtering and verification of incoming traffic by using a web application firewall, which blocks attacks before they can exploit vul- nerabilities and is a substitute for fully sanitizing an application code. vulnerability scanners automate security auditing by scanning your network and websites for different security risks and also possible for some to even automate the patching process. the most popular tools are nexpose is an open source developed by rapid7 carrying out a wide



how to investigate data leakage attacks in hadoop is important but a long-neglected issue as per mcafee's report, which states that different threats to data have increased recently [23]. the following section ex- plains security threats that can hamper the operation of mapreduce and all framework components in the absence of a protected mapreduce environment. a dispersed and replicated data processing of mapreduce can unlock an opportunity for a large range of attacks.



hadoop, as we recognize, is an open-source venture that includes various modules, which are separately developed over time to add different types of functionalities to its core capabilities. security was a late addition, and thus, hadoop lacks a consistent security model. by default, hadoop assumes a trusted environment. hadoop has focused on improving its efficiency. researchers are gradually paying attention to hadoop security concerns and building security modules for it. however, currently, there is no existing evaluation for these hadoop security modules.



a threat is a potential danger to an information system. a threat is that someone, or something, will identify a specific vulnerability and use it against a company or individual to attack the system [15]. the basic principle of security is cia, which refers to confidentiality, integrity, and availability. confidentiality is only possible if only an authenticated user can assess the system; it is also important to determine who should have access to the system and what resources a user can access. hadoop se- curity is divided into two levels. with respect to the time at level-1, the



impersonation attacks occur when an attacker tries to access re- sources by impersonating as an authorized identity. the attacker steals the credentials of an authorized user by using different methods and attacks hadoop cluster resources, services, and jobs. impersonation can be performed by replaying the tickets issued by the authentication server (kerberos) for different purposes. once the attacker gets access to the cluster, they can perform any type of data leak and slow down the pro- cessing of mapreduce.



a dos attack [24] occurs when the resources are unavailable to authorized users. as per the verizon 2017 data breaches report, 11246 attack incidents have been reported and out of those 5 breaches have been successful [25]. dos attacks are accomplished by flooding the target with traffic or causes a crash of data. in each instances, the dos attack deprives legitimate users of the service or resource they expect. there are two general ways of dos attacks: flooding services or crashing services. flood attacks occur once the system receives an excessive amount of traffic for the server to buffer, inflicting it to abate and eventually stop.



well-known flood attacks embody distributed denial of service (ddos), buffer overflow, syn flood, ping to death, and http flood. in hadoop, the name node and authentication server are vulnerable to dos attacks. the name node has a master daemon that is responsible for scheduling and coordinating the execution of mapreduce applications on the data nodes. a dos attack on the name node can halt all computations of mapreduce and read-write operations of hdfs.



xss [26] is a common attack which injects a malicious code into a vulnerable web application. it differs from sql injection because in that it does not directly target the application itself. but instead, the web application users are the ones at risk. xss attacks can be broken down into two types: stored and reflected. stored xss, also known as persistent xss, is more damaging than reflected xss and occurs when a malicious script is injected directly into a vulnerable web application. reflected xss involves reflecting a malicious script of a web application onto a user's browser. the script is embedded into a link and is only activated once that link is clicked on. hadoop web ui's are vulnerable to attacks, and recently, different database installations have been attacked.



hadoop is reportedly easily identifiable to hackers worldwide by simply sniffing to open instances, and around 5307 hadoop clusters are exposed to the web with their security settings off, exploit them with receptive attack by hackers [27]. the online search engine shodan2 shows details about all servers and devices connected to the internet. the merit of this is security recommendation, while the demerit is that it is used by hackers to see all details of any network, server location, and other settings. how to protect data attacks in hadoop is an important issue. to counter these attacks and stop data theft, it is first important to thoroughly understand the vulnerabilities as well as threats, and then, it can be worked toward devising a strategy using defense-in-depth to secure data.



to block the communication between a name node and data nodes, the strategy made was little bit different because hadoop system is resilient in the case of hardware failure, so there is no way to stop any hardware of data node or name node. for implementing block communication, the different types of available ports and services were observed between a name node and data node. hadoop system detects failure dynamically and allocates data to the other data node. the failure detects with the help of the heartbeat signal sent by data nodes to the name node, sowe targeted heartbeat signal meansif a name node re- ceives heartbeat signal regularly name node sending other data to the data node even data node is compromised.



traditional enterprise security products implement security controls, but are still insufficient for holistically addressing the security challenges introduced by big data. to address the problems posed by data aggre- gation, organizations must find new ways to safeguard their critical tools, techniques, and procedures used to acquire, maintain, and analyze data of particular concern to improve the robustness of its security infra- structure, as most commercially available security software have access to all real and derived data available on the network. the add-on security features provided by the third party are not useful in the big data envi- ronment. furthermore, analysts and other users of information technol- ogy need more effective security training that will help them understand the specific threats they face, that how their security choices will impact



bhathal gs, singh a. big data computing with distributed computing frameworks. in: saini h, singh r, kumar g, rather g, santhi k, editors. innovations in electronics and communication engineering. lecture notes in networks and systems, vol. 65. singapore: springer; 2019. 3765-9_49.



