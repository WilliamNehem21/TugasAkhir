prediction of usefulness of surgical procedures, clinical tests, medication procedures, and the discovery of associations among clinical and diagnosis data [37]. the applicability of data mining for healthcare applications is increasingly gaining importance. the availability of diverse-natured medical data for diagnosis and prognosis and of pervasive data mining tech- niques to process these data offers medical data mining a dis- tinctive place to truly assist and impact patient care.



previous models, and uses an independent measure to identify the best subsets for a given cardinality and applies a mining algorithm to select the best subset among all best subsets across different cardinalities. however, the ensemble of a filter based model with another filter based model, once for subset selection and again for ranking proves to be a promising approach, for medical data mining. the ensemble is brought about in a fashion so as to reduce the number of features and also to enhance the classification accuracy.



the objective of this research work is aimed at showing that the selection of more significant features from the available raw medical dataset helps the physician to arrive at an accurate diagnosis. the primary focus is on aggressive dimensionality reduction so as to end up with increase in the prediction accu- racy. the features are subjected to a double filtration process, at the end of which, only the features that increase the accu- racy, and form the subset with the lowest cardinality, with their corresponding rank, are obtained. the method employs an efficient strategy of ensemble feature correlation with rank- ing method. the empirical results show that the proposed multi filtration feature selection (mffs) embedded classifier model achieves remarkable dimensionality reduction in the 22 medical datasets obtained from the uci machine learning repository [10] and kentridge repository [13].



another category of feature selection methods used mutual information score. vinh et al. [32] proposed a novel feature selection method based on the normalization of this well- known mutual information measurement and utilized the information measurement to estimate the potential of the features. the method could not eclipse the strongly correlated features impact on the classification results. correlated features may be accounted for redundancy and hence a single representative feature from that subset may be selected for further processing.



a new approach called redundancy demoting (rd) has been proposed by osl et al. [22]. it takes an arbitrary feature ranking as input, and performs improvement in ranking by identifying redundant features and demoting them to positions in the ranking in which they are not redundant. hybrid schemes that combine wrapper-based and filter-based approaches are also in the literature [2,11,30] are such schemes where the features are ranked and then selected so as to offer superior classification accuracy. in the first stage, the filter model is used to rank the features by the relief algorithm and then the highest relevant features are chosen to the classes with the help of the threshold. in the second stage, they used shapely values to evaluate the contribution of features to the classification task in the ranked feature subset. tanwani et al. [31] gave a study on comprehensive evaluation of a set of diverse machine learning schemes on a number of biomed- ical datasets. sanchez-monedero et al. [27] studied and pro- posed the suitability of extreme learning machines (elm) for resolving bio-informatics and biomedical classification problems.



the basic theoretical idea behind pca is finding the princi- pal components of the dataset that correspond to the compo- nents along which the variation is the most. this is achieved by finding the covariance matrix, i.e., we find the principal com- ponents of the data, which correspond to the components along which there is the most variation. this can be done using the covariance matrix, aat for our input matrix a, as follows. let the eigen values be represented as ki for the covariance matrix. then, the corresponding diagonal matrix is given in



in spite of feature extraction and selection, a problem is persis- tent namely the classifier may be biased towards the attributes with more values. hence this biased nature has to be elimi- nated for which we employ symmetrical uncertainty (su). it overcomes the problem of bias towards attributes with more values, by dividing information gain by the sum of the entro- pies of feature subsets si and sj.



an mlp [15] can be viewed as a logistic regression, where the input is first transformed using a learnt non-linear transforma- tion. the purpose of this transformation is to project the input data into a linearly separable space. this intermediate layer is referred to as a hidden layer. we have employed a back- propagation network with 0.3 as learning rate and 0.02 as momentum. the attributes are normalized in the range of (0.1, 0.9). the training was carried out for 500 epochs.



in this paper, we have proposed an efficient multi filtration feature selection (mffs) method applicable to medical data mining. empirical study on 6 synthetic medical datasets sug- gests that mffs gives better over-all performance than the existing counterparts in terms of all three evaluation criteria, i.e., number of selected features, classification accuracy, and computational time. the comparison to other methods in the literature also suggests mffs has competitive performance. mffs is capable of eliminating irrelevant and redundant fea- tures based on both feature subset selection and ranking mod- els effectively, thus providing a small set of reliable features for the physicians to prescribe further medications.



