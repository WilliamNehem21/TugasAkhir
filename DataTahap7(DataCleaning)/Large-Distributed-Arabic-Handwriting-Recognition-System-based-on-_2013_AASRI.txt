this paper proposes a robust, efficient and scalable distributed arabic handwriting ocr system based on a parallel fastdtw algorithm via cloud computing technologies. the three techniques hadoop, mapreduce and cascading are used to implement the parallel fastdtw algorithm. the experiments were deployed on amazon ec2 elastic map reduce and amazon simple storage service(s3) using a large scaled dataset built from the ifn/enit database.



conducted experiments and evaluations on several arabic handwriting ocr systems show and confirm that: in the first hand, the euclidean distance technique is used for classification. however, this technique is less robustness and more fragile. in the second hand, the dynamic time warp(dtw) algorithm stands among the best techniques for such a mission.



the major problem of the dtw is the slowness of its response time because of the enormous amount of computation to achieve. distributed system, such as cloud computing technologies, provides viable framework to speed up the time of the ocr system based on dtw algorithm. cloud computing is primarily used to deliver many services such as infrastructure(i), platform(p) and software(s) as services. all these services are available to consumers as registration based services in a pay-as-you-consume model.



this paper is organized as follows: an overview on the dtw algorithm and especially the fastdtw and the use of them in arabic character recognition, is presented in section 2. hadoop, mapreduce and cascading models are presented in section 3.the proposed approach is explained in section 4. experimental and results are presented and discussed in section 5. conclusion and future work are presented in the last section.



dtw presents many disadvantages such as the time and space complexity which are exponential. this model is practical only for small and medium data sets(<3,000) and time series are often very long. the fastdtw algorithm can be a solution to solve this problem. fastdtw is based on the multi-resolution



mapreduce is a tools using to parallelize problems that process large datasets with different computers(nodes)(distributed architecture) like a cluster or a grid computing. amazon elastic mapreduce provides the option to analyze vast amounts of data. this advantage is offered by distributing the computational work across a cluster of virtual servers running in the amazon cloud. all clusters are managed using an open-source framework called hadoop.



to examine the proposed idea, a corpus with 16000 pages(370 characters/page) and a reference database formed of 345 shapes representing approximately the different arabic alphabet randomly chosen from the arabic handwritten word images dataset ifn/enit are used. for the preprocessing image, the ifn/enit dataset was already normalized. wavelet transform is used as a features extraction technique.



*2, 2 gb of ram executing a windows xp operating system, cygwin is the shell to run linux command. we used java as a programming language and jdk 1.6 was installed. eclipse 3.4 was used to program and built our application. 100 mg bits/s was the network capacity.



instance 7.5 gb of memory, 850 gb of instance storage, 64-bit platform and finally the extra large instance 15 gb of memory, 1690 gb of instance storage and 64-bit platform. s3 is used to manage the input and output data.



