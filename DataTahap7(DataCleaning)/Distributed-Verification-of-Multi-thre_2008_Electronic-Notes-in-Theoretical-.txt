the applicability of model checking algorithms is limited by the main memory resources due to the so-called state explosion problem. several memory-limited model checking algorithms have been developed, e.g.,[16,20,24], but still the memory remains the core problem in dealing with large programs. though with the advent of 64-bit machines, the theoretical limit of ram has increased to 16 exibytes, the limitations imposed by the hardware and the operating systems hardly allow to use more than 64 gigabytes of ram. the use of virtual memory as a remedy to the state space explosion in model checking can instead slow down the performance significantly.



the model checker offers different blind and directed search algorithms, including depth-first search, breadth-first search, best-first search, ida*, and a*. the types of safety errors addressed by the model checker include: deadlocks, segmentation faults, assertion violations, etc. the model checker branches the execution on threads(either derived from a base class or in form of posix pthreads) or on variable ranges. for search guidance, it offers a range of state-to-error estimates including the active process heuristics to accelerate the search for deadlocks.



a state in steam consists of registers, stack frames, global variables and memory pool. the state size grows with every memory allocation in the program and can easily reach several megabytes. hence, one of the most important challenges for program model checking on the object code level is the tremendous size of the state vector.



predecessor information to reconstruct the solution path, and the transition used to generate the mini-state. additional information include its depth and heuristic estimate to the target state that are used to order the states for expansion. all in all, a mini-state has a constant size in contrast to a state that can change its size due to dynamic memory allocation.



we implemented separate caches, one for the data section, one for the binary section, one for the stack contents, and one for the rest of the system state. all of the components can be individually flushed to and read from disk. we will refer this method as external collapse compression. for the data and binary section, we incrementally check at construction time, whether a change has occurred; for the stack, we check for redundancies at insertion time. in all three cases, the cache is realized by using an avl tree sorted by the individual hash values.



in order to avoid an infinite behavior while reading the mpi queue[line 8], we set a limit on the number of mini-states that are extracted in one scan. the limit is set in the max msg input parameter that is compared against the variable counter[line 20]. similarly, the flag idle reported is used to avoid repeatedly sending the idle messages to the root.



we tried a hash-based distribution as proposed by stern and dill with a linear hash function defined on the full state vector. such a distribution is effective if, with high probability, the successors of a state expanded at a particular computing node are also mapped to the same node. this results in low communication overhead. in our case, with states of huge sizes, such a partitioning can be costly as computing the hash function is expensive. as one solution to the problem, steam offers the option for incremental hashing that relies on the hash difference between the state and its successor only.



we implemented external exploration on top of our tool steam. the distributed exploration is realized through mpi. the experiments are performed on a clustervision cluster of workstations. the cluster consists of 224 computing nodes with a total of 464 processors running opensuse 10. we used the set of nodes consisting of two amd opteron dp 250(2.4 ghz) processors each, connected by infiniband. maximum number of parallel processes was 32.



for the 200 dining philosophers problem, each state was 32kb long. 4 nodes solved the problem with 90gb of external memory consumption in transfer files. a total of 2,256,037 states were generated till a deadlock was found. the states were almost uniformly distributed over the 4 nodes. that implies that for 0.51 million states per node, a total of 16gb is needed in the ram. but due to externalization and collapse compression each node consumed a maximum of 1.5gb of ram including the 500mb mpi overhead.



we have also solved the dinning philosophers instance for 600 philosophers with a state size of 97kb. for 6 computing nodes it took 74 minutes, while consuming 60 gb of hard disk space for transfer files and generating a total of 761k states. parallel depth-first search with a depth-slicing of 100 layers was used. the deadlock was found at layer 2193. for a smaller number of computing nodes, the time exceeded the bounds on the cluster queue.



recent extension contributes an external memory variant of the same algorithm. recently, with the advent of multi-core machines, the trend is directed towards verification on multi-core machines. multi-core machines offer the advantage of having negligible overhead for state transfers due to shared memory. holzmann and bosnacki presented a method for multi-core extension of spin where the safety analysis is applicable to n-core systems but the fair cycle detection to verify



with this work, we have contributed an integrated design for distributed and largescale verification of c++ programs. as the analysis is on the object code no abstraction takes place and the expressivity of concurrent c++ is preserved. the novelty and the algorithmic challenge lies in tackling the states of large sizes. we employed a dual-channel communication that combines mpi and nfs media. instead of the full state, only a signature is sent over mpi. the full state vector is flushed to disk in transfer files.



the experimental results are promising. we observe an almost linear speed up in all examples. future work includes the integration of dynamic load balancing and the evaluation of larger c++ models. even though we could report the full exploration of sample instances, for infinite state systems, the algorithm can run forever. in future, we also plan to accelerate the i/o operations by a more efficient block flushing of transfer states or using databases.



