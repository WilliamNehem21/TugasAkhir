the web has become an indispensable source of information and we use it more and more in our daily activities. the primary mode of interaction with the web is via graphical browsers, which are designed for visual interaction. however, most web pages contain banners, ads, navigation bars, and other data distracting us from the information. as we browse the web, we have to filter through a lot of irrelevant data. this is straightforward for sighted individuals, who can process visual data in no time at all and can quickly locate the information that is most relevant to them.



context-directed browsing. the identification of relevant information on any distinct web page is subjective. however, as soon as the user follows a link, it is often possible to use the context of the link to determine the relevant information on the next page and present it to the user first.



dynamo. to unify different types of content changes and make dynamic content accessible to blind web users, we developed dynamo, the system that treats web page updates uniformly. its methods encompass both web updates enabled through dynamic content and scripting, and updates resulting from static page refreshes, form submissions, and template-based web sites.



model-directed transactions. non-visual web transactions can be facilitated by giving the users a way to quickly access the most relevant concepts at any step of the transaction. we used a process model to direct web transactions. the model was represented by a finite state automaton where each state corresponded to a web page with concepts, and the edges were the actions leading to those states. when a transaction was in a particular state, the most relevant concepts would be identified, extracted, and presented to the user.



the rest of this paper is organized as follows: in section 2, we describe the architecture of the system. in section 3 we describe our approaches to bridging the web accessibility divide. related work appears in section 4, followed by concluding remarks in section 5. the details of experimental setup and user evaluations can be found in our papers[3,9,6].



the dialog generator module generates voicexml dialogs for page navigation. a separate functionality is provided through dialog interpreter to quickly switch between the segments with concept instances. the dialogs are then delivered to the user interface manager, and, once the user chooses an action, the process is repeated.



we use an observation that semantically related information exhibits spatial locality and often shares the same alignment on a web page. since a dom tree represents the layout of a web page, we infer that geometrical alignment of nodes may imply semantic relationship between their respective content. if all descendants of a node are consistently aligned either along x or y axes, we call such a node consistent.



we consider two multisets to be similar if their cosine similarity is above a threshold. we have statistically computed the threshold(see for details) that best determines whether a topic changes between the context and the stext multisets. if the cosine similarity between the multisets is above the threshold, i.e. topic boundary is not detected, the multi-sets are merged. otherwise, we stop expanding the context window in that direction. the details appear in.



the machine-learned svm model is now used to predict the relevance of a given block with respect to some context. given a set of blocks b1, b2,..., bm we compute the feature values for each block by matching the context against the text in the block. next, we use the learned svm model to label the blocks as either relevant or



regardless of whether web sites are implemented using dynamic or static approaches, users browsing for information usually care only about the information that changed as a result of their actions. for example, we could use web browser apis to detect that a new dynamic tab has become visible, or that an incorrectly filled form element was highlighted red. people will care about what information has changed. by the same token, we could use some diff algorithm to compare previous and current pages, and identify that static tabs are a part of the page design, or that the error message in the refreshed web form is the only difference. again, people care about the information that changed, so that they can get their information, complete their task, and move on.



hd is built on the existing hearsay non-visual web browser. hearsay uses a firefox-extension to communicate with a java backend. the hd backend processes all events sent to it by the firefox extension, pushes new web content through the pipeline of various analytic algorithms, generates voicexml dialogs, and manages the user interface through the vxmlsurfera custom voicexml interpreter.



our diff algorithm combines the maps and performs a bipartite matching to find out the matching nodes and makes a list of them. later we use the max flow routine to find the maximum number of matches. and the rest of them are considered as different nodes.



herein we describe the content analyzer module that extracts the relevant concepts. in a nutshell this is achieved by partitioning a web page into segments of semantically related items and classifying them against concepts in the ontology. below we provide an overview.



web page partitioning techniques have been used for content adaptation[23,24] and content caching. vips algorithm uses visual cues to partition a web page into geometric segments. this algorithm is used in, where the segments are described by a set of features(e.g. spatial features, number of images, sizes, links, etc.). the feature values are then fed into an svm, which labels the segments according to their importance.



w.r.t. the context of the link in the source page. in contrast to, where the svm model was learned using features only from the content of web page segment, our svm model uses a feature set, computed from both the context of the link and the content of the block.



the notion of context has been used in different areas of computer science research. for example, defines context of a web page as a collection of text, gathered around the links in other pages pointing to that web page. the context is then used to obtain a summary of the page. summarization using context is also explored by the incommonsense system, where search engine results are summarized to generate text snippets.



the use of contextual information for non-visual web access is not a well-studied problem. a technique resulting from early efforts at context analysis for non-visual web access is described in, where context of a link is used to get the preview of the next web page, so that visually disabled individuals could choose whether or not they should follow the link. this idea is used in access system, to get the preview of the entire page. however, presenting a preview does not guarantee the reduction of browsing time.



in this paper, we described three techniques for bridging the web accessibility divide between sighted and visually impaired users. first, we described the design of context-directed browsing approach, which uses web page partitioning and techniques from nlp and machine learning. using our system, visually impaired individuals can potentially imitate the browsing behavior of sighted users, saving their time on not listening to irrelevant information. contextual browsing also has implications for handheld devices with small screens. we implemented context-direct browsing algorithm to identify and display the most relevant sections of web pages on small-screen handhelds.



we also described the dynamo approach to making dynamic web content accessible and web page updates usable, a problem whose importance, spurred by the advent of web 2.0, has been continually growing. user studies with the hearsaydynamo system indicated that using the system improved disability to web page updates. this initial work has opened up several avenues for improving hearsay implementation and conducting additional research. first, the participants suggested several improvements to the user interface that can be explored to optimize the user experience. second, although hearsay can detect web page updates, many participants wanted to know the semantic roles of those updates. a collaborative approach could leverage a community of users to label content and share the labels among themselves. we believe these goals can build on the foundation outlined here and make dynamic content efficiently accessible to blind web users.



finally, we talked about our process modeling for directing web transactions. we showed that, by delivering relevant page fragments at each transactional step, a model-directed web transaction(i.e. process model) can improve web accessibility and substantially reduce the digital divide between sighted and blind users.





