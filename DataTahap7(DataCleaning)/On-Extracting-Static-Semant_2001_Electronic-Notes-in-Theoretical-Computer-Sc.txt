these two phases correspond to the latter two phases of a compiler. just as code generation uses information provided by the semantics analysis, the dynamic semantics can use information or properties provided by the static semantics. in a strongly typed language, this information consists of a welltyped result that allows the dynamic semantics to avoid any typechecking.



more generally, we can imagine describing program properties other than types. these might include properties of security, resource usage, or effect. our goal for describing these properties might be to provide compile-time analyses that ensure certain runtime behaviors of programs satisfying these properties. for example, we might describe a property about security involving levels of priority and access to secure information. a static analysis that succeeds should tell us that at run time, no security violations can occur. again, we must provide a consistency result that demonstrates the correctness of the static analysis: assertions made by the static analysis are verified/reflected in the dynamic semantics. as the properties become more complex and the analyses become more involved, proving this kind of consistency is increasingly difficult.



separating semantics and compilers into two phases provides two advantages: efficiency and predictability. by performing tests and operations once at compile time, we may avoid performing them at all during run time. static type checking of strongly typed languages is a good example of this. by establishing properties of a program at compile time, we have some prediction or guarantee of the behavior of the program at run time. again, static type checking may ensure that a program will not generate any run-time type errors. generally, these advantages outweigh the disadvantages of having to construct two distinct phases and then prove them correct.



however, as we define languages with more complex features and attempt to provide more information through static analyses, the difficulty of defining these analyses and proving them consistent with the dynamic semantics will grow. an alternative is to define a single semantics that contains both the static and dynamic checks. this is typically easier than defining two semantics, in part because checking of properties can be done when the properties are clearly evident. for example, instead of providing static typechecking, we use dynamic type checking. then we might only check that values have the appropriate type. of course, this might result in a different semantics.



the idea of constructing a type checker from a semantic specification is not new, though little work has been done in this area. in 1991, neil jones considered this problem. his focus is specifically on type systems, not the more general problem of static properties. but as types are the most important static property in use today, starting with them is sensible.



the problem that arises with this approach is that we fail to distinguish between static and dynamic errors. a further problem is that this approach deals with programs one at a time. we do not have any guarantee that every source program processed in this way will result in a residual program that contains no error checking. we have no way of determining if our language is strongly typed(meaning here that all static-error testing can be performed by partial evaluation).



a fundamental limitation of using partial evaluation to specify static semantics is that we do not explicitly construct a type system or some other specification of the static operations. instead, a general partial evaluator implicitly performs these operations. an alternative to partial evaluation is another staging transformation called pass separation. recall that staging transformations are, in general, an methods of separating stages of computations based on the availability of data, with the most common application being developing compilers from interpreters. partial evaluation is perhaps the most widely known and used staging transformation, but others exist, including traditional compiler optimizations(e.g., constant folding) and pass separation.



yield a new program state. rewriting in this system is particularly simple as we only allow rewriting at the top level, i.e., the entire term. this simple form a computation is expressive, but also extremely convenient to reason about because we only have a single form of computation(machine-state rewrite) to consider.



represent dynamic(runtime) components. for the cls machine we take the c component to be the static part and the pair(l, s) to be the dynamic part. we then decompose each rule into two parts: one operating only on the static part and one operating on the dynamic part. for example, a rule of the form



the construction of these new rules is based on the form of the original rules. because the rules have such a simple structure, we can identify a few distinct cases that give rise to meta-rulesrules on how to construct new rules. these meta-rules are based on the dependencies among s, d, s', and d'. in particular, we consider how the terms s' and d' are constructed from s and d.



where did the types in the previous section come from? we assumed that terms came with their types already attached and that the semantics used those types to perform checks. these assumptions stacked the deck in our favor, helping us to achieve our goal of constructing a type system. as a first step, we think this is fair, providing us with as much information as possible. one might argue that what our example really shows is part of the proof of type soundness, telling us that we can erase the types at runtime. but where did these types come from in the first place?



suppose our goal is to start with a language like scheme but then add static checking to ensure that certain kinds of runtime errors cannot occur. essentially, how can we get from a language like scheme(with dynamic typing of values) to one like ml(with static typing of expressions)? we argue that a technique similar to the one proposed in the previous section may work. we start with the very simple notion of type that scheme employs. we then identify a notion of safe state. this will be more complicated than in the previous example. again, we must work backwards using the rewrite rules, arguing that safe states come from safe states. we believe that this approach is worth studying for it may lead to some interesting results.



