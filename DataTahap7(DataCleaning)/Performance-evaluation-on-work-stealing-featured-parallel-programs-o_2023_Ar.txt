the emersion of the intel asymmetric performance multicore prod- ucts will further enliven the use of amps architectures for broader applications. for better application and the best practices in software development, it is necessary to understand their performance behavior well. for simple writing purposes in this paper, we rewrite the cpu of p-core as p-cpu and cpu of e-core as e-cpu.



the memory-bounded speedup in [4] can initially explain the phe- nomenon of how superlinear speedup can occur in cases involving problems with large sizes on a computer cluster. with a large problem size, a single node may run slowly due to memory limitations. by adding a number of nodes, the program speed per node becomes faster. memory-bounded speedup can also explain how superlinear speedup can occur on symmetric multicore processors [5,6]. if the memory capacity increases with the increase in the number of nodes on a computer cluster, whereas in a symmetric multicore, the capacity of cache memory increases with the increase in the number of cpu cores used. from here, investigating whether compute-bounded workloads can encounter superlinear speedup events is an interesting question to investigate.



the normal form heterogeneity model is presented in [10]. it ap- pears that the normal heterogeneity model can accurately model su- perlinear speedups, but it may not be effective in modeling cases of sublinear speedups. sublinear speedups are bound to occur if the fastest cpu is used for the serial execution, which serves as the baseline.



in general, the existing speedup law cannot accurately describe the performance behavior of amps. specifically to the amps, superlinear speedup can occur if code that was previously executed by a slow processor core is then enhanced by one or more faster processor cores. conversely, sublinear speedup can occur in cases where serial execution time is obtained from serial execution using the fastest cpu. this research gap is the basis for this research.



although work-stealing has been proposed as a solution to overcome the problem of unfair workload distribution in dynamic heterogeneous distributed computers, shared memory work-stealing on amps has not been investigated to the best of our knowledge. distributed work- stealing in distributed computers has different characteristics than work-stealing in shared memory multicore architectures such as amps. specifically, work-stealing operations that use communication channels contribute to overhead. on the other hand, there is no communication for work-stealing on amps, which means that it does not contribute to overhead. therefore, investigating the performance of work-stealing on amps is still an interesting topic to discuss.



the authors of [14] presented research on the performance evalu- ation of heterogeneous multicore processors on multi-threaded work- loads. in the paper, they demonstrated that heterogeneous multicore is more efficient than homogeneous multicore when handling a multi- threaded workload. the authors deduced efficiency from the weighted speedup, which is calculated by dividing the total ipc of all running threads by the total ipc of the single slowest core. in this case, the authors conducted ipc measurements in a constant time window.



all the works mentioned above focused on the os-level fairly scheduling for the multi-programmed environment. however, our work tested the fair work distribution among the e-cpus and p-cpus in a multi-threaded program. our point of view regarding fairness is that we argue that the faster cpus should endure more works of parallel fraction than the slower ones as other works presented speedup and power scaling models in [10] and aid in [18]. as authors indicated in [19], unbalanced task distribution causes poor performance.



the os-level mechanism ensures that each kernel-level thread re- ceives a fair share of cpu time while working in the multiprocessing domain. on the other hand, the user-level mechanism allows a mul- tithreaded application to efficiently utilize multiple cpu/core time and improve parallelism. to achieve this, programming for multicore systems uses multithreaded programming with a one-to-one mapping approach, where each user-level thread maps to a kernel-level thread. however, if a user-level thread becomes idle (for instance, while wait- ing for other threads due to load imbalance during computation), the cpu time it receives is wasted. in this case, idle threads do not contribute to the performance of multithreaded programs. unfortu- nately, the kernel does not support the management of user-level threads [20]. thus we need a user-level load-balancing mechanism such as work-stealing for multithreaded applications on amps.



a heterogeneous asymmetric multicore chip has the potential to offer more efficient performance than symmetric multicore, as pre- sented by the authors in [7]. although the authors wrote a retrospective paper in [27] regarding their previous model, they made a relevant recommendation that researchers should continue investigating this asymmetric multicore chip, including dealing with scheduling and overheads. our work in this paper presents performance evaluation results where we assigned threads to the efficient core cpus first and enhanced execution with the higher performance core.



in our experiment, we collected performance data from the intel core i5 1240p. the processor has four p-cpus and two e-cpu clusters, each cluster consisting of 4 cores. both p-cpu clusters are implemented with the golden cove microarchitecture, while the e-cpus are im- plemented with the gracemont microarchitecture. therefore, the intel core i5 1240p is an asymmetric performance multicore processor.



in this work, experiments were conducted in a number of scenarios. the scenarios indicate the order of adding cpus in such a way as to see the difference in scalability between e-cpu first and p-cpu first. e-cpu first means that threads occupy e-cpu first until none are left and then p-cpu. conversely, p-cpu first means that threads occupy p-cpu first until none are left and then e-cpu. to achieve that goal, the benchmark program utilized the linux cpu affinity library [39] to determine which cpu to use.



also has the second parameter to specify the number of threads involved in execution. the mode option determines the cpu mask and indirectly determines the order of the assigned cpus when the program increases the number of threads. for example, two options, which select the cpu mask [8:15,0,2,4,6] and nine threads, will make eight threads occupy e-cpus first and then one thread occupy a p-cpu. then increasing the number of threads to ten will make eight threads occupy e-cpus first, and two threads occupy two p-cpus with id 0 and 2. in addition, our experiments collect performance data repeatedly with python script. python script also sets an environment variable cilk_nworkers.



our works provide evidence that work-stealing strategy performs well on amps. we propose to apply work-stealing to solve the problem of asymmetric performance. if work stealing were unable to overcome the problem, then applied work stealing should result in the p-cores being more idle and experiencing a decrease in the nominal ipc value when operating in asymmetric mode rather than operating exclusively. however, this is not the case, as the evidence suggests otherwise. to state it more clearly, we found that there is no performance loss due to asymmetric performance on the intel core i5 1240p, and there is no decrease in nominal ipc on the p-cores.



