drawn from the same distribution, which is essential for the success of a trained nn(kouw, 2018). thus, many synthetically trained nns have performed poorly on real data. on the other hand, training on real data provides nns that are often, at best, as good as the accuracy of the labels that were determined by humans or human-crafted algorithms(weak supervision, due to limited labels or labels prone to errors karamanolakis et al., 2021). so, the data-driven feature of machine learning, in this case, will be highly compromised(zhou, 2017; zheng et al., 2021). as a result, we should synthetically generate data for training our nn that are as realistic as possible.



the concept of trying to bridge the gap between the training and application data in machine learning is referred to as domain adaptation(kouw, 2018; lemberger and panico, 2020). in this case, the training dataset is assumed to belong to the source domain and the application/inference data are assumed to belong to the target domain, the target of our training. the classic theory of machine learning assumes that the application(target) data of a trained model should come from the same general population(sampled from the same distribution) as the training(source) set. so we need the probability distribution accomplished by projecting the source and target data to the eigenvectors of the two subspaces, then finding a transformation between these projected spaces. such projections can be achieved by neural network embeddings(which are low-dimensional, learned continuous vector representations of discrete variables, koehrsen, 2018) aimed to find the weights of the embedding layers that minimizes the distance between the distribution of the source samples and the target ones. there are many ways to constrain the transformation or weights to make the distributions similar including the use of optimal transport(villani, 2008). in this category, even cycle generative adversarial networks(gans) are used for the purpose of learning a generator to map the target data distribution to the source data distribution(palladino et al., 2020). however, these methods become more difficult to apply when the dimensions of the data are large, as is the case with waveform(including seismic and ultrasound) data. the method proposed in this paper shares the general framework of subspace alignment implemented in an empirical fashion, and specifically tailored for waveform(like seismic) data.



the method proposed here is applicable mainly to supervised learning. also, we assume that the vertical axis of the input sections(images or shot gathers) are not crucial in absolute values to the task at hand. only the relative relation between events matters along that dimension. the reason for this assumption will be clear later.



we assume we do not have labels for the application data, and thus, we cannot perform transfer learning(a form of domain adaptation)(wu et al., 2020). in our case, the source synthetic data are labeled, but the target real data are not. this form of domain adaptation is often addressed with unsupervised ml methods(fernando et al., 2013). in such domain adaptation, another important assumption is implied, and data generation represents the actual physical behavior. in other words, the assumptions used to model the synthetic training set represent the object of application(like the earth). this assumption is used in most optimization applications including fwi(tarantola, 1987b). thus, the



of the real data imprint on the training set. assuming we only have noise in the real data, the autocorrelation of random noise yields a quasi delta function at zero lag proportional to the energy of the noise. a convolution with such a function will incorporate that energy into the synthetic data so that the signal-to-noise ratio(snr) in the transformed synthetic data would be comparable to that of the autocorrelated real data.



thus, the steps involved in the transformations, suggested here for domain adaptation, can be summarized using algorithm 1 during training stage. after training, the application(inference) on the real data can be summarized using algorithm 2. considering the linear nature of the transforms involved, all the transformation steps can be performed efficiently in the fourier domain. we only need to inverse fourier transform of the data at the end of the process.



respectively. we also find it helpful to initialize the network following(he et al., 2016), as well as to average predictions within an ensemble(chollet, 2017) of 5 network initializations to reduce noise in the output data. one more note about training is that we setthe set the



the marine streamer and the number of time samples, respectively. receivers are spaced 25 m apart while the temporal sampling is coarsened to 8 ms. we create a training dataset of 3072 shot gathers by modeling 3 shots in each of the 1024 random subsurface realizations. these shots are then split into partitions of 2765, 154, and 153 samples for training, validation, and testing, respectively. the set of random subsurface models is derived by distorting the layered models using



one of the reasons for the crosscorrelation with a reference trace is to balance the contributions from synthetic and real data in the input data to the network, without affecting the general features of the input data. in the microseismic example, this operation also helped reduce the input data size. the fact that the input-to-the-network is effectively different from the original data is not an issue for nn models, as they adapt to any data we deem as input. what matters is whether the input is consistent between the training(source) and application(target) data. actually, the cross-correlation operation can enhance the data with features(from the cross-talk between unrelated events) that will further enrich the training data with information that can help in identifying the corresponding labels. for example, if a shot gather includes two reflections, the crosscorrelation of that shot gather with a reference trace from it will result in two additional events from the crosstalk of the two events, and these added events will be different from one shot gather to another depending on the distance between the two events and their moveouts. such additional events will add to the information embedded in the input data that the nn model can use to learn to identify the corresponding labels.



we proposed a novel technique to precondition the synthetic training data set for a supervised neural network optimization so that the trained model works better on real data. the concept is based on incorporating as much information from the real data into the training without harming the synthetic data features crucial for the prediction. considering the two data domains(synthetic and real), we specifically cross-correlate an input section from one domain of data with a reference trace from that data followed by convolution with an autocorrelated section from the other domain. for training the nn model, the input section is from the synthetic data domain, and for the application(inference) of the nn model, the input section is from the real data domain. a test of this approach on a microseismic source



we thank umair bin waheed from kfupm, frantisek stanek from seismik, and claire birnie and yuanyuan li from kaust for helpful discussions. we thank microseismic, inc. and newfield exploration mid-continent, inc. for graciously supplying the data. we thank cgg for the marine dataset. we also thank kaust for its support and the seismic wave analysis group(swag) for constructive discussions.



