thus, in this paper, we still adopt soft margin consistency and apply it to semi-supervised problems and then propose the semisupervised soft margin consistency based multi-view maximum entropy discrimination(ssmvmed). but during the process of ssmvmed, there is a potential trouble when the data sets consist of few labeled instances and many unlabeled ones. as we know, compared with unlabeled instances, labeled ones can provide more useful discriminant information while in real-world applications, most data sets consist of few labeled instances and many unlabeled ones and labeling instances is a high-cost task. thus, for traditional semi-supervised problems, the performances of learning machines are sensitive to the data sets. in order to enhance the performance of a learning machine, a widely used and feasible method is generating additional unlabeled instances with the original labeled or unlabeled ones and combining all instances together. these additional unlabeled instances will possess some discriminant information derived from the original labeled and unlabeled instances. here, universum learning is such a kind of method. for



but for these universum-based learning machines, there still exists two key problems. first one is that when generating universum set, the weights of views and features which play different discriminant roles are always neglected. second one is that when generating the additional unlabeled instances, traditional universum-based learning machines only adopt labeled or unlabeled instances for generation. in order to solve the first problem, we adopt weighted multi-view clustering(wmvc) which is a multi-view clustering method and can find the optimal cluster assignment. with wmvc, the weights of views and features can be gotten. for the second problem, we try to design some schemes and adopt both labeled and unlabeled instances to generate the additional unlabeled instances.



smvmed is different from mvmed and amvmed due to the margin of smvmed is soft. smvmed achieves margin consistency by minimizing the kl-divergence between the posteriors of margin parameters from two views. then a trade-off parameter balancing large margin and margin consistency is also introduced to make the model more flexible. the model of smvmed is given below.



(1) course data set is used to describe web pages and we want to predict whether the given web page is a course page or not;(2) citeseer and cora data sets both consist of 4 views and we choose view content and cites here;(3) webkb data set consists of web pages collected from four universities: cornell, texas, wisconsin and washington which have 5 categories, i.e., student, project, course, stuff and faculty. data in webkb are described with two views: content and citation. we treat webkb in four separate data sets grouped by universities;(4) newsgroup data set is of six groups extracted from the 20-newsgroup dataset, i.e., m2, m5,



(paired t-test is different from t-test) and nemenyi statistical test for quantitative evaluation analysis. paired t-test is used to analyze if the differences between two compared learning machines on one data set are significant or not. then for nemenyi statistical test, it is used to analyze if the differences between two compared learning machines on multiple data sets are significant or not. nemenyi is different from another famous test, i.e., friedman statistical test which is used to analyze if the differences between all compared learning machines on multiple data sets are significant or not. since the number of compared learning machines here is two, thus we adopt nemenyi statistical test rather than friedman statistical test. in generally, the differences always indicate the ones in test accuracy. thus, here we conduct quantitative evaluation analysis in terms of test accuracy.



in order to validate the higher training time, we give the computational complexity of them theoretically. as we know, in ssmvmed, it consists of three steps. so here, we will discuss the computational complexities for different steps. for convenience, we let the number of labeled instances be n, the number of original unlabeled instances be l, the number of additional unlabeled instances be u.





