cloud computing is a novel computing paradigm that has revolu- tionized the way applications are designed and provisioned. it promotes the use of computational and networked resources on demand to support application provisioning. as such, resource management is outsourced to cloud providers enabling to reduce or diminish operational and man- agement costs.



the cloud computing success has also led to raising the abstraction level via the supply of extra service types above the infrastructural ones. in particular, platform services are offered to reduce the burden in creating and managing the execution environments within the leashed resources. further, the scope of such services has been also extended to facilitate cloud application design and development.



virtualisation also impacts security as physical hosts are shared by users in the form of virtual machines (vms). this can raise issues where malicious users can see the data stored in a vm or take control over all vms by attacking the hypervisor. further, as applications are distributed over public clouds, networked attacks can be performed over them which can, e.g., cause crucial information to be leaked during transmission or application overload based on denial-of-service attacks.



security services can be of a different nature, while they be also reactive or preventive. reactive services can detect an attack and possibly address it. for example, intrusion detection and protection systems can be used to protect applications from network attacks by, e.g., detecting denial-of-service attacks and blocking their origin ips. preventive ser- vices prevent a security incident from happening by detecting application areas that are vulnerable to certain attacks. the latter service kind can take different forms.



this article focuses on vulnerability scanning tools and databases. the reason for this is that, while malware and antivirus software is well- known and adopted by both organisations and individuals, vulnera- bility scanning tools are not widely used in practice. further, it is hard for a practitioner to choose the right vulnerability scanning tool due to the great tool diversity and varied coverage. as such, this article aims to guide the practitioner in making an informed decision about which vulnerability scanning tool and database to select for his/her application. such a selection relates also to particular challenges which have not been currently confronted in research, thus supplying an invaluable contri- bution to the academic community. in addition, this article contributes towards the identification of those places in the application lifecycle where vulnerability scanning can be performed and supplies valuable insights towards better securing cloud applications. finally, this article investigates and proves that vulnerability scanning tool orchestration can be the answer towards higher vulnerability detection coverage.



the scanning via a tool can be performed in two different modes. in the internal mode, the tool is installed inside the application and attempts to scan its components, hosted in vms or containers. such a mode can address any application component kind due to the potential use of both static and dynamic scanning methods. thus, its scanning accuracy is increased. further, it leads to reduced communication overhead as most of the communication occurs within the same hosting environment. however, it can have the greatest impact in terms of interfering with the application performance as it can steal precious resources from the same hosting environment. in the external mode, the scanning is performed externally to the application and relies on the endpoints on which the application web-based components reside. as such, only the latter component kind can be confronted. this scanning mode has the advan- tage of conducting the scanning without interfering internally with the application hosting environment. however, it might not be able to attain the highest possible scanning accuracy as it cannot exploit static scanning methods while it increases the communication overhead in the applica- tion system.



in a client-server architecture, a server remotely orchestrates the execution of different agents, either residing on the application vms/ containers to perform the scanning internally or on different vms to conduct an external scanning. such an architecture can more easily scale while it can accelerate the scanning by orchestrating the parallel agent execution. in a standalone architecture, the scanning tool is installed in one place, either internal or external to an application, and then initiated to perform the scanning. such an architecture variant/class has the main deficit that the orchestration of the installation, execution and report merging of the respective vulnerability reports burdens the user.



architecture classes, i.e., client-server and standalone, are orthogonal to this mode. this is depicted through the use of different colours in the respective components involved mapping to the four possible architec- ture scenarios (associated with all possible combinations between the two scanning modes and architecture classes).



as such, vulnerability databases (vdbs) are usually employed to cover the information to be included in a vulnerability scanning report. vdbs usually include some common attributes dedicated to properly identifying and explaining vulnerabilities. they might also include extra information, like which artefact is affected and how the vulnerability could be addressed. many vdbs also conform to standards that support a kind of standardised identification or determination of the information to be covered.



executor needs not be solely the system admin. the admin is mostly responsible for application deployment and provisioning, e.g., on vulnerabilities covering the os level or the application execution environment in general. however, the application devop can also be involved, when issues regarding the application code must be confronted.



vulnerabilities only affect the application component code. as such, only certain application components will need to be modified, thus going back to the development lifecycle activity. then, the physical management flow will be followed leading to application redeployment and testing to check whether the vulnerabilities have been properly addressed without breaking the application integration.



based on this manual and time-consuming practice, this paper aims at evaluating vulnerability scanning tools against a criteria-based frame- work. such an evaluation can unveil which tools prevail and according to which aspects. we put special focus in the latter case on the trade-off between scanning accuracy and time to which we add the vulnerability coverage dimension.



a certain procedure for tool search and selection was followed, comprising two main steps: (a) actual search of the tools; (b) selection of the right tools from those discovered based on a set of inclusion criteria. the search relied on a two-front approach. in both fronts, we have relied on a set of keywords deemed as most relevant for the search. this set was slightly differentiated depending on the concerned artefact



the second search front was formulated to cover prototype tools or databases from academia that cannot be found easily from a web search. such tools could be increasingly improved and extended due to the relative research and development tasks conducted by the research or- ganisations producing them. to discover such tools, we employed well known academic data sources, i.e., web of science and scopus, enabling us to have a full literature coverage.



for the survey on vulnerability databases, we focused on a compar- ative evaluation based on the very nature of research question q1. thus, we created a set of evaluation criteria which were either devised by us or drawn from the literature. all criteria were produced by considering two aspects: information coverage, capabilities & support. more details are supplied in section 4.



most vdbs focus on any kind of vulnerability; very few have a more restrained scope. for risk assessment, many vdbs adopt common vulnerability scoring system (cvss) while very few just provide an impact measure. some vdbs supply both impact and risk information for each vulnerability.



access control mode: a tool might need to be executed based on certain access control rights to be operationally successful. thus, if the goal is to find os issues, the tool should have privileged access to check the os-specific part of the hosting component (e.g., vm). this criterion also correlates to the usage level as when that level is non-intrusive, the access control mode can be the least critical.



standards. both cve and cwe seem to be mostly supported. this is logical as these standards enable to better characterise a certain vulner- ability and classify it. next comes cvss followed by cpe which have slightly more than one third of support. this indicates that the need to associate vulnerabilities to the components concerned has not been well recognised yet while the capability to assess the risk related to a vulnerability seems to be neglected.



community. we have found that most tools have their own commu- nities, usually maintained by the tool provider. such communities can interact with the tool provider via emailing lists and blogs, while they can also participate in the tool development. the latter is the cornerstone of open-source communities, usually built around interesting or innovative tools that become sustainable via their participation. as such, security software providers follow faithfully this model which is widely adopted in the software world.



we must appraise the existence of certain organisations, i.e., nist and owasp, that promote in the best possible way security software. nist focuses more on developing standards aiming to bridge interoperability issues, while owasp focuses on promoting innovative tools, creating tool benchmarks plus classifying both tools and vulnerabilities. both organisations appear twice as community members in two respective scanning tools. this also indicates that these organisations have a great belief in the dynamics and capabilities of the tools they support.



similarly, more than half of the tools support information gathering. this is also logical as before any kind of vulnerability assessment is performed, the target object needs to be scanned to find the right places where vulnerability assessment will focus, collected in an information gathering step. depending on the object of focus, information gathering can take different forms. for pure web application scanners, spider software is exploited to find all actual web pages from which the scan- ning can be performed. for more sophisticated scanners, information gathering involves checking the whole host, finding open ports and attempting to infer the components that run on such ports.



object. almost all vulnerability scanners can operate over a web application. however, as the next paragraph and section 5.2 will show, each scanner might have a different focus on the web application vul- nerabilities by, e.g., concentrating only on specific kinds, like the most usual ones.



most old scanners focus solely on web applications as the target scanning object. the main rationale is that application developers would care most how web applications are vulnerable in their operation space (e.g., set of web pages) from the outside. however, as an application might comprise multiple components and might run on vulnerable media, it is apparent that the focus should now move to holistically cover the whole application system. this is actually grabbed by both recent scanners and academic approaches. a good example for the former case is openscap that covers not only pure web application but also cross-level system vulnerabilities. in the latter case, academic approaches extend an almost complete vulnerability scanner, like openvas, to make it perfect. vulnerability coverage. by considering all tools, it seems that only openvas and openscap have the highest vulnerability coverage along with tools that re-use them. by considering that: (a) the percentage of these tools is less than half of the overall tools; (b) openvas and openscap do not focus on scanning the whole web application opera- tional space, it is easy to understand that such a result marks the need to



in fact, this might signify that security experts must now move to a new direction: as there exists a sophisticated state-of-the-art tool, the community must focus on both improving and evolving it over time to also detect new vulnerabilities via the production of respective plugins. in fact, this already occurs for the 2 aforementioned tools. in the context of network vulnerability tests (nvts) or oval definitions, openvas and openscap, respectively, rely on a huge community effort, involving a great amount of os and software providers contributing to the defini- tion and detection of vulnerabilities.



inferencing. only one tool, vulcan, supports some kind of inferencing. this academic tool employs ontology-based reasoning to support infer- encing. restricted to the current tool functionality, this inferencing takes the form of discovering vulnerabilities of component agglomerations apart from those of individual components. however, this ontology- based reasoning approach is quite promising as it could be extended to cover the derivation of extra kinds of vulnerability-related knowledge; these kinds are explained in section 6.



counter-measures. also correlated to mitigation plan production, this criterion highlights the need to associate vulnerabilities with the ways they can be addressed known as counter-measures. in contrast to the previous criterion, though, the situation is much better as almost half of the tools support this criterion. this means that the need to report this correlation kind to users has been well recognised and realised. further, most tools exhibiting this feature support standards like cve or cwe which do provide counter-measure information. this is another benefit related to the support of standards.



openscap seems to be the sole tool going beyond counter-measure reporting to counter-measure enforcement. in particular, through sup- porting scap, this tool can mitigate the vulnerabilities found in some cases via automatic path application. this is the right direction to be followed by all other scanners to really advance their usage and increase their added-value.



tool coverage. scanning tools re-use a great variety of sub-tools, spanning many owasp taxonomy categories, including antivirus pro- tection, information gathering, network scanning, and data validation testing. from these categories, we can discern nessus and openvas, as those sub-tools mostly re-used due to their almost perfect coverage extent, as well as nmap, as the most popular and well-adopted informa- tion gathering/network scanning tool.



best tool nomination for the functionality category. in this criteria category, there is not a clear winner. we can actually see 2 main tool partitions: (a) top approaches which do not support inferencing; (b) the vulcan academic approach which supports inferencing but has a very low coverage and focuses only on web applications as target objects. the latter also indicates that if vulcan was able to use nessus or openvas, we would surely nominate it as the best in this category, as this would immediately improve in an ultimate manner its two main deficits.



in the first partition, the academic tool aws-vuln is the clear winner. it not only has the best possible categorisation and vulnerability coverage but also re-uses a great sub-tools set, which constitutes another proof of its completeness. from the pure tool world, we can discern openvas, which, however, does not cover additional security scanning categories. from the 2 best result partitions, academic tools can be discerned.



tools that employ a client-server architecture are about one third in number. they usually exhibit a better vulnerability coverage and focus on more advanced, it-security expert users. as such, in contrast to the previous paragraph argumentation, these tools should employ a client- server architecture as the most natural way to completely cover vulner- abilities across the whole application plus the most flexible and capable way in terms of administering the scanning. due to these advantages, it is worth sacrificing slightly the simplicity to better attract the main target users, i.e., it-security experts.



modularity. a modular tool can be individually extended as needed. it also allows making a focused scanning by using only those modules necessary for detecting the right vulnerabilities for an application. this need for modular tools seems to be picked up by the tool providers as most tools are modular.



exhibited by particular kinds of applications or their components. partial modularity, on the other hand, is a limited modularity form that implies the existence of one profile from which the user can select some of the scanning rules contained. in some cases, the addition of new rules might be also possible in such a profile.



the evaluation results with respect to the first ossupport type signify that most tools support linux-based oss. this is well expected by considering that such oss have been deemed more secure (at initial deployment time without any extra hardening support [48]). windows come next with around two third of tools able to operate in windows environments. os-x comes third but still having more than half of the tools supporting it.



coverage plus its administration. it might also raise security concerns about how safe are clients operating in admin mode in a client-server architecture. thus, tools operating in a normal, user mode are less intrusive and do not access critical assets via internal scanning. however, as such assets are uncovered, vulnerability coverage is low. on the other hand, an admin mode enables accessing (possibly infected) critical sys- tem assets and thus caters for a better vulnerability coverage. however, acting in this mode comes with issues that might involve the incautious exchange of credentials and the possible scanner infection thus exposing the respective system to great vulnerability.



overall evaluation results of this criteria category, openvas and openscap can be considered as the best. openvas seems better in terms of os support while openscap is better with respect to hardware re- quirements. as the os support can be less important than hardware re- quirements, we might then nominate openscap as the best. in both cases, both tools seem to support only one kind of architecture (deploy- ment). thus, we can deduce that there is still room for improvement of these tools.



based on the above results, the best tool with respect to the overall scanning accuracy and time is findsecuritybugs. in fact, this tool ach- ieves a high accuracy on many vulnerability areas. this might be regar- ded as unexpected as this tool only includes a small set of security rules. however, it can be well justified by the fact that this tool operates directly on the source code so it can indicate with a higher accuracy if a certain issue holds.



the findbugs tool [82] enables conducting static analysis to assess java source code quality. this extension enhances findbugs with the capability to find security bugs. however, it does not enable filtering the security bug detection rules. only explored possibility is to either apply only the security rules or all bug detection rules, i.e., including those originally areas but exhibits good accuracy performance only in two of them. compared to findsecuritybugs, it is slightly better in two vulnerability areas: command injection and path traversal while much better in cross-site scripting and sql injection. this signifies that the agglomeration of these two tools will enable to have a better vulnerability area coverage and thus reach a much higher overall accuracy level. however, some vulnerability areas are not yet deeply covered, including: command in- jection, ldap injection, path traversal, trust boundary violation and xpath injection. this outlines the need to consider another scanning tool to complementarily cover more deeply these areas.



due to the main benefits that semantic technology brings about, we expect a proliferation of semantic vdbs in the near future. currently, only vulcan offers a semantic vdb. however, this vdb is not rich enough and coupled with the right rules to allow inferring various knowledge kinds. further, it seems to manually and not automatically integrate



cwe seems to propose a rich vulnerability and threat taxonomy. however, this taxonomy should be extended accordingly to become a semantic security meta-model able to both cover the most important security concepts and their relations. this coverage along with semantic rules incorporation would then enable inferring further relations be- tween security concepts which would never be acquired by a syntactic modelling approach. apart from this, such relations would improve the scanning accuracy as explicated below.



the first direction indicates that, as there are many special-purpose standards, most of them must be supported by the scanning tools so as to become more complete. further, this promotes interoperability, especially in terms of tool re-use or orchestration, as it will be shown in the next sub-section. for instance, support to cve and cvss can enable to merge vulnerability reporting results from different scanning tools. however, as some standards seem already prevailing while others promising, possibly different priorities must be given to different stan- dards for their adoption. in our opinion, apart from supporting cve, cwe and cvss, there is a great need to also support oval and scap. the first can enable a uniform way to specify and address vulnerabilities while the second their automatic mitigation. the support to scap will be further elaborated in a later sub-section.



the use of scanning tools and highlight the criticality of addressing application vulnerabilities; (b) organisation of conferences, workshops and competitions to highlight recent vulnerability scanning advance- ments as well as the top tools possibly categorised under different competition areas; (c) supply of benchmarks or scanner selection tools to assist users in evaluating and selecting the right scanners; (d) supply of interfaces and mechanisms via which valuable user input (e.g., feedback, plugins) can be provided in a more natural and user-intuitive way.



while most tools provide some engagement support, this is not necessarily the case for academic tools. such tools, while presented in academic forums, are not always made publicly available or offered as open-source. further, such tools do not supply usual engagement facil- ities like mailing lists and code development portals which hinders their sustainability and further evolution.



second, as witnessed by the empirical evaluation, even web appli- cation scanners are not so good and must be complemented by source code analysis tools. if fact, as advocated in aws-vuln, extra tools (e.g., antivirus) apart from vulnerability scanners are needed. thus, it is actually advocated that there is a need to orchestrate a great number of different tool kinds to achieve the best possible coverage against the whole application system in a continuous manner. only in this way, applications can be permanently and fully protected during their whole lifetime as both applications, their components plus penetration methods and techniques continuously evolve over time.



there is a need for a proper agglomeration strategy encompassing the appropriate use of the right tools of the right kind at the right moment. for instance, vulnerability scanning might be decided not to be performed in conjunction with application protection as a very heavy load could be put on the application, especially as protection testing is performed in an intrusive mode. further, it could be decided that source code analysis could be performed each time the applica- tion is modified and in random moments in case we need to detect unexpected and irregular modifications in the application source or binary code.



pattern mining techniques over security logs. while this is a practice currently followed in a manual manner by well-known software pro- viders, it could be an added-value to automate and integrate it in the cloud application management system. this automatic rule production would reduce detection costs and accelerate application evolution to- wards resolving the vulnerabilities detected. by also employing an approach towards properly identifying and publishing vulnerabilities, the community will benefit via: (a) the reduction in effort and time in vulnerability publishing; (b) the rapid addressing of new vulnerabilities.



the vulnerability world is dynamically evolving constantly such that new vulnerabilities are detected each day. as such, a user is usually faced with a great number of vulnerabilities inferred just for a single applica- tion. while a prioritisation of vulnerabilities based on their risk could enable the user to focus more on the most critical ones, we believe that this should be complemented with the capability for automatic vulner- ability mitigation. the latter can enable the user to be burdened only by those vulnerabilities still critical and not automatically solvable.



going beyond topology models, there can be cases where different user applications communicate to each other. thus, one serious vulner- ability in one application might have the risk to be propagated to another. however, by knowing the topology models of applications that communicate to each other, the scanning process can focus on also checking cross-application vulnerabilities (e.g., side effects of wrong transactions initiated by one, already infected application and handled by another application). this is a novel research direction, not considered before in the literature.



the way this can be resolved is twofold. first, by usually inspecting each application from the pair in an individual manner. this relies on the rationale that each application could be considered as an end-user for the other. as such, this end-user could be considered as one source of vul- nerabilities, usually checked by data validation testing and other scan- ning technique kinds. second, by allowing users to supply their own checks that focus on detecting those inconsistent system states which could make the whole system or an application as vulnerable. this can be considered as a kind of user-specific vulnerability detection, enabling a scanning tool to go beyond the detection of known vulnerabilities to- wards application/domain-specific ones. this would certainly contribute to a better tool completeness and suitability. coupled also with the ability to define certain mitigation actions when anticipating such in- consistencies would make the vulnerability scanning tool a full, advanced application security protection software, further enhancing its added- value and applicability.



malware and antivirus software is widely adopted by both organisa- tions and individuals due to the continuous and, in many cases, sophis- ticated risk mitigation support that it features. however, such software takes a reactive approach in dealing with security issues. on the other hand, while vulnerability scanning tools follow a proactive approach by identifying those places in the application system that need improvement to avoid security issues from happening, they are not widely used in practice. further, even when decided to be adopted, the great tool di- versity and varied coverage makes it hard for a practitioner to choose the right vulnerability scanning tool. to this end, this article offers the following contributions: (a) it guides practitioners towards making an informed decision about which vulnerability scanning tools and data- bases to select for their applications. such a guidance is supplied through a comparative evaluation approach that relies on a two-level, hierar- chical comparison criteria framework for both scanning tools and data- bases. the evaluation results supplied unveil which are the best scanning tools and databases per each criterion, category of criteria and in overall;



once the right set of vulnerability categories was identified, the second step of the followed approach involved the assessment of each vulnerability scanning tool considered based on whether it covers each of the categories identified. towards this goal, we have relied both: (a) on the actual findings2 from a previously conducted research that is reflected in sectoolmarket. com about the coverage of a certain set of open-source vulnerability scanners as well as (b) on an individual evaluation, conducted by us, of those scanners which were not included in the aforementioned evaluated set.



it might be observed that some scanning tools have the same coverage. this is due to the fact that all these tools rely on openvas. as we were not able to assess them easily over their actual coverage, we have considered that their accuracy could not be less than that of the sub-tool that they exploit. in this sense, as the coverage of openvas is the best possible, mapping to the top coverage partition, our consideration is precise according to the scale that has been adopted. further, we should also note that we were not able to assess vulcan as its code was not available. however, based on its respective documentation, it relies on a certain limited scanning framework which is expected to have a very low coverage. so, our estimation should be correct here.



