at the application side is called static principle forming and rule forming at the controller side basis on switch demand is called dynamic rule shaping which may be tedious and troublesome[15,16]. to address this issue, a reinforcement learning-based method is proposed that maintains a strategic distance from the bottleneck and provides more efficient rule management at the controller side. this is done by making the automated rule generating process where the controller has the option to build rules dependent on past encounters. the main contributions of this work are as follows:



the remaining of this paper is organized as follows: section 2 presents the software-defined network details and their relation with machine learning-based rule generation. section 3 presents the related work and problem formation. section 4 presents the proposed rule formation agent design and development steps. section 5 presents the experiment results. the paper concludes in the end with a future direction.



the application layer is the top layer of the sdn network which provides services to the controller. the application layer forms the flow rules for the packets in the network. these rules are the basis of packet scheduling. with all of the information related to the network applications and topology of the network, the application layer keeps a check on the state of the network. the application layer communicates with the control layer using the control message. control layer, controller, control plane is the central part of the sdn which is also called the brain of the network. the controller is connected to the application layer, and the infrastructure are sent to the switches which include flow information and all this communication among the control plane and data plane is done via southbound apis. so switch work according to the rules provided by the upper layer to proceed with the network flow. the following research discusses the previous work in the context of rule formation.



the authors in have worked on dynamic flow rules in sdn. this research focuses on the flow rules dynamically this research proposed a methodology to increase the flow of the sdn by creating own flow entries to keep the network data work efficiently using this strategy switch will be able to form rules locally which means modification needs in the rules will be done on the switch level locally which will increase the flow of traffic in the network. this research presented three different procedures to implement this strategy. and the author claimed that this strategy increases the efficiency of the sdn network. this works as the extended version of the openflow.



in order to evaluate the performance of the proposed rfa, the simulation is performed. the tools used for the implementation of this research are open daylight controller and mininet and the platform used is ubuntu which was installed on a virtual environment using a virtual box. for benchmarking with classical sdn implementation the throughput and packet loss and are considered. besides, an important metric flow insertion delay is also considered that reduces the overall delay for the rule insertion which



whenever a flow rule is missing, predictably, packet loss happened because more time is taken by the network to provide the rule. after all, more latency which as a result aids the packet loss that happens due to bottleneck at the switch. when a lot of packets are in the queue and rule is missing for a packet it not only go towards the packet loss of the packet with the missing rule problem but can also aid the packet loss of other packets reside in the queue.



age on the basis of which we train our module and those past rules helped the module to do its job automatically in order to deal with the problem at the switch. we used default rules sent by the application layer in order to train our model which turned out to be a good approach in order to train the model. as we discussed earlier that reinforcement learning works on the basis of reward so it became easy to train a model using machine learning plus circumstances on the sdn controller switch indicates a way of using the reinforcement machine learning approach in order to train our model and make our model work well to reduce the effect of the problem in the software-defined environment. so we answered all the questions we were supposed to answer in our research.



approach gained huge popularity with each positive aspect of technology. we have to deal with the drawbacks also in terms of software-defined networking; there are loopholes which need to be solved as in our research. we followed a problem at a traditional software-defined network which is regarding the flow rules as flow rules plays an important role in a network as they keep the network traffic going so problem regarding them leads the whole network to a deadlock situation. the other aspect when a security threat hits the system in that case too our module finds a way to keep the network traffic going whether a system is under attack or there is any other issue regarding missing rules. we proposed a methodology that works with the mechanism of reinforcement learning to form flow rules automatically and our results show that this approach can help to improve the software-defined networking environment. machine learning algorithms have been used in a lot of researches and regarding solving many areas of problem in software-defined networking so our research can become part of a big intelligent software-defined networking environment. latency has been a considerable problem of the sdn environment which is taken care of in our research plus the issue regarding missing rule is also taken care of and the main scenario which was the base of the research that if at some risk application layer got disconnected, how the traffic is going to process in the network. in future, other supervised learning techniques will be used to perform auto rule formation.



