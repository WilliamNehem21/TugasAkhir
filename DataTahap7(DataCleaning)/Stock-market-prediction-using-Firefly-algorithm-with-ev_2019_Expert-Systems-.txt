series data are very noisy by nature. later the issues regarding the existence of non linearity in time series data has been solved by the approach of so many machine learning techniques such as an artificial neural network(ann), functional link artificial neural network(flann), support vector machine(svm) etc. in the last decades, numerous researchers have explored the prediction ability of neural networks for financial market prediction.



tains the related work which basically focuses on the promising techniques used by various researchers in financial domain to get best possible results. section 3 covers the details of data samples and the dataset regeneration part. section 4 introduces all the predictive models in details and section 5 covers the particulars of statistical and optimized based feature reduction techniques. section 6 provides the result analysis, which includes a schematic layout of the proposed prediction model, parameter setup, detailed steps of the proposed algorithm, the analysis of the experimental



elm works on the principle of single layer feed-forward neural network(slfn), with the basic difference is that unlike slfn the hidden layer nodes and biases of elm need not to be tuned at each iteration. in almost all practical learning algorithms of feed forward neural network, it has been marked that the input weights and hidden layer biases are needed to be adjusted at each iteration. comparing to slfn, the learning speed of elm is very faster with smallest training error and smallest norm of the weights. for developing elm, moore-penrose generalized inverse takes an important task.



work models, cannot be well trained properly. hence, the feature reduction technique is introduced in this study to get the most relevant features of original datasets, for improving the performance of the model. both statistical based and optimized based feature reduction methods are employed to the dataset. the details of which are described in this section.



fa accepts the observed variables are the linear combination of unobservable factors and among these factors some are common to two or more variables and some expect unique to each variable. the factors which are unique do not contribute to the covariance between the variables or it(kim& mueller, 1978) can be said among the observed variables the common factors, which are smaller in number than the observed variable numbers contribute to the covariation. the basic difference of pca from fa is that, in pca, the principal components are having certain mathematical functions of the observed variables, while the combinations of observed variables is not able to express the common factors, whereas, the main objective of fa is to determine the common factors which can produce the correlations among the observed variables satisfactorily.



basically the light intensity variation and attractiveness formulation are the two important issues in firefly algorithm. simply we can say the attractiveness is determined by the brightness or the intensity of the light, which is successively associated with the objective function. the computational complexity of the(lukasik& zak, 2009) algorithm is considered to be o(m2) but the large population size may lead to substantial increase in the time complexity.



apart from the individual algorithm specific parameters, population size and number of iterations are two common controlling parameters, which are kept fixed for all the experimentation of this work. population size has a great role in determining the optimal solutions as a small population may miss the part of the solution space and the large populations may increase the computational time, hence a 100 number of populations is considered for a standard comparison. in addition to this, number of iterations is another parameter which effect the running time to get a good solution, hence number of iterations is also fixed to 100 for all the experimental work.



malization and feature reduction. for normalization, min-max normalization is well thought-out in this study, which scales the data between 0 and 1. in case of feature reduction two types of feature reduction occurs, one is statistical based feature reduction and another one is optimized based feature reduction. pca and fa are used for statistical based feature reduction whereas ga, fo and firefly algorithm with evolutionary framework are designed for optimized feature reduction. now the reduced data are ready for input to elm, oselm and rbpnn prediction models.



bse sensex, nse sensex, s&p 500 index and ftse 100 index are four stock market datasets are selected to evaluate the prediction model. bse sensex is one of the main stock market indices, which includes 30 blue chip company. similarly, nse popularly known as nifty fifty, which covers 50 specialized company and these companies is focused on index as a core product. apart from these two indian stock exchange, this study has considered two non indian stock exchange such as s&p 500 and ftse 100, which is belongs to american and uk exchange respectively. s&p 500 based on 500 market capitalizations whereas ftse 100 based on share index of 100 companies with highest market capitalization. for a better evaluation of the experimented models these four different stock exchange value is considered for experimental purpose.



time horizon. in both the cases elm with the reduced features obtained from pca and fa is performing better for one day ahead prediction than the rest of the time horizon days ahead to be predicted. similarly the result of optimized based feature reduction is analyzed. ga, firefly and firefly algorithm with the evolutionary framework optimized feature reduction methods are well thought out for this study. ga is a stochastic algorithm, where there is no possibility of trap into local optima rather it jumps out of locality. similarly fo is a heuristic population based algorithm, which explore the search space very precisely. combining the concept of firefly algorithm with ga, firefly algorithm with evolutionary framework is designed, where ga updates the current best solutions generated by firefly optimization to move towards the best solutions. the motivation behind this integration of firefly with ga is to find good solutions and simultaneously avoiding for trapping into local minima.



