coding rules to be of practical use, no matter who devises the coding rule set or dictates its use. there exists a number of commercial compilers and quality assurance tools from vendors such as iar systems and parasoft that claim to be able to check code for compliance with a subset of hicpp, misra-c or other standards. other tools, e.g. klocwork, define their own list of informally described rules aimed at avoiding hazards, and users can add new rules by means of complex application program interfaces(usually in c or c++). but, in absence of a formal and concise definition of rules, it is difficult to be certain about what these tools are actually checking, and two different tools could very well disagree about the validity of some particular piece of code with respect to, e.g., the same misra-c rule.



the other salient feature of our coding rule checking tool is that it has been integrated into the gnu compiler collection(gcc, gcc.gnu.org) development tool-chain. in this work we present a new version of our coding rule checker, improved the one presented presented in, that extracts all the needed information about c++ programs while compiling them with gcc. one reason to put together the rule checker and the gcc tool-chain, which is a non-trivial task, is to make the checker readily available in the everyday tool of thousands of developers, which will undoubtedly foster the adoption of coding rules in many projects. moreover, by using a single parser and semantic analysis engine(used for object code generation) both for the compilation and to gather information for the code checker, any possible discrepancy on how code is interpreted(which could happen if a different parser/ anlyzser were used) is avoided. in addition, information gathered by static analyses already present in gcc can be reused to implement rules that need it.



the current rule definition language is a subset of prolog with some syntactic extensions(section 3). in the future we plan to facilitate rule formal definition with a completely declarative logic language featuring sorts, constructive negation, and appropriate quantifiers(some details are given in). our aim is to enable developers not familiar with prolog to formalize coding rules.



the prolog formalization of the rule 3.2.4 codifies a violation of the rule, i.e., that an abstract class class has a member ctor which is a public constructor. if the rule can be violated, concrete instances of ctor and class in the software analysed will be returned to the user by checkrules, along with a warning message, associated with the rule by means of operator#. the arguments of the predicate can be displayed as part of the user message.



the middle-end of gcc contains a set of transformation and optimisation passes that are independent from both the compiled language and the target architecture. many(but not all) of the program features needed for writing rules are common to multiple languages and have a common representation in the middle-end, even if the semantics may differ between different languages. for example, constructs as templates and friends are almost exclusive of c++, and only the c++ front-end knows about them.



in our extended version of gcc we have instrumented both the middle-end and the c++ front-end(totalling around 2.8 klocs of new code), but most of the analysis is done in a new middle-end pass. implementing functionality in the middle-end has many advantages: adding a new pass is simple and clean, and there is no overhead unless the pass is enabled using a corresponding flag. furthermore, the new functionality may be reused for other gcc languages.



for every relevant entity in the code a prolog term of the form entity(global key) is generated, where entity is one of enum, enum value, union, record(either a struct or a class), function, global var, method, field, and bit field. global key is a project wide identifier of the entity, based on name mangling. mangled names are a special encoding of names of functions, variables, etc. generated by the compiler for the linker and other tools that have



the naming scheme for local entities(local variables, function arguments, etc.) is based upon the scope in which they are defined(that is a global entity) and its local identifier. for anonymous entities(e.g., anonymous union fields) a numerical identifier is generated.



number of rule violations and user time(in seconds) consumed by the different steps in our rule checking procedure. bt is the total build time of the project, and btcr is the total build time with structural data gathering enabled. lt is the time that takes checkrules to load the project. columns labeled with a rule number show, in each cell, the number of violations(above) and the checking time(below).



build time increases when-fipa-codingrules is enabled to extract structural information during compilation. the time penalty is less than 5% in the case of bacula, but more than 60% for ppl. in general, the slow down is more noticeable when templates are extensively used. reported build time is small for dlib, in spite of its size, because of the simplicity of the build procedure, consisting in the generation of one single object file.



on the other hand, checking time goes from a fraction of a second(i.e., cells with 0.00 time) to over one minute. empirically, rules where complex sub-goals appear negated are the ones which take longer. in fact, as there is no restriction on the computational complexity of the rules, execution time is potentially unbounded. despite this, the observed performance is reasonable for a project-wide static analysis tool that is not meant to be run in every compilation.



note that, despite the significant number of violations we found for many rules, hicpp 3.3.14 and 3.3.15 are not violated in those projects. one reason is that they deal with language features rarely used by programmers(the declaration of an assignment operator in abstract classes and repeated inheritance, respectively).



we have presented a tool for structural coding rule validation where rules for c++ are formally defined by means of a declarative rule definition language. users can define their own rules and the tool is seamlessly integrated into the work-flow of the developers. basic information about programs is taken from the very same compiler(gcc) used to generate object code, which avoid inconsistencies.



