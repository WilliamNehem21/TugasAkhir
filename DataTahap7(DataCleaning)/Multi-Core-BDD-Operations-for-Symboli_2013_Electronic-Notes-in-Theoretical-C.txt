this paper presents scalable parallel bdd operations for modern multi-core hardware. we aim at increasing the performance of reachability analysis in the context of model checking. existing approaches focus on performing multiple independent bdd operations rather than parallelizing the bdd operations themselves. in the past, attempts at parallelizing bdd operations have been unsuccessful due to communication costs in shared memory.



exponentially. one way to deal with this is to represent all states using boolean functions, instead of storing them individually. this is called symbolic model checking. boolean functions can be stored in memory efficiently using binary decision diagrams(bdds)[1,6].



processor cores. the speedup is a measure for the performance gain of parallelizing an algorithm. if an algorithm with 20 workers is executed 5 times faster than with 1 worker, we say the speedup for 20 workers relative to 1 worker is 5. the ideal speedup in that case would be 20. in this example, the efficiency is 5/20= 25%.



several frameworks implement task-based parallelism, e.g. the compiler-based frameworks cilk and openmp and the library-based framework wool. these frameworks support creating tasks(spawn) and waiting for their completion(sync) to use the results. we selected wool for the parallelization of symbolic reachability for several reasons. according to, wool offers superior scalability in fine-grained task-based parallelism, compared to cilk and openmp. there is also a blog reporting on parallelizing the bdd package buddy using cilk and using wool we expect similar results. finally, it is quite straightforward to implement parallelism using the wool framework.



of course, it can be the case that multiple workers start the same suboperation, as is always the case for the initial task. however, due to the random order of handling suboperations, the workers will quickly branch off to different subtasks. so load balancing depends purely on randomization. for example, if a task has two subtasks, workers start on different subtasks with 50% probability. this increases rapidly with a larger number of subtasks.



traditionally, concurrency conflicts like data races are solved by locks, providing mutual exclusion. since blocked processes must wait, locks have a negative impact on the speedup of parallel programs. recent research has been dedicated to developing non-blocking data structures and algorithms. herlihy and shavit distinguish lock-free algorithms, wait-free algorithms and lockless algorithms. our algorithms fall in the last category. here explicit locks are avoided by using atomic processor instructions like compare and swap.



we encode these values in 32 bits: 15 bits for the hash, 1 bit for the lock, and 16 bits for the reference count. the reference count is prevented from integer overflow by reserving a special value saturated. when the reference count is saturated, it will no longer be increased or decreased.



we experimented with a representative selection of models from the beem database using a symbolic bfs reachability algorithm of dve2-reach from the ltsmin toolset. experiments ran on a 48-core machine, consisting of 4 amd opterontm 6168 processors with 12 cores each. this machine has a numa architecture with 8 memory domains and 6 cores per domain. we first parallelized the bdd operations using work stealing with wool(see section 4.1) by implementing an experimental parallel bdd package sylvan. 2



we also experimented using randomized load balancing(see section 4.2) and report decent performance and scalability elsewhere. the conclusion there is that this alternative approach is viable, but the approach using wool currently gives slightly higher performance and a larger speedup.



performance measurements with this parallel implementation demonstrated relative speedups of up to 32 using 48 cores. compared to the popular bdd package buddy we get a speedup of up to 12 using 48 cores. we demonstrated that parallelizing bdd operations on a low level is a viable method to get good speedups for symbolic reachability on multi-core multi-processors with a non-uniform shared-memory architecture.



