another method to animate an avatar is known as blend shapes or morph targets. in this approach, an animator deforms(morphs) the geometry of the mesh itself, vertex by vertex, by hand, from an initial to a final state. the animation runs interpolating intermediate positions of vertexes. the main advantage of this method is that the animator is free to morph the geometry as he/she sees fit without restrictions. in other words, he is not limited by the structure of a rig. for this reason, some animators prefer this method to generate facial animations, though it is more computationally expensive than rigging.



candide was introduced initially in 1987 by rydfalk. an update to this model was proposed in 2001 by ahlberg. candide is a parametric computer graphical model of human face that was originally proposed to perform face coding in the mpeg-4 standard. the model was inspired by the work of ekman, who identified a set of micro facial expressions that are performed by any person as natural expression of some emotions[7,8]. in this way, the candide model implements a set of action units(au), each one aiming to copy a ekman micro expression. the current model leaves out 10 micro expression be due to restrictions in the polygons resolution of the model. an advantage of candide model is in its inspiration, because the ekman work shows that the set of micro expressions is an indicator of human emotions[7,8]. for this reason, a subset of candide aus may reproduce a specific emotion.



avatar retargeting process works over the part of the mesh that corresponds to the face, for this reason, it is necessary to verify that avatar face/head accessories, e.g., glasses, piercings or hats, of the avatar are independent 3d meshes that not will obstruct the process. if the avatar face is not a clean 3d mesh(without accessories), it is not possible to continue the process.



a 3d artist often packs an avatar with more than one textures map to maintain resolution, but this can be circumvented if the textures map are packed on one image of high resolution that kept desired quality. in order to remove the hassle of working with multiple texture images for the character, it is necessary to have an avatar that only has one texture image assigned. in the case of these happen also is necessary a rebake process to get a unique textures map.



nation coordinate for each vertex of the face, respectively. with this, we are able to move a vertex through 3d space from and to where it is necessary, by modifying the parameter t. expressions are then generated by moving different groups of vertices at the same time to different positions. these instructions can be read from an xml file and as such, adding or modifying expressions on the fly is possible, since the xml file can contain which vertices need to be moved to create a facial expression, as well as where they need to be moved to in 3d space. simply adding or modifying this xml file will make animations modifiable.



affirmations that the experts where shown in the questionnaire. each one of them related to a category. the experts responded with strongly disagree(graded with 1), disagree(graded with 2), borderline agree(graded with 3), agree(graded with 4), and strongly agree(graded with 5).



in this work, a workflow for generating avatars able to reproduce emotional expressions in runtime was proposed. it is based on the retargeting of a predesigned avatar by replacement of its original face with the parametric mask(candide), which avoid performing 3d animation processes every time. candide model was here used because it is inspired by the ekaman emotional model, which makes possible to generate many facial expressions, from a simple wink to more complex expressions such as happiness, sadness and anger. the proposed workflow was implemented to represent emotional states on facial expressions into 6 available avatars licensed under the creative commons copyright.



modification of the avatar face, which is produced by facial replacement. to evaluate the effect of these modifications, a group of six digital artists develops a subjective comparison of each original and retargeting avatar. details about the way in which each avatar was generated were hidden to the experts, in order to avoid any bias. the experts indicated that they observed differences between the two avatars, but in all cases, these variation were considered weak(less than 3 in a scale of 1 to 5), which allows to conclude that the retargeting process generates an avatar very similar to the original.



