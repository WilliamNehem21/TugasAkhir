for four emotive states, facial recognition 89% for seven states and speech recognition 80.46% for happiness and sadness. looking forward, heart-related parameters might be an option to measure emotions accurately and unobtrusive with the help of smart wearables. this can be used in dynamic or outdoor tasks. facial recognition on the other hand is a useful contact-less tool when it comes to emotion recognition during computer interaction.



late depending on the users emotion for a successful joint action. not only humancomputer-interaction would profit from emotion recognition. it would also help in the field of psychology to identify patients emotions who are unable to express their feelings. for example patients with autism spectrum disorder or patients diagnosed with the locked-in-syndrome could benefit from emotion recognition. furthermore, the health care sector is increasingly dependent on technological applications and devices. patients interacting with a virtual avatar, which is able to read and tune according to the users emotions could improve their motivation. for rehabilitative applications this could even lead to a faster and higher recovery success and improves the quality of life. emotions are transmitted in all modalities of human communication like words, tone of voice, facial expression, body language as well as several bio-parameters(as a reaction of the autonomic nervous systems) like heart rate variability, skin conductance etc. mostly all of these modalities can be measured with different technologies and sensors, since with developing technology the possibilities for automated emotion recognition have been improved as well. these methods vary in their potential regarding which emotions can be detected, their accuracy, the options for validating the results as well as their usability under different circumstances.



the bayesian network method. for the same data different classifier result in different accuracy. the majority of studies uses the svm or the fisher linear discriminant classifier. depending on the training set it needs to be tested which of them results in higher accuracy.



emotion detection from visible parameters such as facial expression, gesture or speech is influenced by the subjects culture, age and gender. in some cultures expressions like anger or grief are considered dishonorable and are discouraged, leading the participant to replace their feeling with a fake smile. outward physical expressions rely on the manipulation of social masking and lead to falsified emotion classification. the underlying emotional state may only be classified by measuring internal parameters. the noldus facereader software states to work on this issue by classifying so called action units, as described in section 3.2. multimodal systems might also help to overcome this problem of fake emotions. the ans reacts according to a persons emotional state and can not be tricked as easily. however, the interpretation of a single parameter comes along with side-effects, such



several bio-parameters, elicited by the activity of the ans can be used to acquire data for emotion recognition. their usability depends on the field of application. systems vary in accuracy, distinction between the number of recognized emotions and mobility during the measurement. the different measurement methods and their benefits and limitations are described in the following.



terns are a sign of negative valence and arousal, where shallow and rapid respiration suggests concentration or fear. depressive emotions are connected to shallow and slow respiration patterns. a project from 2017 utilized respiration patterns to recognize emotions based on deep learning algorithms with accuracy for valence and arousal of 73.06% and 80.78% respectively, based on the deap data set.



skt can be used to identify if a person is relaxed or not. measured on the finger-tip, dilated vessels will make the tip warmer during relaxation or colder during stress or anxiety when the vessels constrict. similar to recognition with eda, skt sensors could be implemented into a glove. a study from 2012 used such a smart-glove and elicited emotions with video-clips taken from movies or public databases. for feature extraction, the skin temperature was converted into an electrical signal. the measured arousal was categorized into five states, which then were translated into emotions. the study stated, that positive states are easier to recognize than negative ones. an other study from min woo park et al. differentiated within sadness and happiness and from analyzing skt they reached 89.29% classification accuracy.



some studies break down their data reasonably, while others are less transparent, making comparison a difficult task. for example the study mentioned in section 3.7 measuring skt on the finger tips divided their extracted features into linear and non-linear data. the difference for the accuracy of the two data sets was enormous, which is why an average over the two data sets was calculated, combining all extracted features into one statement. furthermore, the elicitation method plays a crucial role. randomly selected participants might have a disadvantage over trained ables such as smart watches or smart fabrics, it is possible to measure ecg, eda, skt, bvp and rsp data in a mobile and unobtrusive way. the empatica e4 wristband, empatica inc., cambridge, ma has developed a novel method to measure ppg and eda from the wrist. methods like fr and sr solely depend on background information(lightning, noise). with smart wearables being an upcoming trend in modern society previous stationary methods might be utilized more during everyday activities and unobtrusive emotion recognition.



