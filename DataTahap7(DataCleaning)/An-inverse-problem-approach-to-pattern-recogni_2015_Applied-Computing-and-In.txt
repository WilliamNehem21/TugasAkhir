with the advent of high-speed computers and artificial intelligence techniques, this modeling problem underwent a metamorphosis and emerged as a machine learning problem(bauer et al., 2007; gdawiec and domanska, 2011). tikhonov and lanweber regularized that learning algorithms have recently received an increasing interest due to both theoretical and computational motivations(abrukov et al., 2006; kurkova, 2012; tiknonov and arsenin, 1977). fractal, optimization, and a two-dimensional functional relational model have been used as a feature in several pattern recognition methods(chang et al., 2010; lo gerfo et al., 2008; noureddine, in press). considerable attention is currently being devoted to new possibilities of using artificial neural networks(ann) in view of their increasing importance for solving the problem of automated reconstruction of the inner structure of an object. accompanying algorithms that effectively quantify uncertainties, deal with ill-posedness, and fully take the nonlinear model into account are needed therefore, it is necessary to both look for possible ways to improve the classical learning algorithms already existent in the literature, and to identify new methods which can compete with the traditional ones in speed, robustness, and quality of results.



the rest of the paper is organized as follows: the next section describes our model and justifies its use. in section 3, we formulate the proposed regularized learning algorithm. section iv presents main simulation results. we compare our regularization-based algorithm(rba) with the support vector machine(svm) and semanteme-based support vector machine(ssvm) in section 5. finally, we conclude the paper with a summary of the work in section vi.



in this section, misclassification rate(li and wang, 2009) is used to evaluate the efficiency of our algorithm. misclassification rate refers to the ratio of the number of misclassified exemplars to the total number of exemplars in the dataset. the ratio is computed using the formulation(5.3). correspondingly, the classification accuracy is determined by the formulation(5.2).



a number of classification algorithms depend on the similarity or dissimilarity of exemplars, such as euclidean distance and inner production, among others. however, majority of these algorithms only process continuous-attributed data, not discontinuous data. discontinuous data(surface in our examples) are extreme, having disordered and unbalanced distribution.



introduce a new learning algorithm. on one front, improvements have to be done both on the algorithm(different regularizer properties must be investigated) and the applications(non-homogeneous 3-d object recognition). more detailed work is needed to improve the effectiveness of the numerics in general. in addition, the answer to exactly how sensitive the method is to moderate amounts of noise is an open question.



