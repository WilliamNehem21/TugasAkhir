the reuse manager is composed of three modules: the query preprocessor, query expander, and the indexer. the first module, query preprocessor, is composed in turn of three components: tokenizer, in charge of separating terms; pos tagging, which detects different word types; and entity recognizer, which identifies named entities. the second module, query expander, is in charge of semantically expanding, where each of its components represents a different expansion strategy. currently, the module is implemented with three strategies that are respectively based on the use of lexical relationships, the use of a database of word co-occurrence, and the recognition of named entities. finally, the indexer takes the products generated by the previous modules and uses them to explore the structure defined by the definition manager and using the taxonomy manager.



it is important to consider that a given word can have different meanings depending on the context or the way it is used. therefore, the first task is tagging the words according to the way they were used by the query expressed in natural language. to do so, we used a variation of the tagger introduced in(part-of-speech) and we tagged the words according to the 36 categories defined by the penn treebank project 1, which include different types of nouns, verbs, conjunctions, adverbs, etc.



after tagging, we apply the ner technique for recognizing named entities(ner 2) to improve tokenization. a named entity is a word or a set of words that constitutes an entity able to be classified into predefined classes. these classes include location, person, organization, money, percent, date and time. therefore, we apply ner, as it is presented in, to refine the initial list of words. those entities conformed by more than one word are grouped again and tagged with its corresponding named entity class. coming back to our example, the process would identify three entities, calin rovinescu, air canada and montreal, tagging them as person, organization, and location respectively. it makes that the words air and canada are not expanded separately, avoiding meaningless information with respect to the original query.



firstly, we start tokenizing the query in such a way that each word can be treated separately. then, the list of tokens should be filtered by eliminating symbols, letters, words, and other unwanted or useless elements from the semantic point of view. these elements are called stopwords. now, following our example and considering that during the entity recognition calin rovinescu and air canada were detected as entities, the vector of terms would be[used, calin rovinescu, president,inside, air canada,...]. we can note that the words when, by, the, were eliminated since they were classified as stopwords.



our framework is currently implemented as a working prototype, which uses a taxonomy in a given a format and any set of functionalities described as functional datasheets. in this context, we have built a service taxonomy for the geographic domain, following the standard iso 19119 7. we also built and indexed a set of functionalities that form the spl.



indexers, we used the indexing engines lucene 10, sphinx 11 and minion 12. in this way, we look to evaluate the behavior of the three expansion methods despite the chosen indexer. in addition, we wished to compare the expansion methods against the indexers without any type of semantical expansion, which gave a us a total of four elements to compare for each indexer: no semantical expansion, expansion using wordnet, expansion using disco and expansion using ner. also, it is important to highlight that for each indexer we took the 10 most relevant results into consideration.



to understand the obtained results, it is important to describe the reference value for each metric. for the precision-at-n metric, values vary from 0 to 1 for each value of n, where n is the position in the result list. from the total of results for each combination, we observed how many correct results where at position 1, at position 2, and so forth. since we used cumulative precision, there is a value of n from which precision becomes 1, this position on the list represents the value for which all relevant results where already retrieved. in the worst case scenario, this value corresponds with the total of elements. for the recall metric, the value goes from 0 to 1 and it is measured with respect to a window of elements from the retrieved list(in our case the 10 first results). hence, a recall value of 0 represents the case where none of the elements that should have been retrieved was actually retrieved; and 1 the case where all of the elements were retrieved.



after establishing these ranges of values, it becomes possible to observe several particularities on the graphics. on the one side, recall is significantly low for(less than 0.5) for the non expansion column, meaning that half of the functionalities that should have been retrieved, were not. from the techniques that use disco and wordnet, we can observe a clear superiority for disco, above all of the indexers. finally, it is imporant to note that



analyzing both metrhics we can observe that semantical expansion techniques present a significant increment in the correct retrieval of functionalities compared to the same process without expansion. also, cumulative precision at n reaches 100% for lists of 10 candidates in all cases. we can observe a pretty marked diference in the recall metric between all techniques that spreads across all indexers, being disco the one that offers the best values followed by wordnet and followed by ner.



there are at least two main threats to validity. on the one hand the dataset could be much larger in order to obtain better quantitative results. on the other hand, experiments were realized over a single domain, which might expose a correlation between the expansion techniques and the application domain. thus, the lower values of recall for the ner expansion technique might be due to this particular domain, but could perform significantly better in another context.



