in order to determine the pain intensity level accurately, researchers start to automate the pain intensity recognition based on several factors like facial expressions and biopotential signals. facial expressions are recorded for subjects while experiencing pain through a controlled experiment. biopotential signals are simply electronic signals produced by the electrochemical activities of a cell type during physiological processes that occur in the body, they are measured by attaching specific type of sensors to the skin called an electrode. pain recognition automation can be done using either data or both of them. in this work, we are going to use biopotential signals dataset containing four levels of pain; the signals are obtained from electromyography, skin conductance level, and electrocardiogram, the dataset is described in details in section 3. pain intensity levels can be classified using prediction algorithms which feeds on data, as it is known the more data given to the algorithm, the more effective it can be. medical problems suffer from the lack of data because of its sensitivity, therefore, the



this research is divided into five sections. section 1 displays the introduction to the research problem and methodology. section 2 presents some of the previous work done in the field. section 3, explains the generative adversarial networks(gans) framework, section 4, presents the methodology of this work. section 5 provides an overview of the dataset used in this work and section 6, presents the experiments and results.



the problem with this kind of measures is its dependence on patient awareness, communication, and experience of pain. thus, it will not work on cases like traumas and infants. therefore, more reliable ways of measuring the pain level have to be developed, such as automatic pain intensity recognition.



in 2017, lopez-martinez and picard r. introduce an approach containing two-stage learning for the 10 levels vas automatic estimation. the first stage is using recurrent neural network(rnn) for prkachin and solomon pain intensity(pspi) score estimation which is another pain intensity measurement from face images, which are fed to the next stage. the second stage takes the pspi score for each person for vas estimation using hidden conditional random fields(hcrfs); the model is personalized using a unique score for each person from his facial expressiveness. the dataset used contains 25 subjects suffering from shoulder pain; their faces were recorded while doing movements in both the affected and unaffected arms. for evaluation purposes, the dataset was split into train and test sets, after training the model on the training model, the intra class correlation(icc) and the mean absolute error(mae) were used as evaluation measures. moreover, the results of the two algorithms were compared with the support vector regression(svr) as a baseline method.



zhou j. presented framework used recurrent convolutional neural network(rcnn) for automatic frame-level pain intensity estimation. first, the faces images were wrapped to the same frontal pose from different poses using active appearance model(aam). second, since extracting features from each frame in the video separately have its limitation in describing dynamic information from previous frames, features were extracted from adjacent frames using a sliding-window strategy to allow using historical frames while keeping a fixed-length input. finally, the recurrent convolutional neural network architecture was designed for continuous-valued pain intensity, by modifying the loss and activation function in the last layer of the network. at each iteration in the training process, a subset of the training set containing all pspi levels with the same percentage was taken. as for the evaluation phase, following the leave-one-subject-out strategy, the average mean squared error(mse) and product-moment correlation coefficient(pcc) were calculated, the experiments showed promising results compared to other approaches on the same dataset.



thiam p., and schwenker f. presented a personalized pain recognition system depends on a hierarchical fusion architecture to take advantage of all the extracted features, where dimensionality reduction is applied on each set of features first, then each set of the remaining features are sent to the classification process, which consists of three layers, first, each subset is fed to the random forest classifier, second, the results are sent to a pseudo-inverse mapping and a multi-layer perceptron(mlp) mapping, finally, using both of the scores of the second layer, a pseudo-inverse mapping produce the final label. the model was personalized using the hausdorff distance as a similarity metric, to select samples from participants from the training set that are similar to an unseen participant. the evaluation of this model follows the leave-oneparticipant-out strategy, the results outperform the baseline methods using 30 participants.



lopez-martinez, 2018,, presented an approach to handle the pain intensity recognition as a regression problem, where two types of recurrent neural network(rnn) architectures are used, the first architecture is a traditional fully-connected rnn which is able to capture temporal dependencies since the output is fed back to the input. the second architecture is a long shortterm memory neural network(lstm-nn) which learns the longterm dependencies. the experiments showed that the results are obtained using the skin conductance features outperforms other baseline methods, using three evaluations metrics; mean absolute error(mae), root mean square error(rmse), and coefficient of determination(r2).



chu y., in this research, the authors focused on the feature processing, since the dataset used is novel, after extracting features from the raw signals, then the extracted features are reduced using genetic algorithm(ga), to remove redundant and irrelevant features, then applied principal component analysis(pca), to transform the features into linearly uncorrelated space. three types of classification including linear discriminant analysis(lda), knearest neighbour(knn) algorithm, and support vector machine(svm) were applied and evaluated for single-signal datasets, multi-signal datasets, multi-subject datasets and multi-day datasets.



tested using the same classification architecture. moreover, grid search was used to find the most optimal parameters. the experiments results evaluation followed the 10-fold stratified crossvalidation for each subject. the best overall performance was obtained by using all the video and biomedical features.



thiam p., proposed an approach to find which modality from both video and signal features give the best result in the pain intensity recognition problem in person independent setting. each modality was used to train a regression model based on random forest, and in the end, all of the modalities were used together in an early fusion model. the used evaluation metrics are mean absolute error(mae), and root mean squared error(rmse), experiments showed that skin conductance level(scl) give the best results on its own, while the electrocardiogram(ecg) has the worst performance.



a. s. f. thiam p., in this work, a trimodal of audio, video, and signals were used for the pain intensity recognition problem, by doing experiments on several fusion architectures, to take the full advantage of the diversity of the extracted features. both early and late fusion were applied, in the early fusion, all the extracted features is fed to a single random forest classifier. as for the late fusion, two architectures were presented, first, late fusion(a) in which the extracted features belonging to the same channel(audio, video, signals) are concatenated, then fed to the random forest classifier. second, late fusion(b), where each set of extracted features belonging to one modality is fed to a single random forest classifier, both architectures were mapped using the mean and linear discriminant analysis(lda) to decide the output. the evaluation of the fusion architectures showed that user-specific classification outperforms independent settings classification.



kachele m. proposed personalizing pain intensity recognition systems using different techniques with different information sources, the idea behind penalization is to estimate the pain level by identifying the most similar subjects in training set to the new test subject. the used techniques to find the similarities between subjects belongs to three main groups; first, meta-information, which is general information about subjects like age and gender. second, distance-based measures, the proposed techniques belonging to this group are k-nearest neighbor and hausdorff distance. finally, machine learning-based algorithm, this group is divided into measures based on supervised learning, measures based on unsupervised machine learning and proxy classification.



xie q., in this work, a data augmentation method is applied to unlabeled data in a semi-supervised learning setting; this method called unsupervised data augmentation(uda). uda make the model more consistent while training the real unlabeled data, and the augmented unlabeled data, instead of using random noise, uda use more realistic noise generated by previous data augmentation methods, and minimise the kl divergence between the predictions on the real data, and the predictions on the augmented data.



tran t., in this paper, the authors proposed using a novel bayesian formulation for data augmentation, by treating the new data as missing data points that are sampled from the distribution of the given annotated data points. moreover, both the process of the generator distribution and the classification model are trained jointly, and this method showed improvement in the result.



lim s., 2019. in this paper, an algorithm called fast autoaugment is proposed bayesian data augmentation motivates that, fast autoaugment treats augmented data as missing data points, and are recovered by the exploitation-and-exploration of a family of inference-time augmentations, this search is optimized by bayesian algorithm. experiments using several datasets showed that the search time is speeding up than autoaugment, and the error rate was improved.



generative models have existed for a long time; even though the generative models have many benefits, they have not attracted much attention, due to the lack of the needed computational power, and the complication of implementing the structure of such models. the importance of the generative models exceeds the limitation of generating fake samples. they can be useful in a variety of applications such as handling missing data, like, which trained an image completion network to fill the missing parts of images. another usage is enhancing data quality such as in, where blurry images have been enhanced to a clear highresolution image using the generative model.



decision-rule based algorithm originally used in game theory, where two players are playing a turn-based game, to maximize the current player gain, while minimizing the opposite player gain at the same time. in turn-based games, each player has no control on the other player moves, which arises the need of using adversary methods. adversarial training is a discipline of machine learning used when another model or an optimization algorithm enters the worst case input for the model. in the minimax algorithm, each player aims to make the worst possible scenario for his opponent. an evaluation function determines the gain in each game, each player makes the best possible move, in which his evaluation score is maximized and the opposite player score is minimized. an optimal solution for two optimal players would be finding a point where the current player maximum gains are the same as the



gans was first introduced by goodfellow et al. in 2014, the basic idea is to simultaneously train two deep neural networks by allowing them to play a game together. the first network is called the generator(g), responsible for generating fake sample by learning the distribution of the training samples. the second network is the discriminator(d), which is responsible for evaluating the quality of the generated samples by applying traditional supervised learning, to distinguish real from fake samples.



a model that combines energy-based gans with auto-encoders was proposed by, where the discriminator is viewed as an energy function. the energy function is a mapping function of each input point with a scalar value called the energy; the lower energy value is the better. the discriminator energy function can be viewed as the generator cost function, since the good generated sample is given low energy value, in contrast with the bad sample which is given high energy value. in energy-based gans the discriminator aims to give the fake samples high energy values, while the generator aims to generate fake sample in the regions where



feature selection is the process of selecting the most relevant subset of features from the complete set of features in the dataset without losing important information, by discarding the irrelevant or less helpful features in solving the problem at hand. relevant features hold the most useful information for the predictive model while the irrelevant features are redundant, noisy or completely useless features. the feature selection process is considered very important since it reduces the memory storage, training time, computational cost and increases the performance of the predictive model.



feature selection is categorized into three main classes: first, the wrapper methods, that considers feature selection as a search problem, where it tries different combinations of features and evaluate the quality of each combination using a predictive algorithm. second, the filter methods, which deals with feature selection as a pre-processing step, where it uses statistical methods to associate each feature with a score, rank them based on this score, and then use the features with the highest scores in the predictive algorithm. finally, the embedded methods, where the feature selection process is integrated with the prediction algorithm as a part of the learning phase, in each iteration the model learn which combination of features improves the prediction algorithm performance.



third, after doing the experiments using only the real data, we start the data augmentation process using lsgans. the implementation of the lsgans was done using the rivalgan library, which is written using tensorflow library in python. rivalgan library was implemented to generate numerical data using different types of gans. the library was customized to suits the pain intensity dataset.



