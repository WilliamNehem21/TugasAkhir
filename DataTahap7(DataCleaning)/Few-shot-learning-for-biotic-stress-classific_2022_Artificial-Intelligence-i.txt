plants are often exposed to environmental stresses that can cause a vast impact on growth and production. plant stresses can be biotic and abiotic. biotic stresses are caused by pathogens such as fungi, bacteria, viruses. they can also be caused by pests. on the other hand, abiotic stresses are caused by heat, drought, cold, salinity, among others (suzuki et al., 2014).



classifying plant biotic stresses and estimating their severity is a complex task due to several factors: lack of precise delimitation of the lesion area; variation of characteristics between lesions of the same type (such as color, shape, and size); the presence of multiples lesions on the same leaf; and the fact that different stresses may have similar symptoms (barbedo, 2016). in the last few years, deep learning tech- niques have been presenting promising results in image classification and recognition (goodfellow et al., 2016). these techniques have even been used successfully in the classification of plant diseases (boulent et al., 2019), emerging as a useful tool to assist farmers and agriculture professionals.



however, traditional machine learning and deep learning tech- niques present some limitations, among them is the need for large- scale datasets to the models generalize well (wang et al., 2020). so, new methods were proposed aiming to create models that generalized well with fewer data. this led to the proposal of meta-learning methods (hospedales et al., 2020), and in particular, the few-shot learning methods.



as pointed out, there are several works in the literature that applied deep neural networks for plant disease recognition. most of them make use of traditional convolutional neural networks in different crops, obtaining accuracy values over 90% (mohanty et al., 2016; rahman et al., 2018; elhassouny and smarandache, 2019; geetharamani and pandian, 2019; barbedo, 2019).



in this work, we explore the use of few-shot learning and feature extraction for plant stress recognition and severity estimation. to experiment, we will use a dataset of biotic stresses in coffee leaves. the remainder of this paper is organized as follows. section 2 pre- sents some fundamental concepts on deep learning and few-shot learning. moreover, we also describe the methods triplet network and prototypical network used in this work. section 3 presents the experimental setup and results. finally, in section 4, we draw some conclusions.



in the pooling layer, the feature maps from the convolution layer are spatially reduced by a pooling (also known as downsampling) opera- tion. the pooling operation statistically summarizes regions from the activation maps, reducing their size while maintaining their main fea- tures (goodfellow et al., 2016). two common ways of performing the pooling operation are maximum and average polling. in the maximum pooling, we summarize the set of values by its higher value. on the other hand, in the average pooling, we summarize the set of values by calculating the average value.



the fully connected layer consists of a typical feedforward net- work, also known as multilayer perceptrons (mlps) (goodfellow et al., 2016). they usually act as the classifier in the cnns, using the features extracted by the previous layers as input. in the con- text of cnns, the inputs fed to a fully connected network are the feature maps from the convolution and pooling layers. since the feature maps are tensors and the fully connected network receives a vector as input, a flatten operator is applied to the maps, returning a vector.



a cnn architecture can also be used without a fully connected layer, returning the flattened vector as output. this vector is sometimes re- ferred to as an embedding. this kind of architecture that produces em- beddings is extensively used in the few-shot learning context, as presented in the next section.



one of the main shortcomings in machine and deep learning is the need of large-scale data to properly generalize (wang et al., 2020). to address this issue, the meta-learning paradigm was proposed (hospedales et al., 2020), and, in particular, the few-shot learning methods (wang et al., 2020).



most fsl methods proposed are for supervised learning problems such as image classification and object recognition. although not ex- plored in this work, one of the main uses of few-shot learning is learning to generalize to new classes unseen during training (wang et al., 2020; hospedales et al., 2020; chen et al., 2020).



address the few-shot learning problem by learning to fine tune. there are two main approaches to this. one of them attempts to learn good model initialization (i.e. parameters of the network) and the other fo- cuses on learning the optimizer. both approaches diminish the need for data or the number of gradient steps to train the network.



address the few-shot learning problem by learning to compare. these methods attempt to decrease the need for data by training a model to learn the similarity between images, instead of training the model to sim- ply classify the input image. intuitively, if a model can distinguish two dif- ferent images, it can learn to classify unseen classes with few data. these methods are also known as embedding learning methods.



in our work, we focus on distance metric learning-based, also known as embedding learning, methods. this class of methods was chosen due to the promising results in image classification tasks. in the next section, we provide more insight and definitions about this class of methods.



specifically, in the image classification task, the embedding function is usually a cnn without the fully connected layer, since the flattened vector is a lower-dimensional representation of the input data. we may also refer to the embedding function as feature extractor. the sim- ilarity function is usually a distance function like the euclidean or cosine distance, depending on the model used.



the triplet network (tripletnet) (schroff et al., 2015) consists of a few-shot learning model that uses three images as input (thus the name triplet): anchor, positive, and negative. the anchor consists of the image that we want to classify. next, the positive consists of an image that is from the same class as the anchor. lastly, the negative consists of an image from another class (that is not the anchor's class). the ob- jective is to produce a representation of the inputs, so that the anchor is similar to the positive and different from the negative.



(koch et al., 2015), triplet network (schroff et al., 2015), matching net- works (vinyals et al., 2016), prototypical network (snell et al., 2017), and relation network (sung et al., 2018). in our work, we only used the triplet network and prototypical network. the prototypical network was selected because, despite being a relatively straightforward method, it still achieves competitive performance when compared to more



embedding for each example xi from class n. after computing the prototypes, the embedded query sample f(x) is compared with the prototypes using a distance function. unlike the tripletnet, the protonet can produce an output without the addition of another classifier. given the distance function d, the protonet can produce a distribution over classes for a query x based on a softmax over distances to the prototypes (snell et al., 2017):



in this section, we present and diwescuss the experimental results ob- tained by the few-shot learning methods triplet network (schroff et al., 2015) and prototypical network (snell et al., 2017) in the context of plant stresses classification and severity estimation. besides using the tripletnet, and protonet we also evaluated several backbones (feature extractors) in each method to assess the contrast among different feature extractors. first, section 3.1 presents the datasets used in the experiments. next, section 3.2 presents the experimental setup used. section 3.3 presents and discusses the results obtained by the methods used in the experiments.



the datasets used in this work were developed by krohling et al. (2019). the data consists of images of arabica coffee leaves affected by common biotic stresses. the images were captured using smartphones. the datasets are divided in two sets: leaf dataset and symptoms dataset. the data used is available for download in github.1



the experiments performed were divided in three: experiment i, ex- periment ii, and experiment iii. experiment i consists of the biotic stress classification in the leaf dataset. experiment ii consists of the biotic stress classification in the symptoms dataset. finally, experiment iii consists of the severity estimation in the leaf dataset.



in order to evaluate the models' performance we computed the fol- lowing metrics: accuracy (acc), precision (pr), recall (re), and f1-score (f1). since the metrics precision and recall are only defined for binary classification, we computed the macro average over the five classes. we also computed the average training time per epoch.



the experiments were carried out in the google colab3 environment with 16gb ram and a tesla t4 gpu. all the code was written in python, with the help of several open-source libraries: pytorch (paszke et al., 2019), learn2learn (arnold et al., 2020), numpy (harris et al., 2020), seaborn (waskom, 2021), matplotlib (hunter, 2007), and scikit-learn (pedregosa et al., 2011). the source code used in these experiments is available in github.4



the best result reported by esgario et al. (2020) in the biotic stress classification task using the leaf dataset was 95.63%, using a resnet50 as classifier. this result is similar to ours, and the 5-way 5-shot protonet with mobilenetv2 backbone even slightly outperformed it, with an ac- curacy of 96.03%.



interesting thing to note is that the results obtained in the biotic stress classification task were better in the symptoms dataset than the ones obtained in the leaf dataset. this is in agreement with those provided by esgario et al. (2020) and barbedo (2019). since the images in the symptoms datasets focus only on the lesion itself, the network has an easier task of detecting the region of interest. for example, in the leaf dataset, the networks have to deal with background and non-injured areas in the leaf, which makes the classification task harder.



the best result reported by esgario et al. (2020) in the biotic stress classification task using the symptoms dataset was 97.07%, using a resnet50 as a classifier. this result is similar to ours (96.72%). this dif- ference is about 0.35% and it may be due training variation.



the best result reported by esgario et al. (2020) in the severity esti- mation task using the leaf dataset was 86.51%, using a resnet50 as a classifier. in this particular task, the few-shot learning methods achieved a better result, with an accuracy of 93.25%.



