fork-join and split-merge queueing systems are mathematical abstractions of parallel task processing systems in which entering tasks are split into n subtasks which are served by a set of heterogeneous servers. the original task is considered completed once all the subtasks associated with it have been serviced. performance of split-merge and fork-join systems are often quantified with respect to two metrics: task response time and subtask dispersion. recent research effort has been focused on ways to reduce subtask dispersion, or the product of task response time and subtask dispersion, by applying delays to selected subtasks. such delays may be pre-computed statically, or varied dynamically. dynamic in our context refers to the ability to vary the delay applied to a subtask according to the state of the system, at any time before the service of that subtask has begun. we assume that subtasks in service cannot be preempted.



a key dynamic optimisation that benefits both metrics of interest is to remove delays on any subtask with a sibling that has already completed service. this paper incorporates such a policy into existing methods for computing optimal subtask delays in split-merge and fork-join systems. in the context of two case studies, we show that doing so affects the optimal delays computed, and leads to improved subtask dispersion values when compared with existing techniques. indeed, in some cases, it turns out to be beneficial to initially postpone the processing of non-bottleneck subtasks until the bottleneck subtask has completed service.



split-merge and fork-systems are parallel queueing network abstractions which describe task flow and processing in parallel networks. in such systems incoming tasks are split into a number of subtasks, each of which must be served before the whole task can be regarded as completed. two primary performance metrics are of interest in such systems. task response time is the time it takes from the point when a task enters the system to the point where all of the subtasks have been fully serviced. response time has been a very intensively researched topic in queueing systems over the last 50 or so years, see e.g.[2,8,9]. subtask dispersion is the difference in time between the completion of the first and last subtask. subtask dispersion has not received much attention in the literature until recently, see



in dynamic delay systems, it is beneficial in terms of both task response time and subtask dispersion to remove delays on any subtask that has a sibling that has already completed service. this paper defines a new way to calculate subtask delays in split-merge and fork-join systems that is able to incorporate this optimisation. we begin by exploring 2-server split-merge systems with deterministic and exponential service to offer some intuition behind our technique. we then proceed to a 3-server test case to demonstrate that our technique is able to deliver substantial reduction in subtask dispersion compared to existing methods.



this section contains a brief introduction to terms that are fundamental to the understanding of this paper. the section includes an introduction to split-merge and fork-join systems. both systems are queueing network models for describing the processing of a set of subtasks in parallel. in addition it describes related quantitative metrics, including response time, subtask dispersion and a trade-off metric. the trade-off metric can be used to make decisions when both subtask



when a task enters service it is split into n subtasks. each of these subtasks is then processed by its own server. the service time of each server is characterised by a probability distribution. the task is considered to be done with service once all n subtasks have been serviced by their respective servers.



task response time is the length of time it takes for a task to get processed. the clock is started when the task first enters the system. once all the subtasks belonging to that task have been completely serviced the clock stops.



this section introduces a way to calculate subtask dispersion in split-merge systems where a start work signal is sent to sibling subtasks once service of a subtask is completed. the model assumes that a delay applied to a subtask can be arbitrarily preempted at any point before service of the subtask has begun. however, once a subtask has begun service it will be serviced uninterrupted until it completes service. the start work signal is sent because removing delays after the first sibling subtask finishes service reduces both subtask dispersion and task response time.



gj(t, tj, dj) represents the probability distribution function of the service time of the jth server. if tj< dj then the jth server has not yet begun service of its subtask and servicing is begun immediately. otherwise the server has already started servicing its subtask. in this case the service time is renormalised to take into account that some service has already been performed and the service of the subtask has not been completed. the two part g-function is the key difference compared to previous work presented in[14,15,13].



the methods described next are described in section 3 of this paper. they use interruptions to start processing of sibling tasks once a subtask finishes. the results have been calculated with the same simulation framework as the fork-join systems in the previous subsection.



the improvement in subtask dispersion, however, comes at a cost to task response time. method 6 is better in terms of subtask dispersion compared to the old dispersion minimisation technique method 2, but it has a significantly higher response time when compared to the trade-off technique(method 3) and vanilla technique(method 1). however some of these issues can be corrected by method 7 which is the fork-join version of method 6. method 7 does have a slightly increased subtask dispersion when compared to method 6. however, task response time is still higher than found in the two other fork-join systems(methods 4 and 5).



