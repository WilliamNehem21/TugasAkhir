in this paper, a novel multi-module neural network system named mmnns is proposed to solve the imbalance problem in electrocardiogram(ecg) heartbeats classification. four submodules are designed to construct the system: preprocessing, imbalance problem processing, feature extraction and classification. imbalance problem processing module mainly introduces three methods: blsm, ctfm and 2pt, which are proposed from three aspects of resampling, data feature and algorithm respectively. blsm is used to synthesize virtual samples linearly around the minority samples. ctfm consists of dae-based feature extraction part and qrs-based feature selection part, in which selected features and complete features are applied to determine the heartbeat class simultaneously. the processed data are fed into a convolutional neural network(cnn) by applying 2pt to train and fine-tune. mmnns is trained on mit-bih arrhythmia database following aami standard, using intra-patient and inter-patient scheme, especially the latter which is strongly recommended. the comparisons with several state-of-the-art methods using standard criteria on three datasets demonstrate the superiority of mmnns for improving detection of heartbeats and addressing imbalance in ecg heartbeats classification.



a cad system automatically classifies heartbeats, which contains four steps of ecg signal preprocessing, heartbeat segmentation, feature extraction and learning/classification. traditional methods first extract features from the original data, such as p-qrs-t complex features, statistical features, morphological features, wavelet features, etc.(acharya et al., 2015; khandoker, palaniswami,& kar-



four submodules are designed to construct the whole system, of which imbalance problem processing module, the core part, introduces three methods to eliminate the negative effect of imbalanced data distribution, named borderline-smote(blsm), context-feature module(ctfm) and two-phase training(2pt). these methods are from the views of resampling, data feature and algorithm respectively. to the best of our knowledge, this is the first try to combine such three modules together to overcome imbalance problem.



mmnns is trained on two data division schemes of intrapatient and inter-patient simultaneously, especially the latter which is more convincing but has not been adopted by most scholars. to validate the effectiveness of our model, extensive experiments are carried out on three datasets using seven classification assessment metrics and two statistical measures. besides, a large number of comparisons have been made between mmnns and state-of-the-arts.



as the great success in image recognition, speech recognition, natural language processing and other fields, deep learning model has been gradually applied to ecg analysis in recent years. taking convolutional neural network(cnn) as an example, cnn integrates feature extraction and classification, classifying highlevel features automatically learned by itself from original data. li, zhang, zhang, and wei(2017) feed the raw ecg signals directly into a 5-layer cnn for feature extraction and training. the idea of transfer learning is adopted in isin and ozdalili(2017) to take pre-trained alexnet as a feature extractor, and the extracted features are then fed into a simple bp neural network to classify the heartbeats. rahhal et al.(2016) uses active learning technology to improve the performance of the system. zhai and tin(2018) and golrizkhatami and acan(2018) process the input of the final classifier respectively, the former transforms the beats into dual beat couple matrix as 2-d inputs of cnn, and the latter fuses the 2dconvolutional and handcrafted features extracted from each beat. the accuracy of deep learning model is shown higher than that of the traditional classifier combined with manual feature extraction. although the aforementioned methods have achieved considerable classification accuracy, the evaluation metric of accuracy is incomprehensive when training data is skewed. most of these papers fail to pay attention to the adverse effects of imbalance problem yet. as a result, solving the imbalance problem reasonably according to the characteristics of ecg data is the key to improve the



convolutional neural networks(cnns) refer to the neural networks that use convolution operations at least one layer of the network instead of general matrix multiplication, specifically for processing data with grid-like topology(ian goodfellow, 2016). it integrates feature extraction and classification together, different from the traditional classifier which needs to input pre-extracted features. a typical layer in a cnn consists of a convolutional layer and a pooling layer. feature maps of previous layer are computed with several convolutions in parallel to generate a set of linear activation responses. then, they passed through a nonlinear activation function to generate a feature map for next layer. finally, the pooling function is used to further adjust the output. cnns are stacked with several typical layers, which can be used to extract high-level features.



be removed by median filter effectively. as in de et al.(2004), we use a 200 ms width median filter to remove p wave and qrs complex wave, then use a 600 ms width median filter to remove t wave, and afterwards subtract the filtered signal from the original signal to get the baseline correction signal. the 12-tap low-pass



besides, performance measures are essential for effectiveness evaluations and guidance of learning. as stated at the beginning of this section, accuracy, a commonly used metric, favors majority class and easily leads to misleading assessment. metrics like precision, specificity, sensitivity, g-mean, f-measure are introduced into imbalanced learning field. some works even focus on novel evaluation metrics, such as adjusted f-measure(maratea, petrosino,& manzo, 2014).



neural networks and deep learning have gained lots of interests recently, also facing the imbalance problem. in standard back propagation algorithm, weights are updated by minimizing overall error, which are contributed mainly from majority class. thus, we achieve biased classification results. to tackle this problem, two novel loss functions named mfe and msfe are proposed in wang et al.(2016). a stacked denoising autoencoder neural networks algorithm based on cost-sensitive oversampling is designed in zhang, gao, song, and jiang(2016). khan, hayat, bennamoun, sohel, and togneri(2018) present a cost sensitive deep neural network which can automatically learn robust feature representations for both of the majority and minority classes. raj, magg, and wermter(2016) integrate different methods into a novel approach using cost sensitive neural networks to improve the performance on imbalanced datasets.



we design a structure to deal with imbalanced training data combining both data and algorithm level. above all, from data point of view, we apply borderline-smote(blsm) algorithm to one-dimensional ecg time series data, and oversample minority samples by linear synthesis. secondly, considering the features extracted from skewed data are biased towards majority class, a novel context-feature module(ctfm) is introduced in this paper. ctfm integrates feature extraction and feature selection, feeding prominent feature of each heartbeat segmentation and the features in the larger region of its context to the classifier simultaneously. ctfm doubles the number of minority samples, and enhances both the accuracy of model recognition and reliability for feature extraction. finally, we adopt two-phase training, referred to as 2pt. in the first training stage, the convolutional neural network(cnn) is trained with balanced data, and in second stage, the model is finetuned with original skewed data.



data augmentation can be regarded as expanding data by adding similar but different samples in the original training set(zhang, cisse, dauphin,& lopezpaz, 2018). on the basis of this definition, overcoming the imbalance problem in the number of training data can be regarded as local augmentation of minority class in training data. when realizing images classification, we usually obtained augmented data with horizontal reflection, rotation and scaling. inspired by this idea, we plan to add virtual samples



smote resamples the training data by synthesizing each minority sample and its random selected neighbor. since the number of original samples in each class located at the boundaries is small, the newly synthetic samples are mainly distributed in the non-boundary regions. however, the samples near the borderline, also referred as dangerous samples, are more likely to be misclassified than those far away from borderline, and are of more significance to classification tasks. therefore, blsm, an improved version of smote, only oversamples the minority examples near the borderline, which is more in line with our expectations.



in blsm, minority class examples which are easily misclassified will get more training. minority examples on the borderline are first found, and then synthesized with their selected k nearest neighbors. blsm algorithm is modified slightly and adapted to one-dimensional ecg time series data in this paper. the detailed process is shown as follows.



what needs to be stressed is that the number of samples varies greatly from one class to another. if a complete balance is achieved by blsm, it may cause the boundary between different classes to become blurred, which makes it easier to misclassify. therefore, we synthesize the minority class to make they reach only half of the number of majority class.



feature extraction, as another way to deal with dimensionality, converts the original features into new feature set. when solving imbalance problem, extracted features are often biased to predict the majority class samples that lead to poor performance on classification. taking principal component analysis(pca) as an example,(braytee, liu,& kennedy, 2016) point out that pca algorithm seeks the orthogonal feature extractors that maximize the total variance. as a result, the extracted features favor majority class because their number is larger than the minority class. moreover, other feature extraction methods will encounter the same problem. accordingly, improved feature extraction algorithms are proposed recently to adapt to imbalanced training data(moepya, akhoury,& nelwamondo, 2015; ng, zeng, zhang, yeung,& pedrycz, 2016).



in the present study, we construct a ctfm module consisting of two parts: dae-based feature extraction part(part i) and qrs-based feature selection part(part ii). in part i, we choose dae as our extractor to extract sparse high-level features from all the heartbeats obtained in section 3.2.1. on the one hand, the



in this paper, training of the algorithm is done in 800 epochs with a mini batch size of 200. validation set is used to validate the model each 100 rounds of training. after training cnn with processed balanced data, the model is fine-tuned according to the method described in 3.2.3. the final performance of the testing set is measured by eight metrics defined below.



mit-bih arrhythmia database consists of 48 records slightly longer than 30 min of two-channel ecg recordings with digitization rate of 360 hz. the signals of the two channels were recorded by placing the electrodes at different angles on the chest. the upper signal is a modified limb lead ii(mlii), and the lower signal is a modified lead v1, v2, or v5(moody& mark, 2002). ecg data were collected from 47 subjects, including 25 men aged 32-89 and 22 women aged 23-89(records 201 and 202 are from the same male subject). more than 100,900 heart beats in the database have been annotated. only one channel(channel mlii) for each recording is used for the classification task, excluding four records with poor quality(102,104,107,217).



71. each record lasts for two hours, and two signals contained are both sampled at 250 samples per second. two cardiologists annotated each record beat by beat and for changes in st segment and t-wave morphology, rhythm and signal quality.



only one channel of both two datasets is selected to conduct the experiments. the heartbeats obtained from these two datasets are fed into the model which we have already trained before. test results are used for further comparison and evaluation from different aspects.



patient. this may easily lead to biased results due to the heartbeats from same patient have strong correlation and dependence. in the inter-patient scheme, all records are divided into two groups with similar number and similar proportions of each class. heartbeats in ds1 are all from records: 101, 106, 108, 109, 112, 114, 115,



228, 231, 232, 233 and 234. the heartbeats of two groups will not come from the same subjects. in the intra-patient scheme, 49,953 heartbeats(n44800, s1345, v3412, f396) are selected from the total heartbeats as the training set and the rest as the testing set. in



area under the receiver operating characteristic curve(auc) is a sound measure of discrimination and has been widely used as an evaluation metric for classifiers, especially in imbalanced learning. we use a multi-class modification of basic version of roc. the multi-class auc(mauc) is calculated by taking the average of aucs obtained independently for each class for the binary classification task of distinguishing a given class from all the other.



the first algorithm named pcnn aims to feed the original imbalanced data directly into a pure cnn model without any process. algorithm smcnn and bscnn preprocess the original data by oversampling them using smote and blsm respectively. on the basis of bscnn, the classifier cnn is replaced by a cascaded structure of dae and cnn in algorithm bsdc, which feeds oversampled training data to a dae at first and then to a cnn. the fifth algorithm is bscp, which integrates 2pt to bscnn. unlike bscnn, which only fine-tunes the model using balanced training data after oversampling, bscp introduces an extra fine-tuning phase using the original imbalanced data. the last one is our method: mmnns.



