in our earlier research we developed a signal analysis method for nystagmic eye movements investigated in otoneurological tests. for the automatic analysis of such signals poor or invalid nystagmic eye movements should correctly be separated from valid nystagmic eye movements, because valid eye movements can only be used for the data analysis needed for the diagnostics of otoneurological patients. typically, invalid nystagmic eye movements are corrupted by noise or artefacts. thus, we have also studied the classification of nystagmic eye movement candidates into invalid and valid, hereafter called the rejected and accepted, on the basis of machine learning methods. we then observed how their complicated distribution made the classification task difficult and attempted to reduce the greater subset(class) of the rejected eye movement candidates in a learning set, which was performed by cleaning away a part from them. surprisingly, a simple cleaning process impaired classification results of some of the machine learning methods applied. we realized that the reason for such a seemingly conflicting situation originated from the complicated variable distribution of the data.



in our original cleaning procedure we assumed that there would be two classes in the data having their class centres in different areas. this assumption was reasonable in the sense that the poor nystagmic beat candidates were marked to be rejected(manually or automatically) typically since some of their variable values were above some upper bounds. for instance, segments



after the manual selection of nystagmic beat candidates there were 2171 accepted and 3818 rejected beats. after the automatic selection these numbers were 2517 and 3472, and after using the both ways jointly 1645 and 4344, respectively. thus, after cleaning, i.e., reducing the larger class of the rejected beats, the numbers were 2171, 2517 and 1645 for each of the classes in these three situations.



the automatic selection was better in some cases than the manual and automatic ones together. apparently, this stemmed from the fact that manual selection criteria may vary a little from time to time. instead, the automatic selection always functions stably.



the use of greater nearest neighbour numbers(5, 7, 9 or 11) than 3 originally used improved the classification results slightly further since more elements were cleaned out from the majority class compared to the situation of 3 nearest neighbours. however, no such conclusion could be drawn that this phenomenon would typically be present. after all, the properties of data and their distribution are essential.



