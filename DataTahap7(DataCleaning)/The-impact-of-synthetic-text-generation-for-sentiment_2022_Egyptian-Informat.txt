the rest of the paper is organised as below. section 2 presents relevant literature work focused on text generation and sentiment analysis. section 3 depicts methodology along with information regarding datasets, preprocessing techniques, text generation model architectures and evaluation metrics. section 4 discusses the major results from selected text generation models and sentiment analysis experiments on original as well as balanced datasets. finally, section 5 shows conclusion and future work.



ing bleu score calculations. the black dotted line is the division line marking the pre-training and adversarial training process. before the black dotted line, it is the pre-training part where the initial generator was trained. the field after the black dot line is the adversarial training part where the pre-trained generator gets strengthened training aiming to improve the quality and diversity of generated text again.



are used as depicted in section 3.4. first, we will discuss the results with respect to bleu scores with n= 2; 3; 4; 5 of both sentigan and catgan for cr23k and cr100k datasets, respectively. the higher the bleu score, the generated text is more similar to the



these analysis again indicate that sentigan has a gradient vanishing problem on both datasets cr23k and cr100k. this causes sentigan to lose the ability of generating diverse text and can only generate the same sentence every time. based on the above results and analysis on both datasets, catgan is selected to synthesize/generate reviews, which are then used to balance the original datasets.



2.79% and 9.208% for cr23k and cr100k dataset, respectively. in the future, the ongoing work can be extended by exploiting different types of complicated text generation models like gpt-3 and more complex sentiment analysis models in order to have better and more generalized models.



