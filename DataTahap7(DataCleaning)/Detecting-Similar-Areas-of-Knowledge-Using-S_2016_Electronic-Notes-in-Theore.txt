searching for scientific publications online is an essential task for researchers working on a certain topic. however, the extremely large amount of scientific publications found in the web turns the process of finding a publication into a very difficult task whereas, locating peers interested in collaborating on a specific topic or reviewing literature is even more challenging. in this paper, we propose a novel architecture to join multiple bibliographic sources, with the aim of identifying common research areas and potential collaboration networks, through a combination of ontologies, vocabularies, and linked data technologies for enriching a base data model. furthermore, we implement a prototype to provide a centralized repository with bibliographic sources and to find similar knowledge areas using data mining techniques in the domain of ecuadorian researchers community.



the number of publications is rapidly increasing through online resources such as search engines and digital libraries, making more challenging for researchers to pursue a topic, review literature, track research history because the amount of information obtained is too extensive. moreover, most of the academic literature is noisy and disorganized. currently, certain information about researchers and their bibliographic resources are scattered among various digital repositories, text files or bibliographic databases.



when you need to propose projects with several researchers in a specific area belonging to different higher education institutions(hei), different questions arise. for instance, who works in similar areas of research? or, how can i create a network of researchers in a common knowledge area? then, detecting similar areas based on the keywords it could help governments and hei to detect researchers with interests in common, opening an opportunity to generate new research projects and allocate efforts and resources to them. in that case, we could detect potential collaboration networks.



the expansion of this knowledge base will allow our academic community to have a centralized digital repository which has information of ecuadorian researchers based in bibliographic resources. the collaborators are identified through a semantic enrichment of scientific articles produced by researchers who publish with ecuadorian affiliations. this work aims to encourage institutions to collaborate and obtain a semantic repository to identify researchers working in similar areas and, provide updated information accessible and reusable. enhancing the generation of research networks with academic peers in the region could provide a greater opportunity for collaboration between the participating institutions.



obviously, there are many tools and services currently available in the web which already provide a wide variety of functionalities to support the exploration of academic data. each tool or service operates in different ways, that in some cases complicate the literature review or utilization data. these tools or services allow search publications using keywords, author names, conferences, authors affiliations through applications programming interface(apis). they have started using semantic technologies that helps to describe their resources, but each source is different. our approach use these characteristics, to retrieve and enrich bibliographic data from several bibliographic sources to detect similar areas.



the rest of this paper is organized in the following way: section 2 presents the related work. we outline the architecture in section 3, detecting similar areas in the domain of ecuadorian researchers and detecting potential networks of collaboration, using semantic technologies to enrich data extracted from different bibliographic sources in a common model. conclusions and future work are presented in section 4.



each bibliographic source have data that can be duplicated or inconsistent. in our case it is necessary to correct ambiguous data before it is stored. in, there are two methods of disambiguation of authors, the first one uses the names of the authors and their initials, and the second one is an advanced method that uses initial names and authors affiliation. in, presents a framework that uses a clustering method dbscan to identify the author according to their articles. we analyse the similarity between sets of publications of different authors. if the similarity between these resources is found, the correct author to a specific publication is established. in proposed the rexplore system, which uses statistical analysis, semantic technologies, and visual analytics to provide scholarly research data and locate research areas. we use a similar idea, but we will dynamically add new data sources to improve authors information. a similar work is made by, which detect potential collaborative networks through the semantic enrichment of scientific articles. however this work has authors from a single source and ecuadorian affiliation only; while we can present external information when it is needed from various sources. they find similar papers using skos 7 concepts, while we used data mining algo-



in the field of geoscience studies it has shown that is posible to improve data retrieval, reuse, and integrate data repositories through the use of ontologies. for instance in, the geolink project, a part of earthcube 8, integrates seven repositories using ontology design patterns(odps) defined manually. they have a set of odps as the overall scheme, rather than using a monolithic ontology. to obtain data they executed federated queries. conversely, in our proposal all sources form a single repository and we do not use federated queries because the response time is endless. the data model geolink is defined specifically for geodata, which differs from our proposal covering several domains according to the bibliographic source.



previous studies finding a relationship between publications have shown that citation data is often used as an indicator of relatedness. citations are used to measure the impact of documents. nevertheless, there are other approaches to find related papers, the work of shows that digital records can be used as indicators as well. collaborative filtering could be used to find related publications too; in the work of they use the citation web between publications to create the rating matrix and recommend research papers. additionally, relationships based on the citations gives an insight of the hierarchy distribution of publications around a given topic as shown by. although citations are an excellent indicator to express relatedness, we could not find work in the literature to use keywords as



the different data sources represent repositories that contains information about authors and scientific publications of different areas. the sources about authors are distributed in different dspace 10 repositories located in various hei, and those records belong only to ecuadorian authors. each repository contains scientific papers, theses, dissertations, books, monographs of researchers or students.



the scientific publications are extracted from bibliographic sources such as microsoft academics, google scholar, dblp and scopus that make available their data via apis. the data vary in their content due to each source has a different structure. also, the access to the data is restricted in some cases, for example, in scopus we can make a maximum of 5000 querys for each ip, then the source locks the access for seven days. moreover, the sources of publications have not the same fields, for example, scopus has the following fields: data affiliation of authors, tables, graphs of publications, authors study areas, but dblp or microsoft academics do not have these fields. therefore, we see that, it is necessary to make a unification of these variety of data models in a common model that describe literature from different domains stored in a central repository.



the data extraction module is responsible for extracting and describing bibliographic data from several sources using semantics technologies and linked data practices. the data extracted is analyzed in order to define a structure using the documentation available on the source and if it does not exist, the data model of the source is analyzed using web scraping techniques. after that, the model of data is established, the data is extracted and stored on a triple store, in this case apache marmotta 11. some sources have their data recorded with a bibliographical ontology defined by the source owner. if the data already is annotated then it is stored directly on the triple store. otherwise, this data is annotated and stored with bibo ontology. we use the bibliographic data sources to cover different scenarios and find the main problems involved in the process of the extraction and enrichment of bibliographic resources. every time a new source is added, we analyze manually the data model and then extract the data. these two processes are encapsulated into components described below.



the component retrieves authors and publications using different apis, web pages or sparql endpoints from different bibliographical sources. this component is designed abstractly, with the aim to extract information from any bibliographic source. listing 1 and 2 illustrates data responses from microsoft academics and dblp, and those responses have a different format and structure despite being in the same publication. to extract data we use the ldclient 13 library from apache marmotta that offers several forms to consume xml data from web services or web pages such as google scholar which does not have an api. the data is processed in memory using a temporary triple store called sesame 14 which adds information about authors and bibliographical sources that later use to discard erroneous information. finally, the data is stored in the apache marmotta triple store.



some sources do not have tools that allow access to data, it affects the quality of the data in the repository because the results need to be complemented and cleaned. the response from google scholar illustrated in listing 3 has fewer fields with respect to the response from other sources illustrated in listing 1 and 2, although it is the same publication. if a source do not have an api that allows access to the data, this can affect the consistency of the information in scientific publications. to resolve this problem, we use string metric algorithms such as cosine similarity and levenshtein described in to determine the correct value of a publication field. the correct value of a field is the one most repeated among all values from different sources. for example, we have the next values for a title of a publication from each data source:[linked sensor data] from dblp,[linked sensor data] from scopus,[publishing linked sensor data] from google scholar,[linked sensor data] from microsoft academics. we determine with this component that value[linked sensor data] is the correct value for title, because it is the most common among all values extracted titles.



some publications has duplicated entities because these are extracted from several data sources. also in some cases is ambiguous to determine publications of an author when they have similar names. so the data must be processed before to be stored, it is detailed in the 3.3 section.



the module of data enrichment unifies all data of publications and authors in a central repository using bibo ontology. we find characteristics between publications and authors, assigning correspondences between the data model of the source and the common model that we have defined, through a component of mapping ontology model. we have various entities of the same author or publication and this represent a problem of inconsistency. for this reason, we have a component called data disambiguation that solve this problem.



data in the central repository is stored using a model of storage based in graphs. we have defined a graph for each data source(providers graph), a graph for authors(author graph) and a central graph(wkhuska graph) that stores unified information of publications and authors. to make the unification of publications and authors, the data must be analyzed previously to establish correspondence and eliminate duplication.



listing 4 illustrates the publication described using bibo ontology. it is the same publication illustrated in the listing 1 and 2, but enriched with data from different sources into a common data model. the publications are stored in a central repository. however, it is a problem to identify the correct author of a publication if there are multiple authors with the same or similar names. these problem is solved in the component data disambiguation.



of the data. the most reliable source is scopus, because it is the most consistent to searches, for example searches such as juan pablo carvallo vega and juan pablo carvallo ochoa, identify in some cases the difference. other data sources such as dblp does not keep complete records of the author and only use the first name and last name, causing scientific publications are assigned to other authors.



from a specific area and colleagues with similar interests, who could be interested in working together. the module has three components to detect patterns from the data collected. first, all the data is taken from the central repository, it is used in the find groups component to detect some patterns in the data set. the knowledge discovery component is used to extract knowledge from the associated groups. to speed up queries and have organized groups, each group is labeled in the label groups component. finally, results are stored in the central repository for further queries.



we use the vectors generated to execute k-means algorithm in mahout. it was executed using a cosine distance measure as similarity measure. to seed the initial centroids, we use randomseedgenerator, which is used to generate random centroids in mahout. the experiment has 100 iterations as maximum and the number of clusters(k) varies according to the amount of data extracted from the different bibliographic sources, because when new publications or authors are extracted the k must be adjusted. once the algorithm is executed we have our vsm clustered, where each vector belongs to a cluster. in order to make this information readable, we process again this results as you will see next.



we have many keywords that belong to complex domains and hand-label each cluster it is a tedious and expensive task. keywords help us to handle searches effectively and search engines could increase performance in searches by finding a general topic area based on the words that belong to a cluster. labeling clusters help to respond to specific queries(i.e.: show all researchers working in a specific area or all subareas belonging to a general topic area).



lications was enriched. we use wordnet 22 to find synonyms, hypernyms, hyponyms and the concept of a word for all keywords in each cluster. that helps to find a common meaning in the way that words could occur together and find similar meanings. in other words, with the group of words set up, we could find a concept or a topic for each cluster.



