topic detection is a process for determining topics from a collection of textual data. one of the topic detection methods is clustering based, which assumes that the centroids are topics. the clustering method has the advantage that it can process data with negative representations. therefore, the clustering method allows a combination with a broader-representation learning method. in this paper, we adopt deep learning for topic back into the original representation to be interpreted as the topics. our simulation shows that deep autoencoder- based fuzzy c-means improves the coherence score of eigenspace-based fuzzy c-means and is comparable to the leading standard methods, i.e., nonnegative matrix factorization or latent dirichlet allocation.



[4]. firstly, the textual data are transformed into a lower-dimensional eigenspace using singular value decomposition (svd). next, k-means is performed on the eigenspace to extract the topics that are then trans- formed back to the nonnegative subspace of the original space.



topics. therefore, soft clustering is examined to be an alternative clus- tering method for topic detection. fuzzy c-means (fcm) is one of the famous soft clustering methods [5]. using fcm, the textual data may belong to more than one cluster and may have more than one topic. the combination of fcm and lsa called eigenspace-based fuzzy c-means (efcm) is proposed for topic detection [6]. in general, some simulations show that efcm gives coherence scores between those of lda and nmf



currently, deep learning is the primary machine learning method for unstructured data such as images and text [11,12]. deep learning has been extensively studied to extract a good representation of data by neural networks [13]. in this paper, we adopt deep learning to improve the performance of efcm for topic prediction problems by using deep autoencoder (dae) for the representation learning process. we call this topic detection method deep autoencoder-based fuzzy c-means (dfcm). first, the encoder of dae performs a lower-dimensional representation learning. next, fcm groups the lower-dimensional representation to identify the centroids. finally, the decoder of dae transforms the cen- troids back into the original representation to provide the topics. our simulation shows that dfcm improves the coherence score of efcm and is comparable to the leading standard methods, i.e., nmf or lda.



in the era of big data, the existence of high-dimensional data is a big challenge for fcm [24]. by finding a new representation of the original data, two approaches are already used to reduce the difficulty of fcm in high-dimensional data. the first approach uses kernel methods to implicitly get more expressive features by formulating the data into the feature space constructed by some kernel functions [25,26]. the second approach is an explicit transformation of the original data. in addition to the specified nonlinear data transformations [27], random projection been extensively studied to extract a good representation of data by neural networks [13]. combining the deep neural network and an un- supervised clustering method has also become an active research field [30]. in general, there are several approaches to combining deep learning and clustering. the first approach combines representation learning and clustering in two steps: using a dae for representation learning and a clustering method for the next stage [30,31]. the second approach combines a dae and a clustering method simultaneously [32, 33]. the following approach combines clustering with a pre-trained encoder, such as bidirectional encoder representations from trans- formers proposed by google [34]. however, most of the clustering methods used in these approaches are hard clustering. few studies work on the improvement of feature quality by deep learning for fuzzy clus- tering. this study aims to find a good deep representation for fuzzy



this two-step optimization is iterated until a stopping criterion is fulfilled, e.g., the maximum number of iterations, insignificant changes in the objective function j, the membership mik, or the centroids qi [35]. the fcm algorithm is described in more detail in algorithm 1.



let a be a word by document matrix and c be the number of topics. given a and c, the topic detection problem is how to recover c topics from a. in the clustering-based topic detection method, the clustering centers or centroids are interpreted as topics. in this section, we describe where dropout1() and dropout2() are methods to ignore some number of neuron outputs during training randomly, g1() and g2() are activation mize the loss l (x, w), i.e., errors between xi and yi. next, hi becomes a functions, and w1 and w1 are weights. the fitting is performed to mini-



contain a small number of topics. however, a large f implies that each textual datum may contain more topics. we initialize the centroids of fcm for both efcm and dfcm using the best of the 10-run k-means clustering. in dfcm, we use commonly used parameters for daes and avoid dataset-specific tuning as much as possible. specifically, we set symmetrical network dimensions to d-500-500-2000-p for all datasets, where d is the data-space dimension and p is the lower dimension of 10 or 5, as previously implemented [33,39]. we implement this represen- tation learning using python-based keras.1 on the other hand, we use truncatedsvd implementation of scikit-learn for dimension reduction in efcm [40]. finally, we need to tune the parameters, i.e., the lower dimension p and the number of topics c, for efcm and dfcm.



sional representation. compared to the five-dimensional representation, the mean coherence scores of dfcm fluctuate only slightly as the number of epochs increases from 100 to 400 and 700. the dfcm gives the mean coherence scores of 0.1896 for 400 epochs. like the five- dimensional data representation, dfcm still provides a better mean coherence score than efcm, which is about 5% better.



scores of 0.3730, 0.3588, 0.3560, and 0.2815, respectively. these results indicate that dfcm achieves a better average coherence score than efcm, nmf, and lda. however, nmf still gives a better coherence score for the smallest number of topics, i.e., 10. in this berita dataset, nmf and efcm provide almost the same mean coherence scores.



in the previous sub-chapter, the simulations show that dfcm ach- ieves better mean coherence scores than efcm. for the enron dataset, dfcm provides mean coherence scores that are 7% better than the efcm and 4% more than the efcm for the news dataset. the main difference between these two methods is the lower-dimensional repre- sentation learning process for which dfcm uses dae, while efcm uses truncatedsvd. the dae and truncatedsvd produce different lower- dimensional representations. the truncatedsvd creates a lower- dimensional representation with orthogonal dimensions or features, but dae produces lower-dimensional representations with dimensions or features that are not orthogonal. topics generally consist of words that are not necessarily orthogonal, especially in their meaning. also, dae implements denoising processes implicitly to produce these lower- dimensional representations. thus, each of these lower-dimensional characteristics will more or less affect the resulting mean coherence scores.



compared to nmf, dfcm also provides a higher average coherence score. the dfcm achieved a 3% better average coherence score for the enron dataset and a 5% better average coherence score for the news dataset. in contrast to efcm, dfcm and nmf provide lower- dimensional representations with non-orthogonal dimensions or fea- tures. the nmf carried out a topic extraction process in the original space, which consisted of words. thus, the resulting topics can be directly interpreted, and their coherence scores calculated. meanwhile, dfcm extracts the topics in the lower-dimensional space and must be transformed back to the original space so that the extracted topics can be interpreted and coherence scores calculated. however, dfcm makes it possible to process a better representation for textual data that generally have a lot of noise and variation. thus, success of dfcm in achieving better coherence scores is mainly because dfcm processes textual data with better representations. the same condition for lda, where lda performs the topic extraction process in the original space, consists of words.



deep learning is currently a popular supervised learning approach, especially for unstructured data such as images and text. deep learning integrates the feature extraction process with classification or regression processes. in the context of unsupervised learning, dae is a popular deep learning method for representation learning. this method allows per- forming the denoising process while reducing dimensions to produce better lower-dimensional representation. however, integration of rep- resentation learning methods with an unsupervised learning problem such as topic detection is still an opportunity to be developed.



dfcm is a topic detection method that combines dae for represen- tation learning and fuzzy c-means for topic extraction. therefore, dfcm allows processing a better representation for textual data, which generally have a lot of noise and variation. unlike efcm, dfcm extracts topics from lower-dimensional representations with dimensions or fea- tures that are not orthogonal. this representation is more realistic to represent the topics. our simulation shows that dfcm gives a higher accuracy in terms of the coherence score than efcm and the two stan- dard methods of nmf and lda.



