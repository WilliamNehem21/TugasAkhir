abstract finite mixture models is a pattern recognition technique that is used for fitting complex data distributions. parameters of this mixture models are usually determined via the expectation maximization(em) algorithm. a modified version of the em algorithm is proposed earlier to handle data sets with missing values. this algorithm is affected by the occurrence of outliers in the data, the overlap among classes in the data space and the bias in generating the data from its classes. in addition, it only works well when the missing value rate is low. in this paper, a new algorithm is proposed to overcome these problems. a comparison study shows the superiority of the new algorithm over the modified em algorithm and other algorithms commonly used in the literature.



tions of the non-parametric methods. parameters of fmm are usually estimated by the expectation maximization(em) algorithm. it is shown that the em algorithm can estimate parameters of a multivariate normal distribution from a data set with missing values. the em algorithm is modified to estimate parameters of a mixture of multivariate normal distri-



rithm is used in clustering data sets that contain missing values and a number of categorical features[6,7]. also, it is used in learning parameters of the radial basis function networks used in classifying data sets that contain missing values. when there is sufficiently large number of observed values, this algorithm outperforms the em algorithm combined with either the unconditional mean imputation[9,10], or the conditional mean imputation, of the missing values according to the classification performance of the resulting fmm. also, this algorithm outperforms the em algorithm combined with deleting feature vectors that contain missing values, according to the classification performance of the resulting fmm.



however, the outcome of the modified em algorithm is influenced by several modelling assumptions such as the number of components and the probability distribution function of each component[11,12]. for example, the modified em algorithm produces an inaccurate estimation of both fmm parameters and missing values in the input data set when initialised with an fmm that has too few components, each of which has a probability distribution function that does not fit well any of the clusters of the input data set. in addition, this algorithm has a poor performance when the size of the data set is small. it is shown that better results can be obtained by imputing missing values using the distribution of the input feature vectors rather than using a priori probability distribution function used in the fmm. the modified em algorithm fore there is no assurance that the error in estimating the regression value for any new point will be small. similar algorithms to the grnn are used to train neural networks, in which the hidden neurons use gaussian density functions, using data sets that contain missing values[16,17]. these algorithms are used for regression and classification and they exhibit better performances than neural networks that are trained using either only the complete portion of the input data set or the total data set after estimating its missing values using the unconditional mean imputation method.



tains n feature vectors each of which is a vector in d-feature space such that x=[x; x;...; x; y]t, where the first(d1)-features are complete and the d-th feature contains missing



are sensitive to the prior information about density functions of mixture components and the size of data that are fully observed. a supervised classification method, called robust mixture discriminant analysis(rmda), is proposed to deal with label noised data. the proposed algorithm uses only fully observed data to learn mixture model parameters and then uses this mixture model to estimate labels and detect noisy ones. however, imputations made by this algorithm are sensitive to prior information about density functions of mixture components, the size of data that are fully observed and assumptions such as all the uncertain labels are in one feature. in this paper, a new algorithm is proposed to overcome problems of the modified em algorithm[5,13,14]. the proposed algorithm is less sensitive to the modelling assumptions than the modified em algorithm in estimating missing values. in addition, it is less affected by learning problems than the em algorithm in cases such as the occurrence of outliers in



the general regression neural network(referred to as grnn in the rest of this paper) implements a non-parametric algorithm for density estimation and general(non-linear) regression. this algorithm has a better regression performance than conventional non-linear regression techniques, particularly with a sparse data set that has a small number of feature vectors and a large number of features. this is because the regression surface resulting from the grnn is defined everywhere in the data space. for example, when the input data set is sparsely distributed, the smoothing parameter(window size) used in the grnn can be large in order to estimate the regression value at any new point with similar accuracy to the other points. on the other hand, conventional non-linear regression techniques tend to overfit this data set, and therebetween xi and xj, and r is the smoothing parameter for the d-th feature. to identify the optimum value of r, the grnn uses the leave-one-out cross validation method. the optimum r is then used in imputing all the missing values on the d-th feature.



the em algorithm is modified to estimate fmm parameters from a data set with missing values. this algorithm is referred to as the mem algorithm in this paper. in this algorithm, the missing values in the input data set and some other statistics are estimated in the e-step from the parameters of each model component. these values together with the observed values in the data set are then used in the m-step to estimate the fmm parameters. this means that in the e-step multiple imputations are obtained for each missing value in the input data set from the different components in the fmm and then these imputations are used in the m-step to eral methods of data normalization for cluster analysis has shown the superiority of the linear scaling method over many other normalization methods before applying clustering algorithms in terms of the resulting cluster separation and error condition[22,23]. in addition, determine the optimum smoothing parameter r for each incomplete feature in the data set using the leave-one-out cross validation method. the group of fully observed features used in determining r for a certain feature consists of both the complete features and the features that have smaller missing rates than this feature. the feature vectors that are used in the leave-one-out cross validation method should be observed in the entire fully observed feature group.



data set. each one of these gaussian distributions has a covariance matrix r= i4, where i4 is the identity matrix of order four. the difference between both data sets is the correlations between data features. in the first data set, all features are strongly correlated, while in the second data set all features are too weakly correlated.



feature vectors represent three classes each of which has 50 feature vectors belonging to it. two of these classes are overlapping in the data space. correlations between different pairs of features of this data set are moderate. when each one of these five data sets is used, the missing values are put in the third and in the fourth data features. in addition, each one of the fmms learned by all algorithms compared in this study consists of three gaussian components that have non-restricted covariance matrices.



the evaluation criterion used in this study to compare the different algorithms is the mis-classification error(mce). it is computed by comparing the clustering results, obtained using bayes decision rule, of the learned fmm with the true classification of the data feature vectors, assuming each class is represented by a component in the fmm. components of the fmm are allocated to different data classes such that the total number of misclassified feature vectors in the data set is minimum. let the number of feature vectors belonging to class i be ni, from which nm feature vectors are not clustered into the component that represents this class in the fmm. then



in general, local tuning of the general regression causes the lgrem algorithm to outperform the grem algorithm in the case of too weak global correlations between different pairs of data features. this means that the lgrem algorithm is insensitive to global correlations between features of the input data set. in addition, this feature causes the lgrem algorithm to outperform the mem algorithm in cases such as the occurrence of some outlier feature vectors or the overlap among data classes in the data space. this conclusion agrees with the results shown in regarding the preference of the imputation techniques that use the pair-wise relations between data features to those techniques that do not. finally, when the sizes of the data classes are largely different(i.e., unbalanced data) the lgrem algorithm is superior over the other algorithms compared with as it produces the most accurate and unbiased estimation of the missing values and the parameters of the fmm, which is used for clustering the input data set. this is because the imputation of the missing values in the lgrem algorithm is both local and independent of fmm parameters.



in this paper, the grem and the mem algorithms are analysed and their strengths and weaknesses are explained. this analysis leads to the proposal of the lgrem algorithm to overcome problems of both algorithms. a comparison study shows the superiority of the lgrem algorithm over several algorithms commonly used in the literature including the mem algorithm and the grem algorithm in unsupervised learning of fmm parameters using data sets with missing values. the lgrem algorithm produces the most accurate and unbiased estimation of the missing values and the best estimation of the fmm parameters. this is shown clearly when few outliers occurs in the data set, data classes are overlapped in the data space, or when data classes have large differences in their sizes.



