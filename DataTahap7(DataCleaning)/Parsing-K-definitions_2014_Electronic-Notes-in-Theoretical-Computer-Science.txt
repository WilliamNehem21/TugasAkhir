this paper describes our approach in parsing a k definition. the difficulty of the problem is given by the nature of the k-framework, where the user can define the operational semantics of a language by inserting pieces of concrete syntax in the k code in a natural way. our main contribution shows how to make use of sdf and the disambiguation mechanisms in the context of k-framework.



k-maude, the current tool supporting k, proved to be quite scalable and applicable to real world programming languages such as scheme, verilog, java 1.4 and c(with others underway). however, since k-maude relies on the maude parser to parse k definitions, there are cases when new definitions introduce ambiguities. moreover, in order to be translated in maude, these definitions need to be syntactically correct; therefore it is preferable to have a parser able to handle k definitions. the design of such a parser is an intricate task because the k definitions are quite complex, combining k syntactical constructs with fragments of syntax from the defined language. after several experiments we quickly ruled out parsers that use a scanner before the parser because they do not offer the generality of parsing different styles of programs embedded into one another. the solution came in the form of sdf and its scannerless generalized parser. we generate several parsers for different purposes: one to extract the syntax declarations from a definition, another one is used to parse programs, and the last is used to parse the semantic rules(this is the most complex as it must handle constructors from two languages at the same time).



this paper is divided into 6 sections where in section 2 we introduce a representative example to demonstrate the main problem that we are trying to solve. following in section 3 we present an abstract view of the chosen solution as a way of dealing with ambiguities in languages that embed other languages. section 4 provides a more in depth view of the technique used and explains some of the details that lead to the chosen method. some tools that have similar solutions are presented in section 5.



variables to values. we initialize them with empty(identity) elements. in general the cells can have one of the five main syntactic categories(or sorts) used in k: k, bag, list, set and map. the sort k is considered special as it can be extended with new syntactical constructs from the defined language and it is mainly used to match on computations.



the back-end of the k tool is designed to accept k definitions represented as a collection of abstract syntactic trees(asts). an ast is the collapsed representation of the parse tree resulted after parsing a text. working with the ast makes handling the input a lot easier as the unnecessary information is eliminated in the parsing process(e.g., white spaces and commentswhich have no semantic meaning to the language). to obtain the ast we need a tool able to parse definitions as above, solve the ambiguities, infer the types for each construct and transform them into pure k definitions(using asts) like the following one:



the current implementation of the k-tool has a front-end that transforms an annotated bnf definition of a language into a maude module which is later used to parse the k rewrite rules. this solution often leads to ambiguities because of mixfix operators and the preregularity property that needs to be satisfied by the ordered sorts. the biggest downfall was the lack of support for solving the ambiguities and conflicts provided by maude.



the embedding of language constructs in the k syntax is done with the help of the sort k. in fact this is the place where the name of the technique comes from. a term of sort k represents a computation which from a parsing perspective is a language construct. for the rewrite rules we also need meta-variables, which can take the place of non-terminals in syntactic constructs. in practice, putting all of these together is tricky because we want to offer the user easy access to the concrete syntax but also maintain the sanity of the definition.



the chosen method is similar to the one used in asf+sdf in the sense that we add new transitions to connect the grammar of k and the grammar given by the user. for each non-terminal x defined by the user, we add two new productions: x-> k and k-> x. this technique will allow the flexibility of matching the syntactic constructs in contexts that normally would not be allowed by the type system. the idea is to allow the user to write rules as he/she would write the ast representation, but with the convenience of concrete syntax(mixfix form) which in some cases can be more intuitive than writing the constructor. details about how this is being solved are presented in section 4.2.



the k syntax declarations are a rearrangement of the sdf syntax to be closer to the bnf style. it starts with the keyword syntax, an identifier specifying the sort of the constructs and a list of production rules. the list of production rules can be separated by">" to specify that productions in the left hand side have a higher priority than the ones in the right. inside a priority block there can be specified other productions separated by"|" and will be considered to have the same priority. syntactic productions and blocks can be annotated with the typical sdf disambiguation filters: left, right or non-assoc. also the prefer and avoid non-



the third step from the above list generates a grammar similar to the one for programs. the difference is made at step four, where we add transitions to and from sort k. these will allow the user to insert and replace parts of code from the users language. to stop the parser from entering an infinite cycle, we will limit their use with the priority system(see:).



the second step is the list of constructors and the original sort. for example the exp"*" exp-> exp{left, cons("c1syn")} construct will generate the following term:("c1syn","exp",["exp","exp"]). this information is used in the second type checking filters presented in section 3. the exact details will be presented in the next section.



this step is responsible for connecting all of the steps previously described, thus becoming the most complex one in the front end. firstly, the grammar described in section 4.2 is compiled into a new parser that can recognize the k definition.



the sort of a term is the most general sort that the term can be represented by. this information will be used later to disambiguate the cells in rules. from the ast obtained at this step, the k intermediate language will be generated in the form of xml.



secondly, this section describes the method of parsing the rules. this is the most difficult part to get right because here can be found the most complex combinations of k syntax and user defined syntax. this is why the parser after this step will return an abstract syntax forrest, that will be processed afterwards. the disambiguation filter is composed out of four main steps(introduced in section 3):



one of the newest tools that uses syntax definition and rewrite rules that match on concrete syntax, is rascal. this tool is the continuation of the asf+sdf tool set and is based on similar theory when it comes to syntax definition. the program transformation is a bit different, as it is based on customizing visitors. this solution allows for more control, but loses conciseness.



this work started because of the need to replace the k-maude tool, currently used to parse k definitions. after comparing different parser generators, the most promising solution turned out to be sdf and its generalized parser. having the possibility to write modular grammars, allowed us to integrate easily the two grammars in discussion(the k grammar and the defined language grammar). because the downside of the context-free declarative grammars are ambiguities, special procedures needed to be developed to cope with the nondeterminism of the parsing step(most of these problems are generated because of the integration method).



complexity of a k rewrite rule. the second step is to parse the entire definition and get a parse forest. the last step is necessary to filter the unwanted parsing possibilities. the final result should be a clean ast that represents the intended definition and which can now be used in the next steps of the compilation, towards a rewrite engine.



because sdf has a very good connection with eclipse with the help of spoofax, future work in this direction includes a user friendly interface that will speed up the editing and testing phase. we are also working towards giving more intuitive error messages. this involves working with more permissive grammars that accept a bigger language, but with more complex disambiguation filters can reach the same result.



