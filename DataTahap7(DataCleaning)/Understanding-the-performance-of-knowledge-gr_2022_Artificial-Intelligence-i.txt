knowledge graph embeddings a growing number of approaches have been proposed in the literature which attempt to perform this knowledge graph completion task. in this work we focus upon the family of knowledge graph embedding(kge) techniques[17,18]. typically, a kge model learns a low-dimensional representation of each entity and relation in the graph. these embeddings are combined in various ways to produce a scalar value representing a measure of how



over recent years, there has been increasing interest in machine learning with graph structured data, with approaches created for homogeneous graph embeddings, graph-specific neural models and knowledge graph embedding. whilst there have been numerous new model architectures proposed in the literature, there has been less work performed on understanding how these models are affected by the rest of the choices made in the machine learning pipeline, for example, how robust they are across hyperparameter values and model initialisations, or how performance changes across dataset splits. however, the work that does exist demonstrates some interesting observations.



for example, several graph neural network approaches were compared under a fair evaluation procedure, which showed that a change in train/test split would drastically alter the ranking of the models and that simpler baseline approaches, with correctly tuned hyperparameters, could outperform more complex models. the performance of different graph neural networks for graph-level classification has also been compared, with results showing that baselines approaches not using the graph structure can outperform those that do. the need for consistent, rigorous and reproducible benchmarks for graph machine learning is also an area of increasing research interest[23,24].



evaluation of knowledge graph embeddings a study comparing seven different knowledge graph embedding techniques under a consistent evaluation framework on the non-biomedical benchmark datasets fb15k-237 and wnrr has been performed. the authors observe that as new models are introduced, they are often accompanied with new training regimes or objective functions, making assessing the value of the new model architecture alone challenging. they undertake a detailed comparison across combinations of models, training paradigms and hyperparameters, using a bayesian search approach. they find that earlier and comparatively simpler models, are very competitive when trained using modern techniques. however, the study did not consider how model initialisation or dataset splits can affect performance. in a similar study, 19 knowledge graph embedding approaches, implemented in the pykeen framework, are compared across eight different benchmark datasets. one of the aims of the study was to investigate whether original published results could be reproduced, a task they found challenging. additionally they perform detailed experiments over models and training paradigm combinations, searching over the hyperparameter space for a maximum of 24 h or 100 training repeats. again they find that suitably tuned simple models can out-perform complex ones. the study does not consider drug discovery datasets specifically and does not assess how models perform across



biomedical domain specific evaluations the use of various homogeneous graph embedding techniques has been assessed across a range of biomedical tasks such as drug-drug and protein-protein interactions. whilst not exploring knowledge graph embedding techniques, the work explores how various hyperparameters affect predictive performance. they explore random walk and neural network based techniques including deepwalk and graph convolution based auto-encoders, using various task specific homogeneous graphs. an additional review compares both graph and knowledge graph specific approaches and their use in the biomedical world, however no experimental comparisons are made between the different approaches.



being taken from the biosnap repository. however often these graphs are less complex than resources like hetionet, with a typically limited number of entity and relationship types being present. results use k-fold cross validation to assess performance variability over dataset splits, with a grid-search over a range of hyperparameters also being performed.



the effect of different data splitting strategies for predicting drugdrug interaction using graph-based methods(including transe and transd) has been investigated. the work argues that realistic data splits should be used in order to avoid over-optimistic results, with several domain specific and time-based splits being assessed. additionally the work claims that the tuning of various hyperparameters had little impact on the overall model performance.



recently, approaches exploiting knowledge graphs are being leveraged within the drug discovery domain to solve key tasks[34,35]. in a drug discovery knowledge graph, entities often represent key elements such as genes, disease or drugs, whilst the relations between them capture interactions. many important tasks in drug discovery can then be considered as predicting missing links between these entities. for example, performing drug target identification, the process of finding genes involved in the mechanism of a given disease, has been addressed as link prediction between gene and disease entities using the complex model on a drug discovery graph.



there are increasing numbers of public knowledge graphs suitable for use in drug discovery. one of the first such graphs was hetionet, originally created for drug purposing through the use of knowledge graph-based approaches. since its introduction, other datasets have been released including the drug repurposing knowledge graph(drkg), openbiolink and biokg.



as detailed in section 2.1, many knowledge graph specific embedding models have been introduced, with the primary differentiator between them being how they score the plausibility of a given triple. here we briefly detail the models utilised in this study, but interested readers are referred to larger reviews for context and comparisons with other approaches[17,18,38]. the models we have selected are popular approaches from the literature, cover a range of different methodologies and have begun to be explored in the context of drug discovery[5,39].



one or many-to-many relations being present in a knowledge graph. transh to help address the issues of transe, another translational distance based model entitled transh has been introduced. transh allows for entity embeddings to be given a different context depending upon the relation used in certain triple. this is achieved by modelling



it has been observed that biomedical knowledge graphs can exhibit a different topological structure than the benchmark datasets against which the models are typically tested. for example, hetionet has a higher average degree than datasets like fb15k-237 and wn18rr. however it remains unknown how this impacts kge model performance and we leave a detailed comparison between knowledge graphs from the biomedical and other domains for further work.



graph. however there are a series of choices one can make in this evaluation process which can drastically alter the results, thus ultimately making direct comparisons between published results challenging. hence the evaluation protocol is one of the crucial aspects for reproducibility, as the choices made can have a large impact on comparative performance. here we describe our own evaluation procedure in full which closely follows the one established in ali et al..



the model than these corrupted triples. one decision that needs to be taken is if any true triples, those that are already part of g, in the corrupted sets are removed before scoring. following prior work[12,15], we use the filtered evaluation setting where we remove any corrupted triple which is already in the graph, as their presence can skew the results.



it is possible that two or more triples in the test set are given the same score by the model when all are being ranked and how this situation is handled can also affect the results. one can assume the extremes, where the true triple is assumed to be at the start or end of the ranked list. for this work we present the mean of the rank using these two assumptions.



metrics we employ commonly used knowledge graph performance metrics including mean reciprocal rank(mrr) and hits@k(see for definitions). additionally we use the recently introduced adjusted mean rank(amr) owing to its ability to allow comparison between graphs of different sizes. however we would like to highlight that using metrics alone, especially for use cases like drug discovery where model predictions will often result in real-world lab-based experiments being performed, perhaps should not be the sole way in which models are judged.



all work has been performed using the pykeen framework, a python library for knowledge graph embeddings built on top of pytorch. additionally we use the optuna library to perform the hyperparameter optimisation. all experiments were performed on machines with intel(r) xeon(r) gold 5218 cpus and nvidia(r) v100 32 gb gpus. additionally, we kept the software environment consistent throughout all experimentation using python 3.8, cuda 10.1, pytorch 1.7, optuna 2.3 and pykeen 1.0.6.



given enough repeats, a random search can happen upon near optimal parameters by chance, at least for these models and datasets. indeed given the additional average runtime incurred by tpe, a random search may be the better balance of runtime and performance. one final observation is the negative correlation between the amr and hits@10 performance, which is encouraging to see as the hpo search was only optimising the amr value.



pared to complex, rotate demonstrates a decrease in performance using the gene-disease tuned hyperparameters, suggesting it is seemingly not being able to discover an optimal set of hyperparameters using the much sparser signal offered by the reduced edge counts. it is also possible that, given the much smaller size of the validation set used for hyperparameter tuning, that the model was over-fitting to these limited edges and not being able to generalise to unseen examples. overall these experiments suggest that, when a model performs well overall, hyperparameters learned across all relation types can still be optimal for use in a relation type specific setting.



overall observations our results show rotate is often the best performing model of the five on both datasets and throughout all the experiments. this reinforces previous similar findings[15,30] and shows rotate to be a strong baseline in the context of drug discovery. our results also highlight that older approaches like transe can still be very competitive given an optimised training and hyperparameter setup. regarding training setup choices, we found that nssa and adagrad were often the best performing approaches and could serve as starting points for further comparisons. more generally, it can seen that kge models are more than just architectures and should be considered in combination with their training setup and hyperparameter values.



we assessed model performance at target discovery by predicting links specifically between gene and disease entities. this showed that, although predictive performance was comparable to when measured on relations of all types, there were some differences. this suggests that researchers should not assume that model performance at the general link prediction task is indicative of performance in a more focused application. additionally, we highlighted that performing hpo to optimise a single relation type can actually hurt downstream performance on that type in a limited data setting. the issue of potentially trivial examples being present deserves continued attention, especially in the context of drug discovery where the complexity of the underlying data could amplify the risks.



the authors would like to thank ufuk kirik, manasa ramakrishna, tomas bastys, elizaveta semenova and claus bendtsen for help and feedback throughout the preparation of this manuscript. additionally, we would like to thank all of the pykeen team, especially max berrendorf and mehdi ali for their help and support. we would also like to acknowledge the use of the science compute platform(scp) within astrazeneca. stephen bonner is a fellow of the astrazeneca postdoctoral program.



