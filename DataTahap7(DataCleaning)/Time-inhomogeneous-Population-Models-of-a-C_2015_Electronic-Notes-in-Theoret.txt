organisations such as research institutions and universities often increase utilisation of their office workstations by deploying a high-throughput cycle-stealing distributed system. such systems allow users to submit a large number of computing tasks into a central pool. the system observes activity of workstations and continually assigns tasks to idle machines. when a user becomes active on the machine, the scheduler interrupts the task execution. this approach can significantly increase utilisation of the resources. however, it can also lead to wastage of computing cycles if tasks get interrupted too often.



in this paper, we develop a detailed population continuous time markov chain(pctmc) model of the whole system that accurately captures the contention between the interactive users and high-throughput tasks. the pctmc framework is well suited to the inherently time-inhomogeneous nature of the user behaviour and allows to capture a large number of performance and energy consumption metrics. we fit the pctmc model to real data and propose a methodology to forecast cluster availability in the near future. we show how to use historically collected and live data to parametrise the pctmc model and use efficient fluid analysis techniques to predict the desired metrics. additionally, the fast analysis enables exploration of various what-if scenarios. we demonstrate a working implementation of the method using the existing gpa tool for analysis of pctmc models. we argue that this methodology could allow the system maintainers to optimise the energy and performance parameters of the system. moreover, it would benefit the users who could use the model forecasts to better distribute and plan their large scale computations.



the effects of cycle-stealing and of similar computation distribution schemes have been extensively researched in the past. kelly et al present a multi-class closed queueing model of a cycle-stealing system. estrada et al developed a detailed simulation by emulating real software components of the boinc volunteer computing middleware to investigate different scheduling policies. our fluid analysis approach is related to the work of gast et al describing a mean-field model of work-stealing algorithms in computational grids. recently, guenther et al have proposed a forecasting algorithm using pctmc models in the context of bicycle sharing schemes. they have shown a comparable performance to traditional time-series analysis techniques.



these possible extensions give a good idea about the suitability of the pctmc framework to our problem. in section 4, we define a pctmc model where components represent computers in the system, interactive users(those using the machines in usual way such as for web browsing etc.), computing tasks and the central scheduler. each of the components has a number of states that represent the possible events in the system. due to a non-uniform resource pool and types of tasks, the model also captures connections between different resource and task types.



in total, around 1400 machines are taking part in the htcondor cluster. these are workstations from various classrooms and spaces around the campus. there is a large variety of usage patterns; some machines are used for teaching and are physically inaccessible outside teaching hours, some belong to 24 hour access facilities. some of the machines are located in halls of residence and experience higher use during weekends.



the htcondor cluster is accessible to a large number of researchers across different departments. during the monitored period, the number of jobs submitted to the htcondor pool has been small relative to the size of the system. this has increased since and we are hoping that a reliable forecasting tool would enable a larger variety of tasks to be run on the cluster.



htcondor is able to keep detailed information about the state of the workstations and submitted jobs, storing every timestamped state change with additional information. because the pctmc framework represents the system at a more abstract level, we were able to aggregate this data into a much more compact form. instead of storing information about individual workstations, we are only interested in the number of workstations. to capture the variety of different patterns of user behaviour and difference energy profiles of the hardware, we maintain separate populations for classes of workstations(teaching labs, library etc.).



one way to fit a pctmc model of the user behaviour is to consider individual user arrivals and departures. these represent changes in the pctmc populations and therefore correspond to successful transitions in the model. each time between two successive user arrivals will be a sample from the exponential delay corresponding to the transition. we can take the samples from a time window around a time t(for example one hour) and use these to estimate the transition rate parameters for the model at time t. we can repeat this process to obtain the parameters throughout the whole period(one week).



simple statistical tests show that the delay distributions in our data are not exponential. however, experience shows that they can be accurately approximated by multi-phase coxian distributions. these can be included in the model in the form of new intermediate states of the pctmc components.



the components in the model are workstations, interactive users and htcondor jobsboth in the system queue and assigned onto workstations. for a better fit to the data, we consider a number of different workstations/user classes based on the opening hours and teaching patterns of their respective rooms. we also consider a number of different job classes. this is important when modelling hypothetical job submissions. for example, one class can consist of normal job submissions which



each machine becomes available to accept htcondor jobs after a short delay following a user departure. jobs are randomly assigned onto workstations. when a user acquires a machine serving a job, that job gets cancelled and moved back into the central queue. this is a simplification for illustration purposes. the model could also cope with a more realistic behaviour. for example, a standard strategy in htcondor systems is to suspend a job rather than evict it when an interactive user acquires the machine. the job is held optimistically on the machine before being evicted. if the user leaves during this period, the job is resumed on the existing machine.



model and is able to include the time inhomogeneous rate data. gpa then automatically generates the system of odes, which would otherwise be too complicated to derive manually. gpa numerically solves the odes and uses the solution to provide predictions of the metrics in section 2.2 for the forecast period. we repeat this at every successive non-overlapping forecast period throughout a sample day. in real applications, this analysis could be done at any point in time to predict the immediate future for the duration of the forecast period.



we demonstrated the technique on a real-life dataset collected from a year long deployment of htcondor at the newcastle university. we have proposed an aggregation of htcondor logs which can be used to fit parameters of the pctmc model. to cope with the non-exponential nature of the system, we fitted multi-phase coxian distributions to the arrival and departure delays. we use the pctmc model in a simple forecasting algorithm. at each point in time when a forecast is desired, we fit parameters of the model to historical data from the same time during the previous week. this way, we obtain a time-inhomogeneous pctmc model for the forecast period. we extend the model with hypothetical jobs that allow us to evaluate the possible effect of a user submitting a large batch of jobs. we have developed a working implementation of the technique using the existing gpa tool.



our ultimate goal is to develop a tool that can be deployed together with the htcondor system. the tool would regularly aggregate trace logs and automatically create and fit pctmc models. the predictions from the models would be used to suggest to the users the best possible way to submit their jobs. for example, users might have a choice between different granularity levels of splitting their highthroughput work. the tool could suggest an optimal split based on the predicted resource availability such that the wastage caused by interruptions from interactive users is minimised. the tool would also give users an idea of the expected job duration and thus help them to better plan their computing workflow. the models can also be used by the maintainers of the system to explore different parameters and scheduling policies in order to minimise the running costs while maintaining a good throughput for the htcondor computations.



