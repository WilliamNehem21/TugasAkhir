we employ constraints to declaratively specify the rules of a type system. starting from a constraint based formulation of a type system, we introduce special combinators in the type rules to specify in which order constraints may be solved. a solving strategy can then be chosen by giving a particular interpretation to these combinators, and the resulting list of constraints can be fed into a constraint solver; thus the gap between the declarative specification and the deterministic implementation is bridged. this design makes the solver simpler and easier to reuse. our combinators have been used in the development of a real-life compiler.



volpano and smith showed how security analysis can be specified as a type and effect system(type system for short). security analysis aims to reject programs that exhibit unsafe behaviour, i.e., when sensitive information may be copied to a location reserved for less sensitive information. therefore, it is considered to be a validating program analysis, and the implementer must not only implement the analysis, but also provide sensible feedback in case the analysis fails. providing this feedback can be a time consuming and arduous task.



to provide good feedback one may investigate the kinds of mistakes programmers make and use that information to construct a heuristics that can help in finding what is the most likely source of a mistake, cf. which does exactly that for the domain of haskell programming. in this case, the user has knowledge only of the program from which the inconsistent set of constraints was derived. error messages and hints must therefore always be phrased in terms to the source program. moreover, a mistake made in a haskell program might be corrected with a small modification on the part of the user, although such a small correction can change the set of constraints in a significant way.



in the constraint programming community there is quite a large body of work devoted to explaining inconsistencies in sets of constraints, cf.. in contrast with our situation, the user is typically given a set of constraints, and needs to, often interactively, find a valuation for the variables that make it consistent, and delete some constraints in case of over-constrainedness. moreover, the work tends to be of a general nature, applying to different domain areas. this makes the results more generally useful, but also less tailored to our particular domain.



phrasing and implementing domain-aware heuristics is quite an undertaking and by its nature largely language and analysis specific. therefore, it would be nice to have a more generic solution to the generation of feedback that can be more easily reused for different analyses, or even different programming languages. if needs be, heuristics can then be added later on as a refinement. furthermore, while a compiler is being built, it is usually not known what a good solving strategy might be, so we would like to avoid making decisions that determine the solving strategy, and indirectly the feedback provided by the compiler, until it has been completed and experimented with.



in this paper, we describe a framework that can be used by compiler builders to accomplish exactly this. to illustrate it, we show how the framework can be used in the context of type inferencing the polymorphic lambda calculus, i.e., we consider an analysis of the underlying types of, e.g., a security analysis. there is nothing in our development, however, that makes assumptions about the analysis or the programming language involved. the work is implemented as part of the top framework and has been used in the construction of a real-life compiler, the helium compiler for haskell.



the paper is structured as follows. after a section on motivation and application, we need some preliminaries to introduce types and constraints on types. we then consider a variant of the hindley-milner type system[15,1] which uses assumption sets and sets of constraints. in section 5 we introduce a modified type system blame on the type of f. the difference between what is reported as the source of the problem is due to a different strategy in solving constraints: hugs uses a bottom-up approach to solving constraints: first the subexpressions, then the whole expression, while ghc proceeds in a top-down fashion. objectively, there is no reason to prefer one over the other; it is largely a matter of taste on the part of the programmer. the main advantage of our approach is that with little effort both(and many more) strategies can be provided in a single implementation and that the programmer can himself choose which strategy suits him best. if the compiler by the nature of its implementation easily allows different constraint solving orders, we can experiment with several such traversals, see what they come up with, and use that information to come up with a better diagnosis of the problem. the design of our framework naturally allows this.



we now discuss the main lines of our approach. consider any type and effect system, say security analysis. separate the type and effect system into two different parts: a declarative specification in terms of constraints that need to be satisfied(notationally close to the usual type deduction rules), and a solver for the kinds of constraints used in the specification. the analysis process then becomes a matter of traversing the abstract syntax of the program, generating the constraints for the program and feeding the constraints to the solver, so that it can decide whether the constraints are consistent. put differently, a program analysis is performed by interpreting a program written in a constraint language. in that case, the specification describes the mapping from the source program to the constraint language, and the solver is an interpreter for that language.



before constraints are solved, a particular solving strategy is chosen by selecting a semantics for the ordering combinators, ensuring that a list of constraints results that can be fed into the solver. operationally, the ordering process is a third phase that takes place between the generation of constraints in the abstract syntax tree and solving the constraints. the important point here is that different strategies can be used without changing the compiler.



that the solving process imposes a certain bias is implicit in the side conditions for the generalization and instantiation constraints. to solve an instantiation constraint, the right hand side must be a type scheme and not a type scheme variable. this implies that the corresponding generalization constraint has been solved, and the type scheme variable was replaced by a type scheme. when we solve a generalization constraint, the polymorphic type variables in that type are quantified so that their former identity is lost. hence, these type variables should play no further role, which is exactly what actives determines.



before we actually discuss our combinators in detail, we give by way of example a specification of the hindley-milner type system formulated in terms of constraints. such a type system is the basis of a type and effect based analysis, e.g., security analysis, in which annotations are attached to the types, and constraints between the annotations need to be satisfied in order for the program to be valid for the analysis.



typically, the constraint tree has the same shape as the abstract syntax tree of the expression for which the constraints are generated. a constraint is attached to the node n where it is generated. furthermore, we may choose to associate it explicitly with one of the subtrees of n. some language constructs demand that some constraints must be solved before others, and we can encode this in the constraint tree as well.



should be considered before the constraints in tc2. the typical example is that of the constraints for the definition of a let and those for the body. when one considers the rewrite rules for our constraint language in section 3, this is not necessary, because the solver can determine that a given generalization constraint may be solved. however, this gives extra work for the solver, because it must essentially search the set for constraints that may be solved. by insisting that the constraints from the definition are solved before the generalization constraints, we can omit to verify the side conditions for the instantiation and generalization constraints altogether and thereby speed up and simplify the solving process considerably.



the first argument[a] corresponds to the constraints belonging to the node itself, the second argument[([a],[a])] contains pairs of lists of constraints, one for each child. the first element of such a pair contains the constraints for the(recursively flattened) subtree, the second element those constraints that the node associates with the subtree. note that if we did not have both and o, then a treewalk would only take the constraints associated with the node itself, and a list containing the lists of constraints coming from the children as a parameter.



this tree walk puts the recursively flattened constraint subtrees up front, while preserving the order of the trees. these are followed by the constraints associated with each subtree in turn. finally, we append the constraints attached to the node itself. in a similar way, we define the dual top-down tree walk: which infers tuples from right to left, while most other constructs are inferred leftto-right. it also allows us to emulate all instances of g, such as exhibiting m-like behavior for one construct and w-like behavior for another. of course, we could generalize flatten even further to include other orderings. for example, a tree walk that visits the subtree with the most type constraints first.



folklore algorithm m, on the other hand, is a top-down inference algorithm for which we should select the topdown tree walk. spreading with this tree walk implies that we no longer fail at application or conditional nodes, but for identifiers and lambda abstractions. we note that the helium compiler provides flags-m and-w to mimic algorithm m and w respectively, showing that our combinators can indeed be used to give control over the constraint solving order to the programmer. other strategies can be provided easily; that is simply a matter of associating a treewalk with a particular compiler flag.



spreading type constraints gives constraint orderings that correspond more closely to the type inference process of hugs and ghc. regarding the inference process for a conditional expression, both compilers constrain the type of the condition to be of type bool before continuing with the then and else branches. ghc constrains the type of the condition even before its type is inferred: hugs constrains this type afterwards. therefore, the inference process of hugs for a conditional expression corresponds to an inorder bottom-up tree walk. the behavior of ghc can be mimicked by an inorder top-down tree walk.



same comparisons. for a type incorrect program, lee and yi proved that algorithm m considers the least number of constraints before finding an error, and algorithm w the most. they actually advocate using something in the middle: algorithm m complains too soon(and therefore gives little context in its error message) and algorithm w too late. in practice, however, the differences are hardly noticeable, and we believe that spending a bit more time to come up with a better judgement of the problem easily outweighs the extra expense. note also that in our particular situation, the programmer can choose the order he prefers, and therefore either choose a slower or faster solving order. this is an additional, albeit somewhat accidental, feature of our work.



we use our framework in the helium compiler and experience shows that constraint generation and re-ordering only take up a small amount of time. the compiler is not much faster or slower than other compilers for haskell. it is certainly fast enough for interactive program development with an ide.



a soundness proof for a program analysis typically has two parts: first prove the logical deduction system sound with respect to the semantics of the programming language, and then prove the correctness of the algorithm with respect to this deduction system. the soundness of the algorithm then follows by transitivity. in this section we sketch the main steps in conducting such a proof for the type system we have described above. at the end of this section we reflect back on this and indicate how one would proceed in a slightly different but maybe more usual scenario.



a constraint tree,(2) uses an arbitrary treewalk to flatten the tree into a list of constraints and then(3) proceeds to solve these in the order in which they are listed, returns the most general solution for the constraints in the constraint tree. the crucial realization is now that our choice to solve the constraints from a let definition before those of the corresponding let body, ensures that we never block on the side conditions of the rules for the generalization and instantiation constraints. this ensures that we obtain the most general solution if we solve the constraints in the listed order and justifies leaving out those side conditions from the solver, as long as we use a solving order that follows from a valid interpretation of the combinators. soundness then follows from theorem 3.1.



if there is no other analysis to relate to, a soundness proof must be constructed from scratch. note that in such a proof, the ordering combinators play no role of importance, so they do not add to the complexity of the proof. since such a proof needs to conducted anyway, we are no worse off than without the combinators.



we are not the first to consider a more flexible approach in solving constraints. algorithm g, presented by lee and yi, can be instantiated with different parameters, yielding the well-known algorithms w and m(and many others) as instances. their algorithm essentially allows to consider certain constraints earlier in the type inference process. our constraint-based approach has a number of advantages: the soundness of their algorithm follows from the decision to simply perform all unifications before the abstract syntax tree node is left for the final time. this includes unifications which were done during an earlier visit to the node, which is harmless, but not very efficient. additionally, all these moments of performing unifications add complexity to the algorithm: the application case alone involves five substitutions that have to be propagated carefully. our constraint-based approach circumvents this complexity. instances of algorithm g are restricted to one-pass, left-to-right traversals with a type environment that is passed top-down: it is not straightforward to extend this to algorithms that remove the left-to-right bias[24,14].



sulzmann presents constraint propagation policies for modeling w and m in the hm(x) framework. first, general type rules are formulated that mention partial solutions of the constraint problem: later, these rules are specialized to obtain w and m. while interesting soundness and completeness results are discussed for his system, he makes no attempt at defining one implementation that can handle all kinds of propagation policies.



in this paper we have advocated the introduction of a separate constraint ordering phase between the phase that generates the constraints and the phase that solves constraints. we have presented a number of combinators that can be used in the type rules to specify restrictions and, contrarily, degrees of freedom on the order in which constraints may be solved. the freedom can be used to influence the order in which constraints are solved in order to control the decision which constraint will be blamed for an inconsistency, and ultimately, what type error message may result. the restrictions can be used to simplify the solver, so that side conditions do not need to be checked. this may also simplify proofs of correctness, which should follow from the interplay between the use of ordering combinators and the solver. these proofs of soundness should consider all possible solving orders allowed by the ordering combinators.



the combinators we described are only the beginning. once the realization is made that the ordering of constraints is an issue, it is not difficult to come up with a host of new combinators, each with their own special characteristics and uses. for example, combinators can be defined that specify that certain parts of the constraint solving process can be performed in parallel, guaranteeing that the results of these parallel executions can be easily integrated.



