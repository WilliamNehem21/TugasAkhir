a web application could be visited for different purposes. it is possible for a web site to be visited by a regular user as a normal (natural) visit, to be viewed by crawlers, bots, spiders, etc. for indexing purposes, lastly to be exploratory scanned by malicious users prior to an attack. an attack targeted web scan can be viewed as a phase of a potential attack and can lead to more attack detection as compared to traditional detection methods. in this work, we propose a method to detect attack-oriented scans and to distinguish them from other types of visits. in this context, we use access log files of apache (or iss) web servers and try to determine attack situations through examination of the past data. in addition to web scan detec- tions, we insert a rule set to detect sql injection and xss attacks. our approach has been applied on sam- ple data sets and results have been analyzed in terms of performance measures to compare our method and other commonly used detection techniques. furthermore, various tests have been made on log sam- ples from real systems. lastly, several suggestions about further development have been also discussed.



malicious activities. actually, all machine learning algorithms have training phase and training data to built a classification model. in order to increase accuracy of machine learning classi- fier model, a large scale input training data is needed. in turn, an increase in memory consumption would occur. as a result, either the model would turn out to be not trainable, or training phase would last for days. on the other hand, executing the pro- posed rule set on access logs does not cause any memory con- sumption problems. our script simply runs on ubuntu terminal with a single line of code.



the rest of the paper is organized as follows: the related work is presented in section 2. section 3 presents our system model in details. our model evaluation and real system test results are pre- sented in section 4. the concluding remarks are given in section 5.



auxilia and tamilselvan suggest a negative security model for intrusion detections in web applications [7]. this method is one of the dynamic detection techniques that is anomaly-based. the authors propose to use web application firewall (waf) with a rule set protecting web applications from unknown vulnerabilities. when analyzed their rules for hypertext transfer protocol (http) attacks detection, the rules appears to be generated by checking the values of some important http header fields, uniform resource identifier (uri) strings, cookies, etc. associating waf, intrusion detection system (ids), rule engine reasoning together makes this article interesting.



goseva-popstojanova et al. [8] propose a method to classify malicious web sessions through web server logs. firstly, the authors constitute four different data sets from honeypots; on which several web applications were installed. afterwards, 43 dif- ferent features were extracted from web sessions to characterize each session and three machine learning methods that are support vector machine (svm), j48 and partial decision trees (part) were used to make the classifications. the authors assert that when all 43 features used in learning period, their method to distinguish between attack and vulnerability scan sessions attains high accu- racy rates with low probability of false alarms. this comprehensive research provides significant contribution in the area of web security.



in their work, stevanovic et al. [13] use som and modified adaptive resonance theory 2 (modified art2) algorithms for training and 10 features related to web sessions for clustering. then, the authors label these sessions as human visitors, well- behaved web crawlers, malicious crawlers and unknown visitors. in addition to classifying web sessions, similarities among the browsing styles of google, msn, and yahoo are also analyzed in this article. the authors obtain lots of interesting results, one of which is that 52% of malicious web crawlers and human visitors are similar in their browsing strategies; which means that it is hard to distinguish each other.



there are several differences between our work and the above mentioned works. firstly, as in the most of the related works, checking only the user-agent header field from a list is not enough to detect web crawlers in the correct way. correspondingly, we add extra fields to check to make the web crawler detection more accu- rate. additionally, unlike machine learning and data-mining, rule-based detection has been used in the proposed model. finally, in contrast to other works, we prefer to use combined log format in order to make the number of features larger and to get more con- sistent results.



http server. as mentioned earlier, apache/2.4.7 (ubuntu) server is chosen as a web server. apache is known to be the most commonly used web server. according to the w3techs (web tech- nology surveys) [15], as of december 1, 2016; apache is used by



in this work, dvwa 1.0.8 version (release date: 11/01/2011) is used. to install this web application, linux apache mysql php (lamp) server; including mysql, php5, and phpmyadmin, has been installed. the reasons for studying with dvwa are to better understand xss and sql injection attacks and to find out related payloads substituted in query string part of uris. in this way, rule selection to detect these attacks from access logs could be correctly determined. also, web vulnerability scanners used in this work, have scanned this web application for data collection purposes.



as mentioned earlier, our script runs on access log files. the main reason for this choice is the opportunity for detailed analysis about users actions. by examining past data, information security policies for the web applications could be correctly created and implemented. additionally, further exploitations could be pre- vented in advance. unlike the proposed model, network intrusion trators might run our script every day to check for an attack. lastly, real-time detection and prevention is not possible with the pro- posed method which runs off-line. thus, we could not guarantee to run on-line. in fact, this approach is conceptually sufficient for the scope of this work. differently from the test environment; an extra module that directly accesses logs, or a script that analyses logs faster could be developed to use our approach in a live or real environment.



afterwards, we continue by separating ip addresses of type 2 from the rest of the access log file in phase 2. to do this, two differ- ent approaches are used. firstly, user-agent part of all log entries is compared with the user-agent list from robots database that is publicly available in [18]. however, since this list may not be up- to-date, another bot detection rules are added. in order to identify these rules, we use the following observations about web robots:



since there are not any actual, publicly available and labelled data sets to evaluate our model, we create our data sets. in fact, we deploy two different web applications on two different web servers to form type 1 and type 3 traffics. details are expressed in section 3.2.2.



more specifically, the accuracy provides the percentage of type 3 that are detected correctly. the precision determines the fraction of ip addresses correctly classified as type 3 over all ip addresses classified as type 3. the recall (a.k.a. sensitivity) is the fraction of ip addresses correctly classified as type 3 over all ip addresses of type 3. finally, the f1-score is a harmonic mean of precision and



in this work, we studied web vulnerability scans detection through access log files of web servers in addition to detection of xss and sqli attacks. in accordance with this purpose, we used rule-based methodology. firstly, we examined the behavior of the automated vulnerability scanners. moreover, we implemented our model with a python script. afterwards, our model has been evaluated based on data we have collected. finally, we tested our model on the log samples from real systems.



it is clear that our method has very high probability of detection and low probability of false alarm. more specifically, the accuracy and the precision rates of our model are 99.38%, 100.00% respec- tively. more importantly, malicious scans can be captured more precisely because different types of scanning tools including both open source and commercial tools were examined. therefore, our results indicates that static rules can detect successfully web vul- nerability scans. besides, we have observed that our model func- tions properly with larger and live data sets and correctly detects type 3 ip addresses.



future work considerations related to this work are twofold. in the first place, one could make our model possible to analyze other log files such as audit log and error log. secondly, in addition to the scope of this work; different from sqli and xss attacks, other well- known web application attacks like csrf could be addressed too.



