

cheminformatics describes the use of information technology to handle chemical information. the field itself has been integrated with the chemical sciences for many decades. however, the term was coined relatively recently, when the increasing amount of chemical data generated within the drug discovery field(e.g., due to the implementation of combinatorial chemistry and high-throughput screening platforms) made the use of chemical information technologies increasingly mandatory. although cheminformatics has a wide scope, core tasks within the cheminformatics field include the management of chemical databases and datasets, storage and retrieval of chemical information, structure-property relationships, in silico screening, and the design of combinatorial and focused libraries.





several data science and machine learning platforms have been developed thus far, enabling users with limited coding ability to create pipelines for data exploration, analysis, and mining. some well-known examples include knime, pipeline pilot, and alteryx. although they offer free and open-source distributions, most advanced features or thirdparty applications are only available under commercial licenses, which sometimes makes them inaccessible to small research units in developing countries.



all applications within the lideb tools suite were deployed as web apps on the streamlit platform, so scientists can use them through a user-friendly interface using computational resources allocated to the cloud. alternatively, their standalone distributions are written in plain python, under an object-oriented programming(oop) paradigm and using open-source libraries, offering a higher level of customization and code reusability. moreover, they can easily be plugged into existing chemoinformatic pipelines.



in the input step, the molecules to be clustered should be provided as a.csv file, where each line corresponds to a molecule in smiles format. after optional standardization employing molvs, mordred is used to compute 1613 molecular descriptors for each molecule. alternatively, users may provide their own pool of molecular descriptors as tab-delimited.txt file.



in the input step, the molecules are provided again in smiles notation as a.csv file, but instead of calculating molecular descriptors, the molecular representations are encoded into estate1 molecular fingerprints, calculated by rdkit, with a fixed-length fingerprint containing 79 features. the umap algorithm, a nonlinear dimensionality reduction technique based on riemannian geometry and algebraic topology, is then applied to reduce the hyperdimensional space of the fingerprints, retaining the most informative features of the process. the size of the embedded space is determined based on the size of the dataset. in the clustering step, the gmm algorithm is applied to the molecules in the embedded umap space to search for the number of k clusters that maximizes the silhouette score. an elbow plot of the silhouette score vs. k is presented for visual estimation of the optimal k. the same additional cvis previously described for irapca are also calculated and the same output files are generated.



once the optimal cutoff has been identified, chica is re-run with this distance value, generating four output files: clustering_assignations.csv which specifies which molecules have been assigned to which cluster according to the chosen cutoff value; cluster_distribution.csv, which specifies the number of obtained clusters and their sizes; validations.csv which contains the values of the cvis; and settings.csv, which contains the selected parameters in the run.



windows can be narrowed or expanded by the user. if fewer than 400 molecules are recovered in this randomly chosen chembl subset, the property limits are extended by a factor of 1.5, up to five times. for instance, in the case of mw the window is extended, round by round, to



preliminary list of decoys are estimated. for this, the difference between each known active compound and each decoy in terms of the six pairing properties is calculated and summarized with a physicochemical similarity score(pss) ranging from 0(lowest similarity) to 1(highest similarity). the compounds are then ordered with descending pss values, and the top 200 are selected.



the quality of the generated decoy molecules, in terms of the physicochemical matching of decoys and the risk of allowing latent active compounds in the decoy set(lads), was determined by the deviation from the optimal embedding score(doe score) and the assessment of the doppelganger score, respectively, as recommended by vogel et al.. the doe score was obtained by analyzing the spatial distances of the molecules in the chemical space defined by the six normalized physicochemical properties used to match queries and decoys. briefly, the distances from each active compound to all remaining active compounds and decoys were calculated in the normalized multidimensional space and molecules were sorted in ascending order according to these distances. the real class of each molecule(labeled 1 for active compounds and 0 for decoys) and the obtained distances were later used to build a receiving operating characteristic(roc) curve for each active compound, and the average absolute value of the difference between the area of these roc curves and the area of a randomly sorted list of compounds(0.5) was defined as the doe score:



seventy percent of the proteins in each class was sampled for the training set of the classifiers. the remaining proteins were used as a test set to validate the generated models externally. to realize this sampling representatively, we used a clustering strategy that combined dimensionality reduction of zernike descriptors by principal component analysis(pca) followed by the application of the k-means clustering algorithm.



in contrast, in the randomization test the class label was randomized across the proteins comprising the training set. the training set with the randomized variable was used to train new models, from the descriptor selection step. this procedure was repeated 1000 times for each descriptor subset. randomized models are expected to have poor accuracy compared with real models.







